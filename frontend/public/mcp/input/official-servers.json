{
  "category": "official-servers",
  "categoryDisplay": "Official Servers",
  "description": "",
  "totalRepositories": 39,
  "repositories": {
    "21st-dev--magic-mcp": {
      "owner": "21st-dev",
      "name": "magic-mcp",
      "url": "https://github.com/21st-dev/magic-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/21st-dev.webp",
      "description": "Magic MCP is an AI tool that allows developers to create user interface components simply by describing them in everyday language. It works with popular coding environments, making the UI design process faster and more intuitive.",
      "stars": 3741,
      "forks": 246,
      "license": "No License",
      "language": "TypeScript",
      "updated_at": "2025-10-01T20:15:48Z",
      "readme_content": "# 21st.dev Magic AI Agent\n\n![MCP Banner](https://21st.dev/magic-agent-og-image.png)\n\nMagic Component Platform (MCP) is a powerful AI-driven tool that helps developers create beautiful, modern UI components instantly through natural language descriptions. It integrates seamlessly with popular IDEs and provides a streamlined workflow for UI development.\n\n## üåü Features\n\n- **AI-Powered UI Generation**: Create UI components by describing them in natural language\n- **Multi-IDE Support**:\n  - [Cursor](https://cursor.com) IDE integration\n  - [Windsurf](https://windsurf.ai) support\n  - [VSCode](https://code.visualstudio.com/) support\n  - [VSCode + Cline](https://cline.bot) integration (Beta)\n- **Modern Component Library**: Access to a vast collection of pre-built, customizable components inspired by [21st.dev](https://21st.dev)\n- **Real-time Preview**: Instantly see your components as you create them\n- **TypeScript Support**: Full TypeScript support for type-safe development\n- **SVGL Integration**: Access to a vast collection of professional brand assets and logos\n- **Component Enhancement**: Improve existing components with advanced features and animations (Coming Soon)\n\n## üéØ How It Works\n\n1. **Tell Agent What You Need**\n\n   - In your AI Agent's chat, just type `/ui` and describe the component you're looking for\n   - Example: `/ui create a modern navigation bar with responsive design`\n\n2. **Let Magic Create It**\n\n   - Your IDE prompts you to use Magic\n   - Magic instantly builds a polished UI component\n   - Components are inspired by 21st.dev's library\n\n3. **Seamless Integration**\n   - Components are automatically added to your project\n   - Start using your new UI components right away\n   - All components are fully customizable\n\n## üöÄ Getting Started\n\n### Prerequisites\n\n- Node.js (Latest LTS version recommended)\n- One of the supported IDEs:\n  - Cursor\n  - Windsurf\n  - VSCode (with Cline extension)\n\n### Installation\n\n1. **Generate API Key**\n\n   - Visit [21st.dev Magic Console](https://21st.dev/magic/console)\n   - Generate a new API key\n\n2. **Choose Installation Method**\n\n#### Method 1: CLI Installation (Recommended)\n\nOne command to install and configure MCP for your IDE:\n\n```bash\nnpx @21st-dev/cli@latest install <client> --api-key <key>\n```\n\nSupported clients: cursor, windsurf, cline, claude\n\n#### Method 2: Manual Configuration\n\nIf you prefer manual setup, add this to your IDE's MCP config file:\n\n```json\n{\n  \"mcpServers\": {\n    \"@21st-dev/magic\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@21st-dev/magic@latest\", \"API_KEY=\\\"your-api-key\\\"\"]\n    }\n  }\n}\n```\n\nConfig file locations:\n\n- Cursor: `~/.cursor/mcp.json`\n- Windsurf: `~/.codeium/windsurf/mcp_config.json`\n- Cline: `~/.cline/mcp_config.json`\n- Claude: `~/.claude/mcp_config.json`\n\n#### Method 3: VS Code Installation\n\nFor one-click installation, click one of the install buttons below:\n\n[![Install with NPX in VS Code](https://img.shields.io/badge/VS_Code-NPM-0098FF?style=flat-square&logo=visualstudiocode&logoColor=white)](https://insiders.vscode.dev/redirect/mcp/install?name=%4021st-dev%2Fmagic&config=%7B%22command%22%3A%22npx%22%2C%22args%22%3A%5B%22-y%22%2C%22%4021st-dev%2Fmagic%40latest%22%5D%2C%22env%22%3A%7B%22API_KEY%22%3A%22%24%7Binput%3AapiKey%7D%22%7D%7D&inputs=%5B%7B%22type%22%3A%22promptString%22%2C%22id%22%3A%22apiKey%22%2C%22description%22%3A%2221st.dev+Magic+API+Key%22%2C%22password%22%3Atrue%7D%5D) [![Install with NPX in VS Code Insiders](https://img.shields.io/badge/VS_Code_Insiders-NPM-24bfa5?style=flat-square&logo=visualstudiocode&logoColor=white)](https://insiders.vscode.dev/redirect/mcp/install?name=%4021st-dev%2Fmagic&config=%7B%22command%22%3A%22npx%22%2C%22args%22%3A%5B%22-y%22%2C%22%4021st-dev%2Fmagic%40latest%22%5D%2C%22env%22%3A%7B%22API_KEY%22%3A%22%24%7Binput%3AapiKey%7D%22%7D%7D&inputs=%5B%7B%22type%22%3A%22promptString%22%2C%22id%22%3A%22apiKey%22%2C%22description%22%3A%2221st.dev+Magic+API+Key%22%2C%22password%22%3Atrue%7D%5D&quality=insiders)\n\n##### Manual VS Code Setup\n\nFirst, check the install buttons above for one-click installation. For manual setup:\n\nAdd the following JSON block to your User Settings (JSON) file in VS Code. You can do this by pressing `Ctrl + Shift + P` and typing `Preferences: Open User Settings (JSON)`:\n\n```json\n{\n  \"mcp\": {\n    \"inputs\": [\n      {\n        \"type\": \"promptString\",\n        \"id\": \"apiKey\",\n        \"description\": \"21st.dev Magic API Key\",\n        \"password\": true\n      }\n    ],\n    \"servers\": {\n      \"@21st-dev/magic\": {\n        \"command\": \"npx\",\n        \"args\": [\"-y\", \"@21st-dev/magic@latest\"],\n        \"env\": {\n          \"API_KEY\": \"${input:apiKey}\"\n        }\n      }\n    }\n  }\n}\n```\n\nOptionally, you can add it to a file called `.vscode/mcp.json` in your workspace:\n\n```json\n{\n  \"inputs\": [\n    {\n      \"type\": \"promptString\",\n      \"id\": \"apiKey\",\n      \"description\": \"21st.dev Magic API Key\",\n      \"password\": true\n    }\n  ],\n  \"servers\": {\n    \"@21st-dev/magic\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@21st-dev/magic@latest\"],\n      \"env\": {\n        \"API_KEY\": \"${input:apiKey}\"\n      }\n    }\n  }\n}\n```\n\n## ‚ùì FAQ\n\n### How does Magic AI Agent handle my codebase?\n\nMagic AI Agent only writes or modifies files related to the components it generates. It follows your project's code style and structure, and integrates seamlessly with your existing codebase without affecting other parts of your application.\n\n### Can I customize the generated components?\n\nYes! All generated components are fully editable and come with well-structured code. You can modify the styling, functionality, and behavior just like any other React component in your codebase.\n\n### What happens if I run out of generations?\n\nIf you exceed your monthly generation limit, you'll be prompted to upgrade your plan. You can upgrade at any time to continue generating components. Your existing components will remain fully functional.\n\n### How soon do new components get added to 21st.dev's library?\n\nAuthors can publish components to 21st.dev at any time, and Magic Agent will have immediate access to them. This means you'll always have access to the latest components and design patterns from the community.\n\n### Is there a limit to component complexity?\n\nMagic AI Agent can handle components of varying complexity, from simple buttons to complex interactive forms. However, for best results, we recommend breaking down very complex UIs into smaller, manageable components.\n\n## üõ†Ô∏è Development\n\n### Project Structure\n\n```\nmcp/\n‚îú‚îÄ‚îÄ app/\n‚îÇ   ‚îî‚îÄ‚îÄ components/     # Core UI components\n‚îú‚îÄ‚îÄ types/             # TypeScript type definitions\n‚îú‚îÄ‚îÄ lib/              # Utility functions\n‚îî‚îÄ‚îÄ public/           # Static assets\n```\n\n### Key Components\n\n- `IdeInstructions`: Setup instructions for different IDEs\n- `ApiKeySection`: API key management interface\n- `WelcomeOnboarding`: Onboarding flow for new users\n\n## ü§ù Contributing\n\nWe welcome contributions! Please join our [Discord community](https://discord.gg/Qx4rFunHfm) and provide feedback to help improve Magic Agent. The source code is available on [GitHub](https://github.com/serafimcloud/21st).\n\n## üë• Community & Support\n\n- [Discord Community](https://discord.gg/Qx4rFunHfm) - Join our active community\n- [Twitter](https://x.com/serafimcloud) - Follow us for updates\n\n## ‚ö†Ô∏è Beta Notice\n\nMagic Agent is currently in beta. All features are free during this period. We appreciate your feedback and patience as we continue to improve the platform.\n\n## üìù License\n\nMIT License\n\n## üôè Acknowledgments\n\n- Thanks to our beta testers and community members\n- Special thanks to the Cursor, Windsurf, and Cline teams for their collaboration\n- Integration with [21st.dev](https://21st.dev) for component inspiration\n- [SVGL](https://svgl.app) for logo and brand asset integration\n\n---\n\nFor more information, join our [Discord community](https://discord.gg/Qx4rFunHfm) or visit [21st.dev/magic](https://21st.dev/magic).\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "mcp",
        "ai",
        "magic",
        "magic mcp",
        "mcp magic",
        "mcp ai"
      ],
      "category": "official-servers"
    },
    "apify--apify-mcp-server": {
      "owner": "apify",
      "name": "apify-mcp-server",
      "url": "https://github.com/apify/apify-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/apify.webp",
      "description": "Enable interaction with various Apify Actors to perform specific tasks through an MCP server configuration. It supports HTTP server access via Server-Sent Events (SSE) and local server functionality through standard input/output.",
      "stars": 429,
      "forks": 51,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-10-02T00:20:27Z",
      "readme_content": "<h1 align=\"center\">\n    <a href=\"https://mcp.apify.com\">\n        <picture>\n            <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://raw.githubusercontent.com/apify/apify-mcp-server/refs/heads/master/docs/apify_mcp_server_dark_background.png\">\n            <img alt=\"Apify MCP Server\" src=\"https://raw.githubusercontent.com/apify/apify-mcp-server/refs/heads/master/docs/apify_mcp_server_white_background.png\" width=\"500\">\n        </picture>\n    </a>\n    <br>\n    <small><a href=\"https://mcp.apify.com\">mcp.apify.com</a></small>\n</h1>\n\n<p align=center>\n    <a href=\"https://www.npmjs.com/package/@apify/actors-mcp-server\" rel=\"nofollow\"><img src=\"https://img.shields.io/npm/v/@apify/actors-mcp-server.svg\" alt=\"NPM latest version\" data-canonical-src=\"https://img.shields.io/npm/v/@apify/actors-mcp-server.svg\" style=\"max-width: 100%;\"></a>\n    <a href=\"https://www.npmjs.com/package/@apify/actors-mcp-server\" rel=\"nofollow\"><img src=\"https://img.shields.io/npm/dm/@apify/actors-mcp-server.svg\" alt=\"Downloads\" data-canonical-src=\"https://img.shields.io/npm/dm/@apify/actors-mcp-server.svg\" style=\"max-width: 100%;\"></a>\n    <a href=\"https://github.com/apify/actors-mcp-server/actions/workflows/check.yaml\"><img src=\"https://github.com/apify/actors-mcp-server/actions/workflows/check.yaml/badge.svg?branch=master\" alt=\"Build Status\" style=\"max-width: 100%;\"></a>\n    <a href=\"https://apify.com/apify/actors-mcp-server\"><img src=\"https://apify.com/actor-badge?actor=apify/actors-mcp-server\" alt=\"Actor runs\" style=\"max-width: 100%;\"></a>\n</p>\n\nThe Apify Model Context Protocol (MCP) server at [**mcp.apify.com**](https://mcp.apify.com) enables your AI agents to extract data from social media, search engines, maps, e-commerce sites, or any other website using thousands of ready-made scrapers, crawlers, and automation tools available on the [Apify Store](https://apify.com/store).\n\n> **üöÄ Try the hosted Apify MCP Server!**\n>\n> For the easiest setup and most powerful features, including the ability to find and use any Actor from Apify Store, connect your AI assistant to our hosted server:\n>\n> **[`https://mcp.apify.com`](https://mcp.apify.com)**\n>\n> It supports OAuth, so you can connect from clients like Claude.ai or Visual Studio Code with just the URL.\n\n![Apify-MCP-server](https://raw.githubusercontent.com/apify/apify-mcp-server/refs/heads/master/docs/apify-mcp-server.png)\n\n## Table of Contents\n- [üåê Introducing the Apify MCP server](#-introducing-the-apify-mcp-server)\n- [üöÄ Quickstart](#-quickstart)\n- [ü§ñ MCP clients and examples](#-mcp-clients-and-examples)\n- [ü™Ñ Try Apify MCP instantly](#-try-apify-mcp-instantly)\n- [üõ†Ô∏è Tools, resources, and prompts](#-tools-resources-and-prompts)\n- [üêõ Troubleshooting (local MCP server)](#-troubleshooting-local-mcp-server)\n- [‚öôÔ∏è Development](#-development)\n- [ü§ù Contributing](#-contributing)\n- [üìö Learn more](#-learn-more)\n\n# üåê Introducing the Apify MCP server\n\nThe Apify MCP Server allows an AI assistant to use any [Apify Actor](https://apify.com/store) as a tool to perform a specific task.\nFor example, it can:\n- Use [Facebook Posts Scraper](https://apify.com/apify/facebook-posts-scraper) to extract data from Facebook posts from multiple pages/profiles.\n- Use [Google Maps Email Extractor](https://apify.com/lukaskrivka/google-maps-with-contact-details) to extract contact details from Google Maps.\n- Use [Google Search Results Scraper](https://apify.com/apify/google-search-scraper) to scrape Google Search Engine Results Pages (SERPs).\n- Use [Instagram Scraper](https://apify.com/apify/instagram-scraper) to scrape Instagram posts, profiles, places, photos, and comments.\n- Use [RAG Web Browser](https://apify.com/apify/web-scraper) to search the web, scrape the top N URLs, and return their content.\n\n**Video tutorial: Integrate 5,000+ Apify Actors and Agents with Claude**\n\n[![Apify MCP Server Tutorial: Integrate 5,000+ Apify Actors and Agents with Claude](https://img.youtube.com/vi/BKu8H91uCTg/hqdefault.jpg)](https://www.youtube.com/watch?v=BKu8H91uCTg)\n\n# üöÄ Quickstart\n\nYou can use the Apify MCP Server in two ways:\n\n**HTTPS Endpoint (mcp.apify.com)**: Connect from your MCP client via OAuth or by including the `Authorization: Bearer <APIFY_TOKEN>` header in your requests. This is the recommended method for most use cases. Because it supports OAuth, you can connect from clients like [Claude.ai](https://claude.ai) or [Visual Studio Code](https://code.visualstudio.com/) using just the URL: `https://mcp.apify.com`.\n- `https://mcp.apify.com` streamable transport\n\n**Standard Input/Output (stdio)**: Ideal for local integrations and command-line tools like the Claude for Desktop client.\n- Set the MCP client server command to `npx @apify/actors-mcp-server` and the `APIFY_TOKEN` environment variable to your Apify API token.\n- See `npx @apify/actors-mcp-server --help` for more options.\n\nYou can find detailed instructions for setting up the MCP server in the [Apify documentation](https://docs.apify.com/platform/integrations/mcp).\n\n# ü§ñ MCP clients and examples\n\nTo interact with the Apify MCP server, you can use various MCP clients, such as:\n- [Claude Desktop](https://claude.ai/download)\n- [Visual Studio Code](https://code.visualstudio.com/)\n- [Apify Tester MCP Client](https://apify.com/jiri.spilka/tester-mcp-client)\n- Other clients at [https://modelcontextprotocol.io/clients](https://modelcontextprotocol.io/clients)\n- More clients at [https://glama.ai/mcp/clients](https://glama.ai/mcp/clients)\n\nWith MCP server integrated, you can ask your AI assistant things like:\n- \"Search the web and summarize recent trends in AI Agents.\"\n- \"Find the top 10 Italian restaurants in San Francisco.\"\n- \"Find and analyze the Instagram profile of The Rock.\"\n- \"Provide a step-by-step guide on using the Model Context Protocol, including source URLs.\"\n- \"What Apify Actors can I use?\"\n\n### Supported clients matrix\n\nThe following table outlines the tested MCP clients and their level of support for key features.\n\n| Client | Dynamic Tool Discovery | Notes |\n| --- | --- | --- |\n| **Claude.ai (web)** | ‚úÖ Full | |\n| **Claude Desktop** | üü° Partial | Tools may need to be reloaded manually in the client. |\n| **VS Code (Genie)** | ‚úÖ Full | |\n| **Apify Tester MCP Client** | ‚úÖ Full | Designed for testing Apify MCP servers. |\n\nApify MCP Server is compatible with any MCP client that adheres to the [Model Context Protocol](https://modelcontextprotocol.org/), but the level of support for dynamic tool discovery and other features may vary between clients. Therefore, the server uses [mcp-client-capabilities](https://github.com/apify/mcp-client-capabilities) to detect client capabilities and adjust its behavior accordingly.\n\n**Smart tool selection based on client capabilities:**\n\nWhen the `actors` tool category is requested, the server intelligently selects the most appropriate Actor-related tools based on the client's capabilities:\n\n- **Clients with dynamic tool support** (e.g., Claude.ai web, VS Code Genie): The server provides the `add-actor` tool instead of `call-actor`. This allows for a better user experience where users can dynamically discover and add new Actors as tools during their conversation.\n\n- **Clients with limited dynamic tool support** (e.g., Claude Desktop): The server provides the standard `call-actor` tool along with other Actor category tools, ensuring compatibility while maintaining functionality.\n\n# ü™Ñ Try Apify MCP instantly\n\nWant to try Apify MCP without any setup?\n\nCheck out [Apify Tester MCP Client](https://apify.com/jiri.spilka/tester-mcp-client)\n\nThis interactive, chat-like interface provides an easy way to explore the capabilities of Apify MCP without any local setup.\nJust sign in with your Apify account and start experimenting with web scraping, data extraction, and automation tools!\n\nOr use the MCP bundle file (formerly known as Anthropic Desktop extension file, or DXT) for one-click installation: [Apify MCP server MCPB file](https://github.com/apify/apify-mcp-server/releases/latest/download/apify-mcp-server.mcpb)\n\n# üõ†Ô∏è Tools, resources, and prompts\n\nThe MCP server provides a set of tools for interacting with Apify Actors.\nSince the Apify Store is large and growing rapidly, the MCP server provides a way to dynamically discover and use new Actors.\n\n### Actors\n\nAny [Apify Actor](https://apify.com/store) can be used as a tool.\nBy default, the server is pre-configured with one Actor, `apify/rag-web-browser`, and several helper tools.\nThe MCP server loads an Actor's input schema and creates a corresponding MCP tool.\nThis allows the AI agent to know exactly what arguments to pass to the Actor and what to expect in return.\n\n\nFor example, for the `apify/rag-web-browser` Actor, the input parameters are:\n\n```json\n{\n  \"query\": \"restaurants in San Francisco\",\n  \"maxResults\": 3\n}\n```\nYou don't need to manually specify which Actor to call or its input parameters; the LLM handles this automatically.\nWhen a tool is called, the arguments are automatically passed to the Actor by the LLM.\nYou can refer to the specific Actor's documentation for a list of available arguments.\n\n### Helper tools\n\nOne of the most powerful features of using MCP with Apify is dynamic tool discovery.\nIt gives an AI agent the ability to find new tools (Actors) as needed and incorporate them.\nHere are some special MCP operations and how the Apify MCP Server supports them:\n\n- **Apify Actors**: Search for Actors, view their details, and use them as tools for the AI.\n- **Apify documentation**: Search the Apify documentation and fetch specific documents to provide context to the AI.\n- **Actor runs**: Get lists of your Actor runs, inspect their details, and retrieve logs.\n- **Apify storage**: Access data from your datasets and key-value stores.\n\n### Overview of available tools\n\nHere is an overview list of all the tools provided by the Apify MCP Server.\n\n| Tool name | Category | Description | Enabled by default |\n| :--- | :--- | :--- | :---: |\n| `search-actors` | actors | Search for Actors in the Apify Store. | ‚úÖ |\n| `fetch-actor-details` | actors | Retrieve detailed information about a specific Actor. | ‚úÖ |\n| `call-actor` | actors | Call an Actor and get its run results. | ‚úÖ |\n| [`apify-slash-rag-web-browser`](https://apify.com/apify/rag-web-browser) | Actor (see [tool configuration](#tools-configuration)) | An Actor tool to browse the web. | ‚úÖ |\n| `search-apify-docs` | docs | Search the Apify documentation for relevant pages. | ‚úÖ |\n| `fetch-apify-docs` | docs | Fetch the full content of an Apify documentation page by its URL. | ‚úÖ |\n| `get-actor-run` | runs | Get detailed information about a specific Actor run. |  |\n| `get-actor-run-list` | runs | Get a list of an Actor's runs, filterable by status. |  |\n| `get-actor-log` | runs | Retrieve the logs for a specific Actor run. |  |\n| `get-dataset` | storage | Get metadata about a specific dataset. |  |\n| `get-dataset-items` | storage | Retrieve items from a dataset with support for filtering and pagination. |  |\n| `get-dataset-schema` | storage | Generate a JSON schema from dataset items. |  |\n| `get-key-value-store` | storage | Get metadata about a specific key-value store. |  |\n| `get-key-value-store-keys`| storage | List the keys within a specific key-value store. |  |\n| `get-key-value-store-record`| storage | Get the value associated with a specific key in a key-value store. |  |\n| `get-dataset-list` | storage | List all available datasets for the user. |  |\n| `get-key-value-store-list`| storage | List all available key-value stores for the user. |  |\n| `add-actor` | experimental | Add an Actor as a new tool for the user to call. |  |\n| `get-actor-output`* | - | Retrieve the output from an Actor call which is not included in the output preview of the Actor tool. | ‚úÖ |\n\n> **Note:**\n>\n> When using the `actors` tool category, clients that support dynamic tool discovery (like Claude.ai web and VS Code) automatically receive the `add-actor` tool instead of `call-actor` for enhanced Actor discovery capabilities.\n\n> The `get-actor-output` tool is automatically included with any Actor-related tool, such as `call-actor`, `add-actor`, or any specific Actor tool like `apify-slash-rag-web-browser`. When you call an Actor - either through the `call-actor` tool or directly via an Actor tool (e.g., `apify-slash-rag-web-browser`) - you receive a preview of the output. The preview depends on the Actor's output format and length; for some Actors and runs, it may include the entire output, while for others, only a limited version is returned to avoid overwhelming the LLM. To retrieve the full output of an Actor run, use the `get-actor-output` tool (supports limit, offset, and field filtering) with the `datasetId` provided by the Actor call.\n\n### Tools configuration\n\nThe `tools` configuration parameter is used to specify loaded tools - either categories or specific tools directly, and Apify Actors. For example, `tools=storage,runs` loads two categories; `tools=add-actor` loads just one tool.\n\nWhen no query parameters are provided, the MCP server loads the following `tools` by default:\n\n- `actors`\n- `docs`\n- `apify/rag-web-browser`\n\nIf the tools parameter is specified, only the listed tools or categories will be enabled - no default tools will be included.\n\n> **Easy configuration:**\n>\n> Use the [UI configurator](https://mcp.apify.com/) to configure your server, then copy the configuration to your client.\n\n**Configuring the hosted server:**\n\nThe hosted server can be configured using query parameters in the URL. For example, to load the default tools, use:\n\n```\nhttps://mcp.apify.com?tools=actors,docs,apify/rag-web-browser\n```\n\nFor minimal configuration, if you want to use only a single Actor tool - without any discovery or generic calling tools, the server can be configured as follows:\n\n```\nhttps://mcp.apify.com?tools=apify/my-actor\n```\n\nThis setup exposes only the specified Actor (`apify/my-actor`) as a tool. No other tools will be available.\n\n**Configuring the CLI:**\n\nThe CLI can be configured using command-line flags. For example, to load the same tools as in the hosted server configuration, use:\n\n```bash\nnpx @apify/actors-mcp-server --tools actors,docs,apify/rag-web-browser\n```\n\nThe minimal configuration is similar to the hosted server configuration:\n\n```bash\nnpx @apify/actors-mcp-server --tools apify/my-actor\n```\n\nAs above, this exposes only the specified Actor (`apify/my-actor`) as a tool. No other tools will be available.\n\n> **‚ö†Ô∏è Important recommendation**\n>\n> **The default tools configuration may change in future versions.** When no `tools` parameter is specified, the server currently loads default tools, but this behavior is subject to change.\n>\n> **For production use and stable interfaces, always explicitly specify the `tools` parameter** to ensure your configuration remains consistent across updates.\n\n### Backward compatibility\n\nThe v2 configuration preserves backward compatibility with v1 usage. Notes:\n\n- `actors` param (URL) and `--actors` flag (CLI) are still supported.\n  - Internally they are merged into `tools` selectors.\n  - Examples: `?actors=apify/rag-web-browser` ‚â° `?tools=apify/rag-web-browser`; `--actors apify/rag-web-browser` ‚â° `--tools apify/rag-web-browser`.\n- `enable-adding-actors` (CLI) and `enableAddingActors` (URL) are supported but deprecated.\n  - Prefer `tools=experimental` or including the specific tool `tools=add-actor`.\n  - Behavior remains: when enabled with no `tools` specified, the server exposes only `add-actor`; when categories/tools are selected, `add-actor` is also included.\n- `enableActorAutoLoading` remains as a legacy alias for `enableAddingActors` and is mapped automatically.\n- Defaults remain compatible: when no `tools` are specified, the server loads `actors`, `docs`, and `apify/rag-web-browser`.\n  - If any `tools` are specified, the defaults are not added (same as v1 intent for explicit selection).\n- `call-actor` is now included by default via the `actors` category (additive change). To exclude it, specify an explicit `tools` list without `actors`.\n- `preview` category is deprecated and removed. Use specific tool names instead.\n\nExisting URLs and commands using `?actors=...` or `--actors` continue to work unchanged.\n\n### Prompts\n\nThe server provides a set of predefined example prompts to help you get started interacting with Apify through MCP. For example, there is a `GetLatestNewsOnTopic` prompt that allows you to easily retrieve the latest news on a specific topic using the [RAG Web Browser](https://apify.com/apify/rag-web-browser) Actor.\n\n### Resources\n\nThe server does not yet provide any resources.\n\n### Debugging the NPM package\n\nTo debug the server, use the [MCP Inspector](https://github.com/modelcontextprotocol/inspector) tool:\n\n```shell\nexport APIFY_TOKEN=\"your-apify-token\"\nnpx @modelcontextprotocol/inspector npx -y @apify/actors-mcp-server\n```\n\n# ‚öôÔ∏è Development\n\n## Prerequisites\n\n- [Node.js](https://nodejs.org/en) (v18 or higher)\n\nCreate an environment file, `.env`, with the following content:\n```text\nAPIFY_TOKEN=\"your-apify-token\"\n```\n\nBuild the `actor-mcp-server` package:\n\n```bash\nnpm run build\n```\n\n## Start HTTP streamable MCP server\n\nRun using Apify CLI:\n\n```bash\nexport APIFY_TOKEN=\"your-apify-token\"\nexport APIFY_META_ORIGIN=STANDBY\napify run -p\n```\n\nOnce the server is running, you can use the [MCP Inspector](https://github.com/modelcontextprotocol/inspector) to debug the server exposed at `http://localhost:3001`.\n\n## Start standard input/output (stdio) MCP server\n\nYou can launch the MCP Inspector with this command:\n\n```bash\nexport APIFY_TOKEN=\"your-apify-token\"\nnpx @modelcontextprotocol/inspector node ./dist/stdio.js\n```\n\nUpon launching, the Inspector will display a URL that you can open in your browser to begin debugging.\n\n## üê¶ Canary PR releases\n\nApify MCP is split across two repositories: this one for core MCP logic and the private `apify-mcp-server-internal` for the hosted server.\nChanges must be synchronized between both.\n\nTo create a canary release, add the `beta` tag to your PR branch.\nThis publishes the package to [pkg.pr.new](https://pkg.pr.new/) for staging and testing before merging.\nSee [the workflow file](.github/workflows/pre_release.yaml) for details.\n\n## üêã Docker Hub integration\nThe Apify MCP Server is also available on [Docker Hub](https://hub.docker.com/mcp/server/apify-mcp-server/overview), registered via the [mcp-registry](https://github.com/docker/mcp-registry) repository. The entry in `servers/apify-mcp-server/server.yaml` should be deployed automatically by the Docker Hub MCP registry (deployment frequency is unknown). **Before making major changes to the `stdio` server version, be sure to test it locally to ensure the Docker build passes.** To test, change the `source.branch` to your PR branch and run `task build -- apify-mcp-server`. For more details, see [CONTRIBUTING.md](https://github.com/docker/mcp-registry/blob/main/CONTRIBUTING.md).\n\n# üêõ Troubleshooting (local MCP server)\n\n- Make sure you have `node` installed by running `node -v`.\n- Make sure the `APIFY_TOKEN` environment variable is set.\n- Always use the latest version of the MCP server by using `@apify/actors-mcp-server@latest`.\n\n## üí° Limitations\n\nThe Actor input schema is processed to be compatible with most MCP clients while adhering to [JSON Schema](https://json-schema.org/) standards. The processing includes:\n- **Descriptions** are truncated to 500 characters (as defined in `MAX_DESCRIPTION_LENGTH`).\n- **Enum fields** are truncated to a maximum combined length of 200 characters for all elements (as defined in `ACTOR_ENUM_MAX_LENGTH`).\n- **Required fields** are explicitly marked with a `REQUIRED` prefix in their descriptions for compatibility with frameworks that may not handle the JSON schema properly.\n- **Nested properties** are built for special cases like proxy configuration and request list sources to ensure the correct input structure.\n- **Array item types** are inferred when not explicitly defined in the schema, using a priority order: explicit type in items > prefill type > default value type > editor type.\n- **Enum values and examples** are added to property descriptions to ensure visibility, even if the client doesn't fully support the JSON schema.\n- **Rental Actors** are only available for use with the hosted MCP server at https://mcp.apify.com. When running the server locally via stdio, you can only access Actors that are already added to your local toolset. To dynamically search for and use any Actor from the Apify Store‚Äîincluding rental Actors‚Äîconnect to the hosted endpoint.\n\n# ü§ù Contributing\n\nWe welcome contributions to improve the Apify MCP Server! Here's how you can help:\n\n- **üêõ Report issues**: Find a bug or have a feature request? [Open an issue](https://github.com/apify/apify-mcp-server/issues).\n- **üîß Submit pull requests**: Fork the repo and submit pull requests with enhancements or fixes.\n- **üìö Documentation**: Improvements to docs and examples are always welcome.\n- **üí° Share use cases**: Contribute examples to help other users.\n\nFor major changes, please open an issue first to discuss your proposal and ensure it aligns with the project's goals.\n\n# üìö Learn more\n\n- [Model Context Protocol](https://modelcontextprotocol.org/)\n- [What are AI Agents?](https://blog.apify.com/what-are-ai-agents/)\n- [What is MCP and why does it matter?](https://blog.apify.com/what-is-model-context-protocol/)\n- [How to use MCP with Apify Actors](https://blog.apify.com/how-to-use-mcp/)\n- [Tester MCP Client](https://apify.com/jiri.spilka/tester-mcp-client)\n- [Webinar: Building and Monetizing MCP Servers on Apify](https://www.youtube.com/watch?v=w3AH3jIrXXo)\n- [MCP Client development guide](https://github.com/cyanheads/model-context-protocol-resources/blob/main/guides/mcp-client-development-guide.md)\n- [How to build and monetize an AI agent on Apify](https://blog.apify.com/how-to-build-an-ai-agent/)\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "apify",
        "mcp",
        "server",
        "servers apify",
        "apify mcp",
        "mcp server"
      ],
      "category": "official-servers"
    },
    "apimatic--apimatic-validator-mcp": {
      "owner": "apimatic",
      "name": "apimatic-validator-mcp",
      "url": "https://github.com/apimatic/apimatic-validator-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/apimatic.webp",
      "description": "Validates OpenAPI specifications using APIMatic's API to ensure compliance and identify issues early. Supports both JSON and YAML formats and works with OpenAPI versions 2.0 and 3.0.",
      "stars": 5,
      "forks": 13,
      "license": "No License",
      "language": "TypeScript",
      "updated_at": "2025-08-17T01:32:37Z",
      "readme_content": "# APIMatic Validator MCP Server\n\nThis repository provides a Model Context Protocol (MCP) Server for validating OpenAPI specifications using [APIMatic](https://www.apimatic.io/). The server processes OpenAPI files and returns validation summaries by leveraging APIMatic‚Äôs API.\n\n## Features\n\n- Validates OpenAPI 2.0 and 3.0 files\n- Uses APIMatic‚Äôs API for comprehensive validation\n- Supports both JSON and YAML formats\n- Implements Model Context Protocol (MCP) for seamless integration\n\n## Installation\n\nEnsure that **Node.js v18+** is installed.\n\n### Clone the Repository\n```sh\ngit clone https://github.com/apimatic/apimatic-validator-mcp.git\ncd apimatic-validator-mcp\n```\n\n### Install Dependencies\n```sh\nnpm install\n```\n\n### Build the Project\n```sh\nnpm run build\n```\n\n## Configuration\n\nTo use the server, an APIMatic API key is required. Sign up at [APIMatic](https://www.apimatic.io/) and obtain the API key.\n\n\n![image](https://github.com/user-attachments/assets/1e2388dd-1330-4dab-a6e0-c6738a494ab9)\n\n\n### Integration with Claude Desktop\n\nModify the `claude_desktop_config.json` file to integrate the MCP server. If the file does not exist, create one in the following location:\n\n#### Windows\n```sh\ncode $env:AppData\\Claude\\claude_desktop_config.json\n```\n\n#### macOS/Linux\n```sh\ncode ~/Library/Application\\ Support/Claude/claude_desktop_config.json\n```\n\n### Add the MCP Server to the Configuration\n```json\n{\n    \"mcpServers\": {\n        \"APIMatic\": {\n            \"command\": \"node\",\n            \"args\": [\n                \"C:\\\\PATH\\\\TO\\\\PARENT\\\\FOLDER\\\\build\\\\index.js\"\n            ],\n            \"env\": {\n                \"APIMATIC_API_KEY\": \"<Add your APIMatic token here>\"\n            }\n        }\n    }\n}\n```\n\nOnce configured, a hammer icon should appear in Claude Desktop. Open it to verify that the `validate-openapi-using-apimatic` tool is successfully integrated.\n\n## Usage\n\n1. Add an OpenAPI file.\n2. Provide a prompt to validate it.\n3. The validation results will be returned.\n\n[APIMatic MCP Server For OpenAPI Validation.webm](https://github.com/user-attachments/assets/b7d14e20-1c82-4a70-b237-7e5b6bd80993)\n\n\n",
      "npm_url": "https://www.npmjs.com/package/apimatic-validator-mcp",
      "npm_downloads": 197,
      "keywords": [
        "openapi",
        "validator",
        "validates",
        "validates openapi",
        "openapi specifications",
        "apimatic validator"
      ],
      "category": "official-servers"
    },
    "axiomhq--mcp-server-axiom": {
      "owner": "axiomhq",
      "name": "mcp-server-axiom",
      "url": "https://github.com/axiomhq/mcp-server-axiom",
      "imageUrl": "/freedevtools/mcp/pfp/axiomhq.webp",
      "description": "Queries Axiom datasets using Axiom Processing Language (APL) and lists available datasets.",
      "stars": 58,
      "forks": 9,
      "license": "MIT License",
      "language": "Go",
      "updated_at": "2025-09-24T10:23:15Z",
      "readme_content": "\n# [DEPRECATED] mcp-server-axiom\n\n---\n\n### ‚ö†Ô∏è DEPRECATION NOTICE \n\n**This repository is deprecated and no longer maintained.**  \nPlease use the official Axiom MCP Server at [https://mcp.axiom.co](https://mcp.axiom.co) instead.\n\n---\n\n## Overview\nA [Model Context Protocol](https://modelcontextprotocol.io/) server implementation for [Axiom](https://axiom.co) that enables AI agents to query your data using Axiom Processing Language (APL).\n\n## Status\n\nWorks with Claude desktop app. Implements six MCP [tools](https://modelcontextprotocol.io/docs/concepts/tools):\n\n- queryApl: Execute APL queries against Axiom datasets\n- listDatasets: List available Axiom datasets\n- getDatasetSchema: Get dataset schema\n- getSavedQueries: Retrieve saved/starred APL queries\n- getMonitors: List monitoring configurations\n- getMonitorsHistory: Get monitor execution history\n\n**Note:** All tools require an API token for authentication. Use your API token as the `token` parameter.\n\nNo support for MCP [resources](https://modelcontextprotocol.io/docs/concepts/resources) or [prompts](https://modelcontextprotocol.io/docs/concepts/prompts) yet.\n\n## Installation\n\n### Releases\n\nDownload the latest built binary from the [releases page](https://github.com/axiomhq/axiom-mcp/releases).\n\n### Source\n\n```bash\ngo install github.com/axiomhq/axiom-mcp@latest\n```\n\n## Configuration\n\nConfigure using one of these methods:\n\n### Config File Example (config.txt):\n```txt\ntoken xaat-your-api-token\nurl https://api.axiom.co\nquery-rate 1\nquery-burst 1\ndatasets-rate 1\ndatasets-burst 1\nmonitors-rate 1\nmonitors-burst 1\n```\n\n### Command Line Flags:\n```bash\naxiom-mcp \\\n  -token xaat-your-api-token \\\n  -url https://api.axiom.co \\\n  -query-rate 1 \\\n  -query-burst 1 \\\n  -datasets-rate 1 \\\n  -datasets-burst 1 \\\n  -monitors-rate 1 \\\n  -monitors-burst 1\n```\n\n### Environment Variables:\n```bash\nexport AXIOM_TOKEN=xaat-your-api-token\nexport AXIOM_URL=https://api.axiom.co\nexport AXIOM_QUERY_RATE=1\nexport AXIOM_QUERY_BURST=1\nexport AXIOM_DATASETS_RATE=1\nexport AXIOM_DATASETS_BURST=1\nexport AXIOM_MONITORS_RATE=1\nexport AXIOM_MONITORS_BURST=1\n```\n\n## Usage\n\n1. Create a config file:\n```bash\necho \"token xaat-your-api-token\" > config.txt\n```\n\n2. Configure the Claude app to use the MCP server:\n\n```bash\ncode ~/Library/Application\\ Support/Claude/claude_desktop_config.json\n```\n\n```json\n{\n  \"mcpServers\": {\n    \"axiom\": {\n      \"command\": \"/path/to/your/axiom-mcp-binary\",\n      \"args\" : [\"--config\", \"/path/to/your/config.txt\"],\n      \"env\": { // Alternatively, you can set the environment variables here\n        \"AXIOM_TOKEN\": \"xaat-your-api-token\",\n        \"AXIOM_URL\": \"https://api.axiom.co\"\n      }\n    }\n  }\n}\n```\n\n## License\n\nMIT License - see LICENSE file\n",
      "npm_url": "https://www.npmjs.com/package/mcp-server-axiom",
      "npm_downloads": 5052,
      "keywords": [
        "axiomhq",
        "axiom",
        "datasets",
        "servers axiomhq",
        "axiom datasets",
        "server axiom"
      ],
      "category": "official-servers"
    },
    "base--base-mcp": {
      "owner": "base",
      "name": "base-mcp",
      "url": "https://github.com/base/base-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/base.webp",
      "description": "Enables interaction with the Base blockchain and Coinbase API for AI applications. Supports wallet management, fund transfers, smart contract deployments, and onchain lending functionality.",
      "stars": 294,
      "forks": 98,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-09-19T23:15:17Z",
      "readme_content": "# Base MCP Server üîµ\n\n\n\n[![npm version](https://img.shields.io/npm/v/base-mcp.svg)](https://www.npmjs.com/package/base-mcp)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n\nA Model Context Protocol (MCP) server that provides onchain tools for AI applications like Claude Desktop and Cursor, allowing them to interact with the Base Network and Coinbase API.\n\n## Overview\n\nThis MCP server extends any MCP client's capabilities by providing tools to do anything on Base:\n\n- Retrieve wallet addresses\n- List wallet balances\n- Transfer funds between wallets\n- Deploy smart contracts\n- Interact with Morpho vaults for onchain lending\n- Call contract functions\n- Onramp funds via [Coinbase](https://www.coinbase.com/developer-platform/products/onramp)\n- Manage ERC20 tokens\n- List and transfer NFTs (ERC721 and ERC1155)\n- Buy [OpenRouter](http://openrouter.ai/) credits with USDC\n- Resolve Farcaster usernames to Ethereum addresses\n\nThe server interacts with Base, powered by Base Developer Tools and [AgentKit](https://github.com/coinbase/agentkit).\n\n## Extending Base MCP with 3P Protocols, Tools, and Data Sources\n\nBase MCP is designed to be extensible, allowing you to add your own third-party protocols, tools, and data sources. This section provides an overview of how to extend the Base MCP server with new capabilities.\n\n### Adding New Tools\n\nIf you want to add a new tool to the Base MCP server, follow these steps:\n\n1. Create a new directory in the `src/tools` directory for your tool\n2. Implement the tool following the existing patterns:\n   - `index.ts`: Define and export your tools. Tools are defined as AgentKit ActionProviders.\n   - `schemas.ts`: Define input schemas for your tools\n   - `types.ts`: Define types required for your tools\n   - `utils.ts`: Utilities for your tools\n3. Add your tool to the list of available tools in `src/main.ts`\n4. Add documentation for your tool in the README.md\n5. Add examples of how to use your tool in examples.md\n6. Write tests for your tool\n\n### Project Structure\n\nThe Base MCP server follows this structure for tools:\n\n```\nsrc/\n‚îú‚îÄ‚îÄ tools/\n‚îÇ   ‚îú‚îÄ‚îÄ [TOOL_NAME]/ <-------------------------- ADD DIR HERE\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ index.ts (defines and exports tools)\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ schemas.ts (defines input schema)\n‚îÇ   ‚îî‚îÄ‚îÄ utils/ (shared tool utilities)\n```\n\n### Best Practices for Tool Development\n\nWhen developing new tools for Base MCP:\n\n- Follow the existing code style and patterns\n- Ensure your tool has a clear, focused purpose\n- Provide comprehensive input validation\n- Include detailed error handling\n- Write thorough documentation\n- Add examples demonstrating how to use your tool\n- Include tests for your tool\n\nFor more detailed information on contributing to Base MCP, including adding new tools and protocols, see the [CONTRIBUTING.md](CONTRIBUTING.md) file.\n\n## Prerequisites\n\n- Node.js (v16 or higher)\n- npm or yarn\n- Coinbase API credentials (API Key Name and Private Key)\n- A wallet seed phrase\n- Coinbase Project ID (for onramp functionality)\n- Alchemy API Key (required for NFT functionality)\n- Optional: OpenRouter API Key (for buying OpenRouter credits)\n\n## Installation\n\n### Option 1: Install from npm (Recommended)\n\n```bash\n# Install globally\nnpm install -g base-mcp\n\n# Or install locally in your project\nnpm install base-mcp\n```\n\nOnce the package is installed, you can configure clients with the following command:\n\n```bash\nbase-mcp --init\n```\n\n### Option 2: Install from Source\n\n1. Clone this repository:\n\n   ```bash\n   git clone https://github.com/base/base-mcp.git\n   cd base-mcp\n   ```\n\n2. Install dependencies:\n\n   ```bash\n   npm install\n   ```\n\n3. Build the project:\n\n   ```bash\n   npm run build\n   ```\n\n4. Optionally, link it globally:\n   ```bash\n   npm link\n   ```\n\n## Configuration\n\nCreate a `.env` file with your credentials:\n\n```\n# Coinbase API credentials\n# You can obtain these from the Coinbase Developer Portal: https://cdp.coinbase.com/\nCOINBASE_API_KEY_NAME=your_api_key_name\nCOINBASE_API_PRIVATE_KEY=your_private_key\n\n# Wallet seed phrase (12 or 24 words)\n# This is the mnemonic phrase for your wallet\nSEED_PHRASE=your seed phrase here\n\n# Coinbase Project ID (for onramp functionality)\n# You can obtain this from the Coinbase Developer Portal\nCOINBASE_PROJECT_ID=your_project_id\n\n# Alchemy API Key (required for NFT functionality)\n# You can obtain this from https://alchemy.com\nALCHEMY_API_KEY=your_alchemy_api_key\n\n# OpenRouter API Key (optional for buying OpenRouter credits)\n# You can obtain this from https://openrouter.ai/keys\nOPENROUTER_API_KEY=your_openrouter_api_key\n\n# Chain ID (optional for Base Sepolia testnet)\n# Use 84532 for Base Sepolia testnet\n# You do not have to include this if you want to use Base Mainnet\nCHAIN_ID=your_chain_id\n\n# Neynar API Key (required for Farcaster functionality)\n# You can obtain this from https://neynar.com\nNEYNAR_API_KEY=your_neynar_api_key\n```\n\n## Testing\n\nTest the MCP server to verify it's working correctly:\n\n```bash\nnpm test\n```\n\nThis script will verify that your MCP server is working correctly by testing the connection and available tools.\n\n## Examples\n\nSee the [examples.md](examples.md) file for detailed examples of how to interact with the Base MCP tools through Claude.\n\n## Integration with Claude Desktop\n\nTo add this MCP server to Claude Desktop:\n\n1. Create or edit the Claude Desktop configuration file at:\n\n   - macOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\n   - Windows: `%APPDATA%\\Claude\\claude_desktop_config.json`\n   - Linux: `~/.config/Claude/claude_desktop_config.json`\n\nYou can easily access this file via the Claude Desktop app by navigating to Claude > Settings > Developer > Edit Config.\n\n2. Add the following configuration:\n\n   ```json\n   {\n     \"mcpServers\": {\n       \"base-mcp\": {\n         \"command\": \"npx\",\n         \"args\": [\"-y\", \"base-mcp@latest\"],\n         \"env\": {\n           \"COINBASE_API_KEY_NAME\": \"your_api_key_name\",\n           \"COINBASE_API_PRIVATE_KEY\": \"your_private_key\",\n           \"SEED_PHRASE\": \"your seed phrase here\",\n           \"COINBASE_PROJECT_ID\": \"your_project_id\",\n           \"ALCHEMY_API_KEY\": \"your_alchemy_api_key\",\n           \"PINATA_JWT\": \"your_pinata_jwt\",\n           \"OPENROUTER_API_KEY\": \"your_openrouter_api_key\",\n           \"CHAIN_ID\": \"optional_for_base_sepolia_testnet\"\n         },\n         \"disabled\": false,\n         \"autoApprove\": []\n       }\n     }\n   }\n   ```\n\n3. Restart Claude Desktop for the changes to take effect.\n\n## Available Tools\n\n### get-address\n\nRetrieves the address for your wallet.\n\nExample query to Claude:\n\n> \"What's my wallet address?\"\n\n### list-balances\n\nLists all balances for your wallet.\n\nExample query to Claude:\n\n> \"Show me my wallet balances.\"\n\n### transfer-funds\n\nTransfers funds from your wallet to another address.\n\nParameters:\n\n- `destination`: The address to which to transfer funds\n- `assetId`: The asset ID to transfer\n- `amount`: The amount of funds to transfer\n\nExample query to Claude:\n\n> \"Transfer 0.01 ETH to 0x1234567890abcdef1234567890abcdef12345678.\"\n\n### deploy-contract\n\nDeploys a smart contract to the blockchain.\n\nParameters:\n\n- `constructorArgs`: The arguments for the contract constructor\n- `contractName`: The name of the contract to deploy\n- `solidityInputJson`: The JSON input for the Solidity compiler containing contract source and settings\n- `solidityVersion`: The version of the solidity compiler\n\nExample query to Claude:\n\n> \"Deploy a simple ERC20 token contract for me.\"\n\n### check-address-reputation\n\nChecks the reputation of an address.\n\nParameters:\n\n- `address`: The Ethereum address to check\n\nExample query to Claude:\n\n> \"What's the reputation of 0x1234567890abcdef1234567890abcdef12345678?\"\n\n### get_morpho_vaults\n\nGets the vaults for a given asset on Morpho.\n\nParameters:\n\n- `assetSymbol`: Asset symbol by which to filter vaults (optional)\n\nExample query to Claude:\n\n> \"Show me the available Morpho vaults for USDC.\"\n\n### call_contract\n\nCalls a contract function on the blockchain.\n\nParameters:\n\n- `contractAddress`: The address of the contract to call\n- `functionName`: The name of the function to call\n- `functionArgs`: The arguments to pass to the function\n- `abi`: The ABI of the contract\n- `value`: The value of ETH to send with the transaction (optional)\n\nExample query to Claude:\n\n> \"Call the balanceOf function on the contract at 0x1234567890abcdef1234567890abcdef12345678.\"\n\n### get_onramp_assets\n\nGets the assets available for onramping in a given country/subdivision.\n\nParameters:\n\n- `country`: ISO 3166-1 two-digit country code string representing the purchasing user's country of residence\n- `subdivision`: ISO 3166-2 two-digit country subdivision code (required for US)\n\nExample query to Claude:\n\n> \"What assets can I onramp in the US, specifically in New York?\"\n\n### onramp\n\nGets a URL for onramping funds via Coinbase.\n\nParameters:\n\n- `amountUsd`: The amount of funds to onramp\n- `assetId`: The asset ID to onramp\n\nExample query to Claude:\n\n> \"I want to onramp $100 worth of ETH.\"\n\n### erc20_balance\n\nGets the balance of an ERC20 token.\n\nParameters:\n\n- `contractAddress`: The address of the ERC20 contract\n\nExample query to Claude:\n\n> \"What's my balance of the token at 0x1234567890abcdef1234567890abcdef12345678?\"\n\n### erc20_transfer\n\nTransfers an ERC20 token to another address.\n\nParameters:\n\n- `contractAddress`: The address of the ERC20 contract\n- `toAddress`: The address of the recipient\n- `amount`: The amount of tokens to transfer\n\nExample query to Claude:\n\n> \"Transfer 10 USDC to 0x1234567890abcdef1234567890abcdef12345678.\"\n\n### list_nfts\n\nLists NFTs owned by a specific address.\n\nParameters:\n\n- `ownerAddress`: The address of the owner whose NFTs to list\n- `limit`: Maximum number of NFTs to return (default: 50)\n\nExample query to Claude:\n\n> \"Show me the NFTs owned by 0x89A93a48C6Ef8085B9d07e46AaA96DFDeC717040.\"\n\n### transfer_nft\n\nTransfers an NFT to another address. Supports both ERC721 and ERC1155 standards.\n\nParameters:\n\n- `contractAddress`: The address of the NFT contract\n- `tokenId`: The token ID of the NFT to transfer\n- `toAddress`: The address of the recipient\n- `amount`: The amount to transfer (only used for ERC1155, default: 1)\n\nExample query to Claude:\n\n> \"Transfer my NFT with contract 0x3F06FcF75f45F1bb61D56D68fA7b3F32763AA15c and token ID 56090175025510453004781233574040052668718235229192064098345825090519343038548 to 0x1234567890abcdef1234567890abcdef12345678.\"\n\n### buy_openrouter_credits\n\nBuys OpenRouter credits with USDC.\n\nParameters:\n\n- `amountUsd`: The amount of credits to buy, in USD\n\nExample query to Claude:\n\n> \"Buy $20 worth of OpenRouter credits.\"\n\n## Security Considerations\n\n- The configuration file contains sensitive information (API keys and seed phrases). Ensure it's properly secured and not shared.\n- Consider using environment variables or a secure credential manager instead of hardcoding sensitive information.\n- Be cautious when transferring funds or deploying contracts, as these operations are irreversible on the blockchain.\n- When using the onramp functionality, ensure you're on a secure connection.\n- Verify all transaction details before confirming, especially when transferring funds or buying credits.\n\n## Troubleshooting\n\nIf you encounter issues:\n\n1. Check that your Coinbase API credentials are correct\n2. Verify that your seed phrase is valid\n3. Ensure you're on the correct network (Base Mainnet)\n4. Check the Claude Desktop logs for any error messages\n\n## License\n\n[MIT License](LICENSE)\n\n## Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n\nFor detailed guidelines on contributing to Base MCP, including:\n\n- Reporting bugs\n- Suggesting enhancements\n- Development setup\n- Coding standards\n- **Adding new tools, protocols, and data sources** (see also the [Extending Base MCP](#extending-base-mcp-with-3p-protocols-tools-and-data-sources) section above)\n- Testing requirements\n- Documentation standards\n\nPlease refer to our comprehensive [CONTRIBUTING.md](CONTRIBUTING.md) guide.\n\nBasic contribution steps:\n\n1. Fork the repository\n2. Create your feature branch (`git checkout -b feature/amazing-feature`)\n3. Commit your changes (`git commit -m 'Add some amazing feature'`)\n4. Push to the branch (`git push origin feature/amazing-feature`)\n5. Open a Pull Request\n\nPlease make sure your code follows the existing style and includes appropriate tests.",
      "npm_url": "https://www.npmjs.com/package/base-mcp",
      "npm_downloads": 3264,
      "keywords": [
        "coinbase",
        "mcp",
        "base",
        "base mcp",
        "base blockchain",
        "blockchain coinbase"
      ],
      "category": "official-servers"
    },
    "browserbase--mcp-server-browserbase": {
      "owner": "browserbase",
      "name": "mcp-server-browserbase",
      "url": "https://github.com/browserbase/mcp-server-browserbase",
      "imageUrl": "/freedevtools/mcp/pfp/browserbase.webp",
      "description": "Provides cloud browser automation capabilities, enabling interaction with web pages, taking screenshots, and executing JavaScript in a cloud browser environment.",
      "stars": 2679,
      "forks": 290,
      "license": "Apache License 2.0",
      "language": "TypeScript",
      "updated_at": "2025-10-02T00:40:38Z",
      "readme_content": "# Browserbase MCP Server\n\n[![smithery badge](https://smithery.ai/badge/@browserbasehq/mcp-browserbase)](https://smithery.ai/server/@browserbasehq/mcp-browserbase)\n\n\n\n[The Model Context Protocol (MCP)](https://modelcontextprotocol.io/introduction) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you're building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.\n\nThis server provides cloud browser automation capabilities using [Browserbase](https://www.browserbase.com/) and [Stagehand](https://github.com/browserbase/stagehand). It enables LLMs to interact with web pages, take screenshots, extract information, and perform automated actions with atomic precision.\n\n## Features\n\n| Feature            | Description                                                 |\n| ------------------ | ----------------------------------------------------------- |\n| Browser Automation | Control and orchestrate cloud browsers via Browserbase      |\n| Data Extraction    | Extract structured data from any webpage                    |\n| Web Interaction    | Navigate, click, and fill forms with ease                   |\n| Screenshots        | Capture full-page and element screenshots                   |\n| Model Flexibility  | Supports multiple models (OpenAI, Claude, Gemini, and more) |\n| Vision Support     | Use annotated screenshots for complex DOMs                  |\n| Session Management | Create, manage, and close browser sessions                  |\n| Multi-Session      | Run multiple browser sessions in parallel                   |\n\n## How to Setup\n\n### Quickstarts:\n\n#### Add to Cursor\n\nCopy and Paste this link in your Browser:\n\n```text\ncursor://anysphere.cursor-deeplink/mcp/install?name=browserbase&config=eyJjb21tYW5kIjoibnB4IEBicm93c2VyYmFzZWhxL21jcCIsImVudiI6eyJCUk9XU0VSQkFTRV9BUElfS0VZIjoiIiwiQlJPV1NFUkJBU0VfUFJPSkVDVF9JRCI6IiIsIkdFTUlOSV9BUElfS0VZIjoiIn19\n```\n\nWe currently support 2 transports for our MCP server, STDIO and SHTTP. We recommend you use SHTTP with our remote hosted url to take advantage of the server at full capacity.\n\n## SHTTP:\n\nTo use the Browserbase MCP Server through our remote hosted URL, add the following to your configuration.\n\nGo to [smithery.ai](https://smithery.ai/server/@browserbasehq/mcp-browserbase) and enter your API keys and configuration to get a remote hosted URL.\nWhen using our remote hosted server, we provide the LLM costs for Gemini, the [best performing model](https://www.stagehand.dev/evals) in [Stagehand](https://www.stagehand.dev).\n\n\n\nIf your client supports SHTTP:\n\n```json\n{\n  \"mcpServers\": {\n    \"browserbase\": {\n      \"url\": \"your-smithery-url.com\"\n    }\n  }\n}\n```\n\nIf your client doesn't support SHTTP:\n\n```json\n{\n  \"mcpServers\": {\n    \"browserbase\": {\n      \"command\": \"npx\",\n      \"args\": [\"mcp-remote\", \"your-smithery-url.com\"]\n    }\n  }\n}\n```\n\n## STDIO:\n\nYou can either use our Server hosted on NPM or run it completely locally by cloning this repo.\n\n> **‚ùóÔ∏è Important:** If you want to use a different model you have to add --modelName to the args and provide that respective key as an arg. More info below.\n\n### To run on NPM (Recommended)\n\nGo into your MCP Config JSON and add the Browserbase Server:\n\n```json\n{\n  \"mcpServers\": {\n    \"browserbase\": {\n      \"command\": \"npx\",\n      \"args\": [\"@browserbasehq/mcp-server-browserbase\"],\n      \"env\": {\n        \"BROWSERBASE_API_KEY\": \"\",\n        \"BROWSERBASE_PROJECT_ID\": \"\",\n        \"GEMINI_API_KEY\": \"\"\n      }\n    }\n  }\n}\n```\n\nThat's it! Reload your MCP client and Claude will be able to use Browserbase.\n\n### To run 100% local:\n\n```bash\n# Clone the Repo\ngit clone https://github.com/browserbase/mcp-server-browserbase.git\ncd mcp-server-browserbase\n\n# Install the dependencies and build the project\nnpm install && npm run build\n```\n\nThen in your MCP Config JSON run the server. To run locally we can use STDIO or self-host SHTTP.\n\n### STDIO:\n\nTo your MCP Config JSON file add the following:\n\n```json\n{\n  \"mcpServers\": {\n    \"browserbase\": {\n      \"command\": \"node\",\n      \"args\": [\"/path/to/mcp-server-browserbase/cli.js\"],\n      \"env\": {\n        \"BROWSERBASE_API_KEY\": \"\",\n        \"BROWSERBASE_PROJECT_ID\": \"\",\n        \"GEMINI_API_KEY\": \"\"\n      }\n    }\n  }\n}\n```\n\nThen reload your MCP client and you should be good to go!\n\n## Configuration\n\nThe Browserbase MCP server accepts the following command-line flags:\n\n| Flag                       | Description                                                                 |\n| -------------------------- | --------------------------------------------------------------------------- |\n| `--proxies`                | Enable Browserbase proxies for the session                                  |\n| `--advancedStealth`        | Enable Browserbase Advanced Stealth (Only for Scale Plan Users)             |\n| `--keepAlive`              | Enable Browserbase Keep Alive Session                                       |\n| `--contextId <contextId>`  | Specify a Browserbase Context ID to use                                     |\n| `--persist`                | Whether to persist the Browserbase context (default: true)                  |\n| `--port <port>`            | Port to listen on for HTTP/SHTTP transport                                  |\n| `--host <host>`            | Host to bind server to (default: localhost, use 0.0.0.0 for all interfaces) |\n| `--cookies [json]`         | JSON array of cookies to inject into the browser                            |\n| `--browserWidth <width>`   | Browser viewport width (default: 1024)                                      |\n| `--browserHeight <height>` | Browser viewport height (default: 768)                                      |\n| `--modelName <model>`      | The model to use for Stagehand (default: google/gemini-2.0-flash)           |\n| `--modelApiKey <key>`      | API key for the custom model provider (required when using custom models)   |\n| `--experimental`           | Enable experimental features (default: false)                               |\n\nThese flags can be passed directly to the CLI or configured in your MCP configuration file.\n\n### NOTE:\n\nCurrently, these flags can only be used with the local server (npx @browserbasehq/mcp-server-browserbase).\n\n## Configuration Examples\n\n### Proxies\n\nHere are our docs on [Proxies](https://docs.browserbase.com/features/proxies).\n\nTo use proxies, set the --proxies flag in your MCP Config:\n\n```json\n{\n  \"mcpServers\": {\n    \"browserbase\": {\n      \"command\": \"npx\",\n      \"args\": [\"@browserbasehq/mcp-server-browserbase\", \"--proxies\"],\n      \"env\": {\n        \"BROWSERBASE_API_KEY\": \"\",\n        \"BROWSERBASE_PROJECT_ID\": \"\",\n        \"GEMINI_API_KEY\": \"\"\n      }\n    }\n  }\n}\n```\n\n### Advanced Stealth\n\nHere are our docs on [Advanced Stealth](https://docs.browserbase.com/features/stealth-mode#advanced-stealth-mode).\n\nTo use advanced stealth, set the --advancedStealth flag in your MCP Config:\n\n```json\n{\n  \"mcpServers\": {\n    \"browserbase\": {\n      \"command\": \"npx\",\n      \"args\": [\"@browserbasehq/mcp-server-browserbase\", \"--advancedStealth\"],\n      \"env\": {\n        \"BROWSERBASE_API_KEY\": \"\",\n        \"BROWSERBASE_PROJECT_ID\": \"\",\n        \"GEMINI_API_KEY\": \"\"\n      }\n    }\n  }\n}\n```\n\n### Contexts\n\nHere are our docs on [Contexts](https://docs.browserbase.com/features/contexts)\n\nTo use contexts, set the --contextId flag in your MCP Config:\n\n```json\n{\n  \"mcpServers\": {\n    \"browserbase\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"@browserbasehq/mcp-server-browserbase\",\n        \"--contextId\",\n        \"<YOUR_CONTEXT_ID>\"\n      ],\n      \"env\": {\n        \"BROWSERBASE_API_KEY\": \"\",\n        \"BROWSERBASE_PROJECT_ID\": \"\",\n        \"GEMINI_API_KEY\": \"\"\n      }\n    }\n  }\n}\n```\n\n### Browser Viewport Sizing\n\nThe default viewport sizing for a browser session is 1024 x 768. You can adjust the Browser viewport sizing with browserWidth and browserHeight flags.\n\nHere's how to use it for custom browser sizing. We recommend to stick with 16:9 aspect ratios (ie: 1920 x 1080, 1280 x 720, 1024 x 768)\n\n```json\n{\n  \"mcpServers\": {\n    \"browserbase\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"@browserbasehq/mcp-server-browserbase\",\n        \"--browserHeight 1080\",\n        \"--browserWidth 1920\"\n      ],\n      \"env\": {\n        \"BROWSERBASE_API_KEY\": \"\",\n        \"BROWSERBASE_PROJECT_ID\": \"\",\n        \"GEMINI_API_KEY\": \"\"\n      }\n    }\n  }\n}\n```\n\n### Model Configuration\n\nStagehand defaults to using Google's Gemini 2.0 Flash model, but you can configure it to use other models like GPT-4o, Claude, or other providers.\n\n**Important**: When using any custom model (non-default), you must provide your own API key for that model provider using the `--modelApiKey` flag.\n\nHere's how to configure different models:\n\n```json\n{\n  \"mcpServers\": {\n    \"browserbase\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"@browserbasehq/mcp-server-browserbase\",\n        \"--modelName\",\n        \"anthropic/claude-3-5-sonnet-latest\",\n        \"--modelApiKey\",\n        \"your-anthropic-api-key\"\n      ],\n      \"env\": {\n        \"BROWSERBASE_API_KEY\": \"\",\n        \"BROWSERBASE_PROJECT_ID\": \"\"\n      }\n    }\n  }\n}\n```\n\n_Note: The model must be supported in Stagehand. Check out the docs [here](https://docs.stagehand.dev/examples/custom_llms#supported-llms). When using any custom model, you must provide your own API key for that provider._\n\n### Resources\n\nThe server provides access to screenshot resources:\n\n1. **Screenshots** (`screenshot://<screenshot-name>`)\n   - PNG images of captured screenshots\n\n## Key Features\n\n- **AI-Powered Automation**: Natural language commands for web interactions\n- **Multi-Model Support**: Works with OpenAI, Claude, Gemini, and more\n- **Advanced Session Management**: Single and multi-session support for parallel browser automation\n- **Screenshot Capture**: Full-page and element-specific screenshots\n- **Data Extraction**: Intelligent content extraction from web pages\n- **Proxy Support**: Enterprise-grade proxy capabilities\n- **Stealth Mode**: Advanced anti-detection features\n- **Context Persistence**: Maintain authentication and state across sessions\n- **Parallel Workflows**: Run multiple browser sessions simultaneously for complex automation tasks\n\nFor more information about the Model Context Protocol, visit:\n\n- [MCP Documentation](https://modelcontextprotocol.io/docs)\n- [MCP Specification](https://spec.modelcontextprotocol.io/)\n\nFor the official MCP Docs:\n\n- [Browserbase MCP](https://docs.browserbase.com/integrations/mcp/introduction)\n\n## License\n\nLicensed under the Apache 2.0 License.\n\nCopyright 2025 Browserbase, Inc.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "browserbase",
        "browser",
        "server",
        "browserbase mcp",
        "servers browserbase",
        "server browserbase"
      ],
      "category": "official-servers"
    },
    "chargebee--agentkit": {
      "owner": "chargebee",
      "name": "agentkit",
      "url": "https://github.com/chargebee/agentkit",
      "imageUrl": "/freedevtools/mcp/pfp/chargebee.webp",
      "description": "Integrate Chargebee's API services and documentation directly into coding environments to enhance development efficiency. Provide context-aware code snippets and access to Chargebee's knowledge base for streamlined integration processes.",
      "stars": 13,
      "forks": 7,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-09-13T14:48:08Z",
      "readme_content": "<p align=\"center\">\n  <img src=\"https://github.com/chargebee/agentkit/blob/main/media/cb-logo.png?raw=true\" alt=\"Chargebee Icon\" width=\"100\" height=\"100\">\n</p>\n\n# Chargebee AgentKit\n\nSeamlessly add Chargebee to your AI Agents using AgentKit for smarter billing and subscription workflows.\n\nAgentKit is a toolkit that enhances AI applications like Claude, Cursor and other agentic AI applications with Chargebee capabilities. It enables users across different roles to integrate their solutions with Chargebee and manage simplifying billing and subscription management using AI.\n\n### Packages\n\n- **[MCP Server](modelcontextprotocol/README.md):** The Chargebee MCP Server provides a powerful set of tools to enhance developer productivity. It integrates with AI-powered code editors like Cursor, Windsurf, and Cline, as well as general-purpose tools such as Claude Desktop.\n\n## Contribution\n\nTo contribute to this project, please see the [contribution guide](CONTRIBUTING.md).\n\n## License\n\n[MIT](LICENSE)\n",
      "npm_url": "https://www.npmjs.com/package/agentkit",
      "npm_downloads": 131,
      "keywords": [
        "chargebee",
        "agentkit",
        "api",
        "chargebee agentkit",
        "chargebee api",
        "integrate chargebee"
      ],
      "category": "official-servers"
    },
    "chroma-core--chroma-mcp": {
      "owner": "chroma-core",
      "name": "chroma-mcp",
      "url": "https://github.com/chroma-core/chroma-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/chroma-core.webp",
      "description": "Integrates vector database capabilities for managing collections, performing semantic searches, and retrieving data with advanced filtering. Enhances AI models with powerful data retrieval features to improve context and performance.",
      "stars": 374,
      "forks": 68,
      "license": "Apache License 2.0",
      "language": "Python",
      "updated_at": "2025-10-01T14:17:21Z",
      "readme_content": "<p align=\"center\">\n  <a href=\"https://trychroma.com\"><img src=\"https://user-images.githubusercontent.com/891664/227103090-6624bf7d-9524-4e05-9d2c-c28d5d451481.png\" alt=\"Chroma logo\"></a>\n</p>\n\n<p align=\"center\">\n    <b>Chroma - the open-source embedding database</b>. <br />\n    The fastest way to build Python or JavaScript LLM apps with memory!\n</p>\n\n<p align=\"center\">\n  <a href=\"https://discord.gg/MMeYNTmh3x\" target=\"_blank\">\n      <img src=\"https://img.shields.io/discord/1073293645303795742?cacheSeconds=3600\" alt=\"Discord\">\n  </a> |\n  <a href=\"https://github.com/chroma-core/chroma/blob/master/LICENSE\" target=\"_blank\">\n      <img src=\"https://img.shields.io/static/v1?label=license&message=Apache 2.0&color=white\" alt=\"License\">\n  </a> |\n  <a href=\"https://docs.trychroma.com/\" target=\"_blank\">\n      Docs\n  </a> |\n  <a href=\"https://www.trychroma.com/\" target=\"_blank\">\n      Homepage\n  </a>\n</p>\n\n# Chroma MCP Server\n\n[![smithery badge](https://smithery.ai/badge/@chroma-core/chroma-mcp)](https://smithery.ai/server/@chroma-core/chroma-mcp)\n\n[The Model Context Protocol (MCP)](https://modelcontextprotocol.io/introduction) is an open protocol designed for effortless integration between LLM applications and external data sources or tools, offering a standardized framework to seamlessly provide LLMs with the context they require.\n\nThis server provides data retrieval capabilities powered by Chroma, enabling AI models to create collections over generated data and user inputs, and retrieve that data using vector search, full text search, metadata filtering, and more.\n\nThis is a MCP server for self-hosting your access to Chroma. If you are looking for [Package Search](https://www.trychroma.com/package-search) you can find the repository for that [here](https://github.com/chroma-core/package-search).\n\n## Features\n\n- **Flexible Client Types**\n  - Ephemeral (in-memory) for testing and development\n  - Persistent for file-based storage\n  - HTTP client for self-hosted Chroma instances\n  - Cloud client for Chroma Cloud integration (automatically connects to api.trychroma.com)\n\n- **Collection Management**\n  - Create, modify, and delete collections\n  - List all collections with pagination support\n  - Get collection information and statistics\n  - Configure HNSW parameters for optimized vector search\n  - Select embedding functions when creating collections\n\n- **Document Operations**\n  - Add documents with optional metadata and custom IDs\n  - Query documents using semantic search\n  - Advanced filtering using metadata and document content\n  - Retrieve documents by IDs or filters\n  - Full text search capabilities\n\n### Supported Tools\n\n- `chroma_list_collections` - List all collections with pagination support\n- `chroma_create_collection` - Create a new collection with optional HNSW configuration\n- `chroma_peek_collection` - View a sample of documents in a collection\n- `chroma_get_collection_info` - Get detailed information about a collection\n- `chroma_get_collection_count` - Get the number of documents in a collection\n- `chroma_modify_collection` - Update a collection's name or metadata\n- `chroma_delete_collection` - Delete a collection\n- `chroma_add_documents` - Add documents with optional metadata and custom IDs\n- `chroma_query_documents` - Query documents using semantic search with advanced filtering\n- `chroma_get_documents` - Retrieve documents by IDs or filters with pagination\n- `chroma_update_documents` - Update existing documents' content, metadata, or embeddings\n- `chroma_delete_documents` - Delete specific documents from a collection\n\n### Embedding Functions\nChroma MCP supports several embedding functions: `default`, `cohere`, `openai`, `jina`, `voyageai`, and `roboflow`.\n\nThe embedding functions utilize Chroma's collection configuration, which persists the selected embedding function of a collection for retrieval. Once a collection is created using the collection configuration, on retrieval for future queries and inserts, the same embedding function will be used, without needing to specify the embedding function again. Embedding function persistance was added in v1.0.0 of Chroma, so if you created a collection using version <=0.6.3, this feature is not supported.\n\nWhen accessing embedding functions that utilize external APIs, please be sure to add the environment variable for the API key with the correct format, found in [Embedding Function Environment Variables](#embedding-function-environment-variables)\n\n## Usage with Claude Desktop\n\n1. To add an ephemeral client, add the following to your `claude_desktop_config.json` file:\n\n```json\n\"chroma\": {\n    \"command\": \"uvx\",\n    \"args\": [\n        \"chroma-mcp\"\n    ]\n}\n```\n\n2. To add a persistent client, add the following to your `claude_desktop_config.json` file:\n\n```json\n\"chroma\": {\n    \"command\": \"uvx\",\n    \"args\": [\n        \"chroma-mcp\",\n        \"--client-type\",\n        \"persistent\",\n        \"--data-dir\",\n        \"/full/path/to/your/data/directory\"\n    ]\n}\n```\n\nThis will create a persistent client that will use the data directory specified.\n\n3. To connect to Chroma Cloud, add the following to your `claude_desktop_config.json` file:\n\n```json\n\"chroma\": {\n    \"command\": \"uvx\",\n    \"args\": [\n        \"chroma-mcp\",\n        \"--client-type\",\n        \"cloud\",\n        \"--tenant\",\n        \"your-tenant-id\",\n        \"--database\",\n        \"your-database-name\",\n        \"--api-key\",\n        \"your-api-key\"\n    ]\n}\n```\n\nThis will create a cloud client that automatically connects to api.trychroma.com using SSL.\n\n**Note:** Adding API keys in arguments is fine on local devices, but for safety, you can also specify a custom path for your environment configuration file using the `--dotenv-path` argument within the `args` list, for example: `\"args\": [\"chroma-mcp\", \"--dotenv-path\", \"/custom/path/.env\"]`.\n\n4. To connect to a [self-hosted Chroma instance on your own cloud provider](https://docs.trychroma.com/\nproduction/deployment), add the following to your `claude_desktop_config.json` file:\n\n```json\n\"chroma\": {\n    \"command\": \"uvx\",\n    \"args\": [\n      \"chroma-mcp\", \n      \"--client-type\", \n      \"http\", \n      \"--host\", \n      \"your-host\", \n      \"--port\", \n      \"your-port\", \n      \"--custom-auth-credentials\",\n      \"your-custom-auth-credentials\",\n      \"--ssl\",\n      \"true\"\n    ]\n}\n```\n\nThis will create an HTTP client that connects to your self-hosted Chroma instance.\n\n### Demos\n\nFind reference usages, such as shared knowledge bases & adding memory to context windows in the [Chroma MCP Docs](https://docs.trychroma.com/integrations/frameworks/anthropic-mcp#using-chroma-with-claude)\n\n### Using Environment Variables\n\nYou can also use environment variables to configure the client. The server will automatically load variables from a `.env` file located at the path specified by `--dotenv-path` (defaults to `.chroma_env` in the working directory) or from system environment variables. Command-line arguments take precedence over environment variables.\n\n```bash\n# Common variables\nexport CHROMA_CLIENT_TYPE=\"http\"  # or \"cloud\", \"persistent\", \"ephemeral\"\n\n# For persistent client\nexport CHROMA_DATA_DIR=\"/full/path/to/your/data/directory\"\n\n# For cloud client (Chroma Cloud)\nexport CHROMA_TENANT=\"your-tenant-id\"\nexport CHROMA_DATABASE=\"your-database-name\"\nexport CHROMA_API_KEY=\"your-api-key\"\n\n# For HTTP client (self-hosted)\nexport CHROMA_HOST=\"your-host\"\nexport CHROMA_PORT=\"your-port\"\nexport CHROMA_CUSTOM_AUTH_CREDENTIALS=\"your-custom-auth-credentials\"\nexport CHROMA_SSL=\"true\"\n\n# Optional: Specify path to .env file (defaults to .chroma_env)\nexport CHROMA_DOTENV_PATH=\"/path/to/your/.env\" \n```\n\n#### Embedding Function Environment Variables\nWhen using external embedding functions that access an API key, follow the naming convention\n`CHROMA_<>_API_KEY=\"<key>\"`.\nSo to set a Cohere API key, set the environment variable `CHROMA_COHERE_API_KEY=\"\"`. We recommend adding this to a .env file somewhere and using the `CHROMA_DOTENV_PATH` environment variable or `--dotenv-path` flag to set that location for safekeeping.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "chroma",
        "searches",
        "retrieval",
        "servers chroma",
        "chroma core",
        "core chroma"
      ],
      "category": "official-servers"
    },
    "cloudflare--mcp-server-cloudflare": {
      "owner": "cloudflare",
      "name": "mcp-server-cloudflare",
      "url": "https://github.com/cloudflare/mcp-server-cloudflare",
      "imageUrl": "/freedevtools/mcp/pfp/cloudflare.webp",
      "description": "Interact with Cloudflare services including deploying Workers, managing KV Store, handling R2 Storage, executing SQL queries in D1 databases, and retrieving analytics data.",
      "stars": 2989,
      "forks": 258,
      "license": "Apache License 2.0",
      "language": "TypeScript",
      "updated_at": "2025-10-02T03:18:27Z",
      "readme_content": "# Cloudflare MCP Server\n\nModel Context Protocol (MCP) is a [new, standardized protocol](https://modelcontextprotocol.io/introduction) for managing context between large language models (LLMs) and external systems. In this repository, you can find several MCP servers allowing you to connect to Cloudflare's service from an MCP client (e.g. Cursor, Claude) and use natural language to accomplish tasks through your Cloudflare account.\n\nThese MCP servers allow your [MCP Client](https://modelcontextprotocol.io/clients) to read configurations from your account, process information, make suggestions based on data, and even make those suggested changes for you. All of these actions can happen across Cloudflare's many services including application development, security and performance.\n\nThe following servers are included in this repository:\n\n| Server Name                                                    | Description                                                                                     | Server URL                                     |\n| -------------------------------------------------------------- | ----------------------------------------------------------------------------------------------- | ---------------------------------------------- |\n| [**Documentation server**](/apps/docs-vectorize)               | Get up to date reference information on Cloudflare                                              | `https://docs.mcp.cloudflare.com/sse`          |\n| [**Workers Bindings server**](/apps/workers-bindings)          | Build Workers applications with storage, AI, and compute primitives                             | `https://bindings.mcp.cloudflare.com/sse`      |\n| [**Workers Builds server**](/apps/workers-builds)              | Get insights and manage your Cloudflare Workers Builds                                          | `https://builds.mcp.cloudflare.com/sse`        |\n| [**Observability server**](/apps/workers-observability)        | Debug and get insight into your application's logs and analytics                                | `https://observability.mcp.cloudflare.com/sse` |\n| [**Radar server**](/apps/radar)                                | Get global Internet traffic insights, trends, URL scans, and other utilities                    | `https://radar.mcp.cloudflare.com/sse`         |\n| [**Container server**](/apps/sandbox-container)                | Spin up a sandbox development environment                                                       | `https://containers.mcp.cloudflare.com/sse`    |\n| [**Browser rendering server**](/apps/browser-rendering)        | Fetch web pages, convert them to markdown and take screenshots                                  | `https://browser.mcp.cloudflare.com/sse`       |\n| [**Logpush server**](/apps/logpush)                            | Get quick summaries for Logpush job health                                                      | `https://logs.mcp.cloudflare.com/sse`          |\n| [**AI Gateway server**](/apps/ai-gateway)                      | Search your logs, get details about the prompts and responses                                   | `https://ai-gateway.mcp.cloudflare.com/sse`    |\n| [**AutoRAG server**](/apps/autorag)                            | List and search documents on your AutoRAGs                                                      | `https://autorag.mcp.cloudflare.com/sse`       |\n| [**Audit Logs server**](/apps/auditlogs)                       | Query audit logs and generate reports for review                                                | `https://auditlogs.mcp.cloudflare.com/sse`     |\n| [**DNS Analytics server**](/apps/dns-analytics)                | Optimize DNS performance and debug issues based on current set up                               | `https://dns-analytics.mcp.cloudflare.com/sse` |\n| [**Digital Experience Monitoring server**](/apps/dex-analysis) | Get quick insight on critical applications for your organization                                | `https://dex.mcp.cloudflare.com/sse`           |\n| [**Cloudflare One CASB server**](/apps/cloudflare-one-casb)    | Quickly identify any security misconfigurations for SaaS applications to safeguard users & data | `https://casb.mcp.cloudflare.com/sse`          |\n| [**GraphQL server**](/apps/graphql/)                           | Get analytics data using Cloudflare‚Äôs GraphQL API                                               | `https://graphql.mcp.cloudflare.com/sse`       |\n\n## Access the remote MCP server from any MCP client\n\nIf your MCP client has first class support for remote MCP servers, the client will provide a way to accept the server URL directly within its interface (e.g. [Cloudflare AI Playground](https://playground.ai.cloudflare.com/))\n\nIf your client does not yet support remote MCP servers, you will need to set up its respective configuration file using mcp-remote (https://www.npmjs.com/package/mcp-remote) to specify which servers your client can access.\n\n```json\n{\n\t\"mcpServers\": {\n\t\t\"cloudflare-observability\": {\n\t\t\t\"command\": \"npx\",\n\t\t\t\"args\": [\"mcp-remote\", \"https://observability.mcp.cloudflare.com/sse\"]\n\t\t},\n\t\t\"cloudflare-bindings\": {\n\t\t\t\"command\": \"npx\",\n\t\t\t\"args\": [\"mcp-remote\", \"https://bindings.mcp.cloudflare.com/sse\"]\n\t\t}\n\t}\n}\n```\n\n## Using Cloudflare's MCP servers from the OpenAI Responses API\n\nTo use one of Cloudflare's MCP servers with [OpenAI's responses API](https://openai.com/index/new-tools-and-features-in-the-responses-api/), you will need to provide the Responses API with an API token that has the scopes (permissions) required for that particular MCP server.\n\nFor example, to use the [Browser Rendering MCP server](https://github.com/cloudflare/mcp-server-cloudflare/tree/main/apps/browser-rendering) with OpenAI, create an API token in the Cloudflare dashboard [here](https://dash.cloudflare.com/profile/api-tokens), with the following permissions:\n\n<img width=\"937\" alt=\"Screenshot 2025-05-21 at 10 38 02‚ÄØAM\" src=\"https://github.com/user-attachments/assets/872e253f-23ce-43b3-983c-45f9d0f66100\" />\n\n## Need access to more Cloudflare tools?\n\nWe're continuing to add more functionality to this remote MCP server repo. If you'd like to leave feedback, file a bug or provide a feature request, [please open an issue](https://github.com/cloudflare/mcp-server-cloudflare/issues/new/choose) on this repository\n\n## Troubleshooting\n\n\"Claude's response was interrupted ... \"\n\nIf you see this message, Claude likely hit its context-length limit and stopped mid-reply. This happens most often on servers that trigger many chained tool calls such as the observability server.\n\nTo reduce the chance of running in to this issue:\n\n- Try to be specific, keep your queries concise.\n- If a single request calls multiple tools, try to to break it into several smaller tool calls to keep the responses short.\n\n## Paid Features\n\nSome features may require a paid Cloudflare Workers plan. Ensure your Cloudflare account has the necessary subscription level for the features you intend to use.\n\n## Contributing\n\nInterested in contributing, and running this server locally? See [CONTRIBUTING.md](CONTRIBUTING.md) to get started.\n",
      "npm_url": "https://www.npmjs.com/package/@cloudflare/mcp-server-cloudflare",
      "npm_downloads": 63925,
      "keywords": [
        "cloudflare",
        "servers",
        "server",
        "cloudflare services",
        "servers cloudflare",
        "server cloudflare"
      ],
      "category": "official-servers"
    },
    "comet-ml--opik-mcp": {
      "owner": "comet-ml",
      "name": "opik-mcp",
      "url": "https://github.com/comet-ml/opik-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/comet-ml.webp",
      "description": "Integrates with IDEs to provide access to Opik features via a standardized protocol. Facilitates the management of prompts, projects, and metrics with a unified interface and various transport options for flexible integration.",
      "stars": 172,
      "forks": 26,
      "license": "Apache License 2.0",
      "language": "TypeScript",
      "updated_at": "2025-10-02T01:32:29Z",
      "readme_content": "<h1 align=\"center\" style=\"border-bottom: none\">\n    <div>\n        <a href=\"https://www.comet.com/site/products/opik/?from=llm&utm_source=opik&utm_medium=github&utm_content=header_img&utm_campaign=opik-mcp\">\n            <picture>\n                <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://raw.githubusercontent.com/comet-ml/opik-mcp/refs/heads/main/docs/assets/logo-dark-mode.svg\">\n                <source media=\"(prefers-color-scheme: light)\" srcset=\"https://raw.githubusercontent.com/comet-ml/opik-mcp/refs/heads/main/docs/assets/logo-light-mode.svg\">\n                \n            </picture>\n        </a>\n        <br>\n        Opik MCP Server\n    </div>\n    (Model Context Protocol)<br>\n</h1>\n\n<p align=\"center\">\nA Model Context Protocol (MCP) implementation for the <a href=\"https://github.com/comet-ml/opik/\">Opik platform</a> with support for multiple transport mechanisms, enabling seamless integration with IDEs and providing a unified interface for Opik's capabilities.\n</p>\n\n<div align=\"center\">\n\n[![License](https://img.shields.io/github/license/comet-ml/opik-mcp)](https://github.com/comet-ml/opik-mcp/blob/main/LICENSE)\n[![Node.js Version](https://img.shields.io/badge/node-%3E%3D20.11.0-brightgreen)](https://nodejs.org/)\n[![TypeScript](https://img.shields.io/badge/typescript-%5E5.8.2-blue)](https://www.typescriptlang.org/)\n<img src=\"https://badge.mcpx.dev?status=on\" title=\"MCP Enabled\"/>\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.15411156.svg)](https://doi.org/10.5281/zenodo.15411156)\n\n</div>\n\n<p align=\"center\">\n    <a href=\"https://www.comet.com/site/products/opik/?from=llm&utm_source=opik&utm_medium=github&utm_content=website_button&utm_campaign=opik\"><b>Website</b></a> ‚Ä¢\n    <a href=\"https://chat.comet.com\"><b>Slack community</b></a> ‚Ä¢\n    <a href=\"https://x.com/Cometml\"><b>Twitter</b></a> ‚Ä¢\n    <a href=\"https://www.comet.com/docs/opik/?from=llm&utm_source=opik&utm_medium=github&utm_content=docs_button&utm_campaign=opik\"><b>Documentation</b></a>\n</p>\n\n<p align=\"center\">\n    <a href=\"https://glama.ai/mcp/servers/@comet-ml/opik-mcp\" rel=\"nofollow\" target=\"_blank\">\n      <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@comet-ml/opik-mcp/badge\" alt=\"Opik Server MCP server\" />\n    </a>\n</p>\n\n> **‚ö†Ô∏è Notice:** SSE (Server-Sent Events) transport support is currently experimental and untested. For production use, we recommend using the direct process execution approach shown in the IDE integration examples.\n\n## üöÄ What is Opik MCP Server?\n\nOpik MCP Server is an open-source implementation of the Model Context Protocol for the Opik platform. It provides a unified interface for interacting with Opik's capabilities, supporting multiple transport mechanisms for flexible integration into various environments.\n\n<br>\n\nYou can use Opik MCP Server for:\n* **IDE Integration:**\n  * Seamlessly integrate with Cursor and other compatible IDEs\n  * Provide direct access to Opik's capabilities from your development environment\n\n* **Unified API Access:**\n  * Access all Opik features through a standardized protocol\n  * Leverage multiple transport options (stdio, SSE) for different integration scenarios\n\n* **Platform Management:**\n  * Manage prompts, projects, traces, and metrics through a consistent interface\n  * Organize and monitor your LLM applications efficiently\n\n## Features\n\n- **Prompts Management**: Create, list, update, and delete prompts\n- **Projects/Workspaces Management**: Organize and manage projects\n- **Traces**: Track and analyze trace data\n- **Metrics**: Gather and query metrics data\n\n## Quick Start\n\n### Installation\n\n#### Cursor Integration\n\nTo integrate with Cursor IDE, open to the Cursor settings page and navigate\nto the Features tab. If you scroll down to the MCP section you will see the\nbutton `+ Add new MCP server` that will allow you to add the Opik MCP server.\n\nOnce the `New MCP server` modal is open, select `command` as the server type and\nenter the command: `npx -y opik-mcp --apiKey YOUR_API_KEY`.\n\nAlternatively, you can create a `.cursor/mcp.json` in your project and add:\n\n```json\n{\n  \"mcpServers\": {\n    \"opik\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"opik-mcp\",\n        \"--apiKey\",\n        \"YOUR_API_KEY\"\n      ]\n    }\n  }\n}\n```\n\nNote: If you are using the Open-Source version of Opik, you will need to specify\nthe `apiBaseUrl` parameter as `http://localhost:5173/api`.\n\n#### Windsurf Installation\n\nTo install the MCP server in Windsurf, you will need to open the Windsurf settings\nand navigate to the MCP section. From there, click on `View raw config` and update\nthe configuration object to be:\n\n```json\n{\n    \"mcpServers\": {\n      \"opik\": {\n        \"command\": \"npx\",\n        \"args\": [\n          \"-y\",\n          \"opik-mcp\",\n          \"--apiKey\",\n          \"YOUR_API_KEY\"\n        ]\n      }\n    }\n  }\n```\n\nNote: If you are using the Open-Source version of Opik, you will need to specify\nthe `apiBaseUrl` parameter as `http://localhost:5173/api`.\n\n#### Manual Installation\n```bash\n# Clone the repository\ngit clone https://github.com/comet-ml/opik-mcp.git\ncd opik-mcp\n\n# Install dependencies and build\nnpm install\nnpm run build\n```\n\n**Configuration**\n\nCreate a `.env` file based on the example:\n\n```bash\ncp .env.example .env\n# Edit .env with your specific configuration\n```\n\n**Starting the Server**\n\n```bash\n# Start with stdio transport (default)\nnpm run start:stdio\n\n# Start with SSE transport for network access (experimental)\nnpm run start:sse\n```\n\n## Transport Options\n\n### Standard Input/Output\n\nIdeal for local integration where the client and server run on the same machine.\n\n```bash\nmake start-stdio\n```\n\n### Server-Sent Events (SSE)\n\nEnables remote access and multiple simultaneous clients over HTTP. Note that this transport option is experimental.\n\n```bash\nmake start-sse\n```\n\nFor detailed information about the SSE transport, see [docs/sse-transport.md](docs/sse-transport.md).\n\n## Development\n\n### Testing\n\n```bash\n# Run all tests\nnpm test\n\n# Run specific test suite\nnpm test -- tests/transports/sse-transport.test.ts\n```\n\n### Pre-commit Hooks\n\nThis project uses pre-commit hooks to ensure code quality:\n\n```bash\n# Run pre-commit checks manually\nmake precommit\n```\n\n## Documentation\n\n- [SSE Transport](docs/sse-transport.md) - Details on using the SSE transport\n- [API Reference](docs/api-reference.md) - Complete API documentation\n- [Configuration](docs/configuration.md) - Advanced configuration options\n- [IDE Integration](docs/ide-integration.md) - Integration with Cursor IDE\n\n## Citation\n\nIf you use this project in your research, please cite it as follows:\n\n```\nComet ML, Inc, Koc, V., & Boiko, Y. (2025). Opik MCP Server. Github. https://doi.org/10.5281/zenodo.15411156\n```\n\nOr use the following BibTeX entry:\n\n```bibtex\n@software{CometML_Opik_MCP_Server_2025,\n  author = {{Comet ML, Inc} and Koc, V. and Boiko, Y.},\n  title = {{Opik MCP Server}},\n  year = {2025},\n  publisher = {GitHub},\n  url = {https://doi.org/10.5281/zenodo.15411156},\n  doi = {10.5281/zenodo.15411156}\n}\n```\n\nYou can also find citation information in the `CITATION.cff` file in this repository.\n\n## License\n\nApache 2.0",
      "npm_url": "https://www.npmjs.com/package/opik-mcp",
      "npm_downloads": 2065,
      "keywords": [
        "comet",
        "opik",
        "protocol",
        "servers comet",
        "opik features",
        "access opik"
      ],
      "category": "official-servers"
    },
    "devhub--devhub-cms-mcp": {
      "owner": "devhub",
      "name": "devhub-cms-mcp",
      "url": "https://github.com/devhub/devhub-cms-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/devhub.webp",
      "description": "Manage content within the DevHub CMS system, allowing for the creation, updating, and retrieval of blog posts and media through an intuitive interface. Integrates with Large Language Models to enhance content management tasks without direct API integration.",
      "stars": 6,
      "forks": 12,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-09-22T12:49:55Z",
      "readme_content": "# DevHub CMS MCP\n\n[![smithery badge](https://smithery.ai/badge/@devhub/devhub-cms-mcp)](https://smithery.ai/server/@devhub/devhub-cms-mcp)\n\nA [Model Context Protocol (MCP)](https://modelcontextprotocol.io/) integration for managing content in the [DevHub CMS system](https://www.devhub.com/).\n\n## Installation\n\nYou will need the [uv](https://github.com/astral-sh/uv) package manager installed on your local system.\n\n### Manual configuration of Claude Desktop\n\nTo use this server with the [Claude Desktop app](https://claude.ai/download), add the following configuration to the \"mcpServers\" section of your `claude_desktop_config.json`:\n\n```\n{\n    \"mcpServers\": {\n        \"devhub_cms_mcp\": {\n            \"command\": \"uvx\",\n            \"args\": [\n                \"devhub-cms-mcp\"\n            ],\n            \"env\": {\n                \"DEVHUB_API_KEY\": \"YOUR_KEY_HERE\",\n                \"DEVHUB_API_SECRET\": \"YOUR_SECRET_HERE\",\n                \"DEVHUB_BASE_URL\": \"https://yourbrand.cloudfrontend.net\"\n            }\n        }\n    }\n}\n```\n\nAfter updating the config, restart Claude Desktop.\n\n### Manual configuration for Cursor\n\nThis MCP can also be used in cursor with a similar configuration from above added to your [Cursor](https://www.cursor.com/) global environment or to individual projects.\n\nExamples [here](https://docs.cursor.com/context/model-context-protocol#configuring-mcp-servers)\n\n### Installing via Claude Code\n\nClaude Code's command line [supports MCP installs](https://docs.anthropic.com/en/docs/agents-and-tools/claude-code/tutorials#set-up-model-context-protocol-mcp).\n\nYou can add the `devhub-cms-mcp` by updating the environment variables below\n\n```\nclaude mcp add devhub-cms-mcp \\\n    -e DEVHUB_API_KEY=YOUR_KEY_HERE \\\n    -e DEVHUB_API_SECRET=YOUR_SECRET_HERE \\\n    -e DEVHUB_BASE_URL=https://yourbrand.cloudfrontend.net \\\n    -- uvx devhub-cms-mcp\n```\n\n### Installing via Smithery\n\nTo install DevHub CMS MCP for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@devhub/devhub-cms-mcp):\n\n```bash\nnpx -y @smithery/cli install @devhub/devhub-cms-mcp --client claude\n```\n\n## Local development\n\n### Clone the repo (or your fork)\n\n```\ngit clone git@github.com:devhub/devhub-cms-mcp.git\n```\n\n### Manual configuration of Claude Desktop\n\nTo use this server with the Claude Desktop app for local development, add the following configuration to the \"mcpServers\" section of your `claude_desktop_config.json`:\n\n```\n{\n    \"mcpServers\": {\n        \"devhub_cms_mcp\": {\n            \"command\": \"uv\",\n            \"args\": [\n                \"--directory\",\n                \"/YOUR/LOCAL/PATH/devhub-cms-mcp/\",\n                \"run\",\n                \"main.py\"\n            ],\n            \"env\": {\n                \"DEVHUB_API_KEY\": \"YOUR_KEY_HERE\",\n                \"DEVHUB_API_SECRET\": \"YOUR_SECRET_HERE\",\n                \"DEVHUB_BASE_URL\": \"https://yourbrand.cloudfrontend.net\"\n            }\n        }\n    }\n}\n```\n\nAfter updating the config, restart Claude Desktop.\n\n### Configuration for running with `uv` directly\n\nThis MCP requires the following environment variables to be set:\n\n```bash\nexport DEVHUB_API_KEY=\"your_api_key\"\nexport DEVHUB_API_SECRET=\"your_api_secret\"\nexport DEVHUB_BASE_URL=\"https://yourbrand.cloudfrontend.net\"\n```\n\nThen run the MCP\n\n```\nuv run main.py\n```\n\n## Available Tools\n\nThis MCP provides the following tools for interacting with DevHub CMS:\n\n### Business and Location Management\n\n- **get_businesses()**: Gets all businesses within the DevHub account. Returns a list of businesses with their IDs and names.\n- **get_locations(business_id)**: Gets all locations for a specific business. Returns detailed location information including address, coordinates, and URLs.\n- **get_hours_of_operation(location_id, hours_type='primary')**: Gets the hours of operation for a specific DevHub location. Returns a structured list of time ranges for each day of the week.\n- **update_hours(location_id, new_hours, hours_type='primary')**: Updates the hours of operation for a DevHub location.\n- **get_nearest_location(business_id, latitude, longitude)**: Finds the nearest DevHub location based on geographic coordinates.\n- **site_from_url(url)**: Gets the DevHub site ID and details from a URL. Returns site ID, URL, and associated location IDs.\n\n### Content Management\n\n- **get_blog_post(post_id)**: Retrieves a single blog post by ID, including its title, date, and HTML content.\n- **create_blog_post(site_id, title, content)**: Creates a new blog post. The content should be in HTML format and should not include an H1 tag.\n- **update_blog_post(post_id, title=None, content=None)**: Updates an existing blog post's title and/or content.\n\n### Media Management\n\n- **upload_image(base64_image_content, filename)**: Uploads an image to the DevHub media gallery. Supports webp, jpeg, and png formats. The image must be provided as a base64-encoded string.\n\n## Usage with LLMs\n\nThis MCP is designed to be used with Large Language Models that support the Model Context Protocol. It allows LLMs to manage content in DevHub CMS without needing direct API access integrated into the LLM natively.\n\n## Testing\n\nThis package includes a test suite with mocked requests to the DevHub API, allowing you to test the functionality without making actual API calls.\n\n### Running Tests\n\nTo run the tests, first install the package with test dependencies:\n\n```bash\nuv pip install -e \".[test]\"\n```\n\nRun the tests with pytest:\n\n```bash\nuv run pytest\n```\n\nFor more detailed output and test coverage information:\n\n```bash\nuv run pytest -v --cov=devhub_cms_mcp\n```\n\n### Test Structure\n\n- `tests/devhub_cms_mcp/test_mcp_integration.py`: Tests for MCP integration endpoints\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "cms",
        "devhub",
        "blog",
        "devhub cms",
        "content devhub",
        "content management"
      ],
      "category": "official-servers"
    },
    "elie222--inbox-zero": {
      "owner": "elie222",
      "name": "inbox-zero",
      "url": "https://github.com/elie222/inbox-zero",
      "imageUrl": "/freedevtools/mcp/pfp/elie222.webp",
      "description": "Manage emails effectively with an AI assistant that automates various tasks related to inbox organization and communication. Integrates into environments like Cursor and Claude Desktop for enhanced email management.",
      "stars": 8925,
      "forks": 1029,
      "license": "Other",
      "language": "TypeScript",
      "updated_at": "2025-10-02T02:46:22Z",
      "readme_content": "[](https://www.getinboxzero.com)\n\n<p align=\"center\">\n  <a href=\"https://www.getinboxzero.com\">\n    <h1 align=\"center\">Inbox Zero - your 24/7 AI email assistant</h1>\n  </a>\n  <p align=\"center\">\n    Organizes your inbox, pre-drafts replies, and tracks follow‚Äëups - so you reach inbox zero faster. Open source alternative to Fyxer, but more customisable and secure.\n    <br />\n    <a href=\"https://www.getinboxzero.com\">Website</a>\n    ¬∑\n    <a href=\"https://www.getinboxzero.com/discord\">Discord</a>\n    ¬∑\n    <a href=\"https://github.com/elie222/inbox-zero/issues\">Issues</a>\n  </p>\n</p>\n\n<div align=\"center\">\n\n![Stars](https://img.shields.io/github/stars/elie222/inbox-zero?labelColor=black&style=for-the-badge&color=2563EB)\n![Forks](https://img.shields.io/github/forks/elie222/inbox-zero?labelColor=black&style=for-the-badge&color=2563EB)\n\n<a href=\"https://trendshift.io/repositories/6400\" target=\"_blank\"><img src=\"https://trendshift.io/api/badge/repositories/6400\" alt=\"elie222%2Finbox-zero | Trendshift\" style=\"width: 250px; height: 55px;\" width=\"250\" height=\"55\"/></a>\n\n[![Vercel OSS Program](https://vercel.com/oss/program-badge.svg)](https://vercel.com/oss)\n\n</div>\n\n## Mission\n\nTo help you spend less time in your inbox, so you can focus on what matters.\n\n<br />\n\n[![Deploy with Vercel](https://vercel.com/button)](https://vercel.com/new/clone?repository-url=https%3A%2F%2Fgithub.com%2Felie222%2Finbox-zero&env=AUTH_SECRET,GOOGLE_CLIENT_ID,GOOGLE_CLIENT_SECRET,MICROSOFT_CLIENT_ID,MICROSOFT_CLIENT_SECRET,EMAIL_ENCRYPT_SECRET,EMAIL_ENCRYPT_SALT,UPSTASH_REDIS_URL,UPSTASH_REDIS_TOKEN,GOOGLE_PUBSUB_TOPIC_NAME,DATABASE_URL,NEXT_PUBLIC_BASE_URL)\n\n## Features\n\n- **AI Personal Assistant:** Organizes your inbox and pre-drafts replies in your tone and style.\n- **Cursor Rules for email:** Explain in plain English how your AI should handle your inbox.\n- **Reply Zero:** Track emails to reply to and those awaiting responses.\n- **Smart Categories:** Automatically categorize every sender.\n- **Bulk Unsubscriber:** One-click unsubscribe and archive emails you never read.\n- **Cold Email Blocker:** Auto‚Äëblock cold emails.\n- **Email Analytics:** Track your activity and trends over time.\n\nLearn more in our [docs](https://docs.getinboxzero.com).\n\n## Feature Screenshots\n\n|  |                |\n| :------------------------------------------------------: | :-------------------------------------------------------------: |\n|                      _AI Assistant_                      |                          _Reply Zero_                           |\n|     |  |\n|                      _Gmail client_                      |                       _Bulk Unsubscriber_                       |\n\n## Demo Video\n\n[](http://www.youtube.com/watch?v=hfvKvTHBjG0)\n\n## Built with\n\n- [Next.js](https://nextjs.org/)\n- [Tailwind CSS](https://tailwindcss.com/)\n- [shadcn/ui](https://ui.shadcn.com/)\n- [Prisma](https://www.prisma.io/)\n- [Upstash](https://upstash.com/)\n- [Turborepo](https://turbo.build/)\n\n## Star History\n\n[![Star History Chart](https://api.star-history.com/svg?repos=elie222/inbox-zero&type=Date)](https://www.star-history.com/#elie222/inbox-zero&Date)\n\n## Feature Requests\n\nTo request a feature open a [GitHub issue](https://github.com/elie222/inbox-zero/issues), or join our [Discord](https://www.getinboxzero.com/discord).\n\n## Getting Started for Developers\n\nWe offer a hosted version of Inbox Zero at [https://getinboxzero.com](https://getinboxzero.com). To self-host follow the steps below.\n\n### Self-Hosting with Docker on VPS\n\nFor a complete guide on deploying Inbox Zero to a VPS using Docker, see our [Docker Self-Hosting Guide](docs/hosting/docker.md).\n\n### Setup\n\n[Here's a video](https://youtu.be/hVQENQ4WT2Y) on how to set up the project. It covers the same steps mentioned in this document. But goes into greater detail on setting up the external services.\n\n### Requirements\n\n- [Node.js](https://nodejs.org/en/) >= 18.0.0\n- [pnpm](https://pnpm.io/) >= 8.6.12\n- [Docker desktop](https://www.docker.com/products/docker-desktop/) (recommended but optional)\n\nMake sure you have the above installed before starting.\n\nThe external services that are required are (detailed setup instructions below):\n\n- [Google OAuth](https://console.cloud.google.com/apis/credentials)\n- [Google PubSub](https://console.cloud.google.com/cloudpubsub/topic/list)\n\n### Updating .env file: secrets\n\nCreate your own `.env` file from the example supplied:\n\n```bash\ncp apps/web/.env.example apps/web/.env\ncd apps/web\npnpm install\n```\n\nSet the environment variables in the newly created `.env`. You can see a list of required variables in: `apps/web/env.ts`.\n\nThe required environment variables:\n\n- `AUTH_SECRET` -- can be any random string (try using `openssl rand -hex 32` for a quick secure random string)\n- `EMAIL_ENCRYPT_SECRET` -- Secret key for encrypting OAuth tokens (try using `openssl rand -hex 32` for a secure key)\n- `EMAIL_ENCRYPT_SALT` -- Salt for encrypting OAuth tokens (try using `openssl rand -hex 16` for a secure salt)\n\n\n- `NEXT_PUBLIC_BASE_URL` -- The URL where your app is hosted (e.g., `http://localhost:3000` for local development or `https://yourdomain.com` for production).\n- `INTERNAL_API_KEY` -- A secret key for internal API calls (try using `openssl rand -hex 32` for a secure key)\n\n- `UPSTASH_REDIS_URL` -- Redis URL from Upstash. (can be empty if you are using Docker Compose)\n- `UPSTASH_REDIS_TOKEN` -- Redis token from Upstash. (or specify your own random string if you are using Docker Compose)\n\nWhen using Vercel with Fluid Compute turned off, you should set `MAX_DURATION=300` or lower. See Vercel limits for different plans [here](https://vercel.com/docs/functions/configuring-functions/duration#duration-limits).\n\n### Updating .env file with Google OAuth credentials:\n\n- `GOOGLE_CLIENT_ID` -- Google OAuth client ID. More info [here](https://next-auth.js.org/providers/google)\n- `GOOGLE_CLIENT_SECRET` -- Google OAuth client secret. More info [here](https://next-auth.js.org/providers/google)\n\nGo to [Google Cloud](https://console.cloud.google.com/). Create a new project if necessary.\n\nCreate [new credentials](https://console.cloud.google.com/apis/credentials):\n\n1.  If the banner shows up, configure **consent screen** (if not, you can do this later)\n    1. Click the banner, then Click `Get Started`.\n    2. Choose a name for your app, and enter your email.\n    3. In Audience, choose `External`\n    4. Enter your contact information\n    5. Agree to the User Data policy and then click `Create`.\n    6. Return to APIs and Services using the left sidebar.\n2.  Create new [credentials](https://console.cloud.google.com/apis/credentials):\n    1. Click the `+Create Credentials` button. Choose OAuth Client ID.\n    2. In `Application Type`, Choose `Web application`\n    3. Choose a name for your web client\n    4. In Authorized JavaScript origins, add a URI and enter `http://localhost:3000`\n    5. In `Authorized redirect URIs` enter:\n      - `http://localhost:3000/api/auth/callback/google`\n      - `http://localhost:3000/api/google/linking/callback`\n    6. Click `Create`.\n    7. A popup will show up with the new credentials, including the Client ID and secret.\n3.  Update .env file:\n    1. Copy the Client ID to `GOOGLE_CLIENT_ID`\n    2. Copy the Client secret to `GOOGLE_CLIENT_SECRET`\n4.  Update [scopes](https://console.cloud.google.com/auth/scopes)\n\n    1. Go to `Data Access` in the left sidebar (or click link above)\n    2. Click `Add or remove scopes`\n    3. Copy paste the below into the `Manually add scopes` box:\n\n    ```plaintext\n    https://www.googleapis.com/auth/userinfo.profile\n    https://www.googleapis.com/auth/userinfo.email\n    https://www.googleapis.com/auth/gmail.modify\n    https://www.googleapis.com/auth/gmail.settings.basic\n    https://www.googleapis.com/auth/contacts\n    ```\n\n    4. Click `Update`\n    5. Click `Save` in the Data Access page.\n\n5.  Add yourself as a test user\n    1. Go to [Audience](https://console.cloud.google.com/auth/audience)\n    2. In the `Test users` section, click `+Add users`\n    3. Enter your email and press `Save`\n\n### Updating .env file with Microsoft OAuth credentials:\n\n- `MICROSOFT_CLIENT_ID` -- Microsoft OAuth client ID\n- `MICROSOFT_CLIENT_SECRET` -- Microsoft OAuth client secret\n\nGo to [Microsoft Azure Portal](https://portal.azure.com/). Create a new Azure Active Directory app registration:\n\n1. Navigate to Azure Active Directory\n2. Go to \"App registrations\" in the left sidebar or search it in the searchbar\n3. Click \"New registration\"\n\n   1. Choose a name for your application\n   2. Under \"Supported account types\" select \"Accounts in any organizational directory (Any Azure AD directory - Multitenant) and personal Microsoft accounts (e.g. Skype, Xbox)\"\n   3. Set the Redirect URI:\n      - Platform: Web\n      - URL: `http://localhost:3000/api/auth/callback/microsoft`\n   4. Click \"Register\"\n   5. In the \"Manage\" menu click \"Authentication (Preview)\"\n   6. Add the Redirect URI: `http://localhost:3000/api/outlook/linking/callback`\n\n4. Get your credentials:\n\n   1. The \"Application (client) ID\" shown is your `MICROSOFT_CLIENT_ID`\n   2. To get your client secret:\n      - Click \"Certificates & secrets\" in the left sidebar\n      - Click \"New client secret\"\n      - Add a description and choose an expiry\n      - Click \"Add\"\n      - Copy the secret Value (not the ID) - this is your `MICROSOFT_CLIENT_SECRET`\n\n5. Configure API permissions:\n\n   1. In the \"Manage\" menu click \"API permissions\" in the left sidebar\n   2. Click \"Add a permission\"\n   3. Select \"Microsoft Graph\"\n   4. Select \"Delegated permissions\"\n   5. Add the following permissions:\n\n      - openid\n      - profile\n      - email\n      - User.Read\n      - offline_access\n      - Mail.ReadWrite\n      - Mail.Send\n      - Mail.ReadBasic\n      - Mail.Read\n      - Mail.Read.Shared\n      - MailboxSettings.ReadWrite\n      - Contacts.ReadWrite\n\n   6. Click \"Add permissions\"\n   7. Click \"Grant admin consent\" if you're an admin\n\n6. Update your .env file with the credentials:\n   ```plaintext\n   MICROSOFT_CLIENT_ID=your_client_id_here\n   MICROSOFT_CLIENT_SECRET=your_client_secret_here\n   ```\n\n### Updating .env file with LLM parameters\n\nYou need to set an LLM, but you can use a local one too:\n\n- [Anthropic](https://console.anthropic.com/settings/keys)\n- [OpenAI](https://platform.openai.com/api-keys)\n- AWS Bedrock Anthropic\n- Google Gemini\n- Groq Llama 3.3 70B\n- Ollama (local)\n\nFor the LLM, you can use Anthropic, OpenAI, or Anthropic on AWS Bedrock. You\ncan also use Ollama by setting the following enviroment variables:\n\n```sh\nOLLAMA_BASE_URL=http://localhost:11434/api\nNEXT_PUBLIC_OLLAMA_MODEL=phi3\n```\n\nNote: If you need to access Ollama hosted locally and the application is running on Docker setup, you can use `http://host.docker.internal:11434/api` as the base URL. You might also need to set `OLLAMA_HOST` to `0.0.0.0` in the Ollama configuration file.\n\nYou can select the model you wish to use in the app on the `/settings` page of the app.\n\nIf you are using local ollama, you can set it to be default:\n\n```sh\nDEFAULT_LLM_PROVIDER=ollama\n```\n\nIf this is the case you must also set the `ECONOMY_LLM_PROVIDER` environment variable.\n\n### Redis and Postgres\n\nWe use Postgres for the database.\nFor Redis, you can use [Upstash Redis](https://upstash.com/) or set up your own Redis instance.\n\nYou can run Postgres & Redis locally using `docker-compose`\n\n```bash\ndocker-compose up -d # -d will run the services in the background\n```\n\n### Running the app\n\nTo run the migrations:\n\n```bash\npnpm prisma migrate dev\n```\n\nTo run the app locally for development (slower):\n\n```bash\npnpm run dev\n```\n\nOr from the project root:\n\n```bash\nturbo dev\n```\n\nTo build and run the app locally in production mode (faster):\n\n```bash\npnpm run build\npnpm start\n```\n\nOpen [http://localhost:3000](http://localhost:3000) to view the app in your browser.\n\n### Premium\n\nMany features are available only to premium users. To upgrade yourself, make yourself an admin in the `.env`: `ADMINS=hello@gmail.com`\nThen upgrade yourself at: [http://localhost:3000/admin](http://localhost:3000/admin).\n\n### Set up push notifications via Google PubSub to handle emails in real time\n\nFollow instructions [here](https://developers.google.com/gmail/api/guides/push).\n\n1. [Create a topic](https://developers.google.com/gmail/api/guides/push#create_a_topic)\n2. [Create a subscription](https://developers.google.com/gmail/api/guides/push#create_a_subscription)\n3. [Grant publish rights on your topic](https://developers.google.com/gmail/api/guides/push#grant_publish_rights_on_your_topic)\n\nSet env var `GOOGLE_PUBSUB_TOPIC_NAME`.\nWhen creating the subscription select Push and the url should look something like: `https://www.getinboxzero.com/api/google/webhook?token=TOKEN` or `https://abc.ngrok-free.app/api/google/webhook?token=TOKEN` where the domain is your domain. Set `GOOGLE_PUBSUB_VERIFICATION_TOKEN` in your `.env` file to be the value of `TOKEN`.\n\nTo run in development ngrok can be helpful:\n\n```sh\nngrok http 3000\n# or with an ngrok domain to keep your endpoint stable (set `XYZ`):\nngrok http --domain=XYZ.ngrok-free.app 3000\n```\n\nAnd then update the webhook endpoint in the [Google PubSub subscriptions dashboard](https://console.cloud.google.com/cloudpubsub/subscription/list).\n\nTo start watching emails visit: `/api/watch/all`\n\n### Watching for email updates\n\nSet a cron job to run these:\nThe Google watch is necessary. Others are optional.\n\n```json\n  \"crons\": [\n    {\n      \"path\": \"/api/watch/all\",\n      \"schedule\": \"0 1 * * *\"\n    },\n    {\n      \"path\": \"/api/resend/summary/all\",\n      \"schedule\": \"0 16 * * 1\"\n    },\n    {\n      \"path\": \"/api/reply-tracker/disable-unused-auto-draft\",\n      \"schedule\": \"0 3 * * *\"\n    }\n  ]\n```\n\n[Here](https://vercel.com/guides/how-to-setup-cron-jobs-on-vercel#alternative-cron-providers) are some easy ways to run cron jobs. Upstash is a free, easy option. I could never get the Vercel `vercel.json`. Open to PRs if you find a fix for that.\n\n### Docker Build Instructions\n\nWhen building the Docker image, you **must** specify your `NEXT_PUBLIC_BASE_URL` as a build argument. This is because Next.js embeds `NEXT_PUBLIC_*` variables at build time, not runtime.\n\n### Building the Docker image\n\n```bash\n# For production with your custom domain\ndocker build \\\n  --build-arg NEXT_PUBLIC_BASE_URL=\"https://your-domain.com\" \\\n  -t inbox-zero \\\n  -f docker/Dockerfile.prod .\n\n# For local development (default)\ndocker build -t inbox-zero -f docker/Dockerfile.prod .\n```\n\n### Running the container\n\nAfter building, run the container with your runtime secrets:\n\n```bash\ndocker run -p 3000:3000 \\\n  -e DATABASE_URL=\"your-database-url\" \\\n  -e AUTH_SECRET=\"your-auth-secret\" \\\n  -e GOOGLE_CLIENT_ID=\"your-google-client-id\" \\\n  -e GOOGLE_CLIENT_SECRET=\"your-google-client-secret\" \\\n  # ... other runtime environment variables\n  inbox-zero\n```\n\n**Important:** If you need to change `NEXT_PUBLIC_BASE_URL`, you must rebuild the Docker image. It cannot be changed at runtime.\n\nFor more detailed Docker build instructions and security considerations, see [docker/DOCKER_BUILD_GUIDE.md](docker/DOCKER_BUILD_GUIDE.md).\n\n\n### Calendar integrations\n\n*Note:* The calendar integration feature is a work in progress.\n\n#### Google Calendar\n\n1. Visit: https://console.cloud.google.com/apis/library\n2. Search for \"Google Calendar API\"\n3. Click on it and then click \"Enable\"\n4. Visit: [credentials](https://console.cloud.google.com/apis/credentials):\n    1. Click on your project\n    2. In `Authorized redirect URIs` add:\n      - `http://localhost:3000/api/google/calendar/callback`\n\n## Contributing to the project\n\nYou can view open tasks in our [GitHub Issues](https://github.com/elie222/inbox-zero/issues).\nJoin our [Discord](https://www.getinboxzero.com/discord) to discuss tasks and check what's being worked on.\n\n[ARCHITECTURE.md](./ARCHITECTURE.md) explains the architecture of the project (LLM generated).",
      "npm_url": "https://www.npmjs.com/package/inbox-zero",
      "npm_downloads": 54,
      "keywords": [
        "inbox",
        "emails",
        "elie222",
        "elie222 inbox",
        "inbox zero",
        "inbox organization"
      ],
      "category": "official-servers"
    },
    "exa-labs--exa-mcp-server": {
      "owner": "exa-labs",
      "name": "exa-mcp-server",
      "url": "https://github.com/exa-labs/exa-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/exa-labs.webp",
      "description": "Fast and intelligent web search and crawling that combines embeddings and traditional search methods to provide optimal results for large language models (LLMs). It facilitates real-time web information retrieval in a secure environment.",
      "stars": 2791,
      "forks": 216,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-10-02T09:12:40Z",
      "readme_content": "# Exa MCP Server üîç\n[![npm version](https://badge.fury.io/js/exa-mcp-server.svg)](https://www.npmjs.com/package/exa-mcp-server)\n[![smithery badge](https://smithery.ai/badge/exa)](https://smithery.ai/server/exa)\n\n## üÜï `exa-code`: fast, efficient web context for coding agents\n\nVibe coding should never have a bad vibe. `exa-code` is a huge step towards coding agents that never hallucinate.\n\nWhen your coding agent makes a search query, `exa-code` searches over billions\nof Github repos, docs pages, Stackoverflow posts, and more, to find the perfect, token-efficient context that the agent needs to code correctly. It's powered by the Exa search engine.\n\nExamples of queries you can make with `exa-code`:\n* use Exa search in python and make sure content is always livecrawled\n* use correct syntax for vercel ai sdk to call gpt-5 nano asking it how are you\n* how to set up a reproducible Nix Rust development environment\n\n**‚ú® Works with Cursor and Claude Code!** Use the HTTP-based configuration format:\n\n```json\n{\n  \"mcpServers\": {\n    \"exa\": {\n      \"type\": \"http\",\n      \"url\": \"https://mcp.exa.ai/mcp\",\n      \"headers\": {\n        \"Remove-Me\": \"Disable web_search_exa tool if you're just coding. To 100% call exa-code, say 'use exa-code'.\"\n      }\n    }\n  }\n}\n```\n\nYou may include your exa api key in the url like this:\n```\nhttps://mcp.exa.ai/mcp?exaApiKey=YOUREXAKEY\n```\n\nYou may whitelist specific tools in the url with the `enabledTools` parameter which expects a url encoded array strings like this:\n```\nhttps://mcp.exa.ai/mcp?exaApiKey=YOUREXAKEY&enabledTools=%5B%22crawling_exa%ss%5D\n```\n\nYou can also use `exa-code` through [Smithery](https://smithery.ai/server/exa) without an Exa API key.\n\n---\n\nA Model Context Protocol (MCP) server that connects AI assistants like Claude to Exa AI's search capabilities, including web search, research tools, and our new code search feature.\n\n## Remote Exa MCP üåê\n\nConnect directly to Exa's hosted MCP server (instead of running it locally).\n\n### Remote Exa MCP URL\n\n```\nhttps://mcp.exa.ai/mcp\n```\n\n### Claude Desktop Configuration for Remote MCP\n\nAdd this to your Claude Desktop configuration file:\n\n```json\n{\n  \"mcpServers\": {\n    \"exa\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"mcp-remote\",\n        \"https://mcp.exa.ai/mcp\"\n      ]\n    }\n  }\n}\n```\n\n### Cursor and Claude Code Configuration for Remote MCP\n\nFor Cursor and Claude Code, use this HTTP-based configuration format:\n\n```json\n{\n  \"mcpServers\": {\n    \"exa\": {\n      \"type\": \"http\",\n      \"url\": \"https://mcp.exa.ai/mcp\",\n      \"headers\": {}\n    }\n  }\n}\n```\n\n### NPM Installation\n\n```bash\nnpm install -g exa-mcp-server\n```\n\n### Using Claude Code\n\n```bash\nclaude mcp add exa -e EXA_API_KEY=YOUR_API_KEY -- npx -y exa-mcp-server\n```\n\n### Using Exa MCP through Smithery\n\nTo install the Exa MCP server via [Smithery](https://smithery.ai/server/exa), head over to:\n\n[smithery.ai/server/exa](https://smithery.ai/server/exa)\n\n\n## Configuration ‚öôÔ∏è\n\n### 1. Configure Claude Desktop to recognize the Exa MCP server\n\nYou can find claude_desktop_config.json inside the settings of Claude Desktop app:\n\nOpen the Claude Desktop app and enable Developer Mode from the top-left menu bar. \n\nOnce enabled, open Settings (also from the top-left menu bar) and navigate to the Developer Option, where you'll find the Edit Config button. Clicking it will open the claude_desktop_config.json file, allowing you to make the necessary edits. \n\nOR (if you want to open claude_desktop_config.json from terminal)\n\n#### For macOS:\n\n1. Open your Claude Desktop configuration:\n\n```bash\ncode ~/Library/Application\\ Support/Claude/claude_desktop_config.json\n```\n\n#### For Windows:\n\n1. Open your Claude Desktop configuration:\n\n```powershell\ncode %APPDATA%\\Claude\\claude_desktop_config.json\n```\n\n### 2. Add the Exa server configuration:\n\n```json\n{\n  \"mcpServers\": {\n    \"exa\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"exa-mcp-server\"],\n      \"env\": {\n        \"EXA_API_KEY\": \"your-api-key-here\"\n      }\n    }\n  }\n}\n```\n\nReplace `your-api-key-here` with your actual Exa API key from [dashboard.exa.ai/api-keys](https://dashboard.exa.ai/api-keys).\n\n### 3. Available Tools & Tool Selection\n\nThe Exa MCP server includes powerful tools for developers and researchers:\n\n#### üî• **Featured: Code Search Tool**\n- **get_code_context_exa**: üÜï **NEW!** Search and get relevant code snippets, examples, and documentation from open source libraries, GitHub repositories, and programming frameworks. Perfect for finding up-to-date code documentation, implementation examples, API usage patterns, and best practices from real codebases.\n\n#### üåê **Other Available Tools**\n- **web_search_exa**: Performs real-time web searches with optimized results and content extraction.\n- **company_research**: Comprehensive company research tool that crawls company websites to gather detailed information about businesses.\n- **crawling**: Extracts content from specific URLs, useful for reading articles, PDFs, or any web page when you have the exact URL.\n- **linkedin_search**: Search LinkedIn for companies and people using Exa AI. Simply include company names, person names, or specific LinkedIn URLs in your query.\n- **deep_researcher_start**: Start a smart AI researcher for complex questions. The AI will search the web, read many sources, and think deeply about your question to create a detailed research report.\n- **deep_researcher_check**: Check if your research is ready and get the results. Use this after starting a research task to see if it's done and get your comprehensive report.\n\nYou can choose which tools to enable by adding the `--tools` parameter to your Claude Desktop configuration:\n\n#### üíª **Setup for Code Search Only** (Recommended for Developers)\n\n```json\n{\n  \"mcpServers\": {\n    \"exa\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"exa-mcp-server\",\n        \"--tools=get_code_context_exa,web_search_exa\"\n      ],\n      \"env\": {\n        \"EXA_API_KEY\": \"your-api-key-here\"\n      }\n    }\n  }\n}\n```\n\n#### Specify which tools to enable:\n\n```json\n{\n  \"mcpServers\": {\n    \"exa\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"exa-mcp-server\",\n        \"--tools=get_code_context_exa,web_search_exa,company_research,crawling,linkedin_search,deep_researcher_start,deep_researcher_check\"\n      ],\n      \"env\": {\n        \"EXA_API_KEY\": \"your-api-key-here\"\n      }\n    }\n  }\n}\n```\n\nFor enabling multiple tools, use a comma-separated list:\n\n```json\n{\n  \"mcpServers\": {\n    \"exa\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"exa-mcp-server\",\n        \"--tools=get_code_context_exa,web_search_exa,company_research,crawling,linkedin_search,deep_researcher_start,deep_researcher_check\"\n      ],\n      \"env\": {\n        \"EXA_API_KEY\": \"your-api-key-here\"\n      }\n    }\n  }\n}\n```\n\nIf you don't specify any tools, all tools enabled by default will be used.\n\n### 4. Restart Claude Desktop\n\nFor the changes to take effect:\n\n1. Completely quit Claude Desktop (not just close the window)\n2. Start Claude Desktop again\n3. Look for the icon to verify the Exa server is connected\n\n## Using via NPX\n\nIf you prefer to run the server directly, you can use npx:\n\n```bash\n# Run with all tools enabled by default\nnpx exa-mcp-server\n\n# Enable specific tools only\nnpx exa-mcp-server --tools=web_search_exa\n\n# Enable multiple tools\nnpx exa-mcp-server --tools=web_search_exa,get_code_context_exa\n\n# List all available tools\nnpx exa-mcp-server --list-tools\n```\n\n---\n\nBuilt with ‚ù§Ô∏è by team Exa\n",
      "npm_url": "https://www.npmjs.com/package/exa-mcp-server",
      "npm_downloads": 95247,
      "keywords": [
        "search",
        "retrieval",
        "crawling",
        "search crawling",
        "web search",
        "information retrieval"
      ],
      "category": "official-servers"
    },
    "firecrawl--firecrawl-mcp-server": {
      "owner": "firecrawl",
      "name": "firecrawl-mcp-server",
      "url": "https://github.com/firecrawl/firecrawl-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/firecrawl.webp",
      "description": "Integrates with Firecrawl for advanced web scraping capabilities including structured data extraction and batch processing, while supporting both cloud and self-hosted deployments. Provides features like automatic retries, rate limiting, and content analysis powered by LLMs.",
      "stars": 4642,
      "forks": 489,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-10-02T07:44:50Z",
      "readme_content": "<div align=\"center\">\n  <a name=\"readme-top\"></a>\n  <img\n    src=\"https://raw.githubusercontent.com/firecrawl/firecrawl-mcp-server/main/img/fire.png\"\n    height=\"140\"\n  >\n</div>\n\n# Firecrawl MCP Server\n\nA Model Context Protocol (MCP) server implementation that integrates with [Firecrawl](https://github.com/firecrawl/firecrawl) for web scraping capabilities.\n\n> Big thanks to [@vrknetha](https://github.com/vrknetha), [@knacklabs](https://www.knacklabs.ai) for the initial implementation!\n\n## Features\n\n- Web scraping, crawling, and discovery\n- Search and content extraction\n- Deep research and batch scraping\n- Automatic retries and rate limiting\n- Cloud and self-hosted support\n- SSE support\n\n> Play around with [our MCP Server on MCP.so's playground](https://mcp.so/playground?server=firecrawl-mcp-server) or on [Klavis AI](https://www.klavis.ai/mcp-servers).\n\n## Installation\n\n### Running with npx\n\n```bash\nenv FIRECRAWL_API_KEY=fc-YOUR_API_KEY npx -y firecrawl-mcp\n```\n\n### Manual Installation\n\n```bash\nnpm install -g firecrawl-mcp\n```\n\n### Running on Cursor\n\nConfiguring Cursor üñ•Ô∏è\nNote: Requires Cursor version 0.45.6+\nFor the most up-to-date configuration instructions, please refer to the official Cursor documentation on configuring MCP servers:\n[Cursor MCP Server Configuration Guide](https://docs.cursor.com/context/model-context-protocol#configuring-mcp-servers)\n\nTo configure Firecrawl MCP in Cursor **v0.48.6**\n\n1. Open Cursor Settings\n2. Go to Features > MCP Servers\n3. Click \"+ Add new global MCP server\"\n4. Enter the following code:\n   ```json\n   {\n     \"mcpServers\": {\n       \"firecrawl-mcp\": {\n         \"command\": \"npx\",\n         \"args\": [\"-y\", \"firecrawl-mcp\"],\n         \"env\": {\n           \"FIRECRAWL_API_KEY\": \"YOUR-API-KEY\"\n         }\n       }\n     }\n   }\n   ```\n\nTo configure Firecrawl MCP in Cursor **v0.45.6**\n\n1. Open Cursor Settings\n2. Go to Features > MCP Servers\n3. Click \"+ Add New MCP Server\"\n4. Enter the following:\n   - Name: \"firecrawl-mcp\" (or your preferred name)\n   - Type: \"command\"\n   - Command: `env FIRECRAWL_API_KEY=your-api-key npx -y firecrawl-mcp`\n\n> If you are using Windows and are running into issues, try `cmd /c \"set FIRECRAWL_API_KEY=your-api-key && npx -y firecrawl-mcp\"`\n\nReplace `your-api-key` with your Firecrawl API key. If you don't have one yet, you can create an account and get it from https://www.firecrawl.dev/app/api-keys\n\nAfter adding, refresh the MCP server list to see the new tools. The Composer Agent will automatically use Firecrawl MCP when appropriate, but you can explicitly request it by describing your web scraping needs. Access the Composer via Command+L (Mac), select \"Agent\" next to the submit button, and enter your query.\n\n### Running on Windsurf\n\nAdd this to your `./codeium/windsurf/model_config.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"mcp-server-firecrawl\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"firecrawl-mcp\"],\n      \"env\": {\n        \"FIRECRAWL_API_KEY\": \"YOUR_API_KEY\"\n      }\n    }\n  }\n}\n```\n\n### Running with Streamable HTTP Local Mode\n\nTo run the server using Streamable HTTP locally instead of the default stdio transport:\n\n```bash\nenv HTTP_STREAMABLE_SERVER=true FIRECRAWL_API_KEY=fc-YOUR_API_KEY npx -y firecrawl-mcp\n```\n\nUse the url: http://localhost:3000/mcp\n\n### Installing via Smithery (Legacy)\n\nTo install Firecrawl for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@mendableai/mcp-server-firecrawl):\n\n```bash\nnpx -y @smithery/cli install @mendableai/mcp-server-firecrawl --client claude\n```\n\n### Running on VS Code\n\nFor one-click installation, click one of the install buttons below...\n\n[![Install with NPX in VS Code](https://img.shields.io/badge/VS_Code-NPM-0098FF?style=flat-square&logo=visualstudiocode&logoColor=white)](https://insiders.vscode.dev/redirect/mcp/install?name=firecrawl&inputs=%5B%7B%22type%22%3A%22promptString%22%2C%22id%22%3A%22apiKey%22%2C%22description%22%3A%22Firecrawl%20API%20Key%22%2C%22password%22%3Atrue%7D%5D&config=%7B%22command%22%3A%22npx%22%2C%22args%22%3A%5B%22-y%22%2C%22firecrawl-mcp%22%5D%2C%22env%22%3A%7B%22FIRECRAWL_API_KEY%22%3A%22%24%7Binput%3AapiKey%7D%22%7D%7D) [![Install with NPX in VS Code Insiders](https://img.shields.io/badge/VS_Code_Insiders-NPM-24bfa5?style=flat-square&logo=visualstudiocode&logoColor=white)](https://insiders.vscode.dev/redirect/mcp/install?name=firecrawl&inputs=%5B%7B%22type%22%3A%22promptString%22%2C%22id%22%3A%22apiKey%22%2C%22description%22%3A%22Firecrawl%20API%20Key%22%2C%22password%22%3Atrue%7D%5D&config=%7B%22command%22%3A%22npx%22%2C%22args%22%3A%5B%22-y%22%2C%22firecrawl-mcp%22%5D%2C%22env%22%3A%7B%22FIRECRAWL_API_KEY%22%3A%22%24%7Binput%3AapiKey%7D%22%7D%7D&quality=insiders)\n\nFor manual installation, add the following JSON block to your User Settings (JSON) file in VS Code. You can do this by pressing `Ctrl + Shift + P` and typing `Preferences: Open User Settings (JSON)`.\n\n```json\n{\n  \"mcp\": {\n    \"inputs\": [\n      {\n        \"type\": \"promptString\",\n        \"id\": \"apiKey\",\n        \"description\": \"Firecrawl API Key\",\n        \"password\": true\n      }\n    ],\n    \"servers\": {\n      \"firecrawl\": {\n        \"command\": \"npx\",\n        \"args\": [\"-y\", \"firecrawl-mcp\"],\n        \"env\": {\n          \"FIRECRAWL_API_KEY\": \"${input:apiKey}\"\n        }\n      }\n    }\n  }\n}\n```\n\nOptionally, you can add it to a file called `.vscode/mcp.json` in your workspace. This will allow you to share the configuration with others:\n\n```json\n{\n  \"inputs\": [\n    {\n      \"type\": \"promptString\",\n      \"id\": \"apiKey\",\n      \"description\": \"Firecrawl API Key\",\n      \"password\": true\n    }\n  ],\n  \"servers\": {\n    \"firecrawl\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"firecrawl-mcp\"],\n      \"env\": {\n        \"FIRECRAWL_API_KEY\": \"${input:apiKey}\"\n      }\n    }\n  }\n}\n```\n\n## Configuration\n\n### Environment Variables\n\n#### Required for Cloud API\n\n- `FIRECRAWL_API_KEY`: Your Firecrawl API key\n  - Required when using cloud API (default)\n  - Optional when using self-hosted instance with `FIRECRAWL_API_URL`\n- `FIRECRAWL_API_URL` (Optional): Custom API endpoint for self-hosted instances\n  - Example: `https://firecrawl.your-domain.com`\n  - If not provided, the cloud API will be used (requires API key)\n\n#### Optional Configuration\n\n##### Retry Configuration\n\n- `FIRECRAWL_RETRY_MAX_ATTEMPTS`: Maximum number of retry attempts (default: 3)\n- `FIRECRAWL_RETRY_INITIAL_DELAY`: Initial delay in milliseconds before first retry (default: 1000)\n- `FIRECRAWL_RETRY_MAX_DELAY`: Maximum delay in milliseconds between retries (default: 10000)\n- `FIRECRAWL_RETRY_BACKOFF_FACTOR`: Exponential backoff multiplier (default: 2)\n\n##### Credit Usage Monitoring\n\n- `FIRECRAWL_CREDIT_WARNING_THRESHOLD`: Credit usage warning threshold (default: 1000)\n- `FIRECRAWL_CREDIT_CRITICAL_THRESHOLD`: Credit usage critical threshold (default: 100)\n\n### Configuration Examples\n\nFor cloud API usage with custom retry and credit monitoring:\n\n```bash\n# Required for cloud API\nexport FIRECRAWL_API_KEY=your-api-key\n\n# Optional retry configuration\nexport FIRECRAWL_RETRY_MAX_ATTEMPTS=5        # Increase max retry attempts\nexport FIRECRAWL_RETRY_INITIAL_DELAY=2000    # Start with 2s delay\nexport FIRECRAWL_RETRY_MAX_DELAY=30000       # Maximum 30s delay\nexport FIRECRAWL_RETRY_BACKOFF_FACTOR=3      # More aggressive backoff\n\n# Optional credit monitoring\nexport FIRECRAWL_CREDIT_WARNING_THRESHOLD=2000    # Warning at 2000 credits\nexport FIRECRAWL_CREDIT_CRITICAL_THRESHOLD=500    # Critical at 500 credits\n```\n\nFor self-hosted instance:\n\n```bash\n# Required for self-hosted\nexport FIRECRAWL_API_URL=https://firecrawl.your-domain.com\n\n# Optional authentication for self-hosted\nexport FIRECRAWL_API_KEY=your-api-key  # If your instance requires auth\n\n# Custom retry configuration\nexport FIRECRAWL_RETRY_MAX_ATTEMPTS=10\nexport FIRECRAWL_RETRY_INITIAL_DELAY=500     # Start with faster retries\n```\n\n### Usage with Claude Desktop\n\nAdd this to your `claude_desktop_config.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"mcp-server-firecrawl\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"firecrawl-mcp\"],\n      \"env\": {\n        \"FIRECRAWL_API_KEY\": \"YOUR_API_KEY_HERE\",\n\n        \"FIRECRAWL_RETRY_MAX_ATTEMPTS\": \"5\",\n        \"FIRECRAWL_RETRY_INITIAL_DELAY\": \"2000\",\n        \"FIRECRAWL_RETRY_MAX_DELAY\": \"30000\",\n        \"FIRECRAWL_RETRY_BACKOFF_FACTOR\": \"3\",\n\n        \"FIRECRAWL_CREDIT_WARNING_THRESHOLD\": \"2000\",\n        \"FIRECRAWL_CREDIT_CRITICAL_THRESHOLD\": \"500\"\n      }\n    }\n  }\n}\n```\n\n### System Configuration\n\nThe server includes several configurable parameters that can be set via environment variables. Here are the default values if not configured:\n\n```typescript\nconst CONFIG = {\n  retry: {\n    maxAttempts: 3, // Number of retry attempts for rate-limited requests\n    initialDelay: 1000, // Initial delay before first retry (in milliseconds)\n    maxDelay: 10000, // Maximum delay between retries (in milliseconds)\n    backoffFactor: 2, // Multiplier for exponential backoff\n  },\n  credit: {\n    warningThreshold: 1000, // Warn when credit usage reaches this level\n    criticalThreshold: 100, // Critical alert when credit usage reaches this level\n  },\n};\n```\n\nThese configurations control:\n\n1. **Retry Behavior**\n\n   - Automatically retries failed requests due to rate limits\n   - Uses exponential backoff to avoid overwhelming the API\n   - Example: With default settings, retries will be attempted at:\n     - 1st retry: 1 second delay\n     - 2nd retry: 2 seconds delay\n     - 3rd retry: 4 seconds delay (capped at maxDelay)\n\n2. **Credit Usage Monitoring**\n   - Tracks API credit consumption for cloud API usage\n   - Provides warnings at specified thresholds\n   - Helps prevent unexpected service interruption\n   - Example: With default settings:\n     - Warning at 1000 credits remaining\n     - Critical alert at 100 credits remaining\n\n### Rate Limiting and Batch Processing\n\nThe server utilizes Firecrawl's built-in rate limiting and batch processing capabilities:\n\n- Automatic rate limit handling with exponential backoff\n- Efficient parallel processing for batch operations\n- Smart request queuing and throttling\n- Automatic retries for transient errors\n\n## How to Choose a Tool\n\nUse this guide to select the right tool for your task:\n\n- **If you know the exact URL(s) you want:**\n  - For one: use **scrape**\n  - For many: use **batch_scrape**\n- **If you need to discover URLs on a site:** use **map**\n- **If you want to search the web for info:** use **search**\n- **If you want to extract structured data:** use **extract**\n- **If you want to analyze a whole site or section:** use **crawl** (with limits!)\n\n### Quick Reference Table\n\n| Tool         | Best for                            | Returns         |\n| ------------ | ----------------------------------- | --------------- |\n| scrape       | Single page content                 | markdown/html   |\n| batch_scrape | Multiple known URLs                 | markdown/html[] |\n| map          | Discovering URLs on a site          | URL[]           |\n| crawl        | Multi-page extraction (with limits) | markdown/html[] |\n| search       | Web search for info                 | results[]       |\n| extract      | Structured data from pages          | JSON            |\n\n## Available Tools\n\n### 1. Scrape Tool (`firecrawl_scrape`)\n\nScrape content from a single URL with advanced options.\n\n**Best for:**\n\n- Single page content extraction, when you know exactly which page contains the information.\n\n**Not recommended for:**\n\n- Extracting content from multiple pages (use batch_scrape for known URLs, or map + batch_scrape to discover URLs first, or crawl for full page content)\n- When you're unsure which page contains the information (use search)\n- When you need structured data (use extract)\n\n**Common mistakes:**\n\n- Using scrape for a list of URLs (use batch_scrape instead).\n\n**Prompt Example:**\n\n> \"Get the content of the page at https://example.com.\"\n\n**Usage Example:**\n\n```json\n{\n  \"name\": \"firecrawl_scrape\",\n  \"arguments\": {\n    \"url\": \"https://example.com\",\n    \"formats\": [\"markdown\"],\n    \"onlyMainContent\": true,\n    \"waitFor\": 1000,\n    \"timeout\": 30000,\n    \"mobile\": false,\n    \"includeTags\": [\"article\", \"main\"],\n    \"excludeTags\": [\"nav\", \"footer\"],\n    \"skipTlsVerification\": false\n  }\n}\n```\n\n**Returns:**\n\n- Markdown, HTML, or other formats as specified.\n\n### 2. Batch Scrape Tool (`firecrawl_batch_scrape`)\n\nScrape multiple URLs efficiently with built-in rate limiting and parallel processing.\n\n**Best for:**\n\n- Retrieving content from multiple pages, when you know exactly which pages to scrape.\n\n**Not recommended for:**\n\n- Discovering URLs (use map first if you don't know the URLs)\n- Scraping a single page (use scrape)\n\n**Common mistakes:**\n\n- Using batch_scrape with too many URLs at once (may hit rate limits or token overflow)\n\n**Prompt Example:**\n\n> \"Get the content of these three blog posts: [url1, url2, url3].\"\n\n**Usage Example:**\n\n```json\n{\n  \"name\": \"firecrawl_batch_scrape\",\n  \"arguments\": {\n    \"urls\": [\"https://example1.com\", \"https://example2.com\"],\n    \"options\": {\n      \"formats\": [\"markdown\"],\n      \"onlyMainContent\": true\n    }\n  }\n}\n```\n\n**Returns:**\n\n- Response includes operation ID for status checking:\n\n```json\n{\n  \"content\": [\n    {\n      \"type\": \"text\",\n      \"text\": \"Batch operation queued with ID: batch_1. Use firecrawl_check_batch_status to check progress.\"\n    }\n  ],\n  \"isError\": false\n}\n```\n\n### 3. Check Batch Status (`firecrawl_check_batch_status`)\n\nCheck the status of a batch operation.\n\n```json\n{\n  \"name\": \"firecrawl_check_batch_status\",\n  \"arguments\": {\n    \"id\": \"batch_1\"\n  }\n}\n```\n\n### 4. Map Tool (`firecrawl_map`)\n\nMap a website to discover all indexed URLs on the site.\n\n**Best for:**\n\n- Discovering URLs on a website before deciding what to scrape\n- Finding specific sections of a website\n\n**Not recommended for:**\n\n- When you already know which specific URL you need (use scrape or batch_scrape)\n- When you need the content of the pages (use scrape after mapping)\n\n**Common mistakes:**\n\n- Using crawl to discover URLs instead of map\n\n**Prompt Example:**\n\n> \"List all URLs on example.com.\"\n\n**Usage Example:**\n\n```json\n{\n  \"name\": \"firecrawl_map\",\n  \"arguments\": {\n    \"url\": \"https://example.com\"\n  }\n}\n```\n\n**Returns:**\n\n- Array of URLs found on the site\n\n### 5. Search Tool (`firecrawl_search`)\n\nSearch the web and optionally extract content from search results.\n\n**Best for:**\n\n- Finding specific information across multiple websites, when you don't know which website has the information.\n- When you need the most relevant content for a query\n\n**Not recommended for:**\n\n- When you already know which website to scrape (use scrape)\n- When you need comprehensive coverage of a single website (use map or crawl)\n\n**Common mistakes:**\n\n- Using crawl or map for open-ended questions (use search instead)\n\n**Usage Example:**\n\n```json\n{\n  \"name\": \"firecrawl_search\",\n  \"arguments\": {\n    \"query\": \"latest AI research papers 2023\",\n    \"limit\": 5,\n    \"lang\": \"en\",\n    \"country\": \"us\",\n    \"scrapeOptions\": {\n      \"formats\": [\"markdown\"],\n      \"onlyMainContent\": true\n    }\n  }\n}\n```\n\n**Returns:**\n\n- Array of search results (with optional scraped content)\n\n**Prompt Example:**\n\n> \"Find the latest research papers on AI published in 2023.\"\n\n### 6. Crawl Tool (`firecrawl_crawl`)\n\nStarts an asynchronous crawl job on a website and extract content from all pages.\n\n**Best for:**\n\n- Extracting content from multiple related pages, when you need comprehensive coverage.\n\n**Not recommended for:**\n\n- Extracting content from a single page (use scrape)\n- When token limits are a concern (use map + batch_scrape)\n- When you need fast results (crawling can be slow)\n\n**Warning:** Crawl responses can be very large and may exceed token limits. Limit the crawl depth and number of pages, or use map + batch_scrape for better control.\n\n**Common mistakes:**\n\n- Setting limit or maxDepth too high (causes token overflow)\n- Using crawl for a single page (use scrape instead)\n\n**Prompt Example:**\n\n> \"Get all blog posts from the first two levels of example.com/blog.\"\n\n**Usage Example:**\n\n```json\n{\n  \"name\": \"firecrawl_crawl\",\n  \"arguments\": {\n    \"url\": \"https://example.com/blog/*\",\n    \"maxDepth\": 2,\n    \"limit\": 100,\n    \"allowExternalLinks\": false,\n    \"deduplicateSimilarURLs\": true\n  }\n}\n```\n\n**Returns:**\n\n- Response includes operation ID for status checking:\n\n```json\n{\n  \"content\": [\n    {\n      \"type\": \"text\",\n      \"text\": \"Started crawl for: https://example.com/* with job ID: 550e8400-e29b-41d4-a716-446655440000. Use firecrawl_check_crawl_status to check progress.\"\n    }\n  ],\n  \"isError\": false\n}\n```\n\n### 7. Check Crawl Status (`firecrawl_check_crawl_status`)\n\nCheck the status of a crawl job.\n\n```json\n{\n  \"name\": \"firecrawl_check_crawl_status\",\n  \"arguments\": {\n    \"id\": \"550e8400-e29b-41d4-a716-446655440000\"\n  }\n}\n```\n\n**Returns:**\n\n- Response includes the status of the crawl job:\n\n### 8. Extract Tool (`firecrawl_extract`)\n\nExtract structured information from web pages using LLM capabilities. Supports both cloud AI and self-hosted LLM extraction.\n\n**Best for:**\n\n- Extracting specific structured data like prices, names, details.\n\n**Not recommended for:**\n\n- When you need the full content of a page (use scrape)\n- When you're not looking for specific structured data\n\n**Arguments:**\n\n- `urls`: Array of URLs to extract information from\n- `prompt`: Custom prompt for the LLM extraction\n- `systemPrompt`: System prompt to guide the LLM\n- `schema`: JSON schema for structured data extraction\n- `allowExternalLinks`: Allow extraction from external links\n- `enableWebSearch`: Enable web search for additional context\n- `includeSubdomains`: Include subdomains in extraction\n\nWhen using a self-hosted instance, the extraction will use your configured LLM. For cloud API, it uses Firecrawl's managed LLM service.\n**Prompt Example:**\n\n> \"Extract the product name, price, and description from these product pages.\"\n\n**Usage Example:**\n\n```json\n{\n  \"name\": \"firecrawl_extract\",\n  \"arguments\": {\n    \"urls\": [\"https://example.com/page1\", \"https://example.com/page2\"],\n    \"prompt\": \"Extract product information including name, price, and description\",\n    \"systemPrompt\": \"You are a helpful assistant that extracts product information\",\n    \"schema\": {\n      \"type\": \"object\",\n      \"properties\": {\n        \"name\": { \"type\": \"string\" },\n        \"price\": { \"type\": \"number\" },\n        \"description\": { \"type\": \"string\" }\n      },\n      \"required\": [\"name\", \"price\"]\n    },\n    \"allowExternalLinks\": false,\n    \"enableWebSearch\": false,\n    \"includeSubdomains\": false\n  }\n}\n```\n\n**Returns:**\n\n- Extracted structured data as defined by your schema\n\n```json\n{\n  \"content\": [\n    {\n      \"type\": \"text\",\n      \"text\": {\n        \"name\": \"Example Product\",\n        \"price\": 99.99,\n        \"description\": \"This is an example product description\"\n      }\n    }\n  ],\n  \"isError\": false\n}\n```\n\n## Logging System\n\nThe server includes comprehensive logging:\n\n- Operation status and progress\n- Performance metrics\n- Credit usage monitoring\n- Rate limit tracking\n- Error conditions\n\nExample log messages:\n\n```\n[INFO] Firecrawl MCP Server initialized successfully\n[INFO] Starting scrape for URL: https://example.com\n[INFO] Batch operation queued with ID: batch_1\n[WARNING] Credit usage has reached warning threshold\n[ERROR] Rate limit exceeded, retrying in 2s...\n```\n\n## Error Handling\n\nThe server provides robust error handling:\n\n- Automatic retries for transient errors\n- Rate limit handling with backoff\n- Detailed error messages\n- Credit usage warnings\n- Network resilience\n\nExample error response:\n\n```json\n{\n  \"content\": [\n    {\n      \"type\": \"text\",\n      \"text\": \"Error: Rate limit exceeded. Retrying in 2 seconds...\"\n    }\n  ],\n  \"isError\": true\n}\n```\n\n## Development\n\n```bash\n# Install dependencies\nnpm install\n\n# Build\nnpm run build\n\n# Run tests\nnpm test\n```\n\n### Contributing\n\n1. Fork the repository\n2. Create your feature branch\n3. Run tests: `npm test`\n4. Submit a pull request\n\n### Thanks to contributors\n\nThanks to [@vrknetha](https://github.com/vrknetha), [@cawstudios](https://caw.tech) for the initial implementation!\n\nThanks to MCP.so and Klavis AI for hosting and [@gstarwd](https://github.com/gstarwd), [@xiangkaiz](https://github.com/xiangkaiz) and [@zihaolin96](https://github.com/zihaolin96) for integrating our server.\n\n## License\n\nMIT License - see LICENSE file for details\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "firecrawl",
        "scraping",
        "mcp",
        "servers firecrawl",
        "firecrawl mcp",
        "integrates firecrawl"
      ],
      "category": "official-servers"
    },
    "github--github-mcp-server": {
      "owner": "github",
      "name": "github-mcp-server",
      "url": "https://github.com/github/github-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/github.webp",
      "description": "Interact with GitHub repositories, manage issues, and automate workflows using GitHub's APIs for seamless integration and advanced functionality.",
      "stars": 23200,
      "forks": 2658,
      "license": "MIT License",
      "language": "Go",
      "updated_at": "2025-10-02T08:27:57Z",
      "readme_content": "# GitHub MCP Server\n\nThe GitHub MCP Server connects AI tools directly to GitHub's platform. This gives AI agents, assistants, and chatbots the ability to read repositories and code files, manage issues and PRs, analyze code, and automate workflows. All through natural language interactions.\n\n### Use Cases\n\n- Repository Management: Browse and query code, search files, analyze commits, and understand project structure across any repository you have access to.\n- Issue & PR Automation: Create, update, and manage issues and pull requests. Let AI help triage bugs, review code changes, and maintain project boards.\n- CI/CD & Workflow Intelligence: Monitor GitHub Actions workflow runs, analyze build failures, manage releases, and get insights into your development pipeline.\n- Code Analysis: Examine security findings, review Dependabot alerts, understand code patterns, and get comprehensive insights into your codebase.\n- Team Collaboration: Access discussions, manage notifications, analyze team activity, and streamline processes for your team.\n\nBuilt for developers who want to connect their AI tools to GitHub context and capabilities, from simple natural language queries to complex multi-step agent workflows.\n\n---\n\n## Remote GitHub MCP Server\n\n[![Install in VS Code](https://img.shields.io/badge/VS_Code-Install_Server-0098FF?style=flat-square&logo=visualstudiocode&logoColor=white)](https://insiders.vscode.dev/redirect/mcp/install?name=github&config=%7B%22type%22%3A%20%22http%22%2C%22url%22%3A%20%22https%3A%2F%2Fapi.githubcopilot.com%2Fmcp%2F%22%7D) [![Install in VS Code Insiders](https://img.shields.io/badge/VS_Code_Insiders-Install_Server-24bfa5?style=flat-square&logo=visualstudiocode&logoColor=white)](https://insiders.vscode.dev/redirect/mcp/install?name=github&config=%7B%22type%22%3A%20%22http%22%2C%22url%22%3A%20%22https%3A%2F%2Fapi.githubcopilot.com%2Fmcp%2F%22%7D&quality=insiders)\n\nThe remote GitHub MCP Server is hosted by GitHub and provides the easiest method for getting up and running. If your MCP host does not support remote MCP servers, don't worry! You can use the [local version of the GitHub MCP Server](https://github.com/github/github-mcp-server?tab=readme-ov-file#local-github-mcp-server) instead.\n\n### Prerequisites\n\n1. A compatible MCP host with remote server support (VS Code 1.101+, Claude Desktop, Cursor, Windsurf, etc.)\n2. Any applicable [policies enabled](https://github.com/github/github-mcp-server/blob/main/docs/policies-and-governance.md)\n\n### Install in VS Code\n\nFor quick installation, use one of the one-click install buttons above. Once you complete that flow, toggle Agent mode (located by the Copilot Chat text input) and the server will start. Make sure you're using [VS Code 1.101](https://code.visualstudio.com/updates/v1_101) or [later](https://code.visualstudio.com/updates) for remote MCP and OAuth support.\n\nAlternatively, to manually configure VS Code, choose the appropriate JSON block from the examples below and add it to your host configuration:\n\n<table>\n<tr><th>Using OAuth</th><th>Using a GitHub PAT</th></tr>\n<tr><th align=left colspan=2>VS Code (version 1.101 or greater)</th></tr>\n<tr valign=top>\n<td>\n\n```json\n{\n  \"servers\": {\n    \"github\": {\n      \"type\": \"http\",\n      \"url\": \"https://api.githubcopilot.com/mcp/\"\n    }\n  }\n}\n```\n\n</td>\n<td>\n\n```json\n{\n  \"servers\": {\n    \"github\": {\n      \"type\": \"http\",\n      \"url\": \"https://api.githubcopilot.com/mcp/\",\n      \"headers\": {\n        \"Authorization\": \"Bearer ${input:github_mcp_pat}\"\n      }\n    }\n  },\n  \"inputs\": [\n    {\n      \"type\": \"promptString\",\n      \"id\": \"github_mcp_pat\",\n      \"description\": \"GitHub Personal Access Token\",\n      \"password\": true\n    }\n  ]\n}\n```\n\n</td>\n</tr>\n</table>\n\n### Install in other MCP hosts\n- **[GitHub Copilot in other IDEs](/docs/installation-guides/install-other-copilot-ides.md)** - Installation for JetBrains, Visual Studio, Eclipse, and Xcode with GitHub Copilot\n- **[Claude Applications](/docs/installation-guides/install-claude.md)** - Installation guide for Claude Web, Claude Desktop and Claude Code CLI\n- **[Cursor](/docs/installation-guides/install-cursor.md)** - Installation guide for Cursor IDE\n- **[Windsurf](/docs/installation-guides/install-windsurf.md)** - Installation guide for Windsurf IDE\n\n> **Note:** Each MCP host application needs to configure a GitHub App or OAuth App to support remote access via OAuth. Any host application that supports remote MCP servers should support the remote GitHub server with PAT authentication. Configuration details and support levels vary by host. Make sure to refer to the host application's documentation for more info.\n\n### Configuration\nSee [Remote Server Documentation](/docs/remote-server.md) on how to pass additional configuration settings to the remote GitHub MCP Server.\n\n---\n\n## Local GitHub MCP Server\n\n[![Install with Docker in VS Code](https://img.shields.io/badge/VS_Code-Install_Server-0098FF?style=flat-square&logo=visualstudiocode&logoColor=white)](https://insiders.vscode.dev/redirect/mcp/install?name=github&inputs=%5B%7B%22id%22%3A%22github_token%22%2C%22type%22%3A%22promptString%22%2C%22description%22%3A%22GitHub%20Personal%20Access%20Token%22%2C%22password%22%3Atrue%7D%5D&config=%7B%22command%22%3A%22docker%22%2C%22args%22%3A%5B%22run%22%2C%22-i%22%2C%22--rm%22%2C%22-e%22%2C%22GITHUB_PERSONAL_ACCESS_TOKEN%22%2C%22ghcr.io%2Fgithub%2Fgithub-mcp-server%22%5D%2C%22env%22%3A%7B%22GITHUB_PERSONAL_ACCESS_TOKEN%22%3A%22%24%7Binput%3Agithub_token%7D%22%7D%7D) [![Install with Docker in VS Code Insiders](https://img.shields.io/badge/VS_Code_Insiders-Install_Server-24bfa5?style=flat-square&logo=visualstudiocode&logoColor=white)](https://insiders.vscode.dev/redirect/mcp/install?name=github&inputs=%5B%7B%22id%22%3A%22github_token%22%2C%22type%22%3A%22promptString%22%2C%22description%22%3A%22GitHub%20Personal%20Access%20Token%22%2C%22password%22%3Atrue%7D%5D&config=%7B%22command%22%3A%22docker%22%2C%22args%22%3A%5B%22run%22%2C%22-i%22%2C%22--rm%22%2C%22-e%22%2C%22GITHUB_PERSONAL_ACCESS_TOKEN%22%2C%22ghcr.io%2Fgithub%2Fgithub-mcp-server%22%5D%2C%22env%22%3A%7B%22GITHUB_PERSONAL_ACCESS_TOKEN%22%3A%22%24%7Binput%3Agithub_token%7D%22%7D%7D&quality=insiders)\n\n### Prerequisites\n\n1. To run the server in a container, you will need to have [Docker](https://www.docker.com/) installed.\n2. Once Docker is installed, you will also need to ensure Docker is running. The image is public; if you get errors on pull, you may have an expired token and need to `docker logout ghcr.io`.\n3. Lastly you will need to [Create a GitHub Personal Access Token](https://github.com/settings/personal-access-tokens/new).\nThe MCP server can use many of the GitHub APIs, so enable the permissions that you feel comfortable granting your AI tools (to learn more about access tokens, please check out the [documentation](https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/managing-your-personal-access-tokens)).\n\n<details><summary><b>Handling PATs Securely</b></summary>\n\n### Environment Variables (Recommended)\nTo keep your GitHub PAT secure and reusable across different MCP hosts:\n\n1. **Store your PAT in environment variables**\n   ```bash\n   export GITHUB_PAT=your_token_here\n   ```\n   Or create a `.env` file:\n   ```env\n   GITHUB_PAT=your_token_here\n   ```\n\n2. **Protect your `.env` file**\n   ```bash\n   # Add to .gitignore to prevent accidental commits\n   echo \".env\" >> .gitignore\n   ```\n\n3. **Reference the token in configurations**\n   ```bash\n   # CLI usage\n   claude mcp update github -e GITHUB_PERSONAL_ACCESS_TOKEN=$GITHUB_PAT\n\n   # In config files (where supported)\n   \"env\": {\n     \"GITHUB_PERSONAL_ACCESS_TOKEN\": \"$GITHUB_PAT\"\n   }\n   ```\n\n> **Note**: Environment variable support varies by host app and IDE. Some applications (like Windsurf) require hardcoded tokens in config files.\n\n### Token Security Best Practices\n\n- **Minimum scopes**: Only grant necessary permissions\n  - `repo` - Repository operations\n  - `read:packages` - Docker image access\n  - `read:org` - Organization team access\n- **Separate tokens**: Use different PATs for different projects/environments\n- **Regular rotation**: Update tokens periodically\n- **Never commit**: Keep tokens out of version control\n- **File permissions**: Restrict access to config files containing tokens\n  ```bash\n  chmod 600 ~/.your-app/config.json\n  ```\n\n</details>\n\n## Installation\n\n### Install in GitHub Copilot on VS Code\n\nFor quick installation, use one of the one-click install buttons above. Once you complete that flow, toggle Agent mode (located by the Copilot Chat text input) and the server will start.\n\nMore about using MCP server tools in VS Code's [agent mode documentation](https://code.visualstudio.com/docs/copilot/chat/mcp-servers).\n\nInstall in GitHub Copilot on other IDEs (JetBrains, Visual Studio, Eclipse, etc.)\n\nAdd the following JSON block to your IDE's MCP settings.\n\n```json\n{\n  \"mcp\": {\n    \"inputs\": [\n      {\n        \"type\": \"promptString\",\n        \"id\": \"github_token\",\n        \"description\": \"GitHub Personal Access Token\",\n        \"password\": true\n      }\n    ],\n    \"servers\": {\n      \"github\": {\n        \"command\": \"docker\",\n        \"args\": [\n          \"run\",\n          \"-i\",\n          \"--rm\",\n          \"-e\",\n          \"GITHUB_PERSONAL_ACCESS_TOKEN\",\n          \"ghcr.io/github/github-mcp-server\"\n        ],\n        \"env\": {\n          \"GITHUB_PERSONAL_ACCESS_TOKEN\": \"${input:github_token}\"\n        }\n      }\n    }\n  }\n}\n```\n\nOptionally, you can add a similar example (i.e. without the mcp key) to a file called `.vscode/mcp.json` in your workspace. This will allow you to share the configuration with other host applications that accept the same format.\n\n<details>\n<summary><b>Example JSON block without the MCP key included</b></summary>\n<br>\n\n```json\n{\n  \"inputs\": [\n    {\n      \"type\": \"promptString\",\n      \"id\": \"github_token\",\n      \"description\": \"GitHub Personal Access Token\",\n      \"password\": true\n    }\n  ],\n  \"servers\": {\n    \"github\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"-i\",\n        \"--rm\",\n        \"-e\",\n        \"GITHUB_PERSONAL_ACCESS_TOKEN\",\n        \"ghcr.io/github/github-mcp-server\"\n      ],\n      \"env\": {\n        \"GITHUB_PERSONAL_ACCESS_TOKEN\": \"${input:github_token}\"\n      }\n    }\n  }\n}\n```\n\n</details>\n\n### Install in Other MCP Hosts\n\nFor other MCP host applications, please refer to our installation guides:\n\n- **[GitHub Copilot in other IDEs](/docs/installation-guides/install-other-copilot-ides.md)** - Installation for JetBrains, Visual Studio, Eclipse, and Xcode with GitHub Copilot\n- **[Claude Code & Claude Desktop](docs/installation-guides/install-claude.md)** - Installation guide for Claude Code and Claude Desktop\n- **[Cursor](docs/installation-guides/install-cursor.md)** - Installation guide for Cursor IDE\n- **[Google Gemini CLI](docs/installation-guides/install-gemini-cli.md)** - Installation guide for Google Gemini CLI\n- **[Windsurf](docs/installation-guides/install-windsurf.md)** - Installation guide for Windsurf IDE\n\nFor a complete overview of all installation options, see our **[Installation Guides Index](docs/installation-guides)**.\n\n> **Note:** Any host application that supports local MCP servers should be able to access the local GitHub MCP server. However, the specific configuration process, syntax and stability of the integration will vary by host application. While many may follow a similar format to the examples above, this is not guaranteed. Please refer to your host application's documentation for the correct MCP configuration syntax and setup process.\n\n### Build from source\n\nIf you don't have Docker, you can use `go build` to build the binary in the\n`cmd/github-mcp-server` directory, and use the `github-mcp-server stdio` command with the `GITHUB_PERSONAL_ACCESS_TOKEN` environment variable set to your token. To specify the output location of the build, use the `-o` flag. You should configure your server to use the built executable as its `command`. For example:\n\n```JSON\n{\n  \"mcp\": {\n    \"servers\": {\n      \"github\": {\n        \"command\": \"/path/to/github-mcp-server\",\n        \"args\": [\"stdio\"],\n        \"env\": {\n          \"GITHUB_PERSONAL_ACCESS_TOKEN\": \"<YOUR_TOKEN>\"\n        }\n      }\n    }\n  }\n}\n```\n\n## Tool Configuration\n\nThe GitHub MCP Server supports enabling or disabling specific groups of functionalities via the `--toolsets` flag. This allows you to control which GitHub API capabilities are available to your AI tools. Enabling only the toolsets that you need can help the LLM with tool choice and reduce the context size.\n\n_Toolsets are not limited to Tools. Relevant MCP Resources and Prompts are also included where applicable._\n\n### Available Toolsets\n\nThe following sets of tools are available (all are on by default):\n\n<!-- START AUTOMATED TOOLSETS -->\n| Toolset                 | Description                                                   |\n| ----------------------- | ------------------------------------------------------------- |\n| `context`               | **Strongly recommended**: Tools that provide context about the current user and GitHub context you are operating in |\n| `actions` | GitHub Actions workflows and CI/CD operations |\n| `code_security` | Code security related tools, such as GitHub Code Scanning |\n| `dependabot` | Dependabot tools |\n| `discussions` | GitHub Discussions related tools |\n| `experiments` | Experimental features that are not considered stable yet |\n| `gists` | GitHub Gist related tools |\n| `issues` | GitHub Issues related tools |\n| `notifications` | GitHub Notifications related tools |\n| `orgs` | GitHub Organization related tools |\n| `projects` | GitHub Projects related tools |\n| `pull_requests` | GitHub Pull Request related tools |\n| `repos` | GitHub Repository related tools |\n| `secret_protection` | Secret protection related tools, such as GitHub Secret Scanning |\n| `security_advisories` | Security advisories related tools |\n| `users` | GitHub User related tools |\n<!-- END AUTOMATED TOOLSETS -->\n\n## Tools\n\n\n<!-- START AUTOMATED TOOLS -->\n<details>\n\n<summary>Actions</summary>\n\n- **cancel_workflow_run** - Cancel workflow run\n  - `owner`: Repository owner (string, required)\n  - `repo`: Repository name (string, required)\n  - `run_id`: The unique identifier of the workflow run (number, required)\n\n- **delete_workflow_run_logs** - Delete workflow logs\n  - `owner`: Repository owner (string, required)\n  - `repo`: Repository name (string, required)\n  - `run_id`: The unique identifier of the workflow run (number, required)\n\n- **download_workflow_run_artifact** - Download workflow artifact\n  - `artifact_id`: The unique identifier of the artifact (number, required)\n  - `owner`: Repository owner (string, required)\n  - `repo`: Repository name (string, required)\n\n- **get_job_logs** - Get job logs\n  - `failed_only`: When true, gets logs for all failed jobs in run_id (boolean, optional)\n  - `job_id`: The unique identifier of the workflow job (required for single job logs) (number, optional)\n  - `owner`: Repository owner (string, required)\n  - `repo`: Repository name (string, required)\n  - `return_content`: Returns actual log content instead of URLs (boolean, optional)\n  - `run_id`: Workflow run ID (required when using failed_only) (number, optional)\n  - `tail_lines`: Number of lines to return from the end of the log (number, optional)\n\n- **get_workflow_run** - Get workflow run\n  - `owner`: Repository owner (string, required)\n  - `repo`: Repository name (string, required)\n  - `run_id`: The unique identifier of the workflow run (number, required)\n\n- **get_workflow_run_logs** - Get workflow run logs\n  - `owner`: Repository owner (string, required)\n  - `repo`: Repository name (string, required)\n  - `run_id`: The unique identifier of the workflow run (number, required)\n\n- **get_workflow_run_usage** - Get workflow usage\n  - `owner`: Repository owner (string, required)\n  - `repo`: Repository name (string, required)\n  - `run_id`: The unique identifier of the workflow run (number, required)\n\n- **list_workflow_jobs** - List workflow jobs\n  - `filter`: Filters jobs by their completed_at timestamp (string, optional)\n  - `owner`: Repository owner (string, required)\n  - `page`: Page number for pagination (min 1) (number, optional)\n  - `perPage`: Results per page for pagination (min 1, max 100) (number, optional)\n  - `repo`: Repository name (string, required)\n  - `run_id`: The unique identifier of the workflow run (number, required)\n\n- **list_workflow_run_artifacts** - List workflow artifacts\n  - `owner`: Repository owner (string, required)\n  - `page`: Page number for pagination (min 1) (number, optional)\n  - `perPage`: Results per page for pagination (min 1, max 100) (number, optional)\n  - `repo`: Repository name (string, required)\n  - `run_id`: The unique identifier of the workflow run (number, required)\n\n- **list_workflow_runs** - List workflow runs\n  - `actor`: Returns someone's workflow runs. Use the login for the user who created the workflow run. (string, optional)\n  - `branch`: Returns workflow runs associated with a branch. Use the name of the branch. (string, optional)\n  - `event`: Returns workflow runs for a specific event type (string, optional)\n  - `owner`: Repository owner (string, required)\n  - `page`: Page number for pagination (min 1) (number, optional)\n  - `perPage`: Results per page for pagination (min 1, max 100) (number, optional)\n  - `repo`: Repository name (string, required)\n  - `status`: Returns workflow runs with the check run status (string, optional)\n  - `workflow_id`: The workflow ID or workflow file name (string, required)\n\n- **list_workflows** - List workflows\n  - `owner`: Repository owner (string, required)\n  - `page`: Page number for pagination (min 1) (number, optional)\n  - `perPage`: Results per page for pagination (min 1, max 100) (number, optional)\n  - `repo`: Repository name (string, required)\n\n- **rerun_failed_jobs** - Rerun failed jobs\n  - `owner`: Repository owner (string, required)\n  - `repo`: Repository name (string, required)\n  - `run_id`: The unique identifier of the workflow run (number, required)\n\n- **rerun_workflow_run** - Rerun workflow run\n  - `owner`: Repository owner (string, required)\n  - `repo`: Repository name (string, required)\n  - `run_id`: The unique identifier of the workflow run (number, required)\n\n- **run_workflow** - Run workflow\n  - `inputs`: Inputs the workflow accepts (object, optional)\n  - `owner`: Repository owner (string, required)\n  - `ref`: The git reference for the workflow. The reference can be a branch or tag name. (string, required)\n  - `repo`: Repository name (string, required)\n  - `workflow_id`: The workflow ID (numeric) or workflow file name (e.g., main.yml, ci.yaml) (string, required)\n\n</details>\n\n<details>\n\n<summary>Code Security</summary>\n\n- **get_code_scanning_alert** - Get code scanning alert\n  - `alertNumber`: The number of the alert. (number, required)\n  - `owner`: The owner of the repository. (string, required)\n  - `repo`: The name of the repository. (string, required)\n\n- **list_code_scanning_alerts** - List code scanning alerts\n  - `owner`: The owner of the repository. (string, required)\n  - `ref`: The Git reference for the results you want to list. (string, optional)\n  - `repo`: The name of the repository. (string, required)\n  - `severity`: Filter code scanning alerts by severity (string, optional)\n  - `state`: Filter code scanning alerts by state. Defaults to open (string, optional)\n  - `tool_name`: The name of the tool used for code scanning. (string, optional)\n\n</details>\n\n<details>\n\n<summary>Context</summary>\n\n- **get_me** - Get my user profile\n  - No parameters required\n\n- **get_team_members** - Get team members\n  - `org`: Organization login (owner) that contains the team. (string, required)\n  - `team_slug`: Team slug (string, required)\n\n- **get_teams** - Get teams\n  - `user`: Username to get teams for. If not provided, uses the authenticated user. (string, optional)\n\n</details>\n\n<details>\n\n<summary>Dependabot</summary>\n\n- **get_dependabot_alert** - Get dependabot alert\n  - `alertNumber`: The number of the alert. (number, required)\n  - `owner`: The owner of the repository. (string, required)\n  - `repo`: The name of the repository. (string, required)\n\n- **list_dependabot_alerts** - List dependabot alerts\n  - `owner`: The owner of the repository. (string, required)\n  - `repo`: The name of the repository. (string, required)\n  - `severity`: Filter dependabot alerts by severity (string, optional)\n  - `state`: Filter dependabot alerts by state. Defaults to open (string, optional)\n\n</details>\n\n<details>\n\n<summary>Discussions</summary>\n\n- **get_discussion** - Get discussion\n  - `discussionNumber`: Discussion Number (number, required)\n  - `owner`: Repository owner (string, required)\n  - `repo`: Repository name (string, required)\n\n- **get_discussion_comments** - Get discussion comments\n  - `after`: Cursor for pagination. Use the endCursor from the previous page's PageInfo for GraphQL APIs. (string, optional)\n  - `discussionNumber`: Discussion Number (number, required)\n  - `owner`: Repository owner (string, required)\n  - `perPage`: Results per page for pagination (min 1, max 100) (number, optional)\n  - `repo`: Repository name (string, required)\n\n- **list_discussion_categories** - List discussion categories\n  - `owner`: Repository owner (string, required)\n  - `repo`: Repository name. If not provided, discussion categories will be queried at the organisation level. (string, optional)\n\n- **list_discussions** - List discussions\n  - `after`: Cursor for pagination. Use the endCursor from the previous page's PageInfo for GraphQL APIs. (string, optional)\n  - `category`: Optional filter by discussion category ID. If provided, only discussions with this category are listed. (string, optional)\n  - `direction`: Order direction. (string, optional)\n  - `orderBy`: Order discussions by field. If provided, the 'direction' also needs to be provided. (string, optional)\n  - `owner`: Repository owner (string, required)\n  - `perPage`: Results per page for pagination (min 1, max 100) (number, optional)\n  - `repo`: Repository name. If not provided, discussions will be queried at the organisation level. (string, optional)\n\n</details>\n\n<details>\n\n<summary>Gists</summary>\n\n- **create_gist** - Create Gist\n  - `content`: Content for simple single-file gist creation (string, required)\n  - `description`: Description of the gist (string, optional)\n  - `filename`: Filename for simple single-file gist creation (string, required)\n  - `public`: Whether the gist is public (boolean, optional)\n\n- **list_gists** - List Gists\n  - `page`: Page number for pagination (min 1) (number, optional)\n  - `perPage`: Results per page for pagination (min 1, max 100) (number, optional)\n  - `since`: Only gists updated after this time (ISO 8601 timestamp) (string, optional)\n  - `username`: GitHub username (omit for authenticated user's gists) (string, optional)\n\n- **update_gist** - Update Gist\n  - `content`: Content for the file (string, required)\n  - `description`: Updated description of the gist (string, optional)\n  - `filename`: Filename to update or create (string, required)\n  - `gist_id`: ID of the gist to update (string, required)\n\n</details>\n\n<details>\n\n<summary>Issues</summary>\n\n- **add_issue_comment** - Add comment to issue\n  - `body`: Comment content (string, required)\n  - `issue_number`: Issue number to comment on (number, required)\n  - `owner`: Repository owner (string, required)\n  - `repo`: Repository name (string, required)\n\n- **add_sub_issue** - Add sub-issue\n  - `issue_number`: The number of the parent issue (number, required)\n  - `owner`: Repository owner (string, required)\n  - `replace_parent`: When true, replaces the sub-issue's current parent issue (boolean, optional)\n  - `repo`: Repository name (string, required)\n  - `sub_issue_id`: The ID of the sub-issue to add. ID is not the same as issue number (number, required)\n\n- **assign_copilot_to_issue** - Assign Copilot to issue\n  - `issueNumber`: Issue number (number, required)\n  - `owner`: Repository owner (string, required)\n  - `repo`: Repository name (string, required)\n\n- **create_issue** - Open new issue\n  - `assignees`: Usernames to assign to this issue (string[], optional)\n  - `body`: Issue body content (string, optional)\n  - `labels`: Labels to apply to this issue (string[], optional)\n  - `milestone`: Milestone number (number, optional)\n  - `owner`: Repository owner (string, required)\n  - `repo`: Repository name (string, required)\n  - `title`: Issue title (string, required)\n  - `type`: Type of this issue (string, optional)\n\n- **get_issue** - Get issue details\n  - `issue_number`: The number of the issue (number, required)\n  - `owner`: The owner of the repository (string, required)\n  - `repo`: The name of the repository (string, required)\n\n- **get_issue_comments** - Get issue comments\n  - `issue_number`: Issue number (number, required)\n  - `owner`: Repository owner (string, required)\n  - `page`: Page number for pagination (min 1) (number, optional)\n  - `perPage`: Results per page for pagination (min 1, max 100) (number, optional)\n  - `repo`: Repository name (string, required)\n\n- **list_issue_types** - List available issue types\n  - `owner`: The organization owner of the repository (string, required)\n\n- **list_issues** - List issues\n  - `after`: Cursor for pagination. Use the endCursor from the previous page's PageInfo for GraphQL APIs. (string, optional)\n  - `direction`: Order direction. If provided, the 'orderBy' also needs to be provided. (string, optional)\n  - `labels`: Filter by labels (string[], optional)\n  - `orderBy`: Order issues by field. If provided, the 'direction' also needs to be provided. (string, optional)\n  - `owner`: Repository owner (string, required)\n  - `perPage`: Results per page for pagination (min 1, max 100) (number, optional)\n  - `repo`: Repository name (string, required)\n  - `since`: Filter by date (ISO 8601 timestamp) (string, optional)\n  - `state`: Filter by state, by default both open and closed issues are returned when not provided (string, optional)\n\n- **list_sub_issues** - List sub-issues\n  - `issue_number`: Issue number (number, required)\n  - `owner`: Repository owner (string, required)\n  - `page`: Page number for pagination (default: 1) (number, optional)\n  - `per_page`: Number of results per page (max 100, default: 30) (number, optional)\n  - `repo`: Repository name (string, required)\n\n- **remove_sub_issue** - Remove sub-issue\n  - `issue_number`: The number of the parent issue (number, required)\n  - `owner`: Repository owner (string, required)\n  - `repo`: Repository name (string, required)\n  - `sub_issue_id`: The ID of the sub-issue to remove. ID is not the same as issue number (number, required)\n\n- **reprioritize_sub_issue** - Reprioritize sub-issue\n  - `after_id`: The ID of the sub-issue to be prioritized after (either after_id OR before_id should be specified) (number, optional)\n  - `before_id`: The ID of the sub-issue to be prioritized before (either after_id OR before_id should be specified) (number, optional)\n  - `issue_number`: The number of the parent issue (number, required)\n  - `owner`: Repository owner (string, required)\n  - `repo`: Repository name (string, required)\n  - `sub_issue_id`: The ID of the sub-issue to reprioritize. ID is not the same as issue number (number, required)\n\n- **search_issues** - Search issues\n  - `order`: Sort order (string, optional)\n  - `owner`: Optional repository owner. If provided with repo, only issues for this repository are listed. (string, optional)\n  - `page`: Page number for pagination (min 1) (number, optional)\n  - `perPage`: Results per page for pagination (min 1, max 100) (number, optional)\n  - `query`: Search query using GitHub issues search syntax (string, required)\n  - `repo`: Optional repository name. If provided with owner, only issues for this repository are listed. (string, optional)\n  - `sort`: Sort field by number of matches of categories, defaults to best match (string, optional)\n\n- **update_issue** - Edit issue\n  - `assignees`: New assignees (string[], optional)\n  - `body`: New description (string, optional)\n  - `duplicate_of`: Issue number that this issue is a duplicate of. Only used when state_reason is 'duplicate'. (number, optional)\n  - `issue_number`: Issue number to update (number, required)\n  - `labels`: New labels (string[], optional)\n  - `milestone`: New milestone number (number, optional)\n  - `owner`: Repository owner (string, required)\n  - `repo`: Repository name (string, required)\n  - `state`: New state (string, optional)\n  - `state_reason`: Reason for the state change. Ignored unless state is changed. (string, optional)\n  - `title`: New title (string, optional)\n  - `type`: New issue type (string, optional)\n\n</details>\n\n<details>\n\n<summary>Notifications</summary>\n\n- **dismiss_notification** - Dismiss notification\n  - `state`: The new state of the notification (read/done) (string, optional)\n  - `threadID`: The ID of the notification thread (string, required)\n\n- **get_notification_details** - Get notification details\n  - `notificationID`: The ID of the notification (string, required)\n\n- **list_notifications** - List notifications\n  - `before`: Only show notifications updated before the given time (ISO 8601 format) (string, optional)\n  - `filter`: Filter notifications to, use default unless specified. Read notifications are ones that have already been acknowledged by the user. Participating notifications are those that the user is directly involved in, such as issues or pull requests they have commented on or created. (string, optional)\n  - `owner`: Optional repository owner. If provided with repo, only notifications for this repository are listed. (string, optional)\n  - `page`: Page number for pagination (min 1) (number, optional)\n  - `perPage`: Results per page for pagination (min 1, max 100) (number, optional)\n  - `repo`: Optional repository name. If provided with owner, only notifications for this repository are listed. (string, optional)\n  - `since`: Only show notifications updated after the given time (ISO 8601 format) (string, optional)\n\n- **manage_notification_subscription** - Manage notification subscription\n  - `action`: Action to perform: ignore, watch, or delete the notification subscription. (string, required)\n  - `notificationID`: The ID of the notification thread. (string, required)\n\n- **manage_repository_notification_subscription** - Manage repository notification subscription\n  - `action`: Action to perform: ignore, watch, or delete the repository notification subscription. (string, required)\n  - `owner`: The account owner of the repository. (string, required)\n  - `repo`: The name of the repository. (string, required)\n\n- **mark_all_notifications_read** - Mark all notifications as read\n  - `lastReadAt`: Describes the last point that notifications were checked (optional). Default: Now (string, optional)\n  - `owner`: Optional repository owner. If provided with repo, only notifications for this repository are marked as read. (string, optional)\n  - `repo`: Optional repository name. If provided with owner, only notifications for this repository are marked as read. (string, optional)\n\n</details>\n\n<details>\n\n<summary>Organizations</summary>\n\n- **search_orgs** - Search organizations\n  - `order`: Sort order (string, optional)\n  - `page`: Page number for pagination (min 1) (number, optional)\n  - `perPage`: Results per page for pagination (min 1, max 100) (number, optional)\n  - `query`: Organization search query. Examples: 'microsoft', 'location:california', 'created:>=2025-01-01'. Search is automatically scoped to type:org. (string, required)\n  - `sort`: Sort field by category (string, optional)\n\n</details>\n\n<details>\n\n<summary>Projects</summary>\n\n- **add_project_item** - Add project item\n  - `item_id`: The numeric ID of the issue or pull request to add to the project. (number, required)\n  - `item_type`: The item's type, either issue or pull_request. (string, required)\n  - `owner`: If owner_type == user it is the handle for the GitHub user account. If owner_type == org it is the name of the organization. The name is not case sensitive. (string, required)\n  - `owner_type`: Owner type (string, required)\n  - `project_number`: The project's number. (number, required)\n\n- **delete_project_item** - Delete project item\n  - `item_id`: The internal project item ID to delete from the project (not the issue or pull request ID). (number, required)\n  - `owner`: If owner_type == user it is the handle for the GitHub user account. If owner_type == org it is the name of the organization. The name is not case sensitive. (string, required)\n  - `owner_type`: Owner type (string, required)\n  - `project_number`: The project's number. (number, required)\n\n- **get_project** - Get project\n  - `owner`: If owner_type == user it is the handle for the GitHub user account. If owner_type == org it is the name of the organization. The name is not case sensitive. (string, required)\n  - `owner_type`: Owner type (string, required)\n  - `project_number`: The project's number (number, required)\n\n- **get_project_field** - Get project field\n  - `field_id`: The field's id. (number, required)\n  - `owner`: If owner_type == user it is the handle for the GitHub user account. If owner_type == org it is the name of the organization. The name is not case sensitive. (string, required)\n  - `owner_type`: Owner type (string, required)\n  - `project_number`: The project's number. (number, required)\n\n- **get_project_item** - Get project item\n  - `item_id`: The item's ID. (number, required)\n  - `owner`: If owner_type == user it is the handle for the GitHub user account. If owner_type == org it is the name of the organization. The name is not case sensitive. (string, required)\n  - `owner_type`: Owner type (string, required)\n  - `project_number`: The project's number. (number, required)\n\n- **list_project_fields** - List project fields\n  - `owner`: If owner_type == user it is the handle for the GitHub user account. If owner_type == org it is the name of the organization. The name is not case sensitive. (string, required)\n  - `owner_type`: Owner type (string, required)\n  - `per_page`: Number of results per page (max 100, default: 30) (number, optional)\n  - `project_number`: The project's number. (number, required)\n\n- **list_project_items** - List project items\n  - `owner`: If owner_type == user it is the handle for the GitHub user account. If owner_type == org it is the name of the organization. The name is not case sensitive. (string, required)\n  - `owner_type`: Owner type (string, required)\n  - `per_page`: Number of results per page (max 100, default: 30) (number, optional)\n  - `project_number`: The project's number. (number, required)\n  - `query`: Search query to filter items (string, optional)\n\n- **list_projects** - List projects\n  - `owner`: If owner_type == user it is the handle for the GitHub user account. If owner_type == org it is the name of the organization. The name is not case sensitive. (string, required)\n  - `owner_type`: Owner type (string, required)\n  - `per_page`: Number of results per page (max 100, default: 30) (number, optional)\n  - `query`: Filter projects by a search query (matches title and description) (string, optional)\n\n</details>\n\n<details>\n\n<summary>Pull Requests</summary>\n\n- **add_comment_to_pending_review** - Add review comment to the requester's latest pending pull request review\n  - `body`: The text of the review comment (string, required)\n  - `line`: The line of the blob in the pull request diff that the comment applies to. For multi-line comments, the last line of the range (number, optional)\n  - `owner`: Repository owner (string, required)\n  - `path`: The relative path to the file that necessitates a comment (string, required)\n  - `pullNumber`: Pull request number (number, required)\n  - `repo`: Repository name (string, required)\n  - `side`: The side of the diff to comment on. LEFT indicates the previous state, RIGHT indicates the new state (string, optional)\n  - `startLine`: For multi-line comments, the first line of the range that the comment applies to (number, optional)\n  - `startSide`: For multi-line comments, the starting side of the diff that the comment applies to. LEFT indicates the previous state, RIGHT indicates the new state (string, optional)\n  - `subjectType`: The level at which the comment is targeted (string, required)\n\n- **create_and_submit_pull_request_review** - Create and submit a pull request review without comments\n  - `body`: Review comment text (string, required)\n  - `commitID`: SHA of commit to review (string, optional)\n  - `event`: Review action to perform (string, required)\n  - `owner`: Repository owner (string, required)\n  - `pullNumber`: Pull request number (number, required)\n  - `repo`: Repository name (string, required)\n\n- **create_pending_pull_request_review** - Create pending pull request review\n  - `commitID`: SHA of commit to review (string, optional)\n  - `owner`: Repository owner (string, required)\n  - `pullNumber`: Pull request number (number, required)\n  - `repo`: Repository name (string, required)\n\n- **create_pull_request** - Open new pull request\n  - `base`: Branch to merge into (string, required)\n  - `body`: PR description (string, optional)\n  - `draft`: Create as draft PR (boolean, optional)\n  - `head`: Branch containing changes (string, required)\n  - `maintainer_can_modify`: Allow maintainer edits (boolean, optional)\n  - `owner`: Repository owner (string, required)\n  - `repo`: Repository name (string, required)\n  - `title`: PR title (string, required)\n\n- **delete_pending_pull_request_review** - Delete the requester's latest pending pull request review\n  - `owner`: Repository owner (string, required)\n  - `pullNumber`: Pull request number (number, required)\n  - `repo`: Repository name (string, required)\n\n- **get_pull_request** - Get pull request details\n  - `owner`: Repository owner (string, required)\n  - `pullNumber`: Pull request number (number, required)\n  - `repo`: Repository name (string, required)\n\n- **get_pull_request_diff** - Get pull request diff\n  - `owner`: Repository owner (string, required)\n  - `pullNumber`: Pull request number (number, required)\n  - `repo`: Repository name (string, required)\n\n- **get_pull_request_files** - Get pull request files\n  - `owner`: Repository owner (string, required)\n  - `page`: Page number for pagination (min 1) (number, optional)\n  - `perPage`: Results per page for pagination (min 1, max 100) (number, optional)\n  - `pullNumber`: Pull request number (number, required)\n  - `repo`: Repository name (string, required)\n\n- **get_pull_request_review_comments** - Get pull request review comments\n  - `owner`: Repository owner (string, required)\n  - `pullNumber`: Pull request number (number, required)\n  - `repo`: Repository name (string, required)\n\n- **get_pull_request_reviews** - Get pull request reviews\n  - `owner`: Repository owner (string, required)\n  - `pullNumber`: Pull request number (number, required)\n  - `repo`: Repository name (string, required)\n\n- **get_pull_request_status** - Get pull request status checks\n  - `owner`: Repository owner (string, required)\n  - `pullNumber`: Pull request number (number, required)\n  - `repo`: Repository name (string, required)\n\n- **list_pull_requests** - List pull requests\n  - `base`: Filter by base branch (string, optional)\n  - `direction`: Sort direction (string, optional)\n  - `head`: Filter by head user/org and branch (string, optional)\n  - `owner`: Repository owner (string, required)\n  - `page`: Page number for pagination (min 1) (number, optional)\n  - `perPage`: Results per page for pagination (min 1, max 100) (number, optional)\n  - `repo`: Repository name (string, required)\n  - `sort`: Sort by (string, optional)\n  - `state`: Filter by state (string, optional)\n\n- **merge_pull_request** - Merge pull request\n  - `commit_message`: Extra detail for merge commit (string, optional)\n  - `commit_title`: Title for merge commit (string, optional)\n  - `merge_method`: Merge method (string, optional)\n  - `owner`: Repository owner (string, required)\n  - `pullNumber`: Pull request number (number, required)\n  - `repo`: Repository name (string, required)\n\n- **request_copilot_review** - Request Copilot review\n  - `owner`: Repository owner (string, required)\n  - `pullNumber`: Pull request number (number, required)\n  - `repo`: Repository name (string, required)\n\n- **search_pull_requests** - Search pull requests\n  - `order`: Sort order (string, optional)\n  - `owner`: Optional repository owner. If provided with repo, only pull requests for this repository are listed. (string, optional)\n  - `page`: Page number for pagination (min 1) (number, optional)\n  - `perPage`: Results per page for pagination (min 1, max 100) (number, optional)\n  - `query`: Search query using GitHub pull request search syntax (string, required)\n  - `repo`: Optional repository name. If provided with owner, only pull requests for this repository are listed. (string, optional)\n  - `sort`: Sort field by number of matches of categories, defaults to best match (string, optional)\n\n- **submit_pending_pull_request_review** - Submit the requester's latest pending pull request review\n  - `body`: The text of the review comment (string, optional)\n  - `event`: The event to perform (string, required)\n  - `owner`: Repository owner (string, required)\n  - `pullNumber`: Pull request number (number, required)\n  - `repo`: Repository name (string, required)\n\n- **update_pull_request** - Edit pull request\n  - `base`: New base branch name (string, optional)\n  - `body`: New description (string, optional)\n  - `draft`: Mark pull request as draft (true) or ready for review (false) (boolean, optional)\n  - `maintainer_can_modify`: Allow maintainer edits (boolean, optional)\n  - `owner`: Repository owner (string, required)\n  - `pullNumber`: Pull request number to update (number, required)\n  - `repo`: Repository name (string, required)\n  - `reviewers`: GitHub usernames to request reviews from (string[], optional)\n  - `state`: New state (string, optional)\n  - `title`: New title (string, optional)\n\n- **update_pull_request_branch** - Update pull request branch\n  - `expectedHeadSha`: The expected SHA of the pull request's HEAD ref (string, optional)\n  - `owner`: Repository owner (string, required)\n  - `pullNumber`: Pull request number (number, required)\n  - `repo`: Repository name (string, required)\n\n</details>\n\n<details>\n\n<summary>Repositories</summary>\n\n- **create_branch** - Create branch\n  - `branch`: Name for new branch (string, required)\n  - `from_branch`: Source branch (defaults to repo default) (string, optional)\n  - `owner`: Repository owner (string, required)\n  - `repo`: Repository name (string, required)\n\n- **create_or_update_file** - Create or update file\n  - `branch`: Branch to create/update the file in (string, required)\n  - `content`: Content of the file (string, required)\n  - `message`: Commit message (string, required)\n  - `owner`: Repository owner (username or organization) (string, required)\n  - `path`: Path where to create/update the file (string, required)\n  - `repo`: Repository name (string, required)\n  - `sha`: Required if updating an existing file. The blob SHA of the file being replaced. (string, optional)\n\n- **create_repository** - Create repository\n  - `autoInit`: Initialize with README (boolean, optional)\n  - `description`: Repository description (string, optional)\n  - `name`: Repository name (string, required)\n  - `organization`: Organization to create the repository in (omit to create in your personal account) (string, optional)\n  - `private`: Whether repo should be private (boolean, optional)\n\n- **delete_file** - Delete file\n  - `branch`: Branch to delete the file from (string, required)\n  - `message`: Commit message (string, required)\n  - `owner`: Repository owner (username or organization) (string, required)\n  - `path`: Path to the file to delete (string, required)\n  - `repo`: Repository name (string, required)\n\n- **fork_repository** - Fork repository\n  - `organization`: Organization to fork to (string, optional)\n  - `owner`: Repository owner (string, required)\n  - `repo`: Repository name (string, required)\n\n- **get_commit** - Get commit details\n  - `include_diff`: Whether to include file diffs and stats in the response. Default is true. (boolean, optional)\n  - `owner`: Repository owner (string, required)\n  - `page`: Page number for pagination (min 1) (number, optional)\n  - `perPage`: Results per page for pagination (min 1, max 100) (number, optional)\n  - `repo`: Repository name (string, required)\n  - `sha`: Commit SHA, branch name, or tag name (string, required)\n\n- **get_file_contents** - Get file or directory contents\n  - `owner`: Repository owner (username or organization) (string, required)\n  - `path`: Path to file/directory (directories must end with a slash '/') (string, optional)\n  - `ref`: Accepts optional git refs such as `refs/tags/{tag}`, `refs/heads/{branch}` or `refs/pull/{pr_number}/head` (string, optional)\n  - `repo`: Repository name (string, required)\n  - `sha`: Accepts optional commit SHA. If specified, it will be used instead of ref (string, optional)\n\n- **get_latest_release** - Get latest release\n  - `owner`: Repository owner (string, required)\n  - `repo`: Repository name (string, required)\n\n- **get_release_by_tag** - Get a release by tag name\n  - `owner`: Repository owner (string, required)\n  - `repo`: Repository name (string, required)\n  - `tag`: Tag name (e.g., 'v1.0.0') (string, required)\n\n- **get_tag** - Get tag details\n  - `owner`: Repository owner (string, required)\n  - `repo`: Repository name (string, required)\n  - `tag`: Tag name (string, required)\n\n- **list_branches** - List branches\n  - `owner`: Repository owner (string, required)\n  - `page`: Page number for pagination (min 1) (number, optional)\n  - `perPage`: Results per page for pagination (min 1, max 100) (number, optional)\n  - `repo`: Repository name (string, required)\n\n- **list_commits** - List commits\n  - `author`: Author username or email address to filter commits by (string, optional)\n  - `owner`: Repository owner (string, required)\n  - `page`: Page number for pagination (min 1) (number, optional)\n  - `perPage`: Results per page for pagination (min 1, max 100) (number, optional)\n  - `repo`: Repository name (string, required)\n  - `sha`: Commit SHA, branch or tag name to list commits of. If not provided, uses the default branch of the repository. If a commit SHA is provided, will list commits up to that SHA. (string, optional)\n\n- **list_releases** - List releases\n  - `owner`: Repository owner (string, required)\n  - `page`: Page number for pagination (min 1) (number, optional)\n  - `perPage`: Results per page for pagination (min 1, max 100) (number, optional)\n  - `repo`: Repository name (string, required)\n\n- **list_starred_repositories** - List starred repositories\n  - `direction`: The direction to sort the results by. (string, optional)\n  - `page`: Page number for pagination (min 1) (number, optional)\n  - `perPage`: Results per page for pagination (min 1, max 100) (number, optional)\n  - `sort`: How to sort the results. Can be either 'created' (when the repository was starred) or 'updated' (when the repository was last pushed to). (string, optional)\n  - `username`: Username to list starred repositories for. Defaults to the authenticated user. (string, optional)\n\n- **list_tags** - List tags\n  - `owner`: Repository owner (string, required)\n  - `page`: Page number for pagination (min 1) (number, optional)\n  - `perPage`: Results per page for pagination (min 1, max 100) (number, optional)\n  - `repo`: Repository name (string, required)\n\n- **push_files** - Push files to repository\n  - `branch`: Branch to push to (string, required)\n  - `files`: Array of file objects to push, each object with path (string) and content (string) (object[], required)\n  - `message`: Commit message (string, required)\n  - `owner`: Repository owner (string, required)\n  - `repo`: Repository name (string, required)\n\n- **search_code** - Search code\n  - `order`: Sort order for results (string, optional)\n  - `page`: Page number for pagination (min 1) (number, optional)\n  - `perPage`: Results per page for pagination (min 1, max 100) (number, optional)\n  - `query`: Search query using GitHub's powerful code search syntax. Examples: 'content:Skill language:Java org:github', 'NOT is:archived language:Python OR language:go', 'repo:github/github-mcp-server'. Supports exact matching, language filters, path filters, and more. (string, required)\n  - `sort`: Sort field ('indexed' only) (string, optional)\n\n- **search_repositories** - Search repositories\n  - `minimal_output`: Return minimal repository information (default: true). When false, returns full GitHub API repository objects. (boolean, optional)\n  - `page`: Page number for pagination (min 1) (number, optional)\n  - `perPage`: Results per page for pagination (min 1, max 100) (number, optional)\n  - `query`: Repository search query. Examples: 'machine learning in:name stars:>1000 language:python', 'topic:react', 'user:facebook'. Supports advanced search syntax for precise filtering. (string, required)\n\n- **star_repository** - Star repository\n  - `owner`: Repository owner (string, required)\n  - `repo`: Repository name (string, required)\n\n- **unstar_repository** - Unstar repository\n  - `owner`: Repository owner (string, required)\n  - `repo`: Repository name (string, required)\n\n</details>\n\n<details>\n\n<summary>Secret Protection</summary>\n\n- **get_secret_scanning_alert** - Get secret scanning alert\n  - `alertNumber`: The number of the alert. (number, required)\n  - `owner`: The owner of the repository. (string, required)\n  - `repo`: The name of the repository. (string, required)\n\n- **list_secret_scanning_alerts** - List secret scanning alerts\n  - `owner`: The owner of the repository. (string, required)\n  - `repo`: The name of the repository. (string, required)\n  - `resolution`: Filter by resolution (string, optional)\n  - `secret_type`: A comma-separated list of secret types to return. All default secret patterns are returned. To return generic patterns, pass the token name(s) in the parameter. (string, optional)\n  - `state`: Filter by state (string, optional)\n\n</details>\n\n<details>\n\n<summary>Security Advisories</summary>\n\n- **get_global_security_advisory** - Get a global security advisory\n  - `ghsaId`: GitHub Security Advisory ID (format: GHSA-xxxx-xxxx-xxxx). (string, required)\n\n- **list_global_security_advisories** - List global security advisories\n  - `affects`: Filter advisories by affected package or version (e.g. \"package1,package2@1.0.0\"). (string, optional)\n  - `cveId`: Filter by CVE ID. (string, optional)\n  - `cwes`: Filter by Common Weakness Enumeration IDs (e.g. [\"79\", \"284\", \"22\"]). (string[], optional)\n  - `ecosystem`: Filter by package ecosystem. (string, optional)\n  - `ghsaId`: Filter by GitHub Security Advisory ID (format: GHSA-xxxx-xxxx-xxxx). (string, optional)\n  - `isWithdrawn`: Whether to only return withdrawn advisories. (boolean, optional)\n  - `modified`: Filter by publish or update date or date range (ISO 8601 date or range). (string, optional)\n  - `published`: Filter by publish date or date range (ISO 8601 date or range). (string, optional)\n  - `severity`: Filter by severity. (string, optional)\n  - `type`: Advisory type. (string, optional)\n  - `updated`: Filter by update date or date range (ISO 8601 date or range). (string, optional)\n\n- **list_org_repository_security_advisories** - List org repository security advisories\n  - `direction`: Sort direction. (string, optional)\n  - `org`: The organization login. (string, required)\n  - `sort`: Sort field. (string, optional)\n  - `state`: Filter by advisory state. (string, optional)\n\n- **list_repository_security_advisories** - List repository security advisories\n  - `direction`: Sort direction. (string, optional)\n  - `owner`: The owner of the repository. (string, required)\n  - `repo`: The name of the repository. (string, required)\n  - `sort`: Sort field. (string, optional)\n  - `state`: Filter by advisory state. (string, optional)\n\n</details>\n\n<details>\n\n<summary>Users</summary>\n\n- **search_users** - Search users\n  - `order`: Sort order (string, optional)\n  - `page`: Page number for pagination (min 1) (number, optional)\n  - `perPage`: Results per page for pagination (min 1, max 100) (number, optional)\n  - `query`: User search query. Examples: 'john smith', 'location:seattle', 'followers:>100'. Search is automatically scoped to type:user. (string, required)\n  - `sort`: Sort users by number of followers or repositories, or when the person joined GitHub. (string, optional)\n\n</details>\n<!-- END AUTOMATED TOOLS -->\n\n### Additional Tools in Remote Github MCP Server\n\n<details>\n\n<summary>Copilot coding agent</summary>\n\n-   **create_pull_request_with_copilot** - Perform task with GitHub Copilot coding agent\n    -   `owner`: Repository owner. You can guess the owner, but confirm it with the user before proceeding. (string, required)\n    -   `repo`: Repository name. You can guess the repository name, but confirm it with the user before proceeding. (string, required)\n    -   `problem_statement`: Detailed description of the task to be performed (e.g., 'Implement a feature that does X', 'Fix bug Y', etc.) (string, required)\n    -   `title`: Title for the pull request that will be created (string, required)\n    -   `base_ref`: Git reference (e.g., branch) that the agent will start its work from. If not specified, defaults to the repository's default branch (string, optional)\n\n</details>\n\n<details>\n\n<summary>Copilot Spaces</summary>\n\n-   **get_copilot_space** - Get Copilot Space\n    -   `owner`: The owner of the space. (string, required)\n    -   `name`: The name of the space. (string, required)\n\n-   **list_copilot_spaces** - List Copilot Spaces\n</details>\n\n#### Specifying Toolsets\n\nTo specify toolsets you want available to the LLM, you can pass an allow-list in two ways:\n\n1. **Using Command Line Argument**:\n\n   ```bash\n   github-mcp-server --toolsets repos,issues,pull_requests,actions,code_security\n   ```\n\n2. **Using Environment Variable**:\n   ```bash\n   GITHUB_TOOLSETS=\"repos,issues,pull_requests,actions,code_security\" ./github-mcp-server\n   ```\n\nThe environment variable `GITHUB_TOOLSETS` takes precedence over the command line argument if both are provided.\n\n### Using Toolsets With Docker\n\nWhen using Docker, you can pass the toolsets as environment variables:\n\n```bash\ndocker run -i --rm \\\n  -e GITHUB_PERSONAL_ACCESS_TOKEN=<your-token> \\\n  -e GITHUB_TOOLSETS=\"repos,issues,pull_requests,actions,code_security,experiments\" \\\n  ghcr.io/github/github-mcp-server\n```\n\n### The \"all\" Toolset\n\nThe special toolset `all` can be provided to enable all available toolsets regardless of any other configuration:\n\n```bash\n./github-mcp-server --toolsets all\n```\n\nOr using the environment variable:\n\n```bash\nGITHUB_TOOLSETS=\"all\" ./github-mcp-server\n```\n\n## Dynamic Tool Discovery\n\n**Note**: This feature is currently in beta and may not be available in all environments. Please test it out and let us know if you encounter any issues.\n\nInstead of starting with all tools enabled, you can turn on dynamic toolset discovery. Dynamic toolsets allow the MCP host to list and enable toolsets in response to a user prompt. This should help to avoid situations where the model gets confused by the sheer number of tools available.\n\n### Using Dynamic Tool Discovery\n\nWhen using the binary, you can pass the `--dynamic-toolsets` flag.\n\n```bash\n./github-mcp-server --dynamic-toolsets\n```\n\nWhen using Docker, you can pass the toolsets as environment variables:\n\n```bash\ndocker run -i --rm \\\n  -e GITHUB_PERSONAL_ACCESS_TOKEN=<your-token> \\\n  -e GITHUB_DYNAMIC_TOOLSETS=1 \\\n  ghcr.io/github/github-mcp-server\n```\n\n## Read-Only Mode\n\nTo run the server in read-only mode, you can use the `--read-only` flag. This will only offer read-only tools, preventing any modifications to repositories, issues, pull requests, etc.\n\n```bash\n./github-mcp-server --read-only\n```\n\nWhen using Docker, you can pass the read-only mode as an environment variable:\n\n```bash\ndocker run -i --rm \\\n  -e GITHUB_PERSONAL_ACCESS_TOKEN=<your-token> \\\n  -e GITHUB_READ_ONLY=1 \\\n  ghcr.io/github/github-mcp-server\n```\n\n## GitHub Enterprise Server and Enterprise Cloud with data residency (ghe.com)\n\nThe flag `--gh-host` and the environment variable `GITHUB_HOST` can be used to set\nthe hostname for GitHub Enterprise Server or GitHub Enterprise Cloud with data residency.\n\n- For GitHub Enterprise Server, prefix the hostname with the `https://` URI scheme, as it otherwise defaults to `http://`, which GitHub Enterprise Server does not support.\n- For GitHub Enterprise Cloud with data residency, use `https://YOURSUBDOMAIN.ghe.com` as the hostname.\n``` json\n\"github\": {\n    \"command\": \"docker\",\n    \"args\": [\n    \"run\",\n    \"-i\",\n    \"--rm\",\n    \"-e\",\n    \"GITHUB_PERSONAL_ACCESS_TOKEN\",\n    \"-e\",\n    \"GITHUB_HOST\",\n    \"ghcr.io/github/github-mcp-server\"\n    ],\n    \"env\": {\n        \"GITHUB_PERSONAL_ACCESS_TOKEN\": \"${input:github_token}\",\n        \"GITHUB_HOST\": \"https://<your GHES or ghe.com domain name>\"\n    }\n}\n```\n\n## i18n / Overriding Descriptions\n\nThe descriptions of the tools can be overridden by creating a\n`github-mcp-server-config.json` file in the same directory as the binary.\n\nThe file should contain a JSON object with the tool names as keys and the new\ndescriptions as values. For example:\n\n```json\n{\n  \"TOOL_ADD_ISSUE_COMMENT_DESCRIPTION\": \"an alternative description\",\n  \"TOOL_CREATE_BRANCH_DESCRIPTION\": \"Create a new branch in a GitHub repository\"\n}\n```\n\nYou can create an export of the current translations by running the binary with\nthe `--export-translations` flag.\n\nThis flag will preserve any translations/overrides you have made, while adding\nany new translations that have been added to the binary since the last time you\nexported.\n\n```sh\n./github-mcp-server --export-translations\ncat github-mcp-server-config.json\n```\n\nYou can also use ENV vars to override the descriptions. The environment\nvariable names are the same as the keys in the JSON file, prefixed with\n`GITHUB_MCP_` and all uppercase.\n\nFor example, to override the `TOOL_ADD_ISSUE_COMMENT_DESCRIPTION` tool, you can\nset the following environment variable:\n\n```sh\nexport GITHUB_MCP_TOOL_ADD_ISSUE_COMMENT_DESCRIPTION=\"an alternative description\"\n```\n\n## Library Usage\n\nThe exported Go API of this module should currently be considered unstable, and subject to breaking changes. In the future, we may offer stability; please file an issue if there is a use case where this would be valuable.\n\n## License\n\nThis project is licensed under the terms of the MIT open source license. Please refer to [MIT](./LICENSE) for the full terms.\n",
      "npm_url": "https://www.npmjs.com/package/github-mcp-server",
      "npm_downloads": 14761,
      "keywords": [
        "github",
        "repositories",
        "workflows",
        "servers github",
        "github mcp",
        "github github"
      ],
      "category": "official-servers"
    },
    "gotohuman--gotohuman-mcp-server": {
      "owner": "gotohuman",
      "name": "gotohuman-mcp-server",
      "url": "https://github.com/gotohuman/gotohuman-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/gotohuman.webp",
      "description": "Request human reviews for AI-generated content through a webhook-based workflow. Supports seamless integration into AI workflows, allowing for customizable human approval processes.",
      "stars": 41,
      "forks": 9,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-09-21T10:18:54Z",
      "readme_content": "# gotoHuman MCP Server\n\ngotoHuman makes it easy to add **human approvals** to AI agents and agentic workflows.  \nA fully-managed async human-in-the-loop workflow with a customizable approval UI.  \nEnjoy built-in auth, webhooks, notifications, team features, and an evolving training dataset.\n\nUse our MCP server to request human approvals from your AI workflows via MCP or add it to your IDE to help with integration.\n\n## Installation\n\n```bash\nnpx @gotohuman/mcp-server\n```\n\n### Use with Cursor / Claude / Windsurf\n\n```json\n{\n  \"mcpServers\": {\n    \"gotoHuman\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@gotohuman/mcp-server\"],\n      \"env\": {\n        \"GOTOHUMAN_API_KEY\": \"your-api-key\"\n      }\n    }\n  }\n}\n```\n\n[![Install MCP Server](https://cursor.com/deeplink/mcp-install-dark.svg)](cursor://anysphere.cursor-deeplink/mcp/install?name=gotoHuman&config=eyJjb21tYW5kIjoibnB4IC15IEBnb3RvaHVtYW4vbWNwLXNlcnZlciIsImVudiI6eyJHT1RPSFVNQU5fQVBJX0tFWSI6InlvdXItYXBpLWtleSJ9fQ==)\n\nGet your API key and set up an approval step at [app.gotohuman.com](https://app.gotohuman.com)\n\n## Demo\n\nThis is Cursor on the left, but this could be a background agent that also reacts to the approval webhook.\n\nhttps://github.com/user-attachments/assets/380a4223-ea77-4e24-90a5-52669b77f56f\n\n## Tools\n\n### `list-forms`\nList all available review forms.\n  - __Returns__ a list of all available forms in your account incl. high-level info about the added fields\n### `get-form-schema`  \nGet the schema to use when requesting a human review for a given form.\n  - __Params__\n    - `formId`: The form ID to fetch the schema for\n  - __Returns__ the schema, considering the incl. fields and their configuration\n### `request-human-review-with-form`  \nRequest a human review. Will appear in your gotoHuman inbox.\n  - __Params__\n    - `formId`: The form ID for the review\n    - `fieldData`: Content (AI-output to review, context,...) and configuration for the form's fields.  \n    The schema for this needs to be fetched with `get-form-schema`\n    - `metadata`: Optional additional data that will be incl. in the webhook response after form submission\n    - `assignToUsers`: Optional list of user emails to assign the review to\n  - __Returns__ a link to the review in gotoHuman\n\n\n## Development\n\n```bash\n# Install dependencies\nnpm install\n\n# Build the server\nnpm run build\n\n# For testing: Run the MCP inspector\nnpm run inspector\n```\n\n  #### Run locally in MCP Client (e.g. Cursor / Claude / Windsurf)\n\n  ```json\n  {\n  \"mcpServers\": {\n    \"gotoHuman\": {\n      \"command\": \"node\",\n      \"args\": [\"/<absolute-path>/build/index.js\"],\n      \"env\": {\n        \"GOTOHUMAN_API_KEY\": \"your-api-key\"\n      }\n    }\n  }\n}\n```\n> [!NOTE]\n> For Windows, the `args` path needs to be `C:\\\\<absolute-path>\\\\build\\\\index.js`\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "webhook",
        "workflows",
        "workflow",
        "ai workflows",
        "webhook based",
        "reviews ai"
      ],
      "category": "official-servers"
    },
    "graphlit--graphlit-mcp-server": {
      "owner": "graphlit",
      "name": "graphlit-mcp-server",
      "url": "https://github.com/graphlit/graphlit-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/graphlit.webp",
      "description": "Integrate various data sources such as Slack, Gmail, and web feeds into Graphlit projects, enabling efficient retrieval and searching of relevant content via MCP clients.",
      "stars": 366,
      "forks": 47,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-10-01T04:21:42Z",
      "readme_content": "[![npm version](https://badge.fury.io/js/graphlit-mcp-server.svg)](https://badge.fury.io/js/graphlit-mcp-server)\n[![smithery badge](https://smithery.ai/badge/@graphlit/graphlit-mcp-server)](https://smithery.ai/server/@graphlit/graphlit-mcp-server)\n\n# Model Context Protocol (MCP) Server for Graphlit Platform\n\n## Overview\n\nThe Model Context Protocol (MCP) Server enables integration between MCP clients and the Graphlit service. This document outlines the setup process and provides a basic example of using the client.\n\nIngest anything from Slack, Discord, websites, Google Drive, email, Jira, Linear or GitHub into a Graphlit project - and then search and retrieve relevant knowledge within an MCP client like Cursor, Windsurf, Goose or Cline.\n\nYour Graphlit project acts as a searchable, and RAG-ready knowledge base across all your developer and product management tools.\n\nDocuments (PDF, DOCX, PPTX, etc.) and HTML web pages will be extracted to Markdown upon ingestion. Audio and video files will be transcribed upon ingestion.\n\nWeb crawling and web search are built-in as MCP tools, with no need to integrate other tools like Firecrawl, Exa, etc. separately.\n\nYou can read more about the MCP Server use cases and features on our [blog](https://www.graphlit.com/blog/graphlit-mcp-server).\n\nWatch our latest [YouTube video](https://www.youtube.com/watch?v=Or-QqonvcAs&t=4s) on using the Graphlit MCP Server with the Goose MCP client.\n\nFor any questions on using the MCP Server, please join our [Discord](https://discord.gg/ygFmfjy3Qx) community and post on the #mcp channel.\n\n<a href=\"https://glama.ai/mcp/servers/fscrivteod\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/fscrivteod/badge\" alt=\"graphlit-mcp-server MCP server\" />\n</a>\n\n## Tools\n\n### Retrieval\n\n- Query Contents\n- Query Collections\n- Query Feeds\n- Query Conversations\n- Retrieve Relevant Sources\n- Retrieve Similar Images\n- Visually Describe Image\n\n### RAG\n\n- Prompt LLM Conversation\n\n### Extraction\n\n- Extract Structured JSON from Text\n\n### Publishing\n\n- Publish as Audio (ElevenLabs Audio)\n- Publish as Image (OpenAI Image Generation)\n\n### Ingestion\n\n- Files\n- Web Pages\n- Messages\n- Posts\n- Emails\n- Issues\n- Text\n- Memory (Short-Term)\n\n### Data Connectors\n\n- Microsoft Outlook email\n- Google Mail\n- Notion\n- Reddit\n- Linear\n- Jira\n- GitHub Issues\n- Google Drive\n- OneDrive\n- SharePoint\n- Dropbox\n- Box\n- GitHub\n- Slack\n- Microsoft Teams\n- Discord\n- Twitter/X\n- Podcasts (RSS)\n\n### Web\n\n- Web Crawling\n- Web Search (including Podcast Search)\n- Web Mapping\n- Screenshot Page\n\n### Notifications\n\n- Slack\n- Email\n- Webhook\n- Twitter/X\n\n### Operations\n\n- Configure Project\n- Create Collection\n- Add Contents to Collection\n- Remove Contents from Collection\n- Delete Collection(s)\n- Delete Feed(s)\n- Delete Content(s)\n- Delete Conversation(s)\n- Is Feed Done?\n- Is Content Done?\n\n### Enumerations\n\n- List Slack Channels\n- List Microsoft Teams Teams\n- List Microsoft Teams Channels\n- List SharePoint Libraries\n- List SharePoint Folders\n- List Linear Projects\n- List Notion Databases\n- List Notion Pages\n- List Dropbox Folders\n- List Box Folders\n- List Discord Guilds\n- List Discord Channels\n- List Google Calendars\n- List Microsoft Calendars\n\n## Resources\n\n- Project\n- Contents\n- Feeds\n- Collections (of Content)\n- Workflows\n- Conversations\n- Specifications\n\n## Prerequisites\n\nBefore you begin, ensure you have the following:\n\n- Node.js installed on your system (recommended version 18.x or higher).\n- An active account on the [Graphlit Platform](https://portal.graphlit.dev) with access to the API settings dashboard.\n\n## Configuration\n\nThe Graphlit MCP Server supports environment variables to be set for authentication and configuration:\n\n- `GRAPHLIT_ENVIRONMENT_ID`: Your environment ID.\n- `GRAPHLIT_ORGANIZATION_ID`: Your organization ID.\n- `GRAPHLIT_JWT_SECRET`: Your JWT secret for signing the JWT token.\n\nYou can find these values in the API settings dashboard on the [Graphlit Platform](https://portal.graphlit.dev).\n\n## Installation\n\n### Installing via VS Code\n\nFor quick installation, use one of the one-click install buttons below:\n\n[![Install with NPX in VS Code](https://img.shields.io/badge/VS_Code-NPM-0098FF?style=flat-square&logo=visualstudiocode&logoColor=white)](https://insiders.vscode.dev/redirect/mcp/install?name=graphlit&inputs=%5B%7B%22type%22%3A%22promptString%22%2C%22id%22%3A%22organization_id%22%2C%22description%22%3A%22Graphlit%20Organization%20ID%22%2C%22password%22%3Atrue%7D%2C%7B%22type%22%3A%22promptString%22%2C%22id%22%3A%22environment_id%22%2C%22description%22%3A%22Graphlit%20Environment%20ID%22%2C%22password%22%3Atrue%7D%2C%7B%22type%22%3A%22promptString%22%2C%22id%22%3A%22jwt_secret%22%2C%22description%22%3A%22Graphlit%20JWT%20Secret%22%2C%22password%22%3Atrue%7D%5D&config=%7B%22command%22%3A%22npx%22%2C%22args%22%3A%5B%22-y%22%2C%22graphlit-mcp-server%22%5D%2C%22env%22%3A%7B%22GRAPHLIT_ORGANIZATION_ID%22%3A%22%24%7Binput%3Aorganization_id%7D%22%2C%22GRAPHLIT_ENVIRONMENT_ID%22%3A%22%24%7Binput%3Aenvironment_id%7D%22%2C%22GRAPHLIT_JWT_SECRET%22%3A%22%24%7Binput%3Ajwt_secret%7D%22%7D%7D) [![Install with NPX in VS Code Insiders](https://img.shields.io/badge/VS_Code_Insiders-NPM-24bfa5?style=flat-square&logo=visualstudiocode&logoColor=white)](https://insiders.vscode.dev/redirect/mcp/install?name=graphlit&inputs=%5B%7B%22type%22%3A%22promptString%22%2C%22id%22%3A%22organization_id%22%2C%22description%22%3A%22Graphlit%20Organization%20ID%22%2C%22password%22%3Atrue%7D%2C%7B%22type%22%3A%22promptString%22%2C%22id%22%3A%22environment_id%22%2C%22description%22%3A%22Graphlit%20Environment%20ID%22%2C%22password%22%3Atrue%7D%2C%7B%22type%22%3A%22promptString%22%2C%22id%22%3A%22jwt_secret%22%2C%22description%22%3A%22Graphlit%20JWT%20Secret%22%2C%22password%22%3Atrue%7D%5D&config=%7B%22command%22%3A%22npx%22%2C%22args%22%3A%5B%22-y%22%2C%22graphlit-mcp-server%22%5D%2C%22env%22%3A%7B%22GRAPHLIT_ORGANIZATION_ID%22%3A%22%24%7Binput%3Aorganization_id%7D%22%2C%22GRAPHLIT_ENVIRONMENT_ID%22%3A%22%24%7Binput%3Aenvironment_id%7D%22%2C%22GRAPHLIT_JWT_SECRET%22%3A%22%24%7Binput%3Ajwt_secret%7D%22%7D%7D&quality=insiders)\n\nFor manual installation, add the following JSON block to your User Settings (JSON) file in VS Code. You can do this by pressing `Ctrl + Shift + P` and typing `Preferences: Open User Settings (JSON)`.\n\nOptionally, you can add it to a file called `.vscode/mcp.json` in your workspace. This will allow you to share the configuration with others.\n\n> Note that the `mcp` key is not needed in the `.vscode/mcp.json` file.\n\n```json\n{\n  \"mcp\": {\n    \"inputs\": [\n      {\n        \"type\": \"promptString\",\n        \"id\": \"organization_id\",\n        \"description\": \"Graphlit Organization ID\",\n        \"password\": true\n      },\n      {\n        \"type\": \"promptString\",\n        \"id\": \"environment_id\",\n        \"description\": \"Graphlit Environment ID\",\n        \"password\": true\n      },\n      {\n        \"type\": \"promptString\",\n        \"id\": \"jwt_secret\",\n        \"description\": \"Graphlit JWT Secret\",\n        \"password\": true\n      }\n    ],\n    \"servers\": {\n      \"graphlit\": {\n        \"command\": \"npx\",\n        \"args\": [\"-y\", \"graphlit-mcp-server\"],\n        \"env\": {\n          \"GRAPHLIT_ORGANIZATION_ID\": \"${input:organization_id}\",\n          \"GRAPHLIT_ENVIRONMENT_ID\": \"${input:environment_id}\",\n          \"GRAPHLIT_JWT_SECRET\": \"${input:jwt_secret}\"\n        }\n      }\n    }\n  }\n}\n```\n\n### Installing via Windsurf\n\nTo install graphlit-mcp-server in Windsurf IDE application, Cline should use NPX:\n\n```bash\nnpx -y graphlit-mcp-server\n```\n\nYour mcp_config.json file should be configured similar to:\n\n```\n{\n    \"mcpServers\": {\n        \"graphlit-mcp-server\": {\n            \"command\": \"npx\",\n            \"args\": [\n                \"-y\",\n                \"graphlit-mcp-server\"\n            ],\n            \"env\": {\n                \"GRAPHLIT_ORGANIZATION_ID\": \"your-organization-id\",\n                \"GRAPHLIT_ENVIRONMENT_ID\": \"your-environment-id\",\n                \"GRAPHLIT_JWT_SECRET\": \"your-jwt-secret\",\n            }\n        }\n    }\n}\n```\n\n### Installing via Cline\n\nTo install graphlit-mcp-server in Cline IDE application, Cline should use NPX:\n\n```bash\nnpx -y graphlit-mcp-server\n```\n\nYour cline_mcp_settings.json file should be configured similar to:\n\n```\n{\n    \"mcpServers\": {\n        \"graphlit-mcp-server\": {\n            \"command\": \"npx\",\n            \"args\": [\n                \"-y\",\n                \"graphlit-mcp-server\"\n            ],\n            \"env\": {\n                \"GRAPHLIT_ORGANIZATION_ID\": \"your-organization-id\",\n                \"GRAPHLIT_ENVIRONMENT_ID\": \"your-environment-id\",\n                \"GRAPHLIT_JWT_SECRET\": \"your-jwt-secret\",\n            }\n        }\n    }\n}\n```\n\n### Installing via Cursor\n\nTo install graphlit-mcp-server in Cursor IDE application, Cursor should use NPX:\n\n```bash\nnpx -y graphlit-mcp-server\n```\n\nYour mcp.json file should be configured similar to:\n\n```\n{\n    \"mcpServers\": {\n        \"graphlit-mcp-server\": {\n            \"command\": \"npx\",\n            \"args\": [\n                \"-y\",\n                \"graphlit-mcp-server\"\n            ],\n            \"env\": {\n                \"GRAPHLIT_ORGANIZATION_ID\": \"your-organization-id\",\n                \"GRAPHLIT_ENVIRONMENT_ID\": \"your-environment-id\",\n                \"GRAPHLIT_JWT_SECRET\": \"your-jwt-secret\",\n            }\n        }\n    }\n}\n```\n\n### Installing via Smithery\n\nTo install graphlit-mcp-server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@graphlit/graphlit-mcp-server):\n\n```bash\nnpx -y @smithery/cli install @graphlit/graphlit-mcp-server --client claude\n```\n\n### Installing manually\n\nTo use the Graphlit MCP Server in any MCP client application, use:\n\n```\n{\n    \"mcpServers\": {\n        \"graphlit-mcp-server\": {\n            \"command\": \"npx\",\n            \"args\": [\n                \"-y\",\n                \"graphlit-mcp-server\"\n            ],\n            \"env\": {\n                \"GRAPHLIT_ORGANIZATION_ID\": \"your-organization-id\",\n                \"GRAPHLIT_ENVIRONMENT_ID\": \"your-environment-id\",\n                \"GRAPHLIT_JWT_SECRET\": \"your-jwt-secret\",\n            }\n        }\n    }\n}\n```\n\nOptionally, you can configure the credentials for data connectors, such as Slack, Google Email and Notion.\nOnly GRAPHLIT_ORGANIZATION_ID, GRAPHLIT_ENVIRONMENT_ID and GRAPHLIT_JWT_SECRET are required.\n\n```\n{\n    \"mcpServers\": {\n        \"graphlit-mcp-server\": {\n            \"command\": \"npx\",\n            \"args\": [\n                \"-y\",\n                \"graphlit-mcp-server\"\n            ],\n            \"env\": {\n                \"GRAPHLIT_ORGANIZATION_ID\": \"your-organization-id\",\n                \"GRAPHLIT_ENVIRONMENT_ID\": \"your-environment-id\",\n                \"GRAPHLIT_JWT_SECRET\": \"your-jwt-secret\",\n                \"SLACK_BOT_TOKEN\": \"your-slack-bot-token\",\n                \"DISCORD_BOT_TOKEN\": \"your-discord-bot-token\",\n                \"TWITTER_TOKEN\": \"your-twitter-token\",\n                \"GOOGLE_EMAIL_REFRESH_TOKEN\": \"your-google-refresh-token\",\n                \"GOOGLE_EMAIL_CLIENT_ID\": \"your-google-client-id\",\n                \"GOOGLE_EMAIL_CLIENT_SECRET\": \"your-google-client-secret\",\n                \"LINEAR_API_KEY\": \"your-linear-api-key\",\n                \"GITHUB_PERSONAL_ACCESS_TOKEN\": \"your-github-pat\",\n                \"JIRA_EMAIL\": \"your-jira-email\",\n                \"JIRA_TOKEN\": \"your-jira-token\",\n                \"NOTION_API_KEY\": \"your-notion-api-key\"\n            }\n        }\n    }\n}\n```\n\nNOTE: when running 'npx' on Windows, you may need to explicitly call npx via the command prompt.\n\n```\n\"command\": \"C:\\\\Windows\\\\System32\\\\cmd.exe /c npx\"\n```\n\n## Support\n\nPlease refer to the [Graphlit API Documentation](https://docs.graphlit.dev/).\n\nFor support with the Graphlit MCP Server, please submit a [GitHub Issue](https://github.com/graphlit/graphlit-mcp-server/issues).\n\nFor further support with the Graphlit Platform, please join our [Discord](https://discord.gg/ygFmfjy3Qx) community.\n",
      "npm_url": "https://www.npmjs.com/package/graphlit-mcp-server",
      "npm_downloads": 39881,
      "keywords": [
        "graphlit",
        "feeds",
        "servers",
        "servers graphlit",
        "graphlit mcp",
        "feeds graphlit"
      ],
      "category": "official-servers"
    },
    "heroku--heroku-mcp-server": {
      "owner": "heroku",
      "name": "heroku-mcp-server",
      "url": "https://github.com/heroku/heroku-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/heroku.webp",
      "description": "Facilitates interaction with Heroku Platform resources using natural language, enabling users to manage applications, dynos, and add-ons through LLM-driven tools.",
      "stars": 69,
      "forks": 19,
      "license": "Apache License 2.0",
      "language": "TypeScript",
      "updated_at": "2025-10-01T23:05:03Z",
      "readme_content": "# heroku-mcp-server\n\n[![Install MCP Server](https://cursor.com/deeplink/mcp-install-dark.svg)](https://cursor.com/install-mcp?name=heroku&config=eyJjb21tYW5kIjoibnB4IC15IEBoZXJva3UvbWNwLXNlcnZlciIsImVudiI6eyJIRVJPS1VfQVBJX0tFWSI6IjxZT1VSX0hFUk9LVV9BVVRIX1RPS0VOPiJ9fQ%3D%3D)\n[![smithery badge](https://smithery.ai/badge/@heroku/heroku-mcp-server)](https://smithery.ai/server/@heroku/heroku-mcp-server)\n\n> The Heroku Platform MCP Server works on Common Runtime, Cedar Private and Shield Spaces, and Fir Private Spaces.\n\n## Prerequisites\n\n- **Heroku CLI** must be installed globally on your system, version **10.8.1 or higher**.\n  - [Install or upgrade the Heroku CLI](https://devcenter.heroku.com/articles/heroku-cli)\n\n## Deploy on Heroku\n\n[![Deploy](https://www.herokucdn.com/deploy/button.svg)](https://www.heroku.com/deploy?template=https://github.com/heroku/heroku-mcp-server)\n\n## Overview\n\nThe Heroku Platform MCP Server is a specialized Model Context Protocol (MCP) implementation designed to facilitate\nseamless interaction between large language models (LLMs) and the Heroku Platform. This server provides a robust set of\ntools and capabilities that enable LLMs to read, manage, and operate Heroku Platform resources.\n\nKey Features:\n\n- Direct interaction with Heroku Platform resources through LLM-driven tools\n- Secure and authenticated access to Heroku Platform APIs, leveraging the Heroku CLI\n- Natural language interface for Heroku Platform interactions\n\nNote: The Heroku Platform MCP Server is currently in early development. As we continue to enhance and refine the\nimplementation, the available functionality and tools may evolve. We welcome feedback and contributions to help shape\nthe future of this project.\n\n> **Note:** The [Heroku Platform MCP Server](https://devcenter.heroku.com/articles/heroku-mcp-server) requires the\n> Heroku CLI to be installed globally (v10.8.1+). Ensure you have the correct version by running `heroku --version`.\n\n## Configure the Heroku Platform MCP Server\n\nYou can configure Claude Desktop, Zed, Cursor, Windsurf, and other clients to work with the Heroku Platform MCP Server.\n\n### Configure the Heroku Platform MCP Server with `heroku mcp:start`\n\nUse `heroku mcp:start` to launch the Heroku Platform MCP Server. We recommend this method as it leverages your existing\nHeroku CLI authentication, so you don't need to set the\n[`HEROKU_API_KEY`](https://devcenter.heroku.com/articles/heroku-mcp-server#authentication) environment variable. The\n`heroku mcp:start` command is available in Heroku CLI version 10.8.1 and later.\n\nThere are several benefits to configuring with `heroku mcp:start`:\n\n- No need to manage or expose your Heroku API key\n- Uses your current Heroku CLI authentication context\n- Works seamlessly with supported clients\n\n#### Example configuration for [Claude Desktop](https://claude.ai/download):\n\n```json\n{\n  \"mcpServers\": {\n    \"heroku\": {\n      \"command\": \"heroku mcp:start\"\n    }\n  }\n}\n```\n\n#### Example configuration for [Zed](https://github.com/zed-industries/zed):\n\n```json\n{\n  \"context_servers\": {\n    \"heroku\": {\n      \"command\": {\n        \"path\": \"heroku\",\n        \"args\": [\"mcp:start\"]\n      }\n    }\n  }\n}\n```\n\n#### Example configuration for [Cursor](https://www.cursor.com/):\n\n```json\n{\n  \"mcpServers\": {\n    \"heroku\": {\n      \"command\": \"heroku mcp:start\"\n    }\n  }\n}\n```\n\n#### Example configuration for [Windsurf](https://www.windsurf.com/):\n\n```json\n{\n  \"mcpServers\": {\n    \"heroku\": {\n      \"command\": \"heroku mcp:start\"\n    }\n  }\n}\n```\n\n#### Example configuration for [Cline](https://cline.bot):\n\n```json\n{\n  \"mcpServers\": {\n    \"heroku\": {\n      \"command\": \"heroku mcp:start\"\n    }\n  }\n}\n```\n\n#### Example configuration for [VSCode](https://code.visualstudio.com/):\n\n```json\n{\n  \"mcp\": {\n    \"servers\": {\n      \"heroku\": {\n        \"type\": \"stdio\",\n        \"command\": \"heroku\",\n        \"args\": [\"mcp:start\"]\n      }\n    }\n  }\n}\n```\n\n#### Example configuration for [Trae](https://trae.ai):\n\n```json\n{\n  \"mcpServers\": {\n    \"heroku\": {\n      \"command\": \"heroku mcp:start\"\n    }\n  }\n}\n```\n\n> **Note:** When you use `heroku mcp:start`, the server authenticates using your current Heroku CLI session so you don't\n> need to set the `HEROKU_API_KEY` environment variable. We recommend you use `heroku mcp:start`, but if you prefer to\n> use an API key, you can use the alternate configuration below.\n\n### Configure the Heroku Platform MCP Server with `npx -y @heroku/mcp-server`\n\nYou can also launch the Heroku Platform MCP Server using the `npx -y @heroku/mcp-server` command. This method requires\nyou to set the [`HEROKU_API_KEY`](https://devcenter.heroku.com/articles/heroku-mcp-server#authentication) environment\nvariable with your Heroku\n[authorization token](https://devcenter.heroku.com/articles/authentication#retrieving-the-api-token).\n\n#### Generating the `HEROKU_API_KEY`\n\nGenerate a Heroku authorization token with one of these methods:\n\n- Use the Heroku CLI command:\n\n  ```sh\n    heroku authorizations:create\n  ```\n\n- Use an existing token in the CLI\n\n  ```sh\n    heroku auth:token\n  ```\n\n  Copy the token and use it as your `HEROKU_API_KEY` in the following steps.\n\n- In your [Heroku Dashboard](https://dashboard.heroku.com/account/applications):\n  1. Select your avatar, then select **Account Settings**.\n  2. Open the Applications tab.\n  3. Next to **Authorizations**, click **Create authorization**.\n\n#### Example configuration for [Claude Desktop](https://claude.ai/download):\n\n```json\n{\n  \"mcpServers\": {\n    \"heroku\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@heroku/mcp-server\"],\n      \"env\": {\n        \"HEROKU_API_KEY\": \"<YOUR_HEROKU_AUTH_TOKEN>\"\n      }\n    }\n  }\n}\n```\n\n#### Example configuration for [Zed](https://github.com/zed-industries/zed):\n\n```json\n{\n  \"context_servers\": {\n    \"heroku\": {\n      \"command\": {\n        \"path\": \"npx\",\n        \"args\": [\"-y\", \"@heroku/mcp-server\"],\n        \"env\": {\n          \"HEROKU_API_KEY\": \"<YOUR_HEROKU_AUTH_TOKEN>\"\n        }\n      }\n    }\n  }\n}\n```\n\n#### Example configuration for [Cursor](https://www.cursor.com/):\n\n```json\n{\n  \"mcpServers\": {\n    \"heroku\": {\n      \"command\": \"npx -y @heroku/mcp-server\",\n      \"env\": {\n        \"HEROKU_API_KEY\": \"<YOUR_HEROKU_AUTH_TOKEN>\"\n      }\n    }\n  }\n}\n```\n\n#### Example configuration for [Windsurf](https://www.windsurf.com/):\n\n```json\n{\n  \"mcpServers\": {\n    \"heroku\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@heroku/mcp-server\"],\n      \"env\": {\n        \"HEROKU_API_KEY\": \"<YOUR_HEROKU_AUTH_TOKEN>\"\n      }\n    }\n  }\n}\n```\n\n#### Example configuration for [Cline](https://cline.bot):\n\n```json\n{\n  \"mcpServers\": {\n    \"heroku\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@heroku/mcp-server\"],\n      \"env\": {\n        \"HEROKU_API_KEY\": \"<YOUR_HEROKU_AUTH_TOKEN>\"\n      }\n    }\n  }\n}\n```\n\n#### Example configuration for [VSCode](https://code.visualstudio.com/):\n\n```json\n{\n  \"mcp\": {\n    \"servers\": {\n      \"heroku\": {\n        \"type\": \"stdio\",\n        \"command\": \"npx\",\n        \"args\": [\"-y\", \"@heroku/mcp-server\"],\n        \"env\": {\n          \"HEROKU_API_KEY\": \"<YOUR_HEROKU_AUTH_TOKEN>\"\n        }\n      }\n    }\n  }\n}\n```\n\n#### Example configuration for [Trae](https://trae.ai):\n\n```json\n{\n  \"mcpServers\": {\n    \"heroku\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@heroku/mcp-server\"],\n      \"env\": {\n        \"HEROKU_API_KEY\": \"<YOUR_HEROKU_AUTH_TOKEN>\"\n      }\n    }\n  }\n}\n```\n\n> **Note:** When you use `npx -y @heroku/mcp-server`, you must set the `HEROKU_API_KEY` environment variable with your\n> Heroku authorization token.\n\n## Available Tools\n\n### Application Management\n\n- `list_apps` - List all Heroku apps. You can filter apps by personal, collaborator, team, or space.\n- `get_app_info` - Get detailed information about an app, including its configuration, dynos, and add-ons.\n- `create_app` - Create a new app with customizable settings for region, team, and space.\n- `rename_app` - Rename an existing app.\n- `transfer_app` - Transfer ownership of an app to another user or team.\n- `deploy_to_heroku` - Deploy projects to Heroku with an `app.json` configuration, supporting team deployments, private\n  spaces, and environment setups.\n- `deploy_one_off_dyno` - Execute code or commands in a sandboxed environment on a Heroku one-off dyno. Supports file\n  creation, network access, environment variables, and automatic cleanup. Ideal for running scripts, tests, or temporary\n  workloads.\n\n### Process & Dyno Management\n\n- `ps_list` - List all dynos for an app.\n- `ps_scale` - Scale the number of dynos up or down, or resize dynos.\n- `ps_restart` - Restart specific dynos, process types, or all dynos.\n\n### Add-ons\n\n- `list_addons` - List all add-ons for all apps or for a specific app.\n- `get_addon_info` - Get detailed information about a specific add-on.\n- `create_addon` - Provision a new add-on for an app.\n\n### Maintenance & Logs\n\n- `maintenance_on` - Enable maintenance mode for an app.\n- `maintenance_off` - Disable maintenance mode for an app.\n- `get_app_logs` - View application logs.\n\n### Pipeline Management\n\n- `pipelines_create` - Create a new pipeline.\n- `pipelines_promote` - Promote apps to the next stage in a pipeline.\n- `pipelines_list` - List available pipelines.\n- `pipelines_info` - Get detailed pipeline information.\n\n### Team & Space Management\n\n- `list_teams` - List teams you belong to.\n- `list_private_spaces` - List available spaces.\n\n### PostgreSQL Database Management\n\n- `pg_psql` - Execute SQL queries against the Heroku PostgreSQL database.\n- `pg_info` - Display detailed database information.\n- `pg_ps` - View active queries and execution details.\n- `pg_locks` - View database locks and identify blocking transactions.\n- `pg_outliers` - Identify resource-intensive queries.\n- `pg_credentials` - Manage database credentials and access.\n- `pg_kill` - Terminate specific database processes.\n- `pg_maintenance` - Show database maintenance information.\n- `pg_backups` - Manage database backups and schedules.\n- `pg_upgrade` - Upgrade PostgreSQL to a newer version.\n\n## Debugging\n\nYou can use the [MCP inspector](https://modelcontextprotocol.io/docs/tools/inspector) or the\n[VS Code Run and Debug function](https://code.visualstudio.com/docs/debugtest/debugging#_start-a-debugging-session) to\nrun and debug the server.\n\n1. Link the project as a global CLI using `npm link` from the project root.\n2. Build with `npm run build:dev` or watch for file changes and build automatically with `npm run build:watch`.\n\n### Use the MCP Inspector\n\nUse the MCP inspector with no breakpoints in the code:\n\n```\n# Breakpoints are not available\nnpx @modelcontextprotocol/inspector heroku-mcp-server\n```\n\nAlternatively, if you installed the package in a specific directory or are actively developing on the Heroku MCP server:\n\n```\ncd /path/to/servers\nnpx @modelcontextprotocol/inspector dist/index.js\n```\n\n### Use the VS Code Run and Debug Function\n\nUse the VS Code\n[Run and Debug launcher](https://code.visualstudio.com/docs/debugtest/debugging#_start-a-debugging-session) with fully\nfunctional breakpoints in the code:\n\n1. Locate and select the run debug.\n2. Select the configuration labeled \"`MCP Server Launcher`\" in the dropdown.\n3. Select the run/debug button.\n\n### VS Code / Cursor Debugging Setup\n\nTo set up local debugging with breakpoints:\n\n1. Store your Heroku auth token in the VS Code user settings:\n   - Open the Command Palette (Cmd/Ctrl + Shift + P).\n   - Type `Preferences: Open User Settings (JSON)`.\n   - Add the following snippet:\n\n   ```json\n   {\n     \"heroku.mcp.authToken\": \"your-token-here\"\n   }\n   ```\n\n2. Create or update `.vscode/launch.json`:\n\n   ```json\n   {\n     \"version\": \"0.2.0\",\n     \"configurations\": [\n       {\n         \"type\": \"node\",\n         \"request\": \"launch\",\n         \"name\": \"MCP Server Launcher\",\n         \"skipFiles\": [\"<node_internals>/**\"],\n         \"program\": \"${workspaceFolder}/node_modules/@modelcontextprotocol/inspector/bin/cli.js\",\n         \"outFiles\": [\"${workspaceFolder}/**/dist/**/*.js\"],\n         \"env\": {\n           \"HEROKU_API_KEY\": \"${config:heroku.mcp.authToken}\",\n           \"DEBUG\": \"true\"\n         },\n         \"args\": [\"heroku-mcp-server\"],\n         \"sourceMaps\": true,\n         \"console\": \"integratedTerminal\",\n         \"internalConsoleOptions\": \"neverOpen\",\n         \"preLaunchTask\": \"npm: build:watch\"\n       },\n       {\n         \"type\": \"node\",\n         \"request\": \"attach\",\n         \"name\": \"Attach to Debug Hook Process\",\n         \"port\": 9332,\n         \"skipFiles\": [\"<node_internals>/**\"],\n         \"sourceMaps\": true,\n         \"outFiles\": [\"${workspaceFolder}/dist/**/*.js\"]\n       },\n       {\n         \"type\": \"node\",\n         \"request\": \"attach\",\n         \"name\": \"Attach to REPL Process\",\n         \"port\": 9333,\n         \"skipFiles\": [\"<node_internals>/**\"],\n         \"sourceMaps\": true,\n         \"outFiles\": [\"${workspaceFolder}/dist/**/*.js\"]\n       }\n     ],\n     \"compounds\": [\n       {\n         \"name\": \"Attach to MCP Server\",\n         \"configurations\": [\"Attach to Debug Hook Process\", \"Attach to REPL Process\"]\n       }\n     ]\n   }\n   ```\n\n3. Create `.vscode/tasks.json`:\n\n   ```json\n   {\n     \"version\": \"2.0.0\",\n     \"tasks\": [\n       {\n         \"type\": \"npm\",\n         \"script\": \"build:watch\",\n         \"group\": {\n           \"kind\": \"build\",\n           \"isDefault\": true\n         },\n         \"problemMatcher\": [\"$tsc\"]\n       }\n     ]\n   }\n   ```\n\n4. (Optional) Set breakpoints in your TypeScript files.\n\n5. Press F5 or use the **`Run and Debug`** sidebar.\n\nNote: the debugger automatically builds your TypeScript files before launching.\n\n### Installing via Smithery\n\nTo install Heroku Platform MCP Server for Claude Desktop automatically via\n[Smithery](https://smithery.ai/server/@heroku/heroku-mcp-server):\n\n```bash\nnpx -y @smithery/cli install @heroku/heroku-mcp-server --client claude\n```\n\n## Environment Variables\n\nThe Heroku Platform MCP Server supports the following environment variables:\n\n### `HEROKU_API_KEY`\n\nYour Heroku authorization token. Required for authentication with the Heroku Platform.\n\n### `MCP_SERVER_REQUEST_TIMEOUT`\n\nTimeout in milliseconds for command execution. Defaults to 15000 (15 seconds) if not set.\n\nExample configuration with custom timeout:\n\n```json\n{\n  \"mcpServers\": {\n    \"heroku\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@heroku/mcp-server\"],\n      \"env\": {\n        \"HEROKU_API_KEY\": \"<YOUR_HEROKU_AUTH_TOKEN>\",\n        \"MCP_SERVER_REQUEST_TIMEOUT\": \"30000\"\n      }\n    }\n  }\n}\n```\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "heroku",
        "mcp",
        "servers",
        "servers heroku",
        "heroku platform",
        "heroku mcp"
      ],
      "category": "official-servers"
    },
    "hyperbrowserai--mcp": {
      "owner": "hyperbrowserai",
      "name": "mcp",
      "url": "https://github.com/hyperbrowserai/mcp",
      "imageUrl": "/freedevtools/mcp/pfp/hyperbrowserai.webp",
      "description": "Scrape and extract structured data from webpages and crawl internet resources. Provides access to general purpose browser agents for tasks involving web interactions.",
      "stars": 619,
      "forks": 51,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-09-30T07:34:29Z",
      "readme_content": "# Hyperbrowser MCP Server\n[![smithery badge](https://smithery.ai/badge/@hyperbrowserai/mcp)](https://smithery.ai/server/@hyperbrowserai/mcp)\n\n![Frame 5](https://github.com/user-attachments/assets/3309a367-e94b-418a-a047-1bf1ad549c0a)\n\nThis is Hyperbrowser's Model Context Protocol (MCP) Server. It provides various tools to scrape, extract structured data, and crawl webpages. It also provides easy access to general purpose browser agents like OpenAI's CUA, Anthropic's Claude Computer Use, and Browser Use.\n\nMore information about the Hyperbrowser can be found [here](https://docs.hyperbrowser.ai/). The hyperbrowser API supports a superset of features present in the mcp server.\n\nMore information about the Model Context Protocol can be found [here](https://modelcontextprotocol.io/introduction).\n\n## Table of Contents\n\n- [Installation](#installation)\n- [Usage](#usage)\n- [Tools](#tools)\n- [Configuration](#configuration)\n- [License](#license)\n\n## Installation\n\n### Manual Installation\nTo install the server, run:\n\n```bash\nnpx hyperbrowser-mcp <YOUR-HYPERBROWSER-API-KEY>\n```\n\n## Running on Cursor\nAdd to `~/.cursor/mcp.json` like this:\n```json\n{\n  \"mcpServers\": {\n    \"hyperbrowser\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"hyperbrowser-mcp\"],\n      \"env\": {\n        \"HYPERBROWSER_API_KEY\": \"YOUR-API-KEY\"\n      }\n    }\n  }\n}\n```\n\n## Running on Windsurf\nAdd to your `./codeium/windsurf/model_config.json` like this:\n```json\n{\n  \"mcpServers\": {\n    \"hyperbrowser\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"hyperbrowser-mcp\"],\n      \"env\": {\n        \"HYPERBROWSER_API_KEY\": \"YOUR-API-KEY\"\n      }\n    }\n  }\n}\n```\n\n### Development\n\nFor development purposes, you can run the server directly from the source code.\n\n1. Clone the repository:\n\n   ```sh\n   git clone git@github.com:hyperbrowserai/mcp.git hyperbrowser-mcp\n   cd hyperbrowser-mcp\n   ```\n\n2. Install dependencies:\n\n   ```sh\n   npm install # or yarn install\n   npm run build\n   ```\n\n3. Run the server:\n\n   ```sh\n   node dist/server.js\n   ```\n\n## Claude Desktop app\nThis is an example config for the Hyperbrowser MCP server for the Claude Desktop client.\n\n```json\n{\n  \"mcpServers\": {\n    \"hyperbrowser\": {\n      \"command\": \"npx\",\n      \"args\": [\"--yes\", \"hyperbrowser-mcp\"],\n      \"env\": {\n        \"HYPERBROWSER_API_KEY\": \"your-api-key\"\n      }\n    }\n  }\n}\n```\n\n\n## Tools\n* `scrape_webpage` - Extract formatted (markdown, screenshot etc) content from any webpage \n* `crawl_webpages` - Navigate through multiple linked pages and extract LLM-friendly formatted content\n* `extract_structured_data` - Convert messy HTML into structured JSON\n* `search_with_bing` - Query the web and get results with Bing search\n* `browser_use_agent` - Fast, lightweight browser automation with the Browser Use agent\n* `openai_computer_use_agent` - General-purpose automation using OpenAI‚Äôs CUA model\n* `claude_computer_use_agent` - Complex browser tasks using Claude computer use\n* `create_profile` - Creates a new persistent Hyperbrowser profile.\n* `delete_profile` - Deletes an existing persistent Hyperbrowser profile.\n* `list_profiles` - Lists existing persistent Hyperbrowser profiles.\n\n### Installing via Smithery\n\nTo install Hyperbrowser MCP Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@hyperbrowserai/mcp):\n\n```bash\nnpx -y @smithery/cli install @hyperbrowserai/mcp --client claude\n```\n\n## Resources\n\nThe server provides the documentation about hyperbrowser through the `resources` methods. Any client which can do discovery over resources has access to it.\n\n## License\n\nThis project is licensed under the MIT License.\n",
      "npm_url": "https://www.npmjs.com/package/mcp",
      "npm_downloads": 17508,
      "keywords": [
        "hyperbrowserai",
        "webpages",
        "web",
        "servers hyperbrowserai",
        "browser agents",
        "hyperbrowserai mcp"
      ],
      "category": "official-servers"
    },
    "integromat--make-mcp-server": {
      "owner": "integromat",
      "name": "make-mcp-server",
      "url": "https://github.com/integromat/make-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/integromat.webp",
      "description": "Connects to Make accounts to utilize On-Demand scenarios as callable tools for AI assistants, enabling seamless interaction with automation workflows. Returns output in structured JSON for AI processing.",
      "stars": 136,
      "forks": 35,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-09-29T06:26:01Z",
      "readme_content": "# Make MCP Server (legacy)\n\n**A modern, cloud-based version of the Make MCP Server is now available. For most use cases, we recommend using [this new version](https://developers.make.com/mcp-server).**\n\nA Model Context Protocol server that enables Make scenarios to be utilized as tools by AI assistants. This integration allows AI systems to trigger and interact with your Make automation workflows.\n\n## How It Works\n\nThe MCP server:\n\n-   Connects to your Make account and identifies all scenarios configured with \"On-Demand\" scheduling\n-   Parses and resolves input parameters for each scenario, providing AI assistants with meaningful parameter descriptions\n-   Allows AI assistants to invoke scenarios with appropriate parameters\n-   Returns scenario output as structured JSON, enabling AI assistants to properly interpret the results\n\n## Benefits\n\n-   Turn your Make scenarios into callable tools for AI assistants\n-   Maintain complex automation logic in Make while exposing functionality to AI systems\n-   Create bidirectional communication between your AI assistants and your existing automation workflows\n\n## Usage with Claude Desktop\n\n### Prerequisites\n\n-   NodeJS\n-   MCP Client (like Claude Desktop App)\n-   Make API Key with `scenarios:read` and `scenarios:run` scopes\n\n### Installation\n\nTo use this server with the Claude Desktop app, add the following configuration to the \"mcpServers\" section of your `claude_desktop_config.json`:\n\n```json\n{\n    \"mcpServers\": {\n        \"make\": {\n            \"command\": \"npx\",\n            \"args\": [\"-y\", \"@makehq/mcp-server\"],\n            \"env\": {\n                \"MAKE_API_KEY\": \"<your-api-key>\",\n                \"MAKE_ZONE\": \"<your-zone>\",\n                \"MAKE_TEAM\": \"<your-team-id>\"\n            }\n        }\n    }\n}\n```\n\n-   `MAKE_API_KEY` - You can generate an API key in your Make profile.\n-   `MAKE_ZONE` - The zone your organization is hosted in (e.g., `eu2.make.com`).\n-   `MAKE_TEAM` - You can find the ID in the URL of the Team page.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "mcp",
        "ai",
        "automation",
        "servers integromat",
        "mcp server",
        "integromat make"
      ],
      "category": "official-servers"
    },
    "its-dart--dart-mcp-server": {
      "owner": "its-dart",
      "name": "dart-mcp-server",
      "url": "https://github.com/its-dart/dart-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/its-dart.webp",
      "description": "A server implementation for the Model Context Protocol using Dart, enabling connection between AI models and various tools or data sources. It supports prompt management and resource templates, along with tools for task and document management.",
      "stars": 119,
      "forks": 27,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-10-01T20:06:57Z",
      "readme_content": "<div align=\"center\">\n  <h1>Dart MCP Server</h1>\n  <p>\n    <a href=\"https://npmjs.com/package/dart-mcp-server\"><img src=\"https://img.shields.io/npm/v/dart-mcp-server\" alt=\"NPM\"></a>\n    <a href=\"LICENSE\"><img src=\"https://img.shields.io/github/license/its-dart/dart-mcp-server\" alt=\"License\"></a>\n  </p>\n</div>\n\n[Dart](https://dartai.com?nr=1) is Project Management powered by AI.\n\n> [!WARNING]\n> The Dart local MCP server is deprecated in favor of the simplified and improved hosted Dart MCP server, which you can [configure with these instructions](https://help.dartai.com/en/articles/10733406).\n\n<details>\n<summary>Deprecated information</summary>\n`dart-mcp-server` is the official AI [Model Context Protocol (MCP)](https://github.com/modelcontextprotocol) server for Dart.\n\n- [Features](#features)\n  - [Prompts](#prompts)\n  - [Resource templates](#resource-templates)\n  - [Tools](#tools)\n    - [Task management](#task-management)\n    - [Document management](#document-management)\n- [Setup](#setup)\n  - [Find the MCP settings file for the client](#find-the-mcp-settings-file-for-the-client)\n    - [Claude Desktop](#claude-desktop)\n    - [Claude Code](#claude-code)\n    - [Cursor](#cursor)\n    - [Cline](#cline)\n    - [Windsurf](#windsurf)\n    - [Any other client](#any-other-client)\n  - [Set up the MCP server](#set-up-the-mcp-server)\n  - [Variant: setup with Docker](#variant-setup-with-docker)\n- [Help and Resources](#help-and-resources)\n- [Contributing](#contributing)\n- [License](#license)\n\n## Features\n\n### Prompts\n\nThe following prompts are available\n\n- `create-task` - Create a new task in Dart with title, description, status, priority, and assignee\n- `create-doc` - Create a new document in Dart with title, text content, and folder\n- `summarize-tasks` - Get a summary of tasks with optional filtering by status and assignee\n\nThese prompts make it easy for AI assistants to perform common actions in Dart without needing to understand the underlying API details.\n\n### Resource templates\n\nThe following resources are available\n\n- `dart-config:` - Configuration information about the user's space\n- `dart-task:///{taskId}` - Detailed information about specific tasks\n- `dart-doc:///{docId}` - Detailed information about specific docs\n\n### Tools\n\nThe following tools are available\n\n#### Task management\n\n- `get_config` - Get information about the user's space, including available assignees, dartboards, folders, statuses, tags, priorities, and sizes\n- `list_tasks` - List tasks with optional filtering by assignee, status, dartboard, priority, due date, and more\n- `create_task` - Create a new task with title, description, status, priority, size, dates, dartboard, assignees, tags, and parent task\n- `get_task` - Retrieve an existing task by its ID\n- `update_task` - Update an existing task's properties\n- `delete_task` - Move a task to the trash (recoverable)\n- `add_task_comment` - Add a comment to an existing task\n\n#### Document management\n\n- `list_docs` - List docs with optional filtering by folder, title, text content, and more\n- `create_doc` - Create a new doc with title, text content, and folder\n- `get_doc` - Retrieve an existing doc by its ID\n- `update_doc` - Update an existing doc's properties\n- `delete_doc` - Move a doc to the trash (recoverable)\n\nEach tool supports comprehensive input validation and returns structured JSON responses.\n\n## Setup\n\nThe easiest way to run the MCP server is with `npx`, but a Docker setup is also available.\n\n### Find the MCP settings file for the client\n\n#### Claude Desktop\n\n1. [Install Claude Desktop](https://claude.ai/download) as needed\n2. Open the config file by opening the Claude Desktop app, going into its Settings, opening the 'Developer' tab, and clicking the 'Edit Config' button\n3. Follow the 'Set up the MCP server' steps below\n\n#### Claude Code\n\n1. Install [Claude Code](https://docs.anthropic.com/en/docs/claude-code/getting-started) as needed\n2. Copy your authentication token from [your Dart profile](https://app.dartai.com/?settings=account)\n3. Run the following command, being sure to replace `dsa...` with your actual Dart token\n\n   ```bash\n   claude mcp add dart -e DART_TOKEN=dsa_... -- npx -y dart-mcp-server@latest\n   ```\n\n#### Cursor\n\n1. [Install Cursor](https://www.cursor.com/downloads) as needed\n2. Open the config file by opening Cursor, going into 'Cursor Settings' (not the normal VSCode IDE settings), opening the 'MCP' tab, and clicking the 'Add new global MCP server' button\n3. Follow the 'Set up the MCP server' steps below\n\n#### Cline\n\n1. [Install Cline](https://cline.bot/) in your IDE as needed\n2. Open the config file by opening your IDE, opening the Cline sidebar, clicking the 'MCP Servers' icon button that is second from left at the top, opening the 'Installed' tab, and clicking the 'Configure MCP Servers' button\n3. Follow the 'Set up the MCP server' steps below\n\n#### Windsurf\n\n1. [Install Windsurf](https://windsurf.com/download) as needed\n2. Open the config file by opening Windsurf, going into 'Windsurf Settings' (not the normal VSCode IDE settings), opening the 'Cascade' tab, and clicking the 'View raw config' button in the 'Model Context Protocol (MCP) Servers' section\n3. Follow the 'Set up the MCP server' steps below\n\n#### Any other client\n\n1. Find the MCP settings file, usually something like `[client]_mcp_config.json`\n2. Follow the 'Set up the MCP server' steps below\n\n### Set up the MCP server\n\n1. [Install npx](https://nodejs.org/en/download), which comes bundled with Node, as needed\n2. Copy your authentication token from [your Dart profile](https://app.dartai.com/?settings=account)\n3. Add the following to your MCP setup, being sure to replace `dsa...` with your actual Dart token\n\n   ```json\n   {\n     \"mcpServers\": {\n       \"Dart\": {\n         \"command\": \"npx\",\n         \"args\": [\"-y\", \"dart-mcp-server@latest\"],\n         \"env\": {\n           \"DART_TOKEN\": \"dsa_...\"\n         }\n       }\n     }\n   }\n   ```\n\n### Variant: setup with Docker\n\nIf the `npx` setup above does not work well, we also provide a Docker setup. Follow the instructions above to find the MCP settings file\n\n1. [Install Docker](https://www.docker.com/products/docker-desktop/) as needed\n2. Build the Docker container with `docker build -t mcp/dart .`\n3. Copy your authentication token from [your Dart profile](https://app.dartai.com/?settings=account)\n4. Add the following to your MCP setup, being sure to replace `dsa...` with your actual Dart token\n\n   ```json\n   {\n     \"mcpServers\": {\n       \"Dart\": {\n         \"command\": \"bash\",\n         \"args\": [\n           \"-c\",\n           \"docker rm -f dart-mcp >/dev/null 2>&1 || true; docker run -i --rm --name dart-mcp -e DART_TOKEN mcp/dart\"\n         ],\n         \"env\": {\n           \"DART_TOKEN\": \"dsa_...\"\n         }\n       }\n     }\n   }\n   ```\n\n## Help and Resources\n\n- [Homepage](https://dartai.com/?nr=1)\n- [Web App](https://app.dartai.com/)\n- [Help Center](https://help.dartai.com/)\n- [Bugs and Features](https://app.dartai.com/p/r/JFyPnhL9En61)\n- [Library Source](https://github.com/its-dart/dart-mcp-server/)\n- [Chat on Discord](https://discord.gg/RExv8jEkSh)\n- Email us at [support@dartai.com](mailto:support@dartai.com)\n\n## Contributing\n\nContributions are welcome! Please open an issue or submit a pull request.\n\n## License\n\nThis project is licensed under [the MIT License](LICENSE).\n</details>\n",
      "npm_url": "https://www.npmjs.com/package/dart-mcp-server",
      "npm_downloads": 15862,
      "keywords": [
        "dart",
        "server",
        "servers",
        "servers dart",
        "using dart",
        "dart mcp"
      ],
      "category": "official-servers"
    },
    "jamsocket--forevervm": {
      "owner": "jamsocket",
      "name": "forevervm",
      "url": "https://github.com/jamsocket/forevervm",
      "imageUrl": "/freedevtools/mcp/pfp/jamsocket.webp",
      "description": "Enable agents to execute Python code seamlessly within a REPL environment, allowing for on-the-fly script creation and execution. It enhances automation and coding capabilities directly within workflows.",
      "stars": 217,
      "forks": 20,
      "license": "MIT License",
      "language": "Rust",
      "updated_at": "2025-09-18T11:53:49Z",
      "readme_content": "[foreverVM](https://forevervm.com)\n==================================\n\n[![GitHub Repo stars](https://img.shields.io/github/stars/jamsocket/forevervm?style=social)](https://github.com/jamsocket/forevervm)\n[![Chat on Discord](https://img.shields.io/discord/939641163265232947?color=404eed&label=discord)](https://discord.gg/N5sEpsuhh9)\n\n| repo                                                | version                     |\n|-----------------------------------------------------|------------------------------|\n| [cli](https://github.com/jamsocket/forevervm) | [![npm](https://img.shields.io/npm/v/forevervm)](https://www.npmjs.com/package/forevervm) |\n| [sdk](https://github.com/jamsocket/forevervm) | [![npm](https://img.shields.io/npm/v/@forevervm/sdk)](https://www.npmjs.com/package/@forevervm/sdk) |\n\nforeverVM provides an API for running arbitrary, stateful Python code securely.\n\nThe core concepts in foreverVM are **machines** and **instructions**.\n\n**Machines** represent a stateful Python process. You interact with a machine by running **instructions**\n(Python statements and expressions) on it, and receiving the results. A machine processes one instruction\nat a time.\n\nGetting started\n---------------\n\nYou will need an API token (if you need one, reach out to [paul@jamsocket.com](mailto:paul@jamsocket.com)).\n\nThe easiest way to try out foreverVM is using the CLI. First, you will need to log in:\n\n```bash\nnpx forevervm login\n```\n\nOnce logged in, you can open a REPL interface with a new machine:\n\n```bash\nnpx forevervm repl\n```\n\nWhen foreverVM starts your machine, it gives it an ID that you can later use to reconnect to it. You can reconnect to a machine like this:\n\n```bash\nnpx forevervm repl [machine_name]\n```\n\nYou can list your machines (in reverse order of creation) like this:\n\n```bash\nnpx forevervm machine list\n```\n\nYou don't need to terminate machines -- foreverVM will automatically swap them from memory to disk when they are idle, and then\nautomatically swap them back when needed. This is what allows foreverVM to run repls ‚Äúforever‚Äù.\n\nUsing the API\n-------------\n\n```typescript\nimport { ForeverVM } from '@forevervm/sdk'\n\nconst token = process.env.FOREVERVM_TOKEN\nif (!token) {\n  throw new Error('FOREVERVM_TOKEN is not set')\n}\n\n// Initialize foreverVM\nconst fvm = new ForeverVM({ token })\n\n// Connect to a new machine.\nconst repl = fvm.repl()\n\n// Execute some code\nlet execResult = repl.exec('4 + 4')\n\n// Get the result\nconsole.log('result:', await execResult.result)\n\n// We can also print stdout and stderr\nexecResult = repl.exec('for i in range(10):\\n  print(i)')\n\nfor await (const output of execResult.output) {\n  console.log(output.stream, output.data)\n}\n\nprocess.exit(0)\n```\n\nWorking with Tags\n----------------\n\nYou can create machines with tags and filter machines by tags:\n\n```typescript\nimport { ForeverVM } from '@forevervm/sdk'\n\nconst fvm = new ForeverVM({ token: process.env.FOREVERVM_TOKEN })\n\n// Create a machine with tags\nconst machineResponse = await fvm.createMachine({\n  tags: { \n    env: 'production', \n    owner: 'user123',\n    project: 'demo'\n  }\n})\n\n// List machines filtered by tags\nconst productionMachines = await fvm.listMachines({\n  tags: { env: 'production' }\n})\n```\n\nMemory Limits\n----------------\n\nYou can create machines with memory limits by specifying the memory size in megabytes:\n\n```typescript\n// Create a machine with 512MB memory limit\nconst machineResponse = await fvm.createMachine({\n  memory_mb: 512,\n})\n```\n",
      "npm_url": "https://www.npmjs.com/package/forevervm",
      "npm_downloads": 2784,
      "keywords": [
        "jamsocket",
        "forevervm",
        "python",
        "jamsocket forevervm",
        "servers jamsocket",
        "forevervm enable"
      ],
      "category": "official-servers"
    },
    "makenotion--notion-mcp-server": {
      "owner": "makenotion",
      "name": "notion-mcp-server",
      "url": "https://github.com/makenotion/notion-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/makenotion.webp",
      "description": "Integrate applications with Notion by automating tasks and managing workspace contents through the Notion API. Support for various operations is provided while adhering to the limitations of the Notion API.",
      "stars": 3259,
      "forks": 310,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-10-02T09:00:06Z",
      "readme_content": "# Notion MCP Server\n\n> [!NOTE] \n> \n> We‚Äôve introduced **Notion MCP**, a remote MCP server with the following improvements:\n> - Easy installation via standard OAuth. No need to fiddle with JSON or API token anymore.\n> - Powerful tools tailored to AI agents. These tools are designed with optimized token consumption in mind.\n> \n> Learn more and try it out [here](https://developers.notion.com/docs/mcp)\n\n\n![notion-mcp-sm](https://github.com/user-attachments/assets/6c07003c-8455-4636-b298-d60ffdf46cd8)\n\nThis project implements an [MCP server](https://spec.modelcontextprotocol.io/) for the [Notion API](https://developers.notion.com/reference/intro). \n\n![mcp-demo](https://github.com/user-attachments/assets/e3ff90a7-7801-48a9-b807-f7dd47f0d3d6)\n\n### Installation\n\n#### 1. Setting up Integration in Notion:\nGo to [https://www.notion.so/profile/integrations](https://www.notion.so/profile/integrations) and create a new **internal** integration or select an existing one.\n\n\n\nWhile we limit the scope of Notion API's exposed (for example, you will not be able to delete databases via MCP), there is a non-zero risk to workspace data by exposing it to LLMs. Security-conscious users may want to further configure the Integration's _Capabilities_. \n\nFor example, you can create a read-only integration token by giving only \"Read content\" access from the \"Configuration\" tab:\n\n\n\n#### 2. Connecting content to integration:\nEnsure relevant pages and databases are connected to your integration.\n\nTo do this, visit the **Access** tab in your internal integration settings. Edit access and select the pages you'd like to use.\n\n\n\n\nAlternatively, you can grant page access individually. You'll need to visit the target page, and click on the 3 dots, and select \"Connect to integration\". \n\n\n\n#### 3. Adding MCP config to your client:\n\n##### Using npm:\n\n**Cursor & Claude:**\n\nAdd the following to your `.cursor/mcp.json` or `claude_desktop_config.json` (MacOS: `~/Library/Application\\ Support/Claude/claude_desktop_config.json`)\n\n**Option 1: Using NOTION_TOKEN (recommended)**\n```javascript\n{\n  \"mcpServers\": {\n    \"notionApi\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@notionhq/notion-mcp-server\"],\n      \"env\": {\n        \"NOTION_TOKEN\": \"ntn_****\"\n      }\n    }\n  }\n}\n```\n\n**Option 2: Using OPENAPI_MCP_HEADERS (for advanced use cases)**\n```javascript\n{\n  \"mcpServers\": {\n    \"notionApi\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@notionhq/notion-mcp-server\"],\n      \"env\": {\n        \"OPENAPI_MCP_HEADERS\": \"{\\\"Authorization\\\": \\\"Bearer ntn_****\\\", \\\"Notion-Version\\\": \\\"2022-06-28\\\" }\"\n      }\n    }\n  }\n}\n```\n\n**Zed**\n\nAdd the following to your `settings.json`\n\n```json\n{\n  \"context_servers\": {\n    \"some-context-server\": {\n      \"command\": {\n        \"path\": \"npx\",\n        \"args\": [\"-y\", \"@notionhq/notion-mcp-server\"],\n        \"env\": {\n          \"OPENAPI_MCP_HEADERS\": \"{\\\"Authorization\\\": \\\"Bearer ntn_****\\\", \\\"Notion-Version\\\": \\\"2022-06-28\\\" }\"\n        }\n      },\n      \"settings\": {}\n    }\n  }\n}\n```\n\n##### Using Docker:\n\nThere are two options for running the MCP server with Docker:\n\n###### Option 1: Using the official Docker Hub image:\n\nAdd the following to your `.cursor/mcp.json` or `claude_desktop_config.json`:\n\n**Using NOTION_TOKEN (recommended):**\n```javascript\n{\n  \"mcpServers\": {\n    \"notionApi\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"--rm\",\n        \"-i\",\n        \"-e\", \"NOTION_TOKEN\",\n        \"mcp/notion\"\n      ],\n      \"env\": {\n        \"NOTION_TOKEN\": \"ntn_****\"\n      }\n    }\n  }\n}\n```\n\n**Using OPENAPI_MCP_HEADERS (for advanced use cases):**\n```javascript\n{\n  \"mcpServers\": {\n    \"notionApi\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"--rm\",\n        \"-i\",\n        \"-e\", \"OPENAPI_MCP_HEADERS\",\n        \"mcp/notion\"\n      ],\n      \"env\": {\n        \"OPENAPI_MCP_HEADERS\": \"{\\\"Authorization\\\":\\\"Bearer ntn_****\\\",\\\"Notion-Version\\\":\\\"2022-06-28\\\"}\"\n      }\n    }\n  }\n}\n```\n\nThis approach:\n- Uses the official Docker Hub image\n- Properly handles JSON escaping via environment variables\n- Provides a more reliable configuration method\n\n###### Option 2: Building the Docker image locally:\n\nYou can also build and run the Docker image locally. First, build the Docker image:\n\n```bash\ndocker compose build\n```\n\nThen, add the following to your `.cursor/mcp.json` or `claude_desktop_config.json`:\n\n**Using NOTION_TOKEN (recommended):**\n```javascript\n{\n  \"mcpServers\": {\n    \"notionApi\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"--rm\",\n        \"-i\",\n        \"-e\",\n        \"NOTION_TOKEN=ntn_****\",\n        \"notion-mcp-server\"\n      ]\n    }\n  }\n}\n```\n\n**Using OPENAPI_MCP_HEADERS (for advanced use cases):**\n```javascript\n{\n  \"mcpServers\": {\n    \"notionApi\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"--rm\",\n        \"-i\",\n        \"-e\",\n        \"OPENAPI_MCP_HEADERS={\\\"Authorization\\\": \\\"Bearer ntn_****\\\", \\\"Notion-Version\\\": \\\"2022-06-28\\\"}\",\n        \"notion-mcp-server\"\n      ]\n    }\n  }\n}\n```\n\nDon't forget to replace `ntn_****` with your integration secret. Find it from your integration configuration tab:\n\n![Copying your Integration token from the Configuration tab in the developer portal](https://github.com/user-attachments/assets/67b44536-5333-49fa-809c-59581bf5370a)\n\n\n#### Installing via Smithery\n\n[![smithery badge](https://smithery.ai/badge/@makenotion/notion-mcp-server)](https://smithery.ai/server/@makenotion/notion-mcp-server)\n\nTo install Notion API Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@makenotion/notion-mcp-server):\n\n```bash\nnpx -y @smithery/cli install @makenotion/notion-mcp-server --client claude\n```\n\n### Transport Options\n\nThe Notion MCP Server supports two transport modes:\n\n#### STDIO Transport (Default)\nThe default transport mode uses standard input/output for communication. This is the standard MCP transport used by most clients like Claude Desktop.\n\n```bash\n# Run with default stdio transport\nnpx @notionhq/notion-mcp-server\n\n# Or explicitly specify stdio\nnpx @notionhq/notion-mcp-server --transport stdio\n```\n\n#### Streamable HTTP Transport\nFor web-based applications or clients that prefer HTTP communication, you can use the Streamable HTTP transport:\n\n```bash\n# Run with Streamable HTTP transport on port 3000 (default)\nnpx @notionhq/notion-mcp-server --transport http\n\n# Run on a custom port\nnpx @notionhq/notion-mcp-server --transport http --port 8080\n\n# Run with a custom authentication token\nnpx @notionhq/notion-mcp-server --transport http --auth-token \"your-secret-token\"\n```\n\nWhen using Streamable HTTP transport, the server will be available at `http://0.0.0.0:<port>/mcp`.\n\n##### Authentication\nThe Streamable HTTP transport requires bearer token authentication for security. You have three options:\n\n**Option 1: Auto-generated token (recommended for development)**\n```bash\nnpx @notionhq/notion-mcp-server --transport http\n```\nThe server will generate a secure random token and display it in the console:\n```\nGenerated auth token: a1b2c3d4e5f6789abcdef0123456789abcdef0123456789abcdef0123456789ab\nUse this token in the Authorization header: Bearer a1b2c3d4e5f6789abcdef0123456789abcdef0123456789abcdef0123456789ab\n```\n\n**Option 2: Custom token via command line (recommended for production)**\n```bash\nnpx @notionhq/notion-mcp-server --transport http --auth-token \"your-secret-token\"\n```\n\n**Option 3: Custom token via environment variable (recommended for production)**\n```bash\nAUTH_TOKEN=\"your-secret-token\" npx @notionhq/notion-mcp-server --transport http\n```\n\nThe command line argument `--auth-token` takes precedence over the `AUTH_TOKEN` environment variable if both are provided.\n\n##### Making HTTP Requests\nAll requests to the Streamable HTTP transport must include the bearer token in the Authorization header:\n\n```bash\n# Example request\ncurl -H \"Authorization: Bearer your-token-here\" \\\n     -H \"Content-Type: application/json\" \\\n     -H \"mcp-session-id: your-session-id\" \\\n     -d '{\"jsonrpc\": \"2.0\", \"method\": \"initialize\", \"params\": {}, \"id\": 1}' \\\n     http://localhost:3000/mcp\n```\n\n**Note:** Make sure to set either the `NOTION_TOKEN` environment variable (recommended) or the `OPENAPI_MCP_HEADERS` environment variable with your Notion integration token when using either transport mode.\n\n### Examples\n\n1. Using the following instruction\n```\nComment \"Hello MCP\" on page \"Getting started\"\n```\n\nAI will correctly plan two API calls, `v1/search` and `v1/comments`, to achieve the task\n\n2. Similarly, the following instruction will result in a new page named \"Notion MCP\" added to parent page \"Development\"\n```\nAdd a page titled \"Notion MCP\" to page \"Development\"\n```\n\n3. You may also reference content ID directly\n```\nGet the content of page 1a6b35e6e67f802fa7e1d27686f017f2\n```\n\n### Development\n\nBuild\n\n```\nnpm run build\n```\n\nExecute\n\n```\nnpx -y --prefix /path/to/local/notion-mcp-server @notionhq/notion-mcp-server\n```\n\nPublish\n\n```\nnpm publish --access public\n```",
      "npm_url": "https://www.npmjs.com/package/notion-mcp-server",
      "npm_downloads": 9753,
      "keywords": [
        "notion",
        "mcp",
        "managing",
        "notion api",
        "notion mcp",
        "notion automating"
      ],
      "category": "official-servers"
    },
    "metoro-io--metoro-mcp-server": {
      "owner": "metoro-io",
      "name": "metoro-mcp-server",
      "url": "https://github.com/metoro-io/metoro-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/metoro-io.webp",
      "description": "Interact with Kubernetes clusters through Metoro's APIs via the Claude Desktop App. Facilitates communication between AI models and Kubernetes management operations.",
      "stars": 43,
      "forks": 11,
      "license": "MIT License",
      "language": "Go",
      "updated_at": "2025-09-18T11:48:33Z",
      "readme_content": "<div align=\"center\">\n\n</div>\n<br/>\n<div align=\"center\">\n\n![GitHub stars](https://img.shields.io/github/stars/metoro-io/metoro-mcp-server?style=social)\n![GitHub forks](https://img.shields.io/github/forks/metoro-io/metoro-mcp-server?style=social)\n![GitHub issues](https://img.shields.io/github/issues/metoro-io/metoro-mcp-server)\n![GitHub pull requests](https://img.shields.io/github/issues-pr/metoro-io/metoro-mcp-server)\n![GitHub license](https://img.shields.io/github/license/metoro-io/metoro-mcp-server)\n![GitHub contributors](https://img.shields.io/github/contributors/metoro-io/metoro-mcp-server)\n![GitHub last commit](https://img.shields.io/github/last-commit/metoro-io/metoro-mcp-server)\n[![GoDoc](https://pkg.go.dev/badge/github.com/metoro-io/metoro-mcp-server.svg)](https://pkg.go.dev/github.com/metoro-io/metoro-mcp-server)\n[![Go Report Card](https://goreportcard.com/badge/github.com/metoro-io/metoro-mcp-server)](https://goreportcard.com/report/github.com/metoro-io/metoro-mcp-server)\n![Tests](https://github.com/metoro-io/metoro-mcp-server/actions/workflows/go-test.yml/badge.svg)\n\n</div>\n\n# metoro-mcp-server\nThis repository contains th Metoro MCP (Model Context Protocol) Server. This MCP Server allows you to interact with your Kubernetes cluster via the Claude Desktop App!\n\n## What is MCP (Model Context Protocol)? \nYou can read more about the Model Context Protocol here: https://modelcontextprotocol.io\n\nBut in a nutshell\n> The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you‚Äôre building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.\n\n## What is Metoro?\n[Metoro](https://metoro.io/) is an observability platform designed for microservices running in Kubernetes and uses eBPF based instrumentation to generate deep telemetry without code changes.\nThe data that is generated by the eBPF agents is sent to Metoro's backend to be stored and in the Metoro frontend using our apis.\n\nThis MCP server exposes those APIs to an LLM so you can ask your AI questions about your Kubernetes cluster.\n\n## Demo\n\nhttps://github.com/user-attachments/assets/b3f21e9a-45b8-4c17-8d8c-cff560d8694f\n\n## How can I use Metoro MCP Server? \n1. Install the [Claude Desktop App](https://claude.ai/download).\n2. Make sure you have [Golang](https://golang.org/dl/) installed. `brew install go` for mac or `sudo apt-get install golang` for ubuntu.\n3. Clone the repository: `git clone https://github.com/metoro-io/metoro-mcp-server.git`\n4. Navigate to the repository directory: `cd metoro-mcp-server`\n5. Build the server executable: `go build -o metoro-mcp-server`\n\n### If you already have a Metoro Account:\nCopy your auth token from your Metoro account in [Settings](https://us-east.metoro.io/settings) -> Users Settings. \nCreate a file in `~/Library/Application Support/Claude/claude_desktop_config.json` with the following contents:\n```json\n{\n  \"mcpServers\": {\n    \"metoro-mcp-server\": {\n      \"command\": \"<your path to Metoro MCP server go executable>/metoro-mcp-server\",\n      \"args\": [],\n      \"env\": {\n          \"METORO_AUTH_TOKEN\" : \"<your auth token>\",\n          \"METORO_API_URL\": \"https://us-east.metoro.io\"\n       }\n    }\n  }\n}\n```\n\n### If you don't have a Metoro Account:\nNo worries, you can still play around using the [Live Demo Cluster](https://demo.us-east.metoro.io/).\nThe included token is a demo token, publicly available for anyone to use.\n   Create a file in `~/Library/Application Support/Claude/claude_desktop_config.json` with the following contents:\n```json\n{\n  \"mcpServers\": {\n    \"metoro-mcp-server\": {\n      \"command\": \"<your path to Metoro MCP server go executable>/metoro-mcp-server\",\n      \"args\": [],\n      \"env\": {\n          \"METORO_AUTH_TOKEN\" : \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJjdXN0b21lcklkIjoiOThlZDU1M2QtYzY4ZC00MDRhLWFhZjItNDM2ODllNWJiMGUzIiwiZW1haWwiOiJ0ZXN0QGNocmlzYmF0dGFyYmVlLmNvbSIsImV4cCI6MTgyMTI0NzIzN30.7G6alDpcZh_OThYj293Jce5rjeOBqAhOlANR_Fl5auw\",\n          \"METORO_API_URL\": \"https://demo.us-east.metoro.io\"\n       }\n    }\n  }\n}\n```\n\n4. Once you are done editing `claude_desktop_config.json` save the file and restart Claude Desktop app.\n5. You should now see the Metoro MCP Server in the dropdown list of MCP Servers in the Claude Desktop App. You are ready to start using Metoro MCP Server with Claude Desktop App!\n\n## Built with\n\nThis server is built on top of our [Golang MCP SDK](https://github.com/metoro-io/mcp-golang).",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "kubernetes",
        "metoro",
        "clusters",
        "kubernetes clusters",
        "interact kubernetes",
        "metoro apis"
      ],
      "category": "official-servers"
    },
    "microsoft--playwright-mcp": {
      "owner": "microsoft",
      "name": "playwright-mcp",
      "url": "https://github.com/microsoft/playwright-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/microsoft.webp",
      "description": "Automate web interactions on web pages using structured accessibility data for reliable navigation and data extraction without the need for visual models. Enables LLMs to perform actions by leveraging Playwright's accessibility tree.",
      "stars": 21091,
      "forks": 1654,
      "license": "Apache License 2.0",
      "language": "TypeScript",
      "updated_at": "2025-10-02T09:12:41Z",
      "readme_content": "## Playwright MCP\n\nA Model Context Protocol (MCP) server that provides browser automation capabilities using [Playwright](https://playwright.dev). This server enables LLMs to interact with web pages through structured accessibility snapshots, bypassing the need for screenshots or visually-tuned models.\n\n### Key Features\n\n- **Fast and lightweight**. Uses Playwright's accessibility tree, not pixel-based input.\n- **LLM-friendly**. No vision models needed, operates purely on structured data.\n- **Deterministic tool application**. Avoids ambiguity common with screenshot-based approaches.\n\n### Requirements\n- Node.js 18 or newer\n- VS Code, Cursor, Windsurf, Claude Desktop, Goose or any other MCP client\n\n<!--\n// Generate using:\nnode utils/generate-links.js\n-->\n\n### Getting started\n\nFirst, install the Playwright MCP server with your client.\n\n**Standard config** works in most of the tools:\n\n```js\n{\n  \"mcpServers\": {\n    \"playwright\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"@playwright/mcp@latest\"\n      ]\n    }\n  }\n}\n```\n\n[<img src=\"https://img.shields.io/badge/VS_Code-VS_Code?style=flat-square&label=Install%20Server&color=0098FF\" alt=\"Install in VS Code\">](https://insiders.vscode.dev/redirect?url=vscode%3Amcp%2Finstall%3F%257B%2522name%2522%253A%2522playwright%2522%252C%2522command%2522%253A%2522npx%2522%252C%2522args%2522%253A%255B%2522%2540playwright%252Fmcp%2540latest%2522%255D%257D) [<img alt=\"Install in VS Code Insiders\" src=\"https://img.shields.io/badge/VS_Code_Insiders-VS_Code_Insiders?style=flat-square&label=Install%20Server&color=24bfa5\">](https://insiders.vscode.dev/redirect?url=vscode-insiders%3Amcp%2Finstall%3F%257B%2522name%2522%253A%2522playwright%2522%252C%2522command%2522%253A%2522npx%2522%252C%2522args%2522%253A%255B%2522%2540playwright%252Fmcp%2540latest%2522%255D%257D)\n\n\n<details>\n<summary>Claude Code</summary>\n\nUse the Claude Code CLI to add the Playwright MCP server:\n\n```bash\nclaude mcp add playwright npx @playwright/mcp@latest\n```\n</details>\n\n<details>\n<summary>Claude Desktop</summary>\n\nFollow the MCP install [guide](https://modelcontextprotocol.io/quickstart/user), use the standard config above.\n\n</details>\n\n<details>\n<summary>Codex</summary>\n\nCreate or edit the configuration file `~/.codex/config.toml` and add:\n\n```toml\n[mcp_servers.playwright]\ncommand = \"npx\"\nargs = [\"@playwright/mcp@latest\"]\n```\n\nFor more information, see the [Codex MCP documentation](https://github.com/openai/codex/blob/main/codex-rs/config.md#mcp_servers).\n\n</details>\n\n<details>\n<summary>Cursor</summary>\n\n#### Click the button to install:\n\n[<img src=\"https://cursor.com/deeplink/mcp-install-dark.svg\" alt=\"Install in Cursor\">](https://cursor.com/en/install-mcp?name=Playwright&config=eyJjb21tYW5kIjoibnB4IEBwbGF5d3JpZ2h0L21jcEBsYXRlc3QifQ%3D%3D)\n\n#### Or install manually:\n\nGo to `Cursor Settings` -> `MCP` -> `Add new MCP Server`. Name to your liking, use `command` type with the command `npx @playwright/mcp@latest`. You can also verify config or add command like arguments via clicking `Edit`.\n\n</details>\n\n<details>\n<summary>Gemini CLI</summary>\n\nFollow the MCP install [guide](https://github.com/google-gemini/gemini-cli/blob/main/docs/tools/mcp-server.md#configure-the-mcp-server-in-settingsjson), use the standard config above.\n\n</details>\n\n<details>\n<summary>Goose</summary>\n\n#### Click the button to install:\n\n[![Install in Goose](https://block.github.io/goose/img/extension-install-dark.svg)](https://block.github.io/goose/extension?cmd=npx&arg=%40playwright%2Fmcp%40latest&id=playwright&name=Playwright&description=Interact%20with%20web%20pages%20through%20structured%20accessibility%20snapshots%20using%20Playwright)\n\n#### Or install manually:\n\nGo to `Advanced settings` -> `Extensions` -> `Add custom extension`. Name to your liking, use type `STDIO`, and set the `command` to `npx @playwright/mcp`. Click \"Add Extension\".\n</details>\n\n<details>\n<summary>LM Studio</summary>\n\n#### Click the button to install:\n\n[![Add MCP Server playwright to LM Studio](https://files.lmstudio.ai/deeplink/mcp-install-light.svg)](https://lmstudio.ai/install-mcp?name=playwright&config=eyJjb21tYW5kIjoibnB4IiwiYXJncyI6WyJAcGxheXdyaWdodC9tY3BAbGF0ZXN0Il19)\n\n#### Or install manually:\n\nGo to `Program` in the right sidebar -> `Install` -> `Edit mcp.json`. Use the standard config above.\n</details>\n\n<details>\n<summary>opencode</summary>\n\nFollow the MCP Servers [documentation](https://opencode.ai/docs/mcp-servers/). For example in `~/.config/opencode/opencode.json`:\n\n```json\n{\n  \"$schema\": \"https://opencode.ai/config.json\",\n  \"mcp\": {\n    \"playwright\": {\n      \"type\": \"local\",\n      \"command\": [\n        \"npx\",\n        \"@playwright/mcp@latest\"\n      ],\n      \"enabled\": true\n    }\n  }\n}\n\n```\n</details>\n\n<details>\n<summary>Qodo Gen</summary>\n\nOpen [Qodo Gen](https://docs.qodo.ai/qodo-documentation/qodo-gen) chat panel in VSCode or IntelliJ ‚Üí Connect more tools ‚Üí + Add new MCP ‚Üí Paste the standard config above.\n\nClick <code>Save</code>.\n</details>\n\n<details>\n<summary>VS Code</summary>\n\n#### Click the button to install:\n\n[<img src=\"https://img.shields.io/badge/VS_Code-VS_Code?style=flat-square&label=Install%20Server&color=0098FF\" alt=\"Install in VS Code\">](https://insiders.vscode.dev/redirect?url=vscode%3Amcp%2Finstall%3F%257B%2522name%2522%253A%2522playwright%2522%252C%2522command%2522%253A%2522npx%2522%252C%2522args%2522%253A%255B%2522%2540playwright%252Fmcp%2540latest%2522%255D%257D) [<img alt=\"Install in VS Code Insiders\" src=\"https://img.shields.io/badge/VS_Code_Insiders-VS_Code_Insiders?style=flat-square&label=Install%20Server&color=24bfa5\">](https://insiders.vscode.dev/redirect?url=vscode-insiders%3Amcp%2Finstall%3F%257B%2522name%2522%253A%2522playwright%2522%252C%2522command%2522%253A%2522npx%2522%252C%2522args%2522%253A%255B%2522%2540playwright%252Fmcp%2540latest%2522%255D%257D)\n\n#### Or install manually:\n\nFollow the MCP install [guide](https://code.visualstudio.com/docs/copilot/chat/mcp-servers#_add-an-mcp-server), use the standard config above. You can also install the Playwright MCP server using the VS Code CLI:\n\n```bash\n# For VS Code\ncode --add-mcp '{\"name\":\"playwright\",\"command\":\"npx\",\"args\":[\"@playwright/mcp@latest\"]}'\n```\n\nAfter installation, the Playwright MCP server will be available for use with your GitHub Copilot agent in VS Code.\n</details>\n\n<details>\n<summary>Windsurf</summary>\n\nFollow Windsurf MCP [documentation](https://docs.windsurf.com/windsurf/cascade/mcp). Use the standard config above.\n\n</details>\n\n### Configuration\n\nPlaywright MCP server supports following arguments. They can be provided in the JSON configuration above, as a part of the `\"args\"` list:\n\n<!--- Options generated by update-readme.js -->\n\n```\n> npx @playwright/mcp@latest --help\n  --allowed-hosts <hosts...>            comma-separated list of hosts this\n                                        server is allowed to serve from.\n                                        Defaults to the host the server is bound\n                                        to. Pass '*' to disable the host check.\n  --allowed-origins <origins>           semicolon-separated list of origins to\n                                        allow the browser to request. Default is\n                                        to allow all.\n  --blocked-origins <origins>           semicolon-separated list of origins to\n                                        block the browser from requesting.\n                                        Blocklist is evaluated before allowlist.\n                                        If used without the allowlist, requests\n                                        not matching the blocklist are still\n                                        allowed.\n  --block-service-workers               block service workers\n  --browser <browser>                   browser or chrome channel to use,\n                                        possible values: chrome, firefox,\n                                        webkit, msedge.\n  --caps <caps>                         comma-separated list of additional\n                                        capabilities to enable, possible values:\n                                        vision, pdf.\n  --cdp-endpoint <endpoint>             CDP endpoint to connect to.\n  --cdp-header <headers...>             CDP headers to send with the connect\n                                        request, multiple can be specified.\n  --config <path>                       path to the configuration file.\n  --device <device>                     device to emulate, for example: \"iPhone\n                                        15\"\n  --executable-path <path>              path to the browser executable.\n  --extension                           Connect to a running browser instance\n                                        (Edge/Chrome only). Requires the\n                                        \"Playwright MCP Bridge\" browser\n                                        extension to be installed.\n  --grant-permissions <permissions...>  List of permissions to grant to the\n                                        browser context, for example\n                                        \"geolocation\", \"clipboard-read\",\n                                        \"clipboard-write\".\n  --headless                            run browser in headless mode, headed by\n                                        default\n  --host <host>                         host to bind server to. Default is\n                                        localhost. Use 0.0.0.0 to bind to all\n                                        interfaces.\n  --ignore-https-errors                 ignore https errors\n  --init-script <path...>               path to JavaScript file to add as an\n                                        initialization script. The script will\n                                        be evaluated in every page before any of\n                                        the page's scripts. Can be specified\n                                        multiple times.\n  --isolated                            keep the browser profile in memory, do\n                                        not save it to disk.\n  --image-responses <mode>              whether to send image responses to the\n                                        client. Can be \"allow\" or \"omit\",\n                                        Defaults to \"allow\".\n  --no-sandbox                          disable the sandbox for all process\n                                        types that are normally sandboxed.\n  --output-dir <path>                   path to the directory for output files.\n  --port <port>                         port to listen on for SSE transport.\n  --proxy-bypass <bypass>               comma-separated domains to bypass proxy,\n                                        for example\n                                        \".com,chromium.org,.domain.com\"\n  --proxy-server <proxy>                specify proxy server, for example\n                                        \"http://myproxy:3128\" or\n                                        \"socks5://myproxy:8080\"\n  --save-session                        Whether to save the Playwright MCP\n                                        session into the output directory.\n  --save-trace                          Whether to save the Playwright Trace of\n                                        the session into the output directory.\n  --save-video <size>                   Whether to save the video of the session\n                                        into the output directory. For example\n                                        \"--save-video=800x600\"\n  --secrets <path>                      path to a file containing secrets in the\n                                        dotenv format\n  --shared-browser-context              reuse the same browser context between\n                                        all connected HTTP clients.\n  --storage-state <path>                path to the storage state file for\n                                        isolated sessions.\n  --timeout-action <timeout>            specify action timeout in milliseconds,\n                                        defaults to 5000ms\n  --timeout-navigation <timeout>        specify navigation timeout in\n                                        milliseconds, defaults to 60000ms\n  --user-agent <ua string>              specify user agent string\n  --user-data-dir <path>                path to the user data directory. If not\n                                        specified, a temporary directory will be\n                                        created.\n  --viewport-size <size>                specify browser viewport size in pixels,\n                                        for example \"1280x720\"\n```\n\n<!--- End of options generated section -->\n\n### User profile\n\nYou can run Playwright MCP with persistent profile like a regular browser (default), in isolated contexts for testing sessions, or connect to your existing browser using the browser extension.\n\n**Persistent profile**\n\nAll the logged in information will be stored in the persistent profile, you can delete it between sessions if you'd like to clear the offline state.\nPersistent profile is located at the following locations and you can override it with the `--user-data-dir` argument.\n\n```bash\n# Windows\n%USERPROFILE%\\AppData\\Local\\ms-playwright\\mcp-{channel}-profile\n\n# macOS\n- ~/Library/Caches/ms-playwright/mcp-{channel}-profile\n\n# Linux\n- ~/.cache/ms-playwright/mcp-{channel}-profile\n```\n\n**Isolated**\n\nIn the isolated mode, each session is started in the isolated profile. Every time you ask MCP to close the browser,\nthe session is closed and all the storage state for this session is lost. You can provide initial storage state\nto the browser via the config's `contextOptions` or via the `--storage-state` argument. Learn more about the storage\nstate [here](https://playwright.dev/docs/auth).\n\n```js\n{\n  \"mcpServers\": {\n    \"playwright\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"@playwright/mcp@latest\",\n        \"--isolated\",\n        \"--storage-state={path/to/storage.json}\"\n      ]\n    }\n  }\n}\n```\n\n**Browser Extension**\n\nThe Playwright MCP Chrome Extension allows you to connect to existing browser tabs and leverage your logged-in sessions and browser state. See [extension/README.md](extension/README.md) for installation and setup instructions.\n\n### Configuration file\n\nThe Playwright MCP server can be configured using a JSON configuration file. You can specify the configuration file\nusing the `--config` command line option:\n\n```bash\nnpx @playwright/mcp@latest --config path/to/config.json\n```\n\n<details>\n<summary>Configuration file schema</summary>\n\n```typescript\n{\n  // Browser configuration\n  browser?: {\n    // Browser type to use (chromium, firefox, or webkit)\n    browserName?: 'chromium' | 'firefox' | 'webkit';\n\n    // Keep the browser profile in memory, do not save it to disk.\n    isolated?: boolean;\n\n    // Path to user data directory for browser profile persistence\n    userDataDir?: string;\n\n    // Browser launch options (see Playwright docs)\n    // @see https://playwright.dev/docs/api/class-browsertype#browser-type-launch\n    launchOptions?: {\n      channel?: string;        // Browser channel (e.g. 'chrome')\n      headless?: boolean;      // Run in headless mode\n      executablePath?: string; // Path to browser executable\n      // ... other Playwright launch options\n    };\n\n    // Browser context options\n    // @see https://playwright.dev/docs/api/class-browser#browser-new-context\n    contextOptions?: {\n      viewport?: { width: number, height: number };\n      // ... other Playwright context options\n    };\n\n    // CDP endpoint for connecting to existing browser\n    cdpEndpoint?: string;\n\n    // Remote Playwright server endpoint\n    remoteEndpoint?: string;\n  },\n\n  // Server configuration\n  server?: {\n    port?: number;  // Port to listen on\n    host?: string;  // Host to bind to (default: localhost)\n  },\n\n  // List of additional capabilities\n  capabilities?: Array<\n    'tabs' |    // Tab management\n    'install' | // Browser installation\n    'pdf' |     // PDF generation\n    'vision' |  // Coordinate-based interactions\n  >;\n\n  // Directory for output files\n  outputDir?: string;\n\n  // Network configuration\n  network?: {\n    // List of origins to allow the browser to request. Default is to allow all. Origins matching both `allowedOrigins` and `blockedOrigins` will be blocked.\n    allowedOrigins?: string[];\n\n    // List of origins to block the browser to request. Origins matching both `allowedOrigins` and `blockedOrigins` will be blocked.\n    blockedOrigins?: string[];\n  };\n \n  /**\n   * Whether to send image responses to the client. Can be \"allow\" or \"omit\". \n   * Defaults to \"allow\".\n   */\n  imageResponses?: 'allow' | 'omit';\n}\n```\n</details>\n\n### Standalone MCP server\n\nWhen running headed browser on system w/o display or from worker processes of the IDEs,\nrun the MCP server from environment with the DISPLAY and pass the `--port` flag to enable HTTP transport.\n\n```bash\nnpx @playwright/mcp@latest --port 8931\n```\n\nAnd then in MCP client config, set the `url` to the HTTP endpoint:\n\n```js\n{\n  \"mcpServers\": {\n    \"playwright\": {\n      \"url\": \"http://localhost:8931/mcp\"\n    }\n  }\n}\n```\n\n<details>\n<summary><b>Docker</b></summary>\n\n**NOTE:** The Docker implementation only supports headless chromium at the moment.\n\n```js\n{\n  \"mcpServers\": {\n    \"playwright\": {\n      \"command\": \"docker\",\n      \"args\": [\"run\", \"-i\", \"--rm\", \"--init\", \"--pull=always\", \"mcr.microsoft.com/playwright/mcp\"]\n    }\n  }\n}\n```\n\nOr If you prefer to run the container as a long-lived service instead of letting the MCP client spawn it, use:\n\n```\ndocker run -d -i --rm --init --pull=always \\\n  --entrypoint node \\\n  --name playwright \\\n  -p 8931:8931 \\\n  mcr.microsoft.com/playwright/mcp \\\n  cli.js --headless --browser chromium --no-sandbox --port 8931\n```\n\nThe server will listen on host port **8931** and can be reached by any MCP client.  \n\nYou can build the Docker image yourself.\n\n```\ndocker build -t mcr.microsoft.com/playwright/mcp .\n```\n</details>\n\n<details>\n<summary><b>Programmatic usage</b></summary>\n\n```js\nimport http from 'http';\n\nimport { createConnection } from '@playwright/mcp';\nimport { SSEServerTransport } from '@modelcontextprotocol/sdk/server/sse.js';\n\nhttp.createServer(async (req, res) => {\n  // ...\n\n  // Creates a headless Playwright MCP server with SSE transport\n  const connection = await createConnection({ browser: { launchOptions: { headless: true } } });\n  const transport = new SSEServerTransport('/messages', res);\n  await connection.connect(transport);\n\n  // ...\n});\n```\n</details>\n\n### Tools\n\n<!--- Tools generated by update-readme.js -->\n\n<details>\n<summary><b>Core automation</b></summary>\n\n<!-- NOTE: This has been generated via update-readme.js -->\n\n- **browser_click**\n  - Title: Click\n  - Description: Perform click on a web page\n  - Parameters:\n    - `element` (string): Human-readable element description used to obtain permission to interact with the element\n    - `ref` (string): Exact target element reference from the page snapshot\n    - `doubleClick` (boolean, optional): Whether to perform a double click instead of a single click\n    - `button` (string, optional): Button to click, defaults to left\n    - `modifiers` (array, optional): Modifier keys to press\n  - Read-only: **false**\n\n<!-- NOTE: This has been generated via update-readme.js -->\n\n- **browser_close**\n  - Title: Close browser\n  - Description: Close the page\n  - Parameters: None\n  - Read-only: **true**\n\n<!-- NOTE: This has been generated via update-readme.js -->\n\n- **browser_console_messages**\n  - Title: Get console messages\n  - Description: Returns all console messages\n  - Parameters:\n    - `onlyErrors` (boolean, optional): Only return error messages\n  - Read-only: **true**\n\n<!-- NOTE: This has been generated via update-readme.js -->\n\n- **browser_drag**\n  - Title: Drag mouse\n  - Description: Perform drag and drop between two elements\n  - Parameters:\n    - `startElement` (string): Human-readable source element description used to obtain the permission to interact with the element\n    - `startRef` (string): Exact source element reference from the page snapshot\n    - `endElement` (string): Human-readable target element description used to obtain the permission to interact with the element\n    - `endRef` (string): Exact target element reference from the page snapshot\n  - Read-only: **false**\n\n<!-- NOTE: This has been generated via update-readme.js -->\n\n- **browser_evaluate**\n  - Title: Evaluate JavaScript\n  - Description: Evaluate JavaScript expression on page or element\n  - Parameters:\n    - `function` (string): () => { /* code */ } or (element) => { /* code */ } when element is provided\n    - `element` (string, optional): Human-readable element description used to obtain permission to interact with the element\n    - `ref` (string, optional): Exact target element reference from the page snapshot\n  - Read-only: **false**\n\n<!-- NOTE: This has been generated via update-readme.js -->\n\n- **browser_file_upload**\n  - Title: Upload files\n  - Description: Upload one or multiple files\n  - Parameters:\n    - `paths` (array, optional): The absolute paths to the files to upload. Can be single file or multiple files. If omitted, file chooser is cancelled.\n  - Read-only: **false**\n\n<!-- NOTE: This has been generated via update-readme.js -->\n\n- **browser_fill_form**\n  - Title: Fill form\n  - Description: Fill multiple form fields\n  - Parameters:\n    - `fields` (array): Fields to fill in\n  - Read-only: **false**\n\n<!-- NOTE: This has been generated via update-readme.js -->\n\n- **browser_handle_dialog**\n  - Title: Handle a dialog\n  - Description: Handle a dialog\n  - Parameters:\n    - `accept` (boolean): Whether to accept the dialog.\n    - `promptText` (string, optional): The text of the prompt in case of a prompt dialog.\n  - Read-only: **false**\n\n<!-- NOTE: This has been generated via update-readme.js -->\n\n- **browser_hover**\n  - Title: Hover mouse\n  - Description: Hover over element on page\n  - Parameters:\n    - `element` (string): Human-readable element description used to obtain permission to interact with the element\n    - `ref` (string): Exact target element reference from the page snapshot\n  - Read-only: **true**\n\n<!-- NOTE: This has been generated via update-readme.js -->\n\n- **browser_navigate**\n  - Title: Navigate to a URL\n  - Description: Navigate to a URL\n  - Parameters:\n    - `url` (string): The URL to navigate to\n  - Read-only: **false**\n\n<!-- NOTE: This has been generated via update-readme.js -->\n\n- **browser_navigate_back**\n  - Title: Go back\n  - Description: Go back to the previous page\n  - Parameters: None\n  - Read-only: **true**\n\n<!-- NOTE: This has been generated via update-readme.js -->\n\n- **browser_network_requests**\n  - Title: List network requests\n  - Description: Returns all network requests since loading the page\n  - Parameters: None\n  - Read-only: **true**\n\n<!-- NOTE: This has been generated via update-readme.js -->\n\n- **browser_press_key**\n  - Title: Press a key\n  - Description: Press a key on the keyboard\n  - Parameters:\n    - `key` (string): Name of the key to press or a character to generate, such as `ArrowLeft` or `a`\n  - Read-only: **false**\n\n<!-- NOTE: This has been generated via update-readme.js -->\n\n- **browser_resize**\n  - Title: Resize browser window\n  - Description: Resize the browser window\n  - Parameters:\n    - `width` (number): Width of the browser window\n    - `height` (number): Height of the browser window\n  - Read-only: **true**\n\n<!-- NOTE: This has been generated via update-readme.js -->\n\n- **browser_select_option**\n  - Title: Select option\n  - Description: Select an option in a dropdown\n  - Parameters:\n    - `element` (string): Human-readable element description used to obtain permission to interact with the element\n    - `ref` (string): Exact target element reference from the page snapshot\n    - `values` (array): Array of values to select in the dropdown. This can be a single value or multiple values.\n  - Read-only: **false**\n\n<!-- NOTE: This has been generated via update-readme.js -->\n\n- **browser_snapshot**\n  - Title: Page snapshot\n  - Description: Capture accessibility snapshot of the current page, this is better than screenshot\n  - Parameters: None\n  - Read-only: **true**\n\n<!-- NOTE: This has been generated via update-readme.js -->\n\n- **browser_take_screenshot**\n  - Title: Take a screenshot\n  - Description: Take a screenshot of the current page. You can't perform actions based on the screenshot, use browser_snapshot for actions.\n  - Parameters:\n    - `type` (string, optional): Image format for the screenshot. Default is png.\n    - `filename` (string, optional): File name to save the screenshot to. Defaults to `page-{timestamp}.{png|jpeg}` if not specified.\n    - `element` (string, optional): Human-readable element description used to obtain permission to screenshot the element. If not provided, the screenshot will be taken of viewport. If element is provided, ref must be provided too.\n    - `ref` (string, optional): Exact target element reference from the page snapshot. If not provided, the screenshot will be taken of viewport. If ref is provided, element must be provided too.\n    - `fullPage` (boolean, optional): When true, takes a screenshot of the full scrollable page, instead of the currently visible viewport. Cannot be used with element screenshots.\n  - Read-only: **true**\n\n<!-- NOTE: This has been generated via update-readme.js -->\n\n- **browser_type**\n  - Title: Type text\n  - Description: Type text into editable element\n  - Parameters:\n    - `element` (string): Human-readable element description used to obtain permission to interact with the element\n    - `ref` (string): Exact target element reference from the page snapshot\n    - `text` (string): Text to type into the element\n    - `submit` (boolean, optional): Whether to submit entered text (press Enter after)\n    - `slowly` (boolean, optional): Whether to type one character at a time. Useful for triggering key handlers in the page. By default entire text is filled in at once.\n  - Read-only: **false**\n\n<!-- NOTE: This has been generated via update-readme.js -->\n\n- **browser_wait_for**\n  - Title: Wait for\n  - Description: Wait for text to appear or disappear or a specified time to pass\n  - Parameters:\n    - `time` (number, optional): The time to wait in seconds\n    - `text` (string, optional): The text to wait for\n    - `textGone` (string, optional): The text to wait for to disappear\n  - Read-only: **true**\n\n</details>\n\n<details>\n<summary><b>Tab management</b></summary>\n\n<!-- NOTE: This has been generated via update-readme.js -->\n\n- **browser_tabs**\n  - Title: Manage tabs\n  - Description: List, create, close, or select a browser tab.\n  - Parameters:\n    - `action` (string): Operation to perform\n    - `index` (number, optional): Tab index, used for close/select. If omitted for close, current tab is closed.\n  - Read-only: **false**\n\n</details>\n\n<details>\n<summary><b>Browser installation</b></summary>\n\n<!-- NOTE: This has been generated via update-readme.js -->\n\n- **browser_install**\n  - Title: Install the browser specified in the config\n  - Description: Install the browser specified in the config. Call this if you get an error about the browser not being installed.\n  - Parameters: None\n  - Read-only: **false**\n\n</details>\n\n<details>\n<summary><b>Coordinate-based (opt-in via --caps=vision)</b></summary>\n\n<!-- NOTE: This has been generated via update-readme.js -->\n\n- **browser_mouse_click_xy**\n  - Title: Click\n  - Description: Click left mouse button at a given position\n  - Parameters:\n    - `element` (string): Human-readable element description used to obtain permission to interact with the element\n    - `x` (number): X coordinate\n    - `y` (number): Y coordinate\n  - Read-only: **false**\n\n<!-- NOTE: This has been generated via update-readme.js -->\n\n- **browser_mouse_drag_xy**\n  - Title: Drag mouse\n  - Description: Drag left mouse button to a given position\n  - Parameters:\n    - `element` (string): Human-readable element description used to obtain permission to interact with the element\n    - `startX` (number): Start X coordinate\n    - `startY` (number): Start Y coordinate\n    - `endX` (number): End X coordinate\n    - `endY` (number): End Y coordinate\n  - Read-only: **false**\n\n<!-- NOTE: This has been generated via update-readme.js -->\n\n- **browser_mouse_move_xy**\n  - Title: Move mouse\n  - Description: Move mouse to a given position\n  - Parameters:\n    - `element` (string): Human-readable element description used to obtain permission to interact with the element\n    - `x` (number): X coordinate\n    - `y` (number): Y coordinate\n  - Read-only: **true**\n\n</details>\n\n<details>\n<summary><b>PDF generation (opt-in via --caps=pdf)</b></summary>\n\n<!-- NOTE: This has been generated via update-readme.js -->\n\n- **browser_pdf_save**\n  - Title: Save as PDF\n  - Description: Save page as PDF\n  - Parameters:\n    - `filename` (string, optional): File name to save the pdf to. Defaults to `page-{timestamp}.pdf` if not specified.\n  - Read-only: **true**\n\n</details>\n\n<details>\n<summary><b>Verify (opt-in via --caps=verify)</b></summary>\n\n</details>\n\n<details>\n<summary><b>Tracing (opt-in via --caps=tracing)</b></summary>\n\n<!-- NOTE: This has been generated via update-readme.js -->\n\n- **browser_start_tracing**\n  - Title: Start tracing\n  - Description: Start trace recording\n  - Parameters: None\n  - Read-only: **true**\n\n<!-- NOTE: This has been generated via update-readme.js -->\n\n- **browser_stop_tracing**\n  - Title: Stop tracing\n  - Description: Stop trace recording\n  - Parameters: None\n  - Read-only: **true**\n\n</details>\n\n\n<!--- End of tools generated section -->\n",
      "npm_url": "https://www.npmjs.com/package/playwright-mcp",
      "npm_downloads": 88372,
      "keywords": [
        "accessibility",
        "playwright",
        "pages",
        "playwright accessibility",
        "structured accessibility",
        "accessibility data"
      ],
      "category": "official-servers"
    },
    "needle-ai--needle-mcp": {
      "owner": "needle-ai",
      "name": "needle-mcp",
      "url": "https://github.com/needle-ai/needle-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/needle-ai.webp",
      "description": "Manage documents and perform searches utilizing Needle's capabilities through Claude's Desktop Application.",
      "stars": 81,
      "forks": 21,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-16T07:46:37Z",
      "readme_content": "# Build Agents with Needle MCP Server\n\n[![smithery badge](https://smithery.ai/badge/needle-mcp)](https://smithery.ai/server/needle-mcp)\n\n![Screenshot of Feature - Claude](https://github.com/user-attachments/assets/a7286901-e7be-4efe-afd9-72021dce03d4)\n\nMCP (Model Context Protocol) server to manage documents and perform searches using [Needle](https://needle.app) through Claude's Desktop Application.\n\n<a href=\"https://glama.ai/mcp/servers/5jw1t7hur2\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/5jw1t7hur2/badge\" alt=\"Needle Server MCP server\" />\n</a>\n\n## Table of Contents\n\n- [Overview](#overview)\n- [Features](#features)\n- [Usage](#usage)\n  - [Commands in Claude Desktop](#commands-in-claude-desktop)\n  - [Result in Needle](#result-in-needle)\n- [Installation](#installation)\n- [Video Explanation](#youtube-video-explanation)\n\n---\n\n## Overview\n\nNeedle MCP Server allows you to:\n\n- Organize and store documents for quick retrieval.\n- Perform powerful searches via Claude's large language model.\n- Integrate seamlessly with the Needle ecosystem for advanced document management.\n\nMCP (Model Context Protocol) standardizes the way LLMs connect to external data sources. You can use Needle MCP Server to easily enable semantic search tools in your AI applications, making data buried in PDFs, DOCX, XLSX, and other files instantly accessible by LLMs.\n\n**We recommend using our remote MCP server** for the best experience - no local setup required.\n\n---\n\n## Features\n\n- **Document Management:** Easily add and organize documents on the server.\n- **Search & Retrieval:** Claude-based natural language search for quick answers.\n- **Easy Integration:** Works with [Claude Desktop](#commands-in-claude-desktop) and Needle collections.\n\n---\n\n## Usage\n\n### Commands in Claude Desktop\n\nBelow is an example of how the commands can be used in Claude Desktop to interact with the server:\n\n![Using commands in Claude Desktop](https://github.com/user-attachments/assets/9e0ce522-6675-46d9-9bfb-3162d214625b)\n\n1. **Open Claude Desktop** and connect to the Needle MCP Server.  \n2. **Use simple text commands** to search, retrieve, or modify documents.  \n3. **Review search results** returned by Claude in a user-friendly interface.\n\n### Result in Needle\n\nhttps://github.com/user-attachments/assets/0235e893-af96-4920-8364-1e86f73b3e6c\n\n---\n\n## Youtube Video Explanation\n\nFor a full walkthrough on using the Needle MCP Server with Claude and Claude Desktop, watch this [YouTube explanation video](https://youtu.be/nVrRYp9NZYg).\n\n---\n\n## Installation\n\n### 1. Remote MCP Server (Recommended)\n\n**Claude Desktop Config**\n\nCreate or update your config file:\n- For MacOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\n- For Windows: `%APPDATA%/Claude/claude_desktop_config.json`\n\n```json\n{\n  \"mcpServers\": {\n    \"needle\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"mcp-remote\",\n        \"https://mcp.needle.app/mcp\",\n        \"--header\",\n        \"Authorization:Bearer ${NEEDLE_API_KEY}\"\n      ],\n      \"env\": {\n        \"NEEDLE_API_KEY\": \"<your-needle-api-key>\"\n      }\n    }\n  }\n}\n```\n\n**Cursor Config**\n\nCreate or update `.cursor/mcp.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"needle\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"mcp-remote\",\n        \"https://mcp.needle.app/mcp\",\n        \"--header\",\n        \"Authorization:${NEEDLE_AUTH_HEADER}\"\n      ],\n      \"env\": {\n        \"NEEDLE_AUTH_HEADER\": \"Bearer <your-needle-api-key>\"\n      }\n    }\n  }\n}\n```\n\nGet your API key from [Needle Settings](https://needle.app).\n\nWe provide two endpoints:\n- **Streamable HTTP**: `https://mcp.needle.app/mcp` (recommended)\n- **SSE**: `https://mcp.needle.app/sse`\n\nNote: MCP deprecated SSE endpoints in the latest specification, so newer clients should prefer the Streamable HTTP endpoint.\n\n### 2. Local Installation\n\n1. Clone the repository:\n```bash\ngit clone https://github.com/needle-ai/needle-mcp.git\n```\n\n2. Install UV globally using Homebrew:\n```bash\nbrew install uv\n```\n\n3. Create your config file:\n   - For MacOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\n   - For Windows: `%APPDATA%/Claude/claude_desktop_config.json`\n\n**Claude Desktop Config**\n\n```json\n{\n  \"mcpServers\": {\n    \"needle\": {\n      \"command\": \"uv\",\n      \"args\": [\"--directory\", \"/path/to/needle-mcp\", \"run\", \"needle-mcp\"],\n      \"env\": {\n        \"NEEDLE_API_KEY\": \"<your-needle-api-key>\"\n      }\n    }\n  }\n}\n```\n\n**Cursor Config**\n\n```json\n{\n  \"mcpServers\": {\n    \"needle\": {\n      \"command\": \"uv\",\n      \"args\": [\"--directory\", \"/path/to/needle-mcp\", \"run\", \"needle-mcp\"],\n      \"env\": {\n        \"NEEDLE_API_KEY\": \"<your-needle-api-key>\"\n      }\n    }\n  }\n}\n```\n\n4. Replace `/path/to/needle-mcp` with your actual repository path\n5. Add your Needle API key\n6. Restart Claude Desktop\n\n**Installing via Smithery**\n\n```bash\nnpx -y @smithery/cli install needle-mcp --client claude\n```\n\n### 3. Docker Installation\n\n1. Clone and build:\n```bash\ngit clone https://github.com/needle-ai/needle-mcp.git\ncd needle-mcp\ndocker build -t needle-mcp .\n```\n\n2. Add to your Claude Desktop config (`~/Library/Application Support/Claude/claude_desktop_config.json`):\n```json\n{\n  \"mcpServers\": {\n    \"needle\": {\n      \"command\": \"docker\",\n      \"args\": [\"run\", \"--rm\", \"-i\", \"needle-mcp\"],\n      \"env\": {\n        \"NEEDLE_API_KEY\": \"<your-needle-api-key>\"\n      }\n    }\n  }\n}\n```\n\n3. Restart Claude Desktop\n\n## Usage Examples\n\n* \"Create a new collection called 'Technical Docs'\"\n* \"Add this document to the collection, which is https://needle.app\"\n* \"Search the collection for information about AI\"\n* \"List all my collections\"\n\n## Troubleshooting\n\nIf not working:\n- Make sure `uv` is installed globally (if not, uninstall with `pip uninstall uv` and reinstall with `brew install uv`)\n- Or find `uv` path with `which uv` and replace `\"command\": \"uv\"` with the full path\n- Verify your Needle API key is correct\n- Check if the needle-mcp path in config matches your actual repository location\n\n### Reset Claude Desktop Configuration\n\nIf you're seeing old configurations or the integration isn't working:\n\n1. Find all Claude Desktop config files:\n```bash\nfind / -name \"claude_desktop_config.json\" 2>/dev/null\n```\n\n2. Remove all Claude Desktop data:\n- On MacOS: `rm -rf ~/Library/Application\\ Support/Claude/*`\n- On Windows: Delete contents of `%APPDATA%/Claude/`\n\n3. Create a fresh config with only Needle:\n```\nmkdir -p ~/Library/Application\\ Support/Claude\ncat > ~/Library/Application\\ Support/Claude/claude_desktop_config.json\n<< 'EOL'\n{\n  \"mcpServers\": {\n    \"needle\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"--directory\",\n        \"/path/to/needle-mcp\",\n        \"run\",\n        \"needle-mcp\"\n      ],\n      \"env\": {\n        \"NEEDLE_API_KEY\": \"your_needle_api_key\"\n      }\n    }\n  }\n}\nEOL\n```\n\n4. Completely quit Claude Desktop (Command+Q on Mac) and relaunch it\n\n5. If you still see old configurations:\n- Check for additional config files in other locations\n- Try clearing browser cache if using web version\n- Verify the config file is being read from the correct location\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "needle",
        "searches",
        "ai",
        "needle ai",
        "ai needle",
        "servers needle"
      ],
      "category": "official-servers"
    },
    "oceanbase--awesome-oceanbase-mcp": {
      "owner": "oceanbase",
      "name": "awesome-oceanbase-mcp",
      "url": "https://github.com/oceanbase/awesome-oceanbase-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/oceanbase.webp",
      "description": "Facilitates secure interaction with OceanBase databases, allowing users to list tables, read data, and execute SQL queries through a controlled interface.",
      "stars": 78,
      "forks": 27,
      "license": "Apache License 2.0",
      "language": "Python",
      "updated_at": "2025-09-30T01:56:27Z",
      "readme_content": "<div align=\"center\">\n\n# üåä Awesome OceanBase MCP \n\n**Model Context Protocol (MCP) Server Collection for OceanBase Ecosystem**\n\nEnglish | [ÁÆÄ‰Ωì‰∏≠Êñá](README_CN.md)\n\n[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](LICENSE)\n[![Python](https://img.shields.io/badge/Python-3.8+-green.svg)](https://python.org)\n[![TypeScript](https://img.shields.io/badge/TypeScript-5.0+-blue.svg)](https://www.typescriptlang.org/)\n\n</div>\n\n## üìñ Project Overview\n\n**awesome-oceanbase-mcp** is a Model Context Protocol (MCP) server repository specifically designed for the OceanBase ecosystem.\n\nüéØ **Mission**: Enable AI assistants to interact directly with OceanBase databases and ecosystem components through standardized MCP protocols.\n\n‚ú® **Core Values**:\n- ü§ñ **AI-Friendly**: Direct database operations within Claude, ChatGPT and other AI assistants\n- üîí **Secure & Reliable**: Provides safe database access and operation mechanisms\n- üõ†Ô∏è **Complete Ecosystem**: Covers the complete OceanBase product and tool chain\n- üöÄ **Ready to Use**: Simple configuration to get started\n\n## üîç What is MCP?\n\nThe Model Context Protocol (MCP) is an open protocol designed to enable seamless integration between AI applications and external data sources and tools. It provides a standardized way for AI models to access the contextual information and capabilities they need.\n\n## üöÄ Quick Start\n\n### Prerequisites\n\nIf you don't have an OceanBase database instance yet, please:\n- Visit [OceanBase Official Repository](https://github.com/oceanbase/oceanbase) to get the latest version\n- Or use [OceanBase Online Trial](https://www.oceanbase.com/free-trial) for quick setup\n\n## üóÇÔ∏è MCP Server Collection\n\nThis repository provides complete MCP servers for the OceanBase ecosystem:\n\n<table>\n<thead>\n<tr>\n<th width=\"25%\">üîß MCP Server</th>\n<th width=\"60%\">üìù Description</th>\n<th width=\"15%\">üìö Documentation</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><strong>OceanBase MCP Server</strong></td>\n<td>Provides secure interaction capabilities with OceanBase databases, supporting SQL queries, data management operations</td>\n<td><a href=\"src/oceanbase_mcp_server/README.md\">üìñ View</a></td>\n</tr>\n<tr>\n<td><strong>OCP MCP Server</strong></td>\n<td>Integrates with OceanBase Cloud Platform, providing cluster management and monitoring capabilities</td>\n<td><a href=\"doc/ocp_mcp_server.md\">üìñ View</a></td>\n</tr>\n<tr>\n<td><strong>OBCloud MCP Server</strong></td>\n<td>Connects to OBCloud services, providing cloud database management functionality</td>\n<td><a href=\"src/obcloud_mcp_server/README.md\">üìñ View</a></td>\n</tr>\n<tr>\n<td><strong>OKCTL MCP Server</strong></td>\n<td>Manages OceanBase resources and deployments in Kubernetes environments</td>\n<td><a href=\"doc/okctl_mcp_server.md\">üìñ View</a></td>\n</tr>\n<tr>\n<td><strong>OBDIAG MCP Server</strong></td>\n<td>Provides OceanBase diagnostic tool integration, supporting performance analysis and troubleshooting</td>\n<td><a href=\"doc/obdiag_mcp_server.md\">üìñ View</a></td>\n</tr>\n<tr>\n<td><strong>obshell MCP Server</strong></td>\n<td>Enables OceanBase cluster creation, deployment and operations management through obshell</td>\n<td><a href=\"doc/obshell_mcp_server.md\">üìñ View</a></td>\n</tr>\n</tbody>\n</table>\n\nüí° **Usage Tips**: Click on the documentation links to view detailed installation and configuration guides.\n\n## üí¨ Community & Support\n\nWe highly value community feedback and contributions!\n\n### üôã‚Äç‚ôÄÔ∏è Getting Help\n\n- üí¨ **Technical Discussion**: Visit [OceanBase Community Forum](https://ask.oceanbase.com) to connect with developers and community partners\n- üìß **Technical Support**: Get official technical support through the community forum\n- üìñ **Documentation Hub**: Check [OceanBase Official Documentation](https://www.oceanbase.com/docs)\n\n### üêõ Issue Reporting\n\nIf you encounter any issues during usage:\n\n1. First check the documentation for the corresponding MCP server\n2. Search [existing Issues](https://github.com/oceanbase/mcp-oceanbase/issues) to confirm if the issue is known\n3. If it's a new issue, please [create a new Issue](https://github.com/oceanbase/mcp-oceanbase/issues/new)\n\n### ü§ù Contributing\n\nWe welcome all forms of contributions:\n\n- üîß **Code Contribution**: Submit Pull Requests\n- üìù **Documentation**: Improve docs and examples\n- üêõ **Bug Reports**: Report bugs and suggest improvements\n- üí° **Feature Requests**: Propose new feature requirements\n\n## üìÑ License\n\nThis project is released under the [Apache License 2.0](LICENSE).\n\n---\n\n<div align=\"center\">\n\n**‚≠ê If this project is helpful to you, please give us a Star!**\n\nMade with ‚ù§Ô∏è by OceanBase Team\n\n</div>\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "oceanbase",
        "databases",
        "servers",
        "oceanbase databases",
        "oceanbase mcp",
        "servers oceanbase"
      ],
      "category": "official-servers"
    },
    "oxylabs--oxylabs-mcp": {
      "owner": "oxylabs",
      "name": "oxylabs-mcp",
      "url": "https://github.com/oxylabs/oxylabs-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/oxylabs.webp",
      "description": "Fetch and process content from specified URLs using the Oxylabs Web Scraper API, enabling easy access to web data.",
      "stars": 66,
      "forks": 16,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-30T02:00:01Z",
      "readme_content": "<p align=\"center\">\n  <img src=\"https://storage.googleapis.com/oxylabs-public-assets/oxylabs_mcp.svg\" alt=\"Oxylabs + MCP\">\n</p>\n<h1 align=\"center\" style=\"border-bottom: none;\">\n  Oxylabs MCP Server\n</h1>\n\n<p align=\"center\">\n  <em>The missing link between AI models and the real‚Äëworld web: one API that delivers clean, structured data from any site.</em>\n</p>\n\n<div align=\"center\">\n\n[![smithery badge](https://smithery.ai/badge/@oxylabs/oxylabs-mcp)](https://smithery.ai/server/@oxylabs/oxylabs-mcp)\n[![pypi package](https://img.shields.io/pypi/v/oxylabs-mcp?color=%2334D058&label=pypi%20package)](https://pypi.org/project/oxylabs-mcp/)\n[](https://discord.gg/Pds3gBmKMH)\n[![Licence](https://img.shields.io/badge/license-MIT-blue.svg)](LICENSE)\n[![Verified on MseeP](https://mseep.ai/badge.svg)](https://mseep.ai/app/f6a9c0bc-83a6-4f78-89d9-f2cec4ece98d)\n![Coverage badge](https://raw.githubusercontent.com/oxylabs/oxylabs-mcp/coverage/coverage-badge.svg)\n\n<br/>\n<a href=\"https://glama.ai/mcp/servers/@oxylabs/oxylabs-mcp\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@oxylabs/oxylabs-mcp/badge\" alt=\"Oxylabs Server MCP server\" />\n</a>\n\n</div>\n\n---\n\n## üìñ Overview\n\nThe Oxylabs MCP server provides a bridge between AI models and the web. It enables them to scrape any URL, render JavaScript-heavy pages, extract and format content for AI use, bypass anti-scraping measures, and access geo-restricted web data from 195+ countries.\n\n\n## üõ†Ô∏è MCP Tools\n\nOxylabs MCP provides two sets of tools that can be used together or independently:\n\n### Oxylabs Web Scraper API Tools\n1. **universal_scraper**: Uses Oxylabs Web Scraper API for general website scraping;\n2. **google_search_scraper**: Uses Oxylabs Web Scraper API to extract results from Google Search;\n3. **amazon_search_scraper**: Uses Oxylabs Web Scraper API to scrape Amazon search result pages;\n4. **amazon_product_scraper**: Uses Oxylabs Web Scraper API to extract data from individual Amazon product pages.\n\n### Oxylabs AI Studio Tools\n\n5. **ai_scraper**: Scrape content from any URL in JSON or Markdown format with AI-powered data extraction;\n6. **ai_crawler**: Based on a prompt, crawls a website and collects data in Markdown or JSON format across multiple pages;\n7. **ai_browser_agent**: Based on prompt, controls a browser and returns data in Markdown, JSON, HTML, or screenshot formats;\n8. **ai_search**: Search the web for URLs and their contents with AI-powered content extraction.\n\n\n## ‚úÖ Prerequisites\n\nBefore you begin, make sure you have **at least one** of the following:\n\n- **Oxylabs Web Scraper API Account**: Obtain your username and password from [Oxylabs](https://dashboard.oxylabs.io/) (1-week free trial available);\n- **Oxylabs AI Studio API Key**: Obtain your API key from [Oxylabs AI Studio](https://aistudio.oxylabs.io/settings/api-key). (1000 credits free).\n\n## üì¶ Configuration\n\n### Environment variables\n\nOxylabs MCP server supports the following environment variables:\n| Name                       | Description                                   | Default |\n|----------------------------|-----------------------------------------------|---------|\n| `OXYLABS_USERNAME`         | Your Oxylabs Web Scraper API username         |         |\n| `OXYLABS_PASSWORD`         | Your Oxylabs Web Scraper API password         |         |\n| `OXYLABS_AI_STUDIO_API_KEY`| Your Oxylabs AI Studio API key                |         |\n| `LOG_LEVEL`                | Log level for the logs returned to the client | `INFO`  |\n\nBased on provided credentials, the server will automatically expose the corresponding tools:\n- If only `OXYLABS_USERNAME` and `OXYLABS_PASSWORD` are provided, the server will expose the Web Scraper API tools;\n- If only `OXYLABS_AI_STUDIO_API_KEY` is provided, the server will expose the AI Studio tools;\n- If both `OXYLABS_USERNAME` and `OXYLABS_PASSWORD` and `OXYLABS_AI_STUDIO_API_KEY` are provided, the server will expose all tools.\n\n‚ùó‚ùó‚ùó **Important note: if you don't have Web Scraper API or Oxylabs AI studio credentials, delete the corresponding environment variables placeholders.\nLeaving placeholder values will result in exposed tools that do not work.**\n\n\n\n### Configure with uvx\n\n- Install the uvx package manager:\n  ```bash\n  # macOS and Linux\n  curl -LsSf https://astral.sh/uv/install.sh | sh\n  ```\n  OR:\n  ```bash\n  # Windows\n  powershell -ExecutionPolicy ByPass -c \"irm https://astral.sh/uv/install.ps1 | iex\"\n  ```\n- Use the following config:\n  ```json\n  {\n    \"mcpServers\": {\n      \"oxylabs\": {\n        \"command\": \"uvx\",\n        \"args\": [\"oxylabs-mcp\"],\n        \"env\": {\n          \"OXYLABS_USERNAME\": \"OXYLABS_USERNAME\",\n          \"OXYLABS_PASSWORD\": \"OXYLABS_PASSWORD\",\n          \"OXYLABS_AI_STUDIO_API_KEY\": \"OXYLABS_AI_STUDIO_API_KEY\"\n        }\n      }\n    }\n  }\n  ```\n\n### Configure with uv\n\n- Install the uv package manager:\n  ```bash\n  # macOS and Linux\n  curl -LsSf https://astral.sh/uv/install.sh | sh\n  ```\n  OR:\n  ```bash\n  # Windows\n  powershell -ExecutionPolicy ByPass -c \"irm https://astral.sh/uv/install.ps1 | iex\"\n  ```\n\n- Use the following config:\n  ```json\n  {\n    \"mcpServers\": {\n      \"oxylabs\": {\n        \"command\": \"uv\",\n        \"args\": [\n          \"--directory\",\n          \"/<Absolute-path-to-folder>/oxylabs-mcp\",\n          \"run\",\n          \"oxylabs-mcp\"\n        ],\n        \"env\": {\n          \"OXYLABS_USERNAME\": \"OXYLABS_USERNAME\",\n          \"OXYLABS_PASSWORD\": \"OXYLABS_PASSWORD\",\n          \"OXYLABS_AI_STUDIO_API_KEY\": \"OXYLABS_AI_STUDIO_API_KEY\"\n        }\n      }\n    }\n  }\n  ```\n\n### Configure with Smithery Oauth2\n\n- Go to https://smithery.ai/server/@oxylabs/oxylabs-mcp;\n- Click _Auto_ to install the Oxylabs MCP configuration for the respective client;\n- OR use the following config:\n```json\n  {\n    \"mcpServers\": {\n      \"oxylabs\": {\n        \"url\": \"https://server.smithery.ai/@oxylabs/oxylabs-mcp/mcp\"\n      }\n    }\n  }\n```\n- Follow the instructions to authenticate Oxylabs MCP with Oauth2 flow\n\n### Configure with Smithery query parameters\n\nIn case your client does not support the Oauth2 authentication, you can pass the Oxylabs authentication parameters directly in url\n```json\n  {\n    \"mcpServers\": {\n      \"oxylabs\": {\n        \"url\": \"https://server.smithery.ai/@oxylabs/oxylabs-mcp/mcp?oxylabsUsername=OXYLABS_USERNAME&oxylabsPassword=OXYLABS_PASSWORD&oxylabsAiStudioApiKey=OXYLABS_AI_STUDIO_API_KEY\"\n      }\n    }\n  }\n```\n\n### Manual Setup with Claude Desktop\n\nNavigate to **Claude ‚Üí Settings ‚Üí Developer ‚Üí Edit Config** and add one of the configurations above to the `claude_desktop_config.json` file.\n\n### Manual Setup with Cursor AI\n\nNavigate to **Cursor ‚Üí Settings ‚Üí Cursor Settings ‚Üí MCP**. Click **Add new global MCP server** and add one of the configurations above.\n\n\n\n## üìù Logging\n\nServer provides additional information about the tool calls in `notification/message` events\n\n```json\n{\n  \"method\": \"notifications/message\",\n  \"params\": {\n    \"level\": \"info\",\n    \"data\": \"Create job with params: {\\\"url\\\": \\\"https://ip.oxylabs.io\\\"}\"\n  }\n}\n```\n\n```json\n{\n  \"method\": \"notifications/message\",\n  \"params\": {\n    \"level\": \"info\",\n    \"data\": \"Job info: job_id=7333113830223918081 job_status=done\"\n  }\n}\n```\n\n```json\n{\n  \"method\": \"notifications/message\",\n  \"params\": {\n    \"level\": \"error\",\n    \"data\": \"Error: request to Oxylabs API failed\"\n  }\n}\n```\n\n---\n\n## üõ°Ô∏è License\n\nDistributed under the MIT License ‚Äì see [LICENSE](LICENSE) for details.\n\n---\n\n## About Oxylabs\n\nEstablished in 2015, Oxylabs is a market-leading web intelligence collection\nplatform, driven by the highest business, ethics, and compliance standards,\nenabling companies worldwide to unlock data-driven insights.\n\n[![image](https://oxylabs.io/images/og-image.png)](https://oxylabs.io/)\n\n<div align=\"center\">\n<sub>\n  Made with ‚òï by <a href=\"https://oxylabs.io\">Oxylabs</a>.  Feel free to give us a ‚≠ê if MCP saved you a weekend.\n</sub>\n</div>\n\n\n## ‚ú® Key Features\n\n<details>\n<summary><strong> Scrape content from any site</strong></summary>\n<br>\n\n- Extract data from any URL, including complex single-page applications\n- Fully render dynamic websites using headless browser support\n- Choose full JavaScript rendering, HTML-only, or none\n- Emulate Mobile and Desktop viewports for realistic rendering\n\n</details>\n\n<details>\n<summary><strong> Automatically get AI-ready data</strong></summary>\n<br>\n\n- Automatically clean and convert HTML to Markdown for improved readability\n- Use automated parsers for popular targets like Google, Amazon, and more\n\n</details>\n\n<details>\n<summary><strong> Bypass blocks & geo-restrictions</strong></summary>\n<br>\n\n- Bypass sophisticated bot protection systems with high success rate\n- Reliably scrape even the most complex websites\n- Get automatically rotating IPs from a proxy pool covering 195+ countries\n\n</details>\n\n<details>\n<summary><strong> Flexible setup & cross-platform support</strong></summary>\n<br>\n\n- Set rendering and parsing options if needed\n- Feed data directly into AI models or analytics tools\n- Works on macOS, Windows, and Linux\n\n</details>\n\n<details>\n<summary><strong> Built-in error handling and request management</strong></summary>\n<br>\n\n- Comprehensive error handling and reporting\n- Smart rate limiting and request management\n\n</details>\n\n---\n\n\n## Why Oxylabs MCP? &nbsp;üï∏Ô∏è ‚ûú üì¶ ‚ûú ü§ñ\n\nImagine telling your LLM *\"Summarise the latest Hacker News discussion about GPT‚Äë5\"* ‚Äì and it simply answers.  \nMCP (Multi‚ÄëClient Proxy) makes that happen by doing the boring parts for you:\n\n| What Oxylabs MCP does                                             | Why it matters to you                    |\n|-------------------------------------------------------------------|------------------------------------------|\n| **Bypasses anti‚Äëbot walls** with the Oxylabs global proxy network | Keeps you unblocked and anonymous        |\n| **Renders JavaScript** in headless Chrome                         | Single‚Äëpage apps, sorted                 |\n| **Cleans HTML ‚Üí JSON**                                            | Drop straight into vector DBs or prompts |\n| **Optional structured parsers** (Google, Amazon, etc.)            | One‚Äëline access to popular targets       |",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "oxylabs",
        "scraper",
        "fetch",
        "oxylabs web",
        "servers oxylabs",
        "oxylabs oxylabs"
      ],
      "category": "official-servers"
    },
    "pydantic--logfire-mcp": {
      "owner": "pydantic",
      "name": "logfire-mcp",
      "url": "https://github.com/pydantic/logfire-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/pydantic.webp",
      "description": "Analyzes telemetry data and distributed traces using OpenTelemetry. Queries metrics and performs SQL queries for insights into application exceptions and errors in real-time.",
      "stars": 112,
      "forks": 19,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-28T02:55:14Z",
      "readme_content": "<!-- DO NOT MODIFY THIS FILE DIRECTLY, IT IS GENERATED BY THE TESTS! -->\n\n# Pydantic Logfire MCP Server\n\nThis repository contains a Model Context Protocol (MCP) server with tools that can access the OpenTelemetry traces and\nmetrics you've sent to Pydantic Logfire.\n\n<a href=\"https://glama.ai/mcp/servers/@pydantic/logfire-mcp\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@pydantic/logfire-mcp/badge\" alt=\"Pydantic Logfire Server MCP server\" />\n</a>\n\nThis MCP server enables LLMs to retrieve your application's telemetry data, analyze distributed\ntraces, and make use of the results of arbitrary SQL queries executed using the Pydantic Logfire APIs.\n\n## Available Tools\n\n* `find_exceptions_in_file` - Get the details about the 10 most recent exceptions on the file.\n  * Arguments:\n    * `filepath` (string) - The path to the file to find exceptions in.\n    * `age` (integer) - Number of minutes to look back, e.g. 30 for last 30 minutes. Maximum allowed value is 7 days.\n\n* `arbitrary_query` - Run an arbitrary query on the Pydantic Logfire database.\n  * Arguments:\n    * `query` (string) - The query to run, as a SQL string.\n    * `age` (integer) - Number of minutes to look back, e.g. 30 for last 30 minutes. Maximum allowed value is 7 days.\n\n* `logfire_link` - Creates a link to help the user to view the trace in the Logfire UI.\n  * Arguments:\n    * `trace_id` (string) - The trace ID to link to.\n\n* `schema_reference` - The database schema for the Logfire DataFusion database.\n\n\n## Setup\n\n### Install `uv`\n\nThe first thing to do is make sure `uv` is installed, as `uv` is used to run the MCP server.\n\nFor installation instructions, see the [`uv` installation docs](https://docs.astral.sh/uv/getting-started/installation/).\n\nIf you already have an older version of `uv` installed, you might need to update it with `uv self update`.\n\n### Obtain a Pydantic Logfire read token\nIn order to make requests to the Pydantic Logfire APIs, the Pydantic Logfire MCP server requires a \"read token\".\n\nYou can create one under the \"Read Tokens\" section of your project settings in Pydantic Logfire:\nhttps://logfire.pydantic.dev/-/redirect/latest-project/settings/read-tokens\n\n> [!IMPORTANT]\n> Pydantic Logfire read tokens are project-specific, so you need to create one for the specific project you want to expose to the Pydantic Logfire MCP server.\n\n### Manually run the server\n\nOnce you have `uv` installed and have a Pydantic Logfire read token, you can manually run the MCP server using `uvx` (which is provided by `uv`).\n\nYou can specify your read token using the `LOGFIRE_READ_TOKEN` environment variable:\n\n```bash\nLOGFIRE_READ_TOKEN=YOUR_READ_TOKEN uvx logfire-mcp@latest\n```\n\nYou can also set `LOGFIRE_READ_TOKEN` in a `.env` file:\n\n```bash\nLOGFIRE_READ_TOKEN=pylf_v1_us_...\n```\n\n**NOTE:** for this to work, the MCP server needs to run with the directory containing the `.env` file in its working directory.\n\nor using the `--read-token` flag:\n\n```bash\nuvx logfire-mcp@latest --read-token=YOUR_READ_TOKEN\n```\n> [!NOTE]\n> If you are using Cursor, Claude Desktop, Cline, or other MCP clients that manage your MCP servers for you, you **_do\n    NOT_** need to manually run the server yourself. The next section will show you how to configure these clients to make\n    use of the Pydantic Logfire MCP server.\n\n### Base URL\n\nIf you are running Logfire in a self hosted environment, you need to specify the base URL.\nThis can be done using the `LOGFIRE_BASE_URL` environment variable:\n\n```bash\nLOGFIRE_BASE_URL=https://logfire.my-company.com uvx logfire-mcp@latest --read-token=YOUR_READ_TOKEN\n```\n\nYou can also use the `--base-url` argument:\n\n```bash\nuvx logfire-mcp@latest --base-url=https://logfire.my-company.com --read-token=YOUR_READ_TOKEN\n```\n\n## Configuration with well-known MCP clients\n\n### Configure for Cursor\n\nCreate a `.cursor/mcp.json` file in your project root:\n\n```json\n{\n  \"mcpServers\": {\n    \"logfire\": {\n      \"command\": \"uvx\",\n      \"args\": [\"logfire-mcp@latest\", \"--read-token=YOUR-TOKEN\"]\n    }\n  }\n}\n```\n\nThe Cursor doesn't accept the `env` field, so you need to use the `--read-token` flag instead.\n\n### Configure for Claude code\n\nRun the following command:\n\n```bash\nclaude mcp add logfire -e LOGFIRE_READ_TOKEN=YOUR_TOKEN -- uvx logfire-mcp@latest\n```\n\n### Configure for Claude Desktop\n\nAdd to your Claude settings:\n\n```json\n{\n  \"command\": [\"uvx\"],\n  \"args\": [\"logfire-mcp@latest\"],\n  \"type\": \"stdio\",\n  \"env\": {\n    \"LOGFIRE_READ_TOKEN\": \"YOUR_TOKEN\"\n  }\n}\n```\n\n### Configure for Cline\n\nAdd to your Cline settings in `cline_mcp_settings.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"logfire\": {\n      \"command\": \"uvx\",\n      \"args\": [\"logfire-mcp@latest\"],\n      \"env\": {\n        \"LOGFIRE_READ_TOKEN\": \"YOUR_TOKEN\"\n      },\n      \"disabled\": false,\n      \"autoApprove\": []\n    }\n  }\n}\n```\n\n### Configure for VS Code\n\nMake sure you [enabled MCP support in VS Code](https://code.visualstudio.com/docs/copilot/chat/mcp-servers#_enable-mcp-support-in-vs-code).\n\nCreate a `.vscode/mcp.json` file in your project's root directory:\n\n```json\n{\n  \"servers\": {\n    \"logfire\": {\n      \"type\": \"stdio\",\n      \"command\": \"uvx\", // or the absolute /path/to/uvx\n      \"args\": [\"logfire-mcp@latest\"],\n      \"env\": {\n        \"LOGFIRE_READ_TOKEN\": \"YOUR_TOKEN\"\n      }\n    }\n  }\n}\n```\n\n### Configure for Zed\n\nCreate a `.zed/settings.json` file in your project's root directory:\n\n```json\n{\n  \"context_servers\": {\n    \"logfire\": {\n      \"source\": \"custom\",\n      \"command\": \"uvx\",\n      \"args\": [\"logfire-mcp@latest\"],\n      \"env\": {\n        \"LOGFIRE_READ_TOKEN\": \"YOUR_TOKEN\"\n      },\n      \"enabled\": true\n    }\n  }\n}\n```\n\n## Example Interactions\n\n1. Get details about exceptions from traces in a specific file:\n```json\n{\n  \"name\": \"find_exceptions_in_file\",\n  \"arguments\": {\n    \"filepath\": \"app/api.py\",\n    \"age\": 1440\n  }\n}\n```\n\nResponse:\n```json\n[\n  {\n    \"created_at\": \"2024-03-20T10:30:00Z\",\n    \"message\": \"Failed to process request\",\n    \"exception_type\": \"ValueError\",\n    \"exception_message\": \"Invalid input format\",\n    \"function_name\": \"process_request\",\n    \"line_number\": \"42\",\n    \"attributes\": {\n      \"service.name\": \"api-service\",\n      \"code.filepath\": \"app/api.py\"\n    },\n    \"trace_id\": \"1234567890abcdef\"\n  }\n]\n```\n\n2. Run a custom query on traces:\n```json\n{\n  \"name\": \"arbitrary_query\",\n  \"arguments\": {\n    \"query\": \"SELECT trace_id, message, created_at, attributes->>'service.name' as service FROM records WHERE severity_text = 'ERROR' ORDER BY created_at DESC LIMIT 10\",\n    \"age\": 1440\n  }\n}\n```\n\n## Examples of Questions for Claude\n\n1. \"What exceptions occurred in traces from the last hour across all services?\"\n2. \"Show me the recent errors in the file 'app/api.py' with their trace context\"\n3. \"How many errors were there in the last 24 hours per service?\"\n4. \"What are the most common exception types in my traces, grouped by service name?\"\n5. \"Get me the OpenTelemetry schema for traces and metrics\"\n6. \"Find all errors from yesterday and show their trace contexts\"\n\n## Getting Started\n\n1. First, obtain a Pydantic Logfire read token from:\n   https://logfire.pydantic.dev/-/redirect/latest-project/settings/read-tokens\n\n2. Run the MCP server:\n   ```bash\n   uvx logfire-mcp@latest --read-token=YOUR_TOKEN\n   ```\n\n3. Configure your preferred client (Cursor, Claude Desktop, or Cline) using the configuration examples above\n\n4. Start using the MCP server to analyze your OpenTelemetry traces and metrics!\n\n## Contributing\n\nWe welcome contributions to help improve the Pydantic Logfire MCP server. Whether you want to add new trace analysis tools, enhance metrics querying functionality, or improve documentation, your input is valuable.\n\nFor examples of other MCP servers and implementation patterns, see the [Model Context Protocol servers repository](https://github.com/modelcontextprotocol/servers).\n\n## License\n\nPydantic Logfire MCP is licensed under the MIT License. This means you are free to use, modify, and distribute the software, subject to the terms and conditions of the MIT License.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "telemetry",
        "logfire",
        "opentelemetry",
        "analyzes telemetry",
        "pydantic logfire",
        "telemetry data"
      ],
      "category": "official-servers"
    },
    "qdrant--mcp-server-qdrant": {
      "owner": "qdrant",
      "name": "mcp-server-qdrant",
      "url": "https://github.com/qdrant/mcp-server-qdrant",
      "imageUrl": "/freedevtools/mcp/pfp/qdrant.webp",
      "description": "Stores and retrieves information using the Qdrant vector search engine, enabling semantic search for relevant data. It functions as a memory layer for AI applications accessing external tools and information.",
      "stars": 964,
      "forks": 152,
      "license": "Apache License 2.0",
      "language": "Python",
      "updated_at": "2025-10-02T08:49:13Z",
      "readme_content": "# mcp-server-qdrant: A Qdrant MCP server\n\n[![smithery badge](https://smithery.ai/badge/mcp-server-qdrant)](https://smithery.ai/protocol/mcp-server-qdrant)\n\n> The [Model Context Protocol (MCP)](https://modelcontextprotocol.io/introduction) is an open protocol that enables\n> seamless integration between LLM applications and external data sources and tools. Whether you're building an\n> AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to\n> connect LLMs with the context they need.\n\nThis repository is an example of how to create a MCP server for [Qdrant](https://qdrant.tech/), a vector search engine.\n\n## Overview\n\nAn official Model Context Protocol server for keeping and retrieving memories in the Qdrant vector search engine.\nIt acts as a semantic memory layer on top of the Qdrant database.\n\n## Components\n\n### Tools\n\n1. `qdrant-store`\n   - Store some information in the Qdrant database\n   - Input:\n     - `information` (string): Information to store\n     - `metadata` (JSON): Optional metadata to store\n     - `collection_name` (string): Name of the collection to store the information in. This field is required if there are no default collection name.\n                                   If there is a default collection name, this field is not enabled.\n   - Returns: Confirmation message\n2. `qdrant-find`\n   - Retrieve relevant information from the Qdrant database\n   - Input:\n     - `query` (string): Query to use for searching\n     - `collection_name` (string): Name of the collection to store the information in. This field is required if there are no default collection name.\n                                   If there is a default collection name, this field is not enabled.\n   - Returns: Information stored in the Qdrant database as separate messages\n\n## Environment Variables\n\nThe configuration of the server is done using environment variables:\n\n| Name                     | Description                                                         | Default Value                                                     |\n|--------------------------|---------------------------------------------------------------------|-------------------------------------------------------------------|\n| `QDRANT_URL`             | URL of the Qdrant server                                            | None                                                              |\n| `QDRANT_API_KEY`         | API key for the Qdrant server                                       | None                                                              |\n| `COLLECTION_NAME`        | Name of the default collection to use.                              | None                                                              |\n| `QDRANT_LOCAL_PATH`      | Path to the local Qdrant database (alternative to `QDRANT_URL`)     | None                                                              |\n| `EMBEDDING_PROVIDER`     | Embedding provider to use (currently only \"fastembed\" is supported) | `fastembed`                                                       |\n| `EMBEDDING_MODEL`        | Name of the embedding model to use                                  | `sentence-transformers/all-MiniLM-L6-v2`                          |\n| `TOOL_STORE_DESCRIPTION` | Custom description for the store tool                               | See default in [`settings.py`](src/mcp_server_qdrant/settings.py) |\n| `TOOL_FIND_DESCRIPTION`  | Custom description for the find tool                                | See default in [`settings.py`](src/mcp_server_qdrant/settings.py) |\n\nNote: You cannot provide both `QDRANT_URL` and `QDRANT_LOCAL_PATH` at the same time.\n\n> [!IMPORTANT]\n> Command-line arguments are not supported anymore! Please use environment variables for all configuration.\n\n### FastMCP Environment Variables\n\nSince `mcp-server-qdrant` is based on FastMCP, it also supports all the FastMCP environment variables. The most\nimportant ones are listed below:\n\n| Environment Variable                  | Description                                               | Default Value |\n|---------------------------------------|-----------------------------------------------------------|---------------|\n| `FASTMCP_DEBUG`                       | Enable debug mode                                         | `false`       |\n| `FASTMCP_LOG_LEVEL`                   | Set logging level (DEBUG, INFO, WARNING, ERROR, CRITICAL) | `INFO`        |\n| `FASTMCP_HOST`                        | Host address to bind the server to                        | `127.0.0.1`   |\n| `FASTMCP_PORT`                        | Port to run the server on                                 | `8000`        |\n| `FASTMCP_WARN_ON_DUPLICATE_RESOURCES` | Show warnings for duplicate resources                     | `true`        |\n| `FASTMCP_WARN_ON_DUPLICATE_TOOLS`     | Show warnings for duplicate tools                         | `true`        |\n| `FASTMCP_WARN_ON_DUPLICATE_PROMPTS`   | Show warnings for duplicate prompts                       | `true`        |\n| `FASTMCP_DEPENDENCIES`                | List of dependencies to install in the server environment | `[]`          |\n\n## Installation\n\n### Using uvx\n\nWhen using [`uvx`](https://docs.astral.sh/uv/guides/tools/#running-tools) no specific installation is needed to directly run *mcp-server-qdrant*.\n\n```shell\nQDRANT_URL=\"http://localhost:6333\" \\\nCOLLECTION_NAME=\"my-collection\" \\\nEMBEDDING_MODEL=\"sentence-transformers/all-MiniLM-L6-v2\" \\\nuvx mcp-server-qdrant\n```\n\n#### Transport Protocols\n\nThe server supports different transport protocols that can be specified using the `--transport` flag:\n\n```shell\nQDRANT_URL=\"http://localhost:6333\" \\\nCOLLECTION_NAME=\"my-collection\" \\\nuvx mcp-server-qdrant --transport sse\n```\n\nSupported transport protocols:\n\n- `stdio` (default): Standard input/output transport, might only be used by local MCP clients\n- `sse`: Server-Sent Events transport, perfect for remote clients\n- `streamable-http`: Streamable HTTP transport, perfect for remote clients, more recent than SSE\n\nThe default transport is `stdio` if not specified.\n\nWhen SSE transport is used, the server will listen on the specified port and wait for incoming connections. The default\nport is 8000, however it can be changed using the `FASTMCP_PORT` environment variable.\n\n```shell\nQDRANT_URL=\"http://localhost:6333\" \\\nCOLLECTION_NAME=\"my-collection\" \\\nFASTMCP_PORT=1234 \\\nuvx mcp-server-qdrant --transport sse\n```\n\n### Using Docker\n\nA Dockerfile is available for building and running the MCP server:\n\n```bash\n# Build the container\ndocker build -t mcp-server-qdrant .\n\n# Run the container\ndocker run -p 8000:8000 \\\n  -e FASTMCP_HOST=\"0.0.0.0\" \\\n  -e QDRANT_URL=\"http://your-qdrant-server:6333\" \\\n  -e QDRANT_API_KEY=\"your-api-key\" \\\n  -e COLLECTION_NAME=\"your-collection\" \\\n  mcp-server-qdrant\n```\n\n> [!TIP]\n> Please note that we set `FASTMCP_HOST=\"0.0.0.0\"` to make the server listen on all network interfaces. This is\n> necessary when running the server in a Docker container.\n\n### Installing via Smithery\n\nTo install Qdrant MCP Server for Claude Desktop automatically via [Smithery](https://smithery.ai/protocol/mcp-server-qdrant):\n\n```bash\nnpx @smithery/cli install mcp-server-qdrant --client claude\n```\n\n### Manual configuration of Claude Desktop\n\nTo use this server with the Claude Desktop app, add the following configuration to the \"mcpServers\" section of your\n`claude_desktop_config.json`:\n\n```json\n{\n  \"qdrant\": {\n    \"command\": \"uvx\",\n    \"args\": [\"mcp-server-qdrant\"],\n    \"env\": {\n      \"QDRANT_URL\": \"https://xyz-example.eu-central.aws.cloud.qdrant.io:6333\",\n      \"QDRANT_API_KEY\": \"your_api_key\",\n      \"COLLECTION_NAME\": \"your-collection-name\",\n      \"EMBEDDING_MODEL\": \"sentence-transformers/all-MiniLM-L6-v2\"\n    }\n  }\n}\n```\n\nFor local Qdrant mode:\n\n```json\n{\n  \"qdrant\": {\n    \"command\": \"uvx\",\n    \"args\": [\"mcp-server-qdrant\"],\n    \"env\": {\n      \"QDRANT_LOCAL_PATH\": \"/path/to/qdrant/database\",\n      \"COLLECTION_NAME\": \"your-collection-name\",\n      \"EMBEDDING_MODEL\": \"sentence-transformers/all-MiniLM-L6-v2\"\n    }\n  }\n}\n```\n\nThis MCP server will automatically create a collection with the specified name if it doesn't exist.\n\nBy default, the server will use the `sentence-transformers/all-MiniLM-L6-v2` embedding model to encode memories.\nFor the time being, only [FastEmbed](https://qdrant.github.io/fastembed/) models are supported.\n\n## Support for other tools\n\nThis MCP server can be used with any MCP-compatible client. For example, you can use it with\n[Cursor](https://docs.cursor.com/context/model-context-protocol) and [VS Code](https://code.visualstudio.com/docs), which provide built-in support for the Model Context\nProtocol.\n\n### Using with Cursor/Windsurf\n\nYou can configure this MCP server to work as a code search tool for Cursor or Windsurf by customizing the tool\ndescriptions:\n\n```bash\nQDRANT_URL=\"http://localhost:6333\" \\\nCOLLECTION_NAME=\"code-snippets\" \\\nTOOL_STORE_DESCRIPTION=\"Store reusable code snippets for later retrieval. \\\nThe 'information' parameter should contain a natural language description of what the code does, \\\nwhile the actual code should be included in the 'metadata' parameter as a 'code' property. \\\nThe value of 'metadata' is a Python dictionary with strings as keys. \\\nUse this whenever you generate some code snippet.\" \\\nTOOL_FIND_DESCRIPTION=\"Search for relevant code snippets based on natural language descriptions. \\\nThe 'query' parameter should describe what you're looking for, \\\nand the tool will return the most relevant code snippets. \\\nUse this when you need to find existing code snippets for reuse or reference.\" \\\nuvx mcp-server-qdrant --transport sse # Enable SSE transport\n```\n\nIn Cursor/Windsurf, you can then configure the MCP server in your settings by pointing to this running server using\nSSE transport protocol. The description on how to add an MCP server to Cursor can be found in the [Cursor\ndocumentation](https://docs.cursor.com/context/model-context-protocol#adding-an-mcp-server-to-cursor). If you are\nrunning Cursor/Windsurf locally, you can use the following URL:\n\n```\nhttp://localhost:8000/sse\n```\n\n> [!TIP]\n> We suggest SSE transport as a preferred way to connect Cursor/Windsurf to the MCP server, as it can support remote\n> connections. That makes it easy to share the server with your team or use it in a cloud environment.\n\nThis configuration transforms the Qdrant MCP server into a specialized code search tool that can:\n\n1. Store code snippets, documentation, and implementation details\n2. Retrieve relevant code examples based on semantic search\n3. Help developers find specific implementations or usage patterns\n\nYou can populate the database by storing natural language descriptions of code snippets (in the `information` parameter)\nalong with the actual code (in the `metadata.code` property), and then search for them using natural language queries\nthat describe what you're looking for.\n\n> [!NOTE]\n> The tool descriptions provided above are examples and may need to be customized for your specific use case. Consider\n> adjusting the descriptions to better match your team's workflow and the specific types of code snippets you want to\n> store and retrieve.\n\n**If you have successfully installed the `mcp-server-qdrant`, but still can't get it to work with Cursor, please\nconsider creating the [Cursor rules](https://docs.cursor.com/context/rules-for-ai) so the MCP tools are always used when\nthe agent produces a new code snippet.** You can restrict the rules to only work for certain file types, to avoid using\nthe MCP server for the documentation or other types of content.\n\n### Using with Claude Code\n\nYou can enhance Claude Code's capabilities by connecting it to this MCP server, enabling semantic search over your\nexisting codebase.\n\n#### Setting up mcp-server-qdrant\n\n1. Add the MCP server to Claude Code:\n\n    ```shell\n    # Add mcp-server-qdrant configured for code search\n    claude mcp add code-search \\\n    -e QDRANT_URL=\"http://localhost:6333\" \\\n    -e COLLECTION_NAME=\"code-repository\" \\\n    -e EMBEDDING_MODEL=\"sentence-transformers/all-MiniLM-L6-v2\" \\\n    -e TOOL_STORE_DESCRIPTION=\"Store code snippets with descriptions. The 'information' parameter should contain a natural language description of what the code does, while the actual code should be included in the 'metadata' parameter as a 'code' property.\" \\\n    -e TOOL_FIND_DESCRIPTION=\"Search for relevant code snippets using natural language. The 'query' parameter should describe the functionality you're looking for.\" \\\n    -- uvx mcp-server-qdrant\n    ```\n\n2. Verify the server was added:\n\n    ```shell\n    claude mcp list\n    ```\n\n#### Using Semantic Code Search in Claude Code\n\nTool descriptions, specified in `TOOL_STORE_DESCRIPTION` and `TOOL_FIND_DESCRIPTION`, guide Claude Code on how to use\nthe MCP server. The ones provided above are examples and may need to be customized for your specific use case. However,\nClaude Code should be already able to:\n\n1. Use the `qdrant-store` tool to store code snippets with descriptions.\n2. Use the `qdrant-find` tool to search for relevant code snippets using natural language.\n\n### Run MCP server in Development Mode\n\nThe MCP server can be run in development mode using the `mcp dev` command. This will start the server and open the MCP\ninspector in your browser.\n\n```shell\nCOLLECTION_NAME=mcp-dev fastmcp dev src/mcp_server_qdrant/server.py\n```\n\n### Using with VS Code\n\nFor one-click installation, click one of the install buttons below:\n\n[![Install with UVX in VS Code](https://img.shields.io/badge/VS_Code-UVX-0098FF?style=flat-square&logo=visualstudiocode&logoColor=white)](https://insiders.vscode.dev/redirect/mcp/install?name=qdrant&config=%7B%22command%22%3A%22uvx%22%2C%22args%22%3A%5B%22mcp-server-qdrant%22%5D%2C%22env%22%3A%7B%22QDRANT_URL%22%3A%22%24%7Binput%3AqdrantUrl%7D%22%2C%22QDRANT_API_KEY%22%3A%22%24%7Binput%3AqdrantApiKey%7D%22%2C%22COLLECTION_NAME%22%3A%22%24%7Binput%3AcollectionName%7D%22%7D%7D&inputs=%5B%7B%22type%22%3A%22promptString%22%2C%22id%22%3A%22qdrantUrl%22%2C%22description%22%3A%22Qdrant+URL%22%7D%2C%7B%22type%22%3A%22promptString%22%2C%22id%22%3A%22qdrantApiKey%22%2C%22description%22%3A%22Qdrant+API+Key%22%2C%22password%22%3Atrue%7D%2C%7B%22type%22%3A%22promptString%22%2C%22id%22%3A%22collectionName%22%2C%22description%22%3A%22Collection+Name%22%7D%5D) [![Install with UVX in VS Code Insiders](https://img.shields.io/badge/VS_Code_Insiders-UVX-24bfa5?style=flat-square&logo=visualstudiocode&logoColor=white)](https://insiders.vscode.dev/redirect/mcp/install?name=qdrant&config=%7B%22command%22%3A%22uvx%22%2C%22args%22%3A%5B%22mcp-server-qdrant%22%5D%2C%22env%22%3A%7B%22QDRANT_URL%22%3A%22%24%7Binput%3AqdrantUrl%7D%22%2C%22QDRANT_API_KEY%22%3A%22%24%7Binput%3AqdrantApiKey%7D%22%2C%22COLLECTION_NAME%22%3A%22%24%7Binput%3AcollectionName%7D%22%7D%7D&inputs=%5B%7B%22type%22%3A%22promptString%22%2C%22id%22%3A%22qdrantUrl%22%2C%22description%22%3A%22Qdrant+URL%22%7D%2C%7B%22type%22%3A%22promptString%22%2C%22id%22%3A%22qdrantApiKey%22%2C%22description%22%3A%22Qdrant+API+Key%22%2C%22password%22%3Atrue%7D%2C%7B%22type%22%3A%22promptString%22%2C%22id%22%3A%22collectionName%22%2C%22description%22%3A%22Collection+Name%22%7D%5D&quality=insiders)\n\n[![Install with Docker in VS Code](https://img.shields.io/badge/VS_Code-Docker-0098FF?style=flat-square&logo=visualstudiocode&logoColor=white)](https://insiders.vscode.dev/redirect/mcp/install?name=qdrant&config=%7B%22command%22%3A%22docker%22%2C%22args%22%3A%5B%22run%22%2C%22-p%22%2C%228000%3A8000%22%2C%22-i%22%2C%22--rm%22%2C%22-e%22%2C%22QDRANT_URL%22%2C%22-e%22%2C%22QDRANT_API_KEY%22%2C%22-e%22%2C%22COLLECTION_NAME%22%2C%22mcp-server-qdrant%22%5D%2C%22env%22%3A%7B%22QDRANT_URL%22%3A%22%24%7Binput%3AqdrantUrl%7D%22%2C%22QDRANT_API_KEY%22%3A%22%24%7Binput%3AqdrantApiKey%7D%22%2C%22COLLECTION_NAME%22%3A%22%24%7Binput%3AcollectionName%7D%22%7D%7D&inputs=%5B%7B%22type%22%3A%22promptString%22%2C%22id%22%3A%22qdrantUrl%22%2C%22description%22%3A%22Qdrant+URL%22%7D%2C%7B%22type%22%3A%22promptString%22%2C%22id%22%3A%22qdrantApiKey%22%2C%22description%22%3A%22Qdrant+API+Key%22%2C%22password%22%3Atrue%7D%2C%7B%22type%22%3A%22promptString%22%2C%22id%22%3A%22collectionName%22%2C%22description%22%3A%22Collection+Name%22%7D%5D) [![Install with Docker in VS Code Insiders](https://img.shields.io/badge/VS_Code_Insiders-Docker-24bfa5?style=flat-square&logo=visualstudiocode&logoColor=white)](https://insiders.vscode.dev/redirect/mcp/install?name=qdrant&config=%7B%22command%22%3A%22docker%22%2C%22args%22%3A%5B%22run%22%2C%22-p%22%2C%228000%3A8000%22%2C%22-i%22%2C%22--rm%22%2C%22-e%22%2C%22QDRANT_URL%22%2C%22-e%22%2C%22QDRANT_API_KEY%22%2C%22-e%22%2C%22COLLECTION_NAME%22%2C%22mcp-server-qdrant%22%5D%2C%22env%22%3A%7B%22QDRANT_URL%22%3A%22%24%7Binput%3AqdrantUrl%7D%22%2C%22QDRANT_API_KEY%22%3A%22%24%7Binput%3AqdrantApiKey%7D%22%2C%22COLLECTION_NAME%22%3A%22%24%7Binput%3AcollectionName%7D%22%7D%7D&inputs=%5B%7B%22type%22%3A%22promptString%22%2C%22id%22%3A%22qdrantUrl%22%2C%22description%22%3A%22Qdrant+URL%22%7D%2C%7B%22type%22%3A%22promptString%22%2C%22id%22%3A%22qdrantApiKey%22%2C%22description%22%3A%22Qdrant+API+Key%22%2C%22password%22%3Atrue%7D%2C%7B%22type%22%3A%22promptString%22%2C%22id%22%3A%22collectionName%22%2C%22description%22%3A%22Collection+Name%22%7D%5D&quality=insiders)\n\n#### Manual Installation\n\nAdd the following JSON block to your User Settings (JSON) file in VS Code. You can do this by pressing `Ctrl + Shift + P` and typing `Preferences: Open User Settings (JSON)`.\n\n```json\n{\n  \"mcp\": {\n    \"inputs\": [\n      {\n        \"type\": \"promptString\",\n        \"id\": \"qdrantUrl\",\n        \"description\": \"Qdrant URL\"\n      },\n      {\n        \"type\": \"promptString\",\n        \"id\": \"qdrantApiKey\",\n        \"description\": \"Qdrant API Key\",\n        \"password\": true\n      },\n      {\n        \"type\": \"promptString\",\n        \"id\": \"collectionName\",\n        \"description\": \"Collection Name\"\n      }\n    ],\n    \"servers\": {\n      \"qdrant\": {\n        \"command\": \"uvx\",\n        \"args\": [\"mcp-server-qdrant\"],\n        \"env\": {\n          \"QDRANT_URL\": \"${input:qdrantUrl}\",\n          \"QDRANT_API_KEY\": \"${input:qdrantApiKey}\",\n          \"COLLECTION_NAME\": \"${input:collectionName}\"\n        }\n      }\n    }\n  }\n}\n```\n\nOr if you prefer using Docker, add this configuration instead:\n\n```json\n{\n  \"mcp\": {\n    \"inputs\": [\n      {\n        \"type\": \"promptString\",\n        \"id\": \"qdrantUrl\",\n        \"description\": \"Qdrant URL\"\n      },\n      {\n        \"type\": \"promptString\",\n        \"id\": \"qdrantApiKey\",\n        \"description\": \"Qdrant API Key\",\n        \"password\": true\n      },\n      {\n        \"type\": \"promptString\",\n        \"id\": \"collectionName\",\n        \"description\": \"Collection Name\"\n      }\n    ],\n    \"servers\": {\n      \"qdrant\": {\n        \"command\": \"docker\",\n        \"args\": [\n          \"run\",\n          \"-p\", \"8000:8000\",\n          \"-i\",\n          \"--rm\",\n          \"-e\", \"QDRANT_URL\",\n          \"-e\", \"QDRANT_API_KEY\",\n          \"-e\", \"COLLECTION_NAME\",\n          \"mcp-server-qdrant\"\n        ],\n        \"env\": {\n          \"QDRANT_URL\": \"${input:qdrantUrl}\",\n          \"QDRANT_API_KEY\": \"${input:qdrantApiKey}\",\n          \"COLLECTION_NAME\": \"${input:collectionName}\"\n        }\n      }\n    }\n  }\n}\n```\n\nAlternatively, you can create a `.vscode/mcp.json` file in your workspace with the following content:\n\n```json\n{\n  \"inputs\": [\n    {\n      \"type\": \"promptString\",\n      \"id\": \"qdrantUrl\",\n      \"description\": \"Qdrant URL\"\n    },\n    {\n      \"type\": \"promptString\",\n      \"id\": \"qdrantApiKey\",\n      \"description\": \"Qdrant API Key\",\n      \"password\": true\n    },\n    {\n      \"type\": \"promptString\",\n      \"id\": \"collectionName\",\n      \"description\": \"Collection Name\"\n    }\n  ],\n  \"servers\": {\n    \"qdrant\": {\n      \"command\": \"uvx\",\n      \"args\": [\"mcp-server-qdrant\"],\n      \"env\": {\n        \"QDRANT_URL\": \"${input:qdrantUrl}\",\n        \"QDRANT_API_KEY\": \"${input:qdrantApiKey}\",\n        \"COLLECTION_NAME\": \"${input:collectionName}\"\n      }\n    }\n  }\n}\n```\n\nFor workspace configuration with Docker, use this in `.vscode/mcp.json`:\n\n```json\n{\n  \"inputs\": [\n    {\n      \"type\": \"promptString\",\n      \"id\": \"qdrantUrl\",\n      \"description\": \"Qdrant URL\"\n    },\n    {\n      \"type\": \"promptString\",\n      \"id\": \"qdrantApiKey\",\n      \"description\": \"Qdrant API Key\",\n      \"password\": true\n    },\n    {\n      \"type\": \"promptString\",\n      \"id\": \"collectionName\",\n      \"description\": \"Collection Name\"\n    }\n  ],\n  \"servers\": {\n    \"qdrant\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"-p\", \"8000:8000\",\n        \"-i\",\n        \"--rm\",\n        \"-e\", \"QDRANT_URL\",\n        \"-e\", \"QDRANT_API_KEY\",\n        \"-e\", \"COLLECTION_NAME\",\n        \"mcp-server-qdrant\"\n      ],\n      \"env\": {\n        \"QDRANT_URL\": \"${input:qdrantUrl}\",\n        \"QDRANT_API_KEY\": \"${input:qdrantApiKey}\",\n        \"COLLECTION_NAME\": \"${input:collectionName}\"\n      }\n    }\n  }\n}\n```\n\n## Contributing\n\nIf you have suggestions for how mcp-server-qdrant could be improved, or want to report a bug, open an issue!\nWe'd love all and any contributions.\n\n### Testing `mcp-server-qdrant` locally\n\nThe [MCP inspector](https://github.com/modelcontextprotocol/inspector) is a developer tool for testing and debugging MCP\nservers. It runs both a client UI (default port 5173) and an MCP proxy server (default port 3000). Open the client UI in\nyour browser to use the inspector.\n\n```shell\nQDRANT_URL=\":memory:\" COLLECTION_NAME=\"test\" \\\nfastmcp dev src/mcp_server_qdrant/server.py\n```\n\nOnce started, open your browser to http://localhost:5173 to access the inspector interface.\n\n## License\n\nThis MCP server is licensed under the Apache License 2.0. This means you are free to use, modify, and distribute the\nsoftware, subject to the terms and conditions of the Apache License 2.0. For more details, please see the LICENSE file\nin the project repository.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "qdrant",
        "search",
        "ai",
        "servers qdrant",
        "server qdrant",
        "semantic search"
      ],
      "category": "official-servers"
    },
    "salesforcecli--mcp": {
      "owner": "salesforcecli",
      "name": "mcp",
      "url": "https://github.com/salesforcecli/mcp",
      "imageUrl": "/freedevtools/mcp/pfp/salesforcecli.webp",
      "description": "Official Salesforce DX MCP server enabling secure, natural language-driven interaction with Salesforce orgs through LLMs. Built with enhanced security using TypeScript libraries, encrypted authentication, and granular access control. Supports comprehensive Salesforce development workflows including metadata operations, data management, and org administration.",
      "stars": 170,
      "forks": 47,
      "license": "Apache License 2.0",
      "language": "TypeScript",
      "updated_at": "2025-10-02T07:51:21Z",
      "readme_content": "# mcp\n\nMCP Server for Interacting with Salesforce Orgs\n\n[![NPM](https://img.shields.io/npm/v/@salesforce/mcp.svg?label=@salesforce/mcp)](https://www.npmjs.com/package/@salesforce/mcp) [![License](https://img.shields.io/badge/License-Apache--2.0-blue.svg)](https://opensource.org/license/apache-2-0)\n\n## Feedback\n\nReport bugs and issues [here](https://github.com/forcedotcom/mcp/issues).  \nFor feature requests and other related topics, start a Discussion [here](https://github.com/forcedotcom/mcp/discussions).  \n\n## Documentation\n\nFor complete documentation about the Salesforce DX MCP Server, see [this section](https://developer.salesforce.com/docs/atlas.en-us.sfdx_dev.meta/sfdx_dev/sfdx_dev_mcp.htm) in the _Salesforce DX Developer Guide_. The docs include:\n\n* Comprehensive overview, including details about the security features.\n* Quick start guide.\n* Multiple examples of configuring the server in your MCP client.\n* Sample prompts for invoking the core DX MCP tools.\n\n[Here are the release notes.](https://github.com/forcedotcom/mcp/tree/main/releasenotes)\n\n## Overview of the Salesforce DX MCP Server (Beta)\n\nThe Salesforce DX MCP Server is a specialized Model Context Protocol (MCP) implementation designed to facilitate seamless interaction between large language models (LLMs) and Salesforce orgs. This MCP server provides a robust set of tools and capabilities that enable LLMs to read, manage, and operate Salesforce resources securely.\n\n> [!NOTE]\n> _Salesforce DX MCP Server is a pilot or beta service that is subject to the Beta Services Terms at [Agreements - Salesforce.com](https://www.salesforce.com/company/legal/) or a written Unified Pilot Agreement if executed by Customer, and applicable terms in the [Product Terms Directory](https://ptd.salesforce.com/). Use of this pilot or beta service is at the Customer's sole discretion._\n\n## Configure the DX MCP Server\n\nConfigure the Salesforce DX MCP Server for your MCP client by updating its associated MCP JSON file; each client is slightly different, so check your MCP client documentation for details. \n\nHere's an example for VS Code with Copilot in which you create and update a `.vscode/mcp.json` file in your project:\n\n```\n{\n     \"servers\": {\n       \"Salesforce DX\": {\n         \"command\": \"npx\",\n         \"args\": [\"-y\", \"@salesforce/mcp\", \n         \"--orgs\", \"DEFAULT_TARGET_ORG\", \n         \"--toolsets\", \"orgs,metadata,data,users\",\n         \"--tools\", \"run_apex_tests\",\n         \"--allow-non-ga-tools\"]\n       }\n     }\n}\n```\nThe `args` format shown in the preceding example is the same for all MCP clients; it's how you customize the DX MCP Server for your particular environment. Notes:\n\n- The `\"-y\", \"@salesforce/mcp\"` part tells `npx` to automatically install the `@salesforce/mcp` package instead of asking permission. Don't change this. \n- See the *Reference* sections below for the possible flags you can pass the `args` option, and possible values you can pass to the `--orgs`, `--toolsets`, and `--tools` flags. \n- When writing the `args` option, surround both the flag names and their values in double quotes, and separate all flags and values with commas. Some flags are Boolean and don't take a value.\n- The preceding example shows three flags that take a string value (`--orgs`, `--toolsets`, and `--tools`) and one Boolean flag (`--allow-non-ga-tools`).  This configuration starts a DX MCP Server that enables all the MCP tools in the `orgs`, `metadata`, `data`, and `users` toolsets and a specific tool called `run_apex_tests`.  It also enables tools in these configured toolsets that aren't yet generally available. \n\n<details>\n<summary>Reference: Available Flags for the `args` Option</summary>\n\n## Reference: Available Flags for the \"args\" Option\n\nThese are the flags that you can pass to the `args` option. \n\n| Flag Name | Description | Required? |Notes |\n| -----------------| -------| ------- | ----- |\n| `--orgs` | One or more orgs that you've locally authorized. | Yes | You must specify at least one org. <br/> <br/>See [Configure Orgs](README.md#configure-orgs) for the values you can pass to this flag. |\n| `--toolsets` | Sets of tools, based on functionality, that you want to enable. | No | Set to \"all\" to enable every tool in every toolset. <br/> <br/>See [Configure Toolsets](README.md#configure-toolsets) for the values you can pass to this flag.|\n| `--tools` | Individual tool names that you want to enable. | No | You can use this flag in combination with the `--toolsets` flag. For example, you can enable all tools in one toolset, and just one tool in a different toolset. |\n| `--no-telemetry` | Boolean flag to disable telemetry, the automatic collection of data for monitoring and analysis. | No | Telemetry is enabled by default, so specify this flag to disable it.  |\n| `--debug` | Boolean flag that requests that the DX MCP Server print debug logs. | No | Debug mode is disabled by default. <br/> <br/>**NOTE:** Not all MCP clients expose MCP logs, so this flag might not work for all IDEs. |\n| `--allow-non-ga-tools` | Boolean flag to allow the DX MCP Server to use both the generally available (GA) and NON-GA tools that are in the toolsets or tools you specify. | No | By default, the DX MCP server uses only the tools marked GA. |\n| `--dynamic-tools` | (experimental) Boolean flag that enables dynamic tool discovery and loading. When specified, the DX MCP server starts with a minimal set of core tools and loads new tools as needed. | No| This flag is useful for reducing the initial context size and improving LLM performance. Dynamic tool discovery is disabled by default.<br/> <br/>**NOTE:** This feature works in VSCode and Cline but may not work in other environments.|\n\n</details>\n<details>\n\n<summary>Reference: Configure Orgs</summary>\n\n## Configure Orgs\n\nThe Salesforce MCP tools require an org, and so you must include the required `--orgs` flag to specify at least one authorized org when you configure the MCP server. Separate multiple values with commas.\n\nYou must explicitly [authorize the orgs](https://developer.salesforce.com/docs/atlas.en-us.sfdx_dev.meta/sfdx_dev/sfdx_dev_auth_web_flow.htm) on your computer before the MCP server can access them. Use the `org login web` Salesforce CLI command or the VS Code **SFDX: Authorize an Org** command from the command palette.\n\nThese are the available values for the `--orgs` flag:\n\n| --orgs Value | Description |\n| -------- | ---------- |\n| `DEFAULT_TARGET_ORG` | Allow access to your default org. If you've set a local default org in your DX project, the MCP server uses it. If not, the server uses a globally-set default org.|\n| `DEFAULT_TARGET_DEV_HUB` | Allow access to your default Dev Hub org. If you've set a local default Dev Hub org in your DX project, the MCP server uses it. If not, the server uses a globally-set default Dev Hub org.|\n| `ALLOW_ALL_ORGS` | Allow access to all authorized orgs. Use this value with caution.|\n| `<username or alias>` | Allow access to a specific org by specifying its username or alias.|\n\n</details>\n\n<details>\n<summary>Reference: Configure Toolsets and Tools</summary>\n\n## Configure Toolsets\n\nThe Salesforce DX MCP Server supports **toolsets** - a way to selectively enable different groups of MCP tools based on your needs. This allows you to run the MCP server with only the tools you require, which in turn reduces the context.\n\nUse the `--toolsets` flag to specify the toolsets when you configure the Salesforce DX MCP Server. Separate multiple toolsets with commas. \n\nThese are the available toolsets.\n\n| Toolset| Description|\n| ----- | ----- |\n| `all` | Enables all available tools from all toolsets. Use caution, this will load over 60 tools. |\n| `orgs` | [Tools to manage your authorized orgs.](README.md#orgs-toolset)|\n| `data` | [Tools to manage the data in your org, such as listing all accounts.](README.md#data-toolset)|\n| `users` | [Tools to manage org users, such as assigning a permission set.](README.md#users-toolset)|\n| `metadata` | [Tools to deploy and retrieve metadata to and from your org and your DX project.](README.md#metadata-toolset)|\n| `testing` | [Tools to test your code and features](README.md#testing-toolset)|\n| `other` | [Other useful tools, such as tools for static analysis of your code using Salesforce Code Analyzer.](README.md#other-toolset)|\n| `mobile` | [Tools for mobile development and capabilities.](README.md#mobile-toolset)|\n| `mobile-core` | [A subset of mobile tools focused on essential mobile capabilities.](README.md#mobile-core-toolset)|\n| `aura-experts` | [Tools which provides Aura component analysis, blueprinting, and migration expertise.](README.md#aura-experts-toolset)|\n| `lwc-experts`  | [Tools to assist with LWC development, testing, optimization, and best practices.](README.md#lwc-experts-toolset)|\n\n## Configure Tools\n\nThe Salesforce DX MCP Server also supports registering individual **tools**. This can be used in combination with **toolsets** to further fine-tune registered tools.\n\nUse the `--tools` flag to enable specific tools when you configure the Salesforce DX MCP Server. Separate multiple tools with commas. The `--tools` flag is optional.\n\nThe following sections list all the tools that are included in a specific toolset. The tools marked NON-GA are not yet generally available, specify the `--allow-non-ga-tools` flag to use them. \n\n### Core Toolset (always enabled)\n\n- `get_username` - Determines the appropriate username or alias for Salesforce operations, handling both default orgs and Dev Hubs.\n- `resume_tool_operation` - Resumes a long-running operation that wasn't completed by another tool.\n\n### Orgs Toolset\n\n- `list_all_orgs` - Lists all configured Salesforce orgs, with optional connection status checking.\n- `create_org_snapshot` - (NON-GA) Create a scratch org snapshot. \n- `create_scratch_org` - (NON-GA) Create a scratch org. \n- `delete_org` - (NON-GA) Delete a locally-authorized Salesforce scratch org or sandbox.\n- `org_open` - (NON-GA) Open an org in a browser. \n\n**NOTE:** The tools marked NON-GA are not yet generally available, specify the `--allow-non-ga-tools` flag to use them. \n\n### Data Toolset\n\n- `run_soql_query` - Runs a SOQL query against a Salesforce org.\n\n### Users Toolset\n\n- `assign_permission_set` - Assigns a permission set to the user or on behalf of another user.\n\n### Metadata Toolset\n\n- `deploy_metadata` - Deploys metadata from your DX project to an org.\n- `retrieve_metadata` - Retrieves metadata from your org to your DX project.\n\n### Testing Toolset\n\n- `run_agent_test` - Executes agent tests in your org.\n- `run_apex_test` - Executes apex tests in your org.\n\n### Mobile Toolset\n\n- `create_mobile_lwc_app_review` - Provides TypeScript API documentation for Salesforce LWC App Review Service, offering expert guidance for implementing app review features in Lightning Web Components.\n- `create_mobile_lwc_ar_space_capture` - Provides TypeScript API documentation for Salesforce L    WC AR Space Capture, offering expert guidance for implementing AR space capture features in Lightning Web Components.\n- `create_mobile_lwc_barcode_scanner` - Provides TypeScript API documentation for Salesforce LWC Barcode Scanner, offering expert guidance for implementing barcode scanning features in Lightning Web Components.\n- `create_mobile_lwc_biometrics` - Provides TypeScript API documentation for Salesforce LWC Biometrics Service, offering expert guidance for implementing biometric authentication features in Lightning Web Components.\n- `create_mobile_lwc_calendar` - Provides TypeScript API documentation for Salesforce LWC Calendar Service, offering expert guidance for implementing calendar integration features in Lightning Web Components.\n- `create_mobile_lwc_contacts` - Provides TypeScript API documentation for Salesforce LWC Contacts Service, offering expert guidance for implementing contacts management features in Lightning Web Components.\n- `create_mobile_lwc_document_scanner` - Provides TypeScript API documentation for Salesforce LWC Document Scanner, offering expert guidance for implementing document scanning features in Lightning Web Components.\n- `create_mobile_lwc_geofencing` - Provides TypeScript API documentation for Salesforce LWC Geofencing Service, offering expert guidance for implementing geofencing features in Lightning Web Components.\n- `create_mobile_lwc_location` - Provides TypeScript API documentation for Salesforce LWC Location Service, offering expert guidance for implementing location services in Lightning Web Components.\n- `create_mobile_lwc_nfc` - Provides TypeScript API documentation for Salesforce LWC NFC Service, offering expert guidance for implementing NFC features in Lightning Web Components.\n- `create_mobile_lwc_payments` - Provides TypeScript API documentation for Salesforce LWC Payments Service, offering expert guidance for implementing payment processing features in Lightning Web Components.\n- `get_mobile_lwc_offline_analysis` - Analyzes Lightning Web Components for mobile-specific issues and provides detailed recommendations for mobile offline compatibility and performance improvements.\n- `get_mobile_lwc_offline_guidance` - Provides structured review instructions to detect and remediate mobile offline code violations in Lightning Web Components for Salesforce Mobile Apps.\n\n### Mobile-core Toolset\n\n- `create_mobile_lwc_barcode_scanner` - Provides TypeScript API documentation for Salesforce LWC Barcode Scanner, offering expert guidance for implementing barcode scanning features in Lightning Web Components.\n- `create_mobile_lwc_biometrics` - Provides TypeScript API documentation for Salesforce LWC Biometrics Service, offering expert guidance for implementing biometric authentication features in Lightning Web Components.\n- `create_mobile_lwc_location` - Provides TypeScript API documentation for Salesforce LWC Location Service, offering expert guidance for implementing location services in Lightning Web Components.\n- `get_mobile_lwc_offline_analysis` - Analyzes Lightning Web Components for mobile-specific issues and provides detailed recommendations for mobile offline compatibility and performance improvements.\n- `get_mobile_lwc_offline_guidance` - Provides structured review instructions to detect and remediate mobile offline code violations in Lightning Web Components for Salesforce Mobile Apps.\n\n### Aura Experts Toolset\n\n - `create_aura_blueprint_draft` - (GA)\nCreates a comprehensive Product Requirements Document (PRD) blueprint for Aura component migration. Analyzes Aura component files and generates framework-agnostic specifications suitable for LWC migration, including business requirements, technical patterns, and migration guidelines.\n\n - `enhance_aura_blueprint_draft` - (GA)\nEnhances an existing draft PRD with expert analysis and unknown resolution. Takes a draft blueprint and applies specialized Aura expert knowledge to resolve dependencies, add technical insights, and improve the migration specifications for better LWC implementation guidance.\n\n - `transition_prd_to_lwc` - (GA)\nProvides migration bridge guidance for creating LWC components from Aura specifications. Takes the enhanced PRD and generates specific implementation guidance, platform service mappings, and step-by-step instructions for building the equivalent LWC component.\n\n - `orchestrate_aura_migration` - (GA)\nOrchestrates the complete Aura to LWC migration workflow. Provides end-to-end guidance for the entire migration process, from initial analysis through final implementation, including best practices, tooling recommendations, and quality assurance steps.\n\n### Lwc Experts Toolset\n\n#### Component Development\n\n - `create_lwc_component` - (GA) Creates complete LWC components from PRD specifications with proper structure and best practices\n - `create_lwc_jest_tests` - (GA) Generates comprehensive Jest test suites for LWC components with coverage and mocking\n - `review_lwc_jest_tests` - (GA) Reviews and validates Jest test implementations for LWC components\n\n#### Development Guidelines\n\n - `guide_lwc_accessibility` - (GA) Provides accessibility guidelines and testing instructions for LWC components\n - `guide_lwc_best_practices` - (GA) Offers LWC development best practices and coding standards guidance\n - `guide_lwc_development` - (GA) Comprehensive LWC development workflow and implementation guidelines\n - `guide_lwc_rtl_support` - (GA) Right-to-Left internationalization support and RTL development guidance\n - `guide_lwc_slds2_uplift_linter_fixes` - (GA) Analyzes the given LWC code along with the slds-linter output to fix issues using the SLDS2 knowledge\n - `guide_lwc_security` - (GA) Comprehensive security analysis in accordance with Product Security Guidelines and Lightning Web Security Guidelines\n\n#### Workflow Tools\n\n - `orchestrate_lwc_component_creation` - (GA) Step-by-step component creation workflow guidance\n - `orchestrate_lwc_component_optimization` - (GA)  Performance optimization and best practices for LWC components\n - `orchestrate_lwc_component_testing` - (GA) Comprehensive testing workflow and test generation guidance\n - `orchestrate_lwc_slds2_uplift` - (GA) Migration guidance for upgrading to SLDS2 design system\n\n#### LDS (Lightning Design System) Tools\n\n - `explore_lds_uiapi` - (GA) Explores and documents Lightning Design System UI API capabilities\n - `guide_lds_data_consistency` - (GA) Data consistency patterns and best practices for LDS components\n - `guide_lds_development` - (GA) LDS development guidelines and component integration\n - `guide_lds_referential_integrity` - (GA) Referential integrity patterns for LDS data management\n - `orchestrate_lds_data_requirements` - (GA) Step-by-step guidance for analyzing and clarifying LDS data requirements to produce PRD-ready specifications.\n\n#### Migration & Integration Tools\n\n - `verify_aura_migration_completeness` - (GA) Aura to LWC migration completeness checklist and validation\n - `guide_figma_to_lwc_conversion` - (GA) Converts Figma designs to LWC component specifications\n - `run_lwc_accessibility_jest_tests` - (GA) Accessibility testing utilities and Jest integration for LWC components\n\n### Code-Analysis Toolset\n\n- `run_code_analyzer` - (NON-GA) Performs a static analysis of your code using Salesforce Code Analyzer. Includes validating that the code conforms to best practices, checking for security vulnerabilities, and identifying possible performance issues.\n- `describe_code_analyzer_rule` - (NON-GA) Gets the description of a Salesforce Code Analyzer rule, including the engine it belongs to, its severity, and associated tags.\n\n</details>\n",
      "npm_url": "https://www.npmjs.com/package/mcp",
      "npm_downloads": 17508,
      "keywords": [
        "salesforcecli",
        "dx",
        "salesforce",
        "salesforcecli mcp",
        "salesforce dx",
        "dx mcp"
      ],
      "category": "official-servers"
    },
    "screenshotone--mcp": {
      "owner": "screenshotone",
      "name": "mcp",
      "url": "https://github.com/screenshotone/mcp",
      "imageUrl": "/freedevtools/mcp/pfp/screenshotone.webp",
      "description": "Render website screenshots and obtain them as images to integrate screenshot capabilities into applications. Capture and utilize website visuals effortlessly through a powerful API.",
      "stars": 31,
      "forks": 14,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-09-17T13:41:30Z",
      "readme_content": "# ScreenshotOne MCP Server\n\nAn official implementation of an [MCP (Model Context Protocol)](https://modelcontextprotocol.io/) server for [ScreenshotOne](https://screenshotone.com).\n\n[A few more words about why it was built and some thoughts about the future of MCP](https://screenshotone.com/blog/mcp-server/).\n\n<a href=\"https://glama.ai/mcp/servers/nq85q0596a\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/nq85q0596a/badge\" alt=\"ScreenshotOne Server MCP server\" />\n</a>\n\n## Tools\n\n-   `render-website-screenshot`: Render a screenshot of a website and returns it as an image.\n\n## Usage\n\n### Build it\n\nAlways install dependencies and build it first:\n\n```bash\nnpm install && npm run build\n```\n\n### Get your ScreenshotOne API key\n\nSign up at [ScreenshotOne](https://screenshotone.com) and get your API key.\n\n### With Claude for Desktop\n\nAdd the following to your `~/Library/Application\\ Support/Claude/claude_desktop_config.json`:\n\n```json\n{\n    \"mcpServers\": {\n        \"screenshotone\": {\n            \"command\": \"node\",\n            \"args\": [\"path/to/screenshotone/mcp/build/index.js\"],\n            \"env\": {\n                \"SCREENSHOTONE_API_KEY\": \"<your api key>\"\n            }\n        }\n    }\n}\n```\n\n### Standalone or for other projects\n\n```bash\nSCREENSHOTONE_API_KEY=your_api_key && node build/index.js\n```\n\n## License\n\n`ScreenshotOne MCP Server` is licensed [under the MIT License](LICENSE).\n",
      "npm_url": "https://www.npmjs.com/package/mcp",
      "npm_downloads": 17508,
      "keywords": [
        "screenshotone",
        "screenshots",
        "screenshot",
        "servers screenshotone",
        "screenshotone mcp",
        "website screenshots"
      ],
      "category": "official-servers"
    },
    "semgrep--mcp": {
      "owner": "semgrep",
      "name": "mcp",
      "url": "https://github.com/semgrep/mcp",
      "imageUrl": "/freedevtools/mcp/pfp/semgrep.webp",
      "description": "Scan code for security vulnerabilities and create custom rules with Semgrep. It provides an interface for advanced code analysis and security enhancements.",
      "stars": 581,
      "forks": 51,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-10-02T06:30:24Z",
      "readme_content": "## **‚ö†Ô∏è The Semgrep MCP server has been moved from a standalone repo to the [main `semgrep` repository!](https://github.com/semgrep/semgrep) ‚ö†Ô∏è**\n**This repository has been deprecated, and further updates to the Semgrep MCP server will be made via the official `semgrep` binary.**\n\n<p align=\"center\">\n  <a href=\"https://semgrep.dev\">\n    <picture>\n      <source media=\"(prefers-color-scheme: light)\" srcset=\"images/semgrep-logo-light.svg\">\n      <source media=\"(prefers-color-scheme: dark)\" srcset=\"images/semgrep-logo-dark.svg\">\n      <img src=\"https://raw.githubusercontent.com/semgrep/mcp/main/images/semgrep-logo-light.svg\" height=\"60\" alt=\"Semgrep logo\"/>\n    </picture>\n  </a>\n</p>\n<p align=\"center\">\n  <a href=\"https://semgrep.dev/docs/\">\n      <img src=\"https://img.shields.io/badge/Semgrep-docs-2acfa6?style=flat-square\" alt=\"Documentation\" />\n  </a>\n  <a href=\"https://go.semgrep.dev/slack\">\n    <img src=\"https://img.shields.io/badge/Slack-4.5k%20-4A154B?style=flat-square&logo=slack&logoColor=white\" alt=\"Join Semgrep community Slack\" />\n  </a>\n  <a href=\"https://www.linkedin.com/company/semgrep/\">\n    <img src=\"https://img.shields.io/badge/LinkedIn-follow-0a66c2?style=flat-square\" alt=\"Follow on LinkedIn\" />\n  </a>\n  <a href=\"https://x.com/intent/follow?screen_name=semgrep\">\n    <img src=\"https://img.shields.io/badge/semgrep-000000?style=flat-square&logo=x&logoColor=white?style=flat-square\" alt=\"Follow @semgrep on X\" />\n  </a>\n</p>\n\n# Semgrep MCP Server\n[![Add MCP Server semgrep to LM Studio](https://files.lmstudio.ai/deeplink/mcp-install-light.svg)](https://lmstudio.ai/install-mcp?name=semgrep&config=eyJ1cmwiOiJodHRwczovL21jcC5zZW1ncmVwLmFpL21jcCIsImhlYWRlcnMiOnsiQXV0aG9yaXphdGlvbiI6IkJlYXJlciA8WU9VUl9IRl9UT0tFTj4ifX0%3D)\n[![Install in Cursor](https://img.shields.io/badge/Cursor-uv-0098FF?style=flat-square)](cursor://anysphere.cursor-deeplink/mcp/install?name=semgrep&config=eyJjb21tYW5kIjoidXZ4IiwiYXJncyI6WyJzZW1ncmVwLW1jcCJdfQ==)\n[![Install in VS Code UV](https://img.shields.io/badge/VS_Code-uv-0098FF?style=flat-square&logo=githubcopilot&logoColor=white)](https://insiders.vscode.dev/redirect/mcp/install?name=semgrep&config=%7B%22command%22%3A%22uvx%22%2C%22args%22%3A%5B%22semgrep-mcp%22%5D%7D)\n[![Install in VS Code Docker](https://img.shields.io/badge/VS_Code-docker-0098FF?style=flat-square&logo=githubcopilot&logoColor=white)](https://insiders.vscode.dev/redirect/mcp/install?name=semgrep&config=%7B%22command%22%3A%22docker%22%2C%22args%22%3A%5B%22run%22%2C%20%22-i%22%2C%20%22--rm%22%2C%20%22ghcr.io%2Fsemgrep%2Fmcp%22%2C%20%22-t%22%2C%20%22stdio%22%5D%7D)\n[![Install in VS Code semgrep.ai](https://img.shields.io/badge/VS_Code-semgrep.ai-0098FF?style=flat-square&logo=githubcopilot&logoColor=white)](https://insiders.vscode.dev/redirect/mcp/install?name=semgrep.ai&config=%7B%22type%22%3A%20%22sse%22%2C%20%22url%22%3A%22https%3A%2F%2Fmcp.semgrep.ai%2Fsse%22%7D)\n[![PyPI](https://img.shields.io/pypi/v/semgrep-mcp?style=flat-square&color=blue&logo=python&logoColor=white)](https://pypi.org/project/semgrep-mcp/)\n[![Docker](https://img.shields.io/badge/docker-ghcr.io%2Fsemgrep%2Fmcp-0098FF?style=flat-square&logo=docker&logoColor=white)](https://ghcr.io/semgrep/mcp)\n[![Install in VS Code Insiders](https://img.shields.io/badge/VS_Code_Insiders-uv-24bfa5?style=flat-square&logo=githubcopilot&logoColor=white)](https://insiders.vscode.dev/redirect/mcp/install?name=semgrep&config=%7B%22command%22%3A%22uvx%22%2C%22args%22%3A%5B%22semgrep-mcp%22%5D%7D&quality=insiders)\n[![Install in VS Code Insiders](https://img.shields.io/badge/VS_Code_Insiders-docker-24bfa5?style=flat-square&logo=githubcopilot&logoColor=white)](https://insiders.vscode.dev/redirect/mcp/install?name=semgrep&config=%7B%22command%22%3A%22docker%22%2C%22args%22%3A%5B%22run%22%2C%20%22-i%22%2C%20%22--rm%22%2C%20%22ghcr.io%2Fsemgrep%2Fmcp%22%2C%20%22-t%22%2C%20%22stdio%22%5D%7D&quality=insiders)\n\nA Model Context Protocol (MCP) server for using [Semgrep](https://semgrep.dev) to scan code for security vulnerabilities. Secure your [vibe coding](https://semgrep.dev/blog/2025/giving-appsec-a-seat-at-the-vibe-coding-table/)! üòÖ\n\n[Model Context Protocol (MCP)](https://modelcontextprotocol.io/) is a standardized API for LLMs, Agents, and IDEs like Cursor, VS Code, Windsurf, or anything that supports MCP, to get specialized help, get context, and harness the power of tools. Semgrep is a fast, deterministic static analysis tool that semantically understands many [languages](https://semgrep.dev/docs/supported-languages) and comes with over [5,000 rules](https://semgrep.dev/registry). üõ†Ô∏è\n\n> [!NOTE]\n> This beta project is under active development. We would love your feedback, bug reports, feature requests, and code. Join the `#mcp` [community Slack](https://go.semgrep.dev/slack) channel!\n\n## Contents\n\n- [Semgrep MCP Server](#semgrep-mcp-server)\n  - [Contents](#contents)\n  - [Getting started](#getting-started)\n    - [Cursor](#cursor)\n    - [ChatGPT](#chatgpt)\n    - [Hosted Server](#hosted-server)\n      - [Cursor](#cursor-1)\n  - [Demo](#demo)\n  - [API](#api)\n    - [Tools](#tools)\n      - [Scan Code](#scan-code)\n      - [Understand Code](#understand-code)\n      - [Cloud Platform (login and Semgrep token required)](#cloud-platform-login-and-semgrep-token-required)\n      - [Meta](#meta)\n    - [Prompts](#prompts)\n    - [Resources](#resources)\n  - [Usage](#usage)\n    - [Standard Input/Output (stdio)](#standard-inputoutput-stdio)\n      - [Python](#python)\n      - [Docker](#docker)\n    - [Streamable HTTP](#streamable-http)\n      - [Python](#python-1)\n      - [Docker](#docker-1)\n    - [Server-sent events (SSE)](#server-sent-events-sse)\n      - [Python](#python-2)\n      - [Docker](#docker-2)\n  - [Semgrep AppSec Platform](#semgrep-appsec-platform)\n  - [Integrations](#integrations)\n    - [Cursor IDE](#cursor-ide)\n    - [VS Code / Copilot](#vs-code--copilot)\n      - [Manual Configuration](#manual-configuration)\n      - [Using Docker](#using-docker)\n    - [Windsurf](#windsurf)\n    - [Claude Desktop](#claude-desktop)\n    - [Claude Code](#claude-code)\n    - [OpenAI](#openai)\n      - [Agents SDK](#agents-sdk)\n    - [Custom clients](#custom-clients)\n      - [Example Python SSE client](#example-python-sse-client)\n  - [Contributing, community, and running from source](#contributing-community-and-running-from-source)\n    - [Similar tools üîç](#similar-tools-)\n    - [Community projects üåü](#community-projects-)\n    - [MCP server registries](#mcp-server-registries)\n\n## Getting started\n\nRun the [Python package](https://pypi.org/p/semgrep-mcp) as a CLI command using [`uv`](https://docs.astral.sh/uv/guides/tools/):\n\n```bash\nuvx semgrep-mcp # see --help for more options\n```\n\nOr, run as a [Docker container](https://ghcr.io/semgrep/mcp):\n\n```bash\ndocker run -i --rm ghcr.io/semgrep/mcp -t stdio\n```\n\n### Cursor\n\nExample [`mcp.json`](https://docs.cursor.com/context/model-context-protocol)\n\n```json\n{\n  \"mcpServers\": {\n    \"semgrep\": {\n      \"command\": \"uvx\",\n      \"args\": [\"semgrep-mcp\"],\n      \"env\": {\n        \"SEMGREP_APP_TOKEN\": \"<token>\"\n      }\n    }\n  }\n}\n\n```\n\nAdd an instruction to your [`.cursor/rules`](https://docs.cursor.com/context/rules-for-ai) to use automatically:\n\n```text\nAlways scan code generated using Semgrep for security vulnerabilities\n```\n\n### ChatGPT\n\n1. Go to the **Connector Settings** page ([direct link](https://chatgpt.com/admin/ca#settings/ConnectorSettings?create-connector=true))\n1. **Name** the connection `Semgrep`\n1. Set **MCP Server URL** to `https://mcp.semgrep.ai/sse`\n1. Set **Authentication** to `No authentication`\n1. Check the **I trust this application** checkbox\n1. Click **Create**\n\nSee more details at the [official docs](https://platform.openai.com/docs/mcp).\n\n\n### Hosted Server\n\n> [!WARNING]\n> [mcp.semgrep.ai](https://mcp.semgrep.ai) is an experimental server that may break unexpectedly. It will rapidly gain new functionality.üöÄ\n\n#### Cursor\n\n1. **Cmd + Shift + J** to open Cursor Settings\n1. Select **MCP Tools**\n1. Click **New MCP Server**.\n1.\n\n```json\n{\n  \"mcpServers\": {\n    \"semgrep\": {\n      \"type\": \"streamable-http\",\n      \"url\": \"https://mcp.semgrep.ai/mcp\"\n    }\n  }\n}\n```\n\n## Demo\n\n<a href=\"https://www.loom.com/share/8535d72e4cfc4e1eb1e03ea223a702df\"> <img style=\"max-width:300px;\" src=\"https://cdn.loom.com/sessions/thumbnails/8535d72e4cfc4e1eb1e03ea223a702df-1047fabea7261abb-full-play.gif\"> </a>\n\n## API\n\n### Tools\n\nEnable LLMs to perform actions, make deterministic computations, and interact with external services.\n\n#### Scan Code\n\n- `security_check`: Scan code for security vulnerabilities\n- `semgrep_scan`: Scan code files for security vulnerabilities with a given config string\n- `semgrep_scan_with_custom_rule`: Scan code files using a custom Semgrep rule\n\n#### Understand Code\n\n- `get_abstract_syntax_tree`: Output the Abstract Syntax Tree (AST) of code\n\n#### Cloud Platform (login and Semgrep token required)\n- `semgrep_findings`: Fetch Semgrep findings from the Semgrep AppSec Platform API\n\n#### Meta\n\n- `supported_languages`: Return the list of languages Semgrep supports\n- `semgrep_rule_schema`: Fetches the latest semgrep rule JSON Schema\n\n### Prompts\n\nReusable prompts to standardize common LLM interactions.\n\n- `write_custom_semgrep_rule`: Return a prompt to help write a Semgrep rule\n\n### Resources\n\nExpose data and content to LLMs\n\n- `semgrep://rule/schema`: Specification of the Semgrep rule YAML syntax using JSON schema\n- `semgrep://rule/{rule_id}/yaml`: Full Semgrep rule in YAML format from the Semgrep registry\n\n## Usage\n\nThis Python package is published to PyPI as [semgrep-mcp](https://pypi.org/p/semgrep-mcp) and can be installed and run with [pip](https://packaging.python.org/en/latest/guides/installing-using-pip-and-virtual-environments/#install-a-package), [pipx](https://pipx.pypa.io/), [uv](https://docs.astral.sh/uv/), [poetry](https://python-poetry.org/), or any Python package manager.\n\n```text\n$ pipx install semgrep-mcp\n$ semgrep-mcp --help\n\nUsage: semgrep-mcp [OPTIONS]\n\n  Entry point for the MCP server\n\n  Supports both stdio and sse transports. For stdio, it will read from stdin\n  and write to stdout. For sse, it will start an HTTP server on port 8000.\n\nOptions:\n  -v, --version                Show version and exit.\n  -t, --transport [stdio|sse]  Transport protocol to use (stdio or sse)\n  -h, --help                   Show this message and exit.\n```\n\n### Standard Input/Output (stdio)\n\nThe stdio transport enables communication through standard input and output streams. This is particularly useful for local integrations and command-line tools. See the [spec](https://modelcontextprotocol.io/docs/concepts/transports#built-in-transport-types) for more details.\n\n#### Python\n\n```bash\nsemgrep-mcp\n```\n\nBy default, the Python package will run in `stdio` mode. Because it's using the standard input and output streams, it will look like the tool is hanging without any output, but this is expected.\n\n#### Docker\n\nThis server is published to Github's Container Registry ([ghcr.io/semgrep/mcp](http://ghcr.io/semgrep/mcp))\n\n```\ndocker run -i --rm ghcr.io/semgrep/mcp -t stdio\n```\n\nBy default, the Docker container is in `SSE` mode, so you will have to include `-t stdio` after the image name and run with `-i` to run in [interactive](https://docs.docker.com/reference/cli/docker/container/run/#interactive) mode.\n\n### Streamable HTTP\n\nStreamable HTTP enables streaming responses over JSON RPC via HTTP POST requests. See the [spec](https://modelcontextprotocol.io/specification/draft/basic/transports#streamable-http) for more details.\n\nBy default, the server listens on [127.0.0.1:8000/mcp](https://127.0.0.1/mcp) for client connections. To change any of this, set [FASTMCP\\_\\*](https://github.com/modelcontextprotocol/python-sdk/blob/main/src/mcp/server/fastmcp/server.py#L78) environment variables. _The server must be running for clients to connect to it._\n\n#### Python\n\n```bash\nsemgrep-mcp -t streamable-http\n```\n\nBy default, the Python package will run in `stdio` mode, so you will have to include `-t streamable-http`.\n\n#### Docker\n\n```\ndocker run -p 8000:0000 ghcr.io/semgrep/mcp\n```\n\n\n### Server-sent events (SSE)\n\n> [!WARNING]\n> The MCP communiity considers this a legacy transport portcol and is really intended for backwards compatibility. [Streamable HTTP](#streamable-http) is the recommended replacement.\n\nSSE transport enables server-to-client streaming with Server-Send Events for client-to-server and server-to-client communication. See the [spec](https://modelcontextprotocol.io/docs/concepts/transports#server-sent-events-sse) for more details.\n\nBy default, the server listens on [127.0.0.1:8000/sse](https://127.0.0.1/sse) for client connections. To change any of this, set [FASTMCP\\_\\*](https://github.com/modelcontextprotocol/python-sdk/blob/main/src/mcp/server/fastmcp/server.py#L78) environment variables. _The server must be running for clients to connect to it._\n\n#### Python\n\n```bash\nsemgrep-mcp -t sse\n```\n\nBy default, the Python package will run in `stdio` mode, so you will have to include `-t sse`.\n\n#### Docker\n\n```\ndocker run -p 8000:0000 ghcr.io/semgrep/mcp -t sse\n```\n\n## Semgrep AppSec Platform\n\nOptionally, to connect to Semgrep AppSec Platform:\n\n1. [Login](https://semgrep.dev/login/) or sign up\n1. Generate a token from [Settings](https://semgrep.dev/orgs/-/settings/tokens/api)\n1. Add the token to your environment variables:\n   - CLI (`export SEMGREP_APP_TOKEN=<token>`)\n\n   - Docker (`docker run -e SEMGREP_APP_TOKEN=<token>`)\n\n   - MCP config JSON\n\n```json\n    \"env\": {\n      \"SEMGREP_APP_TOKEN\": \"<token>\"\n    }\n```\n\n> [!TIP]\n> Please [reach out for support](https://semgrep.dev/docs/support) if needed. ‚òéÔ∏è\n\n## Integrations\n\n### Cursor IDE\n\nAdd the following JSON block to your `~/.cursor/mcp.json` global or `.cursor/mcp.json` project-specific configuration file:\n\n```json\n{\n  \"mcpServers\": {\n    \"semgrep\": {\n      \"command\": \"uvx\",\n      \"args\": [\"semgrep-mcp\"]\n    }\n  }\n}\n```\n\n\n\nSee [cursor docs](https://docs.cursor.com/context/model-context-protocol) for more info.\n\n### VS Code / Copilot\n\nClick the install buttons at the top of this README for the quickest installation.\n\n#### Manual Configuration\n\nAdd the following JSON block to your User Settings (JSON) file in VS Code. You can do this by pressing `Ctrl + Shift + P` and typing `Preferences: Open User Settings (JSON)`.\n\n```json\n{\n  \"mcp\": {\n    \"servers\": {\n      \"semgrep\": {\n        \"command\": \"uvx\",\n        \"args\": [\"semgrep-mcp\"]\n      }\n    }\n  }\n}\n```\n\nOptionally, you can add it to a file called `.vscode/mcp.json` in your workspace:\n\n```json\n{\n  \"servers\": {\n    \"semgrep\": {\n      \"command\": \"uvx\",\n        \"args\": [\"semgrep-mcp\"]\n    }\n  }\n}\n```\n\n#### Using Docker\n\n```json\n{\n  \"mcp\": {\n    \"servers\": {\n      \"semgrep\": {\n        \"command\": \"docker\",\n        \"args\": [\n          \"run\",\n          \"-i\",\n          \"--rm\",\n          \"ghcr.io/semgrep/mcp\",\n          \"-t\",\n          \"stdio\"\n        ]\n      }\n    }\n  }\n}\n```\n\nSee [VS Code docs](https://code.visualstudio.com/docs/copilot/chat/mcp-servers) for more info.\n\n### Windsurf\n\nAdd the following JSON block to your `~/.codeium/windsurf/mcp_config.json` file:\n\n```json\n{\n  \"mcpServers\": {\n    \"semgrep\": {\n      \"command\": \"uvx\",\n      \"args\": [\"semgrep-mcp\"]\n    }\n  }\n}\n```\n\nSee [Windsurf docs](https://docs.windsurf.com/windsurf/mcp) for more info.\n\n### Claude Desktop\n\nHere is a [short video](https://www.loom.com/share/f4440cbbb5a24149ac17cc7ddcd95cfa) showing Claude Desktop using this server to write a custom rule.\n\nAdd the following JSON block to your `claude_desktop_config.json` file:\n\n```json\n{\n  \"mcpServers\": {\n    \"semgrep\": {\n      \"command\": \"uvx\",\n      \"args\": [\"semgrep-mcp\"]\n    }\n  }\n}\n```\n\nSee [Anthropic docs](https://docs.anthropic.com/en/docs/agents-and-tools/mcp) for more info.\n\n### Claude Code\n\n```bash\nclaude mcp add semgrep uvx semgrep-mcp\n```\n\nSee [Claude Code docs](https://docs.anthropic.com/en/docs/claude-code/tutorials#set-up-model-context-protocol-mcp) for more info.\n\n### OpenAI\n\nSee the offical docs:\n- https://platform.openai.com/docs/mcp\n- https://platform.openai.com/docs/guides/tools-remote-mcp\n\n#### Agents SDK\n\n```python\nasync with MCPServerStdio(\n    params={\n        \"command\": \"uvx\",\n        \"args\": [\"semgrep-mcp\"],\n    }\n) as server:\n    tools = await server.list_tools()\n```\n\nSee [OpenAI Agents SDK docs](https://openai.github.io/openai-agents-python/mcp/) for more info.\n\n### Custom clients\n\n#### Example Python SSE client\n\nSee a full example in [examples/sse_client.py](examples/sse_client.py)\n\n```python\nfrom mcp.client.session import ClientSession\nfrom mcp.client.sse import sse_client\n\n\nasync def main():\n    async with sse_client(\"http://localhost:8000/sse\") as (read_stream, write_stream):\n        async with ClientSession(read_stream, write_stream) as session:\n            await session.initialize()\n            results = await session.call_tool(\n                \"semgrep_scan\",\n                {\n                    \"code_files\": [\n                        {\n                            \"path\": \"hello_world.py\",\n                            \"content\": \"def hello(): print('Hello, World!')\",\n                        }\n                    ]\n                },\n            )\n            print(results)\n```\n\n> [!TIP]\n> Some client libraries want the `URL`: [http://localhost:8000/sse](http://localhost:8000/sse)\n> and others only want the `HOST`: `localhost:8000`.\n> Try out the `URL` in a web browser to confirm the server is running, and there are no network issues.\n\nSee [official SDK docs](https://modelcontextprotocol.io/clients#adding-mcp-support-to-your-application) for more info.\n\n## Contributing, community, and running from source\n\n> [!NOTE]\n> We love your feedback, bug reports, feature requests, and code. Join the `#mcp` [community Slack](https://go.semgrep.dev/slack) channel!\n\nSee [CONTRIBUTING.md](CONTRIBUTING.md) for more info and details on how to run from the MCP server from source code.\n\n### Similar tools üîç\n\n- [semgrep-vscode](https://github.com/semgrep/semgrep-vscode) - Official VS Code extension\n- [semgrep-intellij](https://github.com/semgrep/semgrep-intellij) - IntelliJ plugin\n\n### Community projects üåü\n\n- [semgrep-rules](https://github.com/semgrep/semgrep-rules) - The official collection of Semgrep rules\n- [mcp-server-semgrep](https://github.com/Szowesgad/mcp-server-semgrep) - Original inspiration written by [Szowesgad](https://github.com/Szowesgad) and [stefanskiasan](https://github.com/stefanskiasan)\n\n### MCP server registries\n\n- [Glama](https://glama.ai/mcp/servers/@semgrep/mcp)\n\n<a href=\"https://glama.ai/mcp/servers/@semgrep/mcp\">\n <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/4iqti5mgde/badge\" alt=\"Semgrep Server MCP server\" />\n </a>\n\n- [MCP.so](https://mcp.so/server/mcp/semgrep)\n\n______________________________________________________________________\n\nMade with ‚ù§Ô∏è by the [Semgrep Team](https://semgrep.dev/about/)",
      "npm_url": "https://www.npmjs.com/package/mcp",
      "npm_downloads": 17508,
      "keywords": [
        "semgrep",
        "mcp",
        "vulnerabilities",
        "semgrep provides",
        "semgrep mcp",
        "servers semgrep"
      ],
      "category": "official-servers"
    },
    "stripe--agent-toolkit": {
      "owner": "stripe",
      "name": "agent-toolkit",
      "url": "https://github.com/stripe/agent-toolkit",
      "imageUrl": "/freedevtools/mcp/pfp/stripe.webp",
      "description": "Integrate with Stripe APIs using function calling to manage customers and products, as well as facilitate interactions with various Stripe services.",
      "stars": 966,
      "forks": 146,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-10-02T09:07:59Z",
      "readme_content": "# Stripe Agent Toolkit\n\nThe Stripe Agent Toolkit enables popular agent frameworks including Model Context Protocol (MCP), OpenAI's Agent SDK, LangChain, CrewAI, and Vercel's AI SDK to integrate with Stripe APIs through function calling. The\nlibrary is not exhaustive of the entire Stripe API. It includes support for MCP, Python, and TypeScript and is built directly on top of the Stripe [Python][python-sdk] and [Node][node-sdk] SDKs.\n\nIncluded below are basic instructions, but refer to the [MCP](/modelcontextprotocol) [Python](/python), [TypeScript](/typescript) packages for more information.\n\n## Model Context Protocol\n\nStripe hosts a remote MCP server at `https://mcp.stripe.com`. This allows secure MCP client access via OAuth. View the docs [here](https://docs.stripe.com/mcp#remote).\n\nThe Stripe Agent Toolkit also exposes tools in the [Model Context Protocol (MCP)](https://modelcontextprotocol.com/) format.  Or, to run a local Stripe MCP server using npx, use the following command:\n\n```bash\nnpx -y @stripe/mcp --tools=all --api-key=YOUR_STRIPE_SECRET_KEY\n```\n\n## Python\n\n### Installation\n\nYou don't need this source code unless you want to modify the package. If you just\nwant to use the package run:\n\n```sh\npip install stripe-agent-toolkit\n```\n\n#### Requirements\n\n- Python 3.11+\n\n### Usage\n\nThe library needs to be configured with your account's secret key which is\navailable in your [Stripe Dashboard][api-keys].\n\n```python\nfrom stripe_agent_toolkit.openai.toolkit import StripeAgentToolkit\n\nstripe_agent_toolkit = StripeAgentToolkit(\n    secret_key=\"sk_test_...\",\n    configuration={\n        \"actions\": {\n            \"payment_links\": {\n                \"create\": True,\n            },\n        }\n    },\n)\n```\n\nThe toolkit works with OpenAI's Agent SDK, LangChain, and CrewAI and can be passed as a list of tools. For example:\n\n```python\nfrom agents import Agent\n\nstripe_agent = Agent(\n    name=\"Stripe Agent\",\n    instructions=\"You are an expert at integrating with Stripe\",\n    tools=stripe_agent_toolkit.get_tools()\n)\n```\n\nExamples for OpenAI's Agent SDK,LangChain, and CrewAI are included in [/examples](/python/examples).\n\n#### Context\n\nIn some cases you will want to provide values that serve as defaults when making requests. Currently, the `account` context value enables you to make API calls for your [connected accounts](https://docs.stripe.com/connect/authentication).\n\n```python\nstripe_agent_toolkit = StripeAgentToolkit(\n    secret_key=\"sk_test_...\",\n    configuration={\n        \"context\": {\n            \"account\": \"acct_123\"\n        }\n    }\n)\n```\n\n## TypeScript\n\n### Installation\n\nYou don't need this source code unless you want to modify the package. If you just\nwant to use the package run:\n\n```\nnpm install @stripe/agent-toolkit\n```\n\n#### Requirements\n\n- Node 18+\n\n### Usage\n\nThe library needs to be configured with your account's secret key which is available in your [Stripe Dashboard][api-keys]. Additionally, `configuration` enables you to specify the types of actions that can be taken using the toolkit.\n\n```typescript\nimport { StripeAgentToolkit } from \"@stripe/agent-toolkit/langchain\";\n\nconst stripeAgentToolkit = new StripeAgentToolkit({\n  secretKey: process.env.STRIPE_SECRET_KEY!,\n  configuration: {\n    actions: {\n      paymentLinks: {\n        create: true,\n      },\n    },\n  },\n});\n```\n\n#### Tools\n\nThe toolkit works with LangChain and Vercel's AI SDK and can be passed as a list of tools. For example:\n\n```typescript\nimport { AgentExecutor, createStructuredChatAgent } from \"langchain/agents\";\n\nconst tools = stripeAgentToolkit.getTools();\n\nconst agent = await createStructuredChatAgent({\n  llm,\n  tools,\n  prompt,\n});\n\nconst agentExecutor = new AgentExecutor({\n  agent,\n  tools,\n});\n```\n\n#### Context\n\nIn some cases you will want to provide values that serve as defaults when making requests. Currently, the `account` context value enables you to make API calls for your [connected accounts](https://docs.stripe.com/connect/authentication).\n\n```typescript\nconst stripeAgentToolkit = new StripeAgentToolkit({\n  secretKey: process.env.STRIPE_SECRET_KEY!,\n  configuration: {\n    context: {\n      account: \"acct_123\",\n    },\n  },\n});\n```\n\n#### Metered billing\n\nFor Vercel's AI SDK, you can use middleware to submit billing events for usage. All that is required is the customer ID and the input/output meters to bill.\n\n```typescript\nimport { StripeAgentToolkit } from \"@stripe/agent-toolkit/ai-sdk\";\nimport { openai } from \"@ai-sdk/openai\";\nimport {\n  generateText,\n  experimental_wrapLanguageModel as wrapLanguageModel,\n} from \"ai\";\n\nconst stripeAgentToolkit = new StripeAgentToolkit({\n  secretKey: process.env.STRIPE_SECRET_KEY!,\n  configuration: {\n    actions: {\n      paymentLinks: {\n        create: true,\n      },\n    },\n  },\n});\n\nconst model = wrapLanguageModel({\n  model: openai(\"gpt-4o\"),\n  middleware: stripeAgentToolkit.middleware({\n    billing: {\n      customer: \"cus_123\",\n      meters: {\n        input: \"input_tokens\",\n        output: \"output_tokens\",\n      },\n    },\n  }),\n});\n```\n\n\n\n## Supported API methods\n\n- [Cancel a subscription](https://docs.stripe.com/api/subscriptions/cancel)\n- [Create a coupon](https://docs.stripe.com/api/coupons/create)\n- [Create a customer](https://docs.stripe.com/api/customers/create)\n- [Create a payment link](https://docs.stripe.com/api/payment-link/create)\n- [Create a price](https://docs.stripe.com/api/prices/create)\n- [Create a product](https://docs.stripe.com/api/products/create)\n- [Create a refund](https://docs.stripe.com/api/refunds/create)\n- [Create an invoice item](https://docs.stripe.com/api/invoiceitems/create)\n- [Create an invoice](https://docs.stripe.com/api/invoices/create)\n- [Finalize an invoice](https://docs.stripe.com/api/invoices/finalize)\n- [List all coupons](https://docs.stripe.com/api/coupons/list)\n- [List all customers](https://docs.stripe.com/api/customers/list)\n- [List all disputes](https://docs.stripe.com/api/disputes/list)\n- [List all prices](https://docs.stripe.com/api/prices/list)\n- [List all products](https://docs.stripe.com/api/products/list)\n- [List all subscriptions](https://docs.stripe.com/api/subscriptions/list)\n- [Retrieve balance](https://docs.stripe.com/api/balance/balance_retrieve)\n- [Update a dispute](https://docs.stripe.com/api/disputes/update)\n- [Update a subscription](https://docs.stripe.com/api/subscriptions/update)\n\n[python-sdk]: https://github.com/stripe/stripe-python\n[node-sdk]: https://github.com/stripe/stripe-node\n[api-keys]: https://dashboard.stripe.com/account/apikeys\n",
      "npm_url": "https://www.npmjs.com/package/@stripe/agent-toolkit",
      "npm_downloads": 562528,
      "keywords": [
        "stripe",
        "apis",
        "toolkit",
        "stripe apis",
        "stripe services",
        "stripe agent"
      ],
      "category": "official-servers"
    },
    "tavily-ai--tavily-mcp": {
      "owner": "tavily-ai",
      "name": "tavily-mcp",
      "url": "https://github.com/tavily-ai/tavily-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/tavily-ai.webp",
      "description": "Enables real-time web search and data extraction capabilities by providing seamless integration with various data sources and tools through the Model Context Protocol.",
      "stars": 793,
      "forks": 135,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-10-01T20:17:53Z",
      "readme_content": "# \n\n\n![GitHub Repo stars](https://img.shields.io/github/stars/tavily-ai/tavily-mcp?style=social)\n![npm](https://img.shields.io/npm/dt/tavily-mcp)\n![smithery badge](https://smithery.ai/badge/@tavily-ai/tavily-mcp)\n\n\n\nThe Tavily MCP server provides:\n- search, extract, map, crawl tools\n- Real-time web search capabilities through the tavily-search tool\n- Intelligent data extraction from web pages via the tavily-extract tool\n- Powerful web mapping tool that creates a structured map of website \n- Web crawler that systematically explores websites \n\n\n### üìö Helpful Resources\n- [Tutorial](https://medium.com/@dustin_36183/building-a-knowledge-graph-assistant-combining-tavily-and-neo4j-mcp-servers-with-claude-db92de075df9) on combining Tavily MCP with Neo4j MCP server\n- [Tutorial](https://medium.com/@dustin_36183/connect-your-coding-assistant-to-the-web-integrating-tavily-mcp-with-cline-in-vs-code-5f923a4983d1) on integrating Tavily MCP with Cline in VS Code\n\n## Remote MCP Server\n\nConnect directly to Tavily's remote MCP server instead of running it locally. This provides a seamless experience without requiring local installation or configuration.\n\nSimply use the remote MCP server URL with your Tavily API key:\n\n``` \nhttps://mcp.tavily.com/mcp/?tavilyApiKey=<your-api-key> \n```\n Get your Tavily API key from [tavily.com](https://www.tavily.com/).\n\n\n### Connect to Cursor\n[![Install MCP Server](https://cursor.com/deeplink/mcp-install-dark.svg)](https://cursor.com/en/install-mcp?name=tavily-remote-mcp&config=eyJjb21tYW5kIjoibnB4IC15IG1jcC1yZW1vdGUgaHR0cHM6Ly9tY3AudGF2aWx5LmNvbS9tY3AvP3RhdmlseUFwaUtleT08eW91ci1hcGkta2V5PiIsImVudiI6e319)\n\nClick the ‚¨ÜÔ∏è Add to Cursor ‚¨ÜÔ∏è button, this will do most of the work for you but you will still need to edit the configuration to add your API-KEY. You can get a Tavily API key [here](https://www.tavily.com/).\n\n\nonce you click the button you should be redirect to Cursor ...\n\n### Step 1\nClick the install button\n\n\n\n\n### Step 2\nYou should see the MCP is now installed, if the blue slide is not already turned on, manually turn it on. You also need to edit the configuration to include your own Tavily API key.\n\n\n### Step 3\nYou will then be redirected to your `mcp.json` file where you have to add `your-api-key`.\n\n```json\n{\n  \"mcpServers\": {\n    \"tavily-remote-mcp\": {\n      \"command\": \"npx -y mcp-remote https://mcp.tavily.com/mcp/?tavilyApiKey=<your-api-key>\",\n      \"env\": {}\n    }\n  }\n}\n```\n\n### Connect to Claude Desktop\n\nClaude desktop now supports adding `integrations` which is currently in beta. An integration in this case is the Tavily Remote MCP, below I will explain how to add the MCP as an `integration` in Claude desktop.\n\n### Step 1 \nopen claude desktop, click the button with the two sliders and then navigate to add integrations.\n\n\n### Step 2\nclick `Add integrations`\n\n\n### Step 3\nName the integration and insert the Tavily remote MCP url with your API key. You can get a Tavily API key [here](https://www.tavily.com/). Click `Add` to confirm.\n\n\n### Step 4\nRetrun to the chat screen and you will see the Tavily Remote MCP is now connected to Claude desktop.\n\n\n### OpenAI \nAllow models to use remote MCP servers to perform tasks.\n- You first need to export your OPENAI_API_KEY\n- You must also add your Tavily API-key to `<your-api-key>`, you can get a Tavily API key [here](https://www.tavily.com/)\n\n```python\nfrom openai import OpenAI\n\nclient = OpenAI()\n\nresp = client.responses.create(\n    model=\"gpt-4.1\",\n    tools=[\n        {\n            \"type\": \"mcp\",\n            \"server_label\": \"tavily\",\n            \"server_url\": \"https://mcp.tavily.com/mcp/?tavilyApiKey=<your-api-key>\",\n            \"require_approval\": \"never\",\n        },\n    ],\n    input=\"Do you have access to the tavily mcp server?\",\n)\n\nprint(resp.output_text)\n```\n\n### Clients that don't support remote MCPs\n\nmcp-remote is a lightweight bridge that lets MCP clients that can only talk to local (stdio) servers securely connect to remote MCP servers over HTTP + SSE with OAuth-based auth, so you can host and update your server in the cloud while existing clients keep working. It serves as an experimental stop-gap until popular MCP clients natively support remote, authorized servers.\n\n```json\n{\n    \"tavily-remote\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"mcp-remote\",\n        \"https://mcp.tavily.com/mcp/?tavilyApiKey=<your-api-key>\"\n      ]\n    }\n}\n```\n\n\n\n## Local MCP \n\n### Prerequisites üîß\n\nBefore you begin, ensure you have:\n\n- [Tavily API key](https://app.tavily.com/home)\n  - If you don't have a Tavily API key, you can sign up for a free account [here](https://app.tavily.com/home)\n- [Claude Desktop](https://claude.ai/download) or [Cursor](https://cursor.sh)\n- [Node.js](https://nodejs.org/) (v20 or higher)\n  - You can verify your Node.js installation by running:\n    - `node --version`\n- [Git](https://git-scm.com/downloads) installed (only needed if using Git installation method)\n  - On macOS: `brew install git`\n  - On Linux: \n    - Debian/Ubuntu: `sudo apt install git`\n    - RedHat/CentOS: `sudo yum install git`\n  - On Windows: Download [Git for Windows](https://git-scm.com/download/win)\n\n## Tavily MCP server installation ‚ö°\n\n### Running with NPX \n\n```bash\nnpx -y tavily-mcp@latest \n```\n\n### Installing via Smithery\n\nTo install Tavily MCP Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@tavily-ai/tavily-mcp):\n\n```bash\nnpx -y @smithery/cli install @tavily-ai/tavily-mcp --client claude\n```\n\nAlthough you can launch a server on its own, it's not particularly helpful in isolation. Instead, you should integrate it into an MCP client. Below is an example of how to configure the Claude Desktop app to work with the tavily-mcp server.\n\n\n## Configuring MCP Clients ‚öôÔ∏è\n\nThis repository will explain how to configure [VS Code](https://code.visualstudio.com), [Cursor](https://cursor.sh) and [Claude Desktop](https://claude.ai/desktop) to work with the tavily-mcp server.\n\n### Configuring VS Code üíª\n\nFor one-click installation, click one of the install buttons below:\n\n[![Install with NPX in VS Code](https://img.shields.io/badge/VS_Code-NPM-0098FF?style=flat-square&logo=visualstudiocode&logoColor=white)](https://insiders.vscode.dev/redirect/mcp/install?name=tavily&config=%7B%22command%22%3A%22npx%22%2C%22args%22%3A%5B%22-y%22%2C%22tavily-mcp%400.1.4%22%5D%2C%22env%22%3A%7B%22TAVILY_API_KEY%22%3A%22%24%7Binput%3Atavily_api_key%7D%22%7D%7D&inputs=%5B%7B%22type%22%3A%22promptString%22%2C%22id%22%3A%22tavily_api_key%22%2C%22description%22%3A%22Tavily+API+Key%22%2C%22password%22%3Atrue%7D%5D) [![Install with NPX in VS Code Insiders](https://img.shields.io/badge/VS_Code_Insiders-NPM-24bfa5?style=flat-square&logo=visualstudiocode&logoColor=white)](https://insiders.vscode.dev/redirect/mcp/install?name=tavily&config=%7B%22command%22%3A%22npx%22%2C%22args%22%3A%5B%22-y%22%2C%22tavily-mcp%400.1.4%22%5D%2C%22env%22%3A%7B%22TAVILY_API_KEY%22%3A%22%24%7Binput%3Atavily_api_key%7D%22%7D%7D&inputs=%5B%7B%22type%22%3A%22promptString%22%2C%22id%22%3A%22tavily_api_key%22%2C%22description%22%3A%22Tavily+API+Key%22%2C%22password%22%3Atrue%7D%5D&quality=insiders)\n\n### Manual Installation\n\nFirst check if there are install buttons at the top of this section that match your needs. If you prefer manual installation, follow these steps:\n\nAdd the following JSON block to your User Settings (JSON) file in VS Code. You can do this by pressing `Ctrl + Shift + P` (or `Cmd + Shift + P` on macOS) and typing `Preferences: Open User Settings (JSON)`.\n\n```json\n{\n  \"mcp\": {\n    \"inputs\": [\n      {\n        \"type\": \"promptString\",\n        \"id\": \"tavily_api_key\",\n        \"description\": \"Tavily API Key\",\n        \"password\": true\n      }\n    ],\n    \"servers\": {\n      \"tavily\": {\n        \"command\": \"npx\",\n        \"args\": [\"-y\", \"tavily-mcp@latest\"],\n        \"env\": {\n          \"TAVILY_API_KEY\": \"${input:tavily_api_key}\"\n        }\n      }\n    }\n  }\n}\n```\n\nOptionally, you can add it to a file called `.vscode/mcp.json` in your workspace:\n\n```json\n{\n  \"inputs\": [\n    {\n      \"type\": \"promptString\",\n      \"id\": \"tavily_api_key\",\n      \"description\": \"Tavily API Key\",\n      \"password\": true\n    }\n  ],\n  \"servers\": {\n    \"tavily\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"tavily-mcp@latest\"],\n      \"env\": {\n        \"TAVILY_API_KEY\": \"${input:tavily_api_key}\"\n      }\n    }\n  }\n}\n```\n\n### Configuring Cline ü§ñ\n\nThe easiest way to set up the Tavily MCP server in Cline is through the marketplace with a single click:\n\n1. Open Cline in VS Code\n2. Click on the Cline icon in the sidebar\n3. Navigate to the \"MCP Servers\" tab ( 4 squares )\n4. Search \"Tavily\" and click \"install\"\n5. When prompted, enter your Tavily API key\n\nAlternatively, you can manually set up the Tavily MCP server in Cline:\n\n1. Open the Cline MCP settings file:\n\n   ### For macOS:\n   ```bash\n   # Using Visual Studio Code\n   code ~/Library/Application\\ Support/Code/User/globalStorage/saoudrizwan.claude-dev/settings/cline_mcp_settings.json\n   \n   # Or using TextEdit\n   open -e ~/Library/Application\\ Support/Code/User/globalStorage/saoudrizwan.claude-dev/settings/cline_mcp_settings.json\n   ```\n\n   ### For Windows:\n   ```bash\n   code %APPDATA%\\Code\\User\\globalStorage\\saoudrizwan.claude-dev\\settings\\cline_mcp_settings.json\n   ```\n\n2. Add the Tavily server configuration to the file:\n\n   Replace `your-api-key-here` with your actual [Tavily API key](https://tavily.com/api-keys).\n\n   ```json\n   {\n     \"mcpServers\": {\n       \"tavily-mcp\": {\n         \"command\": \"npx\",\n         \"args\": [\"-y\", \"tavily-mcp@latest\"],\n         \"env\": {\n           \"TAVILY_API_KEY\": \"your-api-key-here\"\n         },\n         \"disabled\": false,\n         \"autoApprove\": []\n       }\n     }\n   }\n   ```\n\n3. Save the file and restart Cline if it's already running.\n\n4. When using Cline, you'll now have access to the Tavily MCP tools. You can ask Cline to use the tavily-search and tavily-extract tools directly in your conversations.\n\n\n### Configuring the Claude Desktop app üñ•Ô∏è\n### For macOS:\n\n```bash\n# Create the config file if it doesn't exist\ntouch \"$HOME/Library/Application Support/Claude/claude_desktop_config.json\"\n\n# Opens the config file in TextEdit \nopen -e \"$HOME/Library/Application Support/Claude/claude_desktop_config.json\"\n\n# Alternative method using Visual Studio Code (requires VS Code to be installed)\ncode \"$HOME/Library/Application Support/Claude/claude_desktop_config.json\"\n```\n\n### For Windows:\n```bash\ncode %APPDATA%\\Claude\\claude_desktop_config.json\n```\n\n### Add the Tavily server configuration:\n\nReplace `your-api-key-here` with your actual [Tavily API key](https://tavily.com/api-keys).\n\n```json\n{\n  \"mcpServers\": {\n    \"tavily-mcp\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"tavily-mcp@latest\"],\n      \"env\": {\n        \"TAVILY_API_KEY\": \"your-api-key-here\"\n      }\n    }\n  }\n}\n```\n\n### 2. Git Installation\n\n1. Clone the repository:\n```bash\ngit clone https://github.com/tavily-ai/tavily-mcp.git\ncd tavily-mcp\n```\n\n2. Install dependencies:\n```bash\nnpm install\n```\n\n3. Build the project:\n```bash\nnpm run build\n```\n### Configuring the Claude Desktop app ‚öôÔ∏è\nFollow the configuration steps outlined in the [Configuring the Claude Desktop app](#configuring-the-claude-desktop-app-Ô∏è) section above, using the below JSON configuration.\n\nReplace `your-api-key-here` with your actual [Tavily API key](https://tavily.com/api-keys) and `/path/to/tavily-mcp` with the actual path where you cloned the repository on your system.\n\n```json\n{\n  \"mcpServers\": {\n    \"tavily\": {\n      \"command\": \"npx\",\n      \"args\": [\"/path/to/tavily-mcp/build/index.js\"],\n      \"env\": {\n        \"TAVILY_API_KEY\": \"your-api-key-here\"\n      }\n    }\n  }\n}\n```\n\n## Acknowledgments ‚ú®\n\n- [Model Context Protocol](https://modelcontextprotocol.io) for the MCP specification\n- [Anthropic](https://anthropic.com) for Claude Desktop",
      "npm_url": "https://www.npmjs.com/package/tavily-mcp",
      "npm_downloads": 695749,
      "keywords": [
        "tavily",
        "search",
        "web",
        "servers tavily",
        "tavily ai",
        "ai tavily"
      ],
      "category": "official-servers"
    },
    "tinybirdco--mcp-tinybird": {
      "owner": "tinybirdco",
      "name": "mcp-tinybird",
      "url": "https://github.com/tinybirdco/mcp-tinybird",
      "imageUrl": "/freedevtools/mcp/pfp/tinybirdco.webp",
      "description": "Connect to Tinybird's real-time analytics platform to search and analyze data using the Tinybird Query API and existing API endpoints.",
      "stars": 73,
      "forks": 16,
      "license": "Apache License 2.0",
      "language": "Python",
      "updated_at": "2025-09-21T10:29:31Z",
      "readme_content": "# üö® DEPRECATED\n\n**This repository is deprecated and no longer maintained.**\n\n**New implementation:** [Tinybird MCP Server](https://www.tinybird.co/docs/forward/analytics-agents/mcp)\n\n**Please migrate to the official implementation.**\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "tinybirdco",
        "tinybird",
        "analytics",
        "using tinybird",
        "servers tinybirdco",
        "tinybird query"
      ],
      "category": "official-servers"
    },
    "translated--lara-mcp": {
      "owner": "translated",
      "name": "lara-mcp",
      "url": "https://github.com/translated/lara-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/translated.webp",
      "description": "Provides translation capabilities with automatic language detection and context-aware translations. Supports multiple language pairs and allows for the fine-tuning of translations using custom instructions.",
      "stars": 73,
      "forks": 12,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-09-30T10:34:10Z",
      "readme_content": "# Lara Translate MCP Server\n\nA Model Context Protocol (MCP) Server for [Lara Translate](https://laratranslate.com/translate) API, enabling powerful translation capabilities with support for language detection, context-aware translations and translation memories.\n\n[![License](https://img.shields.io/github/license/translated/lara-mcp.svg)](https://github.com/translated/lara-mcp/blob/main/LICENSE)\n[![Docker Pulls](https://img.shields.io/docker/pulls/translatednet/lara-mcp.svg)](https://hub.docker.com/r/translatednet/lara-mcp)\n[![npm downloads](https://img.shields.io/npm/dm/@translated/lara-mcp.svg)](https://www.npmjs.com/package/@translated/lara-mcp)\n\n## üìö Table of Contents\n- üìñ [Introduction](#-introduction)\n- üõ† [Available Tools](#-available-tools)\n- üöÄ [Getting Started](#-getting-started)\n  - üìã [HTTP Server](#http-server-)\n  - üîå [STDIO Server](#stdio-server-%EF%B8%8F)\n- üß™ [Verify Installation](#-verify-installation)\n- üíª [Popular Clients that supports MCPs](#-popular-clients-that-supports-mcps)\n- üÜò [Support](#-support)\n\n## üìñ Introduction\n\n<details>\n<summary><strong>What is MCP?</strong></summary>\n\nModel Context Protocol (MCP) is an open standardized communication protocol that enables AI applications to connect with external tools, data sources, and services. Think of MCP like a USB-C port for AI applications - just as USB-C provides a standardized way to connect devices to various peripherals, MCP provides a standardized way to connect AI models to different data sources and tools.\n\nLara Translate MCP Server enables AI applications to access Lara Translate's powerful translation capabilities through this standardized protocol.\n\n> More info about Model Context Protocol on: https://modelcontextprotocol.io/\n</details>\n\n<details>\n<summary><strong>How Lara Translate MCP Works</strong></summary>\n\nLara Translate MCP Server implements the Model Context Protocol to provide seamless translation capabilities to AI applications. The integration follows this flow:\n\n1. **Connection Establishment**: When an MCP-compatible AI application starts, it connects to configured MCP servers, including the Lara Translate MCP Server\n2. **Tool & Resource Discovery**: The AI application discovers available translation tools and resources provided by the Lara Translate MCP Server\n3. **Request Processing**: When translation needs are identified:\n   - The AI application formats a structured request with text to translate, language pairs, and optional context\n   - The MCP server validates the request and transforms it into Lara Translate API calls\n   - The request is securely sent to Lara Translate's API using your credentials\n4. **Translation & Response**: Lara Translate processes the translation using advanced AI models\n5. **Result Integration**: The translation results are returned to the AI application, which can then incorporate them into its response\n\nThis integration architecture allows AI applications to access professional-grade translations without implementing the API directly, while maintaining the security of your API credentials and offering flexibility to adjust translation parameters through natural language instructions.\n</details>\n\n<details>\n<summary><strong>Why to use Lara inside an LLM</strong></summary>\n\nIntegrating Lara with LLMs creates a powerful synergy that significantly enhances translation quality for non-English languages.\n\n#### Why General LLMs Fall Short in Translation\nWhile large language models possess broad linguistic capabilities, they often lack the specialized expertise and up-to-date terminology required for accurate translations in specific domains and languages.\n\n#### Lara‚Äôs Domain-Specific Advantage\nLara overcomes this limitation by leveraging Translation Language Models (T-LMs) trained on billions of professionally translated segments. These models provide domain-specific machine translation that captures cultural nuances and industry terminology that generic LLMs may miss. The result: translations that are contextually accurate and sound natural to native speakers.\n\n#### Designed for Non-English Strength\nLara has a strong focus on non-English languages, addressing the performance gap found in models such as GPT-4. The dominance of English in datasets such as Common Crawl and Wikipedia results in lower quality output in other languages. Lara helps close this gap by providing higher quality understanding, generation, and restructuring in a multilingual context.\n\n#### Faster, Smarter Multilingual Performance\nBy offloading complex translation tasks to specialized T-LMs, Lara reduces computational overhead and minimizes latency‚Äîa common issue for LLMs handling non-English input. Its architecture processes translations in parallel with the LLM, enabling for real-time, high-quality output without compromising speed or efficiency.\n\n#### Cost-Efficient Translation at Scale\nLara also lowers the cost of using models like GPT-4 in non-English workflows. Since tokenization (and pricing) is optimized for English, using Lara allows translation to take place before hitting the LLM, meaning that only the translated English content is processed. This improves cost efficiency and supports competitive scalability for global enterprises.\n</details>\n\n## üõ† Available Tools\n\n### Translation Tools\n\n<details>\n<summary><strong>translate</strong> - Translate text between languages</summary>\n\n**Inputs**:\n- `text` (array): An array of text blocks to translate, each with:\n    - `text` (string): The text content\n    - `translatable` (boolean): Whether this block should be translated\n- `source` (optional string): Source language code (e.g., 'en-EN')\n- `target` (string): Target language code (e.g., 'it-IT')\n- `context` (optional string): Additional context to improve translation quality\n- `instructions` (optional string[]): Instructions to adjust translation behavior\n- `source_hint` (optional string): Guidance for language detection\n\n**Returns**: Translated text blocks maintaining the original structure\n</details>\n\n### Translation Memories Tools\n\n<details>\n\n<summary><strong>list_memories</strong> - List saved translation memories</summary>\n\n**Returns**: Array of memories and their details\n</details>\n\n<details>\n<summary><strong>create_memory</strong> - Create a new translation memory</summary>\n\n**Inputs**:\n- `name` (string): Name of the new memory\n- `external_id` (optional string): ID of the memory to import from MyMemory (e.g., 'ext_my_[MyMemory ID]')\n\n**Returns**: Created memory data\n</details>\n\n<details>\n<summary><strong>update_memory</strong> - Update translation memory name</summary>\n\n**Inputs**:\n- `id` (string): ID of the memory to update\n- `name` (string): The new name for the memory\n\n**Returns**: Updated memory data\n</details>\n\n<details>\n<summary><strong>delete_memory</strong> - Delete a translation memory</summary>\n\n**Inputs**:\n- `id` (string): ID of the memory to delete\n\n**Returns**: Deleted memory data\n</details>\n\n<details>\n<summary><strong>add_translation</strong> - Add a translation unit to memory</summary>\n\n**Inputs**:\n- `id` (string | string[]): ID or IDs of memories where to add the translation unit\n- `source` (string): Source language code\n- `target` (string): Target language code\n- `sentence` (string): The source sentence\n- `translation` (string): The translated sentence\n- `tuid` (optional string): Translation Unit unique identifier\n- `sentence_before` (optional string): Context sentence before\n- `sentence_after` (optional string): Context sentence after\n\n**Returns**: Added translation details\n</details>\n\n<details>\n<summary><strong>delete_translation</strong> - Delete a translation unit from memory</summary>\n\n**Inputs**:\n- `id` (string): ID of the memory\n- `source` (string): Source language code\n- `target` (string): Target language code\n- `sentence` (string): The source sentence\n- `translation` (string): The translated sentence\n- `tuid` (optional string): Translation Unit unique identifier\n- `sentence_before` (optional string): Context sentence before\n- `sentence_after` (optional string): Context sentence after\n\n**Returns**: Removed translation details\n</details>\n\n<details>\n<summary><strong>import_tmx</strong> - Import a TMX file into a memory</summary>\n\n**Inputs**:\n- `id` (string): ID of the memory to update\n- `tmx_content` (string): The content of the tmx file to upload\n- `gzip` (boolean): Indicates if the file is compressed (.gz)\n\n**Returns**: Import details\n</details>\n\n<details>\n<summary><strong>check_import_status</strong> - Checks the status of a TMX file import</summary>\n\n**Inputs**:\n- `id` (string): The ID of the import job\n\n**Returns**: Import details\n</details>\n\n## üöÄ Getting Started\nLara supports both the STDIO and streamable HTTP protocols. For a hassle-free setup, we recommend using the HTTP protocol. If you prefer to use STDIO, it must be installed locally on your machine.\n\nYou'll find setup instructions for both protocols in the sections below.\n\n### HTTP Server üåê\n<details>\n<summary><strong>‚ùå Clients NOT supporting <code>url</code> configuration (e.g., Claude, OpenAI)</strong></summary>\n\nThis installation guide is intended for clients that do NOT support the url-based configuration. This option requires Node.js to be installed on your system.\n\n> If you're unsure how to configure an MCP with your client, please refer to your MCP client's official documentation.\n\n---\n\n1. Open your client's MCP configuration JSON file with a text editor, then copy and paste the following snippet:\n\n```json\n{\n  \"mcpServers\": {\n    \"lara\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"mcp-remote\",\n        \"https://mcp.laratranslate.com/v1\",\n        \"--header\",\n        \"x-lara-access-key-id: ${X_LARA_ACCESS_KEY_ID}\",\n        \"--header\",\n        \"x-lara-access-key-secret: ${X_LARA_ACCESS_KEY_SECRET}\"\n      ],\n      \"env\": {\n        \"X_LARA_ACCESS_KEY_ID\": \"<YOUR_ACCESS_KEY_ID>\",\n        \"X_LARA_ACCESS_KEY_SECRET\": \"<YOUR_ACCESS_KEY_SECRET>\"\n      }\n    }\n  }\n}\n```\n\n2. Replace `<YOUR_ACCESS_KEY_ID>` and `<YOUR_ACCESS_KEY_SECRET>` with your Lara Translate API credentials. Refer to the [Official Documentation](https://developers.laratranslate.com/docs/getting-started#step-3---configure-your-credentials) for details.\n\n3. Restart your MCP client.\n\n</details>\n\n<details>\n<summary><strong>‚úÖ Clients supporting <code>url</code> configuration (e.g., Cursor, Continue)</strong></summary>\n\nThis installation guide is intended for clients that support the url-based configuration. These clients can connect to Lara through a remote HTTP endpoint by specifying a simple configuration object.\n\nSome examples of supported clients include Cursor, Continue, OpenDevin, and Aider.\n> If you're unsure how to configure an MCP with your client, please refer to your MCP client's official documentation.\n\n---\n\n1. Open your client's MCP configuration JSON file with a text editor, then copy and paste the following snippet:\n\n```json\n{\n  \"mcpServers\": {\n    \"lara\": {\n      \"url\": \"https://mcp.laratranslate.com/v1\",\n      \"headers\": {\n        \"x-lara-access-key-id\": \"<YOUR_ACCESS_KEY_ID>\",\n        \"x-lara-access-key-secret\": \"<YOUR_ACCESS_KEY_SECRET>\"\n      }\n    }\n  }\n}\n```\n\n2. Replace `<YOUR_ACCESS_KEY_ID>` and `<YOUR_ACCESS_KEY_SECRET>` with your Lara Translate API credentials. Refer to the [Official Documentation](https://developers.laratranslate.com/docs/getting-started#step-3---configure-your-credentials) for details.\n\n3. Restart your MCP client.\n\n</details>\n\n---\n\n### STDIO Server üñ•Ô∏è\n<details>\n<summary><strong>Using NPX</strong></summary>\n\nThis option requires Node.js to be installed on your system.\n\n1. Add the following to your MCP configuration file:\n```json\n{\n  \"mcpServers\": {\n    \"lara-translate\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@translated/lara-mcp@latest\"],\n      \"env\": {\n        \"LARA_ACCESS_KEY_ID\": \"<YOUR_ACCESS_KEY_ID>\",\n        \"LARA_ACCESS_KEY_SECRET\": \"<YOUR_ACCESS_KEY_SECRET>\"\n      }\n    }\n  }\n}\n```\n\n2. Replace `<YOUR_ACCESS_KEY_ID>` and `<YOUR_ACCESS_KEY_SECRET>` with your actual Lara API credentials.\n</details>\n\n<details>\n<summary><strong>Using Docker</strong></summary>\n\nThis option requires Docker to be installed on your system.\n\n1. Add the following to your MCP configuration file:\n```json\n{\n  \"mcpServers\": {\n    \"lara-translate\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"-i\",\n        \"--rm\",\n        \"-e\",\n        \"LARA_ACCESS_KEY_ID\",\n        \"-e\",\n        \"LARA_ACCESS_KEY_SECRET\",\n        \"translatednet/lara-mcp:latest\"\n      ],\n      \"env\": {\n        \"LARA_ACCESS_KEY_ID\": \"<YOUR_ACCESS_KEY_ID>\",\n        \"LARA_ACCESS_KEY_SECRET\": \"<YOUR_ACCESS_KEY_SECRET>\"\n      }\n    }\n  }\n}\n```\n\n2. Replace `<YOUR_ACCESS_KEY_ID>` and `<YOUR_ACCESS_KEY_SECRET>` with your actual Lara API credentials.\n</details>\n\n<details>\n<summary><strong>Building from Source</strong></summary>\n\n#### Using Node.js\n\n1. Clone the repository:\n```bash\ngit clone https://github.com/translated/lara-mcp.git\ncd lara-mcp\n```\n\n2. Install dependencies and build:\n```bash\n# Install dependencies\npnpm install\n\n# Build\npnpm run build\n```\n\n3. Add the following to your MCP configuration file:\n```json\n{\n  \"mcpServers\": {\n    \"lara-translate\": {\n      \"command\": \"node\",\n      \"args\": [\"<FULL_PATH_TO_PROJECT_FOLDER>/dist/index.js\"],\n      \"env\": {\n        \"LARA_ACCESS_KEY_ID\": \"<YOUR_ACCESS_KEY_ID>\",\n        \"LARA_ACCESS_KEY_SECRET\": \"<YOUR_ACCESS_KEY_SECRET>\"\n      }\n    }\n  }\n}\n```\n4. Replace:\n   - `<FULL_PATH_TO_PROJECT_FOLDER>` with the absolute path to your project folder\n   - `<YOUR_ACCESS_KEY_ID>` and `<YOUR_ACCESS_KEY_SECRET>` with your actual Lara API credentials.\n\n#### Building a Docker Image\n\n1. Clone the repository:\n```bash\ngit clone https://github.com/translated/lara-mcp.git\ncd lara-mcp\n```\n\n2. Build the Docker image:\n```bash\ndocker build -t lara-mcp .\n```\n\n3. Add the following to your MCP configuration file:\n```json\n{\n  \"mcpServers\": {\n    \"lara-translate\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"-i\",\n        \"--rm\",\n        \"-e\",\n        \"LARA_ACCESS_KEY_ID\",\n        \"-e\",\n        \"LARA_ACCESS_KEY_SECRET\",\n        \"lara-mcp\"\n      ],\n      \"env\": {\n        \"LARA_ACCESS_KEY_ID\": \"<YOUR_ACCESS_KEY_ID>\",\n        \"LARA_ACCESS_KEY_SECRET\": \"<YOUR_ACCESS_KEY_SECRET>\"\n      }\n    }\n  }\n}\n```\n\n4. Replace `<YOUR_ACCESS_KEY_ID>` and `<YOUR_ACCESS_KEY_SECRET>` with your actual credentials.\n</details>\n\n## üß™ Verify Installation\n\nAfter restarting your MCP client, you should see Lara Translate MCP in the list of available MCPs.\n> The method for viewing installed MCPs varies by client. Please consult your MCP client's documentation.\n\nTo verify that Lara Translate MCP is working correctly, try translating with a simple prompt:\n```text\nTranslate with Lara \"Hello world\" to Spanish\n```\n\nYour MCP client will begin generating a response. If Lara Translate MCP is properly installed and configured, your client will either request approval for the action or display a notification that Lara Translate is being used.\n\n## üíª Popular Clients that supports MCPs \n\n> For a complete list of MCP clients and their feature support, visit the [official MCP clients page](https://modelcontextprotocol.io/clients).\n\n| Client                                                            | Description                                          |\n|-------------------------------------------------------------------|------------------------------------------------------|\n| [Claude Desktop](https://claude.ai/download)                      | Desktop application for Claude AI                    |\n| [Aixplain](https://aixplain.com/)                                 | Production-ready AI Agents                           |\n| [Cursor](https://www.cursor.com/)                                 | AI-first code editor                                 |\n| [Cline for VS Code](https://github.com/cline/cline)               | VS Code extension for AI assistance                  |\n| [GitHub Copilot MCP](https://github.com/VikashLoomba/copilot-mcp) | VS Code extension for GitHub Copilot MCP integration |\n| [Windsurf](https://windsurf.com/editor)                           | AI-powered code editor and development environment   |\n\n## üÜò Support\n\n- For issues with Lara Translate API: Visit [Lara Translate API and Integrations Support](https://support.laratranslate.com)\n- For issues with this MCP Server: Open an issue on [GitHub](https://github.com/translated/lara-mcp/issues)\n",
      "npm_url": "https://www.npmjs.com/package/lara-mcp",
      "npm_downloads": 0,
      "keywords": [
        "translations",
        "translated",
        "translation",
        "translated lara",
        "translations supports",
        "servers translated"
      ],
      "category": "official-servers"
    },
    "vectorize-io--vectorize-mcp-server": {
      "owner": "vectorize-io",
      "name": "vectorize-mcp-server",
      "url": "https://github.com/vectorize-io/vectorize-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/vectorize-io.webp",
      "description": "Integrates advanced vector retrieval and text extraction capabilities, enabling document searches and Markdown text extraction from various file types. Supports deep research generation from custom data pipelines.",
      "stars": 93,
      "forks": 18,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-09-20T08:33:07Z",
      "readme_content": "# Vectorize MCP Server\n\nA Model Context Protocol (MCP) server implementation that integrates with [Vectorize](https://vectorize.io/) for advanced Vector retrieval and text extraction.\n\n<a href=\"https://glama.ai/mcp/servers/pxwbgk0kzr\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/pxwbgk0kzr/badge\" alt=\"Vectorize MCP server\" />\n</a>\n\n\n## Installation\n\n### Running with npx\n\n```bash\nexport VECTORIZE_ORG_ID=YOUR_ORG_ID\nexport VECTORIZE_TOKEN=YOUR_TOKEN\nexport VECTORIZE_PIPELINE_ID=YOUR_PIPELINE_ID\n\nnpx -y @vectorize-io/vectorize-mcp-server@latest\n```\n\n### VS Code Installation\n\nFor one-click installation, click one of the install buttons below:\n\n[![Install with NPX in VS Code](https://img.shields.io/badge/VS_Code-NPM-0098FF?style=flat-square&logo=visualstudiocode&logoColor=white)](https://insiders.vscode.dev/redirect/mcp/install?name=vectorize&config=%7B%22command%22%3A%22npx%22%2C%22args%22%3A%5B%22-y%22%2C%22%40vectorize-io%2Fvectorize-mcp-server%40latest%22%5D%2C%22env%22%3A%7B%22VECTORIZE_ORG_ID%22%3A%22%24%7Binput%3Aorg_id%7D%22%2C%22VECTORIZE_TOKEN%22%3A%22%24%7Binput%3Atoken%7D%22%2C%22VECTORIZE_PIPELINE_ID%22%3A%22%24%7Binput%3Apipeline_id%7D%22%7D%7D&inputs=%5B%7B%22type%22%3A%22promptString%22%2C%22id%22%3A%22org_id%22%2C%22description%22%3A%22Vectorize+Organization+ID%22%7D%2C%7B%22type%22%3A%22promptString%22%2C%22id%22%3A%22token%22%2C%22description%22%3A%22Vectorize+Token%22%2C%22password%22%3Atrue%7D%2C%7B%22type%22%3A%22promptString%22%2C%22id%22%3A%22pipeline_id%22%2C%22description%22%3A%22Vectorize+Pipeline+ID%22%7D%5D) [![Install with NPX in VS Code Insiders](https://img.shields.io/badge/VS_Code_Insiders-NPM-24bfa5?style=flat-square&logo=visualstudiocode&logoColor=white)](https://insiders.vscode.dev/redirect/mcp/install?name=vectorize&config=%7B%22command%22%3A%22npx%22%2C%22args%22%3A%5B%22-y%22%2C%22%40vectorize-io%2Fvectorize-mcp-server%40latest%22%5D%2C%22env%22%3A%7B%22VECTORIZE_ORG_ID%22%3A%22%24%7Binput%3Aorg_id%7D%22%2C%22VECTORIZE_TOKEN%22%3A%22%24%7Binput%3Atoken%7D%22%2C%22VECTORIZE_PIPELINE_ID%22%3A%22%24%7Binput%3Apipeline_id%7D%22%7D%7D&inputs=%5B%7B%22type%22%3A%22promptString%22%2C%22id%22%3A%22org_id%22%2C%22description%22%3A%22Vectorize+Organization+ID%22%7D%2C%7B%22type%22%3A%22promptString%22%2C%22id%22%3A%22token%22%2C%22description%22%3A%22Vectorize+Token%22%2C%22password%22%3Atrue%7D%2C%7B%22type%22%3A%22promptString%22%2C%22id%22%3A%22pipeline_id%22%2C%22description%22%3A%22Vectorize+Pipeline+ID%22%7D%5D&quality=insiders)\n\n### Manual Installation\n\nFor the quickest installation, use the one-click install buttons at the top of this section.\n\nTo install manually, add the following JSON block to your User Settings (JSON) file in VS Code. You can do this by pressing `Ctrl + Shift + P` and typing `Preferences: Open User Settings (JSON)`.\n\n```json\n{\n  \"mcp\": {\n    \"inputs\": [\n      {\n        \"type\": \"promptString\",\n        \"id\": \"org_id\",\n        \"description\": \"Vectorize Organization ID\"\n      },\n      {\n        \"type\": \"promptString\",\n        \"id\": \"token\",\n        \"description\": \"Vectorize Token\",\n        \"password\": true\n      },\n      {\n        \"type\": \"promptString\",\n        \"id\": \"pipeline_id\",\n        \"description\": \"Vectorize Pipeline ID\"\n      }\n    ],\n    \"servers\": {\n      \"vectorize\": {\n        \"command\": \"npx\",\n        \"args\": [\"-y\", \"@vectorize-io/vectorize-mcp-server@latest\"],\n        \"env\": {\n          \"VECTORIZE_ORG_ID\": \"${input:org_id}\",\n          \"VECTORIZE_TOKEN\": \"${input:token}\",\n          \"VECTORIZE_PIPELINE_ID\": \"${input:pipeline_id}\"\n        }\n      }\n    }\n  }\n}\n```\n\nOptionally, you can add the following to a file called `.vscode/mcp.json` in your workspace to share the configuration with others:\n\n```json\n{\n  \"inputs\": [\n    {\n      \"type\": \"promptString\",\n      \"id\": \"org_id\",\n      \"description\": \"Vectorize Organization ID\"\n    },\n    {\n      \"type\": \"promptString\",\n      \"id\": \"token\",\n      \"description\": \"Vectorize Token\",\n      \"password\": true\n    },\n    {\n      \"type\": \"promptString\",\n      \"id\": \"pipeline_id\",\n      \"description\": \"Vectorize Pipeline ID\"\n    }\n  ],\n  \"servers\": {\n    \"vectorize\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@vectorize-io/vectorize-mcp-server@latest\"],\n      \"env\": {\n        \"VECTORIZE_ORG_ID\": \"${input:org_id}\",\n        \"VECTORIZE_TOKEN\": \"${input:token}\",\n        \"VECTORIZE_PIPELINE_ID\": \"${input:pipeline_id}\"\n      }\n    }\n  }\n}\n```\n\n## Configuration on Claude/Windsurf/Cursor/Cline\n\n```json\n{\n  \"mcpServers\": {\n    \"vectorize\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@vectorize-io/vectorize-mcp-server@latest\"],\n      \"env\": {\n        \"VECTORIZE_ORG_ID\": \"your-org-id\",\n        \"VECTORIZE_TOKEN\": \"your-token\",\n        \"VECTORIZE_PIPELINE_ID\": \"your-pipeline-id\"\n      }\n    }\n  }\n}\n```\n\n## Tools\n\n### Retrieve documents\n\nPerform vector search and retrieve documents (see official [API](https://docs.vectorize.io/api/api-pipelines/api-retrieval)):\n\n```json\n{\n  \"name\": \"retrieve\",\n  \"arguments\": {\n    \"question\": \"Financial health of the company\",\n    \"k\": 5\n  }\n}\n```\n\n### Text extraction and chunking (Any file to Markdown)\n\nExtract text from a document and chunk it into Markdown format (see official [API](https://docs.vectorize.io/api/api-extraction)):\n\n```json\n{\n  \"name\": \"extract\",\n  \"arguments\": {\n    \"base64document\": \"base64-encoded-document\",\n    \"contentType\": \"application/pdf\"\n  }\n}\n```\n\n### Deep Research\n\nGenerate a Private Deep Research from your pipeline (see official [API](https://docs.vectorize.io/api/api-pipelines/api-deep-research)):\n\n```json\n{\n  \"name\": \"deep-research\",\n  \"arguments\": {\n    \"query\": \"Generate a financial status report about the company\",\n    \"webSearch\": true\n  }\n}\n```\n\n## Development\n\n```bash\nnpm install\nnpm run dev\n```\n\n### Release\nChange the package.json version and then:\n```bash\ngit commit -am \"x.y.z\"\ngit tag x.y.z\ngit push origin\ngit push origin --tags\n```\n\n### Contributing\n\n1. Fork the repository\n2. Create your feature branch\n3. Submit a pull request\n\n",
      "npm_url": "https://www.npmjs.com/package/@vectorize-io/vectorize-mcp-server",
      "npm_downloads": 1857,
      "keywords": [
        "markdown",
        "searches",
        "vectorize",
        "servers vectorize",
        "document searches",
        "text extraction"
      ],
      "category": "official-servers"
    },
    "yamadashy--repomix": {
      "owner": "yamadashy",
      "name": "repomix",
      "url": "https://github.com/yamadashy/repomix",
      "imageUrl": "/freedevtools/mcp/pfp/yamadashy.webp",
      "description": "Transforms codebases into AI-friendly formats to facilitate analysis by AI tools, improving code reviews and enabling refactoring suggestions. Consolidates repository information through a single command for better AI understanding.",
      "stars": 19472,
      "forks": 873,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-10-02T08:28:01Z",
      "readme_content": "<div align=\"center\" markdown=\"1\">\n   <sup>Special thanks to:</sup>\n   <br>\n   <br>\n   <a href=\"https://go.warp.dev/repomix\">\n      <img alt=\"Warp sponsorship\" width=\"400\" src=\"https://raw.githubusercontent.com/warpdotdev/brand-assets/main/Github/Sponsor/Warp-Github-LG-01.png\">\n   </a>\n\n### [Warp, built for coding with multiple AI agents](https://go.warp.dev/repomix)\n[Available for MacOS, Linux, & Windows](https://go.warp.dev/repomix)<br>\n\n   <br>\n\n   <a href=\"https://tuple.app/repomix\">\n      \n   </a>\n\n### [Tuple, the premier screen sharing app for developers on macOS and Windows.](https://tuple.app/repomix)\n\n</div>\n\n\n<hr />\n\n<div align=\"center\">\n  <a href=\"https://repomix.com\">\n    \n  </a>\n  <p align=\"center\">\n    <b>Pack your codebase into AI-friendly formats</b>\n  </p>\n</div>\n\n<p align=\"center\">\n  <a href=\"https://repomix.com\"><b>Use Repomix online! üëâ repomix.com</b></a><br>\n</p>\n\n<p align=\"center\">\n  Need discussion? Join us on <a href=\"https://discord.gg/wNYzTwZFku\">Discord</a>!<br>\n  <i>Share your experience and tips</i><br>\n  <i>Stay updated on new features</i><br>\n  <i>Get help with configuration and usage</i><br>\n</p>\n\n<hr />\n\n[![npm](https://img.shields.io/npm/v/repomix.svg?maxAge=1000)](https://www.npmjs.com/package/repomix)\n[![npm](https://img.shields.io/npm/d18m/repomix)](https://www.npmjs.com/package/repomix)\n[![Actions Status](https://github.com/yamadashy/repomix/actions/workflows/ci.yml/badge.svg)](https://github.com/yamadashy/repomix/actions?query=workflow%3A\"ci\")\n[![codecov](https://codecov.io/github/yamadashy/repomix/graph/badge.svg)](https://codecov.io/github/yamadashy/repomix)\n[![Sponsors](https://img.shields.io/github/sponsors/yamadashy?logo=github)](https://github.com/sponsors/yamadashy)\n[![Discord](https://badgen.net/discord/online-members/wNYzTwZFku?icon=discord&label=discord)](https://discord.gg/wNYzTwZFku)\n\n[![DeepWiki](https://img.shields.io/badge/DeepWiki-yamadashy%2Frepomix-blue.svg?logo=data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACwAAAAyCAYAAAAnWDnqAAAAAXNSR0IArs4c6QAAA05JREFUaEPtmUtyEzEQhtWTQyQLHNak2AB7ZnyXZMEjXMGeK/AIi+QuHrMnbChYY7MIh8g01fJoopFb0uhhEqqcbWTp06/uv1saEDv4O3n3dV60RfP947Mm9/SQc0ICFQgzfc4CYZoTPAswgSJCCUJUnAAoRHOAUOcATwbmVLWdGoH//PB8mnKqScAhsD0kYP3j/Yt5LPQe2KvcXmGvRHcDnpxfL2zOYJ1mFwrryWTz0advv1Ut4CJgf5uhDuDj5eUcAUoahrdY/56ebRWeraTjMt/00Sh3UDtjgHtQNHwcRGOC98BJEAEymycmYcWwOprTgcB6VZ5JK5TAJ+fXGLBm3FDAmn6oPPjR4rKCAoJCal2eAiQp2x0vxTPB3ALO2CRkwmDy5WohzBDwSEFKRwPbknEggCPB/imwrycgxX2NzoMCHhPkDwqYMr9tRcP5qNrMZHkVnOjRMWwLCcr8ohBVb1OMjxLwGCvjTikrsBOiA6fNyCrm8V1rP93iVPpwaE+gO0SsWmPiXB+jikdf6SizrT5qKasx5j8ABbHpFTx+vFXp9EnYQmLx02h1QTTrl6eDqxLnGjporxl3NL3agEvXdT0WmEost648sQOYAeJS9Q7bfUVoMGnjo4AZdUMQku50McDcMWcBPvr0SzbTAFDfvJqwLzgxwATnCgnp4wDl6Aa+Ax283gghmj+vj7feE2KBBRMW3FzOpLOADl0Isb5587h/U4gGvkt5v60Z1VLG8BhYjbzRwyQZemwAd6cCR5/XFWLYZRIMpX39AR0tjaGGiGzLVyhse5C9RKC6ai42ppWPKiBagOvaYk8lO7DajerabOZP46Lby5wKjw1HCRx7p9sVMOWGzb/vA1hwiWc6jm3MvQDTogQkiqIhJV0nBQBTU+3okKCFDy9WwferkHjtxib7t3xIUQtHxnIwtx4mpg26/HfwVNVDb4oI9RHmx5WGelRVlrtiw43zboCLaxv46AZeB3IlTkwouebTr1y2NjSpHz68WNFjHvupy3q8TFn3Hos2IAk4Ju5dCo8B3wP7VPr/FGaKiG+T+v+TQqIrOqMTL1VdWV1DdmcbO8KXBz6esmYWYKPwDL5b5FA1a0hwapHiom0r/cKaoqr+27/XcrS5UwSMbQAAAABJRU5ErkJggg==)](https://deepwiki.com/yamadashy/repomix)\n<!-- DeepWiki badge generated by https://deepwiki.ryoppippi.com/ -->\n\nüì¶ Repomix is a powerful tool that packs your entire repository into a single, AI-friendly file.  \nIt is perfect for when you need to feed your codebase to Large Language Models (LLMs) or other AI tools like Claude,\nChatGPT, DeepSeek, Perplexity, Gemini, Gemma, Llama, Grok, and more.\n\nPlease consider sponsoring me.\n\n[](https://github.com/sponsors/yamadashy)\n\n[![Sponsors](https://cdn.jsdelivr.net/gh/yamadashy/sponsor-list/sponsors/sponsors.png)](https://github.com/sponsors/yamadashy)\n\n## üèÜ Open Source Awards Nomination\n\nWe're honored! Repomix has been nominated for the **Powered by AI** category at the [JSNation Open Source Awards 2025](https://osawards.com/javascript/).\n\nThis wouldn't have been possible without all of you using and supporting Repomix. Thank you!\n\n## üéâ New: Repomix Website & Discord Community!\n\n- Try Repomix in your browser at [repomix.com](https://repomix.com/)\n- Join our [Discord Server](https://discord.gg/wNYzTwZFku) for support and discussion\n\n**We look forward to seeing you there!**\n\n## üåü Features\n\n- **AI-Optimized**: Formats your codebase in a way that's easy for AI to understand and process.\n- **Token Counting**: Provides token counts for each file and the entire repository, useful for LLM context limits.\n- **Simple to Use**: You need just one command to pack your entire repository.\n- **Customizable**: Easily configure what to include or exclude.\n- **Git-Aware**: Automatically respects your `.gitignore` files and `.git/info/exclude`.\n- **Security-Focused**: Incorporates [Secretlint](https://github.com/secretlint/secretlint) for robust security checks to detect and prevent inclusion of sensitive information.\n- **Code Compression**: The `--compress` option uses [Tree-sitter](https://github.com/tree-sitter/tree-sitter) to extract key code elements, reducing token count while preserving structure.\n\n## üöÄ Quick Start\n\n### Using the CLI Tool `>_`\n\nYou can try Repomix instantly in your project directory without installation:\n\n```bash\nnpx repomix@latest\n```\n\nOr install globally for repeated use:\n\n```bash\n# Install using npm\nnpm install -g repomix\n\n# Alternatively using yarn\nyarn global add repomix\n\n# Alternatively using bun\nbun add -g repomix\n\n# Alternatively using Homebrew (macOS/Linux)\nbrew install repomix\n\n# Then run in any project directory\nrepomix\n```\n\nThat's it! Repomix will generate a `repomix-output.xml` file in your current directory, containing your entire\nrepository in an AI-friendly format.\n\nYou can then send this file to an AI assistant with a prompt like:\n\n```\nThis file contains all the files in the repository combined into one.\nI want to refactor the code, so please review it first.\n```\n\n\n\nWhen you propose specific changes, the AI might be able to generate code accordingly. With features like Claude's\nArtifacts, you could potentially output multiple files, allowing for the generation of multiple interdependent pieces of\ncode.\n\n\n\nHappy coding! üöÄ\n\n### Using The Website üåê\n\nWant to try it quickly? Visit the official website at [repomix.com](https://repomix.com). Simply enter your repository\nname, fill in any optional details, and click the **Pack** button to see your generated output.\n\n#### Available Options\n\nThe website offers several convenient features:\n\n- Customizable output format (XML, Markdown, or Plain Text)\n- Instant token count estimation\n- Much more!\n\n### Using The Browser Extension üß©\n\nGet instant access to Repomix directly from any GitHub repository! Our Chrome extension adds a convenient \"Repomix\" button to GitHub repository pages.\n\n\n\n#### Install\n- Chrome Extension: [Repomix - Chrome Web Store](https://chromewebstore.google.com/detail/repomix/fimfamikepjgchehkohedilpdigcpkoa)\n- Firefox Add-on: [Repomix - Firefox Add-ons](https://addons.mozilla.org/firefox/addon/repomix/)\n\n#### Features\n- One-click access to Repomix for any GitHub repository\n- More exciting features coming soon!\n\n### Using The VSCode Extension ‚ö°Ô∏è\n\nA community-maintained VSCode extension called [Repomix Runner](https://marketplace.visualstudio.com/items?itemName=DorianMassoulier.repomix-runner) (created by [massdo](https://github.com/massdo)) lets you run Repomix right inside your editor with just a few clicks. Run it on any folder, manage outputs seamlessly, and control everything through VSCode's intuitive interface. \n\nWant your output as a file or just the content? Need automatic cleanup? This extension has you covered. Plus, it works smoothly with your existing repomix.config.json.\n\nTry it now on the [VSCode Marketplace](https://marketplace.visualstudio.com/items?itemName=DorianMassoulier.repomix-runner)!\nSource code is available on [GitHub](https://github.com/massdo/repomix-runner).\n\n### Alternative Tools üõ†Ô∏è\n\nIf you're using Python, you might want to check out `Gitingest`, which is better suited for Python ecosystem and data\nscience workflows:\nhttps://github.com/cyclotruc/gitingest\n\n## üìä Usage\n\nTo pack your entire repository:\n\n```bash\nrepomix\n```\n\nTo pack a specific directory:\n\n```bash\nrepomix path/to/directory\n```\n\nTo pack specific files or directories\nusing [glob patterns](https://github.com/mrmlnc/fast-glob?tab=readme-ov-file#pattern-syntax):\n\n```bash\nrepomix --include \"src/**/*.ts,**/*.md\"\n```\n\nTo exclude specific files or directories:\n\n```bash\nrepomix --ignore \"**/*.log,tmp/\"\n```\n\nTo pack a remote repository:\n\n```bash\nrepomix --remote https://github.com/yamadashy/repomix\n\n# You can also use GitHub shorthand:\nrepomix --remote yamadashy/repomix\n\n# You can specify the branch name, tag, or commit hash:\nrepomix --remote https://github.com/yamadashy/repomix --remote-branch main\n\n# Or use a specific commit hash:\nrepomix --remote https://github.com/yamadashy/repomix --remote-branch 935b695\n\n# Another convenient way is specifying the branch's URL\nrepomix --remote https://github.com/yamadashy/repomix/tree/main\n\n# Commit's URL is also supported\nrepomix --remote https://github.com/yamadashy/repomix/commit/836abcd7335137228ad77feb28655d85712680f1\n\n```\n\nTo pack files from a file list (pipe via stdin):\n\n```bash\n# Using find command\nfind src -name \"*.ts\" -type f | repomix --stdin\n\n# Using git to get tracked files\ngit ls-files \"*.ts\" | repomix --stdin\n\n# Using grep to find files containing specific content\ngrep -l \"TODO\" **/*.ts | repomix --stdin\n\n# Using ripgrep to find files with specific content\nrg -l \"TODO|FIXME\" --type ts | repomix --stdin\n\n# Using ripgrep (rg) to find files\nrg --files --type ts | repomix --stdin\n\n# Using sharkdp/fd to find files\nfd -e ts | repomix --stdin\n\n# Using fzf to select from all files\nfzf -m | repomix --stdin\n\n# Interactive file selection with fzf\nfind . -name \"*.ts\" -type f | fzf -m | repomix --stdin\n\n# Using ls with glob patterns\nls src/**/*.ts | repomix --stdin\n\n# From a file containing file paths\ncat file-list.txt | repomix --stdin\n\n# Direct input with echo\necho -e \"src/index.ts\\nsrc/utils.ts\" | repomix --stdin\n```\n\nThe `--stdin` option allows you to pipe a list of file paths to Repomix, giving you ultimate flexibility in selecting which files to pack.\n\nWhen using `--stdin`, the specified files are effectively added to the include patterns. This means that the normal include and ignore behavior still applies - files specified via stdin will still be excluded if they match ignore patterns.\n\n> [!NOTE]\n> When using `--stdin`, file paths can be relative or absolute, and Repomix will automatically handle path resolution and deduplication.\n\nTo include git logs in the output:\n\n```bash\n# Include git logs with default count (50 commits)\nrepomix --include-logs\n\n# Include git logs with specific commit count\nrepomix --include-logs --include-logs-count 10\n\n# Combine with diffs for comprehensive git context\nrepomix --include-logs --include-diffs\n```\n\nThe git logs include commit dates, messages, and file paths for each commit, providing valuable context for AI analysis of code evolution and development patterns.\n\nTo compress the output:\n\n```bash\nrepomix --compress\n\n# You can also use it with remote repositories:\nrepomix --remote yamadashy/repomix --compress\n```\n\nTo initialize a new configuration file (`repomix.config.json`):\n\n```bash\nrepomix --init\n```\n\nOnce you have generated the packed file, you can use it with Generative AI tools like ChatGPT, DeepSeek, Perplexity, Gemini, Gemma, Llama, Grok, and more.\n\n### Docker Usage üê≥\n\nYou can also run Repomix using Docker.  \nThis is useful if you want to run Repomix in an isolated environment or prefer using containers.\n\nBasic usage (current directory):\n\n```bash\ndocker run -v .:/app -it --rm ghcr.io/yamadashy/repomix\n```\n\nTo pack a specific directory:\n\n```bash\ndocker run -v .:/app -it --rm ghcr.io/yamadashy/repomix path/to/directory\n```\n\nProcess a remote repository and output to a `output` directory:\n\n```bash\ndocker run -v ./output:/app -it --rm ghcr.io/yamadashy/repomix --remote https://github.com/yamadashy/repomix\n```\n\n### Prompt Examples\n\nOnce you have generated the packed file with Repomix, you can use it with AI tools like ChatGPT, DeepSeek, Perplexity, Gemini, Gemma, Llama, Grok, and more.\nHere are some example prompts to get you started:\n\n#### Code Review and Refactoring\n\nFor a comprehensive code review and refactoring suggestions:\n\n```\nThis file contains my entire codebase. Please review the overall structure and suggest any improvements or refactoring opportunities, focusing on maintainability and scalability.\n```\n\n#### Documentation Generation\n\nTo generate project documentation:\n\n```\nBased on the codebase in this file, please generate a detailed README.md that includes an overview of the project, its main features, setup instructions, and usage examples.\n```\n\n#### Test Case Generation\n\nFor generating test cases:\n\n```\nAnalyze the code in this file and suggest a comprehensive set of unit tests for the main functions and classes. Include edge cases and potential error scenarios.\n```\n\n#### Code Quality Assessment\n\nEvaluate code quality and adherence to best practices:\n\n```\nReview the codebase for adherence to coding best practices and industry standards. Identify areas where the code could be improved in terms of readability, maintainability, and efficiency. Suggest specific changes to align the code with best practices.\n```\n\n#### Library Overview\n\nGet a high-level understanding of the library\n\n```\nThis file contains the entire codebase of library. Please provide a comprehensive overview of the library, including its main purpose, key features, and overall architecture.\n```\n\nFeel free to modify these prompts based on your specific needs and the capabilities of the AI tool you're using.\n\n### Community Discussion\n\nCheck out our [community discussion](https://github.com/yamadashy/repomix/discussions/154) where users share:\n\n- Which AI tools they're using with Repomix\n- Effective prompts they've discovered\n- How Repomix has helped them\n- Tips and tricks for getting the most out of AI code analysis\n\nFeel free to join the discussion and share your own experiences! Your insights could help others make better use of\nRepomix.\n\n### Output File Format\n\nRepomix generates a single file with clear separators between different parts of your codebase.  \nTo enhance AI comprehension, the output file begins with an AI-oriented explanation, making it easier for AI models to\nunderstand the context and structure of the packed repository.\n\n#### XML Format (default)\n\nThe XML format structures the content in a hierarchical manner:\n\n```xml\nThis file is a merged representation of the entire codebase, combining all repository files into a single document.\n\n<file_summary>\n  (Metadata and usage AI instructions)\n</file_summary>\n\n<directory_structure>\nsrc/\ncli/\ncliOutput.ts\nindex.ts\n\n(...remaining directories)\n</directory_structure>\n\n<files>\n<file path=\"src/index.js\">\n  // File contents here\n</file>\n\n(...remaining files)\n</files>\n\n<instruction>\n(Custom instructions from `output.instructionFilePath`)\n</instruction>\n```\n\nFor those interested in the potential of XML tags in AI contexts:  \nhttps://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/use-xml-tags\n\n> When your prompts involve multiple components like context, instructions, and examples, XML tags can be a\n> game-changer. They help Claude parse your prompts more accurately, leading to higher-quality outputs.\n\nThis means that the XML output from Repomix is not just a different format, but potentially a more effective way to feed\nyour codebase into AI systems for analysis, code review, or other tasks.\n\n#### Markdown Format\n\nTo generate output in Markdown format, use the `--style markdown` option:\n\n```bash\nrepomix --style markdown\n```\n\nThe Markdown format structures the content in a hierarchical manner:\n\n````markdown\nThis file is a merged representation of the entire codebase, combining all repository files into a single document.\n\n# File Summary\n\n(Metadata and usage AI instructions)\n\n# Repository Structure\n\n```\nsrc/\n  cli/\n    cliOutput.ts\n    index.ts\n```\n\n(...remaining directories)\n\n# Repository Files\n\n## File: src/index.js\n\n```\n// File contents here\n```\n\n(...remaining files)\n\n# Instruction\n\n(Custom instructions from `output.instructionFilePath`)\n````\n\nThis format provides a clean, readable structure that is both human-friendly and easily parseable by AI systems.\n\n#### JSON Format\n\nTo generate output in JSON format, use the `--style json` option:\n\n```bash\nrepomix --style json\n```\n\nThe JSON format structures the content as a hierarchical JSON object with camelCase property names:\n\n```json\n{\n  \"fileSummary\": {\n    \"generationHeader\": \"This file is a merged representation of the entire codebase, combined into a single document by Repomix.\",\n    \"purpose\": \"This file contains a packed representation of the entire repository's contents...\",\n    \"fileFormat\": \"The content is organized as follows...\",\n    \"usageGuidelines\": \"- This file should be treated as read-only...\",\n    \"notes\": \"- Some files may have been excluded based on .gitignore rules...\"\n  },\n  \"userProvidedHeader\": \"Custom header text if specified\",\n  \"directoryStructure\": \"src/\\n  cli/\\n    cliOutput.ts\\n    index.ts\\n  config/\\n    configLoader.ts\",\n  \"files\": {\n    \"src/index.js\": \"// File contents here\",\n    \"src/utils.js\": \"// File contents here\"\n  },\n  \"instruction\": \"Custom instructions from instructionFilePath\"\n}\n```\n\nThis format is ideal for:\n- **Programmatic processing**: Easy to parse and manipulate with JSON libraries\n- **API integration**: Direct consumption by web services and applications  \n- **AI tool compatibility**: Structured format for machine learning and AI systems\n- **Data analysis**: Straightforward extraction of specific information using tools like `jq`\n\n##### Working with JSON Output Using `jq`\n\nThe JSON format makes it easy to extract specific information programmatically:\n\n```bash\n# List all file paths\ncat repomix-output.json | jq -r '.files | keys[]'\n\n# Count total number of files\ncat repomix-output.json | jq '.files | keys | length'\n\n# Extract specific file content\ncat repomix-output.json | jq -r '.files[\"README.md\"]'\ncat repomix-output.json | jq -r '.files[\"src/index.js\"]'\n\n# Find files by extension\ncat repomix-output.json | jq -r '.files | keys[] | select(endswith(\".ts\"))'\n\n# Get files containing specific text\ncat repomix-output.json | jq -r '.files | to_entries[] | select(.value | contains(\"function\")) | .key'\n\n# Extract directory structure\ncat repomix-output.json | jq -r '.directoryStructure'\n\n# Get file summary information\ncat repomix-output.json | jq '.fileSummary.purpose'\ncat repomix-output.json | jq -r '.fileSummary.generationHeader'\n\n# Extract user-provided header (if exists)\ncat repomix-output.json | jq -r '.userProvidedHeader // \"No header provided\"'\n\n# Create a file list with sizes\ncat repomix-output.json | jq -r '.files | to_entries[] | \"\\(.key): \\(.value | length) characters\"'\n```\n\n#### Plain Text Format\n\nTo generate output in plain text format, use the `--style plain` option:\n\n```bash\nrepomix --style plain\n```\n\n```text\nThis file is a merged representation of the entire codebase, combining all repository files into a single document.\n\n================================================================\nFile Summary\n================================================================\n(Metadata and usage AI instructions)\n\n================================================================\nDirectory Structure\n================================================================\nsrc/\n  cli/\n    cliOutput.ts\n    index.ts\n  config/\n    configLoader.ts\n\n(...remaining directories)\n\n================================================================\nFiles\n================================================================\n\n================\nFile: src/index.js\n================\n// File contents here\n\n================\nFile: src/utils.js\n================\n// File contents here\n\n(...remaining files)\n\n================================================================\nInstruction\n================================================================\n(Custom instructions from `output.instructionFilePath`)\n```\n\n### Command Line Options\n\n#### Basic Options\n- `-v, --version`: Show version information and exit\n\n#### CLI Input/Output Options\n- `--verbose`: Enable detailed debug logging (shows file processing, token counts, and configuration details)\n- `--quiet`: Suppress all console output except errors (useful for scripting)\n- `--stdout`: Write packed output directly to stdout instead of a file (suppresses all logging)\n- `--stdin`: Read file paths from stdin, one per line (specified files are processed directly)\n- `--copy`: Copy the generated output to system clipboard after processing\n- `--token-count-tree [threshold]`: Show file tree with token counts; optional threshold to show only files with ‚â•N tokens (e.g., --token-count-tree 100)\n- `--top-files-len <number>`: Number of largest files to show in summary (default: 5, e.g., --top-files-len 20)\n\n#### Repomix Output Options\n- `-o, --output <file>`: Output file path (default: repomix-output.xml, use \"-\" for stdout)\n- `--style <style>`: Output format: xml, markdown, json, or plain (default: xml)\n- `--parsable-style`: Escape special characters to ensure valid XML/Markdown (needed when output contains code that breaks formatting)\n- `--compress`: Extract essential code structure (classes, functions, interfaces) using Tree-sitter parsing\n- `--output-show-line-numbers`: Prefix each line with its line number in the output\n- `--no-file-summary`: Omit the file summary section from output\n- `--no-directory-structure`: Omit the directory tree visualization from output\n- `--no-files`: Generate metadata only without file contents (useful for repository analysis)\n- `--remove-comments`: Strip all code comments before packing\n- `--remove-empty-lines`: Remove blank lines from all files\n- `--truncate-base64`: Truncate long base64 data strings to reduce output size\n- `--header-text <text>`: Custom text to include at the beginning of the output\n- `--instruction-file-path <path>`: Path to file containing custom instructions to include in output\n- `--include-empty-directories`: Include folders with no files in directory structure\n- `--no-git-sort-by-changes`: Don't sort files by git change frequency (default: most changed files first)\n- `--include-diffs`: Add git diff section showing working tree and staged changes\n- `--include-logs`: Add git commit history with messages and changed files\n- `--include-logs-count <count>`: Number of recent commits to include with --include-logs (default: 50)\n\n#### File Selection Options\n- `--include <patterns>`: Include only files matching these glob patterns (comma-separated, e.g., \"src/**/*.js,*.md\")\n- `-i, --ignore <patterns>`: Additional patterns to exclude (comma-separated, e.g., \"*.test.js,docs/**\")\n- `--no-gitignore`: Don't use .gitignore rules for filtering files\n- `--no-default-patterns`: Don't apply built-in ignore patterns (node_modules, .git, build dirs, etc.)\n\n#### Remote Repository Options\n- `--remote <url>`: Clone and pack a remote repository (GitHub URL or user/repo format)\n- `--remote-branch <name>`: Specific branch, tag, or commit to use (default: repository's default branch)\n\n#### Configuration Options\n- `-c, --config <path>`: Use custom config file instead of repomix.config.json\n- `--init`: Create a new repomix.config.json file with defaults\n- `--global`: With --init, create config in home directory instead of current directory\n\n#### Security Options\n- `--no-security-check`: Skip scanning for sensitive data like API keys and passwords\n\n#### Token Count Options\n- `--token-count-encoding <encoding>`: Tokenizer model for counting: o200k_base (GPT-4o), cl100k_base (GPT-3.5/4), etc. (default: o200k_base)\n\n#### MCP\n- `--mcp`: Run as Model Context Protocol server for AI tool integration\n\n#### Examples\n\n```bash\n# Basic usage\nrepomix\n\n# Custom output\nrepomix -o output.xml --style xml\n\n# Output to stdout\nrepomix --stdout > custom-output.txt\n\n# Send output to stdout, then pipe into another command (for example, simonw/llm)\nrepomix --stdout | llm \"Please explain what this code does.\"\n\n# Custom output with compression\nrepomix --compress\n\n# Process specific files\nrepomix --include \"src/**/*.ts\" --ignore \"**/*.test.ts\"\n\n# Remote repository with branch\nrepomix --remote https://github.com/user/repo/tree/main\n\n# Remote repository with commit\nrepomix --remote https://github.com/user/repo/commit/836abcd7335137228ad77feb28655d85712680f1\n\n# Remote repository with shorthand\nrepomix --remote user/repo\n```\n\n### Updating Repomix\n\nTo update a globally installed Repomix:\n\n```bash\n# Using npm\nnpm update -g repomix\n\n# Using yarn\nyarn global upgrade repomix\n\n# Using bun\nbun update -g repomix\n```\n\nUsing `npx repomix` is generally more convenient as it always uses the latest version.\n\n### Remote Repository Processing\n\nRepomix supports processing remote Git repositories without the need for manual cloning. This feature allows you to\nquickly analyze any public Git repository with a single command.\n\nTo process a remote repository, use the `--remote` option followed by the repository URL:\n\n```bash\nrepomix --remote https://github.com/yamadashy/repomix\n```\n\nYou can also use GitHub's shorthand format:\n\n```bash\nrepomix --remote yamadashy/repomix\n```\n\nYou can specify the branch name, tag, or commit hash:\n\n```bash\n# Using --remote-branch option\nrepomix --remote https://github.com/yamadashy/repomix --remote-branch main\n\n# Using branch's URL\nrepomix --remote https://github.com/yamadashy/repomix/tree/main\n```\n\nOr use a specific commit hash:\n\n```bash\n# Using --remote-branch option\nrepomix --remote https://github.com/yamadashy/repomix --remote-branch 935b695\n\n# Using commit's URL\nrepomix --remote https://github.com/yamadashy/repomix/commit/836abcd7335137228ad77feb28655d85712680f1\n```\n\n### Code Compression\n\nThe `--compress` option utilizes [Tree-sitter](https://github.com/tree-sitter/tree-sitter) to perform intelligent code extraction, focusing on essential function and class signatures while removing implementation details. This can help reduce token count while retaining important structural information.\n\n```bash\nrepomix --compress\n```\n\nFor example, this code:\n\n```typescript\nimport { ShoppingItem } from './shopping-item';\n\n/**\n * Calculate the total price of shopping items\n */\nconst calculateTotal = (\n  items: ShoppingItem[]\n) => {\n  let total = 0;\n  for (const item of items) {\n    total += item.price * item.quantity;\n  }\n  return total;\n}\n\n// Shopping item interface\ninterface Item {\n  name: string;\n  price: number;\n  quantity: number;\n}\n```\n\nWill be compressed to:\n\n```typescript\nimport { ShoppingItem } from './shopping-item';\n‚ãÆ----\n/**\n * Calculate the total price of shopping items\n */\nconst calculateTotal = (\n  items: ShoppingItem[]\n) => {\n‚ãÆ----\n// Shopping item interface\ninterface Item {\n  name: string;\n  price: number;\n  quantity: number;\n}\n```\n\n> [!NOTE]\n> This is an experimental feature that we'll be actively improving based on user feedback and real-world usage\n\n### Token Count Optimization\n\nUnderstanding your codebase's token distribution is crucial for optimizing AI interactions. Use the `--token-count-tree` option to visualize token usage across your project:\n\n```bash\nrepomix --token-count-tree\n```\n\nThis displays a hierarchical view of your codebase with token counts:\n\n```\nüî¢ Token Count Tree:\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n‚îî‚îÄ‚îÄ src/ (70,925 tokens)\n    ‚îú‚îÄ‚îÄ cli/ (12,714 tokens)\n    ‚îÇ   ‚îú‚îÄ‚îÄ actions/ (7,546 tokens)\n    ‚îÇ   ‚îî‚îÄ‚îÄ reporters/ (990 tokens)\n    ‚îî‚îÄ‚îÄ core/ (41,600 tokens)\n        ‚îú‚îÄ‚îÄ file/ (10,098 tokens)\n        ‚îî‚îÄ‚îÄ output/ (5,808 tokens)\n```\n\nYou can also set a minimum token threshold to focus on larger files:\n\n```bash\nrepomix --token-count-tree 1000  # Only show files/directories with 1000+ tokens\n```\n\nThis helps you:\n- **Identify token-heavy files** that might exceed AI context limits\n- **Optimize file selection** using `--include` and `--ignore` patterns  \n- **Plan compression strategies** by targeting the largest contributors\n- **Balance content vs. context** when preparing code for AI analysis\n\n### MCP Server Integration\n\nRepomix supports the [Model Context Protocol (MCP)](https://modelcontextprotocol.io), allowing AI assistants to directly interact with your codebase. When run as an MCP server, Repomix provides tools that enable AI assistants to package local or remote repositories for analysis without requiring manual file preparation.\n\n```bash\nrepomix --mcp\n```\n\n#### Configuring MCP Servers\n\nTo use Repomix as an MCP server with AI assistants like Claude, you need to configure the MCP settings:\n\n**For VS Code:**\n\nYou can install the Repomix MCP server in VS Code using one of these methods:\n\n1. **Using the Install Badge:**\n\n  [![Install in VS Code](https://img.shields.io/badge/VS_Code-Install_Server-0098FF?style=flat-square&logo=visualstudiocode&logoColor=white)](https://insiders.vscode.dev/redirect/mcp/install?name=repomix&config=%7B%22command%22%3A%22npx%22%2C%22args%22%3A%5B%22-y%22%2C%22repomix%22%2C%22--mcp%22%5D%7D)\n  [![Install in VS Code Insiders](https://img.shields.io/badge/VS_Code_Insiders-Install_Server-24bfa5?style=flat-square&logo=visualstudiocode&logoColor=white)](https://insiders.vscode.dev/redirect/mcp/install?name=repomix&config=%7B%22command%22%3A%22npx%22%2C%22args%22%3A%5B%22-y%22%2C%22repomix%22%2C%22--mcp%22%5D%7D&quality=insiders)\n\n2. **Using the Command Line:**\n\n  ```bash\n  code --add-mcp '{\"name\":\"repomix\",\"command\":\"npx\",\"args\":[\"-y\",\"repomix\",\"--mcp\"]}'\n  ```\n\n  For VS Code Insiders:\n  ```bash\n  code-insiders --add-mcp '{\"name\":\"repomix\",\"command\":\"npx\",\"args\":[\"-y\",\"repomix\",\"--mcp\"]}'\n  ```\n\n**For Cline (VS Code extension):**\n\nEdit the `cline_mcp_settings.json` file:\n```json\n{\n  \"mcpServers\": {\n    \"repomix\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"repomix\",\n        \"--mcp\"\n      ]\n    }\n  }\n}\n```\n\n**For Cursor:**\n\nIn Cursor, add a new MCP server from `Cursor Settings` > `MCP` > `+ Add new global MCP server` with a configuration similar to Cline.\n\n**For Claude Desktop:**\n\nEdit the `claude_desktop_config.json` file with similar configuration to Cline's config.\n\n**For Claude Code:**\n\nTo configure Repomix as an MCP server in [Claude Code](https://docs.anthropic.com/en/docs/claude-code/overview), use the following command:\n\n```bash\nclaude mcp add repomix -- npx -y repomix --mcp\n```\n\n**Using Docker instead of npx:**\n\nYou can use Docker as an alternative to npx for running Repomix as an MCP server:\n\n```json\n{\n  \"mcpServers\": {\n    \"repomix-docker\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"-i\",\n        \"--rm\",\n        \"ghcr.io/yamadashy/repomix\",\n        \"--mcp\"\n      ]\n    }\n  }\n}\n```\n\nOnce configured, your AI assistant can directly use Repomix's capabilities to analyze codebases without manual file preparation, making code analysis workflows more efficient.\n\n#### Available MCP Tools\n\nWhen running as an MCP server, Repomix provides the following tools:\n\n1. **pack_codebase**: Package a local code directory into a consolidated XML file for AI analysis\n  - Parameters:\n    - `directory`: Absolute path to the directory to pack\n    - `compress`: (Optional, default: false) Enable Tree-sitter compression to extract essential code signatures and structure while removing implementation details. Reduces token usage by ~70% while preserving semantic meaning. Generally not needed since grep_repomix_output allows incremental content retrieval. Use only when you specifically need the entire codebase content for large repositories.\n    - `includePatterns`: (Optional) Specify files to include using fast-glob patterns. Multiple patterns can be comma-separated (e.g., \"**/*.{js,ts}\", \"src/**,docs/**\"). Only matching files will be processed.\n    - `ignorePatterns`: (Optional) Specify additional files to exclude using fast-glob patterns. Multiple patterns can be comma-separated (e.g., \"test/**,*.spec.js\", \"node_modules/**,dist/**\"). These patterns supplement .gitignore and built-in exclusions.\n    - `topFilesLength`: (Optional, default: 10) Number of largest files by size to display in the metrics summary for codebase analysis.\n\n2. **attach_packed_output**: Attach an existing Repomix packed output file for AI analysis\n  - Parameters:\n    - `path`: Path to a directory containing repomix-output.xml or direct path to a packed repository XML file\n    - `topFilesLength`: (Optional, default: 10) Number of largest files by size to display in the metrics summary\n  - Features:\n    - Accepts either a directory containing a repomix-output.xml file or a direct path to an XML file\n    - Registers the file with the MCP server and returns the same structure as the pack_codebase tool\n    - Provides secure access to existing packed outputs without requiring re-processing\n    - Useful for working with previously generated packed repositories\n\n3. **pack_remote_repository**: Fetch, clone, and package a GitHub repository into a consolidated XML file for AI analysis\n  - Parameters:\n    - `remote`: GitHub repository URL or user/repo format (e.g., \"yamadashy/repomix\", \"https://github.com/user/repo\", or \"https://github.com/user/repo/tree/branch\")\n    - `compress`: (Optional, default: false) Enable Tree-sitter compression to extract essential code signatures and structure while removing implementation details. Reduces token usage by ~70% while preserving semantic meaning. Generally not needed since grep_repomix_output allows incremental content retrieval. Use only when you specifically need the entire codebase content for large repositories.\n    - `includePatterns`: (Optional) Specify files to include using fast-glob patterns. Multiple patterns can be comma-separated (e.g., \"**/*.{js,ts}\", \"src/**,docs/**\"). Only matching files will be processed.\n    - `ignorePatterns`: (Optional) Specify additional files to exclude using fast-glob patterns. Multiple patterns can be comma-separated (e.g., \"test/**,*.spec.js\", \"node_modules/**,dist/**\"). These patterns supplement .gitignore and built-in exclusions.\n    - `topFilesLength`: (Optional, default: 10) Number of largest files by size to display in the metrics summary for codebase analysis.\n\n4. **read_repomix_output**: Read the contents of a Repomix-generated output file. Supports partial reading with line range specification for large files.\n  - Parameters:\n    - `outputId`: ID of the Repomix output file to read\n    - `startLine`: (Optional) Starting line number (1-based, inclusive). If not specified, reads from beginning.\n    - `endLine`: (Optional) Ending line number (1-based, inclusive). If not specified, reads to end.\n  - Features:\n    - Specifically designed for web-based environments or sandboxed applications\n    - Retrieves the content of previously generated outputs using their ID\n    - Provides secure access to packed codebase without requiring file system access\n    - Supports partial reading for large files\n\n5. **grep_repomix_output**: Search for patterns in a Repomix output file using grep-like functionality with JavaScript RegExp syntax\n  - Parameters:\n    - `outputId`: ID of the Repomix output file to search\n    - `pattern`: Search pattern (JavaScript RegExp regular expression syntax)\n    - `contextLines`: (Optional, default: 0) Number of context lines to show before and after each match. Overridden by beforeLines/afterLines if specified.\n    - `beforeLines`: (Optional) Number of context lines to show before each match (like grep -B). Takes precedence over contextLines.\n    - `afterLines`: (Optional) Number of context lines to show after each match (like grep -A). Takes precedence over contextLines.\n    - `ignoreCase`: (Optional, default: false) Perform case-insensitive matching\n  - Features:\n    - Uses JavaScript RegExp syntax for powerful pattern matching\n    - Supports context lines for better understanding of matches\n    - Allows separate control of before/after context lines\n    - Case-sensitive and case-insensitive search options\n\n6. **file_system_read_file**: Read a file from the local file system using an absolute path. Includes built-in security validation to detect and prevent access to files containing sensitive information.\n  - Parameters:\n    - `path`: Absolute path to the file to read\n  - Security features:\n    - Implements security validation using [Secretlint](https://github.com/secretlint/secretlint)\n    - Prevents access to files containing sensitive information (API keys, passwords, secrets)\n    - Validates absolute paths to prevent directory traversal attacks\n\n7. **file_system_read_directory**: List the contents of a directory using an absolute path. Returns a formatted list showing files and subdirectories with clear indicators.\n  - Parameters:\n    - `path`: Absolute path to the directory to list\n  - Features:\n    - Shows files and directories with clear indicators (`[FILE]` or `[DIR]`)\n    - Provides safe directory traversal with proper error handling\n    - Validates paths and ensures they are absolute\n    - Useful for exploring project structure and understanding codebase organization\n\n## ‚öôÔ∏è Configuration\n\nCreate a `repomix.config.json` file in your project root for custom configurations.\n\n```bash\nrepomix --init\n```\n\nHere's an explanation of the configuration options:\n\n| Option                           | Description                                                                                                                  | Default                |\n|----------------------------------|------------------------------------------------------------------------------------------------------------------------------|------------------------|\n| `input.maxFileSize`              | Maximum file size in bytes to process. Files larger than this will be skipped                                                | `50000000`            |\n| `output.filePath`                | The name of the output file                                                                                                  | `\"repomix-output.xml\"` |\n| `output.style`                   | The style of the output (`xml`, `markdown`, `json`, `plain`)                                                                 | `\"xml\"`                |\n| `output.parsableStyle`           | Whether to escape the output based on the chosen style schema. Note that this can increase token count.                      | `false`                |\n| `output.compress`                | Whether to perform intelligent code extraction to reduce token count                                                         | `false`                |\n| `output.headerText`              | Custom text to include in the file header                                                                                    | `null`                 |\n| `output.instructionFilePath`     | Path to a file containing detailed custom instructions                                                                       | `null`                 |\n| `output.fileSummary`             | Whether to include a summary section at the beginning of the output                                                          | `true`                 |\n| `output.directoryStructure`      | Whether to include the directory structure in the output                                                                     | `true`                 |\n| `output.files`                   | Whether to include file contents in the output                                                                               | `true`                 |\n| `output.removeComments`          | Whether to remove comments from supported file types                                                                         | `false`                |\n| `output.removeEmptyLines`        | Whether to remove empty lines from the output                                                                                | `false`                |\n| `output.showLineNumbers`         | Whether to add line numbers to each line in the output                                                                       | `false`                |\n| `output.truncateBase64`          | Whether to truncate long base64 data strings (e.g., images) to reduce token count                                            | `false`                |\n| `output.copyToClipboard`         | Whether to copy the output to system clipboard in addition to saving the file                                                | `false`                |\n| `output.topFilesLength`          | Number of top files to display in the summary. If set to 0, no summary will be displayed                                     | `5`                    |\n| `output.tokenCountTree`          | Whether to display file tree with token count summaries. Can be boolean or number (minimum token count threshold)           | `false`                |\n| `output.includeEmptyDirectories` | Whether to include empty directories in the repository structure                                                             | `false`                |\n| `output.git.sortByChanges`       | Whether to sort files by git change count (files with more changes appear at the bottom)                                     | `true`                 |\n| `output.git.sortByChangesMaxCommits` | Maximum number of commits to analyze for git changes                                                                     | `100`                  |\n| `output.git.includeDiffs`       | Whether to include git diffs in the output (includes both work tree and staged changes separately)                          | `false`                |\n| `output.git.includeLogs`        | Whether to include git logs in the output (includes commit history with dates, messages, and file paths)                   | `false`                |\n| `output.git.includeLogsCount`   | Number of git log commits to include                                                                                         | `50`                   |\n| `include`                        | Patterns of files to include (using [glob patterns](https://github.com/mrmlnc/fast-glob?tab=readme-ov-file#pattern-syntax))  | `[]`                   |\n| `ignore.useGitignore`            | Whether to use patterns from the project's `.gitignore` file                                                                 | `true`                 |\n| `ignore.useDefaultPatterns`      | Whether to use default ignore patterns                                                                                       | `true`                 |\n| `ignore.customPatterns`          | Additional patterns to ignore (using [glob patterns](https://github.com/mrmlnc/fast-glob?tab=readme-ov-file#pattern-syntax)) | `[]`                   |\n| `security.enableSecurityCheck`   | Whether to perform security checks on files                                                                                  | `true`                 |\n| `tokenCount.encoding`            | Token count encoding used by OpenAI's [tiktoken](https://github.com/openai/tiktoken) tokenizer (e.g., `o200k_base` for GPT-4o, `cl100k_base` for GPT-4/3.5). See [tiktoken model.py](https://github.com/openai/tiktoken/blob/main/tiktoken/model.py#L24) for encoding details. | `\"o200k_base\"`         |\n\nThe configuration file supports [JSON5](https://json5.org/) syntax, which allows:\n- Comments (both single-line and multi-line)\n- Trailing commas in objects and arrays\n- Unquoted property names\n- More relaxed string syntax\n\nExample configuration:\n\n```json5\n{\n  \"input\": {\n    \"maxFileSize\": 50000000\n  },\n  \"output\": {\n    \"filePath\": \"repomix-output.xml\",\n    \"style\": \"xml\",\n    \"parsableStyle\": false,\n    \"compress\": false,\n    \"headerText\": \"Custom header information for the packed file.\",\n    \"fileSummary\": true,\n    \"directoryStructure\": true,\n    \"files\": true,\n    \"removeComments\": false,\n    \"removeEmptyLines\": false,\n    \"topFilesLength\": 5,\n    \"tokenCountTree\": false, // or true, or a number like 10 for minimum token threshold\n    \"showLineNumbers\": false,\n    \"truncateBase64\": false,\n    \"copyToClipboard\": false,\n    \"includeEmptyDirectories\": false,\n    \"git\": {\n      \"sortByChanges\": true,\n      \"sortByChangesMaxCommits\": 100,\n      \"includeDiffs\": false,\n      \"includeLogs\": false,\n      \"includeLogsCount\": 50\n    }\n  },\n  \"include\": [\"**/*\"],\n  \"ignore\": {\n    \"useGitignore\": true,\n    \"useDefaultPatterns\": true,\n    // Patterns can also be specified in .repomixignore\n    \"customPatterns\": [\n      \"additional-folder\",\n      \"**/*.log\"\n    ],\n  },\n  \"security\": {\n    \"enableSecurityCheck\": true\n  },\n  \"tokenCount\": {\n    \"encoding\": \"o200k_base\"\n  }\n}\n```\n\n### Global Configuration\n\nTo create a global configuration file:\n\n```bash\nrepomix --init --global\n```\n\nThe global configuration file will be created in:\n\n- Windows: `%LOCALAPPDATA%\\Repomix\\repomix.config.json`\n- macOS/Linux: `$XDG_CONFIG_HOME/repomix/repomix.config.json` or `~/.config/repomix/repomix.config.json`\n\nNote: Local configuration (if present) takes precedence over global configuration.\n\n### Include and Ignore\n\n#### Include Patterns\n\nRepomix now supports specifying files to include\nusing [glob patterns](https://github.com/mrmlnc/fast-glob?tab=readme-ov-file#pattern-syntax). This allows for more\nflexible and powerful file selection:\n\n- Use `**/*.js` to include all JavaScript files in any directory\n- Use `src/**/*` to include all files within the `src` directory and its subdirectories\n- Combine multiple patterns like `[\"src/**/*.js\", \"**/*.md\"]` to include JavaScript files in `src` and all Markdown\n  files\n\n#### Ignore Patterns\n\nRepomix offers multiple methods to set ignore patterns for excluding specific files or directories during the packing\nprocess:\n\n- **.gitignore**: By default, patterns listed in your project's `.gitignore` files and `.git/info/exclude` are used. This behavior can be controlled with the `ignore.useGitignore` setting or the `--no-gitignore` cli option.\n- **Default patterns**: Repomix includes a default list of commonly excluded files and directories (e.g., node_modules,\n  .git, binary files). This feature can be controlled with the `ignore.useDefaultPatterns` setting or the `--no-default-patterns` cli option. Please\n  see [defaultIgnore.ts](src/config/defaultIgnore.ts) for more details.\n- **.repomixignore**: You can create a `.repomixignore` file in your project root to define Repomix-specific ignore\n  patterns. This file follows the same format as `.gitignore`.\n- **Custom patterns**: Additional ignore patterns can be specified using the `ignore.customPatterns` option in the\n  configuration file. You can overwrite this setting with the `-i, --ignore` command line option.\n\nPriority Order (from highest to lowest):\n\n1. Custom patterns `ignore.customPatterns`\n2. `.repomixignore`\n3. `.gitignore` and `.git/info/exclude` (if `ignore.useGitignore` is true and `--no-gitignore` is not used)\n4. Default patterns (if `ignore.useDefaultPatterns` is true and `--no-default-patterns` is not used)\n\nThis approach allows for flexible file exclusion configuration based on your project's needs. It helps optimize the size\nof the generated pack file by ensuring the exclusion of security-sensitive files and large binary files, while\npreventing the leakage of confidential information.\n\nNote: Binary files are not included in the packed output by default, but their paths are listed in the \"Repository\nStructure\" section of the output file. This provides a complete overview of the repository structure while keeping the\npacked file efficient and text-based.\n\n### Custom Instruction\n\nThe `output.instructionFilePath` option allows you to specify a separate file containing detailed instructions or\ncontext about your project. This allows AI systems to understand the specific context and requirements of your project,\npotentially leading to more relevant and tailored analysis or suggestions.\n\nHere's an example of how you might use this feature:\n\n1. Create a file named `repomix-instruction.md` in your project root:\n\n```markdown\n# Coding Guidelines\n\n- Follow the Airbnb JavaScript Style Guide\n- Suggest splitting files into smaller, focused units when appropriate\n- Add comments for non-obvious logic. Keep all text in English\n- All new features should have corresponding unit tests\n\n# Generate Comprehensive Output\n\n- Include all content without abbreviation, unless specified otherwise\n- Optimize for handling large codebases while maintaining output quality\n```\n\n2. In your `repomix.config.json`, add the `instructionFilePath` option:\n\n```json5\n{\n  \"output\": {\n    \"instructionFilePath\": \"repomix-instruction.md\",\n    // other options...\n  }\n}\n```\n\nWhen Repomix generates the output, it will include the contents of `repomix-instruction.md` in a dedicated section.\n\nNote: The instruction content is appended at the end of the output file. This placement can be particularly effective\nfor AI systems. For those interested in understanding why this might be beneficial, Anthropic provides some insights in\ntheir documentation:  \nhttps://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/long-context-tips\n\n> Put long-form data at the top: Place your long documents and inputs (~20K+ tokens) near the top of your prompt, above\n> your query, instructions, and examples. This can significantly improve Claude's performance across all models.\n> Queries at the end can improve response quality by up to 30% in tests, especially with complex, multi-document inputs.\n\n### Comment Removal\n\nWhen `output.removeComments` is set to `true`, Repomix will attempt to remove comments from supported file types. This\nfeature can help reduce the size of the output file and focus on the essential code content.\n\nSupported languages include:  \nHTML, CSS, JavaScript, TypeScript, Vue, Svelte, Python, PHP, Ruby, C, C#, Java, Go, Rust, Swift, Kotlin, Dart, Shell,\nand YAML.\n\nNote: The comment removal process is conservative to avoid accidentally removing code. In complex cases, some comments\nmight be retained.\n\n## üîç Security Check\n\nRepomix includes a security check feature that uses [Secretlint](https://github.com/secretlint/secretlint) to detect\npotentially sensitive information in your files. This feature helps you identify possible security risks before sharing\nyour packed repository.\n\nThe security check results will be displayed in the CLI output after the packing process is complete. If any suspicious\nfiles are detected, you'll see a list of these files along with a warning message.\n\nExample output:\n\n```\nüîç Security Check:\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n2 suspicious file(s) detected:\n1. src/utils/test.txt\n2. tests/utils/secretLintUtils.test.ts\n\nPlease review these files for potentially sensitive information.\n```\n\nBy default, Repomix's security check feature is enabled. You can disable it by setting `security.enableSecurityCheck` to\n`false` in your configuration file:\n\n```json\n{\n  \"security\": {\n    \"enableSecurityCheck\": false\n  }\n}\n```\n\nOr using the `--no-security-check` command line option:\n\n```bash\nrepomix --no-security-check\n```\n\n> [!NOTE]\n> Disabling security checks may expose sensitive information. Use this option with caution and only when necessary, such\n> as when working with test files or documentation that contains example credentials.\n\n## ü§ñ Using Repomix with GitHub Actions\n\nYou can also use Repomix in your GitHub Actions workflows. This is useful for automating the process of packing your codebase for AI analysis.\n\nBasic usage:\n\n```yaml\n- name: Pack repository with Repomix\n  uses: yamadashy/repomix/.github/actions/repomix@main\n  with:\n    output: repomix-output.xml\n    style: xml\n```\n\nUse `--style` to generate output in different formats:\n\n```yaml\n- name: Pack repository with Repomix\n  uses: yamadashy/repomix/.github/actions/repomix@main\n  with:\n    output: repomix-output.md\n    style: markdown\n```\n\n```yaml\n- name: Pack repository with Repomix (JSON format)\n  uses: yamadashy/repomix/.github/actions/repomix@main\n  with:\n    output: repomix-output.json\n    style: json\n```\n\nPack specific directories with compression:\n\n```yaml\n- name: Pack repository with Repomix\n  uses: yamadashy/repomix/.github/actions/repomix@main\n  with:\n    directories: src tests\n    include: \"**/*.ts,**/*.md\"\n    ignore: \"**/*.test.ts\"\n    output: repomix-output.txt\n    compress: true\n```\n\nUpload the output file as an artifact:\n\n```yaml\n- name: Pack repository with Repomix\n  uses: yamadashy/repomix/.github/actions/repomix@main\n  with:\n    directories: src\n    output: repomix-output.txt\n    compress: true\n\n- name: Upload Repomix output\n  uses: actions/upload-artifact@v4\n  with:\n    name: repomix-output\n    path: repomix-output.txt\n```\n\nComplete workflow example:\n\n```yaml\nname: Pack repository with Repomix\n\non:\n  workflow_dispatch:\n  push:\n    branches: [ main ]\n  pull_request:\n    branches: [ main ]\n\njobs:\n  pack-repo:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Checkout code\n        uses: actions/checkout@v4\n\n      - name: Pack repository with Repomix\n        uses: yamadashy/repomix/.github/actions/repomix@main\n        with:\n          output: repomix-output.xml\n\n      - name: Upload Repomix output\n        uses: actions/upload-artifact@v4\n        with:\n          name: repomix-output.xml\n          path: repomix-output.xml\n          retention-days: 30\n```\n\nSee the complete workflow example [here](https://github.com/yamadashy/repomix/blob/main/.github/workflows/pack-repository.yml).\n\n### Action Inputs\n\n| Name | Description | Default |\n|------|-------------|---------|\n| `directories` | Space-separated list of directories to process (e.g., `src tests docs`) | `.` |\n| `include` | Comma-separated glob patterns to include files (e.g., `**/*.ts,**/*.md`) | `\"\"` |\n| `ignore` | Comma-separated glob patterns to ignore files (e.g., `**/*.test.ts,**/node_modules/**`) | `\"\"` |\n| `output` | Relative path for the packed file (extension determines format: `.txt`, `.md`, `.xml`) | `repomix-output.xml` |\n| `compress` | Enable smart compression to reduce output size by pruning implementation details | `true` |\n| `style` | Output style (`xml`, `markdown`, `json`, `plain`) | `xml` |\n| `additional-args` | Extra raw arguments for the repomix CLI (e.g., `--no-file-summary --no-security-check`) | `\"\"` |\n| `repomix-version` | Version of the npm package to install (supports semver ranges, tags, or specific versions like `0.2.25`) | `latest` |\n\n### Action Outputs\n\n| Name | Description |\n|------|-------------|\n| `output_file` | Path to the generated output file. Can be used in subsequent steps for artifact upload, LLM processing, or other operations. The file contains a formatted representation of your codebase based on the specified options. |\n\n## üìö Using Repomix as a Library\n\nIn addition to using Repomix as a CLI tool, you can also use it as a library in your Node.js applications.\n\n### Installation\n\n```bash\nnpm install repomix\n```\n\n### Basic Usage\n\n```javascript\nimport { runCli, type CliOptions } from 'repomix';\n\n// Process current directory with custom options\nasync function packProject() {\n  const options = {\n    output: 'output.xml',\n    style: 'xml',\n    compress: true,\n    quiet: true\n  } as CliOptions;\n  \n  const result = await runCli(['.'], process.cwd(), options);\n  return result.packResult;\n}\n```\n\n### Process Remote Repository\n\n```javascript\nimport { runCli, type CliOptions } from 'repomix';\n\n// Clone and process a GitHub repo\nasync function processRemoteRepo(repoUrl) {\n  const options = {\n    remote: repoUrl,\n    output: 'output.xml',\n    compress: true\n  } as CliOptions;\n  \n  return await runCli(['.'], process.cwd(), options);\n}\n```\n\n### Using Core Components\n\nIf you need more control, you can use the low-level APIs:\n\n```javascript\nimport { searchFiles, collectFiles, processFiles, TokenCounter } from 'repomix';\n\nasync function analyzeFiles(directory) {\n  // Find and collect files\n  const { filePaths } = await searchFiles(directory, { /* config */ });\n  const rawFiles = await collectFiles(filePaths, directory);\n  const processedFiles = await processFiles(rawFiles, { /* config */ });\n  \n  // Count tokens\n  const tokenCounter = new TokenCounter('o200k_base');\n  \n  // Return analysis results\n  return processedFiles.map(file => ({\n    path: file.path,\n    tokens: tokenCounter.countTokens(file.content)\n  }));\n}\n```\n\nFor more examples, check the source code at [website/server/src/remoteRepo.ts](https://github.com/yamadashy/repomix/blob/main/website/server/src/remoteRepo.ts) which demonstrates how repomix.com uses the library.\n\n## ü§ù Contribution\n\nWe welcome contributions from the community! To get started, please refer to our [Contributing Guide](CONTRIBUTING.md).\n\n### Contributors\n\n<a href=\"https://github.com/yamadashy/repomix/graphs/contributors\">\n  <img alt=\"contributors\" src=\"https://contrib.rocks/image?repo=yamadashy/repomix\"/>\n</a>\n\n## üîí Privacy Policy\n\n### Repomix CLI Tool\n\n- **Data Collection**: The Repomix CLI tool does **not** collect, transmit, or store any user data, telemetry, or repository information.\n- **Network Usage**: Repomix CLI operates fully offline after installation. The only cases where an internet connection is needed are:\n  - Installation via npm/yarn.\n  - Using the `--remote` flag to process remote repositories.\n  - Checking for updates (manually triggered).\n- **Security Considerations**: Since all processing is local, Repomix CLI is safe to use with private and internal repositories.\n\n### Repomix Website ([repomix.com](https://repomix.com/))\n\n- **Data Collection**: The Repomix website uses **Google Analytics** to collect usage data, such as page views and user interactions. This helps us understand how the website is used and improve the user experience.\n- **File Processing**: When uploading ZIP files or folders, your files are temporarily stored on our servers for processing. All uploaded files and processed data are automatically deleted immediately after processing is complete.\n\n### Repomix Browser Extension\n\n- **Data Collection**: The Repomix browser extension does **not** collect, transmit, or store any user data, telemetry, or repository information.\n- **Permissions**: The extension only requires minimal permissions necessary to add the Repomix button to GitHub repository pages. It does not access or modify repository data.\n\n### Liability Disclaimer\n\nRepomix (the CLI tool, website, and browser extension) is provided \"as is\" without any warranties or guarantees.  \nWe do not take responsibility for how the generated output is used, including but not limited to its accuracy, legality, or any potential consequences arising from its use.\n\n## üìú License\n\nThis project is licensed under the [MIT License](LICENSE).\n\n<p align=\"center\">\n  &nbsp;&nbsp;&nbsp;\n  <a href=\"#repo-content-pjax-container\" target=\"_blank\">\n    Back To Top\n  </a>\n</p>",
      "npm_url": "https://www.npmjs.com/package/repomix",
      "npm_downloads": 622821,
      "keywords": [
        "codebases",
        "yamadashy",
        "ai",
        "codebases ai",
        "yamadashy repomix",
        "ai tools"
      ],
      "category": "official-servers"
    }
  }
}