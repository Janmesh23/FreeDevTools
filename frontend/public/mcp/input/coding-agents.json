{
  "category": "coding-agents",
  "categoryDisplay": "Coding Agents",
  "description": "Full coding agents that enable LLMs to read, edit, and execute code and solve general programming tasks completely autonomously.",
  "totalRepositories": 13,
  "repositories": {
    "VertexStudio--developer": {
      "owner": "VertexStudio",
      "name": "developer",
      "url": "https://github.com/VertexStudio/developer",
      "imageUrl": "",
      "description": "Comprehensive developer tools for file editing, shell command execution, and screen capture capabilities",
      "stars": 14,
      "forks": 7,
      "license": "MIT License",
      "language": "Rust",
      "updated_at": "2025-08-28T06:11:41Z",
      "readme_content": "# Developer MCP Server\n\nA general purpose Model Context Protocol (MCP) server that provides comprehensive developer tools for file editing, shell command execution, and screen capture capabilities. Built using the [rmcp](https://github.com/modelcontextprotocol/rmcp) crate.\n\n## üöÄ Features\n\n### üìù Text Editor\n- **View files** with language detection for markdown formatting\n- **Write/create files** with automatic directory creation\n- **String replacement** with precise matching\n- **Undo functionality** with edit history\n- **File size protection** (400KB limit for text files)\n\n### üñ•Ô∏è Shell Integration\n- **Cross-platform command execution** (PowerShell on Windows, bash/zsh on Unix)\n- **Combined stdout/stderr output** as it appears in terminal\n- **Output size protection** (400KB limit)\n- **Platform-specific optimizations**\n\n### üì∏ Screen Capture\n- **Full display screenshots** with monitor selection\n- **Window-specific capture** by title\n- **Automatic image optimization** (768px max width)\n- **Base64 encoded PNG output**\n\n### üñºÔ∏è Image Processing\n- **Image file processing** from disk\n- **Automatic resizing** while maintaining aspect ratio\n- **Format conversion** to PNG\n- **macOS screenshot filename handling**\n\n### üîÑ Workflow Management\n- **Multi-step problem solving** with sequential progression\n- **Branching workflows** for alternative solution paths\n- **Step revision** capability to update previous steps\n- **Context preservation** across complex reasoning processes\n\n### üîí Security Features\n- **Gitignore integration** - respects `.gitignore` patterns for file access control\n- **Path validation** - requires absolute paths to prevent directory traversal\n- **File size limits** - prevents memory exhaustion attacks\n- **Access pattern filtering** - blocks access to sensitive files\n\n## üìã Requirements\n\n- **Rust** 1.70+ (for building from source)\n- **Claude Desktop** or compatible MCP client\n- **Operating System**: macOS, Linux, or Windows\n\n## üõ†Ô∏è Installation\n\n### Option 1: Build from Source (Recommended)\n\n1. **Clone the repository:**\n   ```bash\n   git clone git@github.com:VertexStudio/developer.git\n   cd developer\n   ```\n\n2. **Build the project:**\n   ```bash\n   cargo build --release\n   ```\n\n3. **The binary will be available at:**\n   ```\n   target/release/developer\n   ```\n\n### Option 2: Development Build\n\nFor development/testing purposes:\n```bash\ncargo build\n# Binary at: target/debug/developer\n```\n\n## ‚öôÔ∏è Configuration\n\n### Claude Desktop Setup\n\n1. **Open Claude Desktop configuration file:**\n\n   **macOS/Linux:**\n   ```bash\n   code ~/Library/Application\\ Support/Claude/claude_desktop_config.json\n   ```\n\n   **Windows:**\n   ```bash\n   code %APPDATA%\\Claude\\claude_desktop_config.json\n   ```\n\n2. **Add the developer server configuration:**\n\n   ```json\n   {\n     \"mcpServers\": {\n       \"developer\": {\n         \"command\": \"/path/to/your/developer/target/release/developer\",\n         \"args\": []\n       }\n     }\n   }\n   ```\n\n   **Example configurations:**\n\n   **Development build:**\n   ```json\n   {\n     \"mcpServers\": {\n       \"developer\": {\n         \"command\": \"/Users/rozgo/vertex/developer/target/debug/developer\",\n         \"args\": []\n       }\n     }\n   }\n   ```\n\n   **Production build:**\n   ```json\n   {\n     \"mcpServers\": {\n       \"developer\": {\n         \"command\": \"/Users/rozgo/vertex/developer/target/release/developer\",\n         \"args\": []\n       }\n     }\n   }\n   ```\n\n3. **Restart Claude Desktop** to load the new configuration.\n\n### File Access Control (Optional)\n\nCreate a `.gitignore` file in your working directory to control which files the server can access:\n\n```gitignore\n# Sensitive files\n.env\n.env.*\nsecrets.*\nprivate/\n*.key\n*.pem\n\n# Build artifacts\ntarget/\nnode_modules/\ndist/\n```\n\nThe server will automatically respect these patterns and block access to matching files.\n\n## üéØ Usage Examples\n\nOnce configured, you can use these tools directly in Claude Desktop:\n\n### Text Editing\n```\n\"Can you view the contents of /path/to/my/file.rs?\"\n\n\"Please create a new file at /path/to/hello.py with a simple hello world script\"\n\n\"Replace the line 'old_function()' with 'new_function()' in /path/to/main.rs\"\n\n\"Undo the last edit to /path/to/main.rs\"\n```\n\n### Shell Commands\n```\n\"Run 'ls -la' to show me the current directory contents\"\n\n\"Execute 'cargo test' to run the test suite\"\n\n\"Run 'git status' to check the repository status\"\n```\n\n### Screen Capture\n```\n\"Take a screenshot of my main display\"\n\n\"Capture a screenshot of the window titled 'VS Code'\"\n\n\"Show me what windows are available for capture\"\n```\n\n### Image Processing\n```\n\"Process the image at /path/to/screenshot.png and show it to me\"\n\n\"Load and display the image from /Users/me/Desktop/diagram.jpg\"\n```\n\n### Workflow Management\n```\n\"Start a workflow to implement a new feature with 5 steps\"\n\n\"Create a branch from step 3 to explore an alternative approach\"\n\n\"Revise step 2 to use a different algorithm\"\n```\n\n## üèóÔ∏è Architecture\n\n```\nDeveloper MCP Server\n‚îú‚îÄ‚îÄ Text Editor     ‚Üí File viewing, editing, string replacement, undo\n‚îú‚îÄ‚îÄ Shell           ‚Üí Cross-platform command execution  \n‚îú‚îÄ‚îÄ Screen Capture  ‚Üí Display and window screenshots\n‚îú‚îÄ‚îÄ Image Processor ‚Üí File-based image processing\n‚îú‚îÄ‚îÄ Workflow        ‚Üí Multi-step problem solving with branching\n‚îî‚îÄ‚îÄ Security Layer  ‚Üí Gitignore integration, path validation\n```\n\n## üîß Tool Reference\n\n### text_editor\n- **Commands:** `view`, `write`, `str_replace`, `undo_edit`\n- **Parameters:** `path` (required), `file_text`, `old_str`, `new_str`\n- **Limits:** 400KB file size, absolute paths only\n\n### shell  \n- **Parameters:** `command` (required)\n- **Features:** Platform detection, output redirection, size limits\n- **Limits:** 400KB output size\n\n### screen_capture\n- **Parameters:** `display` (optional), `window_title` (optional)\n- **Output:** Base64 PNG image, 768px max width\n\n### list_windows\n- **Parameters:** None\n- **Output:** List of capturable window titles\n\n### image_processor\n- **Parameters:** `path` (required)\n- **Features:** Auto-resize, format conversion, macOS compatibility\n- **Limits:** 10MB file size\n\n### workflow\n- **Parameters:** `step_description`, `step_number`, `total_steps`, `next_step_needed` (required), `is_step_revision`, `revises_step`, `branch_from_step`, `branch_id`, `needs_more_steps` (optional)\n- **Features:** Sequential progression, branching, step revision\n- **Output:** JSON workflow status\n\n## üêõ Troubleshooting\n\n### Common Issues\n\n**\"Tool not found\" errors:**\n- Ensure the binary path in your configuration is correct\n- Verify the binary exists and is executable\n- Check Claude Desktop logs for detailed error messages\n\n**\"File access denied\" errors:**\n- Check if the file is blocked by `.gitignore` patterns\n- Ensure you're using absolute paths (not relative paths)\n- Verify file permissions\n\n**\"Command failed\" errors:**\n- Ensure the command exists and is in your system PATH\n- Check if the command requires special permissions\n- Verify the command syntax for your operating system\n\n### Debug Mode\n\nBuild with debug info for troubleshooting:\n```bash\ncargo build\n# Use target/debug/developer in your configuration\n```\n\n### MCP Inspector\n\nUse the official MCP inspector to debug and test tools:\n```bash\nnpx @modelcontextprotocol/inspector target/debug/developer\n```\n\nThis will open a web interface where you can:\n- Inspect available tools and their schemas\n- Test tool calls interactively\n- Debug server responses\n- Validate MCP protocol compliance\n\n### Tools Schema Export\n\nExport the tools JSON schema for debugging or integration:\n```bash\n# Save tools schema to file\ncargo run toolbox > tools.json\n\n# Pretty print tools schema\ncargo run toolbox | jq .\n```\n\n## ü§ù Contributing\n\n1. Fork the repository\n2. Create a feature branch\n3. Add tests for new functionality\n4. Ensure all tests pass: `cargo test`\n5. Submit a pull request\n\n## üìù License\n\n[MIT](LICENSE)\n\n## üîó Related Projects\n\n- [Model Context Protocol](https://modelcontextprotocol.io/) - The protocol specification\n- [Claude Desktop](https://claude.ai/download) - Official Claude desktop application\n- [MCP Servers](https://github.com/modelcontextprotocol/servers) - Official MCP server implementations\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "vertexstudio",
        "agents",
        "developer",
        "vertexstudio developer",
        "coding agents",
        "autonomously vertexstudio"
      ],
      "category": "coding-agents"
    },
    "doggybee--mcp-server-leetcode": {
      "owner": "doggybee",
      "name": "mcp-server-leetcode",
      "url": "https://github.com/doggybee/mcp-server-leetcode",
      "imageUrl": "",
      "description": "An MCP server that enables AI models to search, retrieve, and solve LeetCode problems. Supports metadata filtering, user profiles, submissions, and contest data access.",
      "stars": 32,
      "forks": 9,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-09-27T00:44:30Z",
      "readme_content": "# MCP Server LeetCode\n\n[![npm version](https://img.shields.io/npm/v/@mcpfun/mcp-server-leetcode.svg)](https://www.npmjs.com/package/@mcpfun/mcp-server-leetcode)\n[![GitHub license](https://img.shields.io/github/license/doggybee/mcp-server-leetcode.svg)](https://github.com/doggybee/mcp-server-leetcode/blob/main/LICENSE)\n[![Version](https://img.shields.io/badge/version-1.0.1-blue.svg)](https://github.com/doggybee/mcp-server-leetcode/releases)\n[![smithery badge](https://smithery.ai/badge/@doggybee/mcp-server-leetcode)](https://smithery.ai/server/@doggybee/mcp-server-leetcode)\n\nA Model Context Protocol (MCP) server for LeetCode that enables AI assistants to access LeetCode problems, user information, and contest data.\n\n## Features\n\n- üöÄ Fast access to LeetCode API\n- üîç Search problems, retrieve daily challenges, and check user profiles\n- üèÜ Query contest data and rankings\n- üß© Full support for MCP tools and resources\n- üì¶ Provides both CLI and programmable API\n\n## Installation\n\n### Installing via Smithery\n\nTo install mcp-server-leetcode for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@doggybee/mcp-server-leetcode):\n\n```bash\nnpx -y @smithery/cli install @doggybee/mcp-server-leetcode --client claude\n```\n\n### Global Installation\n\n```bash\nnpm install -g @mcpfun/mcp-server-leetcode\n```\n\nOnce installed, you can run it directly from the command line:\n\n```bash\nmcp-server-leetcode\n```\n\n### Local Installation\n\n```bash\nnpm install @mcpfun/mcp-server-leetcode\n```\n\n## Usage\n\n### Integration with Claude for Desktop\n\nAdd the following to your Claude for Desktop `claude_desktop_config.json` file:\n\n```json\n{\n  \"mcpServers\": {\n    \"leetcode\": {\n      \"command\": \"mcp-server-leetcode\"\n    }\n  }\n}\n```\n\nFor local development:\n\n```json\n{\n  \"mcpServers\": {\n    \"leetcode\": {\n      \"command\": \"node\",\n      \"args\": [\"/path/to/dist/index.js\"]\n    }\n  }\n}\n```\n\n### Use as a Library\n\n```javascript\nimport { LeetCodeService } from '@mcpfun/mcp-server-leetcode';\n\n// Initialize the service\nconst leetcodeService = new LeetCodeService();\n\n// Get daily challenge\nconst dailyChallenge = await leetcodeService.getDailyChallenge();\n\n// Search problems\nconst problems = await leetcodeService.searchProblems({\n  difficulty: 'MEDIUM',\n  tags: 'array+dynamic-programming'\n});\n```\n\n## Available Tools\n\n### Problem-related Tools\n\n| Tool Name | Description | Parameters |\n|--------|------|------|\n| `get-daily-challenge` | Get the daily challenge | None |\n| `get-problem` | Get details for a specific problem | `titleSlug` (string) |\n| `search-problems` | Search for problems based on criteria | `tags` (optional), `difficulty` (optional), `limit` (default 20), `skip` (default 0) |\n\n### User-related Tools\n\n| Tool Name | Description | Parameters |\n|--------|------|------|\n| `get-user-profile` | Get user information | `username` (string) |\n| `get-user-submissions` | Get user submission history | `username` (string), `limit` (optional, default 20) |\n| `get-user-contest-ranking` | Get user contest rankings | `username` (string) |\n\n### Contest-related Tools\n\n| Tool Name | Description | Parameters |\n|--------|------|------|\n| `get-contest-details` | Get contest details | `contestSlug` (string) |\n\n## Available Resources\n\n### Problem Resources\n\n- `leetcode://daily-challenge`: Daily challenge\n- `leetcode://problem/{titleSlug}`: Problem details\n- `leetcode://problems{?tags,difficulty,limit,skip}`: Problem list\n\n### User Resources\n\n- `leetcode://user/{username}/profile`: User profile\n- `leetcode://user/{username}/submissions{?limit}`: User submissions\n- `leetcode://user/{username}/contest-ranking`: User contest ranking\n\n## Local Development\n\nClone the repository and install dependencies:\n\n```bash\ngit clone https://github.com/doggybee/mcp-server-leetcode.git\ncd mcp-server-leetcode\nnpm install\n```\n\nRun in development mode:\n\n```bash\nnpm run dev\n```\n\nBuild the project:\n\n```bash\nnpm run build\n```\n\n## License\n\nMIT\n\n## Related Projects\n\n- [Model Context Protocol](https://modelcontextprotocol.io) - MCP specifications and documentation\n- [Claude for Desktop](https://claude.ai/download) - AI assistant with MCP support\n\n## Acknowledgements\n\n- This project was inspired by [alfa-leetcode-api](https://github.com/alfaarghya/alfa-leetcode-api)\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "agents",
        "doggybee",
        "coding",
        "coding agents",
        "agents coding",
        "doggybee mcp"
      ],
      "category": "coding-agents"
    },
    "ezyang--codemcp": {
      "owner": "ezyang",
      "name": "codemcp",
      "url": "https://github.com/ezyang/codemcp",
      "imageUrl": "",
      "description": "Coding agent with basic read, write and command line tools.",
      "stars": 1559,
      "forks": 133,
      "license": "Apache License 2.0",
      "language": "Python",
      "updated_at": "2025-10-04T08:02:56Z",
      "readme_content": "# codemcp\n\nMake Claude Desktop a pair programming assistant by installing codemcp.  With\nit, you can directly ask Claude to implement features, fix bugs and do\nrefactors on a codebase on your computer; Claude will directly edit files and\nrun tests.  Say goodbye to copying code in and out of Claude's chat window!\n\n\n\ncodemcp offers similar functionality to other AI coding software (Claude Code,\nCursor, Cline, Aider), but it occupies a unique point in the design space:\n\n1. It's intended to be used with **Claude Pro**, Anthropic's $20/mo\n   subscription offering.  I like paying for my usage with a subscription plan\n   because it means **zero marginal cost** for agent actions; no more feeling\n   bad that you wasted five bucks on a changeset that doesn't work.\n\n   Note that if you have Claude Max ($100/mo), Claude Code can also be used\n   with subscription based pricing.  The value proposition for codemcp is\n   murkier in this case (and it is definitely inferior to Claude Code in some\n   respects), but you can still use codemcp with Claude Max if you prefer some\n   of the other UI decisions it makes.  (Also, it's open source, so you can\n   change it if you don't like it, unlike Claude Code!)\n\n2. It's built around **auto-accept by default**.  I want my agent to get as\n   far as it can without my supervision, so I can review everything in one go at\n   the end.  There are two key things that codemcp does differently than most\n   coding agents: we **forbid unrestricted shell**, instead requiring you to\n   predeclare commands the agent can use in ``codemcp.toml``, and we **Git\n   version all LLM edits**, so you can roll back agent changes on a\n   fine-grained basis and don't have to worry about forgetting to commit\n   changes.\n\n3. It's **IDE agnostic**: you ask Claude to make changes, it makes them, and\n   then you can use your favorite IDE setup to review the changes and make\n   further edits.  I use vim as my daily driver editor, and coding environments\n   that require VSCode or a specific editor are a turn off for me.\n\n## IMPORTANT: For master users - Major changes for token efficiency\n\nTo improve codemcp's token efficiency, on master I am in the process of\nchanging codemcp back into a multi-tool tool (instead of a single tool whose\ninstructions are blatted into chat when you InitProject).  This means you have\nto manually approve tool use.  Because tool use approval is persistent across\nmultiple chats, I think this is a reasonable tradeoff to make, but if you\nreally don't like, file a bug at\n[refined-claude](https://github.com/ezyang/refined-claude/issues) browser\nextension for supporting auto-approve tool use.\n\n## Installation\n\nI recommend this specific way of installing and using codemcp:\n\n1. Install `uv` and install git, if they are not installed already.\n\n2. Install [claude-mcp](https://chromewebstore.google.com/detail/mcp-for-claudeai/jbdhaamjibfahpekpnjeikanebpdpfpb) on your browser.\n   This enables you to connect to SSE MCP servers directly from the website,\n   which means you don't need to use Claude Desktop and can easily have\n   multiple chat windows going in parallel.  We expect this extension should\n   be soon obsoleted by the rollout of\n   [Integrations](https://www.anthropic.com/news/integrations).  At time of\n   writing, however, Integrations have not yet arrived for Claude Pro subscribers.\n\n3. Run codemcp using ``uvx --from git+https://github.com/ezyang/codemcp@prod codemcp serve``.\n   You can add ``--port 1234`` if you need it to listen on a non-standard port.\n\n   Pro tip: if you like to live dangerously, you can change `prod` to `main`.  If\n   you want to pin to a specific release, replace it with `0.3.0` or similar.\n\n   Pro tip: you can run codemcp remotely!  If you use\n   [Tailscale](https://tailscale.com/) and trust all devices on your Tailnet,\n   you can do this securely by passing ``--host 100.101.102.103`` (replace the\n   IP with the Tailscale IP address of your node.  This IP typically lives in\n   the 100.64.0.0/10 range.)  **WARNING:** Anyone with access to this MCP can perform\n   arbitrary code execution on your computer, it is **EXTREMELY** unlikely you want to\n   bind to 0.0.0.0.\n\n4. Configure claude-mcp with URL: ``http://127.0.0.1:8000/sse`` (replace the port if needed.)\n\n5. Unfortunately, the web UI inconsistently displays the hammer icon.  However, you can verify\n   that the MCP server is working by looking for \"[MCP codemcp] SSE connection opened\" in the\n   Console, or by asking Claude what tools it has available (it should say\n   tools from codemcp are available.)\n\nIf you prefer to use Claude Desktop or have unusual needs, check out [INSTALL.md](INSTALL.md) for\ninstallation instructions for a variety of non-standard situations.\n\n## Usage\n\nFirst, you must create a `codemcp.toml` file in the Git repository checkout\nyou want to work on.  If you want the agent to be able to do things like run\nyour formatter or run tests, add the commands to execute them in the commands\nsection (note: these commands need to appropriately setup any virtual\nenvironment they need):\n\n```toml\nformat = [\"./run_format.sh\"]\ntest = [\"./run_test.sh\"]\n```\n\nThe ``format`` command is special; it is always run after every file edit.\n\nNext, in Claude Desktop, we recommend creating a Project and putting this in\nthe Project Instructions:\n\n```\nInitialize codemcp with $PROJECT_DIR\n```\n\nWhere `$PROJECT_DIR` is the path to the project you want to work on.\n\nThen chat with Claude about what changes you want to make to the project.\nEvery time codemcp makes a change to your code, it will generate a commit.\n\nTo see some sample transcripts using this tool, check out:\n\n- [Implement a new feature](https://claude.ai/share/a229d291-6800-4cb8-a0df-896a47602ca0)\n- [Fix failing tests](https://claude.ai/share/2b7161ef-5683-4261-ad45-fabc3708f950)\n- [Do a refactor](https://claude.ai/share/f005b43c-a657-43e5-ad9f-4714a5cd746f)\n\ncodemcp will generate a commit per chat and amend it as it is working on your feature.\n\n## Philosophy\n\n- When you get rate limited, take the time to do something else (review\n  Claude's code, review someone else's code, make plans, do some meetings)\n\n- This is *not* an autonomous agent.  At minimum, you have to intervene after\n  every chat to review the changes and request the next change.  While you\n  *can* ask for a long list of things to be done in a single chat, you will\n  likely hit Claude Desktop's output limit and have to manually \"continue\" the\n  agent anyway.  Embrace it, and use the interruptions to make sure Claude is\n  doing the right thing.\n\n- When Claude goes off the rails, it costs you time rather than dollars.\n  Behave accordingly: if time is the bottleneck, watch Claude's incremental\n  output carefully.\n\n## Configuration\n\nHere are all the config options supported by `codemcp.toml`:\n\n```toml\nproject_prompt = \"\"\"\nBefore beginning work on this feature, write a short haiku.  Do this only once.\n\"\"\"\n\n[commands]\nformat = [\"./run_format.sh\"]\ntest = [\"./run_test.sh\"]\n```\n\nThe `project_prompt` will be loaded when you initialize the project in chats.\n\nThe `commands` section allows you to configure commands for specific tools.  The\nnames are told to the LLM, who will decide when it wants to run them.  You can add\ninstructions how to use tools in the `project_prompt`; we also support a more verbose\nsyntax where you can give specific instructions on a tool-by-tool basis:\n\n```\n[commands.test]\ncommand = [\"./run_test.sh\"]\ndoc = \"Accepts a pytest-style test selector as an argument to run a specific test.\"\n```\n\n## Troubleshooting\n\nTo run the server with inspector, use:\n\n```\nPYTHONPATH=. mcp dev codemcp/__main__.py\n```\n\nLogs are written to `~/.codemcp/codemcp.log`. The log level can be set in a global configuration file at `~/.codemcprc`:\n\n```toml\n[logger]\nverbosity = \"INFO\"  # Can be DEBUG, INFO, WARNING, ERROR, or CRITICAL\n```\n\nLogging is not configurable on a per project basis, but this shouldn't matter\nmuch because it's difficult to use Claude Desktop in parallel on multiple\nprojects anyway.\n\n## Contributing\n\nSee [CONTRIBUTING.md](CONTRIBUTING.md).",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "coding",
        "codemcp",
        "agents",
        "coding agents",
        "coding agent",
        "agents coding"
      ],
      "category": "coding-agents"
    },
    "gabrielmaialva33--winx-code-agent": {
      "owner": "gabrielmaialva33",
      "name": "winx-code-agent",
      "url": "https://github.com/gabrielmaialva33/winx-code-agent",
      "imageUrl": "",
      "description": "A high-performance Rust reimplementation of WCGW for code agents, providing shell execution and advanced file management capabilities for LLMs via MCP.",
      "stars": 19,
      "forks": 7,
      "license": "MIT License",
      "language": "Rust",
      "updated_at": "2025-09-27T04:16:40Z",
      "readme_content": "<table style=\"width:100%\" align=\"center\" border=\"0\">\n  <tr>\n    <td width=\"40%\" align=\"center\"></td>\n    <td><h1>‚ú® Ôº∑ÔΩâÔΩéÔΩò Ôº°ÔΩáÔΩÖÔΩéÔΩî ‚ú®</h1></td>\n  </tr>\n</table>\n\n<p align=\"center\">\n  <strong>ü¶Ä A high-performance Rust implementation of WCGW for code agents ü¶Ä</strong>\n</p>\n\n<p align=\"center\">\n  <img src=\"https://img.shields.io/badge/language-Rust-orange?style=flat&logo=rust\" alt=\"Language\" />\n  <img src=\"https://img.shields.io/badge/license-MIT-blue?style=flat&logo=appveyor\" alt=\"License\" />\n  <img src=\"https://img.shields.io/github/languages/count/gabrielmaialva33/winx-code-agent?style=flat&logo=appveyor\" alt=\"GitHub language count\" >\n  <img src=\"https://img.shields.io/github/repo-size/gabrielmaialva33/winx-code-agent?style=flat&logo=appveyor\" alt=\"Repository size\" >\n  <a href=\"https://github.com/gabrielmaialva33/winx-code-agent/commits/main\">\n    <img src=\"https://img.shields.io/github/last-commit/gabrielmaialva33/winx-code-agent?style=flat&logo=appveyor\" alt=\"Last Commit\" >\n  </a>\n  <img src=\"https://img.shields.io/badge/made%20by-Maia-15c3d6?style=flat&logo=appveyor\" alt=\"Made by Maia\" >\n\n[![Trust Score](https://archestra.ai/mcp-catalog/api/badge/quality/gabrielmaialva33/winx-code-agent)](https://archestra.ai/mcp-catalog/gabrielmaialva33__winx-code-agent)\n</p>\n\n---\n\n## üìñ Overview\n\nWinx is a Rust reimplementation of [WCGW](https://github.com/rusiaaman/wcgw), providing shell execution and file\nmanagement capabilities for LLM code agents. Designed for high performance and reliability, Winx integrates with Claude\nand other LLMs via the Model Context Protocol (MCP).\n\n## üåü Features\n\n- ‚ö° **High Performance**: Implemented in Rust for maximum efficiency\n- ü§ñ **Multi-Provider AI Integration** (v0.1.5):\n    - üéØ **DashScope/Qwen3**: Primary AI provider with Alibaba Cloud's Qwen3-Coder-Plus model\n    - üîÑ **NVIDIA NIM**: Fallback 1 with Qwen3-235B-A22B model and thinking mode\n    - üíé **Google Gemini**: Fallback 2 with Gemini-1.5-Pro and Gemini-1.5-Flash models\n    - üîß **AI-Powered Code Analysis**: Detect bugs, security issues, and performance problems\n    - üöÄ **AI Code Generation**: Generate code from natural language descriptions\n    - üìö **AI Code Explanation**: Get detailed explanations of complex code\n    - üé≠ **AI-to-AI Chat**: Winx fairy assistant with personality and multiple conversation modes\n    - üõ°Ô∏è **Smart Fallback System**: Automatic provider switching on failures\n- üìÅ **Advanced File Operations**:\n    - üìñ Read files with line range support\n    - ‚úèÔ∏è Write new files with syntax validation\n    - üîç Edit existing files with intelligent search/replace\n    - üîÑ Smart file caching with change detection\n    - üìè Line-level granular read tracking\n- üñ•Ô∏è **Command Execution**:\n    - üöÄ Run shell commands with status tracking\n    - üìü Interactive shell with persistent session\n    - ‚å®Ô∏è Full input/output control via PTY\n    - üèÉ‚Äç‚ôÇÔ∏è Background process execution\n- üîÄ **Operational Modes**:\n    - üîì `wcgw`: Complete access to all features\n    - üîé `architect`: Read-only mode for planning and analysis\n    - üîí `code_writer`: Restricted access for controlled modifications\n- üìä **Project Management**:\n    - üìù Repository structure analysis\n    - üíæ Context saving and task resumption\n- üñºÔ∏è **Media Support**: Read images and encode as base64\n- üß© **MCP Protocol**: Seamless integration with Claude and other LLMs\n\n---\n\n## üñáÔ∏è Installation & Setup\n\n### Prerequisites\n\n- Rust 1.70 or higher\n- Tokio runtime\n\n### 1. Clone the Repository\n\n```bash\ngit clone https://github.com/gabrielmaialva33/winx-code-agent.git && cd winx\n```\n\n### 2. Build the Project\n\n```bash\n# For development\ncargo build\n\n# For production\ncargo build --release\n```\n\n### 3. Run the Agent\n\n```bash\n# Using cargo\ncargo run\n\n# Or directly\n./target/release/winx-code-agent\n```\n\n---\n\n## üîß Integration with Claude\n\nWinx is designed to work seamlessly with Claude via the MCP interface:\n\n1. **Edit Claude's Configuration**\n   ```json\n   // In claude_desktop_config.json (Mac: ~/Library/Application Support/Claude/claude_desktop_config.json)\n   {\n     \"mcpServers\": {\n       \"winx\": {\n         \"command\": \"/path/to/winx-code-agent\",\n         \"args\": [],\n         \"env\": {\n           \"RUST_LOG\": \"info\",\n           \"DASHSCOPE_API_KEY\": \"your-dashscope-api-key\",\n           \"DASHSCOPE_MODEL\": \"qwen3-coder-plus\",\n           \"NVIDIA_API_KEY\": \"your-nvidia-api-key\",\n           \"NVIDIA_DEFAULT_MODEL\": \"qwen/qwen3-235b-a22b\",\n           \"GEMINI_API_KEY\": \"your-gemini-api-key\",\n           \"GEMINI_MODEL\": \"gemini-1.5-pro\"\n         }\n       }\n     }\n   }\n   ```\n\n2. **Restart Claude** after configuration to see the Winx MCP integration icon.\n\n3. **Start using the tools** through Claude's interface.\n\n---\n\n## üõ†Ô∏è Available Tools\n\n### üöÄ initialize\n\nAlways call this first to set up your workspace environment.\n\n```\ninitialize(\n  type=\"first_call\",\n  any_workspace_path=\"/path/to/project\",\n  mode_name=\"wcgw\"\n)\n```\n\n### üñ•Ô∏è bash_command\n\nExecute shell commands with persistent shell state and full interactive capabilities.\n\n```\n# Execute commands\nbash_command(\n  action_json={\"command\": \"ls -la\"},\n  chat_id=\"i1234\"\n)\n\n# Check command status\nbash_command(\n  action_json={\"status_check\": true},\n  chat_id=\"i1234\"\n)\n\n# Send input to running commands\nbash_command(\n  action_json={\"send_text\": \"y\"},\n  chat_id=\"i1234\"\n)\n\n# Send special keys (Ctrl+C, arrow keys, etc.)\nbash_command(\n  action_json={\"send_specials\": [\"Enter\", \"CtrlC\"]},\n  chat_id=\"i1234\"\n)\n```\n\n### üìÅ File Operations\n\n- **read_files**: Read file content with line range support\n  ```\n  read_files(\n    file_paths=[\"/path/to/file.rs\"],\n    show_line_numbers_reason=null\n  )\n  ```\n\n- **file_write_or_edit**: Write or edit files\n  ```\n  file_write_or_edit(\n    file_path=\"/path/to/file.rs\",\n    percentage_to_change=100,\n    file_content_or_search_replace_blocks=\"content...\",\n    chat_id=\"i1234\"\n  )\n  ```\n\n- **read_image**: Process image files as base64\n  ```\n  read_image(\n    file_path=\"/path/to/image.png\"\n  )\n  ```\n\n### üíæ context_save\n\nSave task context for later resumption.\n\n```\ncontext_save(\n  id=\"task_name\",\n  project_root_path=\"/path/to/project\",\n  description=\"Task description\",\n  relevant_file_globs=[\"**/*.rs\"]\n)\n```\n\n### ü§ñ AI-Powered Tools (v0.1.5)\n\n- **code_analyzer**: AI-powered code analysis for bugs, security, and performance\n  ```\n  code_analyzer(\n    file_path=\"/path/to/code.rs\",\n    language=\"Rust\"\n  )\n  ```\n\n- **ai_generate_code**: Generate code from natural language description\n  ```\n  ai_generate_code(\n    prompt=\"Create a REST API for user management\",\n    language=\"Rust\",\n    context=\"Using Axum framework\",\n    max_tokens=1000,\n    temperature=0.7\n  )\n  ```\n\n- **ai_explain_code**: Get AI explanation and documentation for code\n  ```\n  ai_explain_code(\n    file_path=\"/path/to/code.rs\",\n    language=\"Rust\",\n    detail_level=\"expert\"\n  )\n  ```\n\n- **winx_chat**: Chat with Winx, your AI assistant fairy ‚ú®\n  ```\n  winx_chat(\n    message=\"Oi Winx, como funciona o sistema de fallback?\",\n    conversation_mode=\"technical\",\n    include_system_info=true,\n    personality_level=8\n  )\n  ```\n\n  **Conversation Modes:**\n  - `casual`: Informal, friendly chat with personality üòä\n  - `technical`: Focused technical responses üîß\n  - `help`: Help mode with detailed explanations üÜò\n  - `debug`: Debugging assistance üêõ\n  - `creative`: Creative brainstorming üí°\n  - `mentor`: Teaching and best practices üßô‚Äç‚ôÄÔ∏è\n\n---\n\n## üë®‚Äçüíª Usage Workflow\n\n1. **Initialize the workspace**\n   ```\n   initialize(type=\"first_call\", any_workspace_path=\"/path/to/your/project\")\n   ```\n\n2. **Explore the codebase**\n   ```\n   bash_command(action_json={\"command\": \"find . -type f -name '*.rs' | sort\"}, chat_id=\"i1234\")\n   ```\n\n3. **Read key files**\n   ```\n   read_files(file_paths=[\"/path/to/important_file.rs\"])\n   ```\n\n4. **Make changes**\n   ```\n   file_write_or_edit(file_path=\"/path/to/file.rs\", percentage_to_change=30, \n   file_content_or_search_replace_blocks=\"<<<<<<< SEARCH\\nold code\\n=======\\nnew code\\n>>>>>>> REPLACE\", \n   chat_id=\"i1234\")\n   ```\n\n5. **Run tests**\n   ```\n   bash_command(action_json={\"command\": \"cargo test\"}, chat_id=\"i1234\")\n   ```\n\n6. **Chat with Winx for help**\n   ```\n   winx_chat(message=\"Winx, posso ter ajuda para otimizar este c√≥digo?\", \n   conversation_mode=\"mentor\", include_system_info=true)\n   ```\n\n7. **Save context for later**\n   ```\n   context_save(id=\"my_task\", project_root_path=\"/path/to/project\", \n   description=\"Implementation of feature X\", relevant_file_globs=[\"src/**/*.rs\"])\n   ```\n\n---\n\n## üè∑ Need Support or Assistance?\n\nIf you need help or have any questions about Winx, feel free to reach out via the following channels:\n\n- [GitHub Issues](https://github.com/gabrielmaialva33/winx-code-agent/issues/new?assignees=&labels=question&title=support%3A+):\n  Open a support issue on GitHub.\n- Email: gabrielmaialva33@gmail.com\n\n---\n\n## üìù Changelog\n\n### v0.1.5 (Latest) - Multi-Provider AI Integration\n\n**üöÄ Major Features:**\n- **Multi-Provider AI System**: Primary DashScope, fallback to NVIDIA, then Gemini\n- **DashScope/Qwen3 Integration**: Alibaba Cloud's Qwen3-Coder-Plus as primary AI provider\n- **Smart Fallback System**: Automatic provider switching with comprehensive error handling\n- **3 New AI Tools**: `code_analyzer`, `ai_generate_code`, `ai_explain_code`\n\n**üéØ AI Providers:**\n- **DashScope**: Primary provider with OpenAI-compatible API format\n- **NVIDIA NIM**: Qwen3-235B-A22B with thinking mode and MoE architecture\n- **Google Gemini**: Gemini-1.5-Pro and Gemini-1.5-Flash models\n\n**üõ†Ô∏è Technical Improvements:**\n- Rate limiting and retry logic for all AI providers\n- Comprehensive logging and error reporting\n- Environment-based configuration management\n- Full CI/CD quality checks (formatting, linting, testing)\n\n---\n\n## üôè Special Thanks\n\nA huge thank you to [rusiaaman](https://github.com/rusiaaman) for the inspiring work\non [WCGW](https://github.com/rusiaaman/wcgw), which served as the primary inspiration for this project. Winx\nreimplements WCGW's features in Rust for enhanced performance and reliability.\n\n---\n\n## üìú License\n\nMIT",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "agents",
        "agent",
        "wcgw",
        "code agents",
        "coding agents",
        "code agent"
      ],
      "category": "coding-agents"
    },
    "jinzcdev--leetcode-mcp-server": {
      "owner": "jinzcdev",
      "name": "leetcode-mcp-server",
      "url": "https://github.com/jinzcdev/leetcode-mcp-server",
      "imageUrl": "",
      "description": "MCP server enabling automated access to **LeetCode**'s programming problems, solutions, submissions and public data with optional authentication for user-specific features (e.g., notes), supporting both `leetcode.com` (global) and `leetcode.cn` (China) sites.",
      "stars": 57,
      "forks": 14,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-10-03T07:49:11Z",
      "readme_content": "# LeetCode MCP Server\n\n[![NPM Version](https://img.shields.io/npm/v/@jinzcdev/leetcode-mcp-server.svg)](https://www.npmjs.com/package/@jinzcdev/leetcode-mcp-server)\n[![GitHub License](https://img.shields.io/github/license/jinzcdev/leetcode-mcp-server.svg)](https://img.shields.io/github/license/jinzcdev/leetcode-mcp-server.svg)\n[![smithery badge](https://smithery.ai/badge/@jinzcdev/leetcode-mcp-server)](https://smithery.ai/server/@jinzcdev/leetcode-mcp-server)\n[![Chinese Doc](https://img.shields.io/badge/ÁÆÄ‰Ωì‰∏≠Êñá-ÁÇπÂáªÊü•Áúã-orange)](README_zh-CN.md)\n[![Stars](https://img.shields.io/github/stars/jinzcdev/leetcode-mcp-server)](https://github.com/jinzcdev/leetcode-mcp-server)\n\nThe LeetCode MCP Server is a [Model Context Protocol (MCP)](https://modelcontextprotocol.io/introduction) server that provides seamless integration with LeetCode APIs, enabling advanced automation and intelligent interaction with LeetCode's programming problems, contests, solutions, and user data.\n\n<a href=\"https://glama.ai/mcp/servers/@jinzcdev/leetcode-mcp-server\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@jinzcdev/leetcode-mcp-server/badge\" alt=\"LeetCode Server MCP server\" />\n</a>\n\n## Features\n\n- üåê **Multi-site Support**: Support‚Äã both leetcode.com (Global) and leetcode.cn (China) platforms\n- üìä **Problem Data Retrieval**: Obtain detailed problem descriptions, constraints, examples, official editorials, and ‚Äãuser-submitted solutions\n- üë§ **User Data Access**: Retrieve user profiles, submission history, and contest performance\n- üîí **‚ÄãPrivate Data Access**: Create and query user notes, track problem-solving progress, and analyze submission details (AC/WA analysis)\n- üîç **Advanced Search Capabilities**: Filter problems by tags, difficulty levels, categories, and keywords\n- üìÖ **Daily Challenge Access**: Easily access daily challenge problems\n\n## Prerequisites\n\n1. Node.js (v20.x or above)\n2. (Optional) LeetCode session cookie for authenticated API access\n\n## Installation\n\n### Installing via Smithery\n\nTo install leetcode-mcp-server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@jinzcdev/leetcode-mcp-server):\n\n```bash\nnpx -y @smithery/cli install @jinzcdev/leetcode-mcp-server --client claude\n```\n\n### Manual Installation\n\n```bash\n# Install from npm\nnpm install @jinzcdev/leetcode-mcp-server -g\n\n# Or run with Global site configuration\nnpx -y @jinzcdev/leetcode-mcp-server --site global\n\n# Run with authentication (for accessing private data)\nnpx -y @jinzcdev/leetcode-mcp-server --site global --session <YOUR_LEETCODE_SESSION_COOKIE>\n```\n\nAlternatively, you can clone the repository and run it locally:\n\n```bash\n# Clone the repository\ngit clone https://github.com/jinzcdev/leetcode-mcp-server.git\n\n# Navigate to the project directory\ncd leetcode-mcp-server\n\n# Build the project\nnpm install && npm run build\n\n# Run the server\nnode build/index.js --site global\n```\n\n## Usage\n\n### Visual Studio Code Integration\n\nAdd the following JSON configuration to your User Settings (JSON) file. Access this by pressing `Ctrl/Cmd + Shift + P` and searching for `Preferences: Open User Settings (JSON)`.\n\n#### Option 1: Using Environment Variables\n\n```json\n{\n  \"mcp\": {\n    \"servers\": {\n      \"leetcode\": {\n        \"type\": \"stdio\",\n        \"command\": \"npx\",\n        \"args\": [\"-y\", \"@jinzcdev/leetcode-mcp-server\"],\n        \"env\": {\n          \"LEETCODE_SITE\": \"global\",\n          \"LEETCODE_SESSION\": \"<YOUR_LEETCODE_SESSION_COOKIE>\"\n        }\n      }\n    }\n  }\n}\n```\n\n#### Option 2: Using Command Line Arguments\n\n```json\n{\n  \"mcp\": {\n    \"servers\": {\n      \"leetcode\": {\n        \"type\": \"stdio\",\n        \"command\": \"npx\",\n        \"args\": [\n          \"-y\",\n          \"@jinzcdev/leetcode-mcp-server\",\n          \"--site\",\n          \"global\",\n          \"--session\",\n          \"<YOUR_LEETCODE_SESSION_COOKIE>\"\n        ]\n      }\n    }\n  }\n}\n```\n\nFor LeetCode China site, modify the `--site` parameter to `cn`.\n\n> [!TIP]\n>\n> The server supports the following optional environment variables:\n>\n> - `LEETCODE_SITE`: LeetCode API endpoint ('global' or 'cn', default: 'global')\n> - `LEETCODE_SESSION`: LeetCode session cookie for authenticated API access (default: empty)\n>\n> **Priority Note**:\n> Command-line arguments take precedence over environment variables when both are specified. For example:\n>\n> - If `LEETCODE_SITE=cn` is set but you run `leetcode-mcp-server --site global`, the server will use `global`.\n> - If `LEETCODE_SESSION` exists but you provide `--session \"new_cookie\"`, the command-line session value will be used.\n\n## Available Tools\n\n### Problems\n\n| Tool                    | Global | CN  | Auth Required | Description                                                  |\n| ----------------------- | :----: | :-: | :-----------: | ------------------------------------------------------------ |\n| **get_daily_challenge** |   ‚úÖ   | ‚úÖ  |      ‚ùå       | Retrieves today's LeetCode Daily Challenge problem           |\n| **get_problem**         |   ‚úÖ   | ‚úÖ  |      ‚ùå       | Retrieves details for a specific LeetCode problem            |\n| **search_problems**     |   ‚úÖ   | ‚úÖ  |      ‚ùå       | Searches for LeetCode problems with multiple filter criteria |\n\n### Users\n\n| Tool                              | Global | CN  | Auth Required | Description                                                  |\n| --------------------------------- | :----: | :-: | :-----------: | ------------------------------------------------------------ |\n| **get_user_profile**              |   ‚úÖ   | ‚úÖ  |      ‚ùå       | Retrieves profile information for a LeetCode user            |\n| **get_user_contest_ranking**      |   ‚úÖ   | ‚úÖ  |      ‚ùå       | Obtains contest ranking statistics for a user                |\n| **get_recent_ac_submissions**     |   ‚úÖ   | ‚úÖ  |      ‚ùå       | Retrieves a user's recent accepted submissions               |\n| **get_recent_submissions**        |   ‚úÖ   | ‚ùå  |      ‚ùå       | Retrieves a user's recent submissions history                |\n| **get_user_status**               |   ‚úÖ   | ‚úÖ  |      ‚úÖ       | Retrieves current user's current status                      |\n| **get_problem_submission_report** |   ‚úÖ   | ‚úÖ  |      ‚úÖ       | Provides detailed submission analysis for a specific problem |\n| **get_problem_progress**          |   ‚úÖ   | ‚úÖ  |      ‚úÖ       | Retrieves current user's problem-solving progress            |\n| **get_all_submissions**           |   ‚úÖ   | ‚úÖ  |      ‚úÖ       | Retrieves current user's submission history                  |\n\n### Notes\n\n| Tool             | Global | CN  | Auth Required | Description                                           |\n| ---------------- | :----: | :-: | :-----------: | ----------------------------------------------------- |\n| **search_notes** |   ‚ùå   | ‚úÖ  |      ‚úÖ       | Searches for user notes with filtering options        |\n| **get_note**     |   ‚ùå   | ‚úÖ  |      ‚úÖ       | Retrieves notes for a specific problem by question ID |\n| **create_note**  |   ‚ùå   | ‚úÖ  |      ‚úÖ       | Creates a new note for a specific problem             |\n| **update_note**  |   ‚ùå   | ‚úÖ  |      ‚úÖ       | Updates an existing note with new content             |\n\n### Solutions\n\n| Tool                       | Global | CN  | Auth Required | Description                                                    |\n| -------------------------- | :----: | :-: | :-----------: | -------------------------------------------------------------- |\n| **list_problem_solutions** |   ‚úÖ   | ‚úÖ  |      ‚ùå       | Retrieves a list of community solutions for a specific problem |\n| **get_problem_solution**   |   ‚úÖ   | ‚úÖ  |      ‚ùå       | Retrieves the complete content of a specific solution          |\n\n## Tool Parameters\n\n### Problems\n\n- **get_daily_challenge** - Retrieves today's LeetCode Daily Challenge problem with complete details\n\n  - No parameters required\n\n- **get_problem** - Retrieves details about a specific LeetCode problem\n\n  - `titleSlug`: The URL slug/identifier of the problem (string, required)\n\n- **search_problems** - Searches for LeetCode problems based on multiple filter criteria\n  - `category`: Problem category filter (string, optional, default: \"all-code-essentials\")\n  - `tags`: List of topic tags to filter problems by (string[], optional)\n  - `difficulty`: Problem difficulty level filter (enum: \"EASY\", \"MEDIUM\", \"HARD\", optional)\n  - `searchKeywords`: Keywords to search in problem titles and descriptions (string, optional)\n  - `limit`: Maximum number of problems to return (number, optional, default: 10)\n  - `offset`: Number of problems to skip (number, optional)\n\n### Users\n\n- **get_user_profile** - Retrieves profile information about a LeetCode user\n\n  - `username`: LeetCode username (string, required)\n\n- **get_user_contest_ranking** - Retrieves a user's contest ranking information\n\n  - `username`: LeetCode username (string, required)\n  - `attended`: Whether to include only the contests the user has participated in (boolean, optional, default: true)\n\n- **get_recent_submissions** - Retrieves a user's recent submissions on LeetCode Global\n\n  - `username`: LeetCode username (string, required)\n  - `limit`: Maximum number of submissions to return (number, optional, default: 10)\n\n- **get_recent_ac_submissions** - Retrieves a user's recent accepted submissions\n\n  - `username`: LeetCode username (string, required)\n  - `limit`: Maximum number of submissions to return (number, optional, default: 10)\n\n- **get_user_status** - Retrieves the current user's status\n\n  - No parameters required\n\n- **get_problem_submission_report** - Retrieves detailed information about a specific submission\n\n  - `id`: The numerical submission ID (number, required)\n\n- **get_problem_progress** - Retrieves the current user's problem-solving progress\n\n  - `offset`: Number of questions to skip (number, optional, default: 0)\n  - `limit`: Maximum number of questions to return (number, optional, default: 100)\n  - `questionStatus`: Filter by question status (enum: \"ATTEMPTED\", \"SOLVED\", optional)\n  - `difficulty`: Filter by difficulty levels (string[], optional)\n\n- **get_all_submissions** - Retrieves paginated list of user's submissions\n  - `limit`: Maximum number of submissions to return (number, default: 20)\n  - `offset`: Number of submissions to skip (number, default: 0)\n  - `questionSlug`: Optional problem identifier (string, optional)\n  - `lang`: Programming language filter (string, optional, CN only)\n  - `status`: Submission status filter (enum: \"AC\", \"WA\", optional, CN only)\n  - `lastKey`: Pagination token for retrieving next page (string, optional, CN only)\n\n### Notes\n\n- **search_notes** - Searches for user notes on LeetCode China\n\n  - `keyword`: Search term to filter notes (string, optional)\n  - `limit`: Maximum number of notes to return (number, optional, default: 10)\n  - `skip`: Number of notes to skip (number, optional, default: 0)\n  - `orderBy`: Sort order for returned notes (enum: \"ASCENDING\", \"DESCENDING\", optional, default: \"DESCENDING\")\n\n- **get_note** - Retrieves user notes for a specific LeetCode problem\n  - `questionId`: The question ID of the LeetCode problem (string, required)\n  - `limit`: Maximum number of notes to return (number, optional, default: 10)\n  - `skip`: Number of notes to skip (number, optional, default: 0)\n- **create_note** - Creates a new note for a specific LeetCode problem\n\n  - `questionId`: The question ID of the LeetCode problem (string, required)\n  - `content`: The content of the note, supports markdown format (string, required)\n  - `summary`: An optional short summary or title for the note (string, optional)\n\n- **update_note** - Updates an existing note with new content or summary\n  - `noteId`: The ID of the note to update (string, required)\n  - `content`: The new content for the note, supports markdown format (string, required)\n  - `summary`: An optional new short summary or title for the note (string, optional)\n\n### Solutions\n\n- **list_problem_solutions** - Retrieves a list of community solutions for a specific problem\n\n  - `questionSlug`: The URL slug/identifier of the problem (string, required)\n  - `limit`: Maximum number of solutions to return (number, optional, default: 10)\n  - `skip`: Number of solutions to skip (number, optional)\n  - `userInput`: Search term to filter solutions (string, optional)\n  - `tagSlugs`: Array of tag identifiers to filter solutions (string[], optional, default: [])\n  - `orderBy`: Sorting criteria for the returned solutions\n    - Global: enum: \"HOT\", \"MOST_RECENT\", \"MOST_VOTES\", optional, default: \"HOT\"\n    - CN: enum: \"DEFAULT\", \"MOST_UPVOTE\", \"HOT\", \"NEWEST_TO_OLDEST\", \"OLDEST_TO_NEWEST\", optional, default: \"DEFAULT\"\n\n- **get_problem_solution** - Retrieves the complete content of a specific solution\n  - `topicId`: Unique topic ID of the solution (string, required, Global only)\n  - `slug`: Unique slug/identifier of the solution (string, required, CN only)\n\n## Available Resources\n\n| Resource Name          | Global | CN  | Auth Required | Description                                                  |\n| ---------------------- | :----: | :-: | :-----------: | ------------------------------------------------------------ |\n| **problem-categories** |   ‚úÖ   | ‚úÖ  |      ‚ùå       | A list of all problem classification categories              |\n| **problem-tags**       |   ‚úÖ   | ‚úÖ  |      ‚ùå       | A detailed collection of algorithmic and data structure tags |\n| **problem-langs**      |   ‚úÖ   | ‚úÖ  |      ‚ùå       | A complete list of all supported programming languages       |\n| **problem-detail**     |   ‚úÖ   | ‚úÖ  |      ‚ùå       | Provides details about a specific problem                    |\n| **problem-solution**   |   ‚úÖ   | ‚úÖ  |      ‚ùå       | Provides the complete content of a specific solution         |\n\n## Resource URIs\n\n- **problem-categories** - A list of all problem classification categories\n\n  - URI: `categories://problems/all`\n\n- **problem-tags** - A detailed collection of algorithmic and data structure tags\n\n  - URI: `tags://problems/all`\n\n- **problem-langs** - A complete list of all programming languages supported by LeetCode\n\n  - URI: `langs://problems/all`\n\n- **problem-detail** - Provides details about a specific LeetCode problem\n\n  - URI: `problem://{titleSlug}`\n  - Parameters:\n    - `titleSlug`: Problem identifier as it appears in the LeetCode URL\n\n- **problem-solution** - Provides the complete content of a specific solution\n  - Global URI: `solution://{topicId}`\n    - Parameters:\n      - `topicId`: Unique topic ID of the solution\n  - CN URI: `solution://{slug}`\n    - Parameters:\n      - `slug`: Unique slug/identifier of the solution\n\n## Authentication\n\nUser-specific data access requires LeetCode session authentication:\n\n1. Log in to LeetCode ([Global](https://leetcode.com) or [China](https://leetcode.cn) site)\n2. Extract `LEETCODE_SESSION` cookie from browser developer tools\n3. Configure server with `--session` flag or `LEETCODE_SESSION` environment variable\n\n## Response Format\n\nAll tools return JSON-formatted responses with the following structure:\n\n```json\n{\n  \"content\": [\n    {\n      \"type\": \"text\",\n      \"text\": \"JSON_DATA_STRING\"\n    }\n  ]\n}\n```\n\nThe `JSON_DATA_STRING` contains either the requested data or an error message for failed requests.\n\n## License\n\nThis project is licensed under the MIT License.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "agents",
        "llms",
        "leetcode",
        "coding agents",
        "leetcode programming",
        "agents coding"
      ],
      "category": "coding-agents"
    },
    "juehang--vscode-mcp-server": {
      "owner": "juehang",
      "name": "vscode-mcp-server",
      "url": "https://github.com/juehang/vscode-mcp-server",
      "imageUrl": "",
      "description": "A MCP Server that allows AI such as Claude to read from the directory structure in a VS Code workspace, see problems picked up by linter(s) and the language server, read code files, and make edits.",
      "stars": 256,
      "forks": 46,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-10-04T04:39:27Z",
      "readme_content": "# VS Code MCP Server\n\nA Visual Studio Code extension (available on the [Marketplace](https://marketplace.visualstudio.com/items?itemName=JuehangQin.vscode-mcp-server)) that allows Claude and other MCP clients to code directly in VS Code! Inspired by [Serena](https://github.com/oraios/serena), but using VS Code's built-in capabilities. Perfect for extending existing coding agents like Claude Code with VS Code-specific capabilities (symbol search, document outlines) without duplicating tools they already have. Note that this extension uses the streamable HTTP API, not the SSE API.\n\nThis extension can allow for execution of shell commands. This means that there is a potential security risk, so use with caution, and ensure that you trust the MCP client that you are using and that the port is not exposed to anything. Authentication would help, but as the MCP authentication spec is still in flux, this has not been implemented for now.\n\nPRs are welcome!\n\n## Demo Video\nhttps://github.com/user-attachments/assets/20b87dfb-fc39-4710-a910-b9481dde1e90\n\n## Installation\n\n1. Install the extension from the [Marketplace](https://marketplace.visualstudio.com/items?itemName=JuehangQin.vscode-mcp-server) or clone this repository and run `npm install` and `npm run compile` to build it.\n\n## Claude Desktop Configuration\n\nClaude Desktop can be configured to use this extension as an MCP server. To do this, your `claude_desktop_config.json` file should look like this:\n```\n{\n  \"mcpServers\": {\n    \"vscode-mcp-server\": {\n        \"command\": \"npx\",\n        \"args\": [\"mcp-remote@next\", \"http://localhost:3000/mcp\"]\n    }\n\n  }\n}\n```\n\nI also like to use this extension in a Claude project, as it allows me to specify additional instructions for Claude. I find the following prompt to work well:\n```\nYou are working on an existing codebase, which you can access using your tools. These code tools interact with a VS Code workspace.\n\nWORKFLOW ESSENTIALS:\n1. Always start exploration with list_files_code on root directory (.) first\n2. CRITICAL: Run get_diagnostics_code after EVERY set of code changes before completing tasks\n3. For small edits (‚â§10 lines): use replace_lines_code with exact original content\n4. For large changes, new files, or uncertain content: use create_file_code with overwrite=true\n\nEXPLORATION STRATEGY:\n- Start: list_files_code with path='.' (never recursive on root)\n- Understand structure: read key files like package.json, README, main entry points\n- Find symbols: use search_symbols_code for functions/classes, get_document_symbols_code for file overviews\n- Before editing: read_file_code the target file to understand current content\n\nEDITING BEST PRACTICES:\n- Small modifications: replace_lines_code (requires exact original content match)\n- If replace_lines_code fails: read_file_code the target lines, then retry with correct content\n- Large changes: create_file_code with overwrite=true is more reliable\n- After any changes: get_diagnostics_code to check for errors\n\nPLANNING REQUIREMENTS:\nBefore making code modifications, present a comprehensive plan including:\n- Confidence level (1-10) and reasoning\n- Specific tools you'll use and why\n- Files you'll modify and approach (small edits vs complete rewrites)\n- How you'll verify the changes work (diagnostics, testing, etc.)\n\nERROR HANDLING:\n- Let errors happen naturally - don't add unnecessary try/catch blocks\n- For tool failures: follow the specific recovery guidance in each tool's description\n- If uncertain about file content: use read_file_code to verify before making changes\n\nAPPROVAL PROCESS:\nIMPORTANT: Only run code modification tools after presenting a plan and receiving explicit approval. Each change requires separate approval.\n\nDo not add tests unless specifically requested. If you believe testing is important, explain why and let the user decide.\n```\n\nFor context efficiency when exploring codebases, consider adding this to your CLAUDE.md:\n```\n## VS Code Symbol Tools for Context Efficiency\nUse VS Code symbol tools to reduce context consumption:\n- `get_document_symbols_code` for file structure overview instead of reading entire files\n- `search_symbols_code` to find symbols by name across the project\n- `get_symbol_definition_code` for type info and docs without full file context\n- Workflow: get outline ‚Üí search symbols ‚Üí get definitions ‚Üí read implementation only when needed\n```\n\n\n\nThis extension serves as a Model Context Protocol (MCP) server, exposing VS Code's filesystem and editing capabilities to MCP clients.\n\n## Features\n\nThe VS Code MCP Server extension implements an MCP-compliant server that allows AI models and other MCP clients to:\n\n- **List files and directories** in your VS Code workspace\n- **Read file contents** with encoding support and size limits\n- **Search for symbols** across your workspace\n- **Get symbol definitions** and hover information by line and symbol name\n- **Create new files** using VS Code's WorkspaceEdit API\n- **Make line replacements** in files\n- **Check for diagnostics** (errors and warnings) in your workspace\n- **Execute shell commands** in the integrated terminal with shell integration\n- **Toggle the server** on and off via a status bar item\n\nThis extension enables AI assistants and other tools to interact with your VS Code workspace through the standardized MCP protocol.\n\n## How It Works\n\nThe extension creates an MCP server that:\n\n1. Runs locally on a configurable port (when enabled)\n2. Handles MCP protocol requests via HTTP\n3. Exposes VS Code's functionality as MCP tools\n4. Provides a status bar indicator showing server status, which can be clicked to toggle the server on/off\n\n## Supported MCP Tools\n\n### File Tools\n- **list_files_code**: Lists files and directories in your workspace\n  - Parameters:\n    - `path`: The path to list files from\n    - `recursive` (optional): Whether to list files recursively\n\n- **read_file_code**: Reads file contents\n  - Parameters:\n    - `path`: The path to the file to read\n    - `encoding` (optional): File encoding (default: utf-8)\n    - `maxCharacters` (optional): Maximum character count (default: 100,000)\n\n### Edit Tools\n- **create_file_code**: Creates a new file using VS Code's WorkspaceEdit API\n  - Parameters:\n    - `path`: The path to the file to create\n    - `content`: The content to write to the file\n    - `overwrite` (optional): Whether to overwrite if the file exists (default: false)\n    - `ignoreIfExists` (optional): Whether to ignore if the file exists (default: false)\n\n- **replace_lines_code**: Replaces specific lines in a file\n  - Parameters:\n    - `path`: The path to the file to modify\n    - `startLine`: The start line number (1-based, inclusive)\n    - `endLine`: The end line number (1-based, inclusive)\n    - `content`: The new content to replace the lines with\n    - `originalCode`: The original code for validation\n\n### Diagnostics Tools\n- **get_diagnostics_code**: Checks for warnings and errors in your workspace\n  - Parameters:\n    - `path` (optional): File path to check (if not provided, checks the entire workspace)\n    - `severities` (optional): Array of severity levels to include (0=Error, 1=Warning, 2=Information, 3=Hint). Default: [0, 1]\n    - `format` (optional): Output format ('text' or 'json'). Default: 'text'\n    - `includeSource` (optional): Whether to include the diagnostic source. Default: true\n\n  This tool is particularly useful for:\n  - Code quality checks before committing changes\n  - Verifying fixes resolved all reported issues\n  - Identifying problems in specific files or the entire workspace\n\n### Symbol Tools\n- **search_symbols_code**: Searches for symbols across the workspace\n  - Parameters:\n    - `query`: The search query for symbol names\n    - `maxResults` (optional): Maximum number of results to return (default: 10)\n  \n  This tool is useful for:\n  - Finding definitions of symbols (functions, classes, variables, etc.) across the codebase\n  - Exploring project structure and organization\n  - Locating specific elements by name\n\n- **get_symbol_definition_code**: Gets definition information for a symbol in a file\n  - Parameters:\n    - `path`: The path to the file containing the symbol\n    - `line`: The line number of the symbol\n    - `symbol`: The symbol name to look for on the specified line\n  \n  This tool provides:\n  - Type information, documentation, and source details for symbols\n  - Code context showing the line where the symbol appears\n  - Symbol range information\n  \n  It's particularly useful for:\n  - Understanding what a symbol represents without navigating away\n  - Checking function signatures, type definitions, or documentation\n  - Quick reference for APIs or library functions\n\n- **get_document_symbols_code**: Gets an outline of all symbols in a file, showing the hierarchical structure\n  - Parameters:\n    - `path`: The path to the file to analyze (relative to workspace)\n    - `maxDepth` (optional): Maximum nesting depth to display\n  \n  This tool provides:\n  - Complete symbol tree for a document (similar to VS Code's Outline view)\n  - Hierarchical structure showing classes, functions, methods, variables, etc.\n  - Position information and symbol kinds for each symbol\n  - Summary statistics by symbol type\n  \n  It's particularly useful for:\n  - Understanding file structure and organization at a glance\n  - Getting an overview of all symbols in a document\n  - Analyzing code architecture and relationships\n  - Finding all symbols of specific types within a file\n\n### Shell Tools\n- **execute_shell_command_code**: Executes a shell command in the VS Code integrated terminal with shell integration\n  - Parameters:\n    - `command`: The shell command to execute\n    - `cwd` (optional): Optional working directory for the command (default: '.')\n\n  This tool is useful for:\n  - Running CLI commands and build operations\n  - Executing git commands\n  - Performing any shell operations that require terminal access\n  - Getting command output for analysis and further processing\n\n## Caveats/TODO\n\nCurrently, only one workspace is supported. The extension also only works locally, to avoid exposing your VS Code instance to any network you may be connected to.\n\n## Extension Settings\n\n* `vscode-mcp-server.port`: The port number for the MCP server (default: 3000)\n* `vscode-mcp-server.host`: Host address for the MCP server (default: 127.0.0.1)\n* `vscode-mcp-server.defaultEnabled`: Whether the MCP server should be enabled by default on VS Code startup\n* `vscode-mcp-server.enabledTools`: Configure which tool categories are enabled (file, edit, shell, diagnostics, symbol)\n\n**Selective Tool Configuration**: Useful for coding agents that already have certain capabilities. For example, with Claude Code you might disable file/edit tools and only enable symbol tools to add VS Code-specific symbol searching without tool duplication.\n\n## Using with MCP Clients\n\nTo connect MCP clients to this server, configure them to use:\n```\nhttp://localhost:3000/mcp\n```\n\nOr if you've configured a custom host:\n```\nhttp://[your-host]:3000/mcp\n```\n\nRemember that you need to enable the server first by clicking on the status bar item!\n\n## Contributing\n\nContributions are welcome! Feel free to submit issues or pull requests.\n\n## License\n\n[MIT](LICENSE)\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "vscode",
        "coding",
        "agents",
        "coding agents",
        "vscode mcp",
        "agents coding"
      ],
      "category": "coding-agents"
    },
    "micl2e2--code-to-tree": {
      "owner": "micl2e2",
      "name": "code-to-tree",
      "url": "https://github.com/micl2e2/code-to-tree",
      "imageUrl": "",
      "description": "A single-binary MCP server that converts source code into AST, regardless of language.",
      "stars": 57,
      "forks": 7,
      "license": "MIT License",
      "language": "C",
      "updated_at": "2025-10-02T13:06:27Z",
      "readme_content": "# Table of Contents\n\n-   [MCP Server: code-to-tree](#orgf542482)\n-   [Using code-to-tree](#org862e7dc)\n-   [Configure MCP Clients](#orge54fa87)\n-   [Building (Windows)](#org48a8180)\n-   [Building (macOS)](#orgbaa740e)\n\n\n\n<a id=\"orgf542482\"></a>\n\n# MCP Server: code-to-tree\n\nThe code-to-tree server's goals are:\n\n1.  Give LLMs the capability of **accurately** converting source code into\n    AST(Abstract Syntax Tree), regardless of language.\n2.  One **standalone** binary should be everything the MCP client needs.\n\nThese goals imply:\n\n1.  The underlying syntax parser should be **versatile** enough. Here we\n    choose [tree-sitter](https://github.com/tree-sitter/tree-sitter), and languages are: C, C++, Rust, Ruby, Go, Java, Python.\n2.  The server should be able to carry all capabilities within\n    itself, imposing **minimum** software dependencies on the end user's\n    machine. Here we choose [mcpc](https://github.com/micl2e2/mcpc).\n\n**Screenshots:**\n\n\n\nThe above screenshots are obtained by asking the question specified\nin `q.md`. \n\n(**IMPORTANT NOTE**: LLMs have no responsibility of generating the identical\nresult for the same question,  you will likely get a completely different\nstyle or content. The screenshots or questions provided here are just for the reference)\n\n\n<a id=\"org862e7dc\"></a>\n\n# Using code-to-tree\n\nBefore everthing, you need to have the code-to-tree executable on your\nmachine (`code-to-tree.exe` for Windows, `code-to-tree` for macOS),\nyou can download at GitHub release [page](https://github.com/micl2e2/code-to-tree/releases) or build it yourself. Once\ndownloaded, you configure your MCP clients to install it, check the section\n*\"Configure MCP Clients\"* for more details.\n\n\n<a id=\"orge54fa87\"></a>\n\n# Configure MCP Clients\n\nHere we use Claude as the example.\n\n\n## Windows\n\nIn your Claude configuration\n(`C:\\Users\\YOUR_NAME\\AppData\\Roaming\\Claude\\claude_desktop_config.json`),\nspecify the location of `code-to-tree.exe`:\n\n    {\n        \"mcpServers\": {\n    \t    \"code-to-tree\": { \"command\": \"C:\\\\path\\\\to\\\\code-to-tree.exe\" }\n        }\n    }\n\n\n## macOS\n\nIn your Claude configuration,\n(`~/Library/Application Support/Claude/claude_desktop_config.json`)\nspecify the location of `code-to-tree`\n\n    {\n        \"mcpServers\": {\n    \t    \"code-to-tree\": { \"command\": \"/path/to/code-to-tree\" }\n        }\n    }\n\n\n<a id=\"org48a8180\"></a>\n\n# Building (Windows)\n\n\n## 1. Prepare environment\n\n1.  download & install MSYS2.\n2.  open application \"MSYS2 MINGW64\"\n3.  run `pacman -S make gcc git`\n\n\n## 2. Prepare tree-sitter libraries\n\nHere we need to compile and install tree-sitter and all related grammars.\n\nClone them:\n\n    git clone https://github.com/tree-sitter/tree-sitter\n    \n    git clone https://github.com/tree-sitter/tree-sitter-c\n    \n    git clone https://github.com/tree-sitter/tree-sitter-cpp\n    \n    git clone https://github.com/tree-sitter/tree-sitter-rust\n    \n    git clone https://github.com/tree-sitter/tree-sitter-ruby\n    \n    git clone https://github.com/tree-sitter/tree-sitter-go\n    \n    git clone https://github.com/tree-sitter/tree-sitter-java\n\nCompile and install them:\n\n    cd tree-sitter && OS=1 make install\n    \n    cd tree-sitter-c && OS=1 make install\n    \n    cd tree-sitter-cpp && OS=1 make install\n    \n    cd tree-sitter-rust && OS=1 make install\n    \n    cd tree-sitter-ruby && OS=1 make install\n    \n    cd tree-sitter-go && OS=1 make install\n    \n    cd tree-sitter-java && OS=1 make install\n\n\n## 3. Build code-to-tree\n\nInstall mcpc:\n\n    git clone https://github.com/micl2e2/mcpc\n    cd mcpc && make install\n\nCompile code-to-tree:\n\n    cd mcpc/example/code-to-tree\n    \n    CFLAGS=\"-I/usr/local/include -L/usr/local/lib\" make\n    \n    # Check the binary\n    file code-to-tree.exe\n    \n    # Remember the binary's location\n    pwd\n    # Assume the output is: /c/path/to/code-to-tree.exe\n\n\n<a id=\"orgbaa740e\"></a>\n\n# Building (macOS)\n\n\n## 1. Prepare environment\n\n1.  Xcode Command Line Tools\n\n\n## 2. Prepare tree-sitter libraries\n\nHere we need to compile and install tree-sitter and all related grammars.\n\nClone them:\n\n    git clone https://github.com/tree-sitter/tree-sitter\n    \n    git clone https://github.com/tree-sitter/tree-sitter-c\n    \n    git clone https://github.com/tree-sitter/tree-sitter-cpp\n    \n    git clone https://github.com/tree-sitter/tree-sitter-rust\n    \n    git clone https://github.com/tree-sitter/tree-sitter-ruby\n    \n    git clone https://github.com/tree-sitter/tree-sitter-go\n    \n    git clone https://github.com/tree-sitter/tree-sitter-java\n\nCompile and install them:\n\n    cd tree-sitter && make install\n    \n    cd tree-sitter-c && make install\n    \n    cd tree-sitter-cpp && make install\n    \n    cd tree-sitter-rust && make install\n    \n    cd tree-sitter-ruby && make install\n    \n    cd tree-sitter-go && make install\n    \n    cd tree-sitter-java && make install\n\n\n## 3. Build code-to-tree\n\nInstall mcpc:\n\n    git clone https://github.com/micl2e2/mcpc\n    cd mcpc && make install\n\nCompile code-to-tree:\n\n    cd mcpc/example/code-to-tree\n    \n    make\n    \n    # Check the binary\n    file ./code-to-tree\n    \n    # Remember the binary's location\n    pwd\n    # Assume the output is: /path/to/code-to-tree",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "coding",
        "programming",
        "agents",
        "coding agents",
        "agents coding",
        "micl2e2 code"
      ],
      "category": "coding-agents"
    },
    "nesquikm--mcp-rubber-duck": {
      "owner": "nesquikm",
      "name": "mcp-rubber-duck",
      "url": "https://github.com/nesquikm/mcp-rubber-duck",
      "imageUrl": "",
      "description": "An MCP server that bridges to multiple OpenAI-compatible LLMs - your AI rubber duck debugging panel for explaining problems to various AI \"ducks\" and getting different perspectives",
      "stars": 54,
      "forks": 8,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-09-23T05:53:00Z",
      "readme_content": "# ü¶Ü MCP Rubber Duck\n\nAn MCP (Model Context Protocol) server that acts as a bridge to query multiple OpenAI-compatible LLMs. Just like rubber duck debugging, explain your problems to various AI \"ducks\" and get different perspectives!\n\n[![npm version](https://badge.fury.io/js/mcp-rubber-duck.svg)](https://www.npmjs.com/package/mcp-rubber-duck)\n[![Docker Image](https://img.shields.io/badge/docker-ghcr.io-blue)](https://github.com/nesquikm/mcp-rubber-duck/pkgs/container/mcp-rubber-duck)\n[![MCP Registry](https://img.shields.io/badge/MCP-Registry-green)](https://modelcontextprotocol.io/registry)\n\n```\n     __\n   <(o )___\n    ( ._> /\n     `---'  Quack! Ready to debug!\n```\n\n## Features\n\n- üîå **Universal OpenAI Compatibility**: Works with any OpenAI-compatible API endpoint\n- ü¶Ü **Multiple Ducks**: Configure and query multiple LLM providers simultaneously  \n- üí¨ **Conversation Management**: Maintain context across multiple messages\n- üèõÔ∏è **Duck Council**: Get responses from all your configured LLMs at once\n- üíæ **Response Caching**: Avoid duplicate API calls with intelligent caching\n- üîÑ **Automatic Failover**: Falls back to other providers if primary fails\n- üìä **Health Monitoring**: Real-time health checks for all providers\n- üîó **MCP Bridge**: Connect ducks to other MCP servers for extended functionality\n- üõ°Ô∏è **Granular Security**: Per-server approval controls with session-based approvals\n- üé® **Fun Duck Theme**: Rubber duck debugging with personality!\n\n## Supported Providers\n\nAny provider with an OpenAI-compatible API endpoint, including:\n\n- **OpenAI** (GPT-4, GPT-3.5)\n- **Google Gemini** (Gemini 2.5 Flash, Gemini 2.0 Flash)\n- **Anthropic** (via OpenAI-compatible endpoints)\n- **Groq** (Llama, Mixtral, Gemma)\n- **Together AI** (Llama, Mixtral, and more)\n- **Perplexity** (Online models with web search)\n- **Anyscale** (Open source models)\n- **Azure OpenAI** (Microsoft-hosted OpenAI)\n- **Ollama** (Local models)\n- **LM Studio** (Local models)\n- **Custom** (Any OpenAI-compatible endpoint)\n\n## Quick Start\n\n### For Claude Desktop Users\nüëâ **Complete Claude Desktop setup instructions below in [Claude Desktop Configuration](#claude-desktop-configuration)**\n\n## Installation\n\n### Prerequisites\n\n- Node.js 20 or higher\n- npm or yarn\n- At least one API key for a supported provider\n\n### Installation Methods\n\n#### Option 1: Install from NPM\n\n```bash\nnpm install -g mcp-rubber-duck\n```\n\n#### Option 2: Install from Source\n\n```bash\n# Clone the repository\ngit clone https://github.com/nesquikm/mcp-rubber-duck.git\ncd mcp-rubber-duck\n\n# Install dependencies\nnpm install\n\n# Build the project\nnpm run build\n\n# Run the server\nnpm start\n```\n\n## Configuration\n\n### Method 1: Environment Variables\n\nCreate a `.env` file in the project root:\n\n```env\n# OpenAI\nOPENAI_API_KEY=sk-...\nOPENAI_DEFAULT_MODEL=gpt-4o-mini  # Optional: defaults to gpt-4o-mini\n\n# Google Gemini\nGEMINI_API_KEY=...\nGEMINI_DEFAULT_MODEL=gemini-2.5-flash  # Optional: defaults to gemini-2.5-flash\n\n# Groq\nGROQ_API_KEY=gsk_...\nGROQ_DEFAULT_MODEL=llama-3.3-70b-versatile  # Optional: defaults to llama-3.3-70b-versatile\n\n# Ollama (Local)\nOLLAMA_BASE_URL=http://localhost:11434/v1  # Optional\nOLLAMA_DEFAULT_MODEL=llama3.2  # Optional: defaults to llama3.2\n\n# Together AI\nTOGETHER_API_KEY=...\n\n# Custom Providers (you can add multiple)\n# Format: CUSTOM_{NAME}_* where NAME becomes the provider key (lowercase)\n\n# Example: Add provider \"myapi\"\nCUSTOM_MYAPI_API_KEY=...\nCUSTOM_MYAPI_BASE_URL=https://api.example.com/v1\nCUSTOM_MYAPI_DEFAULT_MODEL=custom-model  # Optional\nCUSTOM_MYAPI_MODELS=model1,model2        # Optional: comma-separated list\nCUSTOM_MYAPI_NICKNAME=My Custom Duck     # Optional: display name\n\n# Example: Add provider \"azure\" \nCUSTOM_AZURE_API_KEY=...\nCUSTOM_AZURE_BASE_URL=https://mycompany.openai.azure.com/v1\n\n# Global Settings\nDEFAULT_PROVIDER=openai\nDEFAULT_TEMPERATURE=0.7\nLOG_LEVEL=info\n\n# MCP Bridge Settings (Optional)\nMCP_BRIDGE_ENABLED=true                      # Enable ducks to access external MCP servers\nMCP_APPROVAL_MODE=trusted                    # always, trusted, or never\nMCP_APPROVAL_TIMEOUT=300                     # seconds\n\n# MCP Server: Context7 Documentation (Example)\nMCP_SERVER_CONTEXT7_TYPE=http\nMCP_SERVER_CONTEXT7_URL=https://mcp.context7.com/mcp\nMCP_SERVER_CONTEXT7_ENABLED=true\n\n# Per-server trusted tools\nMCP_TRUSTED_TOOLS_CONTEXT7=*                 # Trust all Context7 tools\n\n# Optional: Custom Duck Nicknames (Have fun with these!)\nOPENAI_NICKNAME=\"DUCK-4\"              # Optional: defaults to \"GPT Duck\"\nGEMINI_NICKNAME=\"Duckmini\"            # Optional: defaults to \"Gemini Duck\"\nGROQ_NICKNAME=\"Quackers\"              # Optional: defaults to \"Groq Duck\"\nOLLAMA_NICKNAME=\"Local Quacker\"       # Optional: defaults to \"Local Duck\"\nCUSTOM_NICKNAME=\"My Special Duck\"     # Optional: defaults to \"Custom Duck\"\n```\n\n**Note:** Duck nicknames are completely optional! If you don't set them, you'll get the charming defaults (GPT Duck, Gemini Duck, etc.). If you use a `config.json` file, those nicknames take priority over environment variables.\n\n### Method 2: Configuration File\n\nCreate a `config/config.json` file based on the example:\n\n```bash\ncp config/config.example.json config/config.json\n# Edit config/config.json with your API keys and preferences\n```\n\n## Claude Desktop Configuration\n\nThis is the most common setup method for using MCP Rubber Duck with Claude Desktop.\n\n### Step 1: Build the Project\n\nFirst, ensure the project is built:\n\n```bash\n# Clone the repository\ngit clone https://github.com/nesquikm/mcp-rubber-duck.git\ncd mcp-rubber-duck\n\n# Install dependencies and build\nnpm install\nnpm run build\n```\n\n### Step 2: Configure Claude Desktop\n\nEdit your Claude Desktop config file:\n\n- **macOS**: `~/Library/Application Support/Claude/claude_desktop_config.json`\n- **Windows**: `%APPDATA%\\Claude\\claude_desktop_config.json`\n\nAdd the MCP server configuration:\n\n```json\n{\n  \"mcpServers\": {\n    \"rubber-duck\": {\n      \"command\": \"node\",\n      \"args\": [\"/absolute/path/to/mcp-rubber-duck/dist/index.js\"],\n      \"env\": {\n        \"MCP_SERVER\": \"true\",\n        \"OPENAI_API_KEY\": \"your-openai-api-key-here\",\n        \"OPENAI_DEFAULT_MODEL\": \"gpt-4o-mini\",\n        \"GEMINI_API_KEY\": \"your-gemini-api-key-here\", \n        \"GEMINI_DEFAULT_MODEL\": \"gemini-2.5-flash\",\n        \"DEFAULT_PROVIDER\": \"openai\",\n        \"LOG_LEVEL\": \"info\"\n      }\n    }\n  }\n}\n```\n\n**Important**: Replace the placeholder API keys with your actual keys:\n- `your-openai-api-key-here` ‚Üí Your OpenAI API key (starts with `sk-`)\n- `your-gemini-api-key-here` ‚Üí Your Gemini API key from [Google AI Studio](https://aistudio.google.com/apikey)\n\n**Note**: `MCP_SERVER: \"true\"` is required - this tells rubber-duck to run as an MCP server for any MCP client (not related to the MCP Bridge feature).\n\n### Step 3: Restart Claude Desktop\n\n1. Completely quit Claude Desktop (‚åò+Q on Mac)\n2. Launch Claude Desktop again\n3. The MCP server should connect automatically\n\n### Step 4: Test the Integration\n\nOnce restarted, test these commands in Claude:\n\n#### Check Duck Health\n```\nUse the list_ducks tool with check_health: true\n```\nShould show:\n- ‚úÖ **GPT Duck** (openai) - Healthy\n- ‚úÖ **Gemini Duck** (gemini) - Healthy\n\n#### List Available Models\n```\nUse the list_models tool\n```\n\n#### Ask a Specific Duck\n```\nUse the ask_duck tool with prompt: \"What is rubber duck debugging?\", provider: \"openai\"\n```\n\n#### Compare Multiple Ducks\n```\nUse the compare_ducks tool with prompt: \"Explain async/await in JavaScript\"\n```\n\n#### Test Specific Models\n```\nUse the ask_duck tool with prompt: \"Hello\", provider: \"openai\", model: \"gpt-4\"\n```\n\n### Troubleshooting Claude Desktop Setup\n\n#### If Tools Don't Appear\n1. **Check API Keys**: Ensure your API keys are correctly entered without typos\n2. **Verify Build**: Run `ls -la dist/index.js` to confirm the project built successfully  \n3. **Check Logs**: Look for errors in Claude Desktop's developer console\n4. **Restart**: Fully quit and restart Claude Desktop after config changes\n\n#### Connection Issues\n1. **Config File Path**: Double-check you're editing the correct config file path\n2. **JSON Syntax**: Validate your JSON syntax (no trailing commas, proper quotes)\n3. **Absolute Paths**: Ensure you're using the full absolute path to `dist/index.js`\n4. **File Permissions**: Verify Claude Desktop can read the dist directory\n\n#### Health Check Failures\nIf ducks show as unhealthy:\n1. **API Keys**: Verify keys are valid and have sufficient credits/quota\n2. **Network**: Check internet connection and firewall settings\n3. **Rate Limits**: Some providers have strict rate limits for new accounts\n\n## MCP Bridge - Connect to Other MCP Servers\n\nThe MCP Bridge allows your ducks to access tools from other MCP servers, extending their capabilities beyond just chat. Your ducks can now search documentation, access files, query APIs, and much more!\n\n**Note**: This is different from the MCP server integration above:\n- **MCP Bridge** (`MCP_BRIDGE_ENABLED`): Ducks USE external MCP servers as clients\n- **MCP Server** (`MCP_SERVER`): Rubber-duck SERVES as an MCP server to any MCP client\n\n### Quick Setup\n\nAdd these environment variables to enable MCP Bridge:\n\n```bash\n# Basic MCP Bridge Configuration\nMCP_BRIDGE_ENABLED=\"true\"                # Enable ducks to access external MCP servers\nMCP_APPROVAL_MODE=\"trusted\"              # always, trusted, or never\nMCP_APPROVAL_TIMEOUT=\"300\"               # 5 minutes\n\n# Example: Context7 Documentation Server\nMCP_SERVER_CONTEXT7_TYPE=\"http\"\nMCP_SERVER_CONTEXT7_URL=\"https://mcp.context7.com/mcp\"\nMCP_SERVER_CONTEXT7_ENABLED=\"true\"\n\n# Trust all Context7 tools (no approval needed)\nMCP_TRUSTED_TOOLS_CONTEXT7=\"*\"\n```\n\n### Approval Modes\n\n**`always`**: Every tool call requires approval (with session-based memory)\n- First use of a tool ‚Üí requires approval\n- Subsequent uses of the same tool ‚Üí automatic (until restart)\n\n**`trusted`**: Only untrusted tools require approval\n- Tools in trusted lists execute immediately\n- Unknown tools require approval\n\n**`never`**: All tools execute immediately (use with caution)\n\n### Per-Server Trusted Tools\n\nConfigure trust levels per MCP server for granular security:\n\n```bash\n# Trust all tools from Context7 (documentation server)\nMCP_TRUSTED_TOOLS_CONTEXT7=\"*\"\n\n# Trust specific filesystem operations only\nMCP_TRUSTED_TOOLS_FILESYSTEM=\"read-file,list-directory\"\n\n# Trust specific GitHub tools\nMCP_TRUSTED_TOOLS_GITHUB=\"get-repo-info,list-issues\"\n\n# Global fallback for servers without specific config\nMCP_TRUSTED_TOOLS=\"common-safe-tool\"\n```\n\n### MCP Server Configuration\n\nConfigure MCP servers using environment variables:\n\n#### HTTP Servers\n```bash\nMCP_SERVER_{NAME}_TYPE=\"http\"\nMCP_SERVER_{NAME}_URL=\"https://api.example.com/mcp\"\nMCP_SERVER_{NAME}_API_KEY=\"your-api-key\"        # Optional\nMCP_SERVER_{NAME}_ENABLED=\"true\"\n```\n\n#### STDIO Servers  \n```bash\nMCP_SERVER_{NAME}_TYPE=\"stdio\"\nMCP_SERVER_{NAME}_COMMAND=\"python\"\nMCP_SERVER_{NAME}_ARGS=\"/path/to/script.py,--arg1,--arg2\"\nMCP_SERVER_{NAME}_ENABLED=\"true\"\n```\n\n### Example: Enable Context7 Documentation\n\n```bash\n# Enable MCP Bridge\nMCP_BRIDGE_ENABLED=\"true\"\nMCP_APPROVAL_MODE=\"trusted\"\n\n# Configure Context7 server\nMCP_SERVER_CONTEXT7_TYPE=\"http\"\nMCP_SERVER_CONTEXT7_URL=\"https://mcp.context7.com/mcp\"\nMCP_SERVER_CONTEXT7_ENABLED=\"true\"\n\n# Trust all Context7 tools\nMCP_TRUSTED_TOOLS_CONTEXT7=\"*\"\n```\n\nNow your ducks can search and retrieve documentation from Context7:\n\n```\nAsk: \"Can you find React hooks documentation from Context7 and return only the key concepts?\"\nDuck: *searches Context7 and returns focused, essential React hooks information*\n```\n\n### üí° Token Optimization Benefits\n\n**Smart Token Management**: Ducks can retrieve comprehensive data from MCP servers but return only the essential information you need, saving tokens in your host LLM conversations:\n\n- **Ask for specifics**: \"Find TypeScript interfaces documentation and return only the core concepts\"\n- **Duck processes full docs**: Accesses complete documentation from Context7\n- **Returns condensed results**: Provides focused, relevant information while filtering out unnecessary details\n- **Token savings**: Reduces response size by 70-90% compared to raw documentation dumps\n\n**Example Workflow:**\n```\nYou: \"Find Express.js routing concepts from Context7, keep it concise\"\nDuck: *Retrieves full Express docs, processes, and returns only routing essentials*\nResult: 500 tokens instead of 5,000+ tokens of raw documentation\n```\n\n### Session-Based Approvals\n\nWhen using `always` mode, the system remembers your approvals:\n\n1. **First time**: \"Duck wants to use `search-docs` - Approve? ‚úÖ\"\n2. **Next time**: Duck uses `search-docs` automatically (no new approval needed)\n3. **Different tool**: \"Duck wants to use `get-examples` - Approve? ‚úÖ\"  \n4. **Restart**: Session memory clears, start over\n\nThis eliminates approval fatigue while maintaining security!\n\n### Available Tools (Enhanced with MCP)\n\n### ü¶Ü ask_duck\nAsk a single question to a specific LLM provider. When MCP Bridge is enabled, ducks can automatically access tools from connected MCP servers.\n\n```typescript\n{\n  \"prompt\": \"What is rubber duck debugging?\",\n  \"provider\": \"openai\",  // Optional, uses default if not specified\n  \"temperature\": 0.7     // Optional\n}\n```\n\n### üí¨ chat_with_duck\nHave a conversation with context maintained across messages.\n\n```typescript\n{\n  \"conversation_id\": \"debug-session-1\",\n  \"message\": \"Can you help me debug this code?\",\n  \"provider\": \"groq\"  // Optional, can switch providers mid-conversation\n}\n```\n\n### üßπ clear_conversations\nClear all conversation history and start fresh. Useful when switching topics or when context becomes too large.\n\n```typescript\n{\n  // No parameters required\n}\n```\n\n### üìã list_ducks\nList all configured providers and their health status.\n\n```typescript\n{\n  \"check_health\": true  // Optional, performs fresh health check\n}\n```\n\n### üìä list_models\nList available models for LLM providers.\n\n```typescript\n{\n  \"provider\": \"openai\",     // Optional, lists all if not specified\n  \"fetch_latest\": false     // Optional, fetch latest from API vs cached\n}\n```\n\n### üîç compare_ducks\nAsk the same question to multiple providers simultaneously.\n\n```typescript\n{\n  \"prompt\": \"What's the best programming language?\",\n  \"providers\": [\"openai\", \"groq\", \"ollama\"]  // Optional, uses all if not specified\n}\n```\n\n### üèõÔ∏è duck_council\nGet responses from all configured ducks - like a panel discussion!\n\n```typescript\n{\n  \"prompt\": \"How should I architect a microservices application?\"\n}\n```\n\n## Usage Examples\n\n### Basic Query\n```javascript\n// Ask the default duck\nawait ask_duck({ \n  prompt: \"Explain async/await in JavaScript\" \n});\n```\n\n### Conversation\n```javascript\n// Start a conversation\nawait chat_with_duck({\n  conversation_id: \"learning-session\",\n  message: \"What is TypeScript?\"\n});\n\n// Continue the conversation\nawait chat_with_duck({\n  conversation_id: \"learning-session\", \n  message: \"How does it differ from JavaScript?\"\n});\n```\n\n### Compare Responses\n```javascript\n// Get different perspectives\nawait compare_ducks({\n  prompt: \"What's the best way to handle errors in Node.js?\",\n  providers: [\"openai\", \"groq\", \"ollama\"]\n});\n```\n\n### Duck Council\n```javascript\n// Convene the council for important decisions\nawait duck_council({\n  prompt: \"Should I use REST or GraphQL for my API?\"\n});\n```\n\n## Provider-Specific Setup\n\n### Ollama (Local)\n```bash\n# Install Ollama\ncurl -fsSL https://ollama.ai/install.sh | sh\n\n# Pull a model\nollama pull llama3.2\n\n# Ollama automatically provides OpenAI-compatible endpoint at localhost:11434/v1\n```\n\n### LM Studio (Local)\n1. Download LM Studio from https://lmstudio.ai/\n2. Load a model in LM Studio\n3. Start the local server (provides OpenAI-compatible endpoint at localhost:1234/v1)\n\n### Google Gemini\n1. Get API key from [Google AI Studio](https://aistudio.google.com/apikey)\n2. Add to environment: `GEMINI_API_KEY=...`\n3. Uses OpenAI-compatible endpoint (beta)\n\n### Groq\n1. Get API key from https://console.groq.com/keys\n2. Add to environment: `GROQ_API_KEY=gsk_...`\n\n### Together AI\n1. Get API key from https://api.together.xyz/\n2. Add to environment: `TOGETHER_API_KEY=...`\n\n## Verifying OpenAI Compatibility\n\nTo check if a provider is OpenAI-compatible:\n\n1. Look for `/v1/chat/completions` endpoint in their API docs\n2. Check if they support the OpenAI SDK\n3. Test with curl:\n\n```bash\ncurl -X POST \"https://api.provider.com/v1/chat/completions\" \\\n  -H \"Authorization: Bearer YOUR_API_KEY\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"model-name\",\n    \"messages\": [{\"role\": \"user\", \"content\": \"Hello\"}]\n  }'\n```\n\n## Development\n\n### Run in Development Mode\n```bash\nnpm run dev\n```\n\n### Run Tests\n```bash\nnpm test\n```\n\n### Lint Code\n```bash\nnpm run lint\n```\n\n### Type Checking\n```bash\nnpm run typecheck\n```\n\n## Docker Support\n\nMCP Rubber Duck provides multi-platform Docker support, working on **macOS (Intel & Apple Silicon)**, **Linux (x86_64 & ARM64)**, **Windows (WSL2)**, and **Raspberry Pi 3+**.\n\n### Quick Start with Pre-built Image\n\nThe easiest way to get started is with our pre-built multi-architecture image:\n\n```bash\n# Pull the image (works on all platforms)\ndocker pull ghcr.io/nesquikm/mcp-rubber-duck:latest\n\n# Create environment file\ncp .env.template .env\n# Edit .env and add your API keys\n\n# Run with Docker Compose (recommended)\ndocker compose up -d\n```\n\n### Platform-Specific Deployment\n\n#### üñ•Ô∏è Desktop/Server (macOS, Linux, Windows)\n\n```bash\n# Use desktop-optimized settings\n./scripts/deploy.sh --platform desktop\n\n# Or with more resources and local AI\n./scripts/deploy.sh --platform desktop --profile with-ollama\n```\n\n#### ü•ß Raspberry Pi\n\n```bash\n# Use Pi-optimized settings (memory limits, etc.)\n./scripts/deploy.sh --platform pi\n\n# Or copy optimized config directly\ncp .env.pi.example .env\n# Edit .env and add your API keys\ndocker compose up -d\n```\n\n#### üåê Remote Deployment via SSH\n\n```bash\n# Deploy to remote Raspberry Pi\n./scripts/deploy.sh --mode ssh --ssh-host pi@192.168.1.100\n```\n\n### Universal Deployment Script\n\nThe `scripts/deploy.sh` script auto-detects your platform and applies optimal settings:\n\n```bash\n# Auto-detect platform and deploy\n./scripts/deploy.sh\n\n# Options:\n./scripts/deploy.sh --help\n```\n\n**Available options:**\n- `--mode`: `docker` (default), `local`, or `ssh`\n- `--platform`: `pi`, `desktop`, or `auto` (default)\n- `--profile`: `lightweight`, `desktop`, `with-ollama`\n- `--ssh-host`: For remote deployment\n\n### Platform-Specific Configuration\n\n#### Raspberry Pi (Memory-Optimized)\n```env\n# .env.pi.example - Optimized for Pi 3+\nDOCKER_CPU_LIMIT=1.5\nDOCKER_MEMORY_LIMIT=512M\nNODE_OPTIONS=--max-old-space-size=256\n```\n\n#### Desktop/Server (High-Performance)\n```env\n# .env.desktop.example - Optimized for powerful systems\nDOCKER_CPU_LIMIT=4.0\nDOCKER_MEMORY_LIMIT=2G\nNODE_OPTIONS=--max-old-space-size=1024\n```\n\n### Docker Compose Profiles\n\n```bash\n# Default profile (lightweight, good for Pi)\ndocker compose up -d\n\n# Desktop profile (higher resource limits)\ndocker compose --profile desktop up -d\n\n# With local Ollama AI\ndocker compose --profile with-ollama up -d\n```\n\n### Build Multi-Architecture Images\n\nFor developers who want to build and publish their own multi-architecture images:\n\n```bash\n# Build for AMD64 + ARM64\n./scripts/build-multiarch.sh --platforms linux/amd64,linux/arm64\n\n# Build and push to GitHub Container Registry\n./scripts/gh-deploy.sh --public\n```\n\n### Claude Desktop with Remote Docker\n\nConnect Claude Desktop to MCP Rubber Duck running on a remote system:\n\n```json\n{\n  \"mcpServers\": {\n    \"rubber-duck-remote\": {\n      \"command\": \"ssh\",\n      \"args\": [\n        \"user@remote-host\",\n        \"docker exec -i mcp-rubber-duck node /app/dist/index.js\"\n      ]\n    }\n  }\n}\n```\n\n### Platform Compatibility\n\n| Platform | Architecture | Status | Notes |\n|----------|-------------|---------|-------|\n| **macOS Intel** | AMD64 | ‚úÖ Full | Via Docker Desktop |\n| **macOS Apple Silicon** | ARM64 | ‚úÖ Full | Native ARM64 support |\n| **Linux x86_64** | AMD64 | ‚úÖ Full | Direct Docker support |\n| **Linux ARM64** | ARM64 | ‚úÖ Full | Servers, Pi 4+ |\n| **Raspberry Pi 3+** | ARM64 | ‚úÖ Optimized | Memory-limited config |\n| **Windows** | AMD64 | ‚úÖ Full | Via Docker Desktop + WSL2 |\n\n### Manual Docker Commands\n\nIf you prefer not to use docker-compose:\n\n```bash\n# Raspberry Pi\ndocker run -d \\\n  --name mcp-rubber-duck \\\n  --memory=512m --cpus=1.5 \\\n  --env-file .env \\\n  --restart unless-stopped \\\n  ghcr.io/nesquikm/mcp-rubber-duck:latest\n\n# Desktop/Server\ndocker run -d \\\n  --name mcp-rubber-duck \\\n  --memory=2g --cpus=4 \\\n  --env-file .env \\\n  --restart unless-stopped \\\n  ghcr.io/nesquikm/mcp-rubber-duck:latest\n```\n\n## Architecture\n\n```\nmcp-rubber-duck/\n‚îú‚îÄ‚îÄ src/\n‚îÇ   ‚îú‚îÄ‚îÄ server.ts           # MCP server implementation\n‚îÇ   ‚îú‚îÄ‚îÄ config/             # Configuration management\n‚îÇ   ‚îú‚îÄ‚îÄ providers/          # OpenAI client wrapper\n‚îÇ   ‚îú‚îÄ‚îÄ tools/              # MCP tool implementations\n‚îÇ   ‚îú‚îÄ‚îÄ services/           # Health, cache, conversations\n‚îÇ   ‚îî‚îÄ‚îÄ utils/              # Logging, ASCII art\n‚îú‚îÄ‚îÄ config/                 # Configuration examples\n‚îî‚îÄ‚îÄ tests/                  # Test suites\n```\n\n## Troubleshooting\n\n### Provider Not Working\n1. Check API key is correctly set\n2. Verify endpoint URL is correct\n3. Run health check: `list_ducks({ check_health: true })`\n4. Check logs for detailed error messages\n\n### Connection Issues\n- For local providers (Ollama, LM Studio), ensure they're running\n- Check firewall settings for local endpoints\n- Verify network connectivity to cloud providers\n\n### Rate Limiting\n- Enable caching to reduce API calls\n- Configure failover to alternate providers\n- Adjust `max_retries` and `timeout` settings\n\n## Contributing\n\nü¶Ü **Want to help make our duck pond better?** \n\nWe love contributions! Whether you're fixing bugs, adding features, or teaching our ducks new tricks, we'd love to have you join the flock.\n\nCheck out our [Contributing Guide](./CONTRIBUTING.md) to get started. We promise it's more fun than a regular contributing guide - it has ducks! ü¶Ü\n\n**Quick start for contributors:**\n1. Fork the repository\n2. Create a feature branch  \n3. Follow our [conventional commit guidelines](./CONTRIBUTING.md#commit-messages-duck-communication-protocol)\n4. Add tests for new functionality\n5. Submit a pull request\n\n## License\n\nMIT License - see LICENSE file for details\n\n## Acknowledgments\n\n- Inspired by the rubber duck debugging method\n- Built on the Model Context Protocol (MCP)\n- Uses OpenAI SDK for universal compatibility\n\n## üìù Changelog\n\nSee [CHANGELOG.md](./CHANGELOG.md) for a detailed history of changes and releases.\n\n## üì¶ Registry & Directory\n\nMCP Rubber Duck is available through multiple channels:\n\n- **NPM Package**: [npmjs.com/package/mcp-rubber-duck](https://www.npmjs.com/package/mcp-rubber-duck)\n- **Docker Images**: [ghcr.io/nesquikm/mcp-rubber-duck](https://github.com/nesquikm/mcp-rubber-duck/pkgs/container/mcp-rubber-duck)\n- **MCP Registry**: Official MCP server `io.github.nesquikm/rubber-duck`\n- **Glama Directory**: [glama.ai/mcp/servers/@nesquikm/mcp-rubber-duck](https://glama.ai/mcp/servers/@nesquikm/mcp-rubber-duck)\n- **Awesome MCP Servers**: Listed in the [community directory](https://github.com/punkpeye/awesome-mcp-servers)\n\n## Support\n\n- Report issues: https://github.com/nesquikm/mcp-rubber-duck/issues\n- Documentation: https://github.com/nesquikm/mcp-rubber-duck/wiki\n- Discussions: https://github.com/nesquikm/mcp-rubber-duck/discussions\n\n---\n\nü¶Ü **Happy Debugging with your AI Duck Panel!** ü¶Ü",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "agents",
        "ai",
        "openai",
        "coding agents",
        "agents coding",
        "llms ai"
      ],
      "category": "coding-agents"
    },
    "oraios--serena": {
      "owner": "oraios",
      "name": "serena",
      "url": "https://github.com/oraios/serena",
      "imageUrl": "",
      "description": "A fully-featured coding agent that relies on symbolic code operations by using language servers.",
      "stars": 13663,
      "forks": 922,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-10-04T11:58:44Z",
      "readme_content": "<p align=\"center\" style=\"text-align:center\">\n  \n  \n</p>\n\n* :rocket: Serena is a powerful **coding agent toolkit** capable of turning an LLM into a fully-featured agent that works **directly on your codebase**.\n  Unlike most other tools, it is not tied to an LLM, framework or an interface, making it easy to use it in a variety of ways.\n* :wrench: Serena provides essential **semantic code retrieval and editing tools** that are akin to an IDE's capabilities, extracting code entities at the symbol level and exploiting relational structure. When combined with an existing coding agent, these tools greatly enhance (token) efficiency.\n* :free: Serena is **free & open-source**, enhancing the capabilities of LLMs you already have access to free of charge.\n\nYou can think of Serena as providing IDE-like tools to your LLM/coding agent. With it, the agent no longer needs to read entire\nfiles, perform grep-like searches or string replacements to find and edit the right code. Instead, it can use code centered tools like `find_symbol`, `find_referencing_symbols` and `insert_after_symbol`.\n\n<p align=\"center\">\n  <em>Serena is under active development! See the latest updates, upcoming features, and lessons learned to stay up to date.</em>\n</p>\n\n<p align=\"center\">\n  <a href=\"CHANGELOG.md\">\n    <img src=\"https://img.shields.io/badge/Updates-1e293b?style=flat&logo=rss&logoColor=white&labelColor=1e293b\" alt=\"Changelog\" />\n  </a>\n  <a href=\"roadmap.md\">\n    <img src=\"https://img.shields.io/badge/Roadmap-14532d?style=flat&logo=target&logoColor=white&labelColor=14532d\" alt=\"Roadmap\" />\n  </a>\n  <a href=\"lessons_learned.md\">\n    <img src=\"https://img.shields.io/badge/Lessons-Learned-7c4700?style=flat&logo=readthedocs&logoColor=white&labelColor=7c4700\" alt=\"Lessons Learned\" />\n  </a>\n</p>\n\n### LLM Integration\n\nSerena provides the necessary [tools](#list-of-tools) for coding workflows, but an LLM is required to do the actual work,\norchestrating tool use.\n\nFor example, **supercharge the performance of Claude Code** with a [one-line shell command](#claude-code).\n\nIn general, Serena can be integrated with an LLM in several ways:\n\n* by using the **model context protocol (MCP)**.\n  Serena provides an MCP server which integrates with\n    * Claude Code and Claude Desktop,\n    * Terminal-based clients like Codex, Gemini-CLI, Qwen3-Coder, rovodev, OpenHands CLI and others,\n    * IDEs like VSCode, Cursor or IntelliJ,\n    * Extensions like Cline or Roo Code\n    * Local clients like [OpenWebUI](https://docs.openwebui.com/openapi-servers/mcp), [Jan](https://jan.ai/docs/mcp-examples/browser/browserbase#enable-mcp), [Agno](https://docs.agno.com/introduction/playground) and others\n* by using [mcpo to connect it to ChatGPT](docs/serena_on_chatgpt.md) or other clients that don't support MCP but do support tool calling via OpenAPI.\n* by incorporating Serena's tools into an agent framework of your choice, as illustrated [here](docs/custom_agent.md).\n  Serena's tool implementation is decoupled from the framework-specific code and can thus easily be adapted to any agent framework.\n\n### Serena in Action\n\n#### Demonstration 1: Efficient Operation in Claude Code\n\nA demonstration of Serena efficiently retrieving and editing code within Claude Code, thereby saving tokens and time. Efficient operations are not only useful for saving costs, but also for generally improving the generated code's quality. This effect may be less pronounced in very small projects, but often becomes of crucial importance in larger ones.\n\nhttps://github.com/user-attachments/assets/ab78ebe0-f77d-43cc-879a-cc399efefd87\n\n#### Demonstration 2: Serena in Claude Desktop\n\nA demonstration of Serena implementing a small feature for itself (a better log GUI) with Claude Desktop.\nNote how Serena's tools enable Claude to find and edit the right symbols.\n\nhttps://github.com/user-attachments/assets/6eaa9aa1-610d-4723-a2d6-bf1e487ba753\n\n### Programming Language Support & Semantic Analysis Capabilities\n\nSerena's semantic code analysis capabilities build on **language servers** using the widely implemented\nlanguage server protocol (LSP). The LSP provides a set of versatile code querying\nand editing functionalities based on symbolic understanding of the code.\nEquipped with these capabilities, Serena discovers and edits code just like a seasoned developer\nmaking use of an IDE's capabilities would.\nSerena can efficiently find the right context and do the right thing even in very large and\ncomplex projects! So not only is it free and open-source, it frequently achieves better results\nthan existing solutions that charge a premium.\n\nLanguage servers provide support for a wide range of programming languages.\nWith Serena, we provide direct, out-of-the-box support for:\n\n  * Python\n  * TypeScript/Javascript\n  * PHP (uses Intelephense LSP; set `INTELEPHENSE_LICENSE_KEY` environment variable for premium features)\n  * Go (requires installation of gopls)\n  * R (requires installation of the `languageserver` R package)\n  * Rust (requires [rustup](https://rustup.rs/) - uses rust-analyzer from your toolchain)\n  * C/C++ (you may experience issues with finding references, we are working on it)\n  * Zig (requires installation of ZLS - Zig Language Server)\n  * C#\n  * Ruby (by default, uses [ruby-lsp](https://github.com/Shopify/ruby-lsp), specify ruby_solargraph as your language to use the previous solargraph based implementation)\n  * Swift\n  * Kotlin (uses the pre-alpha [official kotlin LS](https://github.com/Kotlin/kotlin-lsp), some issues may appear)\n  * Java (_Note_: startup is slow, initial startup especially so. There may be issues with java on macos and linux, we are working on it.)\n  * Clojure\n  * Dart\n  * Bash\n  * Lua (automatically downloads lua-language-server if not installed)\n  * Nix (requires nixd installation)\n  * Elixir (requires installation of NextLS and Elixir; **Windows not supported**)\n  * Erlang (requires installation of beam and [erlang_ls](https://github.com/erlang-ls/erlang_ls), experimental, might be slow or hang)\n  * AL\n\nSupport for further languages can easily be added by providing a shallow adapter for a new language server implementation,\nsee Serena's [memory on that](.serena/memories/adding_new_language_support_guide.md).\n\n### Community Feedback\n\nMost users report that Serena has strong positive effects on the results of their coding agents, even when used within\nvery capable agents like Claude Code. Serena is often described to be a [game changer](https://www.reddit.com/r/ClaudeAI/comments/1lfsdll/try_out_serena_mcp_thank_me_later/), providing an enormous [productivity boost](https://www.reddit.com/r/ClaudeCode/comments/1mguoia/absolutely_insane_improvement_of_claude_code).\n\nSerena excels at navigating and manipulating complex codebases, providing tools that support precise code retrieval and editing in the presence of large, strongly structured codebases.\nHowever, when dealing with tasks that involve only very few/small files, you may not benefit from including Serena on top of your existing coding agent. \nIn particular, when writing code from scratch, Serena will not provide much value initially, as the more complex structures that Serena handles more gracefully than simplistic, file-based approaches are yet to be created.\n\nSeveral videos and blog posts have talked about Serena:\n\n* YouTube:\n    * [AI Labs](https://www.youtube.com/watch?v=wYWyJNs1HVk&t=1s)\n    * [Yo Van Eyck](https://www.youtube.com/watch?v=UqfxuQKuMo8&t=45s)\n    * [JeredBlu](https://www.youtube.com/watch?v=fzPnM3ySmjE&t=32s)\n\n* Blog posts:\n    * [Serena's Design Principles](https://medium.com/@souradip1000/deconstructing-serenas-mcp-powered-semantic-code-understanding-architecture-75802515d116)\n    * [Serena with Claude Code (in Japanese)](https://blog.lai.so/serena/)\n    * [Turning Claude Code into a Development Powerhouse](https://robertmarshall.dev/blog/turning-claude-code-into-a-development-powerhouse/)\n\n## Table of Contents\n\n<!-- Created with markdown-toc -i README.md -->\n<!-- Install it with npm install -g markdown-toc -->\n\n<!-- toc -->\n\n- [Quick Start](#quick-start)\n  * [Running the Serena MCP Server](#running-the-serena-mcp-server)\n    + [Usage](#usage)\n    + [Using uvx](#using-uvx)\n    + [Local Installation](#local-installation)\n    + [Using Docker (Experimental)](#using-docker-experimental)\n    + [Using Nix](#using-nix)\n    + [Streamable HTTP Mode](#streamable-http-mode)\n    + [Command-Line Arguments](#command-line-arguments)\n  * [Configuration](#configuration)\n  * [Project Activation & Indexing](#project-activation--indexing)\n  * [Claude Code](#claude-code)\n  * [Codex](#codex)\n  * [Other Terminal-Based Clients](#other-terminal-based-clients)\n  * [Claude Desktop](#claude-desktop)\n  * [MCP Coding Clients (Cline, Roo-Code, Cursor, Windsurf, etc.)](#mcp-coding-clients-cline-roo-code-cursor-windsurf-etc)\n  * [Local GUIs and Frameworks](#local-guis-and-frameworks)\n- [Detailed Usage and Recommendations](#detailed-usage-and-recommendations)\n  * [Tool Execution](#tool-execution)\n    + [Shell Execution and Editing Tools](#shell-execution-and-editing-tools)\n  * [Modes and Contexts](#modes-and-contexts)\n    + [Contexts](#contexts)\n    + [Modes](#modes)\n    + [Customization](#customization)\n  * [Onboarding and Memories](#onboarding-and-memories)\n  * [Prepare Your Project](#prepare-your-project)\n    + [Structure Your Codebase](#structure-your-codebase)\n    + [Start from a Clean State](#start-from-a-clean-state)\n    + [Logging, Linting, and Automated Tests](#logging-linting-and-automated-tests)\n  * [Prompting Strategies](#prompting-strategies)\n  * [Running Out of Context](#running-out-of-context)\n  * [Serena's Logs: The Dashboard and GUI Tool](#serenas-logs-the-dashboard-and-gui-tool)\n- [Comparison with Other Coding Agents](#comparison-with-other-coding-agents)\n  * [Subscription-Based Coding Agents](#subscription-based-coding-agents)\n  * [API-Based Coding Agents](#api-based-coding-agents)\n  * [Other MCP-Based Coding Agents](#other-mcp-based-coding-agents)\n- [Acknowledgements](#acknowledgements)\n  * [Sponsors](#sponsors)\n  * [Community Contributions](#community-contributions)\n  * [Technologies](#technologies)\n- [Customizing and Extending Serena](#customizing-and-extending-serena)\n- [List of Tools](#list-of-tools)\n\n<!-- tocstop -->\n\n## Quick Start\n\nSerena can be used in various ways, below you will find instructions for selected integrations.\n\n* For coding with Claude, we recommend using Serena through [Claude Code](#claude-code) or [Claude Desktop](#claude-desktop). You can also use Serena in most other [terminal-based clients](#other-terminal-based-clients).\n* If you want a GUI experience outside an IDE, you can use one of the many [local GUIs](#local-guis-and-frameworks) that support MCP servers.\n  You can also connect Serena to many web clients (including ChatGPT) using [mcpo](docs/serena_on_chatgpt.md).\n* If you want to use Serena integrated in your IDE, see the section on [other MCP clients](#other-mcp-clients---cline-roo-code-cursor-windsurf-etc).\n* You can use Serena as a library for building your own applications. We try to keep the public API stable, but you should still\n  expect breaking changes and pin Serena to a fixed version if you use it as a dependency.\n\nSerena is managed by `uv`, so you will need to [install it](https://docs.astral.sh/uv/getting-started/installation/).\n\n### Running the Serena MCP Server\n\nYou have several options for running the MCP server, which are explained in the subsections below.\n\n#### Usage\n\nThe typical usage involves the client (Claude Code, Claude Desktop, etc.) running\nthe MCP server as a subprocess (using stdio communication),\nso the client needs to be provided with the command to run the MCP server.\n(Alternatively, you can run the MCP server in Streamable HTTP or SSE mode and tell your client\nhow to connect to it.)\n\nNote that no matter how you run the MCP server, Serena will, by default, start a small web-based dashboard on localhost that will display logs and allow shutting down the\nMCP server (since many clients fail to clean up processes correctly).\nThis and other settings can be adjusted in the [configuration](#configuration) and/or by providing [command-line arguments](#command-line-arguments).\n\n#### Using uvx\n\n`uvx` can be used to run the latest version of Serena directly from the repository, without an explicit local installation.\n\n```shell\nuvx --from git+https://github.com/oraios/serena serena start-mcp-server\n```\n\nExplore the CLI to see some of the customization options that serena provides (more info on them below).\n\n#### Local Installation\n\n1. Clone the repository and change into it.\n\n   ```shell\n   git clone https://github.com/oraios/serena\n   cd serena\n   ```\n\n2. Optionally edit the configuration file in your home directory with\n\n   ```shell\n   uv run serena config edit\n   ```\n\n   If you just want the default config, you can skip this part, and a config file will be created when you first run Serena.\n3. Run the server with `uv`:\n\n   ```shell\n   uv run serena start-mcp-server\n   ```\n\n   When running from outside the serena installation directory, be sure to pass it, i.e., use\n\n   ```shell\n    uv run --directory /abs/path/to/serena serena start-mcp-server\n   ```\n\n#### Using Docker (Experimental)\n\n‚ö†Ô∏è Docker support is currently experimental with several limitations. Please read the [Docker documentation](DOCKER.md) for important caveats before using it.\n\nYou can run the Serena MCP server directly via docker as follows,\nassuming that the projects you want to work on are all located in `/path/to/your/projects`:\n\n```shell\ndocker run --rm -i --network host -v /path/to/your/projects:/workspaces/projects ghcr.io/oraios/serena:latest serena start-mcp-server --transport stdio\n```\n\nReplace `/path/to/your/projects` with the absolute path to your projects directory. The Docker approach provides:\n\n* Better security isolation for shell command execution\n* No need to install language servers and dependencies locally\n* Consistent environment across different systems\n\nAlternatively, use docker compose with the `compose.yml` file provided in the repository.\n\nSee the [Docker documentation](DOCKER.md) for detailed setup instructions, configuration options, and known limitations.\n\n#### Using Nix\n\nIf you are using Nix and [have enabled the `nix-command` and `flakes` features](https://nixos.wiki/wiki/flakes), you can run Serena using the following command:\n\n```bash\nnix run github:oraios/serena -- start-mcp-server --transport stdio\n```\n\nYou can also install Serena by referencing this repo (`github:oraios/serena`) and using it in your Nix flake. The package is exported as `serena`.\n\n#### Streamable HTTP Mode\n\n‚ÑπÔ∏è Note that MCP servers which use stdio as a protocol are somewhat unusual as far as client/server architectures go, as the server\nnecessarily has to be started by the client in order for communication to take place via the server's standard input/output stream.\nIn other words, you do not need to start the server yourself. The client application (e.g. Claude Desktop) takes care of this and\ntherefore needs to be configured with a launch command.\n\nWhen using instead the *Streamable HTTP* mode, you control the server lifecycle yourself,\ni.e. you start the server and provide the client with the URL to connect to it.\n\nSimply provide `start-mcp-server` with the `--transport streamable-http` option and optionally provide the port.\nFor example, to run the Serena MCP server in Streamable HTTP mode on port 9121 using a local installation,\nyou would run this command from the Serena directory,\n\n```shell\nuv run serena start-mcp-server --transport streamable-http --port 9121\n```\n\nand then configure your client to connect to `http://localhost:9121/mcp`.\n\n‚ÑπÔ∏è Note that SSE transport is supported as well, but its use is discouraged. \nUse Streamable HTTP instead.\n\n#### Command-Line Arguments\n\nThe Serena MCP server supports a wide range of additional command-line options, including the option to run in Streamable HTTP or SSE mode\nand to adapt Serena to various [contexts and modes of operation](#modes-and-contexts).\n\nRun with parameter `--help` to get a list of available options.\n\n### Configuration\n\nSerena is very flexible in terms of configuration. While for most users, the default configurations will work,\nyou can fully adjust it to your needs by editing a few yaml files. You can disable tools, change Serena's instructions\n(what we denote as the `system_prompt`), adjust the output of tools that just provide a prompt, and even adjust tool descriptions.\n\nSerena is configured in four places:\n\n1. The `serena_config.yml` for general settings that apply to all clients and projects.\n   It is located in your user directory under `.serena/serena_config.yml`.\n   If you do not explicitly create the file, it will be auto-generated when you first run Serena.\n   You can edit it directly or use\n\n   ```shell\n   uvx --from git+https://github.com/oraios/serena serena config edit\n   ```\n\n   (or use the `--directory` command version).\n2. In the arguments passed to the `start-mcp-server` in your client's config (see below),\n   which will apply to all sessions started by the respective client. In particular, the [context](#contexts) parameter\n   should be set appropriately for Serena to be best adjusted to existing tools and capabilities of your client.\n   See for a detailed explanation. You can override all entries from the `serena_config.yml` through command line arguments.\n3. In the `.serena/project.yml` file within your project. This will hold project-level configuration that is used whenever\n   that project is activated. This file will be autogenerated when you first use Serena on that project, but you can also\n   generate it explicitly with\n\n   ```shell\n   uvx --from git+https://github.com/oraios/serena serena project generate-yml\n   ```\n\n   (or use the `--directory` command version).\n4. Through the context and modes. Explore the [modes and contexts](#modes-and-contexts) section for more details.\n\nAfter the initial setup, continue with one of the sections below, depending on how you\nwant to use Serena.\n\n### Project Activation & Indexing\n\nIf you are mostly working with the same project, you can configure to always activate it at startup\nby passing `--project <path_or_name>` to the `start-mcp-server` command in your client's MCP config.\nThis is especially useful for clients which configure MCP servers on a per-project basis, like Claude Code.\n\nOtherwise, the recommended way is to just ask the LLM to activate a project by providing it an absolute path to, or,\nin case the project was activated in the past, by its name. The default project name is the directory name.\n\n* \"Activate the project /path/to/my_project\"\n* \"Activate the project my_project\"\n\nAll projects that have been activated will be automatically added to your `serena_config.yml`, and for each\nproject, the file `.serena/project.yml` will be generated. You can adjust the latter, e.g., by changing the name\n(which you refer to during the activation) or other options. Make sure to not have two different projects with the\nsame name.\n\n‚ÑπÔ∏è For larger projects, we recommend that you index your project to accelerate Serena's tools; otherwise the first\ntool application may be very slow.\nTo do so, run this from the project directory (or pass the path to the project as an argument):\n\n```shell\nuvx --from git+https://github.com/oraios/serena serena project index\n```\n\n(or use the `--directory` command version).\n\n### Claude Code\n\nSerena is a great way to make Claude Code both cheaper and more powerful!\n\nFrom your project directory, add serena with a command like this,\n\n```shell\nclaude mcp add serena -- <serena-mcp-server> --context ide-assistant --project $(pwd)\n```\n\nwhere `<serena-mcp-server>` is your way of [running the Serena MCP server](#running-the-serena-mcp-server).\nFor example, when using `uvx`, you would run\n\n```shell\nclaude mcp add serena -- uvx --from git+https://github.com/oraios/serena serena start-mcp-server --context ide-assistant --project $(pwd)\n```\n\n‚ÑπÔ∏è Serena comes with an instruction text, and Claude needs to read it to properly use Serena's tools.\n  As of version `v1.0.52`, claude code reads the instructions of the MCP server, so this **is handled automatically**.\n  If you are using an older version, or if Claude fails to read the instructions, you can ask it explicitly\n  to \"read Serena's initial instructions\" or run `/mcp__serena__initial_instructions` to load the instruction text.\n  If you want to make use of that, you will have to enable the corresponding tool explicitly by adding `initial_instructions` to the `included_optional_tools`\n  in your config.\n  Note that you may have to make Claude read the instructions when you start a new conversation and after any compacting operation to ensure Claude remains properly configured to use Serena's tools.\n\n### Codex\n\nSerena works with OpenAI's Codex CLI out of the box, but you have to use the `codex` context for it to work properly. (The technical reason is that Codex doesn't fully support the MCP specifications, so some massaging of tools is required.).\n\nUnlike Claude Code, in Codex you add an MCP server globally and not per project. Add the following to\n`~/.codex/config.toml` (create the file if it does not exist):\n\n```toml\n[mcp_servers.serena]\ncommand = \"uvx\"\nargs = [\"--from\", \"git+https://github.com/oraios/serena\", \"serena\", \"start-mcp-server\", \"--context\", \"codex\"]\n```\n\nAfter codex has started, you need to activate the project, which you can do by saying:\n\n\"Activate the current dir as project using serena\"\n\n> If you don't activate the project, you will not be able to use Serena's tools!\n\nThat's it! Have a look at `~/.codex/log/codex-tui.log` to see if any errors occurred.\n\nThe Serena dashboard will run if you have not disabled it in the configuration, but due to Codex's sandboxing the webbrowser\nmay not open automatically. You can open it manually by going to `http://localhost:24282/dashboard/index.html` (or a higher port, if\nthat was already taken).\n\n> Codex will often show the tools as `failed` even though they are successfully executed. This is not a problem, seems to be a bug in Codex. Despite the error message, everything works as expected.\n\n### Other Terminal-Based Clients\n\nThere are many terminal-based coding assistants that support MCP servers, such as [Codex](https://github.com/openai/codex?tab=readme-ov-file#model-context-protocol-mcp),\n[Gemini-CLI](https://github.com/google-gemini/gemini-cli), [Qwen3-Coder](https://github.com/QwenLM/Qwen3-Coder),\n[rovodev](https://community.atlassian.com/forums/Rovo-for-Software-Teams-Beta/Introducing-Rovo-Dev-CLI-AI-Powered-Development-in-your-terminal/ba-p/3043623),\nthe [OpenHands CLI](https://docs.all-hands.dev/usage/how-to/cli-mode) and [opencode](https://github.com/sst/opencode).\n\nThey generally benefit from the symbolic tools provided by Serena. You might want to customize some aspects of Serena\nby writing your own context, modes or prompts to adjust it to your workflow, to other MCP servers you are using, and to\nthe client's internal capabilities.\n\n### Claude Desktop\n\nFor [Claude Desktop](https://claude.ai/download) (available for Windows and macOS), go to File / Settings / Developer / MCP Servers / Edit Config,\nwhich will let you open the JSON file `claude_desktop_config.json`.\nAdd the `serena` MCP server configuration, using a [run command](#running-the-serena-mcp-server) depending on your setup.\n\n* local installation:\n\n   ```json\n   {\n       \"mcpServers\": {\n           \"serena\": {\n               \"command\": \"/abs/path/to/uv\",\n               \"args\": [\"run\", \"--directory\", \"/abs/path/to/serena\", \"serena\", \"start-mcp-server\"]\n           }\n       }\n   }\n   ```\n\n* uvx:\n\n   ```json\n   {\n       \"mcpServers\": {\n           \"serena\": {\n               \"command\": \"/abs/path/to/uvx\",\n               \"args\": [\"--from\", \"git+https://github.com/oraios/serena\", \"serena\", \"start-mcp-server\"]\n           }\n       }\n  }\n  ```\n\n* docker:\n\n  ```json\n   {\n       \"mcpServers\": {\n           \"serena\": {\n               \"command\": \"docker\",\n               \"args\": [\"run\", \"--rm\", \"-i\", \"--network\", \"host\", \"-v\", \"/path/to/your/projects:/workspaces/projects\", \"ghcr.io/oraios/serena:latest\", \"serena\", \"start-mcp-server\", \"--transport\", \"stdio\"]\n           }\n       }\n   }\n   ```\n\nIf you are using paths containing backslashes for paths on Windows\n(note that you can also just use forward slashes), be sure to escape them correctly (`\\\\`).\n\nThat's it! Save the config and then restart Claude Desktop. You are ready for activating your first project.\n\n‚ÑπÔ∏è You can further customize the run command using additional arguments (see [above](#command-line-arguments)).\n\nNote: on Windows and macOS there are official Claude Desktop applications by Anthropic, for Linux there is an [open-source\ncommunity version](https://github.com/aaddrick/claude-desktop-debian).\n\n‚ö†Ô∏è Be sure to fully quit the Claude Desktop application, as closing Claude will just minimize it to the system tray ‚Äì at least on Windows.\n\n‚ö†Ô∏è Some clients may leave behind zombie processes. You will have to find and terminate them manually then.\n    With Serena, you can activate the [dashboard](#serenas-logs-the-dashboard-and-gui-tool) to prevent unnoted processes and also use the dashboard\n    for shutting down Serena.\n\nAfter restarting, you should see Serena's tools in your chat interface (notice the small hammer icon).\n\nFor more information on MCP servers with Claude Desktop, see [the official quick start guide](https://modelcontextprotocol.io/quickstart/user).\n\n### MCP Coding Clients (Cline, Roo-Code, Cursor, Windsurf, etc.)\n\nBeing an MCP Server, Serena can be included in any MCP Client. The same configuration as above,\nperhaps with small client-specific modifications, should work. Most of the popular\nexisting coding assistants (IDE extensions or VSCode-like IDEs) support connections\nto MCP Servers. It is **recommended to use the `ide-assistant` context** for these integrations by adding `\"--context\", \"ide-assistant\"` to the `args` in your MCP client's configuration. Including Serena generally boosts their performance\nby providing them tools for symbolic operations.\n\nIn this case, the billing for the usage continues to be controlled by the client of your choice\n(unlike with the Claude Desktop client). But you may still want to use Serena through such an approach,\ne.g., for one of the following reasons:\n\n1. You are already using a coding assistant (say Cline or Cursor) and just want to make it more powerful.\n2. You are on Linux and don't want to use the [community-created Claude Desktop](https://github.com/aaddrick/claude-desktop-debian).\n3. You want tighter integration of Serena into your IDE and don't mind paying for that.\n\n### Local GUIs and Frameworks\n\nOver the last months, several technologies have emerged that allow you to run a powerful local GUI\nand connect it to an MCP server. They will work with Serena out of the box.\nSome of the leading open source GUI technologies offering this are\n[Jan](https://jan.ai/docs/mcp), [OpenHands](https://github.com/All-Hands-AI/OpenHands/),\n[OpenWebUI](https://docs.openwebui.com/openapi-servers/mcp) and [Agno](https://docs.agno.com/introduction/playground).\nThey allow combining Serena with almost any LLM (including locally running ones) and offer various other integrations.\n\n## Detailed Usage and Recommendations\n\n### Tool Execution\n\nSerena combines tools for semantic code retrieval with editing capabilities and shell execution.\nSerena's behavior can be further customized through [Modes and Contexts](#modes-and-contexts).\nFind the complete list of tools [below](#full-list-of-tools).\n\nThe use of all tools is generally recommended, as this allows Serena to provide the most value:\nOnly by executing shell commands (in particular, tests) can Serena identify and correct mistakes\nautonomously.\n\n#### Shell Execution and Editing Tools\n\nMany clients have their own shell execution tool, and by default Serena's shell tool will be disabled in them\n(e.g., when using the `ide-assistant` or `codex` context). However, when using Serena through something like\nClaude Desktop or ChatGPT, it is recommended to enable Serena's `execute_shell_command` tool to allow\nagentic behavior.\n\nIt should be noted that the `execute_shell_command` tool allows for arbitrary code execution.\nWhen using Serena as an MCP Server, clients will typically ask the user for permission\nbefore executing a tool, so as long as the user inspects execution parameters beforehand,\nthis should not be a problem.\nHowever, if you have concerns, you can choose to disable certain commands in your project's configuration file.\nIf you only want to use Serena purely for analyzing code and suggesting implementations\nwithout modifying the codebase, you can enable read-only mode by setting `read_only: true` in your project configuration file.\nThis will automatically disable all editing tools and prevent any modifications to your codebase while still\nallowing all analysis and exploration capabilities.\n\nIn general, be sure to back up your work and use a version control system in order to avoid\nlosing any work.\n\n### Modes and Contexts\n\nSerena's behavior and toolset can be adjusted using contexts and modes.\nThese allow for a high degree of customization to best suit your workflow and the environment Serena is operating in.\n\n#### Contexts\n\nA context defines the general environment in which Serena is operating.\nIt influences the initial system prompt and the set of available tools.\nA context is set at startup when launching Serena (e.g., via CLI options for an MCP server or in the agent script) and cannot be changed during an active session.\n\nSerena comes with pre-defined contexts:\n\n* `desktop-app`: Tailored for use with desktop applications like Claude Desktop. This is the default.\n* `agent`: Designed for scenarios where Serena acts as a more autonomous agent, for example, when used with Agno.\n* `ide-assistant`: Optimized for integration into IDEs like VSCode, Cursor, or Cline, focusing on in-editor coding assistance.\nChoose the context that best matches the type of integration you are using.\n\nWhen launching Serena, specify the context using `--context <context-name>`.\nNote that for cases where parameter lists are specified (e.g. Claude Desktop), you must add two parameters to the list.\n\nIf you are using a local server (such as Llama.cpp) which requires you to use OpenAI-compatible tool descriptions, use context `oaicompat-agent` instead of `agent`.\n\n#### Modes\n\nModes further refine Serena's behavior for specific types of tasks or interaction styles. Multiple modes can be active simultaneously, allowing you to combine their effects. Modes influence the system prompt and can also alter the set of available tools by excluding certain ones.\n\nExamples of built-in modes include:\n\n* `planning`: Focuses Serena on planning and analysis tasks.\n* `editing`: Optimizes Serena for direct code modification tasks.\n* `interactive`: Suitable for a conversational, back-and-forth interaction style.\n* `one-shot`: Configures Serena for tasks that should be completed in a single response, often used with `planning` for generating reports or initial plans.\n* `no-onboarding`: Skips the initial onboarding process if it's not needed for a particular session.\n* `onboarding`: (Usually triggered automatically) Focuses on the project onboarding process.\n\nModes can be set at startup (similar to contexts) but can also be _switched dynamically_ during a session. You can instruct the LLM to use the `switch_modes` tool to activate a different set of modes (e.g., \"switch to planning and one-shot modes\").\n\nWhen launching Serena, specify modes using `--mode <mode-name>`; multiple modes can be specified, e.g. `--mode planning --mode no-onboarding`.\n\n:warning: **Mode Compatibility**: While you can combine modes, some may be semantically incompatible (e.g., `interactive` and `one-shot`). Serena currently does not prevent incompatible combinations; it is up to the user to choose sensible mode configurations.\n\n#### Customization\n\nYou can create your own contexts and modes to precisely tailor Serena to your needs in two ways:\n\n* You can use Serena's CLI to manage modes and contexts. Check out\n\n    ```shell\n    uvx --from git+https://github.com/oraios/serena serena mode --help\n    ```\n\n    and\n\n    ```shell\n    uvx --from git+https://github.com/oraios/serena serena context --help\n    ```\n\n    _NOTE_: Custom contexts/modes are simply YAML files in `<home>/.serena`, they are automatically registered and available for use by their name (filename without the `.yml` extension). If you don't want to use Serena's CLI, you can create and manage them in any way you see fit.\n* **Using external YAML files**: When starting Serena, you can also provide an absolute path to a custom `.yml` file for a context or mode.\n\nThis customization allows for deep integration and adaptation of Serena to specific project requirements or personal preferences.\n\n### Onboarding and Memories\n\nBy default, Serena will perform an **onboarding process** when\nit is started for the first time for a project.\nThe goal of the onboarding is for Serena to get familiar with the project\nand to store memories, which it can then draw upon in future interactions.\nIf an LLM should fail to complete the onboarding and does not actually write the\nrespective memories to disk, you may need to ask it to do so explicitly.\n\nThe onboarding will usually read a lot of content from the project, thus filling\nup the context. It can therefore be advisable to switch to another conversation\nonce the onboarding is complete.\nAfter the onboarding, we recommend that you have a quick look at the memories and,\nif necessary, edit them or add additional ones.\n\n**Memories** are files stored in `.serena/memories/` in the project directory,\nwhich the agent can choose to read in subsequent interactions.\nFeel free to read and adjust them as needed; you can also add new ones manually.\nEvery file in the `.serena/memories/` directory is a memory file.\nWhenever Serena starts working on a project, the list of memories is\nprovided, and the agent can decide to read them.\nWe found that memories can significantly improve the user experience with Serena.\n\n### Prepare Your Project\n\n#### Structure Your Codebase\n\nSerena uses the code structure for finding, reading and editing code. This means that it will\nwork well with well-structured code but may perform poorly on fully unstructured one (like a \"God class\"\nwith enormous, non-modular functions).\nFurthermore, for languages that are not statically typed, type annotations are highly beneficial.\n\n#### Start from a Clean State\n\nIt is best to start a code generation task from a clean git state. Not only will\nthis make it easier for you to inspect the changes, but also the model itself will\nhave a chance of seeing what it has changed by calling `git diff` and thereby\ncorrect itself or continue working in a followup conversation if needed.\n\n:warning: **Important**: since Serena will write to files using the system-native line endings\nand it might want to look at the git diff, it is important to\nset `git config core.autocrlf` to `true` on Windows.\nWith `git config core.autocrlf` set to `false` on Windows, you may end up with huge diffs\nonly due to line endings. It is generally a good idea to globally enable this git setting on Windows:\n\n```shell\ngit config --global core.autocrlf true\n```\n\n#### Logging, Linting, and Automated Tests\n\nSerena can successfully complete tasks in an _agent loop_, where it iteratively\nacquires information, performs actions, and reflects on the results.\nHowever, Serena cannot use a debugger; it must rely on the results of program executions,\nlinting results, and test results to assess the correctness of its actions.\nTherefore, software that is designed to meaningful interpretable outputs (e.g. log messages)\nand that has a good test coverage is much easier to work with for Serena.\n\nWe generally recommend to start an editing task from a state where all linting checks and tests pass.\n\n### Prompting Strategies\n\nWe found that it is often a good idea to spend some time conceptualizing and planning a task\nbefore actually implementing it, especially for non-trivial task. This helps both in achieving\nbetter results and in increasing the feeling of control and staying in the loop. You can\nmake a detailed plan in one session, where Serena may read a lot of your code to build up the context,\nand then continue with the implementation in another (potentially after creating suitable memories).\n\n### Running Out of Context\n\nFor long and complicated tasks, or tasks where Serena has read a lot of content, you\nmay come close to the limits of context tokens. In that case, it is often a good idea to continue\nin a new conversation. Serena has a dedicated tool to create a summary of the current state\nof the progress and all relevant info for continuing it. You can request to create this summary and\nwrite it to a memory. Then, in a new conversation, you can just ask Serena to read the memory and\ncontinue with the task. In our experience, this worked really well. On the up-side, since in a\nsingle session there is no summarization involved, Serena does not usually get lost (unlike some\nother agents that summarize under the hood), and it is also instructed to occasionally check whether\nit's on the right track.\n\nMoreover, Serena is instructed to be frugal with context\n(e.g., to not read bodies of code symbols unnecessarily),\nbut we found that Claude is not always very good in being frugal (Gemini seemed better at it).\nYou can explicitly instruct it to not read the bodies if you know that it's not needed.\n\n### Serena's Logs: The Dashboard and GUI Tool\n\nSerena provides two convenient ways of accessing the logs of the current session:\n\n* via the **web-based dashboard** (enabled by default)\n\n    This is supported on all platforms.\n    By default, it will be accessible at `http://localhost:24282/dashboard/index.html`,\n    but a higher port may be used if the default port is unavailable/multiple instances are running.\n\n* via the **GUI tool** (disabled by default)\n\n    This is mainly supported on Windows, but it may also work on Linux; macOS is unsupported.\n\nBoth can be enabled, configured or disabled in Serena's configuration file (`serena_config.yml`, see above).\nIf enabled, they will automatically be opened as soon as the Serena agent/MCP server is started.\nThe web dashboard will display usage statistics of Serena's tools if you set  `record_tool_usage_stats: True` in your config.\n\nIn addition to viewing logs, both tools allow to shut down the Serena agent.\nThis function is provided, because clients like Claude Desktop may fail to terminate the MCP server subprocess\nwhen they themselves are closed.\n\n## Comparison with Other Coding Agents\n\nTo our knowledge, Serena is the first fully-featured coding agent where the\nentire functionality\nis available through an MCP server, thus not requiring API keys or\nsubscriptions.\n\n### Subscription-Based Coding Agents\n\nMany prominent subscription-based coding agents are parts of IDEs like\nWindsurf, Cursor and VSCode.\nSerena's functionality is similar to Cursor's Agent, Windsurf's Cascade or\nVSCode's agent mode.\n\nSerena has the advantage of not requiring a subscription.\nA potential disadvantage is that it\nis not directly integrated into an IDE, so the inspection of newly written code\nis not as seamless.\n\nMore technical differences are:\n\n* Serena is not bound to a specific IDE or CLI.\n  Serena's MCP server can be used with any MCP client (including some IDEs),\n  and the Agno-based agent provides additional ways of applying its functionality.\n* Serena is not bound to a specific large language model or API.\n* Serena navigates and edits code using a language server, so it has a symbolic\n  understanding of the code.\n  IDE-based tools often use a RAG-based or purely text-based approach, which is often\n  less powerful, especially for large codebases.\n* Serena is open-source and has a small codebase, so it can be easily extended\n  and modified.\n\n### API-Based Coding Agents\n\nAn alternative to subscription-based agents are API-based agents like Claude\nCode, Cline, Aider, Roo Code and others, where the usage costs map directly\nto the API costs of the underlying LLM.\nSome of them (like Cline) can even be included in IDEs as an extension.\nThey are often very powerful and their main downside are the (potentially very\nhigh) API costs.\n\nSerena itself can be used as an API-based agent (see the section on Agno above).\nWe have not yet written a CLI tool or a\ndedicated IDE extension for Serena (and there is probably no need for the latter, as\nSerena can already be used with any IDE that supports MCP servers).\nIf there is demand for a Serena as a CLI tool like Claude Code, we will\nconsider writing one.\n\nThe main difference between Serena and other API-based agents is that Serena can\nalso be used as an MCP server, thus not requiring\nan API key and bypassing the API costs. This is a unique feature of Serena.\n\n### Other MCP-Based Coding Agents\n\nThere are other MCP servers designed for coding, like [DesktopCommander](https://github.com/wonderwhy-er/DesktopCommanderMCP) and\n[codemcp](https://github.com/ezyang/codemcp).\nHowever, to the best of our knowledge, none of them provide semantic code\nretrieval and editing tools; they rely purely on text-based analysis.\nIt is the integration of language servers and the MCP that makes Serena unique\nand so powerful for challenging coding tasks, especially in the context of\nlarger codebases.\n\n## Acknowledgements\n\n### Sponsors\n\nWe are very grateful to our [sponsors](https://github.com/sponsors/oraios) who help us drive Serena's development. The core team\n(the founders of [Oraios AI](https://oraios-ai.de/)) put in a lot of work in order to turn Serena into a useful open source project. \nSo far, there is no business model behind this project, and sponsors are our only source of income from it.\n\nSponsors help us dedicating more time to the project, managing contributions, and working on larger features (like better tooling based on more advanced\nLSP features, VSCode integration, debugging via the DAP, and several others).\nIf you find this project useful to your work, or would like to accelerate the development of Serena, consider becoming a sponsor.\n\nWe are proud to announce that the Visual Studio Code team, together with Microsoft‚Äôs Open Source Programs Office and GitHub Open Source\nhave decided to sponsor Serena with a one-time contribution!\n\n<p align=\"center\">\n  \n</p>\n\n### Community Contributions\n\nA significant part of Serena, especially support for various languages, was contributed by the open source community.\nWe are very grateful for the many contributors who made this possible and who played an important role in making Serena\nwhat it is today.\n\n### Technologies\nWe built Serena on top of multiple existing open-source technologies, the most important ones being:\n\n1. [multilspy](https://github.com/microsoft/multilspy).\n   A library which wraps language server implementations and adapts them for interaction via Python\n   and which provided the basis for our library Solid-LSP (src/solidlsp).\n   Solid-LSP provides pure synchronous LSP calls and extends the original library with the symbolic logic\n   that Serena required.\n2. [Python MCP SDK](https://github.com/modelcontextprotocol/python-sdk)\n3. [Agno](https://github.com/agno-agi/agno) and\n   the associated [agent-ui](https://github.com/agno-agi/agent-ui),\n   which we use to allow Serena to work with any model, beyond the ones\n   supporting the MCP.\n4. All the language servers that we use through Solid-LSP.\n\nWithout these projects, Serena would not have been possible (or would have been significantly more difficult to build).\n\n## Customizing and Extending Serena\n\nIt is straightforward to extend Serena's AI functionality with your own ideas.\nSimply implement a new tool by subclassing\n`serena.agent.Tool` and implement the `apply` method with a signature\nthat matches the tool's requirements.\nOnce implemented, `SerenaAgent` will automatically have access to the new tool.\n\nIt is also relatively straightforward to add [support for a new programming language](/.serena/memories/adding_new_language_support_guide.md).\n\nWe look forward to seeing what the community will come up with!\nFor details on contributing, see [contributing guidelines](/CONTRIBUTING.md).\n\n## List of Tools\n\nHere is the list of Serena's default tools with a short description (output of `uv run serena tools list`):\n\n* `activate_project`: Activates a project by name.\n* `check_onboarding_performed`: Checks whether project onboarding was already performed.\n* `create_text_file`: Creates/overwrites a file in the project directory.\n* `delete_memory`: Deletes a memory from Serena's project-specific memory store.\n* `execute_shell_command`: Executes a shell command.\n* `find_file`: Finds files in the given relative paths\n* `find_referencing_symbols`: Finds symbols that reference the symbol at the given location (optionally filtered by type).\n* `find_symbol`: Performs a global (or local) search for symbols with/containing a given name/substring (optionally filtered by type).\n* `get_symbols_overview`: Gets an overview of the top-level symbols defined in a given file.\n* `insert_after_symbol`: Inserts content after the end of the definition of a given symbol.\n* `insert_before_symbol`: Inserts content before the beginning of the definition of a given symbol.\n* `list_dir`: Lists files and directories in the given directory (optionally with recursion).\n* `list_memories`: Lists memories in Serena's project-specific memory store.\n* `onboarding`: Performs onboarding (identifying the project structure and essential tasks, e.g. for testing or building).\n* `prepare_for_new_conversation`: Provides instructions for preparing for a new conversation (in order to continue with the necessary context).\n* `read_file`: Reads a file within the project directory.\n* `read_memory`: Reads the memory with the given name from Serena's project-specific memory store.\n* `replace_regex`: Replaces content in a file by using regular expressions.\n* `replace_symbol_body`: Replaces the full definition of a symbol.\n* `search_for_pattern`: Performs a search for a pattern in the project.\n* `think_about_collected_information`: Thinking tool for pondering the completeness of collected information.\n* `think_about_task_adherence`: Thinking tool for determining whether the agent is still on track with the current task.\n* `think_about_whether_you_are_done`: Thinking tool for determining whether the task is truly completed.\n* `write_memory`: Writes a named memory (for future reference) to Serena's project-specific memory store.\n\nThere are several tools that are disabled by default, and have to be enabled explicitly, e.g., through the context or modes.\nNote that several of our default contexts do enable some of these tools. For example, the `desktop-app` context enables the `execute_shell_command` tool.\n\nThe full list of optional tools is (output of `uv run serena tools list --only-optional`):\n\n* `delete_lines`: Deletes a range of lines within a file.\n* `get_current_config`: Prints the current configuration of the agent, including the active and available projects, tools, contexts, and modes.\n* `initial_instructions`: Gets the initial instructions for the current project.\n    Should only be used in settings where the system prompt cannot be set,\n    e.g. in clients you have no control over, like Claude Desktop.\n* `insert_at_line`: Inserts content at a given line in a file.\n* `jet_brains_find_referencing_symbols`: Finds symbols that reference the given symbol\n* `jet_brains_find_symbol`: Performs a global (or local) search for symbols with/containing a given name/substring (optionally filtered by type).\n* `jet_brains_get_symbols_overview`: Retrieves an overview of the top-level symbols within a specified file\n* `remove_project`: Removes a project from the Serena configuration.\n* `replace_lines`: Replaces a range of lines within a file with new content.\n* `restart_language_server`: Restarts the language server, may be necessary when edits not through Serena happen.\n* `summarize_changes`: Provides instructions for summarizing the changes made to the codebase.\n* `switch_modes`: Activates modes by providing a list of their names",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "agent",
        "agents",
        "coding",
        "coding agents",
        "coding agent",
        "agents coding"
      ],
      "category": "coding-agents"
    },
    "pdavis68--RepoMapper": {
      "owner": "pdavis68",
      "name": "RepoMapper",
      "url": "https://github.com.mcas.ms/pdavis68/RepoMapper",
      "imageUrl": "",
      "description": "An MCP server (and command-line tool) to provide a dynamic map of chat-related files from the repository with their function prototypes and related files in order of relevance. Based on the \"Repo Map\" functionality in Aider.chat",
      "stars": 45,
      "forks": 11,
      "license": "Apache License 2.0",
      "language": "Tree-sitter Query",
      "updated_at": "2025-09-27T08:35:39Z",
      "readme_content": "# RepoMap - Command-Line Tool and MCP Server\n\nRepoMap is a powerful tool designed to help, primarily LLMs, understand and navigate complex codebases. It functions both as a command-line application for on-demand analysis and as an MCP (Model Context Protocol) server, providing continuous repository mapping capabilities to other applications. By generating a \"map\" of the software repository, RepoMap highlights important files, code definitions, and their relationships. It leverages Tree-sitter for accurate code parsing and the PageRank algorithm to rank code elements by importance, ensuring that the most relevant information is always prioritized.\n\n\n## Table of Contents\n- [Aider](#aider)\n- [Example Output](#example-output)\n- [Features](#features)\n- [Installation](#installation)\n- [Usage](#usage)\n  - [Basic Usage](#basic-usage)\n  - [Advanced Options](#advanced-options)\n- [How It Works](#how-it-works)\n- [Output Format](#output-format)\n- [Dependencies](#dependencies)\n- [Caching](#caching)\n- [Supported Languages](#supported-languages)\n- [License](#license)\n- [Running as an MCP Server](#running-as-an-mcp-server)\n  - [Setup](#setup)\n  - [Usage](#usage-1)\n- [Changelog](#changelog)\n----------\n\n## Aider\n\nRepoMap is 100% based on Aider's Repo map functionality, but I don't believe it shares any code with it. Allow me to explain.\n\nMy original effort was to take the RepoMap class from Aider, remove all the aider-specific dependencies, and then make it into a command-line tool. Python isn't my native language and I really struggled to get it to work.\n\nSo a few hours ago, I had a different idea. I took the RepoMap and some of its related code from aider and I fed it to an LLM (Either Claude or Gemini 2.5 Pro, can't remember) and had it create specifications for this, basically, from aider's implementation. So it generated a very detailed specification for this application (minus the MCP bits) and then I fed that to, well, Aider with Claude 3.7, and it built the command-line version of this.\n\nI then used a combination of Aider w/Claude 3.7, Cline w/Gemini 2.5 Pro Preview & Gemini 2.5 Flash Preview, and Phind.com, and Gemini.com and Claude.com and ChatGPT.com and after a few hours, I finally got the MCP server sorted out. Again, keeping in mind, Python isn't really my native tongue.\n\n----------\n\n## Example Output\n\n```\n> python repomap.py . --chat-files repomap_class.py\nChat files: ['/mnt/programming/RepoMapper/repomap_class.py']\nrepomap_class.py:\n(Rank value: 10.8111)\n\n  36: CACHE_VERSION = 1\n  39: TAGS_CACHE_DIR = os.path.join(os.getcwd(), f\".repomap.tags.cache.v{CACHE_VERSION}\")\n  40: SQLITE_ERRORS = (sqlite3.OperationalError, sqlite3.DatabaseError)\n  43: Tag = namedtuple(\"Tag\", \"rel_fname fname line name kind\".split())\n  46: class RepoMap:\n  49:     def __init__(\n  93:     def load_tags_cache(self):\n 102:     def save_tags_cache(self):\n 459:     def get_ranked_tags_map_uncached(\n 483:         def try_tags(num_tags: int) -> Tuple[Optional[str], int]:\n 512:     def get_repo_map(\n\nutils.py:\n(Rank value: 0.2297)\n\n  18: Tag = namedtuple(\"Tag\", \"rel_fname fname line name kind\".split())\n  21: def count_tokens(text: str, model_name: str = \"gpt-4\") -> int:\n  35: def read_text(filename: str, encoding: str = \"utf-8\", silent: bool = False) -> Optional[str]:\n\nimportance.py:\n(Rank value: 0.1149)\n\n   8: IMPORTANT_FILENAMES = {\n  27: IMPORTANT_DIR_PATTERNS = {\n  34: def is_important(rel_file_path: str) -> bool:\n  56: def filter_important_files(file_paths: List[str]) -> List[str]:\n\n    ...\n    ...\n    ...\n```\n\n----------\n\n## Features\n\n-   **Smart Code Analysis**: Uses Tree-sitter to parse source code and extract function/class definitions\n-   **Relevance Ranking**: Employs PageRank algorithm to rank code elements by importance\n-   **Token-Aware**: Respects token limits to fit within LLM context windows\n-   **Caching**: Persistent caching for fast subsequent runs\n-   **Multi-Language**: Supports Python, JavaScript, TypeScript, Java, C/C++, Go, Rust, and more\n-   **Important File Detection**: Automatically identifies and prioritizes important files (README, requirements.txt, etc.)\n\n----------\n\n## Installation\n\n```bash\npip install -r requirements.txt\n```\n\n----------\n\n## Usage\n\n### Basic Usage\n\n```bash\n# Map current directory\npython repomap.py .\n\n# Map specific directory with custom token limit\npython repomap.py src/ --map-tokens 2048\n\n# Map specific files\npython repomap.py file1.py file2.py\n\n# Specify chat files (higher priority) vs other files\npython repomap.py --chat-files main.py --other-files src/\n\n# Specify mentioned files and identifiers\npython repomap.py --mentioned-files config.py --mentioned-idents \"main_function\"\n\n# Enable verbose output\npython repomap.py . --verbose\n\n# Force refresh of caches\npython repomap.py . --force-refresh\n\n# Specify model for token counting\npython repomap.py . --model gpt-3.5-turbo\n\n# Set maximum context window\npython repomap.py . --max-context-window 8192\n\n# Exclude files with Page Rank 0\npython repomap.py . --exclude-unranked\n```\n\nThe tool prioritizes files in the following order:\n\n1.  `--chat-files`: These files are given the highest priority, as they're assumed to be the files you're currently working on.\n2.  `--mentioned-files`: These files are given a high priority, as they're explicitly mentioned in the current context.\n3.  `--other-files`: These files are given the lowest priority and are used to provide additional context.\n\n### Advanced Options\n\n```bash\n# Enable verbose output\npython repomap.py . --verbose\n\n# Force refresh of caches\npython repomap.py . --force-refresh\n\n# Specify model for token counting\npython repomap.py . --model gpt-3.5-turbo\n\n# Set maximum context window\npython repomap.py . --max-context-window 8192\n\n# Exclude files with Page Rank 0\npython repomap.py . --exclude-unranked\n\n# Mention specific files or identifiers for higher priority\npython repomap.py . --mentioned-files config.py --mentioned-idents \"main_function\"\n```\n\n----------\n\n## How It Works\n\n1.  **File Discovery**: Scans the repository for source files\n2.  **Code Parsing**: Uses Tree-sitter to parse code and extract definitions/references\n3.  **Graph Building**: Creates a graph where files are nodes and symbol references are edges\n4.  **Ranking**: Applies PageRank algorithm to rank files and symbols by importance\n5.  **Token Optimization**: Uses binary search to fit the most important content within token limits\n6.  **Output Generation**: Formats the results as a readable code map\n\n----------\n\n## Output Format\n\nThe tool generates a structured view of your codebase showing:\n\n-   File paths and important code sections\n-   Function and class definitions\n-   Key relationships between code elements\n-   Prioritized based on actual usage and references\n\n----------\n\n## Dependencies\n\n-   `tiktoken`: Token counting for various LLM models\n-   `networkx`: Graph algorithms (PageRank)\n-   `diskcache`: Persistent caching\n-   `grep-ast`: Tree-sitter integration for code parsing\n-   `tree-sitter`: Code parsing framework\n-   `pygments`: Syntax highlighting and lexical analysis\n\n----------\n\n## Caching\n\nThe tool uses persistent caching to speed up subsequent runs:\n\n-   Cache directory: `.repomap.tags.cache.v1/`\n-   Automatically invalidated when files change\n-   Can be cleared with `--force-refresh`\n\n----------\n\n## Supported Languages\n\nCurrently supports languages with Tree-sitter grammars:\n\n-   arduino\n-   chatito\n-   commonlisp\n-   cpp\n-   csharp\n-   c\n-   dart\n-   d\n-   elisp\n-   elixir\n-   elm\n-   gleam\n-   go\n-   javascript\n-   java\n-   lua\n-   ocaml_interface\n-   ocaml\n-   pony\n-   properties\n-   python\n-   racket\n-   r\n-   ruby\n-   rust\n-   solidity\n-   swift\n-   udev\n-   c_sharp\n-   hcl\n-   kotlin\n-   php\n-   ql\n-   scala\n\n----------\n\n## License\n\nThis implementation is based on the RepoMap design from the Aider project.\n\n----------\n\n## Running as an MCP Server\n\nRepoMap can also be run as an MCP (Model Context Protocol) server, allowing other applications to access its repository mapping capabilities.\n\n### Setup\n\n1. The RepoMap MCP server uses STDIO (standard input/output) for communication. No additional configuration is required for the transport layer.\n2. To set up RepoMap as an MCP server with Cline (or similar tools like Roo), add the following configuration to your Cline settings file (e.g., `cline_mcp_settings.json`):\n\n```json\n{\n  \"mcpServers\": {\n    \"RepoMapper\": {\n      \"disabled\": false,\n      \"timeout\": 60,\n      \"type\": \"stdio\",\n      \"command\": \"/usr/bin/python3\",\n      \"args\": [\n        \"/absolute/path/to/repomap_server.py\"\n      ]\n    }\n  }\n}\n```\n\n- Replace `\"/absolute/path/to/repomap_server.py\"` with the actual path to your `repomap_server.py` file.\n\n### Usage\n\n1. Run the `repomap_server.py` script:\n\n```bash\npython repomap_server.py\n```\n\n2. The server will start and listen for requests via STDIO.\n3. Other applications can then use the `repo_map` tool provided by the server to generate repository maps. They must specify the `project_root` parameter as an absolute path to the project they want to map.\n\n\n## Changelog\n\n7/13/2025 - Removed the project.json dependency. Fixed the MCP server to be a little easier for the LLM to work with in terms of filenames.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "agents",
        "repository",
        "programming",
        "coding agents",
        "agents coding",
        "aider chat"
      ],
      "category": "coding-agents"
    },
    "rinadelph--Agent-MCP": {
      "owner": "rinadelph",
      "name": "Agent-MCP",
      "url": "https://github.com/rinadelph/Agent-MCP",
      "imageUrl": "",
      "description": "A framework for creating multi-agent systems using MCP for coordinated AI collaboration, featuring task management, shared context, and RAG capabilities.",
      "stars": 948,
      "forks": 113,
      "license": "Other",
      "language": "TypeScript",
      "updated_at": "2025-10-04T04:35:14Z",
      "readme_content": "# Agent-MCP\r\n\r\n[![Ask DeepWiki](https://deepwiki.com/badge.svg)](https://deepwiki.com/rinadelph/Agent-MCP)\r\n\r\n> üöÄ **Advanced Tool Notice**: This framework is designed for experienced AI developers who need sophisticated multi-agent orchestration capabilities. Agent-MCP requires familiarity with AI coding workflows, MCP protocols, and distributed systems concepts. We're actively working to improve documentation and ease of use. If you're new to AI-assisted development, consider starting with simpler tools and returning when you need advanced multi-agent capabilities.\r\n>\r\n> üí¨ **Join the Community**: Connect with us on [Discord](https://discord.gg/7Jm7nrhjGn) to get help, share experiences, and collaborate with other developers building multi-agent systems.\r\n\r\nMulti-Agent Collaboration Protocol for coordinated AI software development.\r\n\r\n<div align=\"center\">\r\n  \r\n</div>\r\n\r\nThink **Obsidian for your AI agents** - a living knowledge graph where multiple AI agents collaborate through shared context, intelligent task management, and real-time visualization. Watch your codebase evolve as specialized agents work in parallel, never losing context or stepping on each other's work.\r\n\r\n## Why Multiple Agents?\r\n\r\nBeyond the philosophical issues, traditional AI coding assistants hit practical limitations:\r\n- **Context windows overflow** on large codebases\r\n- **Knowledge gets lost** between conversations\r\n- **Single-threaded execution** creates bottlenecks\r\n- **No specialization** - one agent tries to do everything\r\n- **Constant rework** from lost context and confusion\r\n\r\n## The Multi-Agent Solution\r\n\r\nAgent-MCP transforms AI development from a single assistant to a coordinated team:\r\n\r\n<div align=\"center\">\r\n  \r\n</div>\r\n\r\n**Real-time visualization** shows your AI team at work - purple nodes represent context entries, blue nodes are agents, and connections show active collaborations. It's like having a mission control center for your development team.\r\n\r\n### Core Capabilities\r\n\r\n**Parallel Execution**  \r\nMultiple specialized agents work simultaneously on different parts of your codebase. Backend agents handle APIs while frontend agents build UI components, all coordinated through shared memory.\r\n\r\n**Persistent Knowledge Graph**  \r\n\r\n<div align=\"center\">\r\n  \r\n</div>\r\n\r\nYour project's entire context lives in a searchable, persistent memory bank. Agents query this shared knowledge to understand requirements, architectural decisions, and implementation details. Nothing gets lost between sessions.\r\n\r\n**Intelligent Task Management**  \r\n\r\n<div align=\"center\">\r\n  \r\n</div>\r\n\r\nMonitor every agent's status, assigned tasks, and recent activity. The system automatically manages task dependencies, prevents conflicts, and ensures work flows smoothly from planning to implementation.\r\n\r\n## Quick Start\r\n\r\n### Python Implementation (Recommended)\r\n\r\n```bash\r\n# Clone and setup\r\ngit clone https://github.com/rinadelph/Agent-MCP.git\r\ncd Agent-MCP\r\n\r\n# Check version requirements\r\npython --version  # Should be >=3.10\r\nnode --version    # Should be >=18.0.0\r\nnpm --version     # Should be >=9.0.0\r\n\r\n# If using nvm for Node.js version management\r\nnvm use  # Uses the version specified in .nvmrc\r\n\r\n# Configure environment\r\ncp .env.example .env  # Add your OpenAI API key\r\nuv venv\r\nuv install\r\n\r\n# Start the server\r\nuv run -m agent_mcp.cli --port 8080 --project-dir path-to-directory\r\n\r\n# Launch dashboard (recommended for full experience)\r\ncd agent_mcp/dashboard && npm install && npm run dev\r\n```\r\n\r\n### Node.js/TypeScript Implementation (Alternative)\r\n\r\n```bash\r\n# Clone and setup\r\ngit clone https://github.com/rinadelph/Agent-MCP.git\r\ncd Agent-MCP/agent-mcp-node\r\n\r\n# Install dependencies\r\nnpm install\r\n\r\n# Configure environment\r\ncp .env.example .env  # Add your OpenAI API key\r\n\r\n# Start the server\r\nnpm run server\r\n\r\n# Or use the built version\r\nnpm run build\r\nnpm start\r\n\r\n# Or install globally\r\nnpm install -g agent-mcp-node\r\nagent-mcp --port 8080 --project-dir path-to-directory\r\n```\r\n\r\n## MCP Integration Guide\r\n\r\n### What is MCP?\r\n\r\nThe **Model Context Protocol (MCP)** is an open standard that enables AI assistants to securely connect to external data sources and tools. Agent-MCP leverages MCP to provide seamless integration with various development tools and services.\r\n\r\n### Running Agent-MCP as an MCP Server\r\n\r\nAgent-MCP can function as an MCP server, exposing its multi-agent capabilities to MCP-compatible clients like Claude Desktop, Cline, and other AI coding assistants.\r\n\r\n#### Quick MCP Setup\r\n\r\n```bash\r\n# 1. Install Agent-MCP\r\nuv venv\r\nuv install\r\n\r\n# 2. Start the MCP server\r\nuv run -m agent_mcp.cli --port 8080\r\n\r\n# 3. Configure your MCP client to connect to:\r\n# HTTP: http://localhost:8000/mcp\r\n# WebSocket: ws://localhost:8000/mcp/ws\r\n```\r\n\r\n#### MCP Server Configuration\r\n\r\nCreate an MCP configuration file (`mcp_config.json`):\r\n\r\n```json\r\n{\r\n  \"server\": {\r\n    \"name\": \"agent-mcp\",\r\n    \"version\": \"1.0.0\"\r\n  },\r\n  \"tools\": [\r\n    {\r\n      \"name\": \"create_agent\",\r\n      \"description\": \"Create a new specialized AI agent\"\r\n    },\r\n    {\r\n      \"name\": \"assign_task\", \r\n      \"description\": \"Assign tasks to specific agents\"\r\n    },\r\n    {\r\n      \"name\": \"query_project_context\",\r\n      \"description\": \"Query the shared knowledge graph\"\r\n    },\r\n    {\r\n      \"name\": \"manage_agent_communication\",\r\n      \"description\": \"Handle inter-agent messaging\"\r\n    }\r\n  ],\r\n  \"resources\": [\r\n    {\r\n      \"name\": \"agent_status\",\r\n      \"description\": \"Real-time agent status and activity\"\r\n    },\r\n    {\r\n      \"name\": \"project_memory\",\r\n      \"description\": \"Persistent project knowledge graph\"\r\n    }\r\n  ]\r\n}\r\n```\r\n\r\n#### Using Agent-MCP with Claude Desktop\r\n\r\n1. **Add to Claude Desktop Config**:\r\n   \r\n   Open `~/Library/Application Support/Claude/claude_desktop_config.json` (macOS) or equivalent:\r\n   \r\n   ```json\r\n   {\r\n     \"mcpServers\": {\r\n       \"agent-mcp\": {\r\n         \"command\": \"uv\",\r\n         \"args\": [\"run\", \"-m\", \"agent_mcp.cli\", \"--port\", \"8080\"],\r\n         \"env\": {\r\n           \"OPENAI_API_KEY\": \"your-openai-api-key\"\r\n         }\r\n       }\r\n     }\r\n   }\r\n   ```\r\n\r\n2. **Restart Claude Desktop** to load the MCP server\r\n\r\n3. **Verify Connection**: Claude should show \"üîå agent-mcp\" in the conversation\r\n\r\n#### MCP Tools Available\r\n\r\nOnce connected, you can use these MCP tools directly in Claude:\r\n\r\n**Agent Management**\r\n- `create_agent` - Spawn specialized agents (backend, frontend, testing, etc.)\r\n- `list_agents` - View all active agents and their status\r\n- `terminate_agent` - Safely shut down agents\r\n\r\n**Task Orchestration**  \r\n- `assign_task` - Delegate work to specific agents\r\n- `view_tasks` - Monitor task progress and dependencies\r\n- `update_task_status` - Track completion and blockers\r\n\r\n**Knowledge Management**\r\n- `ask_project_rag` - Query the persistent knowledge graph\r\n- `update_project_context` - Add architectural decisions and patterns\r\n- `view_project_context` - Access stored project information\r\n\r\n**Communication**\r\n- `send_agent_message` - Direct messaging between agents\r\n- `broadcast_message` - Send updates to all agents\r\n- `request_assistance` - Escalate complex issues\r\n\r\n#### Advanced MCP Configuration\r\n\r\n**Custom Transport Options**:\r\n```bash\r\n# HTTP with custom port\r\nuv run -m agent_mcp.cli --port 8080\r\n\r\n# WebSocket with authentication\r\nuv run -m agent_mcp.cli --port 8080 --auth-token your-secret-token\r\n\r\n# Unix socket (Linux/macOS)\r\nuv run -m agent_mcp.cli --port 8080\r\n```\r\n\r\n**Environment Variables**:\r\n```bash\r\nexport AGENT_MCP_HOST=0.0.0.0          # Server host\r\nexport AGENT_MCP_PORT=8000              # Server port  \r\nexport AGENT_MCP_LOG_LEVEL=INFO         # Logging level\r\nexport AGENT_MCP_PROJECT_DIR=/your/project  # Default project directory\r\nexport AGENT_MCP_MAX_AGENTS=10          # Maximum concurrent agents\r\n```\r\n\r\n### MCP Client Examples\r\n\r\n#### Python Client\r\n```python\r\nimport asyncio\r\nfrom mcp import Client\r\n\r\nasync def main():\r\n    async with Client(\"http://localhost:8000/mcp\") as client:\r\n        # Create a backend agent\r\n        result = await client.call_tool(\"create_agent\", {\r\n            \"role\": \"backend\",\r\n            \"specialization\": \"API development\"\r\n        })\r\n        \r\n        # Assign a task\r\n        await client.call_tool(\"assign_task\", {\r\n            \"agent_id\": result[\"agent_id\"],\r\n            \"task\": \"Implement user authentication endpoints\"\r\n        })\r\n        \r\n        # Query project context\r\n        context = await client.call_tool(\"ask_project_rag\", {\r\n            \"query\": \"What's our current database schema?\"\r\n        })\r\n        print(context)\r\n\r\nasyncio.run(main())\r\n```\r\n\r\n#### JavaScript Client\r\n```javascript\r\nimport { MCPClient } from '@modelcontextprotocol/client';\r\n\r\nconst client = new MCPClient('http://localhost:8000/mcp');\r\n\r\nasync function createAgent() {\r\n  await client.connect();\r\n  \r\n  const agent = await client.callTool('create_agent', {\r\n    role: 'frontend',\r\n    specialization: 'React components'\r\n  });\r\n  \r\n  console.log('Created agent:', agent.agent_id);\r\n  \r\n  await client.disconnect();\r\n}\r\n\r\ncreateAgent().catch(console.error);\r\n```\r\n\r\n### Troubleshooting MCP Connection\r\n\r\n**Connection Issues**:\r\n```bash\r\n# Check if MCP server is running\r\ncurl http://localhost:8000/mcp/health\r\n\r\n# Verify WebSocket connection\r\nwscat -c ws://localhost:8000/mcp/ws\r\n\r\n# Check server logs\r\nuv run -m agent_mcp.cli --port 8080 --log-level DEBUG\r\n```\r\n\r\n**Common Issues**:\r\n- **Port conflicts**: Change port with `--port` flag\r\n- **Permission errors**: Ensure OpenAI API key is set\r\n- **Client timeout**: Increase timeout in client configuration\r\n- **Agent limit reached**: Check active agent count with `list_agents`\r\n\r\n### Integration Examples\r\n\r\n**VS Code with MCP**:\r\nUse the MCP extension to integrate Agent-MCP directly into your editor workflow.\r\n\r\n**Terminal Usage**:\r\n```bash\r\n# Quick task assignment via curl\r\ncurl -X POST http://localhost:8000/mcp/tools/assign_task \\\r\n  -H \"Content-Type: application/json\" \\\r\n  -d '{\"task\": \"Add error handling to API endpoints\", \"agent_role\": \"backend\"}'\r\n```\r\n\r\n**CI/CD Integration**:\r\n```yaml\r\n# GitHub Actions example\r\n- name: Run Agent-MCP Code Review\r\n  run: |\r\n    uv run -m agent_mcp.cli --port 8080 --daemon\r\n    curl -X POST localhost:8000/mcp/tools/assign_task \\\r\n      -d '{\"task\": \"Review PR for security issues\", \"agent_role\": \"security\"}'\r\n```\r\n\r\n## How It Works: Breaking Complexity into Simple Steps\r\n\r\n```mermaid\r\ngraph LR\r\n    A[Step 1] --> B[Step 2] --> C[Step 3] --> D[Step 4] --> E[Done!]\r\n    style A fill:#4ecdc4,color:#fff\r\n    style E fill:#ff6b6b,color:#fff\r\n```\r\n\r\nEvery task can be broken down into linear steps. This is the core insight that makes Agent-MCP powerful.\r\n\r\n### The Problem with Complex Tasks\r\n\r\n```mermaid\r\ngraph TD\r\n    A[\"Build User Authentication\"] -->|Single Agent Tries Everything| B{???}\r\n    B --> C[Database?]\r\n    B --> D[API?]\r\n    B --> E[Frontend?]\r\n    B --> F[Security?]\r\n    B --> G[Tests?]\r\n    C -.->|Confused| H[Incomplete Implementation]\r\n    D -.->|Overwhelmed| H\r\n    E -.->|Context Lost| H\r\n    F -.->|Assumptions| H\r\n    G -.->|Forgotten| H\r\n    style A fill:#ff6b6b,color:#fff\r\n    style H fill:#666,color:#fff\r\n```\r\n\r\n### The Agent-MCP Solution\r\n\r\n```mermaid\r\ngraph TD\r\n    A[\"Build User Authentication\"] -->|Break Down| B[Linear Tasks]\r\n    B --> C[\"Agent 1: Database\"]\r\n    B --> D[\"Agent 2: API\"]\r\n    B --> E[\"Agent 3: Frontend\"]\r\n    \r\n    C --> C1[Create users table]\r\n    C1 --> C2[Add indexes]\r\n    C2 --> C3[Create sessions table]\r\n    \r\n    D --> D1[POST /register]\r\n    D1 --> D2[POST /login]\r\n    D2 --> D3[POST /logout]\r\n    \r\n    E --> E1[Login Form]\r\n    E1 --> E2[Register Form]\r\n    E2 --> E3[Auth Context]\r\n    \r\n    C3 --> F[Working System]\r\n    D3 --> F\r\n    E3 --> F\r\n    \r\n    style A fill:#4ecdc4,color:#fff\r\n    style F fill:#4ecdc4,color:#fff\r\n```\r\n\r\nEach agent focuses on their linear chain. No confusion. No context pollution. Just clear, deterministic progress.\r\n\r\n## The 5-Step Workflow\r\n\r\n### 1. Initialize Admin Agent\r\n```\r\nYou are the admin agent.\r\nAdmin Token: \"your_admin_token_from_server\"\r\n\r\nYour role is to:\r\n- Coordinate all development work\r\n- Create and manage worker agents\r\n- Maintain project context\r\n- Assign tasks based on agent specializations\r\n```\r\n\r\n### 2. Load Your Project Blueprint (MCD)\r\n```\r\nAdd this MCD (Main Context Document) to project context:\r\n\r\n[paste your MCD here - see docs/mcd-guide.md for structure]\r\n\r\nStore every detail in the knowledge graph. This becomes the single source of truth for all agents.\r\n```\r\n\r\nThe MCD (Main Context Document) is your project's comprehensive blueprint - think of it as writing the book of your application before building it. It includes:\r\n- Technical architecture and design decisions\r\n- Database schemas and API specifications\r\n- UI component hierarchies and workflows\r\n- Task breakdowns with clear dependencies\r\n\r\nSee our [MCD Guide](./docs/mcd-guide.md) for detailed examples and templates.\r\n\r\n### 3. Deploy Your Agent Team\r\n```\r\nCreate specialized agents for parallel development:\r\n\r\n- backend-worker: API endpoints, database operations, business logic\r\n- frontend-worker: UI components, state management, user interactions\r\n- integration-worker: API connections, data flow, system integration\r\n- test-worker: Unit tests, integration tests, validation\r\n- devops-worker: Deployment, CI/CD, infrastructure\r\n```\r\n\r\nEach agent specializes in their domain, leading to higher quality implementations and faster development.\r\n\r\n### 4. Initialize and Deploy Workers\r\n```\r\n# In new window for each worker:\r\nYou are [worker-name] agent.\r\nYour Admin Token: \"worker_token_from_admin\"\r\n\r\nQuery the project knowledge graph to understand:\r\n1. Overall system architecture\r\n2. Your specific responsibilities\r\n3. Integration points with other components\r\n4. Coding standards and patterns to follow\r\n5. Current implementation status\r\n\r\nBegin implementation following the established patterns.\r\n\r\nAUTO --worker --memory\r\n```\r\n\r\n**Important: Setting Agent Modes**\r\n\r\nAgent modes (like `--worker`, `--memory`, `--playwright`) are not just flags - they activate specific behavioral patterns. In Claude Code, you can make these persistent by:\r\n\r\n1. Copy the mode instructions to your clipboard\r\n2. Type `#` to open Claude's memory feature\r\n3. Paste the instructions for persistent behavior\r\n\r\nExample for Claude Code memory:\r\n```\r\n# When I use \"AUTO --worker --memory\", follow these patterns:\r\n- Always check file status before editing\r\n- Query project RAG for context before implementing\r\n- Document all changes in task notes\r\n- Work on one file at a time, completing it before moving on\r\n- Update task status after each completion\r\n```\r\n\r\nThis ensures consistent behavior across your entire session without repeating instructions.\r\n\r\n### 5. Monitor and Coordinate\r\n\r\nThe dashboard provides real-time visibility into your AI development team:\r\n\r\n**Network Visualization** - Watch agents collaborate and share information  \r\n**Task Progress** - Track completion across all parallel work streams  \r\n**Memory Health** - Ensure context remains fresh and accessible  \r\n**Activity Timeline** - See exactly what each agent is doing\r\n\r\nAccess at `http://localhost:3847` after launching the dashboard.\r\n\r\n## Advanced Features\r\n\r\n### Specialized Agent Modes\r\n\r\nAgent modes fundamentally change how agents behave. They're not just configuration - they're behavioral contracts that ensure agents follow specific patterns optimized for their role.\r\n\r\n**Standard Worker Mode**\r\n```\r\nAUTO --worker --memory\r\n```\r\nOptimized for implementation tasks:\r\n- Granular file status checking before any edits\r\n- Sequential task completion (one at a time)\r\n- Automatic documentation of changes\r\n- Integration with project RAG for context\r\n- Task status updates after each completion\r\n\r\n**Frontend Specialist Mode**\r\n```\r\nAUTO --worker --playwright\r\n```\r\nEnhanced with visual validation capabilities:\r\n- All standard worker features\r\n- Browser automation for component testing\r\n- Screenshot capabilities for visual regression\r\n- DOM interaction for end-to-end testing\r\n- Component-by-component implementation with visual verification\r\n\r\n**Research Mode**\r\n```\r\nAUTO --memory\r\n```\r\nRead-only access for analysis and planning:\r\n- No file modifications allowed\r\n- Deep context exploration via RAG\r\n- Pattern identification across codebase\r\n- Documentation generation\r\n- Architecture analysis and recommendations\r\n\r\n**Memory Management Mode**\r\n```\r\nAUTO --memory --manager\r\n```\r\nFor context curation and optimization:\r\n- Memory health monitoring\r\n- Stale context identification\r\n- Knowledge graph optimization\r\n- Context summarization for new agents\r\n- Cross-agent knowledge transfer\r\n\r\nEach mode enforces specific behaviors that prevent common mistakes and ensure consistent, high-quality output.\r\n\r\n### Project Memory Management\r\n\r\nThe system maintains several types of memory:\r\n\r\n**Project Context** - Architectural decisions, design patterns, conventions  \r\n**Task Memory** - Current status, blockers, implementation notes  \r\n**Agent Memory** - Individual agent learnings and specializations  \r\n**Integration Points** - How different components connect\r\n\r\nAll memory is:\r\n- Searchable via semantic queries\r\n- Version controlled for rollback\r\n- Tagged for easy categorization\r\n- Automatically garbage collected when stale\r\n\r\n### Conflict Resolution\r\n\r\nFile-level locking prevents agents from overwriting each other's work:\r\n\r\n1. Agent requests file access\r\n2. System checks if file is locked\r\n3. If locked, agent works on other tasks or waits\r\n4. After completion, lock is released\r\n5. Other agents can now modify the file\r\n\r\nThis happens automatically - no manual coordination needed.\r\n\r\n## Short-Lived vs. Long-Lived Agents: The Critical Difference\r\n\r\n### Traditional Long-Lived Agents\r\nMost AI coding assistants maintain conversations across entire projects:\r\n- **Accumulated context grows unbounded** - mixing unrelated code, decisions, and conversations\r\n- **Confused priorities** - yesterday's bug fix mingles with today's feature request\r\n- **Hallucination risks increase** - agents invent connections between unrelated parts\r\n- **Performance degrades over time** - every response processes irrelevant history\r\n- **Security vulnerability** - one carefully crafted prompt could expose your entire project\r\n\r\n### Agent-MCP's Ephemeral Agents\r\nEach agent is purpose-built for a single task:\r\n- **Minimal, focused context** - only what's needed for the specific task\r\n- **Crystal clear objectives** - one task, one goal, no ambiguity\r\n- **Deterministic behavior** - limited context means predictable outputs\r\n- **Consistently fast responses** - no context bloat to slow things down\r\n- **Secure by design** - agents literally cannot access what they don't need\r\n\r\n### A Practical Example\r\n\r\n**Traditional Approach**: \"Update the user authentication system\"\r\n```\r\nAgent: I'll update your auth system. I see from our previous conversation about \r\ndatabase migrations, UI components, API endpoints, deployment scripts, and that \r\nbug in the payment system... wait, which auth approach did we decide on? Let me \r\ntry to piece this together from our 50+ message history...\r\n\r\n[Agent produces confused implementation mixing multiple patterns]\r\n```\r\n\r\n**Agent-MCP Approach**: Same request, broken into focused tasks\r\n```\r\nAgent 1 (Database): Create auth tables with exactly these fields...\r\nAgent 2 (API): Implement /auth endpoints following REST patterns...\r\nAgent 3 (Frontend): Build login forms using existing component library...\r\nAgent 4 (Tests): Write auth tests covering these specific scenarios...\r\nAgent 5 (Integration): Connect components following documented interfaces...\r\n\r\n[Each agent completes their specific task without confusion]\r\n```\r\n\r\n## The Theory Behind Linear Decomposition\r\n\r\n### The Philosophy: Short-Lived Agents, Granular Tasks\r\n\r\nMost AI development approaches suffer from a fundamental flaw: they try to maintain massive context windows with a single, long-running agent. This leads to:\r\n\r\n- **Context pollution** - Irrelevant information drowns out what matters\r\n- **Hallucination risks** - Agents invent connections between unrelated parts\r\n- **Security vulnerabilities** - Agents with full context can be manipulated\r\n- **Performance degradation** - Large contexts slow down reasoning\r\n- **Unpredictable behavior** - Too much context creates chaos\r\n\r\n### Our Solution: Ephemeral Agents with Shared Memory\r\n\r\nAgent-MCP implements a radically different approach:\r\n\r\n**Short-Lived, Focused Agents**  \r\nEach agent lives only as long as their specific task. They:\r\n- Start with minimal context (just what they need)\r\n- Execute granular, linear tasks with clear boundaries\r\n- Document their work in shared memory\r\n- Terminate upon completion\r\n\r\n**Shared Knowledge Graph (RAG)**  \r\nInstead of cramming everything into context windows:\r\n- Persistent memory stores all project knowledge\r\n- Agents query only what's relevant to their task\r\n- Knowledge accumulates without overwhelming any single agent\r\n- Clear separation between working memory and reference material\r\n\r\n**Result**: Agents that are fast, focused, and safe. They can't be manipulated to reveal full project details because they never have access to it all at once.\r\n\r\n### Why This Matters for Safety\r\n\r\nTraditional long-context agents are like giving someone your entire codebase, documentation, and secrets in one conversation. Our approach is like having specialized contractors who only see the blueprint for their specific room.\r\n\r\n- **Reduced attack surface** - Agents can't leak what they don't know\r\n- **Deterministic behavior** - Limited context means predictable outputs\r\n- **Audit trails** - Every agent action is logged and traceable\r\n- **Rollback capability** - Mistakes are isolated to specific tasks\r\n\r\n### The Cleanup Protocol: Keeping Your System Lean\r\n\r\nAgent-MCP enforces strict lifecycle management:\r\n\r\n**Maximum 10 Active Agents**\r\n- Hard limit prevents resource exhaustion\r\n- Forces thoughtful task allocation\r\n- Maintains system performance\r\n\r\n**Automatic Cleanup Rules**\r\n- Agent finishes task ‚Üí Immediately terminated\r\n- Agent idle 60+ seconds ‚Üí Killed and task reassigned\r\n- Need more than 10 agents ‚Üí Least productive agents removed\r\n\r\n**Why This Matters**\r\n- **No zombie processes** eating resources\r\n- **Fresh context** for every task\r\n- **Predictable resource usage**\r\n- **Clean system state** always\r\n\r\nThis isn't just housekeeping - it's fundamental to the security and performance benefits of the short-lived agent model.\r\n\r\n### The Fundamental Principle\r\n\r\n**Any task that cannot be expressed as `Step 1 ‚Üí Step 2 ‚Üí Step N` is not atomic enough.**\r\n\r\nThis principle drives everything in Agent-MCP:\r\n\r\n1. **Complex goals** must decompose into **linear sequences**\r\n2. **Linear sequences** can execute **in parallel** when independent\r\n3. **Each step** must have **clear prerequisites** and **deterministic outputs**\r\n4. **Integration points** are **explicit** and **well-defined**\r\n\r\n### Why Linear Decomposition Works\r\n\r\n**Traditional Approach**: \"Build a user authentication system\"\r\n- Vague requirements lead to varied implementations\r\n- Agents make different assumptions\r\n- Integration becomes a nightmare\r\n\r\n**Agent-MCP Approach**: \r\n```\r\nChain 1: Database Layer\r\n  1.1: Create users table with id, email, password_hash\r\n  1.2: Add unique index on email\r\n  1.3: Create sessions table with user_id, token, expiry\r\n  1.4: Write migration scripts\r\n  \r\nChain 2: API Layer  \r\n  2.1: Implement POST /auth/register endpoint\r\n  2.2: Implement POST /auth/login endpoint\r\n  2.3: Implement POST /auth/logout endpoint\r\n  2.4: Add JWT token generation\r\n  \r\nChain 3: Frontend Layer\r\n  3.1: Create AuthContext provider\r\n  3.2: Build LoginForm component\r\n  3.3: Build RegisterForm component\r\n  3.4: Implement protected routes\r\n```\r\n\r\nEach step is atomic, testable, and has zero ambiguity. Multiple agents can work these chains in parallel without conflict.\r\n\r\n## Why Developers Choose Agent-MCP\r\n\r\n**The Power of Parallel Development**  \r\nInstead of waiting for one agent to finish the backend before starting the frontend, deploy specialized agents to work simultaneously. Your development speed is limited only by how well you decompose tasks.\r\n\r\n**No More Lost Context**  \r\nEvery decision, implementation detail, and architectural choice is stored in the shared knowledge graph. New agents instantly understand the project state without reading through lengthy conversation histories.\r\n\r\n**Predictable, Reliable Outputs**  \r\nFocused agents with limited context produce consistent results. The same task produces the same quality output every time, making development predictable and testable.\r\n\r\n**Built-in Conflict Prevention**  \r\nFile-level locking and task assignment prevent agents from stepping on each other's work. No more merge conflicts from simultaneous edits.\r\n\r\n**Complete Development Transparency**  \r\nWatch your AI team work in real-time through the dashboard. Every action is logged, every decision traceable. It's like having a live view into your development pipeline.\r\n\r\n**For Different Team Sizes**\r\n\r\n**Solo Developers**: Transform one AI assistant into a coordinated team. Work on multiple features simultaneously without losing track.\r\n\r\n**Small Teams**: Augment human developers with AI specialists that maintain perfect context across sessions.\r\n\r\n**Large Projects**: Handle complex systems where no single agent could hold all the context. The shared memory scales infinitely.\r\n\r\n**Learning & Teaching**: Perfect for understanding software architecture. Watch how tasks decompose and integrate in real-time.\r\n\r\n## System Requirements\r\n\r\n- **Python**: 3.10+ with pip or uv\r\n- **Node.js**: 18.0.0+ (recommended: 22.16.0)\r\n- **npm**: 9.0.0+ (recommended: 10.9.2)\r\n- **OpenAI API key** (for embeddings and RAG)\r\n- **RAM**: 4GB minimum\r\n- **AI coding assistant**: Claude Code or Cursor\r\n\r\nFor consistent development environment:\r\n```bash\r\n# Using nvm (Node Version Manager)\r\nnvm use  # Automatically uses Node v22.16.0 from .nvmrc\r\n\r\n# Or manually check versions\r\nnode --version  # Should be >=18.0.0\r\nnpm --version   # Should be >=9.0.0\r\npython --version  # Should be >=3.10\r\n```\r\n\r\n## Troubleshooting\r\n\r\n**\"Admin token not found\"**  \r\nCheck the server startup logs - token is displayed when MCP server starts.\r\n\r\n**\"Worker can't access tasks\"**  \r\nEnsure you're using the worker token (not admin token) when initializing workers.\r\n\r\n**\"Agents overwriting each other\"**  \r\nVerify all workers are initialized with the `--worker` flag for proper coordination.\r\n\r\n**\"Dashboard connection failed\"**  \r\n1. Ensure MCP server is running first\r\n2. Check Node.js version (18+ required)\r\n3. Reinstall dashboard dependencies\r\n\r\n**\"Memory queries returning stale data\"**  \r\nRun memory garbage collection through the dashboard or restart with `--refresh-memory`.\r\n\r\n## Documentation\r\n\r\n- [Getting Started Guide](./docs/getting-started.md) - Complete walkthrough with examples\r\n- [MCD Creation Guide](./docs/mcd-guide.md) - Write effective project blueprints\r\n- [Theoretical Foundation](./docs/chapter-1-cognitive-empathy.md) - Understanding AI cognition\r\n- [Architecture Overview](./docs/architecture.md) - System design and components\r\n- [API Reference](./docs/api-reference.md) - Complete technical documentation\r\n\r\n## Community and Support\r\n\r\n**Get Help**\r\n- [Discord Community](https://discord.gg/7Jm7nrhjGn) - Active developer discussions\r\n- [GitHub Issues](https://github.com/rinadelph/Agent-MCP/issues) - Bug reports and features\r\n- [Discussions](https://github.com/rinadelph/Agent-MCP/discussions) - Share your experiences\r\n\r\n**Contributing**\r\nWe welcome contributions! See our [Contributing Guide](CONTRIBUTING.md) for:\r\n- Code style and standards\r\n- Testing requirements\r\n- Pull request process\r\n- Development setup\r\n\r\n## License\r\n\r\n[![License: AGPL v3](https://img.shields.io/badge/License-AGPL_v3-blue.svg)](https://www.gnu.org/licenses/agpl-3.0)\r\n\r\nThis project is licensed under the **GNU Affero General Public License v3.0 (AGPL-3.0)**.\r\n\r\n**What this means:**\r\n- ‚úÖ You can use, modify, and distribute this software\r\n- ‚úÖ You can use it for commercial purposes\r\n- ‚ö†Ô∏è **Important**: If you run a modified version on a server that users interact with over a network, you **must** provide the source code to those users\r\n- ‚ö†Ô∏è Any derivative works must also be licensed under AGPL-3.0\r\n- ‚ö†Ô∏è You must include copyright notices and license information\r\n\r\nSee the [LICENSE](LICENSE) file for complete terms and conditions.\r\n\r\n**Why AGPL?** We chose AGPL to ensure that improvements to Agent-MCP benefit the entire community, even when used in server/SaaS deployments. This prevents proprietary forks that don't contribute back to the ecosystem.\r\n\r\n---\r\n\r\nBuilt by developers who believe AI collaboration should be as sophisticated as human collaboration.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "agents",
        "agent",
        "mcp",
        "agents coding",
        "coding agents",
        "agent mcp"
      ],
      "category": "coding-agents"
    },
    "stippi--code-assistant": {
      "owner": "stippi",
      "name": "code-assistant",
      "url": "https://github.com/stippi/code-assistant",
      "imageUrl": "",
      "description": "Coding agent with basic list, read, replace_in_file, write, execute_command and web search tools. Supports multiple projects concurrently.",
      "stars": 101,
      "forks": 17,
      "license": "MIT License",
      "language": "Rust",
      "updated_at": "2025-10-03T00:45:37Z",
      "readme_content": "# Code Assistant\n\n[![CI](https://github.com/stippi/code-assistant/actions/workflows/build.yml/badge.svg)](https://github.com/stippi/code-assistant/actions/workflows/build.yml)\n\nAn AI coding assistant built in Rust that provides both command-line and graphical interfaces for autonomous code analysis and modification.\n\n## Key Features\n\n**Multi-Modal Tool Execution**: Adapts to different LLM capabilities with pluggable tool invocation modes - native function calling, XML-style tags, and triple-caret blocks - ensuring compatibility across various AI providers.\n\n**Real-Time Streaming Interface**: Advanced streaming processors parse and display tool invocations as they stream from the LLM, with smart filtering to prevent unsafe tool combinations.\n\n**Session-Based Project Management**: Each chat session is tied to a specific project and maintains persistent state, working memory, and draft messages with attachment support.\n\n**Multiple Interface Options**: Choose between a modern GUI built on Zed's GPUI framework, traditional terminal interface, or headless MCP server mode for integration with MCP clients such as Claude Desktop.\n\n**Intelligent Project Exploration**: Autonomously builds understanding of codebases through working memory that tracks file structures, dependencies, and project context.\n\n**Auto-Loaded Repository Guidance**: Automatically includes `AGENTS.md` (or `CLAUDE.md` fallback) from the project root in the assistant's system context to align behavior with repo-specific instructions.\n\n## Installation\n\n```bash\ngit clone https://github.com/stippi/code-assistant\ncd code-assistant\ncargo build --release\n```\n\nThe binary will be available at `target/release/code-assistant`.\n\n## Project Configuration\n\nCreate `~/.config/code-assistant/projects.json` to define available projects:\n\n```jsonc\n{\n  \"code-assistant\": {\n    \"path\": \"/Users/<username>/workspace/code-assistant\",\n    \"format_on_save\": {\n      \"**/*.rs\": \"cargo fmt\" // Formats all files in project, so make sure files are already formatted\n    }\n  },\n  \"my-project\": {\n    \"path\": \"/Users/<username>/workspace/my-project\",\n    \"format_on_save\": {\n      \"**/*.ts\": \"prettier --write {path}\" // If the formatter accepts a path, provide \"{path}\"\n    }\n  }\n}\n```\n\n### Format-on-Save Feature\n\nThe _optional_ `format_on_save` field allows automatic formatting of files after modifications. It maps file patterns (using glob syntax) to shell commands:\n- Files matching the glob patterns will be automatically formatted after being modified by the assistant\n- The tool parameters are updated to reflect the formatted content, keeping the LLM's mental model in sync\n- This prevents edit conflicts caused by auto-formatting\n\nSee [docs/format-on-save-feature.md](docs/format-on-save-feature.md) for detailed documentation.\n\n**Important Notes:**\n- When launching from a folder not in this configuration, a temporary project is created automatically\n- The assistant has access to the current project (including temporary ones) plus all configured projects\n- Each chat session is permanently associated with its initial project and folder - this cannot be changed later\n- Tool syntax (native/xml/caret) is also fixed per session at creation time\n- The LLM provider selected at startup is used for the entire application session (UI switching planned for future releases)\n\n## Usage\n\n### GUI Mode (Recommended)\n\n```bash\n# Start with graphical interface\ncode-assistant --ui\n\n# Start GUI with initial task\ncode-assistant --ui --task \"Analyze the authentication system\"\n```\n\n### Terminal Mode\n\n```bash\n# Basic usage\ncode-assistant --task \"Explain the purpose of this codebase\"\n\n# With specific provider and model\ncode-assistant --task \"Add error handling\" --provider openai --model gpt-5\n```\n\n### MCP Server Mode\n\n```bash\ncode-assistant server\n```\n\n## Configuration\n\n<details>\n<summary>Claude Desktop Integration</summary>\n\nConfigure in Claude Desktop settings (**Developer** tab ‚Üí **Edit Config**):\n\n```jsonc\n{\n  \"mcpServers\": {\n    \"code-assistant\": {\n      \"command\": \"/path/to/code-assistant/target/release/code-assistant\",\n      \"args\": [\"server\"],\n      \"env\": {\n        \"PERPLEXITY_API_KEY\": \"pplx-...\", // optional, enables perplexity_ask tool\n        \"SHELL\": \"/bin/zsh\" // your login shell, required when configuring \"env\" here\n      }\n    }\n  }\n}\n```\n</details>\n\n<details>\n<summary>LLM Providers</summary>\n\n**Anthropic** (default):\n```bash\nexport ANTHROPIC_API_KEY=\"sk-ant-...\"\ncode-assistant --provider anthropic --model claude-sonnet-4-20250514\n```\n\n**OpenAI**:\n```bash\nexport OPENAI_API_KEY=\"sk-...\"\ncode-assistant --provider openai --model gpt-4o\n```\n\n**SAP AI Core**:\nCreate `~/.config/code-assistant/ai-core.json`:\n```json\n{\n  \"auth\": {\n    \"client_id\": \"<service-key-client-id>\",\n    \"client_secret\": \"<service-key-client-secret>\",\n    \"token_url\": \"https://<your-url>/oauth/token\",\n    \"api_base_url\": \"https://<your-url>/v2/inference\"\n  },\n  \"models\": {\n    \"claude-sonnet-4\": \"<deployment-id>\"\n  }\n}\n```\n\n**Ollama**:\n```bash\ncode-assistant --provider ollama --model llama2 --num-ctx 4096\n```\n\n**Other providers**: Vertex AI (Google), OpenRouter, Groq, MistralAI\n</details>\n\n<details>\n<summary>Advanced Options</summary>\n\n**Tool Syntax Modes**:\n- `--tool-syntax native`: Use the provider's built-in tool calling (most reliable, but streaming of parameters depends on provider)\n- `--tool-syntax xml`: XML-style tags for streaming of parameters\n- `--tool-syntax caret`: Triple-caret blocks for token-efficency and streaming of parameters\n\n**Session Recording**:\n```bash\n# Record session (Anthropic only)\ncode-assistant --record session.json --task \"Optimize database queries\"\n\n# Playback session\ncode-assistant --playback session.json --fast-playback\n```\n\n**Other Options**:\n- `--continue-task`: Resume from previous session state\n- `--use-diff-format`: Enable alternative diff format for file editing\n- `--verbose`: Enable detailed logging\n- `--base-url`: Custom API endpoint\n</details>\n\n## Architecture Highlights\n\nThe code-assistant features several innovative architectural decisions:\n\n**Adaptive Tool Syntax**: Automatically generates different system prompts and streaming processors based on the target LLM's capabilities, allowing the same core logic to work across providers with varying function calling support.\n\n**Smart Tool Filtering**: Real-time analysis of tool invocation patterns prevents logical errors like attempting to edit files before reading them, with the ability to truncate responses mid-stream when unsafe combinations are detected.\n\n**Multi-Threaded Streaming**: Sophisticated async architecture that handles real-time parsing of tool invocations while maintaining responsive UI updates and proper state management across multiple chat sessions.\n\n## Contributing\n\nContributions are welcome! The codebase demonstrates advanced patterns in async Rust, AI agent architecture, and cross-platform UI development.\n\n## Roadmap\n\nThis section is not really a roadmap, as the items are in no particular order.\nBelow are some topics that are likely the next focus.\n\n- **Block Replacing in Changed Files**: When streaming a tool use block, we already know the LLM attempts to use `replace_in_file` and we know in which file quite early.\n  If we also know this file has changed since the LLM last read it, we can block the attempt with an appropriate error message.\n- **Compact Tool Use Failures**: When the LLM produces an invalid tool call, or a mismatching search block, we should be able to strip the failed attempt from the message history, saving tokens.\n- **Improve UI**: There are various ways in which the UI can be improved.\n- **Add Memory Tools**: Add tools that facilitate building up a knowledge base useful work working in a given project.\n- **Security**: Ideally, the execution for all tools would run in some sort of sandbox that restricts access to the files in the project tracked by git.\n  Currently, the tools reject absolute paths, but do not check whether the relative paths point outside the project or try to access git-ignored files.\n  The `execute_command` tool runs a shell with the provided command line, which at the moment is completely unchecked.\n- **Fuzzy matching search blocks**: Investigate the benefit of fuzzy matching search blocks.\n  Currently, files are normalized (always `\\n` line endings, no trailing white space).\n  This increases the success rate of matching search blocks quite a bit, but certain ways of fuzzy matching might increase the success even more.\n  Failed matches introduce quite a bit of inefficiency, since they almost always trigger the LLM to re-read a file.\n  Even when the error output of the `replace_in_file` tool includes the complete file and tells the LLM *not* to re-read the file.\n- **Edit user messages**: Editing a user message should create a new branch in the session.\n  The user should still be able to toggle the active banches.\n- **Select in messages**: Allow to copy/paste from any message in the session.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "agents",
        "agent",
        "coding",
        "coding agents",
        "coding agent",
        "code assistant"
      ],
      "category": "coding-agents"
    },
    "tiianhk--MaxMSP-MCP-Server": {
      "owner": "tiianhk",
      "name": "MaxMSP-MCP-Server",
      "url": "https://github.com/tiianhk/MaxMSP-MCP-Server",
      "imageUrl": "",
      "description": "A coding agent for Max (Max/MSP/Jitter), which is a visual programming language for music and multimedia.",
      "stars": 95,
      "forks": 9,
      "license": "MIT License",
      "language": "Max",
      "updated_at": "2025-10-04T09:04:17Z",
      "readme_content": "# MaxMSP-MCP Server\n\nThis project uses the [Model Context Protocol](https://modelcontextprotocol.io/introduction) (MCP) to let LLMs directly understand and generate Max patches.\n\n### Understand: LLM Explaining a Max Patch\n\n\n[Video link](https://www.youtube.com/watch?v=YKXqS66zrec). Acknowledgement: the patch being explained is downloaded from [here](https://github.com/jeffThompson/MaxMSP_TeachingSketches/blob/master/02_MSP/07%20Ring%20Modulation.maxpat). Text comments in the original file are deleted.\n\n### Generate: LLM Making an FM Synth\n\n\nCheck out the [full video](https://www.youtube.com/watch?v=Ns89YuE5-to) where you can listen to the synthesised sounds.\n\nThe LLM agent has access to the official documentation of each object, as well as objects in the current patch and subpatch windows, which helps in retrieving and explaining objects, debugging, and verifying their own actions.\n\n## Installation  \n\n### Prerequisites  \n\n - Python 3.8 or newer  \n - [uv package manager](https://github.com/astral-sh/uv)  \n - Max 9 or newer (because some of the scripts require the Javascript V8 engine), we have not tested it on Max 8 or earlier versions of Max yet.  \n\n### Installing the MCP server\n\n1. Install uv:\n```\n# On macOS and Linux:\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n# On Windows:\npowershell -ExecutionPolicy ByPass -c \"irm https://astral.sh/uv/install.ps1 | iex\"\n```\n2. Clone this repository and open its directory:\n```\ngit clone https://github.com/tiianhk/MaxMSP-MCP-Server.git\ncd MaxMSP-MCP-Server\n```\n3. Start a new environment and install python dependencies:\n```\nuv venv\nuv pip install -r requirements.txt\n```\n4. Connect the MCP server to a MCP client (which hosts LLMs):\n```\n# Claude:\npython install.py --client claude\n# or Cursor:\npython install.py --client cursor\n```\nTo use other clients (check the [list](https://modelcontextprotocol.io/clients)), you need to download, mannually add the configuration file path to [here](https://github.com/tiianhk/MaxMSP-MCP-Server/blob/main/install.py#L6-L13), and connect by running `python install.py --client {your_client_name}`.\n\n### Installing to a Max patch  \n\nUse or copy from `MaxMSP_Agent/demo.maxpat`. In the first tab, click the `script npm version` message to verify that [npm](https://github.com/npm/cli) is installed. Then click `script npm install` to install the required dependencies. Switch to the second tab to access the agent. Click `script start` to initiate communication with Python. Once connected, you can interact with the LLM interface to have it explain, modify, or create Max objects within the patch.\n\n## Disclaimer\n\nThis is a third party implementation and not made by Cycling '74.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "maxmsp",
        "agents",
        "agent",
        "coding agent",
        "coding agents",
        "agents coding"
      ],
      "category": "coding-agents"
    }
  }
}