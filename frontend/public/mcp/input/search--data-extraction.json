{
  "category": "search--data-extraction",
  "categoryDisplay": "Search & Data Extraction",
  "description": "",
  "totalRepositories": 67,
  "repositories": {
    "0xdaef0f--job-searchoor": {
      "owner": "0xdaef0f",
      "name": "job-searchoor",
      "url": "https://github.com/0xDAEF0F/job-searchoor",
      "imageUrl": "",
      "description": "An MCP server for searching job listings with filters for date, keywords, remote work options, and more.",
      "stars": 47,
      "forks": 7,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-09-27T23:29:56Z",
      "readme_content": "# Job Searchoor MCP Server\n[![Twitter Follow](https://img.shields.io/twitter/follow/Alex?style=social)](https://x.com/0xdaef0f)\n\nAn MCP server implementation that provides job search functionality.\n\n![mc-demo](https://github.com/user-attachments/assets/87159634-5e4c-41af-ad54-4c5ef19bf9d0)\n\n## Tools\n\nget_jobs\n\nGet available jobs with filtering options\nInputs:\n\nsinceWhen (string): Since when to get available jobs. e.g., '1d' or '1w' (only days and weeks are supported)\nkeywords (string[], optional): Keywords to filter jobs by\nexcludeKeywords (string[], optional): Keywords to exclude from the jobs\nisRemote (boolean, optional): Whether to filter jobs by remote work\n\n## Usage with Claude Desktop\n\nAdd this to your claude_desktop_config.json:\n\n```json{\n\"mcpServers\": {\n    \"job-search\": {\n        \"command\": \"npx\",\n        \"args\": [\"-y\", \"job-searchoor\"]\n    }\n}\n```\n\nLicense\n\nThis MCP server is licensed under the MIT License. This means you are free to use, modify, and distribute the software, subject to the terms and conditions of the MIT License. For more details, please see the LICENSE file in the project repository.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "searching",
        "searchoor",
        "search",
        "job searchoor",
        "searching job",
        "job listings"
      ],
      "category": "search--data-extraction"
    },
    "Aas-ee--open-webSearch": {
      "owner": "Aas-ee",
      "name": "open-webSearch",
      "url": "https://github.com/Aas-ee/open-webSearch",
      "imageUrl": "",
      "description": "Web search using free multi-engine search (NO API KEYS REQUIRED) — Supports Bing, Baidu, DuckDuckGo, Brave, Exa, and CSDN.",
      "stars": 360,
      "forks": 63,
      "license": "Apache License 2.0",
      "language": "TypeScript",
      "updated_at": "2025-10-04T06:39:48Z",
      "readme_content": "<div align=\"center\">\n\n# Open-WebSearch MCP Server\n\n[![ModelScope](https://img.shields.io/endpoint?url=https://gist.githubusercontent.com/Aas-ee/3af09e0f4c7821fb2e9acb96483a5ff0/raw/badge.json&color=%23de5a16)](https://www.modelscope.cn/mcp/servers/Aasee1/open-webSearch)\n[![Trust Score](https://archestra.ai/mcp-catalog/api/badge/quality/Aas-ee/open-webSearch)](https://archestra.ai/mcp-catalog/aas-ee__open-websearch)\n[![smithery badge](https://smithery.ai/badge/@Aas-ee/open-websearch)](https://smithery.ai/server/@Aas-ee/open-websearch)\n![Version](https://img.shields.io/github/v/release/Aas-ee/open-websearch)\n![License](https://img.shields.io/github/license/Aas-ee/open-websearch)\n![Issues](https://img.shields.io/github/issues/Aas-ee/open-websearch)\n\n**[🇨🇳 中文](./README-zh.md) | 🇺🇸 English**\n\n</div>\n\nA Model Context Protocol (MCP) server based on multi-engine search results, supporting free web search without API keys.\n\n## Features\n\n- Web search using multi-engine results\n    - bing\n    - baidu\n    - ~~linux.do~~ temporarily unsupported\n    - csdn\n    - duckduckgo\n    - exa\n    - brave\n    - juejin\n- HTTP proxy configuration support for accessing restricted resources\n- No API keys or authentication required\n- Returns structured results with titles, URLs, and descriptions\n- Configurable number of results per search\n- Customizable default search engine\n- Support for fetching individual article content\n    - csdn\n    - github (README files)\n\n## TODO\n- Support for ~~Bing~~ (already supported), ~~DuckDuckGo~~ (already supported), ~~Exa~~ (already supported), ~~Brave~~ (already supported), Google and other search engines\n- Support for more blogs, forums, and social platforms\n- Optimize article content extraction, add support for more sites\n- ~~Support for GitHub README fetching~~ (already supported)\n\n## Installation Guide\n\n### NPX Quick Start (Recommended)\n\nThe fastest way to get started:\n\n```bash\n# Basic usage\nnpx open-websearch@latest\n\n# With environment variables (Linux/macOS)\nDEFAULT_SEARCH_ENGINE=duckduckgo ENABLE_CORS=true npx open-websearch@latest\n\n# Windows PowerShell\n$env:DEFAULT_SEARCH_ENGINE=\"duckduckgo\"; $env:ENABLE_CORS=\"true\"; npx open-websearch@latest\n\n# Windows CMD\nset MODE=stdio && set DEFAULT_SEARCH_ENGINE=duckduckgo && npx open-websearch@latest\n\n# Cross-platform (requires cross-env, Used for local development)\nnpm install -g open-websearch\nnpx cross-env DEFAULT_SEARCH_ENGINE=duckduckgo ENABLE_CORS=true open-websearch\n```\n\n**Environment Variables:**\n\n| Variable | Default                 | Options | Description |\n|----------|-------------------------|---------|-------------|\n| `ENABLE_CORS` | `false`                 | `true`, `false` | Enable CORS |\n| `CORS_ORIGIN` | `*`                     | Any valid origin | CORS origin configuration |\n| `DEFAULT_SEARCH_ENGINE` | `bing`                  | `bing`, `duckduckgo`, `exa`, `brave`, `baidu`, `csdn`, `juejin` | Default search engine |\n| `USE_PROXY` | `false`                 | `true`, `false` | Enable HTTP proxy |\n| `PROXY_URL` | `http://127.0.0.1:7890` | Any valid URL | Proxy server URL |\n| `MODE` | `both`                  | `both`, `http`, `stdio` | Server mode: both HTTP+STDIO, HTTP only, or STDIO only |\n| `PORT` | `3000`                  | 1-65535 | Server port |\n| `ALLOWED_SEARCH_ENGINES` | empty (all available) | Comma-separated engine names | Limit which search engines can be used; if the default engine is not in this list, the first allowed engine becomes the default |\n\n**Common configurations:**\n```bash\n# Enable proxy for restricted regions\nUSE_PROXY=true PROXY_URL=http://127.0.0.1:7890 npx open-websearch@latest\n\n# Full configuration\nDEFAULT_SEARCH_ENGINE=duckduckgo ENABLE_CORS=true USE_PROXY=true PROXY_URL=http://127.0.0.1:7890 PORT=8080 npx open-websearch@latest\n```\n\n### Local Installation\n\n1. Clone or download this repository\n2. Install dependencies:\n```bash\nnpm install\n```\n3. Build the server:\n```bash\nnpm run build\n```\n4. Add the server to your MCP configuration:\n\n**Cherry Studio:**\n```json\n{\n  \"mcpServers\": {\n    \"web-search\": {\n      \"name\": \"Web Search MCP\",\n      \"type\": \"streamableHttp\",\n      \"description\": \"Multi-engine web search with article fetching\",\n      \"isActive\": true,\n      \"baseUrl\": \"http://localhost:3000/mcp\"\n    }\n  }\n}\n```\n\n**VSCode (Claude Dev Extension):**\n```json\n{\n  \"mcpServers\": {\n    \"web-search\": {\n      \"transport\": {\n        \"type\": \"streamableHttp\",\n        \"url\": \"http://localhost:3000/mcp\"\n      }\n    },\n    \"web-search-sse\": {\n      \"transport\": {\n        \"type\": \"sse\",\n        \"url\": \"http://localhost:3000/sse\"\n      }\n    }\n  }\n}\n```\n\n**Claude Desktop:**\n```json\n{\n  \"mcpServers\": {\n    \"web-search\": {\n      \"transport\": {\n        \"type\": \"streamableHttp\",\n        \"url\": \"http://localhost:3000/mcp\"\n      }\n    },\n    \"web-search-sse\": {\n      \"transport\": {\n        \"type\": \"sse\",\n        \"url\": \"http://localhost:3000/sse\"\n      }\n    }\n  }\n}\n```\n\n**NPX Command Line Configuration:**\n```json\n{\n  \"mcpServers\": {\n    \"web-search\": {\n      \"args\": [\n        \"open-websearch@latest\"\n      ],\n      \"command\": \"npx\",\n      \"env\": {\n        \"MODE\": \"stdio\",\n        \"DEFAULT_SEARCH_ENGINE\": \"duckduckgo\",\n        \"ALLOWED_SEARCH_ENGINES\": \"duckduckgo,bing,exa\"\n      }\n    }\n  }\n}\n```\n\n**Local STDIO Configuration for Cherry Studio (Windows):**\n```json\n{\n  \"mcpServers\": {\n    \"open-websearch-local\": {\n      \"command\": \"node\",\n      \"args\": [\"C:/path/to/your/project/build/index.js\"],\n      \"env\": {\n        \"MODE\": \"stdio\",\n        \"DEFAULT_SEARCH_ENGINE\": \"duckduckgo\",\n        \"ALLOWED_SEARCH_ENGINES\": \"duckduckgo,bing,exa\"\n      }\n    }\n  }\n}\n```\n\n### Docker Deployment\n\nQuick deployment using Docker Compose:\n\n```bash\ndocker-compose up -d\n```\n\nOr use Docker directly:\n```bash\ndocker run -d --name web-search -p 3000:3000 -e ENABLE_CORS=true -e CORS_ORIGIN=* ghcr.io/aas-ee/open-web-search:latest\n```\n\nEnvironment variable configuration:\n\n| Variable | Default                 | Options | Description |\n|----------|-------------------------|---------|-------------|\n| `ENABLE_CORS` | `false`                 | `true`, `false` | Enable CORS |\n| `CORS_ORIGIN` | `*`                     | Any valid origin | CORS origin configuration |\n| `DEFAULT_SEARCH_ENGINE` | `bing`                  | `bing`, `duckduckgo`, `exa`, `brave` | Default search engine |\n| `USE_PROXY` | `false`                 | `true`, `false` | Enable HTTP proxy |\n| `PROXY_URL` | `http://127.0.0.1:7890` | Any valid URL | Proxy server URL |\n| `PORT` | `3000`                  | 1-65535 | Server port |\n\nThen configure in your MCP client:\n```json\n{\n  \"mcpServers\": {\n    \"web-search\": {\n      \"name\": \"Web Search MCP\",\n      \"type\": \"streamableHttp\",\n      \"description\": \"Multi-engine web search with article fetching\",\n      \"isActive\": true,\n      \"baseUrl\": \"http://localhost:3000/mcp\"\n    },\n    \"web-search-sse\": {\n      \"transport\": {\n        \"name\": \"Web Search MCP\",\n        \"type\": \"sse\",\n        \"description\": \"Multi-engine web search with article fetching\",\n        \"isActive\": true,\n        \"url\": \"http://localhost:3000/sse\"\n      }\n    }\n  }\n}\n```\n\n## Usage Guide\n\nThe server provides four tools: `search`, `fetchLinuxDoArticle`, `fetchCsdnArticle`, and `fetchGithubReadme`.\n\n### search Tool Usage\n\n```typescript\n{\n  \"query\": string,        // Search query\n  \"limit\": number,        // Optional: Number of results to return (default: 10)\n  \"engines\": string[]     // Optional: Engines to use (bing,baidu,linuxdo,csdn,duckduckgo,exa,brave,juejin) default bing\n}\n```\n\nUsage example:\n```typescript\nuse_mcp_tool({\n  server_name: \"web-search\",\n  tool_name: \"search\",\n  arguments: {\n    query: \"search content\",\n    limit: 3,  // Optional parameter\n    engines: [\"bing\", \"csdn\", \"duckduckgo\", \"exa\", \"brave\", \"juejin\"] // Optional parameter, supports multi-engine combined search\n  }\n})\n```\n\nResponse example:\n```json\n[\n  {\n    \"title\": \"Example Search Result\",\n    \"url\": \"https://example.com\",\n    \"description\": \"Description text of the search result...\",\n    \"source\": \"Source\",\n    \"engine\": \"Engine used\"\n  }\n]\n```\n\n### fetchCsdnArticle Tool Usage\n\nUsed to fetch complete content of CSDN blog articles.\n\n```typescript\n{\n  \"url\": string    // URL from CSDN search results using the search tool\n}\n```\n\nUsage example:\n```typescript\nuse_mcp_tool({\n  server_name: \"web-search\",\n  tool_name: \"fetchCsdnArticle\",\n  arguments: {\n    url: \"https://blog.csdn.net/xxx/article/details/xxx\"\n  }\n})\n```\n\nResponse example:\n```json\n[\n  {\n    \"content\": \"Example search result\"\n  }\n]\n```\n\n### fetchLinuxDoArticle Tool Usage\n\nUsed to fetch complete content of Linux.do forum articles.\n\n```typescript\n{\n  \"url\": string    // URL from linuxdo search results using the search tool\n}\n```\n\nUsage example:\n```typescript\nuse_mcp_tool({\n  server_name: \"web-search\",\n  tool_name: \"fetchLinuxDoArticle\",\n  arguments: {\n    url: \"https://xxxx.json\"\n  }\n})\n```\n\nResponse example:\n```json\n[\n  {\n    \"content\": \"Example search result\"\n  }\n]\n```\n\n### fetchGithubReadme Tool Usage\n\nUsed to fetch README content from GitHub repositories.\n\n```typescript\n{\n  \"url\": string    // GitHub repository URL (supports HTTPS, SSH formats)\n}\n```\n\nUsage example:\n```typescript\nuse_mcp_tool({\n  server_name: \"web-search\",\n  tool_name: \"fetchGithubReadme\",\n  arguments: {\n    url: \"https://github.com/Aas-ee/open-webSearch\"\n  }\n})\n```\n\nSupported URL formats:\n- HTTPS: `https://github.com/owner/repo`\n- HTTPS with .git: `https://github.com/owner/repo.git`\n- SSH: `git@github.com:owner/repo.git`\n- URLs with parameters: `https://github.com/owner/repo?tab=readme`\n\nResponse example:\n```json\n[\n  {\n    \"content\": \"<div align=\\\"center\\\">\\n\\n# Open-WebSearch MCP Server...\"\n  }\n]\n```\n\n### fetchJuejinArticle Tool Usage\n\nUsed to fetch complete content of Juejin articles.\n\n```typescript\n{\n  \"url\": string    // Juejin article URL from search results\n}\n```\n\nUsage example:\n```typescript\nuse_mcp_tool({\n  server_name: \"web-search\",\n  tool_name: \"fetchJuejinArticle\",\n  arguments: {\n    url: \"https://juejin.cn/post/7520959840199360563\"\n  }\n})\n```\n\nSupported URL format:\n- `https://juejin.cn/post/{article_id}`\n\nResponse example:\n```json\n[\n  {\n    \"content\": \"🚀 开源 AI 联网搜索工具：Open-WebSearch MCP 全新升级，支持多引擎 + 流式响应...\"\n  }\n]\n```\n\n## Usage Limitations\n\nSince this tool works by scraping multi-engine search results, please note the following important limitations:\n\n1. **Rate Limiting**:\n    - Too many searches in a short time may cause the used engines to temporarily block requests\n    - Recommendations:\n        - Maintain reasonable search frequency\n        - Use the limit parameter judiciously\n        - Add delays between searches when necessary\n\n2. **Result Accuracy**:\n    - Depends on the HTML structure of corresponding engines, may fail when engines update\n    - Some results may lack metadata like descriptions\n    - Complex search operators may not work as expected\n\n3. **Legal Terms**:\n    - This tool is for personal use only\n    - Please comply with the terms of service of corresponding engines\n    - Implement appropriate rate limiting based on your actual use case\n\n4. **Search Engine Configuration**:\n   - Default search engine can be set via the `DEFAULT_SEARCH_ENGINE` environment variable\n   - Supported engines: bing, duckduckgo, exa, brave\n   - The default engine is used when searching specific websites\n\n5. **Proxy Configuration**:\n   - HTTP proxy can be configured when certain search engines are unavailable in specific regions\n   - Enable proxy with environment variable `USE_PROXY=true`\n   - Configure proxy server address with `PROXY_URL`\n\n## Contributing\n\nWelcome to submit issue reports and feature improvement suggestions!\n\n### Contributor Guide\n\nIf you want to fork this repository and publish your own Docker image, you need to make the following configurations:\n\n#### GitHub Secrets Configuration\n\nTo enable automatic Docker image building and publishing, please add the following secrets in your GitHub repository settings (Settings → Secrets and variables → Actions):\n\n**Required Secrets:**\n- `GITHUB_TOKEN`: Automatically provided by GitHub (no setup needed)\n\n**Optional Secrets (for Alibaba Cloud ACR):**\n- `ACR_REGISTRY`: Your Alibaba Cloud Container Registry URL (e.g., `registry.cn-hangzhou.aliyuncs.com`)\n- `ACR_USERNAME`: Your Alibaba Cloud ACR username\n- `ACR_PASSWORD`: Your Alibaba Cloud ACR password\n- `ACR_IMAGE_NAME`: Your image name in ACR (e.g., `your-namespace/open-web-search`)\n\n#### CI/CD Workflow\n\nThe repository includes a GitHub Actions workflow (`.github/workflows/docker.yml`) that automatically:\n\n1. **Trigger Conditions**:\n    - Push to `main` branch\n    - Push version tags (`v*`)\n    - Manual workflow trigger\n\n2. **Build and Push to**:\n    - GitHub Container Registry (ghcr.io) - always enabled\n    - Alibaba Cloud Container Registry - only enabled when ACR secrets are configured\n\n3. **Image Tags**:\n    - `ghcr.io/your-username/open-web-search:latest`\n    - `your-acr-address/your-image-name:latest` (if ACR is configured)\n\n#### Fork and Publish Steps:\n\n1. **Fork the repository** to your GitHub account\n2. **Configure secrets** (if you need ACR publishing):\n    - Go to Settings → Secrets and variables → Actions in your forked repository\n    - Add the ACR-related secrets listed above\n3. **Push changes** to the `main` branch or create version tags\n4. **GitHub Actions will automatically build and push** your Docker image\n5. **Use your image**, update the Docker command:\n   ```bash\n   docker run -d --name web-search -p 3000:3000 -e ENABLE_CORS=true -e CORS_ORIGIN=* ghcr.io/your-username/open-web-search:latest\n   ```\n\n#### Notes:\n- If you don't configure ACR secrets, the workflow will only publish to GitHub Container Registry\n- Make sure your GitHub repository has Actions enabled\n- The workflow will use your GitHub username (converted to lowercase) as the GHCR image name\n\n<div align=\"center\">\n\n## Star History\nIf you find this project helpful, please consider giving it a ⭐ Star!\n\n[![Star History Chart](https://api.star-history.com/svg?repos=Aas-ee/open-webSearch&type=Date)](https://www.star-history.com/#Aas-ee/open-webSearch&Date)\n\n</div>\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "websearch",
        "bing",
        "search",
        "engine search",
        "websearch web",
        "web search"
      ],
      "category": "search--data-extraction"
    },
    "Bigsy--Clojars-MCP-Server": {
      "owner": "Bigsy",
      "name": "Clojars-MCP-Server",
      "url": "https://github.com/Bigsy/Clojars-MCP-Server",
      "imageUrl": "",
      "description": "Clojars MCP Server for upto date dependency information of Clojure libraries",
      "stars": 5,
      "forks": 6,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-06-25T16:21:55Z",
      "readme_content": "# Clojars MCP Server\n\n[![npm version](https://img.shields.io/npm/v/clojars-deps-server.svg)](https://www.npmjs.com/package/clojars-deps-server)\n\nA [Model Context Protocol (MCP)](https://github.com/ModelContext/protocol) server that provides tools for fetching dependency information from [Clojars](https://clojars.org/), the Clojure community's artifact repository for Cline, Roo Code, Cody, Claude Desktop etc.\n\n<a href=\"https://glama.ai/mcp/servers/i37857er6w\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/i37857er6w/badge\" alt=\"Clojars-MCP-Server MCP server\" /></a>\n\n## Installation\n\n### Installing via npx\n\nThe quickest way to use the Clojars MCP Server is to run it directly with npx:\n\n```bash\nnpx clojars-deps-server\n```\n\nYou can also install it globally:\n\n```bash\nnpm install -g clojars-deps-server\n```\n\n### Installing via Smithery\n\nTo install Clojars Dependency Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/clojars-deps-server):\n\n```bash\nnpx -y @smithery/cli install clojars-deps-server --client claude\n```\n\n### Manual Installation\n1. Clone this repository:\n```bash\ngit clone https://github.com/yourusername/clojars-deps-server.git\ncd clojars-deps-server\n```\n\n2. Install dependencies:\n```bash\nnpm install\n```\n\n3. Build the server:\n```bash\nnpm run build\n```\n\n4. Add the server to your Claude configuration:\n\nFor VSCode Claude extension, add to `cline_mcp_settings.json` (typically located at `~/Library/Application Support/Code/User/globalStorage/saoudrizwan.claude-dev/settings/` on macOS):\n```json\n{\n  \"mcpServers\": {\n    \"clojars-deps-server\": {\n      \"command\": \"node\",\n      \"args\": [\"/path/to/clojars-deps-server/build/index.js\"]\n    }\n  }\n}\n```\n\nFor Claude desktop app, add to `claude_desktop_config.json` (typically located at `~/Library/Application Support/Claude/` on macOS):\n```json\n{\n  \"mcpServers\": {\n    \"clojars-deps-server\": {\n      \"command\": \"node\",\n      \"args\": [\"/path/to/clojars-deps-server/build/index.js\"]\n    }\n  }\n}\n```\n\nAfter adding the server configuration, Claude will automatically detect and connect to the server on startup. The server's capabilities will be listed in Claude's system prompt under \"Connected MCP Servers\", making them available for use.\n\n\n## Features\n\n- Get the latest version of any Clojars dependency\n- Check if a specific version of a dependency exists\n- Get version history of dependencies with configurable limits\n- Simple, focused responses\n- Easy integration with Claude through MCP\n\n## How It Works\n\nWhen this MCP server is configured in Claude's settings, it automatically becomes available in Claude's system prompt under the \"Connected MCP Servers\" section. This makes Claude aware of the server's capabilities and allows it to use the provided tools through the `use_mcp_tool` command.\n\nThe server exposes three tools:\n\n### get_clojars_latest_version\n```json\n{\n  \"name\": \"get_clojars_latest_version\",\n  \"description\": \"Get the latest version of a Clojars dependency (Maven artifact)\",\n  \"inputSchema\": {\n    \"type\": \"object\",\n    \"properties\": {\n      \"dependency\": {\n        \"type\": \"string\",\n        \"description\": \"Clojars dependency name in format \\\"group/artifact\\\" (e.g. \\\"metosin/reitit\\\")\"\n      }\n    },\n    \"required\": [\"dependency\"]\n  }\n}\n```\n\n### check_clojars_version_exists\n```json\n{\n  \"name\": \"check_clojars_version_exists\",\n  \"description\": \"Check if a specific version of a Clojars dependency exists\",\n  \"inputSchema\": {\n    \"type\": \"object\",\n    \"properties\": {\n      \"dependency\": {\n        \"type\": \"string\",\n        \"description\": \"Clojars dependency name in format \\\"group/artifact\\\" (e.g. \\\"metosin/reitit\\\")\"\n      },\n      \"version\": {\n        \"type\": \"string\",\n        \"description\": \"Version to check (e.g. \\\"0.7.2\\\")\"\n      }\n    },\n    \"required\": [\"dependency\", \"version\"]\n  }\n}\n```\n\n### get_clojars_history\n```json\n{\n  \"name\": \"get_clojars_history\",\n  \"description\": \"Get version history of a Clojars dependency\",\n  \"inputSchema\": {\n    \"type\": \"object\",\n    \"properties\": {\n      \"dependency\": {\n        \"type\": \"string\",\n        \"description\": \"Clojars dependency name in format \\\"group/artifact\\\" (e.g. \\\"metosin/reitit\\\")\"\n      },\n      \"limit\": {\n        \"type\": \"number\",\n        \"description\": \"Number of versions to return (default: 15, max: 100)\",\n        \"minimum\": 1,\n        \"maximum\": 100\n      }\n    },\n    \"required\": [\"dependency\"]\n  }\n}\n```\n\nThe tool names and descriptions are specifically designed to help Claude understand that these tools are for retrieving version information from Clojars. When users ask about Clojars dependencies, Claude can recognize that these tools are appropriate for the task based on:\n- The tool names explicitly indicate their purpose\n- The descriptions specify they're for \"Clojars dependency (Maven artifact)\"\n- The example formats show typical Clojars dependency patterns\n\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "clojure",
        "clojars",
        "mcp",
        "clojure libraries",
        "information clojure",
        "server clojars"
      ],
      "category": "search--data-extraction"
    },
    "ConechoAI--openai-websearch-mcp": {
      "owner": "ConechoAI",
      "name": "openai-websearch-mcp",
      "url": "https://github.com/ConechoAI/openai-websearch-mcp/",
      "imageUrl": "",
      "description": "This is a Python-based MCP server that provides OpenAI `web_search` build-in tool.",
      "stars": 65,
      "forks": 13,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-29T16:14:37Z",
      "readme_content": "# OpenAI WebSearch MCP Server 🔍\n\n[![PyPI version](https://badge.fury.io/py/openai-websearch-mcp.svg)](https://badge.fury.io/py/openai-websearch-mcp)\n[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)\n[![MCP Compatible](https://img.shields.io/badge/MCP-Compatible-green.svg)](https://modelcontextprotocol.io/)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n\nAn advanced MCP server that provides intelligent web search capabilities using OpenAI's reasoning models. Perfect for AI assistants that need up-to-date information with smart reasoning capabilities.\n\n## ✨ Features\n\n- **🧠 Reasoning Model Support**: Full compatibility with OpenAI's latest reasoning models (gpt-5, gpt-5-mini, gpt-5-nano, o3, o4-mini)\n- **⚡ Smart Effort Control**: Intelligent `reasoning_effort` defaults based on use case\n- **🔄 Multi-Mode Search**: Fast iterations with gpt-5-mini or deep research with gpt-5\n- **🌍 Localized Results**: Support for location-based search customization\n- **📝 Rich Descriptions**: Complete parameter documentation for easy integration\n- **🔧 Flexible Configuration**: Environment variable support for easy deployment\n\n## 🚀 Quick Start\n\n### One-Click Installation for Claude Desktop\n\n```bash\nOPENAI_API_KEY=sk-xxxx uvx --with openai-websearch-mcp openai-websearch-mcp-install\n```\n\nReplace `sk-xxxx` with your OpenAI API key from the [OpenAI Platform](https://platform.openai.com/).\n\n## ⚙️ Configuration\n\n### Claude Desktop\n\nAdd to your `claude_desktop_config.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"openai-websearch-mcp\": {\n      \"command\": \"uvx\",\n      \"args\": [\"openai-websearch-mcp\"],\n      \"env\": {\n        \"OPENAI_API_KEY\": \"your-api-key-here\",\n        \"OPENAI_DEFAULT_MODEL\": \"gpt-5-mini\"\n      }\n    }\n  }\n}\n```\n\n### Cursor\n\nAdd to your MCP settings in Cursor:\n\n1. Open Cursor Settings (`Cmd/Ctrl + ,`)\n2. Search for \"MCP\" or go to Extensions → MCP\n3. Add server configuration:\n\n```json\n{\n  \"mcpServers\": {\n    \"openai-websearch-mcp\": {\n      \"command\": \"uvx\",\n      \"args\": [\"openai-websearch-mcp\"],\n      \"env\": {\n        \"OPENAI_API_KEY\": \"your-api-key-here\",\n        \"OPENAI_DEFAULT_MODEL\": \"gpt-5-mini\"\n      }\n    }\n  }\n}\n```\n\n### Claude Code\n\nClaude Code automatically detects MCP servers configured for Claude Desktop. Use the same configuration as above for Claude Desktop.\n\n### Local Development\n\nFor local testing, use the absolute path to your virtual environment:\n\n```json\n{\n  \"mcpServers\": {\n    \"openai-websearch-mcp\": {\n      \"command\": \"/path/to/your/project/.venv/bin/python\",\n      \"args\": [\"-m\", \"openai_websearch_mcp\"],\n      \"env\": {\n        \"OPENAI_API_KEY\": \"your-api-key-here\",\n        \"OPENAI_DEFAULT_MODEL\": \"gpt-5-mini\",\n        \"PYTHONPATH\": \"/path/to/your/project/src\"\n      }\n    }\n  }\n}\n```\n\n## 🛠️ Available Tools\n\n### `openai_web_search`\n\nIntelligent web search with reasoning model support.\n\n#### Parameters\n\n| Parameter | Type | Description | Default |\n|-----------|------|-------------|---------|\n| `input` | `string` | The search query or question to search for | *Required* |\n| `model` | `string` | AI model to use. Supports gpt-4o, gpt-4o-mini, gpt-5, gpt-5-mini, gpt-5-nano, o3, o4-mini | `gpt-5-mini` |\n| `reasoning_effort` | `string` | Reasoning effort level: low, medium, high, minimal | Smart default |\n| `type` | `string` | Web search API version | `web_search_preview` |\n| `search_context_size` | `string` | Context amount: low, medium, high | `medium` |\n| `user_location` | `object` | Optional location for localized results | `null` |\n\n## 💬 Usage Examples\n\nOnce configured, simply ask your AI assistant to search for information using natural language:\n\n### Quick Search\n> \"Search for the latest developments in AI reasoning models using openai_web_search\"\n\n### Deep Research  \n> \"Use openai_web_search with gpt-5 and high reasoning effort to provide a comprehensive analysis of quantum computing breakthroughs\"\n\n### Localized Search\n> \"Search for local tech meetups in San Francisco this week using openai_web_search\"\n\nThe AI assistant will automatically use the `openai_web_search` tool with appropriate parameters based on your request.\n\n## 🤖 Model Selection Guide\n\n### Quick Multi-Round Searches 🚀\n- **Recommended**: `gpt-5-mini` with `reasoning_effort: \"low\"`\n- **Use Case**: Fast iterations, real-time information, multiple quick queries\n- **Benefits**: Lower latency, cost-effective for frequent searches\n\n### Deep Research 🔬\n- **Recommended**: `gpt-5` with `reasoning_effort: \"medium\"` or `\"high\"`\n- **Use Case**: Comprehensive analysis, complex topics, detailed investigation\n- **Benefits**: Multi-round reasoned results, no need for agent iterations\n\n### Model Comparison\n\n| Model | Reasoning | Default Effort | Best For |\n|-------|-----------|----------------|----------|\n| `gpt-4o` | ❌ | N/A | Standard search |\n| `gpt-4o-mini` | ❌ | N/A | Basic queries |\n| `gpt-5-mini` | ✅ | `low` | Fast iterations |\n| `gpt-5` | ✅ | `medium` | Deep research |\n| `gpt-5-nano` | ✅ | `medium` | Balanced approach |\n| `o3` | ✅ | `medium` | Advanced reasoning |\n| `o4-mini` | ✅ | `medium` | Efficient reasoning |\n\n## 📦 Installation\n\n### Using uvx (Recommended)\n\n```bash\n# Install and run directly\nuvx openai-websearch-mcp\n\n# Or install globally\nuvx install openai-websearch-mcp\n```\n\n### Using pip\n\n```bash\n# Install from PyPI\npip install openai-websearch-mcp\n\n# Run the server\npython -m openai_websearch_mcp\n```\n\n### From Source\n\n```bash\n# Clone the repository\ngit clone https://github.com/yourusername/openai-websearch-mcp.git\ncd openai-websearch-mcp\n\n# Install dependencies\nuv sync\n\n# Run in development mode\nuv run python -m openai_websearch_mcp\n```\n\n## 👩‍💻 Development\n\n### Setup Development Environment\n\n```bash\n# Clone and setup\ngit clone https://github.com/yourusername/openai-websearch-mcp.git\ncd openai-websearch-mcp\n\n# Create virtual environment and install dependencies\nuv sync\n\n# Run tests\nuv run python -m pytest\n\n# Install in development mode\nuv pip install -e .\n```\n\n### Environment Variables\n\n| Variable | Description | Default |\n|----------|-------------|---------|\n| `OPENAI_API_KEY` | Your OpenAI API key | *Required* |\n| `OPENAI_DEFAULT_MODEL` | Default model to use | `gpt-5-mini` |\n\n## 🐛 Debugging\n\n### Using MCP Inspector\n\n```bash\n# For uvx installations\nnpx @modelcontextprotocol/inspector uvx openai-websearch-mcp\n\n# For pip installations\nnpx @modelcontextprotocol/inspector python -m openai_websearch_mcp\n```\n\n### Common Issues\n\n**Issue**: \"Unsupported parameter: 'reasoning.effort'\"\n**Solution**: This occurs when using non-reasoning models (gpt-4o, gpt-4o-mini) with reasoning_effort parameter. The server automatically handles this by only applying reasoning parameters to compatible models.\n\n**Issue**: \"No module named 'openai_websearch_mcp'\"\n**Solution**: Ensure you've installed the package correctly and your Python path includes the package location.\n\n## 📄 License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n\n## 🙏 Acknowledgments\n\n- 🤖 Generated with [Claude Code](https://claude.ai/code)\n- 🔥 Powered by [OpenAI's Web Search API](https://openai.com)\n- 🛠️ Built on the [Model Context Protocol](https://modelcontextprotocol.io/)\n\n---\n\n**Co-Authored-By**: Claude <noreply@anthropic.com>",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "websearch",
        "web_search",
        "openai",
        "openai web_search",
        "openai websearch",
        "websearch mcp"
      ],
      "category": "search--data-extraction"
    },
    "DappierAI--dappier-mcp": {
      "owner": "DappierAI",
      "name": "dappier-mcp",
      "url": "https://github.com/DappierAI/dappier-mcp",
      "imageUrl": "",
      "description": "Enable fast, free real-time web search and access premium data from trusted media brands—news, financial markets, sports, entertainment, weather, and more. Build powerful AI agents with Dappier.",
      "stars": 32,
      "forks": 10,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-28T11:52:36Z",
      "readme_content": "## 📽️ Watch the Demo Video (Live!)\n\n> 📌 Click the image below — use **Ctrl+Click** (or **Cmd+Click on Mac**) to open in a new tab.\n\n<a href=\"https://youtu.be/2Q_PwLFkYTQ\">\n  \n</a>\n\n# Dappier MCP Server\n\nEnable fast, free real-time web search and access premium data from trusted media brands—news, financial markets, sports, entertainment, weather, and more. Build powerful AI agents with Dappier.\n\n> Explore a wide range of data models in our marketplace at [marketplace.dappier.com](https://marketplace.dappier.com/marketplace).\n\n<br>\n\n<a href=\"https://smithery.ai/server/@DappierAI/dappier-mcp\" target=\"_blank\"><img alt=\"Smithery Badge\" src=\"https://smithery.ai/badge/@DappierAI/dappier-mcp\"></a>\n\n<br>\n\n<a href=\"https://glama.ai/mcp/servers/@DappierAI/dappier-mcp\" target=\"_blank\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@DappierAI/dappier-mcp/badge\" />\n</a>\n\n<br>\n\n[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/dappierai-dappier-mcp-badge.png)](https://mseep.ai/app/dappierai-dappier-mcp)\n\n<br>\n\n## Getting Started\n\nGet Dappier API Key. Head to [Dappier](https://platform.dappier.com/profile/api-keys) to sign up and generate an API key.\n\n\n## Installing via Smithery\n\nTo install dappier-mcp for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@DappierAI/dappier-mcp):\n\n```bash\nnpx -y @smithery/cli install @DappierAI/dappier-mcp --client claude\n```\n\n## Installation\n\nInstall `uv` first.\n\n**MacOS/Linux**:\n```bash\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n```\n\n**Windows**:\n```bash\npowershell -ExecutionPolicy ByPass -c \"irm https://astral.sh/uv/install.ps1 | iex\"\n```\n\n## Usage\n\n### Claude Desktop\n\nUpdate your Claude configuration file (`claude_desktop_config.json`) with the following content:\n\n```json\n{\n  \"mcpServers\": {\n    \"dappier\": {\n      \"command\": \"uvx\",\n      \"args\": [\"dappier-mcp\"],\n      \"env\": {\n        \"DAPPIER_API_KEY\": \"YOUR_API_KEY_HERE\"\n      }\n    }\n  }\n}\n```\n\n> **Hint**: You may need to provide the full path to the `uvx` executable in the `command` field. You can obtain this by running `which uvx` on macOS/Linux or `where uvx` on Windows.\n\n**Configuration file location:**\n- **macOS**: `~/Library/Application Support/Claude/claude_desktop_config.json`\n- **Windows**: `%APPDATA%\\Claude\\claude_desktop_config.json`\n\n**Accessing via application:**\n- **macOS**:\n  1. Open the Claude Desktop application.\n  2. In the menu bar, click on `Claude` > `Settings`.\n  3. Navigate to the `Developer` tab.\n  4. Click on `Edit Config` to open the configuration file in your default text editor.\n- **Windows**:\n  1. Open the Claude Desktop application.\n  2. Click on the gear icon to access `Settings`.\n  3. Navigate to the `Developer` tab.\n  4. Click on `Edit Config` to open the configuration file in your default text editor.\n\n> **Note**: If the `Developer` tab is not visible, ensure you're using the latest version of Claude Desktop. \n\n---\n\n### Cursor\n\nUpdate your Cursor configuration file (`mcp.json`) with the following content:\n\n```json\n{\n  \"mcpServers\": {\n    \"dappier\": {\n      \"command\": \"uvx\",\n      \"args\": [\"dappier-mcp\"],\n      \"env\": {\n        \"DAPPIER_API_KEY\": \"YOUR_API_KEY_HERE\"\n      }\n    }\n  }\n}\n```\n\n> **Hint**: You may need to provide the full path to the `uvx` executable in the `command` field. You can obtain this by running `which uvx` on macOS/Linux or `where uvx` on Windows.\n\n**Configuration file location:**\n- **Global Configuration**:\n  - **macOS**: `~/.cursor/mcp.json`\n  - **Windows**: `%USERPROFILE%\\.cursor\\mcp.json`\n- **Project-Specific Configuration**:\n  - Place the `mcp.json` file inside the `.cursor` directory within your project folder: `<project-root>/.cursor/mcp.json`\n\n**Accessing via application:**\n1. Open the Cursor application.\n2. Navigate to `Settings` > `MCP`.\n3. Click on `Add New Global MCP Server`.\n4. The application will open the `mcp.json` file in your default text editor for editing.\n\n> **Note**: On Windows, if the project-level configuration is not recognized, consider adding the MCP server through the Cursor settings interface. \n\n---\n\n### Windsurf\n\nUpdate your Windsurf configuration file (`mcp_config.json`) with the following content:\n\n```json\n{\n  \"mcpServers\": {\n    \"dappier\": {\n      \"command\": \"uvx\",\n      \"args\": [\"dappier-mcp\"],\n      \"env\": {\n        \"DAPPIER_API_KEY\": \"YOUR_API_KEY_HERE\"\n      }\n    }\n  }\n}\n```\n\n> **Hint**: You may need to provide the full path to the `uvx` executable in the `command` field. You can obtain this by running `which uvx` on macOS/Linux or `where uvx` on Windows.\n\n**Configuration file location:**\n- **macOS**: `~/.codeium/windsurf/mcp_config.json`\n- **Windows**: `%USERPROFILE%\\.codeium\\windsurf\\mcp_config.json`\n\n**Accessing via application:**\n1. Open the Windsurf application.\n2. Navigate to `Settings` > `Cascade`.\n3. Scroll down to the `Model Context Protocol (MCP) Servers` section.\n4. Click on `View raw config` to open the `mcp_config.json` file in your default text editor.\n\n> **Note**: After editing the configuration file, click the `Refresh` button in the MCP Servers section to apply the changes. \n\n## Features\n\nThe Dappier MCP Remote Server provides powerful real-time capabilities out of the box — no training or fine-tuning needed. Use it to build live, interactive tools powered by the latest web data, financial markets, or AI-curated content.\n\n### Real-Time Web Search  \n**Model ID:** `am_01j06ytn18ejftedz6dyhz2b15`  \n\nSearch the live web using Dappier’s AI-powered index. Get real-time access to:\n\n- Breaking news from across the globe  \n- Weather forecasts and local updates  \n- Travel alerts and flight info  \n- Trending topics and viral content  \n- Online deals and shopping highlights  \n\nIdeal for use cases like news agents, travel planners, alert bots, and more.\n\n### Stock Market Insights  \n**Model ID:** `am_01j749h8pbf7ns8r1bq9s2evrh`  \n\nThis model delivers instant access to market data, financial headlines, and trade insights. Perfect for portfolio dashboards, trading copilots, and investment tools.\n\nIt provides:\n\n- Real-time stock prices  \n- Financial news and company updates  \n- Trade signals and trends  \n- Market movement summaries  \n- AI-curated analysis using live data from Polygon.io  \n\n### AI-Powered Content Recommendations  \n\nChoose from several domain-specific AI models tailored for content discovery, summarization, and feed generation.\n\n#### Sports News  \n**Model ID:** `dm_01j0pb465keqmatq9k83dthx34`  \nStay updated with real-time sports headlines, game recaps, and expert analysis.\n\n#### Lifestyle Updates  \n**Model ID:** `dm_01j0q82s4bfjmsqkhs3ywm3x6y`  \nExplore curated lifestyle content — covering wellness, entertainment, and everyday inspiration.\n\n#### iHeartDogs AI  \n**Model ID:** `dm_01j1sz8t3qe6v9g8ad102kvmqn`  \nYour intelligent dog care assistant — access training tips, health advice, and behavior insights.\n\n#### iHeartCats AI  \n**Model ID:** `dm_01j1sza0h7ekhaecys2p3y0vmj`  \nAn expert AI for all things feline — from nutrition to playtime to grooming routines.\n\n#### GreenMonster  \n**Model ID:** `dm_01j5xy9w5sf49bm6b1prm80m27`  \nDiscover sustainable lifestyle ideas, ethical choices, and green innovations.\n\n#### WISH-TV AI  \n**Model ID:** `dm_01jagy9nqaeer9hxx8z1sk1jx6`  \nTap into hyperlocal news, politics, culture, health, and multicultural updates.\n\nEach recommendation includes:\n\n- A clear title and concise summary  \n- The original publication date  \n- The trusted source and domain  \n- Image preview (if available)  \n- A relevance score for prioritization\n\nAdvanced options let you:\n\n- Tune the search algorithm (`semantic`, `most_recent`, `trending`, etc.)  \n- Focus results on a specific domain (`ref`)  \n- Adjust how many results you want (`similarity_top_k`, `num_articles_ref`)  \n\n## Debugging\n\nRun the MCP inspector to debug the server:\n```bash\nnpx @modelcontextprotocol/inspector uvx dappier-mcp\n```\n\n## Contributing\n\nWe welcome contributions to expand and improve the Dappier MCP Server. Whether you want to add new search capabilities, enhance existing functionality, or improve documentation, your input is valuable.\n\nFor examples of other MCP servers and implementation patterns, see:\n[https://github.com/modelcontextprotocol/servers](https://github.com/modelcontextprotocol/servers)\n\nPull requests are welcome! Feel free to contribute new ideas, bug fixes, or enhancements.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "dappierai",
        "dappier",
        "search",
        "agents dappier",
        "dappierai dappier",
        "dappier mcp"
      ],
      "category": "search--data-extraction"
    },
    "Dumpling-AI--mcp-server-dumplingai": {
      "owner": "Dumpling-AI",
      "name": "mcp-server-dumplingai",
      "url": "https://github.com/Dumpling-AI/mcp-server-dumplingai",
      "imageUrl": "",
      "description": "Access data, web scraping, and document conversion APIs by [Dumpling AI](https://www.dumplingai.com/)",
      "stars": 27,
      "forks": 6,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-09-07T08:58:27Z",
      "readme_content": "# Dumpling AI MCP Server\n\nA Model Context Protocol (MCP) server implementation that integrates with Dumpling AI for data scraping, content processing, knowledge management, AI agents, and code execution capabilities.\n\n[![smithery badge](https://smithery.ai/badge/@Dumpling-AI/mcp-server-dumplingai)](https://smithery.ai/server/@Dumpling-AI/mcp-server-dumplingai)\n\n## Features\n\n- Complete integration with all Dumpling AI API endpoints\n- Data APIs for YouTube transcripts, search, autocomplete, maps, places, news, and reviews\n- Web scraping with support for scraping, crawling, screenshots, and structured data extraction\n- Document conversion tools for text extraction, PDF operations, video processing\n- Extract data from documents, images, audio, and video\n- AI capabilities including agent completions, knowledge base management, and image generation\n- Developer tools for running JavaScript and Python code in a secure environment\n- Automatic error handling and detailed response formatting\n\n## Installation\n\n### Installing via Smithery\n\nTo install mcp-server-dumplingai for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@Dumpling-AI/mcp-server-dumplingai):\n\n```bash\nnpx -y @smithery/cli install @Dumpling-AI/mcp-server-dumplingai --client claude\n```\n\n### Running with npx\n\n```bash\nenv DUMPLING_API_KEY=your_api_key npx -y mcp-server-dumplingai\n```\n\n### Manual Installation\n\n```bash\nnpm install -g mcp-server-dumplingai\n```\n\n### Running on Cursor\n\nConfiguring Cursor 🖥️ Note: Requires Cursor version 0.45.6+\n\nTo configure Dumpling AI MCP in Cursor:\n\n1. Open Cursor Settings\n2. Go to Features > MCP Servers\n3. Click \"+ Add New MCP Server\"\n4. Enter the following:\n\n```\n{\n  \"mcpServers\": {\n    \"dumplingai\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"mcp-server-dumplingai\"],\n      \"env\": {\n        \"DUMPLING_API_KEY\": \"<your-api-key>\"\n      }\n    }\n  }\n}\n```\n\n> If you are using Windows and are running into issues, try `cmd /c \"set DUMPLING_API_KEY=your-api-key && npx -y mcp-server-dumplingai\"`\n\nReplace `your-api-key` with your Dumpling AI API key.\n\n## Configuration\n\n### Environment Variables\n\n- `DUMPLING_API_KEY`: Your Dumpling AI API key (required)\n\n## Available Tools\n\n### Data APIs\n\n#### 1. Get YouTube Transcript (`get-youtube-transcript`)\n\nExtract transcripts from YouTube videos with optional timestamps.\n\n```json\n{\n  \"name\": \"get-youtube-transcript\",\n  \"arguments\": {\n    \"videoUrl\": \"https://www.youtube.com/watch?v=dQw4w9WgXcQ\",\n    \"includeTimestamps\": true,\n    \"timestampsToCombine\": 3,\n    \"preferredLanguage\": \"en\"\n  }\n}\n```\n\n#### 2. Search (`search`)\n\nPerform Google web searches and optionally scrape content from results.\n\n```json\n{\n  \"name\": \"search\",\n  \"arguments\": {\n    \"query\": \"machine learning basics\",\n    \"country\": \"us\",\n    \"language\": \"en\",\n    \"dateRange\": \"pastMonth\",\n    \"scrapeResults\": true,\n    \"numResultsToScrape\": 3,\n    \"scrapeOptions\": {\n      \"format\": \"markdown\",\n      \"cleaned\": true\n    }\n  }\n}\n```\n\n#### 3. Get Autocomplete (`get-autocomplete`)\n\nGet Google search autocomplete suggestions for a query.\n\n```json\n{\n  \"name\": \"get-autocomplete\",\n  \"arguments\": {\n    \"query\": \"how to learn\",\n    \"country\": \"us\",\n    \"language\": \"en\",\n    \"location\": \"New York\"\n  }\n}\n```\n\n#### 4. Search Maps (`search-maps`)\n\nSearch Google Maps for locations and businesses.\n\n```json\n{\n  \"name\": \"search-maps\",\n  \"arguments\": {\n    \"query\": \"coffee shops\",\n    \"gpsPositionZoom\": \"37.7749,-122.4194,14z\",\n    \"language\": \"en\",\n    \"page\": 1\n  }\n}\n```\n\n#### 5. Search Places (`search-places`)\n\nSearch for places with more detailed information.\n\n```json\n{\n  \"name\": \"search-places\",\n  \"arguments\": {\n    \"query\": \"hotels in paris\",\n    \"country\": \"fr\",\n    \"language\": \"en\",\n    \"page\": 1\n  }\n}\n```\n\n#### 6. Search News (`search-news`)\n\nSearch for news articles with customizable parameters.\n\n```json\n{\n  \"name\": \"search-news\",\n  \"arguments\": {\n    \"query\": \"climate change\",\n    \"country\": \"us\",\n    \"language\": \"en\",\n    \"dateRange\": \"pastWeek\"\n  }\n}\n```\n\n#### 7. Get Google Reviews (`get-google-reviews`)\n\nRetrieve Google reviews for businesses or places.\n\n```json\n{\n  \"name\": \"get-google-reviews\",\n  \"arguments\": {\n    \"businessName\": \"Eiffel Tower\",\n    \"location\": \"Paris, France\",\n    \"limit\": 10,\n    \"sortBy\": \"relevance\"\n  }\n}\n```\n\n### Web Scraping\n\n#### 8. Scrape (`scrape`)\n\nExtract content from a web page with formatting options.\n\n```json\n{\n  \"name\": \"scrape\",\n  \"arguments\": {\n    \"url\": \"https://example.com\",\n    \"format\": \"markdown\",\n    \"cleaned\": true,\n    \"renderJs\": true\n  }\n}\n```\n\n#### 9. Crawl (`crawl`)\n\nRecursively crawl websites and extract content with customizable parameters.\n\n```json\n{\n  \"name\": \"crawl\",\n  \"arguments\": {\n    \"baseUrl\": \"https://example.com\",\n    \"maxPages\": 10,\n    \"crawlBeyondBaseUrl\": false,\n    \"depth\": 2,\n    \"scrapeOptions\": {\n      \"format\": \"markdown\",\n      \"cleaned\": true,\n      \"renderJs\": true\n    }\n  }\n}\n```\n\n#### 10. Screenshot (`screenshot`)\n\nCapture screenshots of web pages with customizable viewport and format options.\n\n```json\n{\n  \"name\": \"screenshot\",\n  \"arguments\": {\n    \"url\": \"https://example.com\",\n    \"width\": 1280,\n    \"height\": 800,\n    \"fullPage\": true,\n    \"format\": \"png\",\n    \"waitFor\": 1000\n  }\n}\n```\n\n#### 11. Extract (`extract`)\n\nExtract structured data from web pages using AI-powered instructions.\n\n```json\n{\n  \"name\": \"extract\",\n  \"arguments\": {\n    \"url\": \"https://example.com/products\",\n    \"instructions\": \"Extract all product names, prices, and descriptions from this page\",\n    \"schema\": {\n      \"products\": [\n        {\n          \"name\": \"string\",\n          \"price\": \"number\",\n          \"description\": \"string\"\n        }\n      ]\n    },\n    \"renderJs\": true\n  }\n}\n```\n\n### Document Conversion\n\n#### 12. Doc to Text (`doc-to-text`)\n\nConvert documents to plaintext with optional OCR.\n\n```json\n{\n  \"name\": \"doc-to-text\",\n  \"arguments\": {\n    \"url\": \"https://example.com/document.pdf\",\n    \"options\": {\n      \"ocr\": true,\n      \"language\": \"en\"\n    }\n  }\n}\n```\n\n#### 13. Convert to PDF (`convert-to-pdf`)\n\nConvert various file formats to PDF.\n\n```json\n{\n  \"name\": \"convert-to-pdf\",\n  \"arguments\": {\n    \"url\": \"https://example.com/document.docx\",\n    \"format\": \"docx\",\n    \"options\": {\n      \"quality\": 90,\n      \"pageSize\": \"A4\",\n      \"margin\": 10\n    }\n  }\n}\n```\n\n#### 14. Merge PDFs (`merge-pdfs`)\n\nCombine multiple PDFs into a single document.\n\n```json\n{\n  \"name\": \"merge-pdfs\",\n  \"arguments\": {\n    \"urls\": [\"https://example.com/doc1.pdf\", \"https://example.com/doc2.pdf\"],\n    \"options\": {\n      \"addPageNumbers\": true,\n      \"addTableOfContents\": true\n    }\n  }\n}\n```\n\n#### 15. Trim Video (`trim-video`)\n\nExtract a specific clip from a video.\n\n```json\n{\n  \"name\": \"trim-video\",\n  \"arguments\": {\n    \"url\": \"https://example.com/video.mp4\",\n    \"startTime\": 30,\n    \"endTime\": 60,\n    \"output\": \"mp4\",\n    \"options\": {\n      \"quality\": 720,\n      \"fps\": 30\n    }\n  }\n}\n```\n\n#### 16. Extract Document (`extract-document`)\n\nExtract specific content from documents in various formats.\n\n```json\n{\n  \"name\": \"extract-document\",\n  \"arguments\": {\n    \"url\": \"https://example.com/document.pdf\",\n    \"format\": \"structured\",\n    \"options\": {\n      \"ocr\": true,\n      \"language\": \"en\",\n      \"includeMetadata\": true\n    }\n  }\n}\n```\n\n#### 17. Extract Image (`extract-image`)\n\nExtract text and information from images.\n\n```json\n{\n  \"name\": \"extract-image\",\n  \"arguments\": {\n    \"url\": \"https://example.com/image.jpg\",\n    \"extractionType\": \"text\",\n    \"options\": {\n      \"language\": \"en\",\n      \"detectOrientation\": true\n    }\n  }\n}\n```\n\n#### 18. Extract Audio (`extract-audio`)\n\nTranscribe and extract information from audio files.\n\n```json\n{\n  \"name\": \"extract-audio\",\n  \"arguments\": {\n    \"url\": \"https://example.com/audio.mp3\",\n    \"language\": \"en\",\n    \"options\": {\n      \"model\": \"enhanced\",\n      \"speakerDiarization\": true,\n      \"wordTimestamps\": true\n    }\n  }\n}\n```\n\n#### 19. Extract Video (`extract-video`)\n\nExtract content from videos including transcripts, scenes, and objects.\n\n```json\n{\n  \"name\": \"extract-video\",\n  \"arguments\": {\n    \"url\": \"https://example.com/video.mp4\",\n    \"extractionType\": \"transcript\",\n    \"options\": {\n      \"language\": \"en\",\n      \"speakerDiarization\": true\n    }\n  }\n}\n```\n\n#### 20. Read PDF Metadata (`read-pdf-metadata`)\n\nExtract metadata from PDF files.\n\n```json\n{\n  \"name\": \"read-pdf-metadata\",\n  \"arguments\": {\n    \"url\": \"https://example.com/document.pdf\",\n    \"includeExtended\": true\n  }\n}\n```\n\n#### 21. Write PDF Metadata (`write-pdf-metadata`)\n\nUpdate metadata in PDF files.\n\n```json\n{\n  \"name\": \"write-pdf-metadata\",\n  \"arguments\": {\n    \"url\": \"https://example.com/document.pdf\",\n    \"metadata\": {\n      \"title\": \"New Title\",\n      \"author\": \"John Doe\",\n      \"keywords\": [\"keyword1\", \"keyword2\"]\n    }\n  }\n}\n```\n\n### AI\n\n#### 22. Generate Agent Completion (`generate-agent-completion`)\n\nGet AI agent completions with optional tool definitions.\n\n```json\n{\n  \"name\": \"generate-agent-completion\",\n  \"arguments\": {\n    \"prompt\": \"How can I improve my website's SEO?\",\n    \"model\": \"gpt-4\",\n    \"temperature\": 0.7,\n    \"maxTokens\": 500,\n    \"context\": [\"The website is an e-commerce store selling handmade crafts.\"]\n  }\n}\n```\n\n#### 23. Search Knowledge Base (`search-knowledge-base`)\n\nSearch a knowledge base for relevant information.\n\n```json\n{\n  \"name\": \"search-knowledge-base\",\n  \"arguments\": {\n    \"kbId\": \"kb_12345\",\n    \"query\": \"How to optimize database performance\",\n    \"limit\": 5,\n    \"similarityThreshold\": 0.7\n  }\n}\n```\n\n#### 24. Add to Knowledge Base (`add-to-knowledge-base`)\n\nAdd entries to a knowledge base.\n\n```json\n{\n  \"name\": \"add-to-knowledge-base\",\n  \"arguments\": {\n    \"kbId\": \"kb_12345\",\n    \"entries\": [\n      {\n        \"text\": \"MongoDB is a document-based NoSQL database.\",\n        \"metadata\": {\n          \"source\": \"MongoDB documentation\",\n          \"category\": \"databases\"\n        }\n      }\n    ],\n    \"upsert\": true\n  }\n}\n```\n\n#### 25. Generate AI Image (`generate-ai-image`)\n\nGenerate images using AI models.\n\n```json\n{\n  \"name\": \"generate-ai-image\",\n  \"arguments\": {\n    \"prompt\": \"A futuristic city with flying cars and neon lights\",\n    \"width\": 1024,\n    \"height\": 1024,\n    \"numImages\": 1,\n    \"quality\": \"hd\",\n    \"style\": \"photorealistic\"\n  }\n}\n```\n\n#### 26. Generate Image (`generate-image`)\n\nGenerate images using various AI providers.\n\n```json\n{\n  \"name\": \"generate-image\",\n  \"arguments\": {\n    \"prompt\": \"A golden retriever in a meadow of wildflowers\",\n    \"provider\": \"dalle\",\n    \"width\": 1024,\n    \"height\": 1024,\n    \"numImages\": 1\n  }\n}\n```\n\n### Developer Tools\n\n#### 27. Run JavaScript Code (`run-js-code`)\n\nExecute JavaScript code with optional dependencies.\n\n```json\n{\n  \"name\": \"run-js-code\",\n  \"arguments\": {\n    \"code\": \"const result = [1, 2, 3, 4].reduce((sum, num) => sum + num, 0); console.log(`Sum: ${result}`); return result;\",\n    \"dependencies\": {\n      \"lodash\": \"^4.17.21\"\n    },\n    \"timeout\": 5000\n  }\n}\n```\n\n#### 28. Run Python Code (`run-python-code`)\n\nExecute Python code with optional dependencies.\n\n```json\n{\n  \"name\": \"run-python-code\",\n  \"arguments\": {\n    \"code\": \"import numpy as np\\narr = np.array([1, 2, 3, 4, 5])\\nmean = np.mean(arr)\\nprint(f'Mean: {mean}')\\nreturn mean\",\n    \"dependencies\": [\"numpy\", \"pandas\"],\n    \"timeout\": 10000,\n    \"saveOutputFiles\": true\n  }\n}\n```\n\n## Error Handling\n\nThe server provides robust error handling:\n\n- Detailed error messages with HTTP status codes\n- API key validation\n- Input validation using Zod schemas\n- Network error handling with descriptive messages\n\nExample error response:\n\n```json\n{\n  \"content\": [\n    {\n      \"type\": \"text\",\n      \"text\": \"Error: Failed to fetch YouTube transcript: 404 Not Found\"\n    }\n  ],\n  \"isError\": true\n}\n```\n\n## Development\n\n```bash\n# Install dependencies\nnpm install\n\n# Build\nnpm run build\n\n```\n\n## License\n\nMIT License - see LICENSE file for details\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "scraping",
        "apis",
        "search",
        "server dumplingai",
        "apis dumpling",
        "dumplingai access"
      ],
      "category": "search--data-extraction"
    },
    "Himalayas-App--himalayas-mcp": {
      "owner": "Himalayas-App",
      "name": "himalayas-mcp",
      "url": "https://github.com/Himalayas-App/himalayas-mcp",
      "imageUrl": "",
      "description": "Access tens of thousands of remote job listings and company information. This public MCP server provides real-time access to Himalayas' remote jobs database.",
      "stars": 9,
      "forks": 2,
      "license": "No License",
      "language": "",
      "updated_at": "2025-09-20T10:52:19Z",
      "readme_content": "# Himalayas Remote Jobs MCP Server\n\nAccess thousands of remote job listings and company information directly from your AI coding assistant! This public MCP server provides real-time access to Himalayas.app's remote job database.\n\n🌐 **Public Server URL:** `https://mcp.himalayas.app/sse`\n\n[![Install MCP Server](https://cursor.com/deeplink/mcp-install-dark.svg)](https://cursor.com/install-mcp?name=himalayas&config=eyJjb21tYW5kIjoibnB4IG1jcC1yZW1vdGUgaHR0cHM6Ly9tY3AuaGltYWxheWFzLmFwcC9zc2UifQ%3D%3D)\n\n## Available Tools\n\n### 🔍 Job Search Tools\n\n#### `search_jobs`\nSearch for specific jobs using keywords with advanced filtering.\n\n**Parameters:**\n- `keyword` (string, optional): Search term (e.g., 'Python', 'React', 'Product Manager', 'Data Scientist')\n- `page` (number, optional): Page number for pagination (default: 1)\n- `country` (string, optional): Filter jobs by country\n- `worldwide` (boolean, optional): Show ONLY 100% remote jobs available worldwide\n\n**Example usage:**\n- \"Search for Python developer jobs\"\n- \"Find React jobs in the United States\"\n- \"Look for product manager positions worldwide\"\n\n#### `get_jobs`\nRetrieve the latest remote job listings with optional filtering.\n\n**Parameters:**\n- `page` (number, optional): Page number for pagination (default: 1)\n- `country` (string, optional): Filter jobs by country (e.g., 'Canada', 'United States', 'UK', 'Germany')\n- `worldwide` (boolean, optional): Show ONLY 100% remote jobs available worldwide (default: false)\n\n**Example usage:**\n- \"Get remote jobs in Canada\"\n- \"Show me worldwide remote opportunities\"\n- \"Find jobs on page 2\"\n\n### 🏢 Company Search Tools\n\n#### `search_companies`\nSearch for specific companies using keywords.\n\n**Parameters:**\n- `keyword` (string, optional): Search term (e.g., 'startup', 'fintech', 'AI', company name)\n- `page` (number, optional): Page number for pagination (default: 1)\n- `country` (string, optional): Filter companies by country\n- `worldwide` (boolean, optional): Show only companies with 100% remote jobs available worldwide\n\n**Example usage:**\n- \"Search for AI startups\"\n- \"Find fintech companies with remote jobs\"\n- \"Look for companies named 'Stripe'\"\n\n#### `get_companies`\nBrowse remote-friendly companies with optional filtering.\n\n**Parameters:**\n- `page` (number, optional): Page number for pagination (default: 1)\n- `country` (string, optional): Filter companies by country\n- `worldwide` (boolean, optional): Show only companies with 100% remote jobs available worldwide\n\n**Example usage:**\n- \"Show me remote companies in Europe\"\n- \"Find companies with worldwide remote jobs\"\n\n## Setup Instructions\n\n### 🖥️ Claude Desktop\n\n1. Open Claude Desktop and navigate to **Settings → Developer → Edit Config**\n2. Replace the content with this configuration:\n\n```json\n{\n  \"mcpServers\": {\n    \"himalayas\": {\n      \"command\": \"npx\",\n      \"args\": [\"mcp-remote\", \"https://mcp.himalayas.app/sse\"]\n    }\n  }\n}\n```\n\n3. Save the file and restart Claude Desktop (Cmd/Ctrl + R)\n4. When Claude restarts, a browser window will open for OAuth login - complete the authorization\n5. You'll see the Himalayas tools available by clicking the tools icon (🔨) in the bottom right\n\n### ⚡ Cursor\n\n1. Open Cursor and go to **Settings → Features → Rules for AI**\n2. Choose **Type: \"Command\"**\n3. In the **Command** field, enter:\n```bash\nnpx mcp-remote https://mcp.himalayas.app/sse\n```\n4. Save the configuration and restart Cursor\n5. Complete the OAuth flow when prompted\n\n### 🌊 Windsurf\n\n1. Edit your `mcp_config.json` file\n2. Add this configuration:\n\n```json\n{\n  \"mcpServers\": {\n    \"himalayas\": {\n      \"command\": \"npx\",\n      \"args\": [\"mcp-remote\", \"https://mcp.himalayas.app/sse\"]\n    }\n  }\n}\n```\n\n3. Save the file and restart Windsurf\n4. Complete the OAuth authorization when prompted\n\n## Example Conversations\n\nOnce connected, you can have natural conversations with your AI assistant:\n\n### Job Searching\n- *\"I'm looking for remote Python developer jobs in Europe\"*\n- *\"Show me the latest remote data science positions\"*\n- *\"Find part-time remote marketing jobs worldwide\"*\n- *\"Search for senior software engineer roles in Canada\"*\n\n### Company Research\n- *\"Find AI startups that offer remote work\"*\n- *\"Show me fintech companies with remote opportunities\"*\n- *\"Look for companies in Germany that hire remotely\"*\n- *\"Find verified companies with 4-day work weeks\"*\n\n### Advanced Queries\n- *\"Compare remote job opportunities between the US and UK\"*\n- *\"Find companies that offer both remote work and competitive salaries\"*\n- *\"Show me the tech stack used by remote-first companies\"*\n\n## What You'll Get\n\nEach response includes rich, formatted information:\n\n### Job Listings\n- 🚀 Job title and company name\n- 💼 Employment type (Full-time, Part-time, Contract)\n- 🌍 Location restrictions or worldwide availability\n- 💰 Salary ranges and currency\n- 🛠️ Required skills and technologies\n- 🔗 Direct application links\n- 🏢 Company verification status\n\n### Company Profiles\n- 🏢 Company name and verification status\n- 👥 Company size and founding year\n- 🌍 Locations and remote work policies\n- 🔥 Number of open positions\n- 🎁 Benefits and perks information\n- ⚡ Technology stacks used\n- 🌐 Company website and social links\n\n## Troubleshooting\n\n### Connection Issues\n- Ensure you have Node.js installed for the `npx` command\n- Try restarting your AI assistant after configuration changes\n- Clear authentication cache if needed: `rm -rf ~/.mcp-auth`\n\n### Authentication Problems\n- Complete the OAuth flow in the browser window that opens\n- Make sure you're using the correct server URL: `https://mcp.himalayas.app/sse`\n- Check that your internet connection is stable\n\n### Tool Not Appearing\n- Verify the configuration file syntax is correct (valid JSON)\n- Restart your AI assistant completely\n- Check the tools icon/menu in your AI assistant's interface\n\n## Support\n\nIf you encounter any issues:\n1. Check the troubleshooting section above\n2. Verify your configuration matches the examples exactly\n3. Try the connection with the [MCP Inspector](https://modelcontextprotocol.io/docs/tools/inspector) first\n\n---\n\n**Powered by [Himalayas.app](https://himalayas.app)** - The best place to find remote jobs and companies 🏔️\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "himalayas",
        "jobs",
        "listings",
        "access himalayas",
        "himalayas mcp",
        "himalayas app"
      ],
      "category": "search--data-extraction"
    },
    "Ihor-Sokoliuk--MCP-SearXNG": {
      "owner": "Ihor-Sokoliuk",
      "name": "MCP-SearXNG",
      "url": "https://github.com/ihor-sokoliuk/mcp-searxng",
      "imageUrl": "",
      "description": "A Model Context Protocol Server for [SearXNG](https://docs.searxng.org)",
      "stars": 247,
      "forks": 48,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-10-03T04:28:35Z",
      "readme_content": "# SearXNG MCP Server\n\nAn [MCP server](https://modelcontextprotocol.io/introduction) implementation that integrates the [SearXNG](https://docs.searxng.org) API, providing web search capabilities.\n\n[![https://nodei.co/npm/mcp-searxng.png?downloads=true&downloadRank=true&stars=true](https://nodei.co/npm/mcp-searxng.png?downloads=true&downloadRank=true&stars=true)](https://www.npmjs.com/package/mcp-searxng)\n\n[![https://badgen.net/docker/pulls/isokoliuk/mcp-searxng](https://badgen.net/docker/pulls/isokoliuk/mcp-searxng)](https://hub.docker.com/r/isokoliuk/mcp-searxng)\n\n<a href=\"https://glama.ai/mcp/servers/0j7jjyt7m9\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/0j7jjyt7m9/badge\" alt=\"SearXNG Server MCP server\" /></a>\n\n## Features\n\n- **Web Search**: General queries, news, articles, with pagination.\n- **URL Content Reading**: Advanced content extraction with pagination, section filtering, and heading extraction.\n- **Intelligent Caching**: URL content is cached with TTL (Time-To-Live) to improve performance and reduce redundant requests.\n- **Pagination**: Control which page of results to retrieve.\n- **Time Filtering**: Filter results by time range (day, month, year).\n- **Language Selection**: Filter results by preferred language.\n- **Safe Search**: Control content filtering level for search results.\n\n## Tools\n\n- **searxng_web_search**\n  - Execute web searches with pagination\n  - Inputs:\n    - `query` (string): The search query. This string is passed to external search services.\n    - `pageno` (number, optional): Search page number, starts at 1 (default 1)\n    - `time_range` (string, optional): Filter results by time range - one of: \"day\", \"month\", \"year\" (default: none)\n    - `language` (string, optional): Language code for results (e.g., \"en\", \"fr\", \"de\") or \"all\" (default: \"all\")\n    - `safesearch` (number, optional): Safe search filter level (0: None, 1: Moderate, 2: Strict) (default: instance setting)\n\n- **web_url_read**\n  - Read and convert the content from a URL to markdown with advanced content extraction options\n  - Inputs:\n    - `url` (string): The URL to fetch and process\n    - `startChar` (number, optional): Starting character position for content extraction (default: 0)\n    - `maxLength` (number, optional): Maximum number of characters to return\n    - `section` (string, optional): Extract content under a specific heading (searches for heading text)\n    - `paragraphRange` (string, optional): Return specific paragraph ranges (e.g., '1-5', '3', '10-')\n    - `readHeadings` (boolean, optional): Return only a list of headings instead of full content\n\n## Configuration\n\n### Setting the SEARXNG_URL\n\nThe `SEARXNG_URL` environment variable defines which SearxNG instance to connect to.\n\n#### Environment Variable Format\n```bash\nSEARXNG_URL=<protocol>://<hostname>[:<port>]\n```\n\n#### Examples\n```bash\n# Local development (default)\nSEARXNG_URL=http://localhost:8080\n\n# Public instance\nSEARXNG_URL=https://search.example.com\n\n# Custom port\nSEARXNG_URL=http://my-searxng.local:3000\n```\n\n#### Setup Instructions\n1. Choose a SearxNG instance from the [list of public instances](https://searx.space/) or use your local environment\n2. Set the `SEARXNG_URL` environment variable to the complete instance URL\n3. If not specified, the default value `http://localhost:8080` will be used\n\n### Using Authentication (Optional)\n\nIf you are using a password protected SearxNG instance you can set a username and password for HTTP Basic Auth:\n\n- Set the `AUTH_USERNAME` environment variable to your username\n- Set the `AUTH_PASSWORD` environment variable to your password\n\n**Note:** Authentication is only required for password-protected SearxNG instances. See the usage examples below for how to configure authentication with different installation methods.\n\n### Proxy Support (Optional)\n\nThe server supports HTTP and HTTPS proxies through environment variables. This is useful when running behind corporate firewalls or when you need to route traffic through a specific proxy server.\n\n#### Proxy Environment Variables\n\nSet one or more of these environment variables to configure proxy support:\n\n- `HTTP_PROXY`: Proxy URL for HTTP requests\n- `HTTPS_PROXY`: Proxy URL for HTTPS requests  \n- `http_proxy`: Alternative lowercase version for HTTP requests\n- `https_proxy`: Alternative lowercase version for HTTPS requests\n\n#### Proxy URL Formats\n\nThe proxy URL can be in any of these formats:\n\n```bash\n# Basic proxy\nexport HTTP_PROXY=http://proxy.company.com:8080\nexport HTTPS_PROXY=http://proxy.company.com:8080\n\n# Proxy with authentication\nexport HTTP_PROXY=http://username:password@proxy.company.com:8080\nexport HTTPS_PROXY=http://username:password@proxy.company.com:8080\n```\n\n**Note:** If no proxy environment variables are set, the server will make direct connections as normal. See the usage examples below for how to configure proxy settings with different installation methods.\n\n### [NPX](https://www.npmjs.com/package/mcp-searxng)\n\n```json\n{\n  \"mcpServers\": {\n    \"searxng\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"mcp-searxng\"],\n      \"env\": {\n        \"SEARXNG_URL\": \"YOUR_SEARXNG_INSTANCE_URL\"\n      }\n    }\n  }\n}\n```\n\n<details>\n<summary>Additional NPX Configuration Options</summary>\n\n#### With Authentication\n```json\n{\n  \"mcpServers\": {\n    \"searxng\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"mcp-searxng\"],\n      \"env\": {\n        \"SEARXNG_URL\": \"YOUR_SEARXNG_INSTANCE_URL\",\n        \"AUTH_USERNAME\": \"your_username\",\n        \"AUTH_PASSWORD\": \"your_password\"\n      }\n    }\n  }\n}\n```\n\n#### With Proxy Support\n```json\n{\n  \"mcpServers\": {\n    \"searxng\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"mcp-searxng\"],\n      \"env\": {\n        \"SEARXNG_URL\": \"YOUR_SEARXNG_INSTANCE_URL\",\n        \"HTTP_PROXY\": \"http://proxy.company.com:8080\",\n        \"HTTPS_PROXY\": \"http://proxy.company.com:8080\"\n      }\n    }\n  }\n}\n```\n\n#### With Authentication and Proxy Support\n```json\n{\n  \"mcpServers\": {\n    \"searxng\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"mcp-searxng\"],\n      \"env\": {\n        \"SEARXNG_URL\": \"YOUR_SEARXNG_INSTANCE_URL\",\n        \"AUTH_USERNAME\": \"your_username\",\n        \"AUTH_PASSWORD\": \"your_password\",\n        \"HTTP_PROXY\": \"http://proxy.company.com:8080\",\n        \"HTTPS_PROXY\": \"http://proxy.company.com:8080\"\n      }\n    }\n  }\n}\n```\n\n</details>\n\n### [NPM](https://www.npmjs.com/package/mcp-searxng)\n\n```bash\nnpm install -g mcp-searxng\n```\n\n```json\n{\n  \"mcpServers\": {\n    \"searxng\": {\n      \"command\": \"mcp-searxng\",\n      \"env\": {\n        \"SEARXNG_URL\": \"YOUR_SEARXNG_INSTANCE_URL\"\n      }\n    }\n  }\n}\n```\n\n<details>\n<summary>Additional NPM Configuration Options</summary>\n\n#### With Authentication\n```json\n{\n  \"mcpServers\": {\n    \"searxng\": {\n      \"command\": \"mcp-searxng\",\n      \"env\": {\n        \"SEARXNG_URL\": \"YOUR_SEARXNG_INSTANCE_URL\",\n        \"AUTH_USERNAME\": \"your_username\",\n        \"AUTH_PASSWORD\": \"your_password\"\n      }\n    }\n  }\n}\n```\n\n#### With Proxy Support\n```json\n{\n  \"mcpServers\": {\n    \"searxng\": {\n      \"command\": \"mcp-searxng\",\n      \"env\": {\n        \"SEARXNG_URL\": \"YOUR_SEARXNG_INSTANCE_URL\",\n        \"HTTP_PROXY\": \"http://proxy.company.com:8080\",\n        \"HTTPS_PROXY\": \"http://proxy.company.com:8080\"\n      }\n    }\n  }\n}\n```\n\n#### With Authentication and Proxy Support\n```json\n{\n  \"mcpServers\": {\n    \"searxng\": {\n      \"command\": \"mcp-searxng\",\n      \"env\": {\n        \"SEARXNG_URL\": \"YOUR_SEARXNG_INSTANCE_URL\",\n        \"AUTH_USERNAME\": \"your_username\",\n        \"AUTH_PASSWORD\": \"your_password\",\n        \"HTTP_PROXY\": \"http://proxy.company.com:8080\",\n        \"HTTPS_PROXY\": \"http://proxy.company.com:8080\"\n      }\n    }\n  }\n}\n```\n\n</details>\n\n### Docker\n\n#### Using [Pre-built Image from Docker Hub](https://hub.docker.com/r/isokoliuk/mcp-searxng)\n\n```bash\ndocker pull isokoliuk/mcp-searxng:latest\n```\n\n```json\n{\n  \"mcpServers\": {\n    \"searxng\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\", \"-i\", \"--rm\",\n        \"-e\", \"SEARXNG_URL\",\n        \"isokoliuk/mcp-searxng:latest\"\n      ],\n      \"env\": {\n        \"SEARXNG_URL\": \"YOUR_SEARXNG_INSTANCE_URL\"\n      }\n    }\n  }\n}\n```\n\n<details>\n<summary>Additional Docker Configuration Options</summary>\n\n#### With Authentication\n```json\n{\n  \"mcpServers\": {\n    \"searxng\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\", \"-i\", \"--rm\",\n        \"-e\", \"SEARXNG_URL\",\n        \"-e\", \"AUTH_USERNAME\",\n        \"-e\", \"AUTH_PASSWORD\",\n        \"isokoliuk/mcp-searxng:latest\"\n      ],\n      \"env\": {\n        \"SEARXNG_URL\": \"YOUR_SEARXNG_INSTANCE_URL\",\n        \"AUTH_USERNAME\": \"your_username\",\n        \"AUTH_PASSWORD\": \"your_password\"\n      }\n    }\n  }\n}\n```\n\n#### With Proxy Support\n```json\n{\n  \"mcpServers\": {\n    \"searxng\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\", \"-i\", \"--rm\",\n        \"-e\", \"SEARXNG_URL\",\n        \"-e\", \"HTTP_PROXY\",\n        \"-e\", \"HTTPS_PROXY\",\n        \"isokoliuk/mcp-searxng:latest\"\n      ],\n      \"env\": {\n        \"SEARXNG_URL\": \"YOUR_SEARXNG_INSTANCE_URL\",\n        \"HTTP_PROXY\": \"http://proxy.company.com:8080\",\n        \"HTTPS_PROXY\": \"http://proxy.company.com:8080\"\n      }\n    }\n  }\n}\n```\n\n#### With Authentication and Proxy Support\n```json\n{\n  \"mcpServers\": {\n    \"searxng\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\", \"-i\", \"--rm\",\n        \"-e\", \"SEARXNG_URL\",\n        \"-e\", \"AUTH_USERNAME\",\n        \"-e\", \"AUTH_PASSWORD\",\n        \"-e\", \"HTTP_PROXY\",\n        \"-e\", \"HTTPS_PROXY\",\n        \"isokoliuk/mcp-searxng:latest\"\n      ],\n      \"env\": {\n        \"SEARXNG_URL\": \"YOUR_SEARXNG_INSTANCE_URL\",\n        \"AUTH_USERNAME\": \"your_username\",\n        \"AUTH_PASSWORD\": \"your_password\",\n        \"HTTP_PROXY\": \"http://proxy.company.com:8080\",\n        \"HTTPS_PROXY\": \"http://proxy.company.com:8080\"\n      }\n    }\n  }\n}\n```\n\n</details>\n\n#### Build Locally\n\n```bash\ndocker build -t mcp-searxng:latest -f Dockerfile .\n```\n\n```json\n{\n  \"mcpServers\": {\n    \"searxng\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\", \"-i\", \"--rm\",\n        \"-e\", \"SEARXNG_URL\",\n        \"mcp-searxng:latest\"\n      ],\n      \"env\": {\n        \"SEARXNG_URL\": \"YOUR_SEARXNG_INSTANCE_URL\"\n      }\n    }\n  }\n}\n```\n\n<details>\n<summary>Additional Build Locally Configuration Options</summary>\n\n#### With Authentication\n```json\n{\n  \"mcpServers\": {\n    \"searxng\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\", \"-i\", \"--rm\",\n        \"-e\", \"SEARXNG_URL\",\n        \"-e\", \"AUTH_USERNAME\",\n        \"-e\", \"AUTH_PASSWORD\",\n        \"mcp-searxng:latest\"\n      ],\n      \"env\": {\n        \"SEARXNG_URL\": \"YOUR_SEARXNG_INSTANCE_URL\",\n        \"AUTH_USERNAME\": \"your_username\",\n        \"AUTH_PASSWORD\": \"your_password\"\n      }\n    }\n  }\n}\n```\n\n#### With Proxy Support\n```json\n{\n  \"mcpServers\": {\n    \"searxng\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\", \"-i\", \"--rm\",\n        \"-e\", \"SEARXNG_URL\",\n        \"-e\", \"HTTP_PROXY\",\n        \"-e\", \"HTTPS_PROXY\",\n        \"mcp-searxng:latest\"\n      ],\n      \"env\": {\n        \"SEARXNG_URL\": \"YOUR_SEARXNG_INSTANCE_URL\",\n        \"HTTP_PROXY\": \"http://proxy.company.com:8080\",\n        \"HTTPS_PROXY\": \"http://proxy.company.com:8080\"\n      }\n    }\n  }\n}\n```\n\n#### With Authentication and Proxy Support\n```json\n{\n  \"mcpServers\": {\n    \"searxng\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\", \"-i\", \"--rm\",\n        \"-e\", \"SEARXNG_URL\",\n        \"-e\", \"AUTH_USERNAME\",\n        \"-e\", \"AUTH_PASSWORD\",\n        \"-e\", \"HTTP_PROXY\",\n        \"-e\", \"HTTPS_PROXY\",\n        \"mcp-searxng:latest\"\n      ],\n      \"env\": {\n        \"SEARXNG_URL\": \"YOUR_SEARXNG_INSTANCE_URL\",\n        \"AUTH_USERNAME\": \"your_username\",\n        \"AUTH_PASSWORD\": \"your_password\",\n        \"HTTP_PROXY\": \"http://proxy.company.com:8080\",\n        \"HTTPS_PROXY\": \"http://proxy.company.com:8080\"\n      }\n    }\n  }\n}\n```\n\n</details>\n\n#### Docker Compose\n\nCreate a `docker-compose.yml` file:\n\n```yaml\nservices:\n  mcp-searxng:\n    image: isokoliuk/mcp-searxng:latest\n    stdin_open: true\n    environment:\n      - SEARXNG_URL=YOUR_SEARXNG_INSTANCE_URL\n```\n\nThen configure your MCP client:\n\n```json\n{\n  \"mcpServers\": {\n    \"searxng\": {\n      \"command\": \"docker-compose\",\n      \"args\": [\"run\", \"--rm\", \"mcp-searxng\"]\n    }\n  }\n}\n```\n\n<details>\n<summary>Additional Docker Compose Configuration Options</summary>\n\n#### With Authentication\n```yaml\nservices:\n  mcp-searxng:\n    image: isokoliuk/mcp-searxng:latest\n    stdin_open: true\n    environment:\n      - SEARXNG_URL=YOUR_SEARXNG_INSTANCE_URL\n      - AUTH_USERNAME=your_username\n      - AUTH_PASSWORD=your_password\n```\n\n#### With Proxy Support\n```yaml\nservices:\n  mcp-searxng:\n    image: isokoliuk/mcp-searxng:latest\n    stdin_open: true\n    environment:\n      - SEARXNG_URL=YOUR_SEARXNG_INSTANCE_URL\n      - HTTP_PROXY=http://proxy.company.com:8080\n      - HTTPS_PROXY=http://proxy.company.com:8080\n```\n\n#### With Authentication and Proxy Support\n```yaml\nservices:\n  mcp-searxng:\n    image: isokoliuk/mcp-searxng:latest\n    stdin_open: true\n    environment:\n      - SEARXNG_URL=YOUR_SEARXNG_INSTANCE_URL\n      - AUTH_USERNAME=your_username\n      - AUTH_PASSWORD=your_password\n      - HTTP_PROXY=http://proxy.company.com:8080\n      - HTTPS_PROXY=http://proxy.company.com:8080\n```\n\n#### Using Local Build\n```yaml\nservices:\n  mcp-searxng:\n    build: .\n    stdin_open: true\n    environment:\n      - SEARXNG_URL=YOUR_SEARXNG_INSTANCE_URL\n```\n\n</details>\n\n### HTTP Transport (Optional)\n\nThe server supports both STDIO (default) and HTTP transports:\n\n#### STDIO Transport (Default)\n- **Best for**: Claude Desktop and most MCP clients\n- **Usage**: Automatic - no additional configuration needed\n\n#### HTTP Transport  \n- **Best for**: Web-based applications and remote MCP clients\n- **Usage**: Set the `MCP_HTTP_PORT` environment variable\n\n```json\n{\n  \"mcpServers\": {\n    \"searxng-http\": {\n      \"command\": \"mcp-searxng\",\n      \"env\": {\n        \"SEARXNG_URL\": \"YOUR_SEARXNG_INSTANCE_URL\",\n        \"MCP_HTTP_PORT\": \"3000\"\n      }\n    }\n  }\n}\n```\n\n<details>\n<summary>Additional HTTP Transport Configuration Options</summary>\n\n#### HTTP Server with Authentication\n```json\n{\n  \"mcpServers\": {\n    \"searxng-http\": {\n      \"command\": \"mcp-searxng\",\n      \"env\": {\n        \"SEARXNG_URL\": \"YOUR_SEARXNG_INSTANCE_URL\",\n        \"MCP_HTTP_PORT\": \"3000\",\n        \"AUTH_USERNAME\": \"your_username\",\n        \"AUTH_PASSWORD\": \"your_password\"\n      }\n    }\n  }\n}\n```\n\n#### HTTP Server with Proxy Support\n```json\n{\n  \"mcpServers\": {\n    \"searxng-http\": {\n      \"command\": \"mcp-searxng\",\n      \"env\": {\n        \"SEARXNG_URL\": \"YOUR_SEARXNG_INSTANCE_URL\",\n        \"MCP_HTTP_PORT\": \"3000\",\n        \"HTTP_PROXY\": \"http://proxy.company.com:8080\",\n        \"HTTPS_PROXY\": \"http://proxy.company.com:8080\"\n      }\n    }\n  }\n}\n```\n\n#### HTTP Server with Authentication and Proxy Support\n```json\n{\n  \"mcpServers\": {\n    \"searxng-http\": {\n      \"command\": \"mcp-searxng\",\n      \"env\": {\n        \"SEARXNG_URL\": \"YOUR_SEARXNG_INSTANCE_URL\",\n        \"MCP_HTTP_PORT\": \"3000\",\n        \"AUTH_USERNAME\": \"your_username\",\n        \"AUTH_PASSWORD\": \"your_password\",\n        \"HTTP_PROXY\": \"http://proxy.company.com:8080\",\n        \"HTTPS_PROXY\": \"http://proxy.company.com:8080\"\n      }\n    }\n  }\n}\n```\n\n</details>\n\n**HTTP Endpoints:**\n- **MCP Protocol**: `POST/GET/DELETE /mcp` \n- **Health Check**: `GET /health`\n- **CORS**: Enabled for web clients\n\n**Testing HTTP Server:**\n```bash\n# Start HTTP server\nMCP_HTTP_PORT=3000 SEARXNG_URL=http://localhost:8080 mcp-searxng\n\n# Check health\ncurl http://localhost:3000/health\n```\n\n## Running evals\n\nThe evals package loads an mcp client that then runs the src/index.ts file, so there is no need to rebuild between tests. You can see the full documentation [here](https://www.mcpevals.io/docs).\n\n```bash\nSEARXNG_URL=SEARXNG_URL OPENAI_API_KEY=your-key npx mcp-eval evals.ts src/index.ts\n```\n\n## For Developers\n\n### Contributing to the Project\n\nWe welcome contributions! Here's how to get started:\n\n#### 0. Coding Guidelines\n\n- Use TypeScript for type safety\n- Follow existing error handling patterns\n- Keep error messages concise but informative\n- Write unit tests for new functionality\n- Ensure all tests pass before submitting PRs\n- Maintain test coverage above 90%\n- Test changes with the MCP inspector\n- Run evals before submitting PRs\n\n#### 1. Fork and Clone\n\n```bash\n# Fork the repository on GitHub, then clone your fork\ngit clone https://github.com/YOUR_USERNAME/mcp-searxng.git\ncd mcp-searxng\n\n# Add the original repository as upstream\ngit remote add upstream https://github.com/ihor-sokoliuk/mcp-searxng.git\n```\n\n#### 2. Development Setup\n\n```bash\n# Install dependencies\nnpm install\n\n# Start development with file watching\nnpm run watch\n```\n\n#### 3. Development Workflow\n\n1. **Create a feature branch:**\n   ```bash\n   git checkout -b feature/your-feature-name\n   ```\n\n2. **Make your changes** in `src/` directory\n   - Main server logic: `src/index.ts`\n   - Error handling: `src/error-handler.ts`\n\n3. **Build and test:**\n   ```bash\n   npm run build               # Build the project\n   npm test                     # Run unit tests\n   npm run test:coverage       # Run tests with coverage report\n   npm run inspector            # Run MCP inspector\n   ```\n\n4. **Run evals to ensure functionality:**\n   ```bash\n   SEARXNG_URL=http://localhost:8080 OPENAI_API_KEY=your-key npx mcp-eval evals.ts src/index.ts\n   ```\n\n#### 4. Submitting Changes\n\n```bash\n# Commit your changes\ngit add .\ngit commit -m \"feat: description of your changes\"\n\n# Push to your fork\ngit push origin feature/your-feature-name\n\n# Create a Pull Request on GitHub\n```\n\n### Testing\n\nThe project includes comprehensive unit tests with excellent coverage.\n\n#### Running Tests\n\n```bash\n# Run all tests\nnpm test\n\n# Run with coverage reporting\nnpm run test:coverage\n\n# Watch mode for development\nnpm run test:watch\n```\n\n#### Test Statistics\n- **Unit tests** covering all core modules\n- **100% success rate** with dynamic coverage reporting via c8\n- **HTML coverage reports** generated in `coverage/` directory\n\n#### What's Tested\n- Error handling (network, server, configuration errors)\n- Type validation and schema guards\n- Proxy configurations and environment variables\n- Resource generation and logging functionality\n- All module imports and function availability\n\n## License\n\nThis MCP server is licensed under the MIT License. This means you are free to use, modify, and distribute the software, subject to the terms and conditions of the MIT License. For more details, please see the LICENSE file in the project repository.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "searxng",
        "protocol",
        "search",
        "searxng model",
        "server searxng",
        "mcp searxng"
      ],
      "category": "search--data-extraction"
    },
    "Linked-API--linkedapi-mcp": {
      "owner": "Linked-API",
      "name": "linkedapi-mcp",
      "url": "https://github.com/Linked-API/linkedapi-mcp",
      "imageUrl": "",
      "description": "MCP server that lets AI assistants control LinkedIn accounts and retrieve real-time data.",
      "stars": 11,
      "forks": 3,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-09-27T12:16:10Z",
      "readme_content": "Linked API MCP server connects your LinkedIn account to AI assistants like Claude, Cursor, and VS Code. Ask them to search for leads, send messages, analyze profiles, and much more – they'll handle it through our cloud browser, safely and automatically.\n\n## Use cases\n- **Sales automation assistant**. Ask your AI to find leads, check their profiles, and draft personalized outreach. It can search for \"software engineers at companies with 50-200 employees in San Francisco\", analyze their backgrounds, and suggest connection messages that actually make sense.\n- **Recruitment assistant**. Let your assistant search for candidates with specific skills, review their experience, and send initial outreach. It handles the time-consuming parts while you focus on actually talking to people.\n- **Conversation assistant**. Your AI can read your existing LinkedIn conversations and help you respond naturally. It understands the context of your chats, suggests relevant replies, and can even send follow-up messages.\n- **Market research assistant**. Need competitor analysis? Your assistant can gather data about companies, their employees, and recent activities. Get insights about industry trends without spending hours on LinkedIn.\n\n## Get started\nTo start using Linked API MCP, spend 2 minutes reading these essential guides:\n\n1. [Installation](https://linkedapi.io/mcp/installation/) – set up MCP in Claude, Cursor, VS Code, or Windsurf.\n2. [Available tools](https://linkedapi.io/mcp/available-tools/) – explore all the LinkedIn tools your assistant can call.\n3. [Usage examples](https://linkedapi.io/mcp/usage-examples/) – see real-world examples to get you started quickly.\n\n## License\nThis project is licensed under the MIT – see the [LICENSE](https://github.com/Linked-API/linkedapi-mcp/blob/main/LICENSE) file for details.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "linkedapi",
        "linkedin",
        "api",
        "linkedapi mcp",
        "api linkedapi",
        "linked api"
      ],
      "category": "search--data-extraction"
    },
    "OctagonAI--octagon-deep-research-mcp": {
      "owner": "OctagonAI",
      "name": "octagon-deep-research-mcp",
      "url": "https://github.com/OctagonAI/octagon-deep-research-mcp",
      "imageUrl": "",
      "description": "Lightning-Fast, High-Accuracy Deep Research Agent",
      "stars": 61,
      "forks": 9,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-10-04T03:20:57Z",
      "readme_content": "# Octagon Deep Research MCP\n\n![Favicon](https://docs.octagonagents.com/logo.svg) The Octagon Deep Research MCP server provides specialized AI-powered comprehensive research and analysis capabilities by integrating with advanced deep research agents. No rate limits, faster than ChatGPT Deep Research, more thorough than Grok DeepSearch or Perplexity Deep Research. Add unlimited deep research functionality to any MCP client including Claude Desktop, Cursor, and other popular MCP-enabled applications.\n\n**Powered by [Octagon AI](https://docs.octagonagents.com)** - Learn more about the Deep Research Agent at [docs.octagonagents.com](https://docs.octagonagents.com/guide/agents/deep-research-agent.html)\n\n[![Demo](https://img.youtube.com/vi/yh1cyrm9aus/0.jpg)](https://youtu.be/yh1cyrm9aus)\n\n## 🏆 Why Teams Choose Octagon's Enterprise-Grade Deep Research API\n\n👉 **8–10x faster** than the leading incumbent—complex analyses complete in seconds, not minutes  \n👉 **Greater depth & accuracy** —pulls data from 3x more high-quality sources and cross-checks every figure  \n👉 **Unlimited parallel runs**—no rate caps, so your analysts can launch as many deep-dive tasks as they need (unlike ChatGPT Pro's 125-task monthly limit)  \n\n## 🚀 Core Differentiators\n\n✅ **No Rate Limits** - Execute unlimited deep research queries without restrictions (vs ChatGPT Pro's 125-task monthly limit)  \n✅ **Superior Performance** - Faster than ChatGPT Deep Research, more thorough than Grok DeepSearch or Perplexity Deep Research  \n✅ **Enterprise-Grade Speed** - 8-10x faster than leading incumbents, with 3x more source coverage  \n✅ **Universal MCP Integration** - Add deep research functionality to any MCP client  \n✅ **Multi-Domain Expertise** - Comprehensive research across any topic or industry  \n✅ **Advanced Data Synthesis** - Multi-source aggregation with cross-verification of every figure  \n\n## Features\n\n✅ **Comprehensive Research Capabilities**\n   - Multi-source data aggregation and synthesis\n   - Academic research and literature review\n   - Competitive landscape analysis\n   - Market intelligence and trend analysis\n   - Technical and scientific research\n   - Policy and regulatory research\n   - Real-time web scraping and data extraction\n     \n✅ **Universal Domain Coverage**\n   - Technology and AI research\n   - Healthcare and medical research\n   - Environmental and sustainability studies\n   - Economic and business analysis\n   - Scientific and engineering research\n   - Social and cultural studies\n   - Political and policy analysis\n     \n✅ **Advanced Analysis Tools**\n   - Comprehensive report generation\n   - Cross-source verification\n   - Trend identification and forecasting\n   - Comparative analysis frameworks\n\n## Get Your Octagon API Key\n\nTo use Octagon Deep Research MCP, you need to:\n\n1. Sign up for a free account at [Octagon](https://app.octagonai.co/signup/?redirectToAfterSignup=https://app.octagonai.co/api-keys)\n2. After logging in, from left menu, navigate to **API Keys** \n3. Generate a new API key\n4. Use this API key in your configuration as the `OCTAGON_API_KEY` value\n\n## Prerequisites\n\nBefore installing or running Octagon Deep Research MCP, you need to have `npx` (which comes with Node.js and npm) installed on your system.\n\n### Mac (macOS)\n\n1. **Install Homebrew** (if you don't have it):\n   ```bash\n   /bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n   ```\n2. **Install Node.js (includes npm and npx):**\n   ```bash\n   brew install node\n   ```\n   This will install the latest version of Node.js, npm, and npx.\n\n3. **Verify installation:**\n   ```bash\n   node -v\n   npm -v\n   npx -v\n   ```\n\n### Windows\n\n1. **Download the Node.js installer:**\n   - Go to [https://nodejs.org/](https://nodejs.org/) and download the LTS version for Windows.\n2. **Run the installer** and follow the prompts. This will install Node.js, npm, and npx.\n3. **Verify installation:**\n   Open Command Prompt and run:\n   ```cmd\n   node -v\n   npm -v\n   npx -v\n   ```\n\nIf you see version numbers for all three, you are ready to proceed with the installation steps below.\n\n## Installation\n\n### Running on Claude Desktop\n\nTo configure Octagon Deep Research MCP for Claude Desktop:\n\n1. Open Claude Desktop\n2. Go to Settings > Developer > Edit Config\n3. Add the following to your `claude_desktop_config.json` (Replace `your-octagon-api-key` with your Octagon API key):\n```json\n{\n  \"mcpServers\": {\n    \"octagon-deep-research-mcp\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"octagon-deep-research-mcp@latest\"],\n      \"env\": {\n        \"OCTAGON_API_KEY\": \"YOUR_API_KEY_HERE\"\n      }\n    }\n  }\n}\n```\n4. Restart Claude for the changes to take effect\n\n### Running on Cursor\n\nConfiguring Cursor Desktop 🖥️\nNote: Requires Cursor version 0.45.6+\n\nTo configure Octagon Deep Research MCP in Cursor:\n\n1. Open Cursor Settings\n2. Go to Features > MCP Servers \n3. Click \"+ Add New MCP Server\"\n4. Enter the following:\n   - Name: \"octagon-deep-research-mcp\" (or your preferred name)\n   - Type: \"command\"\n   - Command: `env OCTAGON_API_KEY=your-octagon-api-key npx -y octagon-deep-research-mcp`\n\n> If you are using Windows and are running into issues, try `cmd /c \"set OCTAGON_API_KEY=your-octagon-api-key && npx -y octagon-deep-research-mcp\"`\n\nReplace `your-octagon-api-key` with your Octagon API key.\n\nAfter adding, refresh the MCP server list to see the new tools. The Composer Agent will automatically use Octagon Deep Research MCP when appropriate, but you can explicitly request it by describing your research needs. Access the Composer via Command+L (Mac), select \"Agent\" next to the submit button, and enter your query.\n\n### Running on Windsurf\n\nAdd this to your `./codeium/windsurf/model_config.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"octagon-deep-research-mcp\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"octagon-deep-research-mcp@latest\"],\n      \"env\": {\n        \"OCTAGON_API_KEY\": \"YOUR_API_KEY_HERE\"\n      }\n    }\n  }\n}\n```\n\n### Running with npx\n\n```bash\nenv OCTAGON_API_KEY=your_octagon_api_key npx -y octagon-deep-research-mcp\n```\n\n### Manual Installation\n\n```bash\nnpm install -g octagon-deep-research-mcp\n```\n\n## Documentation\n\nFor comprehensive documentation on using Deep Research capabilities, please visit our official documentation at:\n[https://docs.octagonagents.com](https://docs.octagonagents.com)\n\nSpecifically for the Deep Research Agent: [Deep Research Agent Guide](https://docs.octagonagents.com/guide/agents/deep-research-agent.html)\n\nThe documentation includes:\n- Detailed API references\n- Research methodology guidelines\n- Examples and use cases\n- Best practices for comprehensive research\n- Advanced features and capabilities\n\n## Available Tool\n\n### octagon-deep-research-agent\nComprehensive deep research and analysis across any topic or domain.\n\nThe tool uses a single `prompt` parameter that accepts a natural language query. Include all relevant details in your prompt for optimal results.\n\n## 📚 Example Research Queries\n\n### Technology & AI Research\n- \"Research the current state of quantum computing development and commercial applications across major tech companies\"\n- \"Analyze the competitive landscape in large language models, comparing capabilities, limitations, and market positioning\"\n- \"Investigate recent developments in autonomous vehicle technology and regulatory challenges\"\n- \"Study the evolution of edge computing architectures and their impact on IoT deployment\"\n\n### Healthcare & Medical Research\n- \"Research breakthrough medical treatments for Alzheimer's disease developed in the last 3 years\"\n- \"Analyze the effectiveness of different COVID-19 vaccine technologies and their global distribution\"\n- \"Investigate the current state of gene therapy research for rare diseases\"\n- \"Study mental health treatment innovations and their accessibility across different demographics\"\n\n### Environmental & Sustainability\n- \"Research sustainable agriculture practices and their adoption rates globally\"\n- \"Analyze renewable energy adoption trends and policy drivers across different countries\"\n- \"Investigate the environmental impact of cryptocurrency mining and proposed solutions\"\n- \"Study carbon capture technologies and their commercial viability\"\n\n### Business & Economics\n- \"Analyze the gig economy's impact on traditional employment models and worker protections\"\n- \"Research the evolution of remote work policies post-pandemic and their effectiveness on productivity\"\n- \"Investigate supply chain resilience strategies adopted after global disruptions\"\n- \"Study the impact of digital transformation on traditional retail businesses\"\n\n### Social & Cultural Studies\n- \"Research the impact of social media algorithms on information consumption patterns and political polarization\"\n- \"Analyze changing demographics in urban areas and their impact on city planning\"\n- \"Investigate the effectiveness of different approaches to digital literacy education\"\n- \"Study the cultural impact of streaming services on traditional media consumption\"\n\n### Science & Engineering\n- \"Research advances in materials science for semiconductor manufacturing\"\n- \"Analyze the development of fusion energy technologies and timeline to commercialization\"\n- \"Investigate innovations in water purification technologies for developing regions\"\n- \"Study the engineering challenges and solutions for space exploration missions\"\n\n### Policy & Governance\n- \"Investigate recent developments in AI regulation across different countries and their potential impact\"\n- \"Research privacy legislation trends and their effects on technology companies\"\n- \"Analyze different approaches to cryptocurrency regulation globally\"\n- \"Study the effectiveness of various climate policy mechanisms\"\n\n### Cybersecurity & Privacy\n- \"Investigate cybersecurity threats in IoT devices and enterprise mitigation strategies\"\n- \"Research the evolution of ransomware attacks and defensive technologies\"\n- \"Analyze privacy-preserving technologies and their adoption in consumer applications\"\n- \"Study the security implications of quantum computing on current encryption methods\"\n\n### Education & Learning\n- \"Research the effectiveness of different online learning platforms and methodologies\"\n- \"Analyze the impact of AI tools on academic research and education\"\n- \"Investigate innovative approaches to STEM education in underserved communities\"\n- \"Study the future of skills-based learning and certification programs\"\n\n## 🔍 Research Capabilities\n\n- **Multi-Source Analysis**: Aggregates information from academic papers, industry reports, news sources, and expert opinions\n- **Real-Time Data**: Accesses current information and recent developments\n- **Cross-Verification**: Validates findings across multiple reliable sources\n- **Trend Analysis**: Identifies patterns and forecasts future developments\n- **Competitive Intelligence**: Comprehensive competitive landscape analysis\n- **Technical Deep-Dives**: Detailed analysis of complex technical topics\n- **Policy Impact Assessment**: Analysis of regulatory and policy implications\n- **Market Dynamics**: Understanding of market forces and business implications\n\n## Troubleshooting\n\n1. **API Key Issues**: Ensure your Octagon API key is correctly set in the environment or config file.\n2. **Connection Issues**: Make sure the connectivity to the Octagon API is working properly.\n3. **Rate Limiting**: No rate limits apply to Deep Research MCP - execute unlimited queries.\n\n## License\n\nMIT \n\n---\n\n⭐ Star this repo if you find it helpful for your research needs!\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "octagon",
        "octagonai",
        "deep",
        "octagon deep",
        "deep research",
        "extraction octagonai"
      ],
      "category": "search--data-extraction"
    },
    "Pearch-ai--mcp_pearch": {
      "owner": "Pearch-ai",
      "name": "mcp_pearch",
      "url": "https://github.com/Pearch-ai/mcp_pearch",
      "imageUrl": "",
      "description": "Best people search engine that reduces the time spent on talent discovery",
      "stars": 6,
      "forks": 4,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-15T17:17:25Z",
      "readme_content": "# Pearch.ai MCP\n\n[![smithery badge](https://smithery.ai/badge/@Pearch-ai/mcp_pearch)](https://smithery.ai/server/@Pearch-ai/mcp_pearch)\n\nOur people search API and MCP deliver the most precise results on the market. You simply ask in natural language, and we provide top-quality candidates. Designed for seamless integration with any ATS or hiring platform, our solution is backed by scientific methods, trusted by recruiters, and consistently rated the highest-quality sourcing tool.\n\n[Evaluating AI Recruitment Sourcing Tools by Human Preference](https://arxiv.org/abs/2504.02463v1)\n\n\n## Prerequisites\n\n- Python 3.7 or newer\n- Pearch.ai API key\n- FastMCP package\n\n## API Key Setup\n\n1. Visit [Pearch.ai Dashboard](https://s.pearch.ai/settings) to obtain your API key\n2. Set your API key as an environment variable:\n   ```bash\n   export PEARCH_API_KEY='your-api-key-here'\n   ```\n\n## Installation\n\n### Installing via Smithery\n\nTo install mcp_pearch for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@Pearch-ai/mcp_pearch):\n\n```bash\nnpx -y @smithery/cli install @Pearch-ai/mcp_pearch --client claude\n```\n\n### Option 1: macOS[uv] \n\n```bash\n# Install Python and uv\nbrew install python\nbrew install uv\n\n# Create and activate virtual environment\nuv venv\nsource .venv/bin/activate\n\n# Install FastMCP\nuv pip install fastmcp\n```\n\n### Option 2: Linux[pip] \n\n```bash\n# Install system dependencies\nsudo apt update\nsudo apt install python3 python3-venv python3-pip\n\n# Create and activate virtual environment\npython3 -m venv .venv\nsource .venv/bin/activate\n\n# Install FastMCP\npip install fastmcp\n```\n\n## Usage\n\n### Standard Installation\n\n```bash\nfastmcp install pearch_mcp.py --name \"Pearch.ai\" --env-var PEARCH_API_KEY=pearch_mcp_key\n```\n\n### Development Mode\n\nFor local development and testing:\n\n```bash\n# Set your API key\nexport PEARCH_API_KEY='your-api-key-here'\n\n# Start development server\nfastmcp dev pearch_mcp.py\n```\n\n## Support\n\nIf you encounter any issues or have questions:\n- Open an issue in the repository\n- Contact support at [f@pearch.ai](mailto:f@pearch.ai)\n\n## License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "search",
        "talent",
        "mcp_pearch",
        "talent discovery",
        "search engine",
        "people search"
      ],
      "category": "search--data-extraction"
    },
    "QuentinCody--catalysishub-mcp-server": {
      "owner": "QuentinCody",
      "name": "catalysishub-mcp-server",
      "url": "https://github.com/QuentinCody/catalysishub-mcp-server",
      "imageUrl": "",
      "description": "Unofficial MCP server for searching and retrieving scientific data from the Catalysis Hub database, providing access to computational catalysis research and surface reaction data.",
      "stars": 1,
      "forks": 3,
      "license": "Other",
      "language": "Python",
      "updated_at": "2025-05-20T14:47:46Z",
      "readme_content": "# Catalysis Hub MCP Server\n\nA Model Context Protocol (MCP) server interface to Catalysis Hub's GraphQL API, enabling programmatic access to catalysis research data through flexible GraphQL queries.\n\n## Key Features\n\n- **Direct GraphQL Access**: Execute any valid GraphQL query against Catalysis Hub's API\n- **Comprehensive Data Access**:\n  - Catalytic reactions (equations, conditions, catalysts)\n  - Material systems (structures, properties, descriptors)\n  - Research publications (titles, DOIs, authors)\n  - Surface reaction data (adsorption energies, binding sites)\n- **MCP Standard Compliance**: Implements the Model Context Protocol for AI-agent interoperability\n- **Flexible Query Support**: Execute complex queries with variables parameterization\n- **Error Handling**: Robust error reporting for API connectivity and query execution\n\n## License and Citation\n\nThis project is available under the MIT License with an Academic Citation Requirement. This means you can freely use, modify, and distribute the code, but any academic or scientific publication that uses this software must provide appropriate attribution.\n\n### For academic/research use:\nIf you use this software in a research project that leads to a publication, presentation, or report, you **must** cite this work according to the format provided in [CITATION.md](CITATION.md).\n\n### For commercial/non-academic use:\nCommercial and non-academic use follows the standard MIT License terms without the citation requirement.\n\nBy using this software, you agree to these terms. See [LICENSE.md](LICENSE.md) for the complete license text.\n\n## Implementation Details\n\n- **Server Configuration** (matches `claude_desktop_config.json`):\n  ```json\n  {\n    \"command\": \"/Users/quentincody/.env/bin/python3\",\n    \"args\": [\"/Users/quentincody/catalysishub-mcp-server/catalysishub_mcp_server.py\"],\n    \"options\": {\n      \"cwd\": \"/Users/quentincody/catalysishub-mcp-server\"\n    }\n  }\n  ```\n- **Core Dependency**: `httpx` for asynchronous HTTP requests\n- **Transport**: Standard input/output communication following MCP specifications\n\n## Setup & Installation\n\n1. **Clone the repository**:\n   ```bash\n   git clone <repository_url>\n   cd catalysishub-mcp-server\n   ```\n\n2. **Install dependencies**:\n   ```bash\n   pip install -r requirements.txt\n   ```\n\n3. **Verify installation**:\n   ```bash\n   python3 catalysishub_mcp_server.py --version\n   # Should output: catalysishub-mcp-server 0.1.0\n   ```\n\n## Usage Examples\n\n### Basic Query Execution\n```python\nfrom mcp.client import MCPClient\n\nasync with MCPClient(\"catalysishub\") as hub:\n    result = await hub.catalysishub_graphql(\n        query=\"\"\"{\n            reactions(first: 5) {\n                edges {\n                    node {\n                        id\n                        Equation\n                        Temperature\n                    }\n                }\n            }\n        }\"\"\"\n    )\n    print(json.loads(result))\n```\n\n### Parameterized Query with Variables\n```python\nvariables = {\n    \"materialId\": \"mp-1234\",\n    \"firstResults\": 5\n}\n\nresponse = await hub.catalysishub_graphql(\n    query=\"\"\"query GetMaterial($materialId: String!, $firstResults: Int!) {\n        systems(uniqueId: $materialId) {\n            edges {\n                node {\n                    energy\n                    Cifdata\n                    relatedReactions(first: $firstResults) {\n                        edges {\n                            node {\n                                id\n                                Equation\n                            }\n                        }\n                    }\n                }\n            }\n        }\n    }\"\"\",\n    variables=variables\n)\n```\n\n## Query Optimization Tips\n\n1. **Use GraphQL Fragments**:\n   ```graphql\n   fragment ReactionDetails on Reaction {\n       id\n       Equation\n       ActivationEnergy\n       Catalyst {\n           formula\n           surface\n       }\n   }\n   \n   query {\n       reactions(first: 10) {\n           edges {\n               node {\n                   ...ReactionDetails\n               }\n           }\n       }\n   }\n   ```\n\n2. **Batch Related Queries**:\n   ```graphql\n   query BatchQuery {\n       reactions: reactions(first: 5) { edges { node { id Equation } } }\n       materials: systems(first: 5) { edges { node { formula energy } } }\n   }\n   ```\n\n## Response Structure\n\nSuccessful responses follow this structure:\n```json\n{\n    \"data\": { /* Query results */ },\n    \"extensions\": {\n        \"responseMetadata\": {\n            \"requestDuration\": 145,\n            \"apiVersion\": \"2024-06\"\n        }\n    }\n}\n```\n\nError responses include detailed diagnostics:\n```json\n{\n    \"errors\": [{\n        \"message\": \"Cannot query field 'invalidField' on type 'Reaction'\",\n        \"locations\": [{\"line\": 5, \"column\": 21}],\n        \"path\": [\"query\", \"reactions\", \"edges\", \"node\", \"invalidField\"]\n    }]\n}\n```\n\n## Troubleshooting\n\n**Common Issues**:\n- `HTTP Request Error`: Verify network connectivity to `api.catalysis-hub.org`\n- `JSON Decode Error`: Check query syntax using Catalysis Hub's [GraphQL Playground](https://www.catalysis-hub.org/api/graphql)\n- `Timeout Errors`: Add `timeout` parameter to complex queries\n\n## Acknowledgements\n\nThis project builds on the Model Context Protocol (MCP) framework and is designed to interface with the Catalysis Hub database, a comprehensive resource for catalysis research data.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "catalysishub",
        "catalysis",
        "search",
        "catalysishub mcp",
        "data catalysis",
        "catalysis research"
      ],
      "category": "search--data-extraction"
    },
    "SecretiveShell--MCP-searxng": {
      "owner": "SecretiveShell",
      "name": "MCP-searxng",
      "url": "https://github.com/SecretiveShell/MCP-searxng",
      "imageUrl": "",
      "description": "An MCP Server to connect to searXNG instances",
      "stars": 103,
      "forks": 17,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-10-03T22:32:01Z",
      "readme_content": "# MCP-searxng\n\nAn MCP server for connecting agentic systems to search systems via [searXNG](https://docs.searxng.org/).\n\n<p align=\"center\">\n  <a href=\"https://glama.ai/mcp/servers/sl2zl8vaz8\">\n    <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/sl2zl8vaz8/badge\" alt=\"MCP SearxNG Badge\"/>\n  </a>\n</p>\n\n## Tools\n\nSearch the web with SearXNG\n\n## Prompts\n\n```python\nsearch(query: str) -> f\"Searching for {query} using searXNG\"\n```\n\n## Usage\n\n### via uvx\n\n1) configure your client JSON like\n\n```json\n{\n  \"mcpServers\": {\n    \"searxng\": {\n      \"command\": \"uvx\", \n      \"args\": [\n        \"mcp-searxng\"\n      ]\n    }\n  }\n}\n```\n\n### via git clone\n\n1) Add the server to claude desktop (the entrypoint is main.py)\n\nClone the repo and add this JSON to claude desktop\n\nyou can run this server with `uvx mcp-searxng`, or use a local copy of the repo\n\n```json\n{\n  \"mcpServers\": {\n    \"searxng\": {\n      \"command\": \"uv\", \n      \"args\": [\n        \"--project\",\n        \"/absoloute/path/to/MCP-searxng/\",\n        \"run\",\n        \"/absoloute/path/to/MCP-searxng/mcp-searxng/main.py\"\n      ]\n    }\n  }\n}\n```\n\nyou will need to change the paths to match your environment\n\n### Custom SearXNG URL\n\n2) set the environment variable `SEARXNG_URL` to the URL of the searxng server (default is `http://localhost:8080`)\n\n3) run your MCP client and you should be able to search the web with searxng\n\nNote: if you are using claude desktop make sure to kill the process (task manager or equivalent) before running the server again\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "searxng",
        "mcp",
        "search",
        "searxng mcp",
        "mcp searxng",
        "searxng instances"
      ],
      "category": "search--data-extraction"
    },
    "Tomatio13--mcp-server-tavily": {
      "owner": "Tomatio13",
      "name": "mcp-server-tavily",
      "url": "https://github.com/Tomatio13/mcp-server-tavily",
      "imageUrl": "",
      "description": "[vectorize-io/vectorize-mcp-server](https://github.com/vectorize-io/vectorize-mcp-server/) ☁️ 📇 - [Vectorize](https://vectorize.io) MCP server for advanced retrieval, Private Deep Research, Anything-to-Markdown file extraction and text chunking.",
      "stars": 43,
      "forks": 11,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-14T15:59:56Z",
      "readme_content": "# tavily-search MCP server\n\nA MCP server project\n\n[![Trust Score](https://archestra.ai/mcp-catalog/api/badge/quality/Tomatio13/mcp-server-tavily)](https://archestra.ai/mcp-catalog/tomatio13__mcp-server-tavily)\n<a href=\"https://glama.ai/mcp/servers/s0hka6zney\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/s0hka6zney/badge\" alt=\"tavily-search MCP server\" /></a>\n\n## Components\n\nThis server uses the Tavily API to perform searches based on specified queries.\n- Search results are returned in text format.\n- Search results include AI responses, URIs, and titles of the search results.\n\n### Tools\n\nThis server implements the following tools:\n- search: Performs searches based on specified queries\n  - Required argument: \"query\"\n  - Optional argument: \"search_depth\" (basic or advanced)\n\n### Installing via Smithery\n\nTo install Tavily Search for Claude Desktop automatically via [Smithery](https://smithery.ai/server/tavily-search):\n\n```bash\nnpx -y @smithery/cli install tavily-search --client claude\n```\n\n### Install\n\n1. Download the repository.\n```bash\ngit clone https://github.com/Tomatio13/mcp-server-tavily.git\n``` \n2. Open the Claude Desktop configuration file.\n```\nOn MacOS: `~/Library/Application\\ Support/Claude/claude_desktop_config.json`\nOn Windows: `C:\\Users\\[username]\\AppData\\Roaming\\Claude\\claude_desktop_config.json`\n```\n\n3. Edit the configuration file as follows:\n  ```yaml\n  \"mcpServers\": {\n    \"tavily-search\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"--directory\",\n        \"C:\\\\your_path\\\\mcp-server-tavily\",\n        \"run\",\n        \"tavily-search\"\n      ],\n      \"env\": {\n        \"TAVILY_API_KEY\": \"YOUR_TAVILY_API_KEY\",\n        \"PYTHONIOENCODING\": \"utf-8\"\n      }\n    }\n  }\n  ```\n\n4. Restart Claude Desktop.\n\n## Usage\n\nIn Claude Desktop, when you ask \"Please search for something\", you will receive search results.\n\nSearch example:\n```\nPlease search in detail for today's events in Kamakura\n```\nResponse example:\n```\nAccording to the search results, the following events start today, December 1st:\n\"Kamakura Promotion Photo Contest 2025\"\nPeriod: December 1, 2024 - January 31, 2025\nA photo contest for those who love Kamakura\nApplications start accepting from today\nAlso, as a related upcoming event:\nOn December 7th, an exhibition by 12 Kamakura artists will be held at the Seibu Press Inn Kamakura Ofuna Station East Exit Lounge.\n```\n\n## Log Storage Location\n\nLogs are stored in the following location:\n\nFor Windows:\n```\nC:\\Users\\[username]\\AppData\\Roaming\\Claude\\logs\\mcp-server-tavily-search\n```\n## Execution with Cursor\n\n1. Create a shell script (e.g., `script.sh`) as shown below:\n\n```bash\n#!/bin/bash\nTARGET_DIR=/path/to/mcp-server-tavily\ncd \"${TARGET_DIR}\"\nexport TAVILY_API_KEY=\"your-api-key\"\nexport PYTHONIOENCODING=utf-8\nuv --directory $PWD run tavily-search\n```\n\n2. Configure Cursor's MCP Server settings as follows:\n\n```\nName: tavily-search\nType: command\nCommand: /path/to/your/script.sh\n```\n\n3. Save the settings.\n\n4. Once the settings are saved, you can ask Cursor's Composer-Agent to \"search for something,\" and it will return the search results.\n\n## Running in Local Environment Using Docker Compose\n\n### Purpose\nFor operating systems other than Windows/MacOS where Claude Desktop cannot be used,\nthis section explains how to set up and run an MCP server and client in a local environment\nusing Docker compose.\n\n### Steps\n1. Install Docker.\n2. Download the repository.\n```bash\ngit clone https://github.com/Tomatio13/mcp-server-tavily.git\n``` \n3. Run Docker compose.\n```bash\ndocker compose up -d\n``` \n4. Execute the client.\n```bash\ndocker exec mcp_server uv --directory /usr/src/app/mcp-server-tavily/src run client.py\n```\n5. Execution Results\n6. After searching for available tools as shown below, a query will be issued to Tavily and a response will be returned:\n```bash\n2024-12-01 11:21:56,930 - tavily-search-server - INFO - Starting Tavily search server\n2024-12-01 11:21:56,932 - tavily-search-server - INFO - Server initialized, starting main loop\n2024-12-01 11:21:56,936 - mcp.server - INFO - Processing request of type ListToolsRequest\n2024-12-01 11:21:56,936 - tavily-search-server - INFO - Listing available tools\n利用可能なツール: nextCursor=None tools=[Tool(name='search', description='Search the web using Tavily API', inputSchema={'type': 'object', 'properties': {'query': {'type': 'string', 'description': 'Search query'}, 'search_depth': {'type': 'string', 'description': 'Search depth (basic or advanced)', 'enum': ['basic', 'advanced']}}, 'required': ['query']})]\n2024-12-01 11:21:56,937 - mcp.server - INFO - Processing request of type CallToolRequest\n2024-12-01 11:21:56,937 - tavily-search-server - INFO - TOOL_CALL_DEBUG: Tool called - name: search, arguments: {'query': '今日の東京タワーのイベントを教えて下さい'}\n2024-12-01 11:21:56,937 - tavily-search-server - INFO - Executing search with query: '今日の東京タワーのイベントを教えて下さい'\n2024-12-01 11:22:00,243 - httpx - INFO - HTTP Request: POST https://api.tavily.com/search \"HTTP/1.1 200 OK\"\n2024-12-01 11:22:00,243 - tavily-search-server - INFO - Search successful - Answer generated\n2024-12-01 11:22:00,243 - tavily-search-server - INFO - Search successful - Results available\nツール実行結果: content=[TextContent(type='text', text='AI Answer:\\n今日の東京タワーのイベントは以下の通りです：\\n1. Candlelight: エド・シーランとコールドプレイのヒットメドレー - 12月01日\\n2. チームラボプラネッツ TOKYO - 12月01日から1月21日\\n\\n他にもイベントがある可能性がありますので、公式ウェブサイト等で最新情報をご確認ください。\\n\\n\\n\\nSearch Results:\\n\\n1. 東京タワー (東京): 現在のイベントとチケット | Fever\\nURL: https://feverup.com/ja/tokyo/venue/tokyo-tower\\nSummary: Summary not found\\n\\n\\n2. 東京タワー(東京都)の施設で開催するイベント一覧｜ウォーカープラス\\nURL: https://www.walkerplus.com/spot/ar0313s03867/e_list.html\\nSummary: Summary not found\\n\\n\\n3. 東京タワー - Tokyo Tower\\nURL: https://www.tokyotower.co.jp/event/\\nSummary: Summary not found\\n')] isError=False\n``` \n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "retrieval",
        "search",
        "extraction",
        "file extraction",
        "data extraction",
        "server vectorize"
      ],
      "category": "search--data-extraction"
    },
    "adawalli--nexus": {
      "owner": "adawalli",
      "name": "nexus",
      "url": "https://github.com/adawalli/nexus",
      "imageUrl": "",
      "description": "AI-powered web search server using Perplexity Sonar models with source citations. Zero-install setup via NPX.",
      "stars": 14,
      "forks": 5,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-10-04T12:26:18Z",
      "readme_content": "<!-- markdownlint-disable MD033 MD041 -->\n\n<div align=\"center\">\n\n# 🔍 Nexus MCP Server\n\n**AI integration without the complexity**\n\n[![npm version](https://img.shields.io/npm/v/nexus-mcp.svg)](https://www.npmjs.com/package/nexus-mcp)\n![NPM Downloads](https://img.shields.io/npm/dt/nexus-mcp?style=flat-square&logo=npm&label=downloads)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![TypeScript](https://img.shields.io/badge/TypeScript-Ready-blue.svg)](https://www.typescriptlang.org/)\n[![MCP Compatible](https://img.shields.io/badge/MCP-Compatible-green.svg)](https://modelcontextprotocol.io/)\n[![CodeRabbit Pull Request Reviews](https://img.shields.io/coderabbit/prs/github/adawalli/nexus)](https://coderabbit.ai)\n\n_Intelligent AI model search and discovery with zero-install simplicity_\n\n[Quick Start](#-quick-start) • [Features](#-features) • [Documentation](#-documentation) • [Contributing](#-contributing)\n\n</div>\n\n---\n\n## 🚀 What is Nexus?\n\nNexus is a production-ready **Model Context Protocol (MCP) server** that brings AI-powered web search directly into your development environment. Get intelligent search results with proper citations in **Claude Desktop**, **Cursor**, or any MCP-compatible client - all with a single command.\n\n### Why Nexus?\n\n- **🎯 Zero Setup**: Ready in 30 seconds with `npx` - no installation, no configuration\n- **🧠 AI-Powered**: Uses Perplexity Sonar models for intelligent, current web search\n- **📚 Source Citations**: Get authoritative sources with every search result\n- **🔧 Developer-First**: Built for developers who want AI capabilities without complexity\n- **⚡ Production-Ready**: Enterprise-grade reliability with comprehensive error handling\n\n## ✨ Features\n\n<table>\n<tr>\n<td width=\"50%\">\n\n### 🚀 **Zero-Install Simplicity**\n\n- Ready in 30 seconds with `npx`\n- No dependencies or build steps\n- Cross-platform compatibility\n- Always up-to-date\n\n### 🧠 **AI-Powered Intelligence**\n\n- Perplexity Sonar model integration\n- Real-time web content search\n- Context-aware result ranking\n- Multiple model options\n\n</td>\n<td width=\"50%\">\n\n### 📚 **Professional Quality**\n\n- Source citations and metadata\n- Comprehensive error handling\n- Production-grade reliability\n- TypeScript implementation\n\n### 🔧 **Developer Experience**\n\n- MCP protocol compliance\n- Extensive documentation\n- Configurable parameters\n- Community support\n\n</td>\n</tr>\n</table>\n\n## 🏃‍♂️ Quick Start\n\n**🚀 Zero-install setup - Ready in 30 seconds!**\n\n### Prerequisites\n\n- Node.js 16 or higher\n- An OpenRouter API key (get one at [OpenRouter](https://openrouter.ai))\n\n### Zero-Config Installation\n\nNo build steps, no dependencies, no setup required:\n\n```bash\n# Set your OpenRouter API key\nexport OPENROUTER_API_KEY=your-api-key-here\n\n# Run the server instantly\nnpx nexus-mcp\n```\n\nThat's it! The server is now running and ready for MCP client connections.\n\n### Testing the NPX Installation\n\n```bash\n# Test the CLI help\nnpx nexus-mcp --help\n\n# Test the version\nnpx nexus-mcp --version\n\n# Run with your API key\nOPENROUTER_API_KEY=your-key npx nexus-mcp\n```\n\n## Alternative: Local Development Installation\n\nFor local development or customization:\n\n1. Clone the repository:\n\n```bash\ngit clone https://github.com/adawalli/nexus.git\ncd nexus\n```\n\n2. Install dependencies:\n\n```bash\nnpm install\n```\n\n3. Build the server:\n\n```bash\nnpm run build\n```\n\n4. Configure your OpenRouter API key:\n\n```bash\n# Copy the example environment file\ncp .env.example .env\n\n# Edit .env and add your actual API key\n# OPENROUTER_API_KEY=your-api-key-here\n```\n\n5. Test the server:\n\n```bash\nnpm start\n```\n\n## Integration with MCP Clients\n\n### 🚀 Quick Setup with NPX (Recommended)\n\nThe easiest way to integrate with any MCP client is using NPX:\n\n### Claude Code\n\nAdd this server to your Claude Code MCP settings:\n\n1. Open your MCP settings file (usually `~/.claude/mcp_settings.json`)\n\n2. Add the server configuration using NPX:\n\n```json\n{\n  \"mcpServers\": {\n    \"nexus\": {\n      \"command\": \"npx\",\n      \"args\": [\"nexus-mcp\"],\n      \"env\": {\n        \"OPENROUTER_API_KEY\": \"your-api-key-here\"\n      }\n    }\n  }\n}\n```\n\n3. Restart Claude Code\n\n**That's it!** No installation, no build steps, no path configuration required.\n\n### Cursor\n\nConfigure the server in Cursor's MCP settings:\n\n1. Open Cursor settings and navigate to MCP servers\n\n2. Add a new server with:\n\n   - **Name**: `nexus`\n   - **Command**: `npx`\n   - **Args**: `[\"nexus-mcp\"]`\n   - **Environment Variables**:\n     - `OPENROUTER_API_KEY`: `your-api-key-here`\n\n3. Restart Cursor\n\n### Other MCP Clients\n\nFor any MCP-compatible client, use these connection details:\n\n- **Transport**: stdio\n- **Command**: `npx`\n- **Args**: `[\"nexus-mcp\"]`\n- **Environment Variables**: `OPENROUTER_API_KEY=your-api-key-here`\n\n### Alternative: Local Installation\n\nIf you prefer using a local installation (after following the local development setup):\n\n```json\n{\n  \"mcpServers\": {\n    \"nexus\": {\n      \"command\": \"node\",\n      \"args\": [\"/path/to/nexus-mcp/dist/cli.js\"],\n      \"env\": {\n        \"OPENROUTER_API_KEY\": \"your-api-key-here\"\n      }\n    }\n  }\n}\n```\n\n## Usage\n\nOnce integrated, you can use the search tool in your MCP client:\n\n### Basic Search\n\n```\nUse the search tool to find information about \"latest developments in AI\"\n```\n\n### Advanced Search with Parameters\n\n```\nSearch for \"climate change solutions\" using:\n- Model: perplexity/sonar\n- Max tokens: 2000\n- Temperature: 0.3\n```\n\n## Available Tools\n\n### `search`\n\nThe main search tool that provides AI-powered web search capabilities.\n\n**Parameters:**\n\n- `query` (required): Search query (1-2000 characters)\n- `model` (optional): Perplexity model to use (default: \"perplexity/sonar\")\n- `maxTokens` (optional): Maximum response tokens (1-4000, default: 1000)\n- `temperature` (optional): Response randomness (0-2, default: 0.7)\n\n**Example Response:**\n\n```\nBased on current information, here are the latest developments in AI...\n\n[Detailed AI-generated response with current information]\n\n---\n**Search Metadata:**\n- Model: perplexity/sonar\n- Response time: 1250ms\n- Tokens used: 850\n- Sources: 5 found\n```\n\n## Configuration\n\n### Environment Variables\n\n- `OPENROUTER_API_KEY` (required): Your OpenRouter API key\n- `NODE_ENV` (optional): Environment setting (development, production, test)\n- `LOG_LEVEL` (optional): Logging level (debug, info, warn, error)\n\n### Advanced Configuration\n\nThe server supports additional configuration through environment variables:\n\n- `OPENROUTER_TIMEOUT_MS`: Request timeout in milliseconds (default: 30000)\n- `OPENROUTER_MAX_RETRIES`: Maximum retry attempts (default: 3)\n- `OPENROUTER_BASE_URL`: Custom OpenRouter API base URL\n\n## Resources\n\nThe server provides a configuration status resource at `config://status` that shows:\n\n- Server health status\n- Configuration information (with masked API key)\n- Search tool availability\n- Server uptime and version\n\n## Troubleshooting\n\n### NPX-Specific Issues\n\n**\"npx: command not found\"**\n\n- Ensure Node.js 16+ is installed: `node --version`\n- Update npm: `npm install -g npm@latest`\n\n**\"Cannot find package 'nexus-mcp'\"**\n\n- The package may not be published yet. Use local installation instead\n- Verify network connectivity for npm registry access\n\n**NPX takes a long time to start**\n\n- This is normal on first run as NPX downloads the package\n- Subsequent runs will be faster due to caching\n- For faster startup, use local installation instead\n\n**\"Permission denied\" errors with NPX**\n\n- Try: `npx --yes nexus-mcp --stdio`\n- Or set npm permissions: `npm config set user 0 && npm config set unsafe-perm true`\n\n### Common Issues\n\n**\"Search functionality is not available\"**\n\n- Ensure `OPENROUTER_API_KEY` environment variable is set\n- Verify your API key is valid at [OpenRouter](https://openrouter.ai)\n- Check the server logs for initialization errors\n\n**\"Authentication failed: Invalid API key\"**\n\n- Double-check your API key format and validity\n- Ensure the key has sufficient credits/permissions\n- Test the key directly at OpenRouter dashboard\n\n**\"Rate limit exceeded\"**\n\n- Wait for the rate limit to reset (usually 1 minute)\n- Consider upgrading your OpenRouter plan for higher limits\n- Monitor usage in your OpenRouter dashboard\n\n**Connection timeouts**\n\n- Check your internet connection\n- The server will automatically retry failed requests\n- Increase timeout if needed: `OPENROUTER_TIMEOUT_MS=60000`\n\n**MCP client can't connect to server**\n\n- Verify your MCP configuration uses the correct command and arguments\n- Check that Node.js 16+ is available in your MCP client's environment\n- Ensure the API key is properly set in the environment variables\n\n### Debug Logging\n\nEnable debug logging by:\n\n**For local development:** Add `LOG_LEVEL=debug` to your `.env` file\n\n**For MCP clients:** Add `LOG_LEVEL: \"debug\"` to the `env` section of your MCP configuration\n\nThis will provide detailed information about:\n\n- Configuration loading\n- API requests and responses\n- Error details and stack traces\n- Performance metrics\n\n### Testing Connection\n\nYou can test if the server is working by checking the configuration status resource in your MCP client, or by running a simple search query.\n\n## Development\n\nFor developers working on this server:\n\n```bash\n# Development with hot reload\nnpm run dev\n\n# Run tests\nnpm test\n\n# Run tests with coverage\nnpm run test:coverage\n\n# Lint code\nnpm run lint\n\n# Format code\nnpm run format\n```\n\n## 💰 API Credits and Costs\n\nThis server uses OpenRouter's API, which charges based on token usage:\n\n- **Perplexity Sonar models**: Check current pricing at [OpenRouter Models](https://openrouter.ai/models)\n- **Usage monitoring**: Track consumption through the OpenRouter dashboard\n- **Cost control**: Set usage limits in your OpenRouter account\n- **Optimization**: Nexus includes built-in rate limiting and intelligent caching\n\n## 📚 Documentation\n\n<div align=\"center\">\n\n| 📖 **Guide**        | 🔗 **Link**                                 | 📝 **Description**               |\n| ------------------- | ------------------------------------------- | -------------------------------- |\n| **Quick Start**     | [Getting Started](#-quick-start)            | Zero-install setup in 30 seconds |\n| **API Reference**   | [MCP Tools](CLAUDE.md#development-commands) | Complete command reference       |\n| **Configuration**   | [Environment Setup](#configuration)         | Advanced configuration options   |\n| **Contributing**    | [Contributing Guide](CONTRIBUTING.md)       | Join our open source community   |\n| **Troubleshooting** | [Common Issues](#troubleshooting)           | Solutions to common problems     |\n\n</div>\n\n## 🤝 Contributing\n\nWe welcome contributions from developers of all experience levels!\n\n<table>\n<tr>\n<td width=\"33%\">\n\n### 🚀 **Get Started**\n\n- Fork the repository\n- Read our [Contributing Guide](CONTRIBUTING.md)\n- Check out [good first issues](https://github.com/search?q=repo%3Anexus-mcp+label%3A%22good+first+issue%22&type=issues)\n\n</td>\n<td width=\"33%\">\n\n### 🐛 **Report Issues**\n\n- [Bug Reports](https://github.com/adawalli/nexus/issues/new)\n- [Feature Requests](https://github.com/adawalli/nexus/issues/new)\n- [Ask Questions](https://github.com/adawalli/nexus/issues/new)\n\n</td>\n<td width=\"33%\">\n\n### 💬 **Join Community**\n\n- [GitHub Discussions](https://github.com/adawalli/nexus/discussions)\n- [Code of Conduct](CODE_OF_CONDUCT.md)\n- [Roadmap & Project Board](https://github.com/adawalli/nexus/projects)\n\n</td>\n</tr>\n</table>\n\n### 🌟 Recognition\n\nContributors are recognized in our:\n\n- [Contributors list](https://github.com/adawalli/nexus/graphs/contributors)\n- Release notes for significant contributions\n- Community spotlights and testimonials\n\n## 🔗 Related Projects\n\n- [Model Context Protocol](https://modelcontextprotocol.io) - The standard we implement\n- [OpenRouter](https://openrouter.ai) - Our AI model provider\n- [Claude Desktop](https://claude.ai) - Primary MCP client\n- [Cursor](https://cursor.sh) - AI-powered code editor with MCP support\n\n## 📞 Support & Community\n\n<div align=\"center\">\n\n| 💬 **Need Help?**    | 🔗 **Resource**                                                                                      |\n| -------------------- | ---------------------------------------------------------------------------------------------------- |\n| **Quick Questions**  | [GitHub Discussions](https://github.com/adawalli/nexus/discussions)                                  |\n| **Bug Reports**      | [GitHub Issues](https://github.com/adawalli/nexus/issues)                                            |\n| **Documentation**    | [OpenRouter Docs](https://openrouter.ai/docs) • [MCP Specification](https://modelcontextprotocol.io) |\n| **Feature Requests** | [Enhancement Proposals](https://github.com/adawalli/nexus/issues/new)                                |\n\n</div>\n\n## 📄 License\n\n**MIT License** - see [LICENSE](LICENSE) file for details.\n\n---\n\n<div align=\"center\">\n\n**Made with ❤️ by the open source community**\n\n[⭐ Star us on GitHub](https://github.com/adawalli/nexus) • [📦 View on NPM](https://www.npmjs.com/package/nexus-mcp) • [📚 Read the Docs](CLAUDE.md)\n\n_Nexus: AI integration without the complexity_\n\n[![Star History Chart](https://api.star-history.com/svg?repos=adawalli/nexus&type=Date)](https://star-history.com/#adawalli/nexus&Date)\n\n</div>\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "sonar",
        "search",
        "citations",
        "search server",
        "web search",
        "extraction adawalli"
      ],
      "category": "search--data-extraction"
    },
    "ananddtyagi--webpage-screenshot-mcp": {
      "owner": "ananddtyagi",
      "name": "webpage-screenshot-mcp",
      "url": "https://github.com/ananddtyagi/webpage-screenshot-mcp",
      "imageUrl": "",
      "description": "A MCP server for taking screenshots of webpages to use as feedback during UI developement.",
      "stars": 37,
      "forks": 4,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-10-03T22:32:42Z",
      "readme_content": "# Webpage Screenshot MCP Server\n\nAn MCP (Model Context Protocol) server that captures screenshots of web pages using Puppeteer. This server allows AI agents to visually verify web applications and see their progress when generating web apps.\n\n![Screen Recording May 27 2025 (2)](https://github.com/user-attachments/assets/9f186ec4-5a5c-449b-9a30-a5ec0cdba695)\n\n\n## Features\n\n- **Full page screenshots**: Capture entire web pages or just the viewport\n- **Element screenshots**: Target specific elements using CSS selectors\n- **Multiple formats**: Support for PNG, JPEG, and WebP formats\n- **Customizable options**: Set viewport size, image quality, wait conditions, and delays\n- **Base64 encoding**: Returns screenshots as base64 encoded images for easy integration\n- **Authentication support**: Manual login and cookie persistence\n- **Default browser integration**: Use your system's default browser for a more natural experience\n- **Session persistence**: Keep browser sessions open for multi-step workflows\n\n## Installation\n\n### Quick Start (Claude Desktop Extension)\n\nDrag and drop the generated `screenshot-webpage-mcp.dxt` file into Claude Desktop for automatic installation!\n\n### Manual Installation\n\nTo install and build the MCP from source:\n\n```bash\n# Clone the repository (if you haven't already)\ngit clone https://github.com/ananddtyagi/webpage-screenshot-mcp.git\ncd webpage-screenshot-mcp\n\n# Install dependencies\nnpm install\n\n# Build the project\nnpm run build\n```\n\nThe MCP server is built using TypeScript and compiled to JavaScript. The `dist` folder contains the compiled JavaScript files. \n\n### Adding to Claude or Cursor\n\nTo add this MCP to Claude Desktop or Cursor:\n\n1. **Claude Desktop**:\n   - Go to Settings > Developer\n   - Click \"Edit Config\"\n   - Add the following:\n\n   ```json\n    \"webpage-screenshot\": {\n      \"command\": \"node\",\n      \"args\": [\n        \"~/path/to/webpage-screenshot-mcp/dist/index.js\"\n      ]\n    }\n   ```\n   - Save and reload Claude\n\n2. **Cursor**:\n   - Open Cursor and go to Cursor Settings > MCP\n   - Click \"Add new global MCP server\"\n   - Add the following:\n  \n  ```json\n    \"webpage-screenshot\": {\n      \"command\": \"node\",\n      \"args\": [\"~/path/to/webpage-screenshot-mcp/dist/index.js\"]\n    }\n   ```\n\n   - Save and reload Cursor\n\n## Usage\n\n### Tools\n\nThis MCP server provides several tools:\n\n#### 1. login-and-wait\n\nOpens a webpage in a visible browser window for manual login, waits for user to complete login, then saves cookies.\n\n```json\n{\n  \"url\": \"https://example.com/login\",\n  \"waitMinutes\": 5,\n  \"successIndicator\": \".dashboard-welcome\",\n  \"useDefaultBrowser\": true\n}\n```\n\n- `url` (required): The URL of the login page\n- `waitMinutes` (optional): Maximum minutes to wait for login (default: 5)\n- `successIndicator` (optional): CSS selector or URL pattern that indicates successful login\n- `useDefaultBrowser` (optional): Whether to use the system's default browser (default: true)\n\n#### 2. screenshot-page\n\nCaptures a screenshot of a given URL and returns it as base64 encoded image.\n\n```json\n{\n  \"url\": \"https://example.com/dashboard\",\n  \"fullPage\": true,\n  \"width\": 1920,\n  \"height\": 1080,\n  \"format\": \"png\",\n  \"quality\": 80,\n  \"waitFor\": \"networkidle2\",\n  \"delay\": 500,\n  \"useSavedAuth\": true,\n  \"reuseAuthPage\": true,\n  \"useDefaultBrowser\": true,\n  \"visibleBrowser\": true\n}\n```\n\n- `url` (required): The URL of the webpage to screenshot\n- `fullPage` (optional): Whether to capture the full page or just the viewport (default: true)\n- `width` (optional): Viewport width in pixels (default: 1920)\n- `height` (optional): Viewport height in pixels (default: 1080)\n- `format` (optional): Image format - \"png\", \"jpeg\", or \"webp\" (default: \"png\")\n- `quality` (optional): Quality of the image (0-100), only applicable for jpeg and webp\n- `waitFor` (optional): When to consider page loaded - \"load\", \"domcontentloaded\", \"networkidle0\", or \"networkidle2\" (default: \"networkidle2\")\n- `delay` (optional): Additional delay in milliseconds after page load (default: 0)\n- `useSavedAuth` (optional): Whether to use saved cookies from previous login (default: true)\n- `reuseAuthPage` (optional): Whether to use the existing authenticated page (default: false)\n- `useDefaultBrowser` (optional): Whether to use the system's default browser (default: false)\n- `visibleBrowser` (optional): Whether to show the browser window (default: false)\n\n#### 3. screenshot-element\n\nCaptures a screenshot of a specific element on a webpage using a CSS selector.\n\n```json\n{\n  \"url\": \"https://example.com/dashboard\",\n  \"selector\": \".user-profile\",\n  \"waitForSelector\": true,\n  \"format\": \"png\",\n  \"quality\": 80,\n  \"padding\": 10,\n  \"useSavedAuth\": true,\n  \"useDefaultBrowser\": true,\n  \"visibleBrowser\": true\n}\n```\n\n- `url` (required): The URL of the webpage\n- `selector` (required): CSS selector for the element to screenshot\n- `waitForSelector` (optional): Whether to wait for the selector to appear (default: true)\n- `format` (optional): Image format - \"png\", \"jpeg\", or \"webp\" (default: \"png\")\n- `quality` (optional): Quality of the image (0-100), only applicable for jpeg and webp\n- `padding` (optional): Padding around the element in pixels (default: 0)\n- `useSavedAuth` (optional): Whether to use saved cookies from previous login (default: true)\n- `useDefaultBrowser` (optional): Whether to use the system's default browser (default: false)\n- `visibleBrowser` (optional): Whether to show the browser window (default: false)\n\n#### 4. clear-auth-cookies\n\nClears saved authentication cookies for a specific domain or all domains.\n\n```json\n{\n  \"url\": \"https://example.com\"\n}\n```\n\n- `url` (optional): URL of the domain to clear cookies for. If not provided, clears all cookies.\n\n## Default Browser Mode\n\nThe default browser mode allows you to use your system's regular browser (Chrome, Edge, etc.) instead of Puppeteer's bundled Chromium. This is useful for:\n\n1. Using your existing browser sessions and extensions\n2. Manually logging in to websites with your saved credentials\n3. Having a more natural browsing experience for multi-step workflows\n4. Testing with the same browser environment as your users\n\nTo enable default browser mode, set `useDefaultBrowser: true` and `visibleBrowser: true` in your tool parameters.\n\n### How Default Browser Mode Works\n\nWhen you enable default browser mode:\n\n1. The tool will attempt to locate your system's default browser (Chrome, Edge, etc.)\n2. It launches your browser with remote debugging enabled on a random port\n3. Puppeteer connects to this browser instance instead of launching its own\n4. Your existing profiles, extensions, and cookies are available during the session\n5. The browser window remains visible so you can interact with it manually\n\nThis mode is particularly useful for workflows that require authentication or complex user interactions.\n\n## Browser Persistence\n\nThe MCP server can maintain a persistent browser session across multiple tool calls:\n\n1. When you use `login-and-wait`, the browser session is kept open\n2. Subsequent calls to `screenshot-page` or `screenshot-element` with `reuseAuthPage: true` will use the same page\n3. This allows for multi-step workflows without having to re-authenticate\n\n## Cookie Management\n\nCookies are automatically saved for each domain you visit:\n\n1. After using `login-and-wait`, cookies are saved to the `.mcp-screenshot-cookies` directory in your home folder\n2. These cookies are automatically loaded when visiting the same domain again with `useSavedAuth: true`\n3. You can clear cookies using the `clear-auth-cookies` tool\n\n## Example Workflow: Protected Page Screenshots\n\nHere's an example workflow for taking screenshots of pages that require authentication:\n\n1. **Manual Login Phase**\n\n```json\n{\n  \"name\": \"login-and-wait\",\n  \"parameters\": {\n    \"url\": \"https://example.com/login\",\n    \"waitMinutes\": 3,\n    \"successIndicator\": \".dashboard-welcome\",\n    \"useDefaultBrowser\": true\n  }\n}\n```\n\nThis will open your default browser with the login page. You can manually log in, and once complete (either by detecting the success indicator or after navigating away from the login page), the session cookies will be saved.\n\n2. **Take Screenshots Using Saved Session**\n\n```json\n{\n  \"name\": \"screenshot-page\",\n  \"parameters\": {\n    \"url\": \"https://example.com/account\",\n    \"fullPage\": true,\n    \"useSavedAuth\": true,\n    \"reuseAuthPage\": true,\n    \"useDefaultBrowser\": true,\n    \"visibleBrowser\": true\n  }\n}\n```\n\nThis will take a screenshot of the account page using your saved authentication cookies in the same browser window.\n\n3. **Take Screenshots of Specific Elements**\n\n```json\n{\n  \"name\": \"screenshot-element\",\n  \"parameters\": {\n    \"url\": \"https://example.com/dashboard\",\n    \"selector\": \".user-profile-section\",\n    \"useSavedAuth\": true,\n    \"useDefaultBrowser\": true,\n    \"visibleBrowser\": true\n  }\n}\n```\n\n4. **Clear Cookies When Done**\n\n```json\n{\n  \"name\": \"clear-auth-cookies\",\n  \"parameters\": {\n    \"url\": \"https://example.com\"\n  }\n}\n```\n\nThis workflow allows you to interact with protected pages as if you were a regular user, completing the full authentication flow in your default browser.\n\n## Headless vs. Visible Mode\n\n- **Headless mode** (`visibleBrowser: false`): Faster and more suitable for automated workflows where no user interaction is needed.\n- **Visible mode** (`visibleBrowser: true`): Shows the browser window, allowing for user interaction and manual verification. Required for `useDefaultBrowser: true`.\n\n## Platform Support\n\nThe default browser detection works on:\n\n- **macOS**: Detects Chrome, Edge, and Safari\n- **Windows**: Detects Chrome and Edge via registry or common installation paths\n- **Linux**: Detects Chrome and Chromium via system commands\n\n## Troubleshooting\n\n### Common Issues\n\n1. **Default browser not found**: If the system can't find your default browser, it will fall back to Puppeteer's bundled Chromium.\n2. **Connection issues**: If there are problems connecting to the browser's debugging port, check if another instance is already using that port.\n3. **Cookie issues**: If authentication isn't working, try clearing cookies with the `clear-auth-cookies` tool.\n\n### Debugging\n\nThe MCP server logs helpful error messages to the console when issues occur. Check these messages for troubleshooting information.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "screenshots",
        "webpages",
        "webpage",
        "screenshot mcp",
        "screenshots webpages",
        "webpage screenshot"
      ],
      "category": "search--data-extraction"
    },
    "blazickjp--arxiv-mcp-server": {
      "owner": "blazickjp",
      "name": "arxiv-mcp-server",
      "url": "https://github.com/blazickjp/arxiv-mcp-server",
      "imageUrl": "",
      "description": "Search ArXiv research papers",
      "stars": 1743,
      "forks": 121,
      "license": "Apache License 2.0",
      "language": "Python",
      "updated_at": "2025-10-04T01:41:17Z",
      "readme_content": "[![Twitter Follow](https://img.shields.io/twitter/follow/JoeBlazick?style=social)](https://twitter.com/JoeBlazick)\n[![smithery badge](https://smithery.ai/badge/arxiv-mcp-server)](https://smithery.ai/server/arxiv-mcp-server)\n[![Python Version](https://img.shields.io/badge/python-3.11+-blue.svg)](https://www.python.org/downloads/)\n[![Tests](https://github.com/blazickjp/arxiv-mcp-server/actions/workflows/tests.yml/badge.svg)](https://github.com/blazickjp/arxiv-mcp-server/actions/workflows/tests.yml)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![PyPI Downloads](https://img.shields.io/pypi/dm/arxiv-mcp-server.svg)](https://pypi.org/project/arxiv-mcp-server/)\n[![PyPI Version](https://img.shields.io/pypi/v/arxiv-mcp-server.svg)](https://pypi.org/project/arxiv-mcp-server/)\n\n# ArXiv MCP Server\n\n> 🔍 Enable AI assistants to search and access arXiv papers through a simple MCP interface.\n\nThe ArXiv MCP Server provides a bridge between AI assistants and arXiv's research repository through the Model Context Protocol (MCP). It allows AI models to search for papers and access their content in a programmatic way.\n\n<div align=\"center\">\n  \n🤝 **[Contribute](https://github.com/blazickjp/arxiv-mcp-server/blob/main/CONTRIBUTING.md)** • \n📝 **[Report Bug](https://github.com/blazickjp/arxiv-mcp-server/issues)**\n\n<a href=\"https://www.pulsemcp.com/servers/blazickjp-arxiv-mcp-server\"><img src=\"https://www.pulsemcp.com/badge/top-pick/blazickjp-arxiv-mcp-server\" width=\"400\" alt=\"Pulse MCP Badge\"></a>\n</div>\n\n## ✨ Core Features\n\n- 🔎 **Paper Search**: Query arXiv papers with filters for date ranges and categories\n- 📄 **Paper Access**: Download and read paper content\n- 📋 **Paper Listing**: View all downloaded papers\n- 🗃️ **Local Storage**: Papers are saved locally for faster access\n- 📝 **Prompts**: A Set of Research Prompts\n\n## 🚀 Quick Start\n\n### Installing via Smithery\n\nTo install ArXiv Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/arxiv-mcp-server):\n\n```bash\nnpx -y @smithery/cli install arxiv-mcp-server --client claude\n```\n\n### Installing Manually\nInstall using uv:\n\n```bash\nuv tool install arxiv-mcp-server\n```\n\nFor development:\n\n```bash\n# Clone and set up development environment\ngit clone https://github.com/blazickjp/arxiv-mcp-server.git\ncd arxiv-mcp-server\n\n# Create and activate virtual environment\nuv venv\nsource .venv/bin/activate\n\n# Install with test dependencies\nuv pip install -e \".[test]\"\n```\n\n### 🔌 MCP Integration\n\nAdd this configuration to your MCP client config file:\n\n```json\n{\n    \"mcpServers\": {\n        \"arxiv-mcp-server\": {\n            \"command\": \"uv\",\n            \"args\": [\n                \"tool\",\n                \"run\",\n                \"arxiv-mcp-server\",\n                \"--storage-path\", \"/path/to/paper/storage\"\n            ]\n        }\n    }\n}\n```\n\nFor Development:\n\n```json\n{\n    \"mcpServers\": {\n        \"arxiv-mcp-server\": {\n            \"command\": \"uv\",\n            \"args\": [\n                \"--directory\",\n                \"path/to/cloned/arxiv-mcp-server\",\n                \"run\",\n                \"arxiv-mcp-server\",\n                \"--storage-path\", \"/path/to/paper/storage\"\n            ]\n        }\n    }\n}\n```\n\n## 💡 Available Tools\n\nThe server provides four main tools:\n\n### 1. Paper Search\nSearch for papers with optional filters:\n\n```python\nresult = await call_tool(\"search_papers\", {\n    \"query\": \"transformer architecture\",\n    \"max_results\": 10,\n    \"date_from\": \"2023-01-01\",\n    \"categories\": [\"cs.AI\", \"cs.LG\"]\n})\n```\n\n### 2. Paper Download\nDownload a paper by its arXiv ID:\n\n```python\nresult = await call_tool(\"download_paper\", {\n    \"paper_id\": \"2401.12345\"\n})\n```\n\n### 3. List Papers\nView all downloaded papers:\n\n```python\nresult = await call_tool(\"list_papers\", {})\n```\n\n### 4. Read Paper\nAccess the content of a downloaded paper:\n\n```python\nresult = await call_tool(\"read_paper\", {\n    \"paper_id\": \"2401.12345\"\n})\n```\n\n## 📝 Research Prompts\n\nThe server offers specialized prompts to help analyze academic papers:\n\n### Paper Analysis Prompt\nA comprehensive workflow for analyzing academic papers that only requires a paper ID:\n\n```python\nresult = await call_prompt(\"deep-paper-analysis\", {\n    \"paper_id\": \"2401.12345\"\n})\n```\n\nThis prompt includes:\n- Detailed instructions for using available tools (list_papers, download_paper, read_paper, search_papers)\n- A systematic workflow for paper analysis\n- Comprehensive analysis structure covering:\n  - Executive summary\n  - Research context\n  - Methodology analysis\n  - Results evaluation\n  - Practical and theoretical implications\n  - Future research directions\n  - Broader impacts\n\n## ⚙️ Configuration\n\nConfigure through environment variables:\n\n| Variable | Purpose | Default |\n|----------|---------|---------|\n| `ARXIV_STORAGE_PATH` | Paper storage location | ~/.arxiv-mcp-server/papers |\n\n## 🧪 Testing\n\nRun the test suite:\n\n```bash\npython -m pytest\n```\n\n## 📄 License\n\nReleased under the MIT License. See the LICENSE file for details.\n\n---\n\n<div align=\"center\">\n\nMade with ❤️ by the Pearl Labs Team\n\n<a href=\"https://glama.ai/mcp/servers/04dtxi5i5n\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/04dtxi5i5n/badge\" alt=\"ArXiv Server MCP server\" /></a>\n</div>\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "search",
        "arxiv",
        "mcp",
        "search arxiv",
        "arxiv mcp",
        "server search"
      ],
      "category": "search--data-extraction"
    },
    "cameronrye--gopher-mcp": {
      "owner": "cameronrye",
      "name": "gopher-mcp",
      "url": "https://github.com/cameronrye/gopher-mcp",
      "imageUrl": "",
      "description": "Modern, cross-platform MCP server enabling AI assistants to browse and interact with both Gopher protocol and Gemini protocol resources safely and efficiently. Features dual protocol support, TLS security, and structured content extraction.",
      "stars": 1,
      "forks": 1,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-19T02:54:19Z",
      "readme_content": "# Gopher & Gemini MCP Server\n\n[![CI](https://github.com/cameronrye/gopher-mcp/actions/workflows/ci.yml/badge.svg)](https://github.com/cameronrye/gopher-mcp/actions/workflows/ci.yml)\n[![Documentation](https://github.com/cameronrye/gopher-mcp/actions/workflows/docs.yml/badge.svg)](https://github.com/cameronrye/gopher-mcp/actions/workflows/docs.yml)\n[![PyPI version](https://badge.fury.io/py/gopher-mcp.svg)](https://badge.fury.io/py/gopher-mcp)\n[![Python 3.11+](https://img.shields.io/badge/python-3.11+-blue.svg)](https://www.python.org/downloads/)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![codecov](https://codecov.io/gh/cameronrye/gopher-mcp/branch/main/graph/badge.svg)](https://codecov.io/gh/cameronrye/gopher-mcp)\n[![Code style: ruff](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/ruff/main/assets/badge/v2.json)](https://github.com/astral-sh/ruff)\n[![Checked with mypy](https://www.mypy-lang.org/static/mypy_badge.svg)](https://mypy-lang.org/)\n[![Downloads](https://pepy.tech/badge/gopher-mcp)](https://pepy.tech/project/gopher-mcp)\n\nA modern, cross-platform [Model Context Protocol (MCP)](https://modelcontextprotocol.io/) server that enables AI assistants to\nbrowse and interact with both [Gopher protocol](<https://en.wikipedia.org/wiki/Gopher_(protocol)>) and\n[Gemini protocol](https://geminiprotocol.net/) resources safely and efficiently.\n\n## 🌟 Overview\n\nThe Gopher & Gemini MCP Server bridges vintage and modern alternative internet protocols with AI assistants, allowing LLMs like\nClaude to explore the unique content and communities that thrive on both Gopherspace and Geminispace. Built with FastMCP and\nmodern Python practices, it provides secure, efficient gateways to these distinctive internet protocols.\n\n**Key Benefits:**\n\n- 🔍 **Discover alternative internet content** - Access unique resources on both Gopher and Gemini protocols\n- 🛡️ **Safe exploration** - Built-in security safeguards, TLS encryption, and content filtering\n- 🚀 **Modern implementation** - Uses FastMCP framework with async/await patterns\n- 🔧 **Developer-friendly** - Comprehensive testing, type hints, and documentation\n- 🔐 **Advanced security** - TOFU certificate validation and client certificate support for Gemini\n\n## ✨ Features\n\n- 🔧 **Dual Protocol Support**: `gopher_fetch` and `gemini_fetch` tools for comprehensive protocol coverage\n- 📋 **Comprehensive Gopher Support**: Handles menus (type 1), text files (type 0), search servers (type 7), and binary files\n- 🌐 **Full Gemini Implementation**: Native gemtext parsing, TLS security, and status code handling\n- 🔐 **Advanced Security**: TOFU certificate validation, client certificates, and secure TLS connections\n- 🛡️ **Safety First**: Built-in timeouts, size limits, input sanitization, and host allowlists\n- 🤖 **LLM-Optimized**: Returns structured JSON responses designed for AI consumption\n- 🖥️ **Cross-Platform**: Works seamlessly on Windows, macOS, and Linux\n- 🔬 **Modern Development**: Full type checking, linting, testing, and CI/CD pipeline\n- ⚡ **High Performance**: Async/await patterns with intelligent caching\n\n## 📚 Documentation\n\nComplete documentation is available at **[cameronrye.github.io/gopher-mcp](https://cameronrye.github.io/gopher-mcp)**\n\n- 📖 [Installation Guide](https://cameronrye.github.io/gopher-mcp/installation/)\n- 🔧 [API Reference](https://cameronrye.github.io/gopher-mcp/api-reference/)\n- 🚀 [Advanced Features](https://cameronrye.github.io/gopher-mcp/advanced-features/)\n- 🤖 [AI Assistant Guide](https://cameronrye.github.io/gopher-mcp/ai-assistant-guide/)\n\n## 🚀 Quick Start\n\n### 📋 Prerequisites\n\n- **Python 3.11+** - [Download here](https://www.python.org/downloads/)\n- **uv package manager** - [Install uv](https://docs.astral.sh/uv/getting-started/installation/)\n\n### 📦 Installation\n\n#### Option 1: Development Installation (Recommended)\n\n```bash\n# Clone the repository\ngit clone https://github.com/cameronrye/gopher-mcp.git\ncd gopher-mcp\n\n# Set up development environment\n./scripts/dev-setup.sh  # Unix/macOS\n# or\nscripts\\dev-setup.bat   # Windows\n\n# Run the server\nuv run task serve\n```\n\n#### Option 2: PyPI Installation\n\n```bash\n# Install from PyPI (recommended for end users)\npip install gopher-mcp\n\n# Or with uv\nuv add gopher-mcp\n```\n\n#### Option 3: Development Installation\n\n```bash\n# Install directly from GitHub\nuv add git+https://github.com/cameronrye/gopher-mcp.git\n\n# Or install in development mode\ngit clone https://github.com/cameronrye/gopher-mcp.git\ncd gopher-mcp\nuv sync --all-extras\n```\n> methods above.\n\n### 🔧 Claude Desktop Integration\n\nAdd to your `claude_desktop_config.json`:\n\n**Unix/macOS/Linux:**\n\n```json\n{\n  \"mcpServers\": {\n    \"gopher\": {\n      \"command\": \"uv\",\n      \"args\": [\"--directory\", \"/path/to/gopher-mcp\", \"run\", \"task\", \"serve\"],\n      \"env\": {\n        \"MAX_RESPONSE_SIZE\": \"1048576\",\n        \"TIMEOUT_SECONDS\": \"30\"\n      }\n    }\n  }\n}\n```\n\n**Windows:**\n\n```json\n{\n  \"mcpServers\": {\n    \"gopher\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"--directory\",\n        \"C:\\\\path\\\\to\\\\gopher-mcp\",\n        \"run\",\n        \"task\",\n        \"serve\"\n      ],\n      \"env\": {\n        \"MAX_RESPONSE_SIZE\": \"1048576\",\n        \"TIMEOUT_SECONDS\": \"30\"\n      }\n    }\n  }\n}\n```\n\n## 🛠️ Cross-Platform Development\n\nThis project includes a **unified Python-based task management system** that works across all platforms:\n\n### Recommended (All Platforms)\n\n```bash\npython task.py <command>    # Unified Python task runner (recommended)\n```\n\n### Alternative Options\n\n```bash\n# Unix/macOS/Linux\nmake <command>              # Traditional make (delegates to task.py)\n\n# Universal fallback\nuv run task <command>       # Direct taskipy usage\n```\n\n### Available Commands\n\n| Command            | Description                    |\n| ------------------ | ------------------------------ |\n| `dev-setup`        | Set up development environment |\n| `install-hooks`    | Install pre-commit hooks       |\n| `lint`             | Run ruff linting               |\n| `format`           | Format code with ruff          |\n| `typecheck`        | Run mypy type checking         |\n| `quality`          | Run all quality checks         |\n| `check`            | Run lint + typecheck           |\n| `test`             | Run all tests                  |\n| `test-cov`         | Run tests with coverage        |\n| `test-unit`        | Run unit tests only            |\n| `test-integration` | Run integration tests          |\n| `serve`            | Run MCP server (stdio)         |\n| `serve-http`       | Run MCP server (HTTP)          |\n| `docs-serve`       | Serve docs locally             |\n| `docs-build`       | Build documentation            |\n| `clean`            | Clean build artifacts          |\n| `ci`               | Run CI pipeline locally        |\n\n## 📖 Usage\n\nThe server provides two powerful MCP tools for exploring alternative internet protocols:\n\n### `gopher_fetch` Tool\n\nFetches Gopher menus, text files, or metadata by URL with comprehensive error handling and security safeguards.\n\n**Parameters:**\n\n- `url` (string, required): Full Gopher URL (e.g., `gopher://gopher.floodgap.com/1/`)\n\n**Response Types:**\n\n- **MenuResult**: For Gopher menus (type 1) and search results (type 7)\n  - Contains structured menu items with type, display text, selector, host, and port\n- **TextResult**: For text files (type 0)\n  - Returns the full text content with metadata\n- **BinaryResult**: Metadata only for binary files (types 4, 5, 6, 9, g, I)\n  - Provides file information without downloading binary content\n- **ErrorResult**: For errors or unsupported content\n  - Includes detailed error messages and troubleshooting hints\n\n### `gemini_fetch` Tool\n\nFetches Gemini content with full TLS security, TOFU certificate validation, and native gemtext parsing.\n\n**Parameters:**\n\n- `url` (string, required): Full Gemini URL (e.g., `gemini://geminiprotocol.net/`)\n\n**Response Types:**\n\n- **GeminiGemtextResult**: For gemtext content (text/gemini)\n  - Parsed gemtext document with structured lines, links, and headings\n- **GeminiSuccessResult**: For other text and binary content\n  - Raw content with MIME type information\n- **GeminiInputResult**: For input requests (status 10-11)\n  - Prompts for user input with optional sensitive flag\n- **GeminiRedirectResult**: For redirects (status 30-31)\n  - New URL for temporary or permanent redirects\n- **GeminiErrorResult**: For errors (status 40-69)\n  - Detailed error information with status codes\n- **GeminiCertificateResult**: For certificate requests (status 60-69)\n  - Certificate requirement information\n\n### 🌐 Example URLs to Try\n\n#### Gopher Protocol\n\n```bash\n# Classic Gopher menu\ngopher://gopher.floodgap.com/1/\n\n# Gopher news and information\ngopher://gopher.floodgap.com/1/gopher\n\n# Search example (type 7)\ngopher://gopher.floodgap.com/7/v2/vs\n\n# Text file example\ngopher://gopher.floodgap.com/0/gopher/welcome\n```\n\n#### Gemini Protocol\n\n```bash\n# Gemini protocol homepage\ngemini://geminiprotocol.net/\n\n# Gemini software directory\ngemini://geminiprotocol.net/software/\n\n# Example personal gemlog\ngemini://warmedal.se/~antenna/\n\n# Gemini search aggregator\ngemini://kennedy.gemi.dev/\n```\n\n### 🤖 Example AI Interactions\n\nOnce configured, you can ask Claude:\n\n**Gopher Exploration:**\n\n- _\"Browse the main Gopher menu at gopher.floodgap.com\"_\n- _\"Search for 'python' on the Veronica-2 search server\"_\n- _\"Show me the welcome text from Floodgap's Gopher server\"_\n- _\"What's available in the Gopher community directory?\"_\n\n**Gemini Exploration:**\n\n- _\"Fetch the Gemini protocol homepage\"_\n- _\"Show me the software directory on geminiprotocol.net\"_\n- _\"Browse the latest posts from a gemlog\"_\n- _\"What's the difference between Gopher and Gemini protocols?\"_\n\n## 🔧 Development\n\n### 📁 Project Structure\n\n```text\ngopher-mcp/\n├── src/gopher_mcp/          # Main package\n│   ├── __init__.py          # Package initialization\n│   ├── server.py            # FastMCP server implementation\n│   ├── gopher_client.py     # Gopher protocol client\n│   ├── models.py            # Pydantic data models\n│   ├── tools.py             # MCP tool definitions\n│   └── utils.py             # Utility functions\n├── tests/                   # Comprehensive test suite\n│   ├── test_server.py       # Server tests\n│   ├── test_gopher_client.py # Client tests\n│   └── test_integration.py  # Integration tests\n├── docs/                    # MkDocs documentation\n├── scripts/                 # Development scripts\n├── .github/workflows/       # CI/CD pipelines\n├── Makefile                 # Unix/macOS task runner\n├── task.bat                 # Windows task runner\n└── pyproject.toml           # Modern Python project config\n```\n\n### 🔄 Development Workflow\n\n1. **Setup**: `uv run task dev-setup` - Install dependencies and pre-commit hooks\n2. **Code**: Make your changes with full IDE support (type hints, linting)\n3. **Quality**: `uv run task quality` - Run all quality checks (lint + typecheck + test)\n4. **Test**: `uv run task test-cov` - Run tests with coverage reporting\n5. **Commit**: Pre-commit hooks ensure code quality automatically\n\n### 🧪 Testing\n\n```bash\n# Run all tests\nuv run task test\n\n# Run with coverage\nuv run task test-cov\n\n# Run specific test types\nuv run task test-unit\nuv run task test-integration\n\n# Run tests in watch mode during development\nuv run pytest --watch\n```\n\n## ⚙️ Configuration\n\nThe server can be configured through environment variables for both protocols:\n\n### Gopher Configuration\n\n| Variable                   | Description                    | Default         | Example                |\n| -------------------------- | ------------------------------ | --------------- | ---------------------- |\n| `GOPHER_MAX_RESPONSE_SIZE` | Maximum response size in bytes | `1048576` (1MB) | `2097152`              |\n| `GOPHER_TIMEOUT_SECONDS`   | Request timeout in seconds     | `30`            | `60`                   |\n| `GOPHER_CACHE_ENABLED`     | Enable response caching        | `true`          | `false`                |\n| `GOPHER_CACHE_TTL_SECONDS` | Cache time-to-live in seconds  | `300`           | `600`                  |\n| `GOPHER_ALLOWED_HOSTS`     | Comma-separated allowed hosts  | `None` (all)    | `example.com,test.com` |\n\n### Gemini Configuration\n\n| Variable                      | Description                        | Default         | Example                |\n| ----------------------------- | ---------------------------------- | --------------- | ---------------------- |\n| `GEMINI_MAX_RESPONSE_SIZE`    | Maximum response size in bytes     | `1048576` (1MB) | `2097152`              |\n| `GEMINI_TIMEOUT_SECONDS`      | Request timeout in seconds         | `30`            | `60`                   |\n| `GEMINI_CACHE_ENABLED`        | Enable response caching            | `true`          | `false`                |\n| `GEMINI_CACHE_TTL_SECONDS`    | Cache time-to-live in seconds      | `300`           | `600`                  |\n| `GEMINI_ALLOWED_HOSTS`        | Comma-separated allowed hosts      | `None` (all)    | `example.org,test.org` |\n| `GEMINI_TOFU_ENABLED`         | Enable TOFU certificate validation | `true`          | `false`                |\n| `GEMINI_CLIENT_CERTS_ENABLED` | Enable client certificate support  | `true`          | `false`                |\n\n### Example Configuration\n\n```bash\n# Gopher settings\nexport GOPHER_MAX_RESPONSE_SIZE=2097152\nexport GOPHER_TIMEOUT_SECONDS=60\nexport GOPHER_CACHE_ENABLED=true\nexport GOPHER_ALLOWED_HOSTS=\"gopher.floodgap.com,gopher.quux.org\"\n\n# Gemini settings\nexport GEMINI_MAX_RESPONSE_SIZE=2097152\nexport GEMINI_TIMEOUT_SECONDS=60\nexport GEMINI_TOFU_ENABLED=true\nexport GEMINI_CLIENT_CERTS_ENABLED=true\nexport GEMINI_ALLOWED_HOSTS=\"geminiprotocol.net,warmedal.se\"\n\n# Run with custom config\nuv run task serve\n```\n\n## 🤝 Contributing\n\nWe welcome contributions! Please see our [Contributing Guidelines](CONTRIBUTING.md) for details.\n\n### Quick Contribution Steps\n\n1. **Fork** the repository on GitHub\n2. **Clone** your fork: `git clone https://github.com/your-username/gopher-mcp.git`\n3. **Setup** development environment: `uv run task dev-setup`\n4. **Create** a feature branch: `git checkout -b feature/amazing-feature`\n5. **Make** your changes with tests\n6. **Quality** check: `uv run task quality`\n7. **Commit** your changes: `git commit -m 'Add amazing feature'`\n8. **Push** to your fork: `git push origin feature/amazing-feature`\n9. **Submit** a pull request with a clear description\n\n### Development Standards\n\n- ✅ **Type hints** for all functions and methods\n- ✅ **Comprehensive tests** with >90% coverage\n- ✅ **Documentation** for all public APIs\n- ✅ **Security** considerations for all network operations\n- ✅ **Cross-platform** compatibility (Windows, macOS, Linux)\n\n## 📄 License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n\n## 🙏 Acknowledgments\n\n- **[Model Context Protocol](https://modelcontextprotocol.io/)** by Anthropic - The foundation that makes this integration possible\n- **[FastMCP](https://github.com/jlowin/fastmcp)** - High-level Python framework for building MCP servers\n- **[Pituophis](https://github.com/dotcomboom/pituophis)** - Excellent Python Gopher client library\n- **The Gopher Protocol Community** - Keeping the spirit of the early internet alive\n\n## 🔗 Related Projects\n\n- [Model Context Protocol Servers](https://github.com/modelcontextprotocol/servers) - Official MCP server implementations\n- [Awesome MCP Servers](https://github.com/punkpeye/awesome-mcp-servers) - Curated list of MCP servers\n- [Claude Desktop](https://claude.ai/download) - AI assistant that supports MCP\n\n## 📞 Support\n\n- 🐛 **Bug Reports**: [GitHub Issues](https://github.com/cameronrye/gopher-mcp/issues)\n- 💡 **Feature Requests**: [GitHub Discussions](https://github.com/cameronrye/gopher-mcp/discussions)\n- 📖 **Documentation**: [Project Docs](https://cameronrye.github.io/gopher-mcp/)\n- 💬 **Community**: [MCP Discord](https://discord.gg/modelcontextprotocol)\n\n---\n\n<div align=\"center\">\n\n**Made with ❤️ for the intersection of vintage internet protocols and modern AI**\n\n[⭐ Star this project](https://github.com/cameronrye/gopher-mcp) if you find it useful!\n\n</div>\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "gopher",
        "protocol",
        "tls",
        "gopher protocol",
        "gopher mcp",
        "mcp server"
      ],
      "category": "search--data-extraction"
    },
    "chanmeng--google-news-mcp-server": {
      "owner": "chanmeng",
      "name": "google-news-mcp-server",
      "url": "https://github.com/ChanMeng666/server-google-news",
      "imageUrl": "",
      "description": "Google News integration with automatic topic categorization, multi-language support, and comprehensive search capabilities including headlines, stories, and related topics through [SerpAPI](https://serpapi.com/).",
      "stars": 0,
      "forks": 0,
      "license": "Unknown",
      "language": "Unknown",
      "updated_at": "",
      "readme_content": "",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "serpapi",
        "news",
        "google",
        "google news",
        "news integration",
        "topics serpapi"
      ],
      "category": "search--data-extraction"
    },
    "deadletterq--mcp-opennutrition": {
      "owner": "deadletterq",
      "name": "mcp-opennutrition",
      "url": "https://github.com/deadletterq/mcp-opennutrition",
      "imageUrl": "",
      "description": "Local MCP server for searching 300,000+ foods, nutrition facts, and barcodes from the OpenNutrition database.",
      "stars": 114,
      "forks": 9,
      "license": "GNU General Public License v3.0",
      "language": "TypeScript",
      "updated_at": "2025-09-23T13:03:29Z",
      "readme_content": "# MCP OpenNutrition\n\nA Model Context Protocol (MCP) server providing access to the comprehensive OpenNutrition food database with 300,000+ food items, nutritional data, and barcode lookups.\n\nOpenNutrition addresses the longstanding issues with fragmented and unreliable nutritional data by combining authoritative public sources (USDA, CNF, FRIDA, AUSNUT). Unlike other databases that suffer from inconsistent user-generated content or restrictive commercial licensing, OpenNutrition provides transparent, comprehensive, and accurate nutritional data that's freely accessible to developers and researchers.\n\n## Tools\n\n- **Search by Name**: Find foods by name, brand, or partial matches\n- **Browse Foods**: Get paginated lists of all available foods\n- **Get by ID**: Retrieve detailed nutritional information using food IDs\n- **Barcode Lookup**: Find foods using EAN-13 barcodes\n\n## Installation\n\n1. Install dependencies:\n   ```bash\n   npm install\n   ```\n\n2. Build the project:\n   ```bash\n   npm run build\n   ```\n\n3. Add to your Claude/Cline MCP configuration (set the same version of node that you used to build the project):\n   ```json\n   \"mcp-opennutrition\": {\n       \"command\": \"/Users/YOUR.USERNAME/.nvm/versions/node/v20.19.0/bin/node\",\n       \"args\": [\n           \"/ABSOLUTE/PATH/TO/PARENT/FOLDER/mcp-opennutrition/build/index.js\"\n       ]\n   }\n   ```\n\n## Data Source\n\nThis server uses the [OpenNutrition dataset](https://www.opennutrition.app/).\n\nThe dataset provides comprehensive nutritional profiles including macronutrients, vitamins, and minerals.\n\n## Usage\n\nOnce configured, the MCP server runs fully locally on your machine and automatically provides food and nutrition query capabilities to Claude/Cline. All data processing and queries happen locally with no external API calls, ensuring privacy and fast response times.\n\n## Example\n\nHere is an example of how Claude uses the tool for a brownie recipe:",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "opennutrition",
        "nutrition",
        "foods",
        "opennutrition database",
        "mcp opennutrition",
        "opennutrition local"
      ],
      "category": "search--data-extraction"
    },
    "dealx--mcp-server": {
      "owner": "dealx",
      "name": "mcp-server",
      "url": "https://github.com/DealExpress/mcp-server",
      "imageUrl": "",
      "description": "MCP Server for DealX platform",
      "stars": 0,
      "forks": 0,
      "license": "Unknown",
      "language": "Unknown",
      "updated_at": "",
      "readme_content": "",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "dealx",
        "mcp",
        "search",
        "dealx mcp",
        "server dealx",
        "server mcp"
      ],
      "category": "search--data-extraction"
    },
    "devflowinc--trieve": {
      "owner": "devflowinc",
      "name": "trieve",
      "url": "https://github.com/devflowinc/trieve/tree/main/clients/mcp-server",
      "imageUrl": "",
      "description": "Crawl, embed, chunk, search, and retrieve information from datasets through [Trieve](https://trieve.ai)",
      "stars": 2510,
      "forks": 224,
      "license": "MIT License",
      "language": "Rust",
      "updated_at": "2025-10-04T09:42:16Z",
      "readme_content": "<p align=\"center\">\n  <img height=\"100\" src=\"https://cdn.trieve.ai/trieve-logo.png\" alt=\"Trieve Logo\">\n</p>\n<p align=\"center\">\n<strong><a href=\"https://dashboard.trieve.ai\">Sign Up (1k chunks free)</a> | <a href=\"https://pdf2md.trieve.ai\">PDF2MD</a> | <a href=\"https://docs.trieve.ai\">Hacker News Search Engine</a> | <a href=\"https://docs.trieve.ai\">Documentation</a> | <a href=\"https://cal.com/nick.k/meet\">Meet a Maintainer</a> | <a href=\"https://discord.gg/eBJXXZDB8z\">Discord</a> | <a href=\"https://matrix.to/#/#trieve-general:trieve.ai\">Matrix</a>\n</strong>\n</p>\n\n<p align=\"center\">\n    <a href=\"https://github.com/devflowinc/trieve/stargazers\">\n        <img src=\"https://img.shields.io/github/stars/devflowinc/trieve.svg?style=flat&color=yellow\" alt=\"Github stars\"/>\n    </a>\n    <a href=\"https://discord.gg/CuJVfgZf54\">\n        <img src=\"https://img.shields.io/discord/1130153053056684123.svg?label=Discord&logo=Discord&colorB=7289da&style=flat\" alt=\"Join Discord\"/>\n    </a>\n    <a href=\"https://matrix.to/#/#trieve-general:trieve.ai\">\n        <img src=\"https://img.shields.io/badge/matrix-join-purple?style=flat&logo=matrix&logocolor=white\" alt=\"Join Matrix\"/>\n    </a>\n    <a href=\"https://smithery.ai/server/trieve-mcp-server\">\n        <img src=\"https://smithery.ai/badge/trieve-mcp-server\" alt=\"smithery badge\"/>\n    </a>\n    <a href=\"https://insiders.vscode.dev/redirect?url=vscode%3Amcp%2Finstall%3F%257B%2522name%2522%253A%2522trieve-mcp-server%2522%252C%2522command%2522%253A%2522npx%2522%252C%2522args%2522%253A%255B%2522more%2520args...%2522%255D%257D\">\n        <img src=\"https://img.shields.io/badge/vscode-mcp-install?style=flat&logoColor=%230078d4&label=vscode-mcp&labelColor=%230078d4&link=https%3A%2F%2Finsiders.vscode.dev%2Fredirect%3Furl%3Dvscode%253Amcp%252Finstall%253F%25257B%252522name%252522%25253A%252522trieve-mcp-server%252522%25252C%252522command%252522%25253A%252522npx%252522%25252C%252522args%252522%25253A%25255B%252522more%252520args...%252522%25255D%25257D\" alt=\"vscode mcp install badge\"/>\n    </a>\n</p>\n\n<h2 align=\"center\">\n    <b>All-in-one solution for search, recommendations, and RAG</b>\n</h2>\n\n<a href=\"https://trieve.ai\">\n  <img src=\"https://cdn.trieve.ai/landing-tabs/light-api.webp\">\n</a>\n\n## Quick Links\n\n- [API Reference + Docs](https://docs.trieve.ai/api-reference)\n- [OpenAPI specification](https://api.trieve.ai/redoc)\n- [Typescript SDK](https://ts-sdk.trieve.ai/)\n- [Python SDK](https://pypi.org/project/trieve-py-client/)\n\n## Features\n\n- **🔒 Self-Hosting in your VPC or on-prem**: We have full self-hosting guides for AWS, GCP, Kubernetes generally, and docker compose available on our [documentation page here](https://docs.trieve.ai/self-hosting/docker-compose).\n- **🧠 Semantic Dense Vector Search**: Integrates with OpenAI or Jina embedding models and [Qdrant](https://qdrant.tech) to provide semantic vector search.\n- **🔍 Typo Tolerant Full-Text/Neural Search**: Every uploaded chunk is vector'ized with [naver/efficient-splade-VI-BT-large-query](https://huggingface.co/naver/efficient-splade-VI-BT-large-query) for typo tolerant, quality neural sparse-vector search.\n- **🖊️ Sub-Sentence Highlighting**: Highlight the matching words or sentences within a chunk and bold them on search to enhance UX for your users. Shout out to the [simsearch](https://github.com/smartdatalake/simsearch) crate!\n- **🌟 Recommendations**: Find similar chunks (or files if using grouping) with the recommendation API. Very helpful if you have a platform where users' favorite, bookmark, or upvote content.\n- **🤖 Convenient RAG API Routes**: We integrate with OpenRouter to provide you with access to any LLM you would like for RAG. Try our routes for [fully-managed RAG with topic-based memory management](https://api.trieve.ai/redoc#tag/message/operation/create_message_completion_handler) or [select your own context RAG](https://api.trieve.ai/redoc#tag/chunk/operation/generate_off_chunks).\n- **💼 Bring Your Own Models**: If you'd like, you can bring your own text-embedding, SPLADE, cross-encoder re-ranking, and/or large-language model (LLM) and plug it into our infrastructure.\n- **🔄 Hybrid Search with cross-encoder re-ranking**: For the best results, use hybrid search with [BAAI/bge-reranker-large](https://huggingface.co/BAAI/bge-reranker-large) re-rank optimization.\n- **📆 Recency Biasing**: Easily bias search results for what was most recent to prevent staleness\n- **🛠️ Tunable Merchandizing**: Adjust relevance using signals like clicks, add-to-carts, or citations\n- **🕳️ Filtering**: Date-range, substring match, tag, numeric, and other filter types are supported.\n- **👥 Grouping**: Mark multiple chunks as being part of the same file and search on the file-level such that the same top-level result never appears twice\n\n**Are we missing a feature that your use case would need?** - call us at [628-222-4090](mailto:+16282224090), make a [Github issue](https://github.com/devflowinc/trieve/issues), or join the [Matrix community](https://matrix.to/#/#trieve-general:trieve.ai) and tell us! We are a small company who is still very hands-on and eager to build what you need; professional services are available.\n\n## Local development with Linux\n\n### Installing via Smithery\n\nTo install Trieve for Claude Desktop automatically via [Smithery](https://smithery.ai/server/trieve-mcp-server):\n\n```bash\nnpx -y @smithery/cli install trieve-mcp-server --client claude\n```\n\n### Debian/Ubuntu Packages needed packages\n\n```sh\nsudo apt install curl \\\ngcc \\\ng++ \\\nmake \\\npkg-config \\\npython3 \\\npython3-pip \\\nlibpq-dev \\\nlibssl-dev \\\nopenssl\n```\n\n### Arch Packages needed\n\n```sh\nsudo pacman -S base-devel postgresql-libs\n```\n\n### Install NodeJS and Yarn\n\nYou can install [NVM](https://github.com/nvm-sh/nvm) using its install script.\n\n```\ncurl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.39.5/install.sh | bash\n```\n\nYou should restart the terminal to update bash profile with NVM. Then, you can install NodeJS LTS release and Yarn.\n\n```\nnvm install --lts\nnpm install -g yarn\n```\n\n### Make server tmp dir\n\n```\nmkdir server/tmp\n```\n\n### Install rust\n\n```\ncurl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh\n```\n\n### Install cargo-watch\n\n```\ncargo install cargo-watch\n```\n\n### Setup env's\n\nYou might need to create the `analytics` directory in ./frontends\n\n```\ncp .env.analytics ./frontends/analytics/.env\ncp .env.chat ./frontends/chat/.env\ncp .env.search ./frontends/search/.env\ncp .env.example ./server/.env\ncp .env.dashboard ./frontends/dashboard/.env\n```\n\n### Add your `LLM_API_KEY` to `./server/.env`\n\n[Here is a guide for acquiring that](https://blog.streamlit.io/beginners-guide-to-openai-api/#get-your-own-openai-api-key).\n\n#### Steps once you have the key\n\n1. Open the `./server/.env` file\n2. Replace the value for `LLM_API_KEY` to be your own OpenAI API key.\n3. Replace the value for `OPENAI_API_KEY` to be your own OpenAI API key.\n\n### Export the following keys in your terminal for local dev\n\nThe PAGEFIND_CDN_BASE_URL and S3_SECRET_KEY_CSVJSONL could be set to a random list of strings.\n\n```\nexport OPENAI_API_KEY=\"your_OpenAI_api_key\" \\\nLLM_API_KEY=\"your_OpenAI_api_key\" \\\nPAGEFIND_CDN_BASE_URL=\"lZP8X4h0Q5Sj2ZmV,aAmu1W92T6DbFUkJ,DZ5pMvz8P1kKNH0r,QAqwvKh8rI5sPmuW,YMwgsBz7jLfN0oX8\" \\\nS3_SECRET_KEY_CSVJSONL=\"Gq6wzS3mjC5kL7i4KwexnL3gP8Z1a5Xv,V2c4ZnL0uHqBzFvR2NcN8Pb1g6CjmX9J,TfA1h8LgI5zYkH9A9p7NvWlL0sZzF9p8N,pKr81pLq5n6MkNzT1X09R7Qb0Vn5cFr0d,DzYwz82FQiW6T3u9A4z9h7HLOlJb7L2V1\" \\\nGROQ_API_KEY=\"GROQ_API_KEY_if_applicable\"\n\n```\n\n### Start docker container services needed for local dev\n\n```\ncat .env.chat .env.search .env.server .env.docker-compose > .env\n\n./convenience.sh -l\n```\n\n### Install front-end packages for local dev\n\n```\ncd frontends\nyarn\n```\n`cd ..`\n\n```\ncd clients/ts-sdk\nyarn build\n```\n`cd ../..`\n\n### Start services for local dev\n\nIt is recommend to manage services through [tmuxp, see the guide here](https://gist.github.com/skeptrunedev/101c7a13bb9b9242999830655470efac) or terminal tabs.\n\n```\ncd frontends\nyarn\nyarn dev\n```\n\n```\ncd server\ncargo watch -x run\n```\n\n```\ncd server\ncargo run --bin ingestion-worker\n```\n\n```\ncd server\ncargo run --bin file-worker\n```\n\n```\ncd server\ncargo run --bin delete-worker\n```\n\n```\ncd search\nyarn\nyarn dev\n```\n\n### Verify Working Setup\n\nAfter the cargo build has finished (after the `tmuxp load trieve`):\n- check that you can see redoc with the OpenAPI reference at [localhost:8090/redoc](http://localhost:8090/redoc)\n- make an account create a dataset with test data at [localhost:5173](http://localhost:5173)\n- search that dataset with test data at [localhost:5174](http://localhost:5174)\n\n### Additional Instructions for testing cross encoder reranking models\n\nTo test the Cross Encoder rerankers in local dev, \n- click on the dataset, go to the Dataset Settings -> Dataset Options -> Additional Options and uncheck the `Fulltext Enabled` option.\n- in the Embedding Settings, select your reranker model and enter the respective key in the adjacent textbox, and hit save.\n- in the search playground, set Type -> Semantic and select Rerank By -> Cross Encoder\n- if AIMon Reranker is selected in the Embedding Settings, you can enter an optional Task Definition in the search playground to specify the domain of context documents to the AIMon reranker.\n\n\n### Debugging issues with local dev\n\nReach out to us on [discord](https://discord.gg/E9sPRZqpDT) for assistance. We are available and more than happy to assist.\n\n## Debug diesel by getting the exact generated SQL\n\n`diesel::debug_query(&query).to_string();`\n\n\n\n## Running evals\n\nThe evals package loads an mcp client that then runs the index.ts file, so there is no need to rebuild between tests. You can load environment variables by prefixing the npx command. Full documentation can be found [here](https://www.mcpevals.io/docs).\n\n```bash\nOPENAI_API_KEY=your-key  npx mcp-eval evals.ts clients/mcp-server/src/index.ts\n```\n## Local Setup for Testing Stripe Features\n\nInstall Stripe CLI.\n\n1. `stripe login`\n2. `stripe listen --forward-to localhost:8090/api/stripe/webhook`\n3. set the `STRIPE_WEBHOOK_SECRET` in the `server/.env` to the resulting webhook signing secret\n4. `stripe products create --name trieve --default-price-data.unit-amount 1200 --default-price-data.currency usd`\n5. `stripe plans create --amount=1200 --currency=usd --interval=month --product={id from response of step 3}`\n\n## Contributors\n\n<a href=\"https://github.com/devflowinc/trieve/graphs/contributors\">\n  <img alt=\"contributors\" src=\"https://contrib.rocks/image?repo=devflowinc/trieve\"/>\n</a>\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "datasets",
        "trieve",
        "devflowinc",
        "datasets trieve",
        "extraction devflowinc",
        "devflowinc trieve"
      ],
      "category": "search--data-extraction"
    },
    "emicklei--melrose-mcp": {
      "owner": "emicklei",
      "name": "melrose-mcp",
      "url": "https://github.com/emicklei/melrose-mcp",
      "imageUrl": "",
      "description": "Plays [Melrōse](https://melrōse.org) music expressions as MIDI",
      "stars": 7,
      "forks": 2,
      "license": "MIT License",
      "language": "Go",
      "updated_at": "2025-09-28T09:15:35Z",
      "readme_content": "## melrose-mcp \n\n[melrōse](https://melrōse.org) is a tool to create and play music by programming melodies. It uses a custom language to compose notes and create loops and tracks to play.\n\n`melrose-mcp` is an [MCP](http://modelcontextprotocol.io/) server (STDIO) that uses the packages from the `melrōse` tool.\n\n## installation\n\nThis mcp server requires the [Go SDK](https://go.dev) for installation.\nTo produce music, you need to setup a MIDI receiver such as a DAW or a hardware synthesizer.\n\n    go install github.com/emicklei/melrose-mcp@latest\n\n## tools\n\n- melrose_play\n- melrose_bpm\n- melrose_devices\n- melrose_change_output_device\n\n## session\n\n>   change the tempo in melrose to a lower value\n\n`melrose_bmp` request:\n```javascript\n{\n  `bpm`: `80`\n}\n```\n\n`llm` response:\n```\nI've reduced the tempo to 80 beats per minute from the default of 120 BPM. This creates a more relaxed and slower pace for any musical expressions we play.\n```\n\n>   play a tune from Claude Debussy\n\n`llm` response:\n```\nI'll create a small segment inspired by Claude Debussy's impressionist style using the Melrose language at our current tempo of 80 BPM.\n```\n\n`melrose_play` request:\n```javascript\n{\n  `expression`: `sequence('(F4 A4 D5) 4(F4 A4 D5) (G4 B4 E5) 4(G4 B4 E5) (A4 C5 F5) 4(A4 C5 F5) (G4 B4 E5) 4(G4 B4 E5) (F4 A4 D5) 8= 8(E4 G4 C5) 4(F4 A4 D5)')`\n}\n```\n`melrose_play` response:\n```javascript\n12.749963417s\n```\n\n## configuration in your client\n\nBelow an example to configure it in Claude Desktop.\n\n    {\n        \"mcpServers\": {\n            \"melrōse\": {\n              \"command\": \"<YOUR DIRECTORY TO THE BINARY>/melrose-mcp\",\n              \"args\": []\n            }\n        }\n    }\n\n## example prompts\n\n- play the music sequence  C D E\n- play the first bar of fur elise\n- play the famous tune from Mike Oldfield called Tubular Bells\n- play the notes of Frere Jacques\n- play the C# chord\n\nTo help your LLM understand the Melrōse language better, you can use this [context](resources/melrose_llm_context.txt).\n\n### Contributions\n\nFixes, suggestions, documentation improvements are all welcome.\nFork this project and submit small Pull requests. \nDiscuss larger ones in the Issues list.\nYou can also sponsor Melrōse via [Github Sponsors](https://github.com/sponsors/emicklei).\n\nSoftware is licensed under [MIT](LICENSE).\n&copy; 2025 [ernestmicklei.com](http://ernestmicklei.com)",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "midi",
        "music",
        "search",
        "expressions midi",
        "music expressions",
        "org music"
      ],
      "category": "search--data-extraction"
    },
    "erithwik--mcp-hn": {
      "owner": "erithwik",
      "name": "mcp-hn",
      "url": "https://github.com/erithwik/mcp-hn",
      "imageUrl": "",
      "description": "An MCP server to search Hacker News, get top stories, and more.",
      "stars": 53,
      "forks": 15,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-17T07:45:41Z",
      "readme_content": "# Hacker News MCP Server\n\n[![smithery badge](https://smithery.ai/badge/mcp-hn)](https://smithery.ai/server/mcp-hn)\n\nA Model Context Protocol (MCP) server that provides tools for fetching information from Hacker News.\n\n<a href=\"https://glama.ai/mcp/servers/e0rco8dfgt\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/e0rco8dfgt/badge\" alt=\"mcp-hn MCP server\" /></a>\n\n## Tools\n\n- `get_stories` Fetching (top, new, ask_hn, show_hn) stories\n- `get_story_info` Fetching comments associated with a story\n- `search_stories` Searching for stories by query\n- `get_user_info` Fetching user info\n\n## Example Usage\n\nUse prompts like the following:\n\n```\nUser: Get the top stories of today\n  Output: Uses `get_stories` tool and returns a story about AI\nUser: What does the details of the story today that talks about the future of AI\n  Output: Uses `get_story_info` tool based on the results of the previous tool\nUser: What has the user `pg` been up to?\n  Output: Uses `get_user_info` tool and returns a summary of the user's activity\nUser: What does hackernews say about careers in AI?\n  Output: Uses `search_stories` tool and returns a summary of the comments\n```\n\nA more detailed example with the puppeteer MCP server:\n\n```\nUser: What are the top stories of today?\n  Output: Uses `get_stories` tool and returns a story about AI\nUser: Can you use the puppeteer tool to read the article about <AI> and also use the hackernews tool to view the comments and give me a summary of what the main comments are about the article?\n  Output: Uses puppeteer tool to read the article about AI and then uses the `get_story_info` hn tool to get the comments and returns a summary of the comments\n```\n\n## Quickstart\n\n### Installing via Smithery\n\nTo install Hacker News MCP Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/mcp-hn):\n\n```bash\nnpx -y @smithery/cli install mcp-hn --client claude\n```\n\n### Claude Desktop:\n\nUpdate the following:\n\nOn MacOS: `~/Library/Application\\ Support/Claude/claude_desktop_config.json`\n\nOn Windows: `%APPDATA%/Claude/claude_desktop_config.json`\n\nWith the following for production:\n\n```json\n{\n  \"mcpServers\": {\n    \"mcp-hn\": {\n      \"command\": \"uvx\",\n      \"args\": [\"mcp-hn\"]\n    }\n  }\n}\n```\n\n\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "search",
        "mcp",
        "hacker",
        "search hacker",
        "erithwik mcp",
        "server search"
      ],
      "category": "search--data-extraction"
    },
    "fatwang2--search1api-mcp": {
      "owner": "fatwang2",
      "name": "search1api-mcp",
      "url": "https://github.com/fatwang2/search1api-mcp",
      "imageUrl": "",
      "description": "Search via search1api (requires paid API key)",
      "stars": 156,
      "forks": 36,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-10-04T08:43:28Z",
      "readme_content": "# Search1API MCP Server\n\n[中文文档](./README_zh.md)\n\nA Model Context Protocol (MCP) server that provides search and crawl functionality using Search1API.\n\n## Prerequisites\n\n- Node.js >= 18.0.0\n- A valid Search1API API key (See **Setup Guide** below on how to obtain and configure)\n\n## Installation (Standalone / General)\n\n1.  **Clone the repository:**\n    ```bash\n    git clone https://github.com/fatwang2/search1api-mcp.git\n    cd search1api-mcp\n    ```\n\n2.  **Configure API Key:** Before building, you need to provide your Search1API key. See the **Setup Guide** section below for different methods (e.g., using a `.env` file or environment variables).\n\n3.  **Install dependencies and build:**\n    ```bash\n    npm install\n    npm run build\n    ```\n    *Note: If using the project's `.env` file method for the API key, ensure it exists before this step.*\n\n## Usage (Standalone / General)\n\nEnsure your API key is configured (see **Setup Guide**).\n\nStart the server:\n```bash\nnpm start\n```\n\nThe server will then be ready to accept connections from MCP clients.\n\n## Setup Guide\n\n### 1. Get Search1API Key\n\n1.  Register at [Search1API](https://www.search1api.com/?utm_source=mcp)\n2.  Get your API key from your dashboard.\n\n### 2. Configure API Key\n\nYou need to make your API key available to the server. Choose **one** of the following methods:\n\n**Method A: Project `.env` File (Recommended for Standalone or LibreChat)**\n\nThis method is required if integrating with the current version of LibreChat (see specific section below).\n\n1.  In the `search1api-mcp` project root directory, create a file named `.env`:\n    ```bash\n    # In the search1api-mcp directory\n    echo \"SEARCH1API_KEY=your_api_key_here\" > .env\n    ```\n2.  Replace `your_api_key_here` with your actual key.\n3.  Make sure this file exists **before** running `npm install && npm run build`.\n\n**Method B: Environment Variable (Standalone Only)**\n\nSet the `SEARCH1API_KEY` environment variable before starting the server.\n\n```bash\nexport SEARCH1API_KEY=\"your_api_key_here\"\nnpm start\n```\n\n**Method C: MCP Client Configuration (Advanced)**\n\nSome MCP clients allow specifying environment variables directly in their configuration. This is useful for clients like Cursor, VS Code extensions, etc.\n\n```json\n{\n  \"mcpServers\": {\n    \"search1api\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"search1api-mcp\"\n      ],\n      \"env\": {\n        \"SEARCH1API_KEY\": \"YOUR_SEARCH1API_KEY\"\n      }\n    }\n  }\n}\n```\n\n**Note for LibreChat Users:** Due to current limitations in LibreChat, Method A (Project `.env` File) is the **required** method. See the dedicated integration section below for full instructions.\n\n## Integration with LibreChat (Docker)\n\nThis section details the required steps for integrating with LibreChat via Docker.\n\n**Overview:**\n\n1.  Clone this server's repository into a location accessible by your LibreChat `docker-compose.yml`.\n2.  Configure the required API key using the **Project `.env` File method** within this server's directory.\n3.  Build this server.\n4.  Tell LibreChat how to run this server by editing `librechat.yaml`.\n5.  Make sure the built server code is available inside the LibreChat container via a Docker volume bind.\n6.  Restart LibreChat.\n\n**Step-by-Step:**\n\n1.  **Clone the Repository:**\n    Navigate to the directory on your host machine where you manage external services for LibreChat (this is often alongside your `docker-compose.yml`). A common location is a dedicated `mcp-server` directory.\n    ```bash\n    # Example: Navigate to where docker-compose.yml lives, then into mcp-server\n    cd /path/to/your/librechat/setup/mcp-server\n    git clone https://github.com/fatwang2/search1api-mcp.git\n    ```\n\n2.  **Navigate into the Server Directory:**\n    ```bash\n    cd search1api-mcp\n    ```\n\n3.  **Configure API Key (Project `.env` File Method - Required for LibreChat):**\n    ```bash\n    # Create the .env file\n    echo \"SEARCH1API_KEY=your_api_key_here\" > .env\n    # IMPORTANT: Replace 'your_api_key_here' with your actual Search1API key\n    ```\n\n4.  **Install Dependencies and Build:**\n    This step compiles the server code into the `build` directory.\n    ```bash\n    npm install\n    npm run build\n    ```\n\n5.  **Configure `librechat.yaml`:**\n    Edit your main `librechat.yaml` file to tell LibreChat how to execute this MCP server. Add an entry under `mcp_servers`:\n    ```yaml\n    # In your main librechat.yaml\n    mcp_servers:\n      # You can add other MCP servers here too\n      search1api:\n        # Optional: Display name for the server in LibreChat UI\n        # name: Search1API Tools\n\n        # Command tells LibreChat to use 'node'\n        command: node\n\n        # Args specify the script for 'node' to run *inside the container*\n        args:\n          - /app/mcp-server/search1api-mcp/build/index.js\n    ```\n    *   The `args` path (`/app/...`) is the location *inside* the LibreChat API container where the built server will be accessed (thanks to the volume bind in the next step).\n\n6.  **Configure Docker Volume Bind:**\n    Edit your `docker-compose.yml` (or more likely, your `docker-compose.override.yml`) to map the `search1api-mcp` directory from your host machine into the LibreChat API container. Find the `volumes:` section for the `api:` service:\n    ```yaml\n    # In your docker-compose.yml or docker-compose.override.yml\n    services:\n      api:\n        # ... other service config ...\n        volumes:\n          # ... other volumes likely exist here ...\n\n          # Add this volume bind:\n          - ./mcp-server/search1api-mcp:/app/mcp-server/search1api-mcp\n    ```\n    *   **Host Path (`./mcp-server/search1api-mcp`):** This is the path on your host machine *relative* to where your `docker-compose.yml` file is located. Adjust it if you cloned the repo elsewhere.\n    *   **Container Path (`:/app/mcp-server/search1api-mcp`):** This is the path *inside* the container. It **must match** the directory structure used in the `librechat.yaml` `args` path.\n\n7.  **Restart LibreChat:**\n    Apply the changes by rebuilding (if you modified `docker-compose.yml`) and restarting your LibreChat stack.\n    ```bash\n    docker compose down && docker compose up -d --build\n    # Or: docker compose restart api (if only librechat.yaml changed)\n    ```\n\nNow, the Search1API server should be available as a tool provider within LibreChat.\n\n## Features\n\n- Web search functionality\n- News search functionality\n- Web page content extraction\n- Website sitemap extraction\n- Deep thinking and complex problem solving with DeepSeek R1\n- Seamless integration with Claude Desktop, Cursor, Windsurf, Cline and other MCP clients\n\n## Tools\n\n### 1. Search Tool\n- Name: `search`\n- Description: Search the web using Search1API\n- Parameters:\n  * `query` (required): Search query in natural language. Be specific and concise for better results\n  * `max_results` (optional, default: 10): Number of results to return\n  * `search_service` (optional, default: \"google\"): Search service to use (google, bing, duckduckgo, yahoo, x, reddit, github, youtube, arxiv, wechat, bilibili, imdb, wikipedia)\n  * `crawl_results` (optional, default: 0): Number of results to crawl for full webpage content\n  * `include_sites` (optional): List of sites to include in search\n  * `exclude_sites` (optional): List of sites to exclude from search\n  * `time_range` (optional): Time range for search results (\"day\", \"month\", \"year\")\n\n### 2. News Tool\n- Name: `news`\n- Description: Search for news articles using Search1API\n- Parameters:\n  * `query` (required): Search query in natural language. Be specific and concise for better results\n  * `max_results` (optional, default: 10): Number of results to return\n  * `search_service` (optional, default: \"bing\"): Search service to use (google, bing, duckduckgo, yahoo, hackernews)\n  * `crawl_results` (optional, default: 0): Number of results to crawl for full webpage content\n  * `include_sites` (optional): List of sites to include in search\n  * `exclude_sites` (optional): List of sites to exclude from search\n  * `time_range` (optional): Time range for search results (\"day\", \"month\", \"year\")\n\n### 3. Crawl Tool\n- Name: `crawl`\n- Description: Extract content from a URL using Search1API\n- Parameters:\n  * `url` (required): URL to crawl\n\n### 4. Sitemap Tool\n- Name: `sitemap`\n- Description: Get all related links from a URL\n- Parameters:\n  * `url` (required): URL to get sitemap\n\n### 5. Reasoning Tool\n- Name: `reasoning`\n- Description: A tool for deep thinking and complex problem solving with fast deepseek r1 model and web search ability(You can change to any other model in search1api website but the speed is not guaranteed)\n- Parameters:\n  * `content` (required): The question or problem that needs deep thinking\n\n### 6. Trending Tool\n- Name: `trending`\n- Description: Get trending topics from popular platforms\n- Parameters:\n  * `search_service` (required): Specify the platform to get trending topics from (github, hackernews)\n  * `max_results` (optional, default: 10): Maximum number of trending items to return\n\n## Version History\n\n- v0.2.0: Added fallback `.env` support for LibreChat integration and updated dependencies.\n- v0.1.8: Added X(Twitter) and Reddit search services\n- v0.1.7: Added Trending tool for GitHub and Hacker News\n- v0.1.6: Added Wikipedia search service\n- v0.1.5: Added new search parameters (include_sites, exclude_sites, time_range) and new search services (arxiv, wechat, bilibili, imdb)\n- v0.1.4: Added reasoning tool with deepseek r1 and updated the Cursor and Windsurf configuration guide\n- v0.1.3: Added news search functionality\n- v0.1.2: Added sitemap functionality\n- v0.1.1: Added web crawling functionality\n- v0.1.0: Initial release with search functionality\n\n## License\n\nThis project is licensed under the MIT License - see the LICENSE file for details.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "search1api",
        "fatwang2",
        "search",
        "search1api mcp",
        "fatwang2 search1api",
        "search search1api"
      ],
      "category": "search--data-extraction"
    },
    "format37--youtube_mcp": {
      "owner": "format37",
      "name": "youtube_mcp",
      "url": "https://github.com/format37/youtube_mcp",
      "imageUrl": "",
      "description": "dlp to download audio and OpenAI's Whisper-1 for more precise transcription than youtube captions. Provide a YouTube URL and get back the full transcript splitted by chunks for long videos.",
      "stars": 23,
      "forks": 3,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-09-23T10:24:54Z",
      "readme_content": "# youtube_mcp\nYoutube transcribation MCP server\n\n## Demo Video\n\n[![YouTube MCP Demo](https://img.youtube.com/vi/bS5vKuehzEE/maxresdefault.jpg)](https://youtu.be/bS5vKuehzEE)\n\n*Click the image above to watch the demo video*\n\n## Requirements:\n* OpenAI API key\n* Cookies\n\n## Server installation\n* Clone the repo:\n```\ngit clone https://github.com/format37/youtube_mcp.git\ncd youtube_mcp\nnano .env\n```\n* Extract your cookies. See [cookies.md](./cookies.md)  \nPlace cookies.txt in the ./mcp/ folder.\n* Generate MCP_KEY:\n```\npython token_generator.py\n```\n* Define .env:\n```\nCONTAINER_NAME=youtube_mcp_main\nPORT=7001\nMCP_KEY=YOUR-MCP-KEY\nOPENAI_API_KEY=YOUR-OPENAI-KRY\n```\n* Provide run access\n```\nsudo chmod +x compose.sh\nsudo chmod +x logs.sh\nsudo chmod +x update.sh\n```\n* Run\n```\n./compose.sh\n```\n* Check that port is opened for incoming connections.\n\n## Client configuration\n3. Add Bybit server to the Claude desktop config:\nExample:\n```\n{\n    \"mcpServers\": {\n      \"youtube\": {\n        \"command\": \"npx\",\n        \"args\": [\n          \"mcp-remote\",\n          \"http://localhost:7001/sse\",\n          \"--header\",\n          \"Authorization:YOUR-TOKEN\",\n          \"--allow-http\"\n        ],\n        \"disabled\": false\n      }\n    }\n}\n```\n## Client side\n```\n4. Restart Claude desktop\n5. Check that tws tools are listed in the tools list. Ask Claude to check ibkr account",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "youtube_mcp",
        "transcript",
        "transcription",
        "format37 youtube_mcp",
        "youtube_mcp dlp",
        "transcription youtube"
      ],
      "category": "search--data-extraction"
    },
    "genomoncology--biomcp": {
      "owner": "genomoncology",
      "name": "biomcp",
      "url": "https://github.com/genomoncology/biomcp",
      "imageUrl": "",
      "description": "Biomedical research server providing access to PubMed, ClinicalTrials.gov, and MyVariant.info.",
      "stars": 317,
      "forks": 54,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-10-04T07:02:52Z",
      "readme_content": "# BioMCP: Biomedical Model Context Protocol\n\nBioMCP is an open source (MIT License) toolkit that empowers AI assistants and\nagents with specialized biomedical knowledge. Built following the Model Context\nProtocol (MCP), it connects AI systems to authoritative biomedical data\nsources, enabling them to answer questions about clinical trials, scientific\nliterature, and genomic variants with precision and depth.\n\n[](https://www.youtube.com/watch?v=bKxOWrWUUhM)\n\n## MCPHub Certification\n\nBioMCP is certified by [MCPHub](https://mcphub.com/mcp-servers/genomoncology/biomcp). This certification ensures that BioMCP follows best practices for Model Context Protocol implementation and provides reliable biomedical data access.\n\n## Why BioMCP?\n\nWhile Large Language Models have broad general knowledge, they often lack\nspecialized domain-specific information or access to up-to-date resources.\nBioMCP bridges this gap for biomedicine by:\n\n- Providing **structured access** to clinical trials, biomedical literature,\n  and genomic variants\n- Enabling **natural language queries** to specialized databases without\n  requiring knowledge of their specific syntax\n- Supporting **biomedical research** workflows through a consistent interface\n- Functioning as an **MCP server** for AI assistants and agents\n\n## Biomedical Data Sources\n\nBioMCP integrates with multiple biomedical data sources:\n\n### Literature Sources\n\n- **PubTator3/PubMed** - Peer-reviewed biomedical literature with entity annotations\n- **bioRxiv/medRxiv** - Preprint servers for biology and health sciences\n- **Europe PMC** - Open science platform including preprints\n\n### Clinical & Genomic Sources\n\n- **ClinicalTrials.gov** - Clinical trial registry and results database\n- **NCI Clinical Trials Search API** - National Cancer Institute's curated cancer trials database\n  - Advanced search filters (biomarkers, prior therapies, brain metastases)\n  - Organization and intervention databases\n  - Disease vocabulary with synonyms\n- **BioThings Suite** - Comprehensive biomedical data APIs:\n  - **MyVariant.info** - Consolidated genetic variant annotation\n  - **MyGene.info** - Real-time gene annotations and information\n  - **MyDisease.info** - Disease ontology and synonym information\n  - **MyChem.info** - Drug/chemical annotations and properties\n- **TCGA/GDC** - The Cancer Genome Atlas for cancer variant data\n- **1000 Genomes** - Population frequency data via Ensembl\n- **cBioPortal** - Cancer genomics portal with mutation occurrence data\n\n### Regulatory & Safety Sources\n\n- **OpenFDA** - FDA regulatory and safety data:\n  - **Drug Adverse Events (FAERS)** - Post-market drug safety reports\n  - **Drug Labels (SPL)** - Official prescribing information\n  - **Device Events (MAUDE)** - Medical device adverse events, with genomic device filtering\n\n## Available MCP Tools\n\nBioMCP provides 24 specialized tools for biomedical research:\n\n### Core Tools (3)\n\n#### 1. Think Tool (ALWAYS USE FIRST!)\n\n**CRITICAL**: The `think` tool MUST be your first step for ANY biomedical research task.\n\n```python\n# Start analysis with sequential thinking\nthink(\n    thought=\"Breaking down the query about BRAF mutations in melanoma...\",\n    thoughtNumber=1,\n    totalThoughts=3,\n    nextThoughtNeeded=True\n)\n```\n\nThe sequential thinking tool helps:\n\n- Break down complex biomedical problems systematically\n- Plan multi-step research approaches\n- Track reasoning progress\n- Ensure comprehensive analysis\n\n#### 2. Search Tool\n\nThe search tool supports two modes:\n\n##### Unified Query Language (Recommended)\n\nUse the `query` parameter with structured field syntax for powerful cross-domain searches:\n\n```python\n# Simple natural language\nsearch(query=\"BRAF melanoma\")\n\n# Field-specific search\nsearch(query=\"gene:BRAF AND trials.condition:melanoma\")\n\n# Complex queries\nsearch(query=\"gene:BRAF AND variants.significance:pathogenic AND articles.date:>2023\")\n\n# Get searchable fields schema\nsearch(get_schema=True)\n\n# Explain how a query is parsed\nsearch(query=\"gene:BRAF\", explain_query=True)\n```\n\n**Supported Fields:**\n\n- **Cross-domain**: `gene:`, `variant:`, `disease:`\n- **Trials**: `trials.condition:`, `trials.phase:`, `trials.status:`, `trials.intervention:`\n- **Articles**: `articles.author:`, `articles.journal:`, `articles.date:`\n- **Variants**: `variants.significance:`, `variants.rsid:`, `variants.frequency:`\n\n##### Domain-Based Search\n\nUse the `domain` parameter with specific filters:\n\n```python\n# Search articles (includes automatic cBioPortal integration)\nsearch(domain=\"article\", genes=[\"BRAF\"], diseases=[\"melanoma\"])\n\n# Search with mutation-specific cBioPortal data\nsearch(domain=\"article\", genes=[\"BRAF\"], keywords=[\"V600E\"])\nsearch(domain=\"article\", genes=[\"SRSF2\"], keywords=[\"F57*\"])  # Wildcard patterns\n\n# Search trials\nsearch(domain=\"trial\", conditions=[\"lung cancer\"], phase=\"3\")\n\n# Search variants\nsearch(domain=\"variant\", gene=\"TP53\", significance=\"pathogenic\")\n```\n\n**Note**: When searching articles with a gene parameter, cBioPortal data is automatically included:\n\n- Gene-level summaries show mutation frequency across cancer studies\n- Mutation-specific searches (e.g., \"V600E\") show study-level occurrence data\n- Cancer types are dynamically resolved from cBioPortal API\n\n#### 3. Fetch Tool\n\nRetrieve full details for a single article, trial, or variant:\n\n```python\n# Fetch article details (supports both PMID and DOI)\nfetch(domain=\"article\", id=\"34567890\")  # PMID\nfetch(domain=\"article\", id=\"10.1101/2024.01.20.23288905\")  # DOI\n\n# Fetch trial with all sections\nfetch(domain=\"trial\", id=\"NCT04280705\", detail=\"all\")\n\n# Fetch variant details\nfetch(domain=\"variant\", id=\"rs113488022\")\n```\n\n**Domain-specific options:**\n\n- **Articles**: `detail=\"full\"` retrieves full text if available\n- **Trials**: `detail` can be \"protocol\", \"locations\", \"outcomes\", \"references\", or \"all\"\n- **Variants**: Always returns full details\n\n### Individual Tools (21)\n\nFor users who prefer direct access to specific functionality, BioMCP also provides 21 individual tools:\n\n#### Article Tools (2)\n\n- **article_searcher**: Search PubMed/PubTator3 and preprints\n- **article_getter**: Fetch detailed article information (supports PMID and DOI)\n\n#### Trial Tools (5)\n\n- **trial_searcher**: Search ClinicalTrials.gov or NCI CTS API (via source parameter)\n- **trial_getter**: Fetch all trial details from either source\n- **trial_protocol_getter**: Fetch protocol information only (ClinicalTrials.gov)\n- **trial_references_getter**: Fetch trial publications (ClinicalTrials.gov)\n- **trial_outcomes_getter**: Fetch outcome measures and results (ClinicalTrials.gov)\n- **trial_locations_getter**: Fetch site locations and contacts (ClinicalTrials.gov)\n\n#### Variant Tools (2)\n\n- **variant_searcher**: Search MyVariant.info database\n- **variant_getter**: Fetch comprehensive variant details\n\n#### NCI-Specific Tools (6)\n\n- **nci_organization_searcher**: Search NCI's organization database\n- **nci_organization_getter**: Get organization details by ID\n- **nci_intervention_searcher**: Search NCI's intervention database (drugs, devices, procedures)\n- **nci_intervention_getter**: Get intervention details by ID\n- **nci_biomarker_searcher**: Search biomarkers used in trial eligibility criteria\n- **nci_disease_searcher**: Search NCI's controlled vocabulary of cancer conditions\n\n#### Gene, Disease & Drug Tools (3)\n\n- **gene_getter**: Get real-time gene information from MyGene.info\n- **disease_getter**: Get disease definitions and synonyms from MyDisease.info\n- **drug_getter**: Get drug/chemical information from MyChem.info\n\n**Note**: All individual tools that search by gene automatically include cBioPortal summaries when the `include_cbioportal` parameter is True (default). Trial searches can expand disease conditions with synonyms when `expand_synonyms` is True (default).\n\n## Quick Start\n\n### For Claude Desktop Users\n\n1. **Install `uv`** if you don't have it (recommended):\n\n   ```bash\n   # MacOS\n   brew install uv\n\n   # Windows/Linux\n   pip install uv\n   ```\n\n2. **Configure Claude Desktop**:\n   - Open Claude Desktop settings\n   - Navigate to Developer section\n   - Click \"Edit Config\" and add:\n   ```json\n   {\n     \"mcpServers\": {\n       \"biomcp\": {\n         \"command\": \"uv\",\n         \"args\": [\"run\", \"--with\", \"biomcp-python\", \"biomcp\", \"run\"]\n       }\n     }\n   }\n   ```\n   - Restart Claude Desktop and start chatting about biomedical topics!\n\n### Python Package Installation\n\n```bash\n# Using pip\npip install biomcp-python\n\n# Using uv (recommended for faster installation)\nuv pip install biomcp-python\n\n# Run directly without installation\nuv run --with biomcp-python biomcp trial search --condition \"lung cancer\"\n```\n\n## Configuration\n\n### Environment Variables\n\nBioMCP supports optional environment variables for enhanced functionality:\n\n```bash\n# cBioPortal API authentication (optional)\nexport CBIO_TOKEN=\"your-api-token\"  # For authenticated access\nexport CBIO_BASE_URL=\"https://www.cbioportal.org/api\"  # Custom API endpoint\n\n# Performance tuning\nexport BIOMCP_USE_CONNECTION_POOL=\"true\"  # Enable HTTP connection pooling (default: true)\nexport BIOMCP_METRICS_ENABLED=\"false\"     # Enable performance metrics (default: false)\n```\n\n## Running BioMCP Server\n\nBioMCP supports multiple transport protocols to suit different deployment scenarios:\n\n### Local Development (STDIO)\n\nFor direct integration with Claude Desktop or local MCP clients:\n\n```bash\n# Default STDIO mode for local development\nbiomcp run\n\n# Or explicitly specify STDIO\nbiomcp run --mode stdio\n```\n\n### HTTP Server Mode\n\nBioMCP supports multiple HTTP transport protocols:\n\n#### Legacy SSE Transport (Worker Mode)\n\nFor backward compatibility with existing SSE clients:\n\n```bash\nbiomcp run --mode worker\n# Server available at http://localhost:8000/sse\n```\n\n#### Streamable HTTP Transport (Recommended)\n\nThe new MCP-compliant Streamable HTTP transport provides optimal performance and standards compliance:\n\n```bash\nbiomcp run --mode streamable_http\n\n# Custom host and port\nbiomcp run --mode streamable_http --host 127.0.0.1 --port 8080\n```\n\nFeatures of Streamable HTTP transport:\n\n- Single `/mcp` endpoint for all operations\n- Dynamic response mode (JSON for quick operations, SSE for long-running)\n- Session management support (future)\n- Full MCP specification compliance (2025-03-26)\n- Better scalability for cloud deployments\n\n### Deployment Options\n\n#### Docker\n\n```bash\n# Build the Docker image locally\ndocker build -t biomcp:latest .\n\n# Run the container\ndocker run -p 8000:8000 biomcp:latest biomcp run --mode streamable_http\n```\n\n#### Cloudflare Workers\n\nThe worker mode can be deployed to Cloudflare Workers for global edge deployment.\n\nNote: All APIs work without authentication, but tokens may provide higher rate limits.\n\n## Command Line Interface\n\nBioMCP provides a comprehensive CLI for direct database interaction:\n\n```bash\n# Get help\nbiomcp --help\n\n# Run the MCP server\nbiomcp run\n\n# Article search examples\nbiomcp article search --gene BRAF --disease Melanoma  # Includes preprints by default\nbiomcp article search --gene BRAF --no-preprints      # Exclude preprints\nbiomcp article get 21717063 --full\n\n# Clinical trial examples\nbiomcp trial search --condition \"Lung Cancer\" --phase PHASE3\nbiomcp trial search --condition melanoma --source nci --api-key YOUR_KEY  # Use NCI API\nbiomcp trial get NCT04280705 Protocol\nbiomcp trial get NCT04280705 --source nci --api-key YOUR_KEY  # Get from NCI\n\n# Variant examples with external annotations\nbiomcp variant search --gene TP53 --significance pathogenic\nbiomcp variant get rs113488022  # Includes TCGA, 1000 Genomes, and cBioPortal data by default\nbiomcp variant get rs113488022 --no-external  # Core annotations only\n\n# NCI-specific examples (requires NCI API key)\nbiomcp organization search \"MD Anderson\" --api-key YOUR_KEY\nbiomcp organization get ORG123456 --api-key YOUR_KEY\nbiomcp intervention search pembrolizumab --api-key YOUR_KEY\nbiomcp intervention search --type Device --api-key YOUR_KEY\nbiomcp biomarker search \"PD-L1\" --api-key YOUR_KEY\nbiomcp disease search melanoma --source nci --api-key YOUR_KEY\n```\n\n## Testing & Verification\n\nTest your BioMCP setup with the MCP Inspector:\n\n```bash\nnpx @modelcontextprotocol/inspector uv run --with biomcp-python biomcp run\n```\n\nThis opens a web interface where you can explore and test all available tools.\n\n## Enterprise Version: OncoMCP\n\nOncoMCP extends BioMCP with GenomOncology's enterprise-grade precision oncology\nplatform (POP), providing:\n\n- **HIPAA-Compliant Deployment**: Secure on-premise options\n- **Real-Time Trial Matching**: Up-to-date status and arm-level matching\n- **Healthcare Integration**: Seamless EHR and data warehouse connectivity\n- **Curated Knowledge Base**: 15,000+ trials and FDA approvals\n- **Sophisticated Patient Matching**: Using integrated clinical and molecular\n  profiles\n- **Advanced NLP**: Structured extraction from unstructured text\n- **Comprehensive Biomarker Processing**: Mutation and rule processing\n\nLearn more: [GenomOncology](https://genomoncology.com/)\n\n## MCP Registries\n\n[![smithery badge](https://smithery.ai/badge/@genomoncology/biomcp)](https://smithery.ai/server/@genomoncology/biomcp)\n\n<a href=\"https://glama.ai/mcp/servers/@genomoncology/biomcp\">\n<img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@genomoncology/biomcp/badge\" />\n</a>\n\n## Example Use Cases\n\n### Gene Information Retrieval\n\n```python\n# Get comprehensive gene information\ngene_getter(gene_id_or_symbol=\"TP53\")\n# Returns: Official name, summary, aliases, links to databases\n```\n\n### Disease Synonym Expansion\n\n```python\n# Get disease information with synonyms\ndisease_getter(disease_id_or_name=\"GIST\")\n# Returns: \"gastrointestinal stromal tumor\" and other synonyms\n\n# Search trials with automatic synonym expansion\ntrial_searcher(conditions=[\"GIST\"], expand_synonyms=True)\n# Searches for: GIST OR \"gastrointestinal stromal tumor\" OR \"GI stromal tumor\"\n```\n\n### Integrated Biomedical Research\n\n```python\n# 1. Always start with thinking\nthink(thought=\"Analyzing BRAF V600E in melanoma treatment\", thoughtNumber=1)\n\n# 2. Get gene context\ngene_getter(\"BRAF\")\n\n# 3. Search for pathogenic variants\nvariant_searcher(gene=\"BRAF\", hgvsp=\"V600E\", significance=\"pathogenic\")\n\n# 4. Find relevant clinical trials with disease expansion\ntrial_searcher(conditions=[\"melanoma\"], interventions=[\"BRAF inhibitor\"])\n```\n\n## Documentation\n\nFor comprehensive documentation, visit [https://biomcp.org](https://biomcp.org)\n\n### Developer Guides\n\n- [HTTP Client Guide](./docs/http-client-guide.md) - Using the centralized HTTP client\n- [Migration Examples](./docs/migration-examples.md) - Migrating from direct HTTP usage\n- [Error Handling Guide](./docs/error-handling.md) - Comprehensive error handling patterns\n- [Integration Testing Guide](./docs/integration-testing.md) - Best practices for reliable integration tests\n- [Third-Party Endpoints](./THIRD_PARTY_ENDPOINTS.md) - Complete list of external APIs used\n- [Testing Guide](./docs/development/testing.md) - Running tests and understanding test categories\n\n## Development\n\n### Running Tests\n\n```bash\n# Run all tests (including integration tests)\nmake test\n\n# Run only unit tests (excluding integration tests)\nuv run python -m pytest tests -m \"not integration\"\n\n# Run only integration tests\nuv run python -m pytest tests -m \"integration\"\n```\n\n**Note**: Integration tests make real API calls and may fail due to network issues or rate limiting.\nIn CI/CD, integration tests are run separately and allowed to fail without blocking the build.\n\n## BioMCP Examples Repo\n\nLooking to see BioMCP in action?\n\nCheck out the companion repository:\n👉 **[biomcp-examples](https://github.com/genomoncology/biomcp-examples)**\n\nIt contains real prompts, AI-generated research briefs, and evaluation runs across different models.\nUse it to explore capabilities, compare outputs, or benchmark your own setup.\n\nHave a cool example of your own?\n**We’d love for you to contribute!** Just fork the repo and submit a PR with your experiment.\n\n## License\n\nThis project is licensed under the MIT License.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "biomcp",
        "biomedical",
        "genomoncology",
        "genomoncology biomcp",
        "biomcp biomedical",
        "extraction genomoncology"
      ],
      "category": "search--data-extraction"
    },
    "hbg--mcp-paperswithcode": {
      "owner": "hbg",
      "name": "mcp-paperswithcode",
      "url": "https://github.com/hbg/mcp-paperswithcode",
      "imageUrl": "",
      "description": "🐍 ☁️ MCP to search through PapersWithCode API",
      "stars": 14,
      "forks": 5,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-08-09T01:30:55Z",
      "readme_content": "# mcp-paperswithcode\n\n[![smithery badge](https://smithery.ai/badge/@hbg/mcp-paperswithcode)](https://smithery.ai/server/@hbg/mcp-paperswithcode)\n\n# 🦾 Features\n\n> Allows AI assistants to find and read papers, as well as view related code repositories for further context.\n\nThis MCP server provides a Model Context Protocol (MCP) client that interfaces with the PapersWithCode API.\n\nIt includes tools for searching, retrieving, and parsing information on research papers, authors, datasets, conferences, and more.\n\n# 🚀 Getting Started\n\n### Installing via Smithery\n\nTo install mcp-paperswithcode for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@hbg/mcp-paperswithcode):\n\n```bash\nnpx -y @smithery/cli install @hbg/mcp-paperswithcode --client claude\n```\n\n# 🛠️ Tools\n\n## 📚 Paper Tools\n\n### `search_papers`\nSearch for papers using optional filters.\n\n- `abstract` (str, optional): Filter by abstract text.\n- `title` (str, optional): Filter by title text.\n- `arxiv_id` (str, optional): Filter by ArXiv ID.\n\n### `get_paper`\nGet a paper's metadata by its ID.\n\n- `paper_id` (str): The paper ID.\n\n### `read_paper_from_url`\nExtract readable text from a paper given its URL.\n\n- `paper_url` (str): The direct PDF or HTML URL to a paper.\n\n### `list_paper_results`\nList benchmark results associated with a paper.\n\n- `paper_id` (str): The paper ID.\n\n### `list_paper_tasks`\nList tasks associated with a paper.\n\n- `paper_id` (str): The paper ID.\n\n### `list_paper_methods`\nList methods discussed in a paper.\n\n- `paper_id` (str): The paper ID.\n\n### `list_paper_repositories`\nList code repositories linked to a paper.\n\n- `paper_id` (str): The paper ID.\n\n### `list_paper_datasets`\nList datasets mentioned or used in a paper.\n\n- `paper_id` (str): The paper ID.\n\n## 🧠 Research Area Tools\n\n### `search_research_areas`\nSearch research areas by name.\n\n- `name` (str): Partial or full name of the research area.\n\n### `get_research_area`\nGet metadata for a specific research area.\n\n- `area_id` (str): The area ID.\n\n### `list_research_area_tasks`\nList tasks associated with a research area.\n\n- `area_id` (str): The area ID.\n\n## 👨‍🔬 Author Tools\n\n### `search_authors`\nSearch authors by full name.\n\n- `full_name` (str): Full name of the author.\n\n### `get_paper_author`\nGet metadata for an author by ID.\n\n- `author_id` (str): The author ID.\n\n### `list_papers_by_author_id`\nList all papers written by an author via ID.\n\n- `author_id` (str): The author ID.\n\n### `list_papers_by_author_name`\nSearch by name and return papers for the first matching author.\n\n- `author_name` (str): Full name of the author.\n\n## 🎓 Conference Tools\n\n### `list_conferences`\nList conferences, optionally filter by name.\n\n- `conference_name` (str, optional): Full or partial name.\n\n### `get_conference`\nGet metadata for a specific conference.\n\n- `conference_id` (str): The conference ID.\n\n### `list_conference_proceedings`\nList all proceedings under a conference.\n\n- `conference_id` (str): The conference ID.\n\n### `get_conference_proceeding`\nGet details for a specific conference proceeding.\n\n- `conference_id` (str): The conference ID.\n- `proceeding_id` (str): The proceeding ID.\n\n### `list_conference_papers`\nList all papers for a specific conference proceeding.\n\n- `conference_id` (str): The conference ID.\n- `proceeding_id` (str): The proceeding ID.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "paperswithcode",
        "search",
        "hbg",
        "paperswithcode mcp",
        "search paperswithcode",
        "mcp paperswithcode"
      ],
      "category": "search--data-extraction"
    },
    "hellokaton--unsplash-mcp-server": {
      "owner": "hellokaton",
      "name": "unsplash-mcp-server",
      "url": "https://github.com/hellokaton/unsplash-mcp-server",
      "imageUrl": "",
      "description": "A MCP server for Unsplash image search.",
      "stars": 174,
      "forks": 19,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-10-03T22:30:58Z",
      "readme_content": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/hellokaton-unsplash-mcp-server-badge.png)](https://mseep.ai/app/hellokaton-unsplash-mcp-server)\n\n# Unsplash MCP Server\n\nEnglish | [简体中文](README_zh.md)\n\n> A simple MCP server for seamless Unsplash image integration and search capabilities.\n\n[![Python 3.9+](https://img.shields.io/badge/python-3.9+-blue.svg)](https://www.python.org/downloads/)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![smithery badge](https://smithery.ai/badge/@hellokaton/unsplash-mcp-server)](https://smithery.ai/server/@hellokaton/unsplash-mcp-server)\n\n## 📋 Overview\n\nUnsplash MCP Server is used for searching rich, high-quality images. It's ideal for developers who want to integrate Unsplash functionality into their own applications.\n\n## ✨ Features\n\n- **Advanced Image Search**: Search Unsplash's extensive photo library with filters for:\n  - Keyword relevance\n  - Color schemes\n  - Orientation options\n  - Custom sorting and pagination\n\n## 🔑 Obtaining Unsplash Access Key\n\nBefore installing this server, you'll need to obtain an Unsplash API Access Key:\n\n1. Create a developer account at [Unsplash](https://unsplash.com/developers)\n2. Register a new application\n3. Get your Access Key from the application details page\n4. Use this key in the configuration steps below\n\nFor more details, refer to the [official Unsplash API documentation](https://unsplash.com/documentation).\n\n## 🚀 Installation\n\nTo install Unsplash Image Integration Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@hellokaton/unsplash-mcp-server):\n\n### IDE Setup\n\n**Cursor IDE**\n\n```bash\nnpx -y @smithery/cli@latest install @hellokaton/unsplash-mcp-server --client cursor --key 7558c683-****-****\n```\n\n**Windsurf**\n\n```bash\nnpx -y @smithery/cli@latest install @hellokaton/unsplash-mcp-server --client windsurf --key 7558c683-****-****\n```\n\n**Cline**\n\n```bash\nnpx -y @smithery/cli@latest install @hellokaton/unsplash-mcp-server --client cline --key 7558c683-****-****\n```\n\n### Manual Installation\n\n```bash\n# Clone the repository\ngit clone https://github.com/hellokaton/unsplash-mcp-server.git\n\n# Navigate to project directory\ncd unsplash-mcp-server\n\n# Create virtual environment\nuv venv\n\n# Install dependencies\nuv pip install .\n```\n\n**Cursor Editor Integration**\n\nAdd the following configuration to your Cursor editor's `settings.json`:\n\n⚠️ **Note:** Please adjust the following configuration according to your actual installation:\n\n- If `uv` is not in your system PATH, use an absolute path (e.g., `/path/to/uv`)\n- `./server.py` should be modified to the actual location of your server script (can use absolute path or path relative to workspace)\n\n\n\n```json\n{\n  \"mcpServers\": {\n    \"unsplash\": {\n      \"command\": \"uv\",\n      \"args\": [\"run\", \"--with\", \"fastmcp\", \"fastmcp\", \"run\", \"./server.py\"],\n      \"env\": {\n        \"UNSPLASH_ACCESS_KEY\": \"${YOUR_ACCESS_KEY}\"\n      }\n    }\n  }\n}\n```\n\n### Using in Cursor\n\n\n\n## 🛠️ Available Tools\n\n### Search Photos\n\n```json\n{\n  \"tool\": \"search_photos\",\n  \"query\": \"mountain\",\n  \"per_page\": 5,\n  \"orientation\": \"landscape\"\n}\n```\n\n## 🔄 Other Implementations\n\n- Golang: [unsplash-mcp-server](https://github.com/douglarek/unsplash-mcp-server)\n- Java: [unsplash-mcp-server](https://github.com/JavaProgrammerLB/unsplash-mcp-server)\n\n## 📄 License\n\n[MIT License](LICENSE)\n\n## 📬 Contact\n\n- [Twitter/X](https://x.com/hellokaton)\n- [GitHub Issues](https://github.com/hellokaton/unsplash-mcp-server/issues)",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "mcp",
        "unsplash",
        "search",
        "unsplash mcp",
        "image search",
        "mcp server"
      ],
      "category": "search--data-extraction"
    },
    "imprvhub--mcp-claude-hackernews": {
      "owner": "imprvhub",
      "name": "mcp-claude-hackernews",
      "url": "https://github.com/imprvhub/mcp-claude-hackernews",
      "imageUrl": "",
      "description": "An integration that allows Claude Desktop to interact with Hacker News using the Model Context Protocol (MCP).",
      "stars": 7,
      "forks": 6,
      "license": "Mozilla Public License 2.0",
      "language": "TypeScript",
      "updated_at": "2025-09-15T07:41:41Z",
      "readme_content": "# MCP Claude Hacker News\n[![Verified on MseeP](https://mseep.ai/badge.svg)](https://mseep.ai/app/63f8ec05-a890-4a4d-9d8d-bea3c9a05c54) [![Trust Score](https://archestra.ai/mcp-catalog/api/badge/quality/imprvhub/mcp-claude-hackernews)](https://archestra.ai/mcp-catalog/imprvhub__mcp-claude-hackernews)\n[![smithery badge](https://smithery.ai/badge/@imprvhub/mcp-claude-spotify)](https://smithery.ai/server/@imprvhub/mcp-claude-hackernews)\n\n\n<table style=\"border-collapse: collapse; width: 100%; table-layout: fixed;\">\n<tr>\n<td style=\"padding: 15px; vertical-align: middle; border: none; text-align: center;\">\n  <a href=\"https://mseep.ai/app/imprvhub-mcp-claude-hackernews\">\n    <img src=\"https://mseep.net/pr/imprvhub-mcp-claude-hackernews-badge.png\" alt=\"MseeP.ai Security Assessment Badge\" />\n  </a>\n</td>  \n<td style=\"width: 40%; padding: 15px; vertical-align: middle; border: none;\">An integration that allows Claude Desktop to interact with Hacker News using the Model Context Protocol (MCP).</td>\n<td style=\"width: 60%; padding: 0; vertical-align: middle; border: none; min-width: 300px; text-align: center;\"><a href=\"https://glama.ai/mcp/servers/@imprvhub/mcp-claude-hackernews\">\n  <img style=\"max-width: 100%; height: auto; min-width: 300px;\" src=\"https://glama.ai/mcp/servers/@imprvhub/mcp-claude-hackernews/badge\" alt=\"mcp-claude-hackernews MCP server\" />\n</a></td>\n</tr>\n</table>\n\n\n## Features\n\n- Browse latest stories from Hacker News\n- View top and best-rated stories\n- Get story details\n- Read comments for stories\n- Clean formatting of Hacker News content for better readability\n\n## Demo\n\n<p>\n  <a href=\"https://www.youtube.com/watch?v=SmPD6MLifJo\">\n    \n  </a>\n</p>\n\n## Requirements\n\n- Node.js 16 or higher\n- Claude Desktop\n- Internet connection to access Hacker News API\n\n## Installation\n\n### Installing Manually\n1. Clone or download this repository:\n```bash\ngit clone https://github.com/imprvhub/mcp-claude-hackernews\ncd mcp-claude-hackernews\n```\n\n2. Install dependencies:\n```bash\nnpm install\n```\n\n3. Build the project:\n```bash\nnpm run build\n```\n\n## Running the MCP Server\n\nThere are two ways to run the MCP server:\n\n### Option 1: Running manually\n\n1. Open a terminal or command prompt\n2. Navigate to the project directory\n3. Run the server directly:\n\n```bash\nnode build/index.js\n```\n\nKeep this terminal window open while using Claude Desktop. The server will run until you close the terminal.\n\n### Option 2: Auto-starting with Claude Desktop (recommended for regular use)\n\nThe Claude Desktop can automatically start the MCP server when needed. To set this up:\n\n#### Configuration\n\nThe Claude Desktop configuration file is located at:\n\n- **macOS**: `~/Library/Application Support/Claude/claude_desktop_config.json`\n- **Windows**: `%APPDATA%\\Claude\\claude_desktop_config.json`\n- **Linux**: `~/.config/Claude/claude_desktop_config.json`\n\nEdit this file to add the Hacker News MCP configuration. If the file doesn't exist, create it:\n\n```json\n{\n  \"mcpServers\": {\n    \"hackerNews\": {\n      \"command\": \"node\",\n      \"args\": [\"ABSOLUTE_PATH_TO_DIRECTORY/mcp-claude-hackernews/build/index.js\"]\n    }\n  }\n}\n```\n\n**Important**: Replace `ABSOLUTE_PATH_TO_DIRECTORY` with the **complete absolute path** where you installed the MCP\n  - macOS/Linux example: `/Users/username/mcp-claude-hackernews`\n  - Windows example: `C:\\\\Users\\\\username\\\\mcp-claude-hackernews`\n\nIf you already have other MCPs configured, simply add the \"hackerNews\" section inside the \"mcpServers\" object. Here's an example of a configuration with multiple MCPs:\n\n```json\n{\n  \"mcpServers\": {\n    \"otherMcp1\": {\n      \"command\": \"...\",\n      \"args\": [\"...\"]\n    },\n    \"otherMcp2\": {\n      \"command\": \"...\",\n      \"args\": [\"...\"]\n    },\n    \"hackerNews\": {\n      \"command\": \"node\",\n      \"args\": [\n        \"ABSOLUTE_PATH_TO_DIRECTORY/mcp-claude-hackernews/build/index.js\"\n      ]\n    }\n  }\n}\n```\n\nThe MCP server will automatically start when Claude Desktop needs it, based on the configuration in your `claude_desktop_config.json` file.\n\n## Usage\n\n1. Restart Claude Desktop after modifying the configuration\n2. In Claude, use the Hacker News tools to interact with Hacker News\n3. The MCP server runs as a child process managed by Claude Desktop\n\n## Available Tools\n\nThe Hacker News MCP provides **5 specialized tools** for different functions:\n\n| Tool | Description | Parameters | Example Usage |\n|------|-------------|------------|---------------|\n| `hn_latest` | Get the most recent stories from Hacker News | `limit`: Optional number of stories (1-50, default: 10) | Get 20 latest stories |\n| `hn_top` | Get the top-ranked stories from Hacker News | `limit`: Optional number of stories (1-50, default: 10) | Get 15 top stories |\n| `hn_best` | Get the best stories from Hacker News | `limit`: Optional number of stories (1-50, default: 10) | Get 25 best stories |\n| `hn_story` | Get detailed information about a specific story | `story_id`: Required story ID (number) | Get story details by ID |\n| `hn_comments` | Get comments for a story | `story_id`: Story ID (number) OR `story_index`: Index from last list (1-based) | Get comments by story ID or index |\n\n### Tool Parameters Details\n\n#### `hn_latest`, `hn_top`, `hn_best`\n- **`limit`** (optional): Number of stories to fetch\n  - Type: Number\n  - Range: 1-50\n  - Default: 10\n\n#### `hn_story`\n- **`story_id`** (required): The ID of the story to fetch\n  - Type: Number\n  - Example: 12345678\n\n#### `hn_comments`\n- **`story_id`** (optional): The ID of the story to get comments for\n  - Type: Number\n  - Example: 12345678\n- **`story_index`** (optional): The index of the story from the last fetched list\n  - Type: Number (1-based)\n  - Example: 3 (for the 3rd story in the last list)\n\n*Note: For `hn_comments`, you must provide either `story_id` OR `story_index`*\n\n## Example Usage\n\nHere are various examples of how to use the Hacker News MCP with Claude:\n\n### Direct Tool Usage:\n\n```\n\"Use hn_latest to get 20 recent stories\"\n\"Use hn_top with limit 15 to get top stories\"\n\"Use hn_best to get 25 best stories\"\n\"Use hn_story with story_id 29384756 to get story details\"\n\"Use hn_comments with story_index 3 to get comments for the 3rd story\"\n\"Use hn_comments with story_id 12345678 to get comments for that story\"\n```\n\n### Natural Language Queries:\n\nYou can also interact with the MCP using natural language. Claude will interpret these requests and use the appropriate tools:\n\n- \"Show me the top 30 stories on Hacker News today\"\n- \"What are the 40 latest posts on Hacker News?\"\n- \"I'd like to see the 20 best articles from Hacker News\"\n- \"Can you fetch me 30 recent tech news stories from Hacker News?\"\n- \"Tell me what's the top 50 trending topics on Hacker News\"\n- \"Show me 20 Hacker News stories about machine learning\"\n- \"Get me the 40 most recent Hacker News headlines\"\n- \"What are the 30 most active discussions on Hacker News right now?\"\n- \"I'm interested in reading the 40 most popular Hacker News articles this week\"\n- \"Show me a list of 20 best programming articles from Hacker News\"\n- \"Get the comments for story number 5 from the last list\"\n- \"Show me the details of story ID 12345678\"\n\n### Language Translation Requests:\n\nYou can request Hacker News content to be translated into different languages:\n\n- \"Show me the top 30 stories on Hacker News today in Spanish\"\n- \"Get the 20 latest Hacker News posts and translate them to French\"\n- \"I'd like to see the 40 best articles from Hacker News in German\"\n- \"Show me 30 recent Hacker News stories translated to Japanese\"\n- \"Get the top 20 Hacker News articles and present them in Portuguese\"\n\n## Troubleshooting\n\n### \"Server disconnected\" error\nIf you see the error \"MCP Hacker News: Server disconnected\" in Claude Desktop:\n\n1. **Verify the server is running**:\n   - Open a terminal and manually run `node build/index.js` from the project directory\n   - If the server starts successfully, use Claude while keeping this terminal open\n\n2. **Check your configuration**:\n   - Ensure the absolute path in `claude_desktop_config.json` is correct for your system\n   - Double-check that you've used double backslashes (`\\\\`) for Windows paths\n   - Verify you're using the complete path from the root of your filesystem\n\n3. **Try the auto-start option**:\n   - Set up the auto-start script for your operating system as described in the \"Setting up auto-start scripts\" section\n   - This ensures the server is always running when you need it\n\n### Tools not appearing in Claude\nIf the Hacker News tools don't appear in Claude:\n- Make sure you've restarted Claude Desktop after configuration\n- Check the Claude Desktop logs for any MCP communication errors\n- Ensure the MCP server process is running (run it manually to confirm)\n- Verify that the MCP server is correctly registered in the Claude Desktop MCP registry\n\n### Checking if the server is running\nTo check if the server is running:\n\n- **Windows**: Open Task Manager, go to the \"Details\" tab, and look for \"node.exe\"\n- **macOS/Linux**: Open Terminal and run `ps aux | grep node`\n\nIf you don't see the server running, start it manually or use the auto-start method.\n\n## Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n\n## License\n\nThis project is licensed under the Mozilla Public License 2.0 - see the [LICENSE](https://github.com/imprvhub/mcp-claude-hackernews/blob/main/LICENSE) file for details.\n\n## Related Links\n\n- [Model Context Protocol](https://modelcontextprotocol.io/)\n- [Hacker News API](https://github.com/HackerNews/API)\n- [Claude Desktop](https://claude.ai/download)\n- [MCP Series](https://github.com/mcp-series)",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "hackernews",
        "hacker",
        "mcp",
        "hackernews integration",
        "hacker news",
        "interact hacker"
      ],
      "category": "search--data-extraction"
    },
    "imprvhub--mcp-domain-availability": {
      "owner": "imprvhub",
      "name": "mcp-domain-availability",
      "url": "https://github.com/imprvhub/mcp-domain-availability",
      "imageUrl": "",
      "description": "A Model Context Protocol (MCP) server that enables Claude Desktop to check domain availability across 50+ TLDs. Features DNS/WHOIS verification, bulk checking, and smart suggestions. Zero-clone installation via uvx.",
      "stars": 18,
      "forks": 5,
      "license": "Mozilla Public License 2.0",
      "language": "Python",
      "updated_at": "2025-09-29T14:44:45Z",
      "readme_content": "## MCP Domain Availability Checker\n\n[![smithery badge](https://smithery.ai/badge/@imprvhub/mcp-domain-availability)](https://smithery.ai/server/@imprvhub/mcp-domain-availability)\n\n<table style=\"border-collapse: collapse; width: 100%; table-layout: fixed;\">\n<tr>\n<td style=\"width: 40%; padding: 15px; vertical-align: middle; border: none;\">A Model Context Protocol (MCP) integration that provides Claude Desktop with domain availability checking across popular TLDs.</td>\n<td style=\"width: 60%; padding: 0; vertical-align: middle; border: none; min-width: 300px; text-align: center;\"><a href=\"https://glama.ai/mcp/servers/@imprvhub/mcp-domain-availability\">\n  <img style=\"max-width: 100%; height: auto; min-width: 300px;\" src=\"https://glama.ai/mcp/servers/@imprvhub/mcp-domain-availability/badge\" alt=\"Domain Availability MCP server\" />\n</a></td>\n</tr>\n</table>\n\n### Features\n\n- **Domain Availability Checking**\n  - Check availability across 50+ popular TLD extensions\n  - Support for popular (.com, .io, .ai), country (.us, .uk, .de), and new TLDs (.app, .dev, .tech)\n  - Dual verification using DNS and WHOIS for accuracy\n  - Smart TLD suggestions organized by popularity\n\n- **Search Capabilities**\n  - Check specific domains with exact TLD matching\n  - Bulk checking across supported extensions for a given name\n  - Parallel processing for faster domain queries\n  - Organized results by TLD categories\n\n- **MCP Integration**\n  - Easy setup with uvx package management\n  - Seamless integration with Claude Desktop\n  - Real-time availability status updates\n  - Performance metrics and timing information\n\n- **AI Assistant Features**\n  - Natural language domain queries through Claude\n  - Automated domain suggestion workflows\n  - Smart recommendations based on availability\n\n### Demo\n<p>\n <a href=\"https://www.youtube.com/watch?v=pJjrkEihlWE\">\n   \n </a>\n</p>\n\n<details>\n<summary>Timestamps:</summary>\nClick on any timestamp to jump to that section of the video\n\n[**00:00**](https://www.youtube.com/watch?v=pJjrkEihlWE&t=0s) - **Checking google.com availability**  \nTesting a well-known premium domain to demonstrate the domain checking functionality and alternative TLD suggestions.\n\n[**00:20**](https://www.youtube.com/watch?v=pJjrkEihlWE&t=20s) - **Testing myawesomesite.com**  \nVerifying availability for a custom domain name and exploring alternative extension options.\n\n[**00:40**](https://www.youtube.com/watch?v=pJjrkEihlWE&t=40s) - **Verifying techstartup2026.io**  \nExploring tech startup domain options and checking availability across multiple TLD extensions.\n\n[**01:00**](https://www.youtube.com/watch?v=pJjrkEihlWE&t=60s) - **Analyzing aitools domain**  \nChecking competitive AI industry domains and analyzing market availability for startup naming.\n</details>\n\n### Requirements\n\n- Python 3.10 or higher\n- Claude Desktop\n- [uv](https://docs.astral.sh/uv/) package manager\n\n#### Dependencies Installation\n\nInstall uv package manager using one of these methods:\n\n**Official installer (recommended):**\n```bash\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n```\n\n**Homebrew (macOS/Linux):**\n```bash\nbrew install uv\n```\n\n**Install Homebrew (if needed):**\n- Visit [https://brew.sh](https://brew.sh) for installation instructions on all operating systems\n- Or run: `/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"`\n\nThe MCP server automatically manages Python dependencies through uvx.\n\n### Installation\n\n#### Zero-Clone Installation (Recommended)\n\nThe MCP Domain Availability Checker supports direct installation without cloning repositories, using uvx for package management.\n\n#### Configuration\n\nThe Claude Desktop configuration file is located at:\n\n- **macOS**: `~/Library/Application Support/Claude/claude_desktop_config.json`\n- **Windows**: `%APPDATA%\\Claude\\claude_desktop_config.json`\n- **Linux**: `~/.config/Claude/claude_desktop_config.json`\n\nEdit this file to add the Domain Availability MCP configuration:\n\n```json\n{\n  \"mcpServers\": {\n    \"mcp-domain-availability\": {\n      \"command\": \"uvx\",\n      \"args\": [\n        \"--python=3.10\",\n        \"--from\",\n        \"git+https://github.com/imprvhub/mcp-domain-availability\",\n        \"mcp-domain-availability\"\n      ]\n    }\n  }\n}\n```\n\nIf you already have other MCPs configured, simply add the \"mcp-domain-availability\" section inside the \"mcpServers\" object:\n\n```json\n{\n  \"mcpServers\": {\n    \"otherMcp\": {\n      \"command\": \"...\",\n      \"args\": [\"...\"]\n    },\n    \"mcp-domain-availability\": {\n      \"command\": \"uvx\",\n      \"args\": [\n        \"--python=3.10\",\n        \"--from\",\n        \"git+https://github.com/imprvhub/mcp-domain-availability\",\n        \"mcp-domain-availability\"\n      ]\n    }\n  }\n}\n```\n\n### Installing via Smithery\n\nTo install mcp-domain-availability for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@imprvhub/mcp-domain-availability):\n\n```bash\nnpx -y @smithery/cli install @imprvhub/mcp-domain-availability --client claude\n```\n\n#### Manual Installation\n\nFor development or local testing:\n\n1. Clone the repository:\n```bash\ngit clone https://github.com/imprvhub/mcp-domain-availability\ncd mcp-domain-availability\n```\n\n2. Install dependencies:\n```bash\nuv sync\n```\n\n3. Run locally:\n```bash\nuv run src/mcp_domain_availability/main.py\n```\n\n### How It Works\n\nThe MCP Domain Availability Checker uses multiple verification methods to determine domain availability:\n\n1. **DNS Resolution**: Checks if the domain resolves to an IP address\n2. **WHOIS Lookup**: Queries WHOIS databases for registration information\n3. **Socket Connection**: Falls back to socket-based checking when other methods aren't available\n\nThe tool combines results from these methods to provide accurate availability status, with parallel processing for checking multiple domains simultaneously.\n\n### Available Tools\n\n#### Domain Checking\n\n| Tool Name | Description | Usage |\n|-----------|-------------|-------|\n| `check_domain` | Check domain availability with --domain flag | `mysite.com --domain` or `mysite --domain` |\n\n### Supported TLD Categories\n\n#### Popular TLDs (12)\ncom, net, org, io, ai, app, dev, co, xyz, me, info, biz\n\n#### Country TLDs (35)\nus, uk, ca, au, de, fr, it, es, nl, jp, kr, cn, in, br, mx, ar, cl, co, pe, ru, pl, cz, ch, at, se, no, dk, fi, be, pt, gr, tr, za, eg, ma, ng, ke\n\n#### New TLDs\ntech, online, site, website, store, shop, cloud, digital, blog, news & more.\n\n### Example Usage\n\nHere are examples of how to use the MCP Domain Availability Checker with Claude:\n\n#### Single Domain Check\n\n```\nCheck if mysite.com is available using --domain\n```\n\n#### Domain Name Research\n\n```\nCheck availability for \"startup\" across all TLDs using --domain\n```\n\n#### Specific Domain Verification\n\n```\nIs awesome.io available? Use --domain to check\n```\n\n### Output Format\n\nThe tool provides comprehensive results including:\n\n- **Requested Domain**: Status of the exact domain queried (if specific TLD provided)\n- **Available Domains**: List of available domains sorted alphabetically\n- **Unavailable Domains**: List of registered domains\n- **Summary Statistics**: Breakdown by TLD categories (Popular, Country, New TLDs)\n- **Performance Metrics**: Check duration for each domain\n\n### Troubleshooting\n\n#### \"Server disconnected\" error\nIf you see connection errors in Claude Desktop:\n\n1. **Verify uvx installation**:\n   - Run `uvx --version` to ensure uvx is properly installed\n   - Reinstall uv if necessary: `curl -LsSf https://astral.sh/uv/install.sh | sh`\n\n2. **Check Python version**:\n   - Ensure Python 3.10+ is available: `python3 --version`\n\n### DNS resolution issues\nIf domain checks are failing:\n\n1. **Network connectivity**:\n   - Verify internet connection is stable\n   - Check if DNS servers are accessible\n\n2. **Rate limiting**:\n   - Large bulk checks may hit rate limits from DNS/WHOIS services\n   - The tool uses a semaphore to limit concurrent requests to 20\n\n#### Configuration issues\nIf the MCP server isn't starting:\n\n1. **Verify configuration syntax**:\n   - Ensure JSON syntax is valid in `claude_desktop_config.json`\n   - Check that all brackets and quotes are properly matched\n\n2. **Restart Claude Desktop**:\n   - Close and restart Claude Desktop after configuration changes\n\n## Development\n\n#### Project Structure\n\n- `main.py`: Main entry point with MCP server and domain checking logic\n- Domain checking functions with DNS, WHOIS, and socket fallback methods\n- TLD management with categorized lists\n- Async processing for parallel domain checks\n\n#### Building\n\n```bash\nuv build\n```\n\n### Testing\n\n```bash\nuv run pytest\n```\n\n#### Local Development\n\n```bash\nuv run main.py\n```\n\n### Security Considerations\n\nThe MCP Domain Availability Checker makes external network requests to DNS servers and WHOIS services. Users should be aware that:\n\n- Domain queries may be logged by DNS providers\n- WHOIS queries are typically logged and may be rate-limited\n- No personal information is transmitted beyond the domain names being checked\n- All queries are read-only and do not modify any external systems\n\n### Contributing\n\nContributions are welcome! Areas for improvement include:\n\n- Adding support for additional TLD categories\n- Implementing caching mechanisms for faster repeated queries\n- Enhancing WHOIS parsing for more detailed domain information\n- Improving error handling and retry mechanisms\n\n### License\n\nThis project is licensed under the Mozilla Public License 2.0 - see the [LICENSE](https://github.com/imprvhub/mcp-domain-availability/blob/main/LICENSE) file for details.\n\n\n## Related Links\n\n- [Model Context Protocol](https://modelcontextprotocol.io/)\n- [Claude Desktop](https://claude.ai/download)\n- [uv Package Manager](https://docs.astral.sh/uv/)\n- [MCP Series](https://github.com/mcp-series)",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "mcp",
        "protocol",
        "availability",
        "mcp domain",
        "imprvhub mcp",
        "mcp server"
      ],
      "category": "search--data-extraction"
    },
    "imprvhub--mcp-rss-aggregator": {
      "owner": "imprvhub",
      "name": "mcp-rss-aggregator",
      "url": "https://github.com/imprvhub/mcp-rss-aggregator",
      "imageUrl": "",
      "description": "Model Context Protocol Server for aggregating RSS feeds in Claude Desktop.",
      "stars": 10,
      "forks": 6,
      "license": "Mozilla Public License 2.0",
      "language": "TypeScript",
      "updated_at": "2025-09-27T11:11:34Z",
      "readme_content": "# MCP RSS Aggregator\n[![Trust Score](https://archestra.ai/mcp-catalog/api/badge/quality/imprvhub/mcp-rss-aggregator)](https://archestra.ai/mcp-catalog/imprvhub__mcp-rss-aggregator)\n[![Verified on MseeP](https://mseep.ai/badge.svg)](https://mseep.ai/app/51854dcf-6cb2-4bd0-a37f-5b87ba25d7c7)\n[![smithery badge](https://smithery.ai/badge/@imprvhub/mcp-rss-aggregator)](https://smithery.ai/server/@imprvhub/mcp-rss-aggregator)\n\n<table style=\"border-collapse: collapse; width: 100%; table-layout: fixed;\">\n<tr>\n<td style=\"padding: 15px; vertical-align: middle; border: none; text-align: center;\">\n  <a href=\"https://mseep.ai/app/imprvhub-mcp-rss-aggregator\">\n    <img src=\"https://mseep.net/pr/imprvhub-mcp-rss-aggregator-badge.png\" alt=\"MseeP.ai Security Assessment Badge\" />\n  </a>\n</td>\n<td style=\"width: 40%; padding: 15px; vertical-align: middle; border: none;\">An integration that allows Claude Desktop to fetch and read content from your favorite RSS feeds using the Model Context Protocol (MCP).</td>\n<td style=\"width: 60%; padding: 0; vertical-align: middle; border: none; min-width: 300px; text-align: center;\"><a href=\"https://glama.ai/mcp/servers/@imprvhub/mcp-rss-aggregator\">\n  <img style=\"max-width: 100%; height: auto; min-width: 300px;\" src=\"https://glama.ai/mcp/servers/@imprvhub/mcp-rss-aggregator/badge\" alt=\"RSS Aggregator MCP server\" />\n</a></td>\n</tr>\n</table>\n\n## Features\n\n- Read articles from your favorite RSS feeds directly in Claude Desktop\n- Support for OPML files to import your existing feed subscriptions\n- Organize feeds by categories\n- Get the latest articles across all your feeds\n- Filter articles by feed source or category\n- Well-formatted article presentation with titles, snippets, and links\n\n## Demo\n\n<p>\n  <a href=\"https://youtu.be/9pvm078fHkQ\">\n    \n  </a>\n</p>\n\n<details>\n<summary> Timestamps </summary>\n\nClick on any timestamp to jump to that section of the video\n\n[00:00](https://youtu.be/9pvm078fHkQ&t=0s) - **Sample RSS Feed Demonstration**:\nUsing the default 'sample-feeds.opml' file included in the repository. This segment displays how Claude processes and presents news content from sources like TechCrunch, The Verge, and other technology publications through the MCP (Model Context Protocol).\n\n[01:05](https://youtu.be/9pvm078fHkQ&t=65s) - **Configuration File Editing Process**:\nStep-by-step walkthrough of accessing and modifying the claude_desktop_config.json file to change the OPML file path reference from the default sample to a customized 'my-feeds.opml' file.\n\n[01:15](https://youtu.be/9pvm078fHkQ&t=75s) - **Application Restart Procedure**:\nIllustrating the necessary step of closing and reopening the Claude Desktop application to properly load and apply the modified OPML file configuration changes.\n\n[01:25](https://youtu.be/9pvm078fHkQ&t=85s) - **Custom RSS Feed Results**:\nDemonstration of the results after implementing the custom OPML file. This section highlights the expanded and more diverse news sources now available through Claude Desktop, including Spanish-language content.\n</details>\n\n## Requirements\n\n- Node.js 16 or higher\n- Claude Desktop\n- Internet connection to access RSS feeds\n\n## Installation\n\n### Installing Manually\n1. Clone or download this repository:\n```bash\ngit clone https://github.com/imprvhub/mcp-rss-aggregator\ncd mcp-rss-aggregator\n```\n\n2. Install dependencies:\n```bash\nnpm install\n```\n\n3. Build the project:\n```bash\nnpm run build\n```\n\n## Feed Configuration\n\nThe RSS Aggregator supports both OPML and JSON formats for feed configuration.\n\n### Using OPML (Recommended)\n\nOPML (Outline Processor Markup Language) is a standard format used by most RSS readers to export and import feed subscriptions. \n\nA sample OPML file with popular feeds is included in the `public/sample-feeds.opml` file. You can:\n\n1. Use this file as-is\n2. Edit it to add your own feeds\n3. Replace it with an export from your existing RSS reader\n\nMost RSS readers allow you to export your subscriptions as an OPML file.\n\n### Using JSON\n\nAlternatively, you can define your feeds in a JSON file with the following format:\n\n```json\n[\n  {\n    \"title\": \"Hacker News\",\n    \"url\": \"https://news.ycombinator.com/rss\",\n    \"htmlUrl\": \"https://news.ycombinator.com/\",\n    \"category\": \"Tech News\"\n  },\n  {\n    \"title\": \"TechCrunch\",\n    \"url\": \"https://techcrunch.com/feed/\",\n    \"htmlUrl\": \"https://techcrunch.com/\",\n    \"category\": \"Tech News\"\n  }\n]\n```\n\n## Running the MCP Server\n\nThere are two ways to run the MCP server:\n\n### Option 1: Running manually\n\n1. Open a terminal or command prompt\n2. Navigate to the project directory\n3. Run the server directly:\n\n```bash\nnode build/index.js\n```\n\nKeep this terminal window open while using Claude Desktop. The server will run until you close the terminal.\n\n### Option 2: Auto-starting with Claude Desktop (recommended for regular use)\n\nThe Claude Desktop can automatically start the MCP server when needed. To set this up:\n\n#### Configuration\n\nThe Claude Desktop configuration file is located at:\n\n- **macOS**: `~/Library/Application Support/Claude/claude_desktop_config.json`\n- **Windows**: `%APPDATA%\\Claude\\claude_desktop_config.json`\n- **Linux**: `~/.config/Claude/claude_desktop_config.json`\n\nEdit this file to add the RSS Aggregator MCP configuration. If the file doesn't exist, create it:\n\n```json\n{\n  \"mcpServers\": {\n    \"rssAggregator\": {\n      \"command\": \"node\",\n      \"args\": [\"ABSOLUTE_PATH_TO_DIRECTORY/mcp-rss-aggregator/build/index.js\"],\n      \"feedsPath\": \"ABSOLUTE_PATH_TO_YOUR_FEEDS_FILE.opml\"\n    }\n  }\n}\n```\n\n**Important Notes**: \n- Replace `ABSOLUTE_PATH_TO_DIRECTORY` with the **complete absolute path** where you installed the MCP\n  - macOS/Linux example: `/Users/username/mcp-rss-aggregator`\n  - Windows example: `C:\\\\Users\\\\username\\\\mcp-rss-aggregator`\n- Replace `ABSOLUTE_PATH_TO_YOUR_FEEDS_FILE.opml` with the path to your OPML or JSON file\n  - If omitted, the sample feeds file will be used\n\nIf you already have other MCPs configured, simply add the \"rssAggregator\" section inside the \"mcpServers\" object:\n\n```json\n{\n  \"mcpServers\": {\n    \"otherMcp1\": {\n      \"command\": \"...\",\n      \"args\": [\"...\"]\n    },\n    \"rssAggregator\": {\n      \"command\": \"node\",\n      \"args\": [\n        \"ABSOLUTE_PATH_TO_DIRECTORY/mcp-rss-aggregator/build/index.js\"\n      ],\n      \"feedsPath\": \"ABSOLUTE_PATH_TO_YOUR_FEEDS_FILE.opml\"\n    }\n  }\n}\n```\n\nThe MCP server will automatically start when Claude Desktop needs it, based on the configuration in your `claude_desktop_config.json` file.\n\n## Usage\n\n1. Restart Claude Desktop after modifying the configuration\n2. In Claude, use the `rss` command to interact with the RSS Aggregator MCP Server\n3. The MCP server runs as a subprocess managed by Claude Desktop\n\n## Available Commands\n\nThe RSS Aggregator MCP provides a tool named `rss` with several commands:\n\n| Command | Description | Parameters | Example |\n|---------|-------------|------------|---------|\n| `latest` | Show latest articles from all feeds | Optional limit (--N) | `rss latest --20` |\n| `top` or `best` | Show top articles from all feeds | Optional limit (--N) | `rss top --15` |\n| `list` | List all available feeds | None | `rss list` |\n| `--[feed-id]` | Show articles from a specific feed | Optional limit (--N) | `rss --hackernews --10` |\n| `[category]` | Show articles from a specific category | Optional limit (--N) | `rss \"Tech News\" --20` |\n| `set-feeds-path --[path]` | Set path to OPML/JSON file | Path to file | `rss set-feeds-path --/path/to/feeds.opml` |\n\n## Example Usage\n\nHere are various examples of how to use the RSS Aggregator with Claude:\n\n### Direct Commands:\n\n```\nrss latest\nrss top --20\nrss list\nrss \"Tech News\"\nrss --hackernews\nrss --techcrunch --15\n```\n\n### Natural Language Queries:\n\nYou can also interact with the MCP using natural language. Claude will interpret these requests and use the appropriate commands:\n\n- \"What are the latest news on Hacker News?\"\n- \"Show me the top tech articles today\"\n- \"Fetch the latest articles from my programming feeds\"\n- \"List all my RSS feeds\"\n\n## Extended Usage Examples\n\n### Daily News Briefing\n\nGet your news briefing from all your sources:\n\n```\nrss latest --25\n```\n\nThis will fetch the 25 most recent articles across all your feeds, giving you a quick overview of the latest news.\n\n### Exploring Top Content\n\nFind the most important or popular articles:\n\n```\nrss top --20\n```\n\n### Category-Based Reading\n\nFocus on specific content categories:\n\n```\nrss \"Tech News\" --30\nrss \"Politics\" --15\nrss \"Science\" --10\n```\n\n### Source-Specific Updates\n\nRead updates from specific sources you follow:\n\n```\nrss --hackernews --20\nrss --nytimes\nrss --techcrunch --15\n```\n\n### Discover Your Available Feeds\n\nFind out what feeds you have configured:\n\n```\nrss list\n```\n\n### Combining Multiple Requests\n\nYou can make multiple sequential requests to build a comprehensive view:\n\n```\nrss \"Tech News\" --10\nrss \"Finance\" --10\nrss top --5\n```\n\n### Practical Workflows\n\n1. **Morning Routine**:\n   ```\n   rss top --10\n   rss \"News\" --5\n   ```\n\n2. **Industry Research**:\n   ```\n   rss \"Industry News\" --15\n   rss --bloomberg --5\n   ```\n\n3. **Tech Updates**:\n   ```\n   rss --hackernews --10\n   rss --techcrunch --5\n   ```\n\n### Working with Claude\n\nYou can ask Claude to analyze or summarize the articles:\n\n1. After running: `rss latest --10`\n   Ask: \"Can you summarize these articles?\"\n\n2. After running: `rss \"Tech News\" --15`\n   Ask: \"What are the key trends in these tech articles?\"\n\n3. After running: `rss --nytimes --washingtonpost --10`\n   Ask: \"Compare how these sources cover current events\"\n\n## Troubleshooting\n\n### \"Server disconnected\" error\nIf you see the error \"MCP RSS Aggregator: Server disconnected\" in Claude Desktop:\n\n1. **Verify the server is running**:\n   - Open a terminal and manually run `node build/index.js` from the project directory\n   - If the server starts successfully, use Claude while keeping this terminal open\n\n2. **Check your configuration**:\n   - Ensure the absolute path in `claude_desktop_config.json` is correct for your system\n   - Double-check that you've used double backslashes (`\\\\`) for Windows paths\n   - Verify you're using the complete path from the root of your filesystem\n\n### Tools not appearing in Claude\nIf the RSS Aggregator tools don't appear in Claude:\n- Make sure you've restarted Claude Desktop after configuration\n- Check the Claude Desktop logs for any MCP communication errors\n- Ensure the MCP server process is running (run it manually to confirm)\n\n### Feeds not loading\nIf your feeds aren't loading properly:\n- Make sure your OPML/JSON file is correctly formatted\n- Check if the `feedsPath` in your configuration is correct\n- Try running the server manually with a known good feeds file\n\n## Contributing\n\nContributions to improve the RSS Aggregator are welcome! Here are some ways you can contribute:\n\n1. Add support for more feed formats\n2. Improve feed parsing and error handling\n3. Add more visualization options for articles\n4. Improve categorization and filtering capabilities\n\n## License\n\nThis project is licensed under the Mozilla Public License 2.0 - see the [LICENSE](https://github.com/imprvhub/mcp-rss-aggregator/blob/main/LICENSE) file for details.\n\n## Related Links\n\n- [Model Context Protocol](https://modelcontextprotocol.io/)\n- [Claude Desktop](https://claude.ai/download)\n- [MCP Series](https://github.com/mcp-series)",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "feeds",
        "rss",
        "aggregator",
        "rss aggregator",
        "rss feeds",
        "mcp rss"
      ],
      "category": "search--data-extraction"
    },
    "isnow890--naver-search-mcp": {
      "owner": "isnow890",
      "name": "naver-search-mcp",
      "url": "https://github.com/isnow890/naver-search-mcp",
      "imageUrl": "",
      "description": "MCP server for Naver Search API integration, supporting blog, news, shopping search and DataLab analytics features.",
      "stars": 40,
      "forks": 16,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-09-30T12:55:28Z",
      "readme_content": "# Naver Search MCP Server\n\n[![한국어](https://img.shields.io/badge/한국어-README-yellow)](README-ko.md)\n\n[![Trust Score](https://archestra.ai/mcp-catalog/api/badge/quality/isnow890/naver-search-mcp)](https://archestra.ai/mcp-catalog/isnow890__naver-search-mcp)\n[![smithery badge](https://smithery.ai/badge/@isnow890/naver-search-mcp)](https://smithery.ai/server/@isnow890/naver-search-mcp)\n[![MCP.so](https://img.shields.io/badge/MCP.so-Naver%20Search%20MCP-blue)](https://mcp.so/server/naver-search-mcp/isnow890)\n\n#### Version History\n\n###### 1.0.45 (2025-09-28)\n\n- Smithery compatibility issues resolved - Now available through Smithery with latest features\n- Excel compatibility issues fixed in category search - Replaced with JSON functionality\n- Web Korean search (`search_webkr`) functionality restored\n- Full compatibility with Smithery platform installations\n\n###### 1.0.44 (2025-08-31)\n\n- `get_current_korean_time` tool added - Essential time context tool for Korean timezone\n- Enhanced all existing tool descriptions to reference time tool for temporal queries\n- Improved temporal context handling for \"today\", \"now\", \"current\" searches\n- Comprehensive Korean time formatting with multiple output formats\n\n###### 1.0.4 (2025-08-21)\n\n- `find_category` tool added - with fuzzy matching and ranking system support\n- Enhanced parameter validation with Zod schema\n- Improved category search workflow\n\n###### 1.0.30 (2025-08-04)\n\n- MCP SDK upgraded to 1.17.1\n- Fixed compatibility issues with Smithery specification changes\n- Added comprehensive DataLab shopping category code documentation\n\n###### 1.0.2 (2025-04-26)\n\n- README updated: cafe article search tool and version history section improved\n\n###### 1.0.1 (2025-04-26)\n\n- Cafe article search feature added\n- Shopping category info added to zod\n- Source code refactored\n\n###### 1.0.0 (2025-04-08)\n\n- Initial release\n\n#### Prerequisites\n\n- Naver Developers API Key (Client ID and Secret)\n- Node.js 18 or higher\n- NPM 8 or higher\n- Docker (optional, for container deployment)\n\n#### Getting API Keys\n\n1. Visit [Naver Developers](https://developers.naver.com/apps/#/register)\n2. Click \"Register Application\"\n3. Enter application name and select ALL of the following APIs:\n   - Search (for blog, news, book search, etc.)\n   - DataLab (Search Trends)\n   - DataLab (Shopping Insight)\n4. Set the obtained Client ID and Client Secret as environment variables\n\n## Tool Details\n\n### Available tools:\n\n#### 🕐 Time & Context Tools\n\n- **get_current_korean_time**: Get current Korean time (KST) with comprehensive date/time information. Essential for understanding \"today\", \"now\", or \"current\" context in Korean timezone. Always use this tool when temporal context is needed for searches or analysis.\n\n#### 🆕 Category Search\n\n- **find_category**: Category search tool - No more need to manually check category numbers via URL for trend and shopping insight searches. The LLM will find it out as you say.\n\n#### Search Tools\n\n- **search_webkr**: Search Naver web documents\n- **search_news**: Search Naver news\n- **search_blog**: Search Naver blogs\n- **search_cafearticle**: Search Naver cafe articles\n- **search_shop**: Search Naver shopping\n- **search_image**: Search Naver images\n- **search_kin**: Search Naver KnowledgeiN\n- **search_book**: Search Naver books\n- **search_encyc**: Search Naver encyclopedia\n- **search_academic**: Search Naver academic papers\n- **search_local**: Search Naver local places\n\n#### DataLab Tools\n\n- **datalab_search**: Analyze search term trends\n- **datalab_shopping_category**: Analyze shopping category trends\n- **datalab_shopping_by_device**: Analyze shopping trends by device\n- **datalab_shopping_by_gender**: Analyze shopping trends by gender\n- **datalab_shopping_by_age**: Analyze shopping trends by age group\n- **datalab_shopping_keywords**: Analyze shopping keyword trends\n- **datalab_shopping_keyword_by_device**: Analyze shopping keyword trends by device\n- **datalab_shopping_keyword_by_gender**: Analyze shopping keyword trends by gender\n- **datalab_shopping_keyword_by_age**: Analyze shopping keyword trends by age group\n\n#### Complete Category List:\n\nFor a complete list of category codes, you can download from Naver Shopping Partner Center or extract them by browsing Naver Shopping categories.\n\n### 🎯 Business Use Cases & Scenarios\n\n#### 🛍️ E-commerce Market Research\n\n```javascript\n// Fashion trend discovery\nfind_category(\"fashion\") → Check top fashion categories and codes\ndatalab_shopping_category → Analyze seasonal fashion trends\ndatalab_shopping_age → Identify fashion target demographics\ndatalab_shopping_keywords → Compare \"dress\" vs \"jacket\" vs \"coat\"\n```\n\n#### 📱 Digital Marketing Strategy\n\n```javascript\n// Beauty industry analysis\nfind_category(\"cosmetics\") → Find beauty categories\ndatalab_shopping_gender → 95% female vs 5% male shoppers\ndatalab_shopping_device → Mobile dominance in beauty shopping\ndatalab_shopping_keywords → \"tint\" vs \"lipstick\" keyword performance\n```\n\n#### 🏢 Business Intelligence & Competitive Analysis\n\n```javascript\n// Tech product insights\nfind_category(\"smartphone\") → Check electronics categories\ndatalab_shopping_category → Track iPhone vs Galaxy trends\ndatalab_shopping_age → 20-30s as main smartphone buyers\ndatalab_shopping_device → PC vs mobile shopping behavior\n```\n\n#### 📊 Seasonal Business Planning\n\n```javascript\n// Holiday shopping analysis\nfind_category(\"gift\") → Gift categories\ndatalab_shopping_category → Black Friday, Christmas trends\ndatalab_shopping_keywords → \"Mother's Day gift\" vs \"birthday gift\"\ndatalab_shopping_age → Age-based gift purchasing patterns\n```\n\n#### 🎯 Customer Persona Development\n\n```javascript\n// Fitness market analysis\nfind_category(\"exercise\") → Sports/fitness categories\ndatalab_shopping_gender → Male vs female fitness spending\ndatalab_shopping_age → Primary fitness demographics (20-40s)\ndatalab_shopping_keywords → \"home workout\" vs \"gym\" trend analysis\n```\n\n### 📈 Advanced Analysis Scenarios\n\n#### Market Entry Strategy\n\n1. **Category Discovery**: Use `find_category` to explore market segments\n2. **Trend Analysis**: Identify growing vs declining categories\n3. **Demographic Targeting**: Age/gender analysis for customer targeting\n4. **Competitive Intelligence**: Keyword performance comparison\n5. **Device Strategy**: Mobile vs PC shopping optimization\n\n#### Product Launch Planning\n\n1. **Market Validation**: Category growth trends and seasonality\n2. **Target Customers**: Demographic analysis for product positioning\n3. **Marketing Channels**: Device preferences for advertising strategy\n4. **Competitive Landscape**: Keyword competition and opportunities\n5. **Pricing Strategy**: Category performance and price correlation\n\n#### Performance Monitoring\n\n1. **Category Health**: Monitor product category trends\n2. **Keyword Tracking**: Track brand and product keyword performance\n3. **Demographic Shifts**: Monitor changing customer demographics\n4. **Seasonal Patterns**: Plan inventory and marketing campaigns\n5. **Competitive Benchmarking**: Compare performance against category averages\n\n### Quick Reference: Popular Category Codes\n\n| Category            | Code     | Korean        |\n| ------------------- | -------- | ------------- |\n| Fashion/Clothing    | 50000000 | 패션의류      |\n| Cosmetics/Beauty    | 50000002 | 화장품/미용   |\n| Digital/Electronics | 50000003 | 디지털/가전   |\n| Sports/Leisure      | 50000004 | 스포츠/레저   |\n| Food/Beverages      | 50000008 | 식품/음료     |\n| Health/Medical      | 50000009 | 건강/의료용품 |\n\n💡 **Tip**: Use `find_category` with fuzzy searches like \"beauty\", \"fashion\", \"electronics\" to easily find categories.\n\n## Installation\n\n### Method 1: NPX Installation (Recommended)\n\nThe most reliable way to use this MCP server is through direct NPX installation. For detailed package information, see the [NPM package page](https://www.npmjs.com/package/@isnow890/naver-search-mcp).\n\n#### Claude Desktop Configuration\n\nAdd to Claude Desktop config file (`%APPDATA%\\Claude\\claude_desktop_config.json` on Windows, `~/Library/Application Support/Claude/claude_desktop_config.json` on macOS/Linux):\n\n```json\n{\n  \"mcpServers\": {\n    \"naver-search\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@isnow890/naver-search-mcp\"],\n      \"env\": {\n        \"NAVER_CLIENT_ID\": \"your_client_id\",\n        \"NAVER_CLIENT_SECRET\": \"your_client_secret\"\n      }\n    }\n  }\n}\n```\n\n#### Cursor AI Configuration\n\nAdd to `mcp.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"naver-search\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@isnow890/naver-search-mcp\"],\n      \"env\": {\n        \"NAVER_CLIENT_ID\": \"your_client_id\",\n        \"NAVER_CLIENT_SECRET\": \"your_client_secret\"\n      }\n    }\n  }\n}\n```\n\n### Method 2: Smithery Installation (Alternative - Known Issues)\n\n⚠️ **Important Notice**: Smithery installation may experience connection timeouts and hanging issues due to WebSocket relay infrastructure problems. This is a known Smithery platform issue, not a problem with this MCP server code. **NPX installation (Method 1) is strongly recommended for reliable operation.**\n\n#### Known Issues with Smithery:\n- Server initialization hangs or times out\n- `Error -32001: Request timed out`\n- WebSocket connection drops after handshake\n- Server shuts down unexpectedly before processing requests\n\n#### If you still want to try Smithery:\n\n##### For Claude Desktop:\n\n```bash\nnpx -y @smithery/cli@latest install @isnow890/naver-search-mcp --client claude\n```\n\n##### For other AI clients:\n\n```bash\n# Cursor\nnpx -y @smithery/cli@latest install @isnow890/naver-search-mcp --client cursor\n\n# Windsurf\nnpx -y @smithery/cli@latest install @isnow890/naver-search-mcp --client windsurf\n\n# Cline\nnpx -y @smithery/cli@latest install @isnow890/naver-search-mcp --client cline\n```\n\n**If you experience timeout issues with Smithery, please switch to Method 1 (NPX) for stable operation.**\n\n### Method 3: Local Installation\n\nFor local development or custom modifications:\n\n#### Step 1: Download and Build Source Code\n\n##### Clone with Git\n\n```bash\ngit clone https://github.com/isnow890/naver-search-mcp.git\ncd naver-search-mcp\nnpm install\nnpm run build\n```\n\n##### Or Download ZIP File\n\n1. Download the latest version from [GitHub Releases](https://github.com/isnow890/naver-search-mcp/releases)\n2. Extract the ZIP file to your desired location\n3. Navigate to the extracted folder in terminal:\n\n```bash\ncd /path/to/naver-search-mcp\nnpm install\nnpm run build\n```\n\n⚠️ **Important**: You must run `npm run build` after installation to generate the `dist` folder that contains the compiled JavaScript files.\n\n#### Step 2: Claude Desktop Configuration\n\nAfter building, you'll need the following information:\n\n- **NAVER_CLIENT_ID**: Client ID from Naver Developers\n- **NAVER_CLIENT_SECRET**: Client Secret from Naver Developers\n- **Installation Path**: Absolute path to the downloaded folder\n\n##### Windows Configuration\n\nAdd to Claude Desktop config file (`%APPDATA%\\Claude\\claude_desktop_config.json`):\n\n```json\n{\n  \"mcpServers\": {\n    \"naver-search\": {\n      \"type\": \"stdio\",\n      \"command\": \"cmd\",\n      \"args\": [\n        \"/c\",\n        \"node\",\n        \"C:\\\\path\\\\to\\\\naver-search-mcp\\\\dist\\\\src\\\\index.js\"\n      ],\n      \"cwd\": \"C:\\\\path\\\\to\\\\naver-search-mcp\",\n      \"env\": {\n        \"NAVER_CLIENT_ID\": \"your-naver-client-id\",\n        \"NAVER_CLIENT_SECRET\": \"your-naver-client-secret\"\n      }\n    }\n  }\n}\n```\n\n##### macOS/Linux Configuration\n\nAdd to Claude Desktop config file (`~/Library/Application Support/Claude/claude_desktop_config.json`):\n\n```json\n{\n  \"mcpServers\": {\n    \"naver-search\": {\n      \"type\": \"stdio\",\n      \"command\": \"node\",\n      \"args\": [\"/path/to/naver-search-mcp/dist/src/index.js\"],\n      \"cwd\": \"/path/to/naver-search-mcp\",\n      \"env\": {\n        \"NAVER_CLIENT_ID\": \"your-naver-client-id\",\n        \"NAVER_CLIENT_SECRET\": \"your-naver-client-secret\"\n      }\n    }\n  }\n}\n```\n\n##### Path Configuration Important Notes\n\n⚠️ **Important**: You must change the following paths in the above configuration to your actual installation paths:\n\n- **Windows**: Change `C:\\\\path\\\\to\\\\naver-search-mcp` to your actual downloaded folder path\n- **macOS/Linux**: Change `/path/to/naver-search-mcp` to your actual downloaded folder path\n- **Build Path**: Make sure the path points to `dist/src/index.js` (not just `index.js`)\n\nFinding your path:\n\n```bash\n# Check current location\npwd\n\n# Absolute path examples\n# Windows: C:\\Users\\username\\Downloads\\naver-search-mcp\n# macOS: /Users/username/Downloads/naver-search-mcp\n# Linux: /home/username/Downloads/naver-search-mcp\n```\n\n#### Step 3: Restart Claude Desktop\n\nAfter completing the configuration, completely close and restart Claude Desktop to activate the Naver Search MCP server.\n\n---\n\n## Alternative Installation Methods\n\n### Method 4: Docker Installation\n\nFor containerized deployment:\n\n```bash\ndocker run -i --rm \\\n  -e NAVER_CLIENT_ID=your_client_id \\\n  -e NAVER_CLIENT_SECRET=your_client_secret \\\n  mcp/naver-search\n```\n\nDocker configuration for Claude Desktop:\n\n```json\n{\n  \"mcpServers\": {\n    \"naver-search\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"-i\",\n        \"--rm\",\n        \"-e\",\n        \"NAVER_CLIENT_ID=your_client_id\",\n        \"-e\",\n        \"NAVER_CLIENT_SECRET=your_client_secret\",\n        \"mcp/naver-search\"\n      ]\n    }\n  }\n}\n```\n\n## Build\n\nDocker build:\n\n```bash\ndocker build -t mcp/naver-search .\n```\n\n## License\n\nMIT License\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "naver",
        "search",
        "mcp",
        "naver search",
        "search mcp",
        "server naver"
      ],
      "category": "search--data-extraction"
    },
    "jae-jae--fetcher-mcp": {
      "owner": "jae-jae",
      "name": "fetcher-mcp",
      "url": "https://github.com/jae-jae/fetcher-mcp",
      "imageUrl": "",
      "description": "MCP server for fetching web page content using Playwright headless browser, supporting Javascript rendering and intelligent content extraction, and outputting Markdown or HTML format.",
      "stars": 879,
      "forks": 71,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-10-03T22:32:23Z",
      "readme_content": "<div align=\"center\">\n  <img src=\"https://raw.githubusercontent.com/jae-jae/fetcher-mcp/refs/heads/main/icon.svg\" width=\"100\" height=\"100\" alt=\"Fetcher MCP Icon\" />\n</div>\n\n[中文](https://www.readme-i18n.com/jae-jae/fetcher-mcp?lang=zh) |\n[Deutsch](https://www.readme-i18n.com/jae-jae/fetcher-mcp?lang=de) |\n[Español](https://www.readme-i18n.com/jae-jae/fetcher-mcp?lang=es) |\n[français](https://www.readme-i18n.com/jae-jae/fetcher-mcp?lang=fr) |\n[日本語](https://www.readme-i18n.com/jae-jae/fetcher-mcp?lang=ja) |\n[한국어](https://www.readme-i18n.com/jae-jae/fetcher-mcp?lang=ko) |\n[Português](https://www.readme-i18n.com/jae-jae/fetcher-mcp?lang=pt) |\n[Русский](https://www.readme-i18n.com/jae-jae/fetcher-mcp?lang=ru)\n\n# Fetcher MCP\n\nMCP server for fetch web page content using Playwright headless browser.\n\n> 🌟 **Recommended**: [OllaMan](https://ollaman.com/) - Powerful Ollama AI Model Manager.\n\n## Advantages\n\n- **JavaScript Support**: Unlike traditional web scrapers, Fetcher MCP uses Playwright to execute JavaScript, making it capable of handling dynamic web content and modern web applications.\n\n- **Intelligent Content Extraction**: Built-in Readability algorithm automatically extracts the main content from web pages, removing ads, navigation, and other non-essential elements.\n\n- **Flexible Output Format**: Supports both HTML and Markdown output formats, making it easy to integrate with various downstream applications.\n\n- **Parallel Processing**: The `fetch_urls` tool enables concurrent fetching of multiple URLs, significantly improving efficiency for batch operations.\n\n- **Resource Optimization**: Automatically blocks unnecessary resources (images, stylesheets, fonts, media) to reduce bandwidth usage and improve performance.\n\n- **Robust Error Handling**: Comprehensive error handling and logging ensure reliable operation even when dealing with problematic web pages.\n\n- **Configurable Parameters**: Fine-grained control over timeouts, content extraction, and output formatting to suit different use cases.\n\n## Quick Start\n\nRun directly with npx:\n\n```bash\nnpx -y fetcher-mcp\n```\n\nFirst time setup - install the required browser by running the following command in your terminal:\n\n```bash\nnpx playwright install chromium\n```\n\n### HTTP and SSE Transport\n\nUse the `--transport=http` parameter to start both Streamable HTTP endpoint and SSE endpoint services simultaneously:\n\n```bash\nnpx -y fetcher-mcp --log --transport=http --host=0.0.0.0 --port=3000\n```\n\nAfter startup, the server provides the following endpoints:\n\n- `/mcp` - Streamable HTTP endpoint (modern MCP protocol)\n- `/sse` - SSE endpoint (legacy MCP protocol)\n\nClients can choose which method to connect based on their needs.\n\n### Debug Mode\n\nRun with the `--debug` option to show the browser window for debugging:\n\n```bash\nnpx -y fetcher-mcp --debug\n```\n\n## Configuration MCP\n\nConfigure this MCP server in Claude Desktop:\n\nOn MacOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\n\nOn Windows: `%APPDATA%/Claude/claude_desktop_config.json`\n\n```json\n{\n  \"mcpServers\": {\n    \"fetcher\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"fetcher-mcp\"]\n    }\n  }\n}\n```\n\n## Docker Deployment\n\n### Running with Docker\n\n```bash\ndocker run -p 3000:3000 ghcr.io/jae-jae/fetcher-mcp:latest\n```\n\n### Deploying with Docker Compose\n\nCreate a `docker-compose.yml` file:\n\n```yaml\nversion: \"3.8\"\n\nservices:\n  fetcher-mcp:\n    image: ghcr.io/jae-jae/fetcher-mcp:latest\n    container_name: fetcher-mcp\n    restart: unless-stopped\n    ports:\n      - \"3000:3000\"\n    environment:\n      - NODE_ENV=production\n    # Using host network mode on Linux hosts can improve browser access efficiency\n    # network_mode: \"host\"\n    volumes:\n      # For Playwright, may need to share certain system paths\n      - /tmp:/tmp\n    # Health check\n    healthcheck:\n      test: [\"CMD\", \"wget\", \"--spider\", \"-q\", \"http://localhost:3000\"]\n      interval: 30s\n      timeout: 10s\n      retries: 3\n```\n\nThen run:\n\n```bash\ndocker-compose up -d\n```\n\n## Features\n\n- `fetch_url` - Retrieve web page content from a specified URL\n\n  - Uses Playwright headless browser to parse JavaScript\n  - Supports intelligent extraction of main content and conversion to Markdown\n  - Supports the following parameters:\n    - `url`: The URL of the web page to fetch (required parameter)\n    - `timeout`: Page loading timeout in milliseconds, default is 30000 (30 seconds)\n    - `waitUntil`: Specifies when navigation is considered complete, options: 'load', 'domcontentloaded', 'networkidle', 'commit', default is 'load'\n    - `extractContent`: Whether to intelligently extract the main content, default is true\n    - `maxLength`: Maximum length of returned content (in characters), default is no limit\n    - `returnHtml`: Whether to return HTML content instead of Markdown, default is false\n    - `waitForNavigation`: Whether to wait for additional navigation after initial page load (useful for sites with anti-bot verification), default is false\n    - `navigationTimeout`: Maximum time to wait for additional navigation in milliseconds, default is 10000 (10 seconds)\n    - `disableMedia`: Whether to disable media resources (images, stylesheets, fonts, media), default is true\n    - `debug`: Whether to enable debug mode (showing browser window), overrides the --debug command line flag if specified\n\n- `fetch_urls` - Batch retrieve web page content from multiple URLs in parallel\n  - Uses multi-tab parallel fetching for improved performance\n  - Returns combined results with clear separation between webpages\n  - Supports the following parameters:\n    - `urls`: Array of URLs to fetch (required parameter)\n    - Other parameters are the same as `fetch_url`\n\n- `browser_install` - Install Playwright Chromium browser binary automatically\n\n  - Installs required Chromium browser binary when not available\n  - Automatically suggested when browser installation errors occur\n  - Supports the following parameters:\n    - `withDeps`: Install system dependencies required by Chromium browser, default is false\n    - `force`: Force installation even if Chromium is already installed, default is false\n\n## Tips\n\n### Handling Special Website Scenarios\n\n#### Dealing with Anti-Crawler Mechanisms\n\n- **Wait for Complete Loading**: For websites using CAPTCHA, redirects, or other verification mechanisms, include in your prompt:\n\n  ```\n  Please wait for the page to fully load\n  ```\n\n  This will use the `waitForNavigation: true` parameter.\n\n- **Increase Timeout Duration**: For websites that load slowly:\n  ```\n  Please set the page loading timeout to 60 seconds\n  ```\n  This adjusts both `timeout` and `navigationTimeout` parameters accordingly.\n\n#### Content Retrieval Adjustments\n\n- **Preserve Original HTML Structure**: When content extraction might fail:\n\n  ```\n  Please preserve the original HTML content\n  ```\n\n  Sets `extractContent: false` and `returnHtml: true`.\n\n- **Fetch Complete Page Content**: When extracted content is too limited:\n\n  ```\n  Please fetch the complete webpage content instead of just the main content\n  ```\n\n  Sets `extractContent: false`.\n\n- **Return Content as HTML**: When HTML format is needed instead of default Markdown:\n  ```\n  Please return the content in HTML format\n  ```\n  Sets `returnHtml: true`.\n\n### Debugging and Authentication\n\n#### Enabling Debug Mode\n\n- **Dynamic Debug Activation**: To display the browser window during a specific fetch operation:\n  ```\n  Please enable debug mode for this fetch operation\n  ```\n  This sets `debug: true` even if the server was started without the `--debug` flag.\n\n#### Using Custom Cookies for Authentication\n\n- **Manual Login**: To login using your own credentials:\n\n  ```\n  Please run in debug mode so I can manually log in to the website\n  ```\n\n  Sets `debug: true` or uses the `--debug` flag, keeping the browser window open for manual login.\n\n- **Interacting with Debug Browser**: When debug mode is enabled:\n\n  1. The browser window remains open\n  2. You can manually log into the website using your credentials\n  3. After login is complete, content will be fetched with your authenticated session\n\n- **Enable Debug for Specific Requests**: Even if the server is already running, you can enable debug mode for a specific request:\n  ```\n  Please enable debug mode for this authentication step\n  ```\n  Sets `debug: true` for this specific request only, opening the browser window for manual login.\n\n## Development\n\n### Install Dependencies\n\n```bash\nnpm install\n```\n\n### Install Playwright Browser\n\nInstall the browsers needed for Playwright:\n\n```bash\nnpm run install-browser\n```\n\n### Build the Server\n\n```bash\nnpm run build\n```\n\n## Debugging\n\nUse MCP Inspector for debugging:\n\n```bash\nnpm run inspector\n```\n\nYou can also enable visible browser mode for debugging:\n\n```bash\nnode build/index.js --debug\n```\n\n## Related Projects\n\n- [g-search-mcp](https://github.com/jae-jae/g-search-mcp): A powerful MCP server for Google search that enables parallel searching with multiple keywords simultaneously. Perfect for batch search operations and data collection.\n\n## License\n\nLicensed under the [MIT License](https://choosealicense.com/licenses/mit/)\n\n[![Powered by DartNode](https://dartnode.com/branding/DN-Open-Source-sm.png)](https://dartnode.com \"Powered by DartNode - Free VPS for Open Source\")\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "html",
        "fetching",
        "fetcher",
        "fetcher mcp",
        "mcp server",
        "fetching web"
      ],
      "category": "search--data-extraction"
    },
    "jae-jae--g-search-mcp": {
      "owner": "jae-jae",
      "name": "g-search-mcp",
      "url": "https://github.com/jae-jae/g-search-mcp",
      "imageUrl": "",
      "description": "A powerful MCP server for Google search that enables parallel searching with multiple keywords simultaneously.",
      "stars": 210,
      "forks": 24,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-10-02T14:10:50Z",
      "readme_content": "<div align=\"center\">\n  <img src=\"https://github.com/jae-jae/g-search-mcp/raw/main/icon.svg\" width=\"120\" height=\"120\" alt=\"g-search-mcp Logo\" />\n</div>\n\n# G-Search MCP\n\nA powerful MCP server for Google search that enables parallel searching with multiple keywords simultaneously.\n\n> This project is modified from [google-search](https://github.com/web-agent-master/google-search).\n\n> 🌟 **Recommended**: [OllaMan](https://ollaman.com/) - Powerful Ollama AI Model Manager.\n\n## Advantages\n\n- **Parallel Searching**: Supports searching with multiple keywords on Google simultaneously, improving search efficiency\n- **Browser Optimization**: Opens multiple tabs in a single browser instance for efficient parallel searching\n- **Automatic Verification Handling**: Intelligently detects CAPTCHA and enables visible browser mode for user verification when needed\n- **User Behavior Simulation**: Simulates real user browsing patterns to reduce the possibility of detection by search engines\n- **Structured Data**: Returns structured search results in JSON format for easy processing and analysis\n- **Configurable Parameters**: Supports various parameter configurations such as search result limits, timeout settings, locale settings, etc.\n\n## Quick Start\n\nRun directly with npx:\n\n```bash\nnpx -y g-search-mcp\n```\n\nFirst time setup - install the required browser by running the following command in your terminal:\n\n```bash\nnpx playwright install chromium\n```\n\n### Debug Mode\n\nUse the `--debug` option to run in debug mode (showing browser window):\n\n```bash\nnpx -y g-search-mcp --debug\n```\n\n## Configure MCP\n\nConfigure this MCP server in Claude Desktop:\n\nMacOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\nWindows: `%APPDATA%/Claude/claude_desktop_config.json`\n\n```json\n{\n  \"mcpServers\": {\n    \"g-search\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"g-search-mcp\"]\n    }\n  }\n}\n```\n\n## Features\n\n- `search` - Execute Google searches with multiple keywords and return results\n  - Uses Playwright browser to perform searches\n  - Supports the following parameters:\n    - `queries`: Array of search queries to execute (required parameter)\n    - `limit`: Maximum number of results to return per query, default is 10\n    - `timeout`: Page loading timeout in milliseconds, default is 60000 (60 seconds)\n    - `noSaveState`: Whether to avoid saving browser state, default is false\n    - `locale`: Locale setting for search results, default is en-US\n    - `debug`: Whether to enable debug mode (showing browser window), overrides the --debug flag in command line\n\n**Example usage**:\n\n```\nUse the search tool to search for \"machine learning\" and \"artificial intelligence\" on Google\n```\n\n**Example response**:\n\n```json\n{\n  \"searches\": [\n    {\n      \"query\": \"machine learning\",\n      \"results\": [\n        {\n          \"title\": \"What is Machine Learning? | IBM\",\n          \"link\": \"https://www.ibm.com/topics/machine-learning\",\n          \"snippet\": \"Machine learning is a branch of artificial intelligence (AI) and computer science which focuses on the use of data and algorithms to imitate the way that humans learn, gradually improving its accuracy.\"\n        },\n        ...\n      ]\n    },\n    {\n      \"query\": \"artificial intelligence\",\n      \"results\": [\n        {\n          \"title\": \"What is Artificial Intelligence (AI)? | IBM\",\n          \"link\": \"https://www.ibm.com/topics/artificial-intelligence\",\n          \"snippet\": \"Artificial intelligence leverages computers and machines to mimic the problem-solving and decision-making capabilities of the human mind.\"\n        },\n        ...\n      ]\n    }\n  ]\n}\n```\n\n## Usage Tips\n\n### Handling Special Website Scenarios\n\n#### Adjusting Search Parameters\n\n- **Search Result Quantity**: For more search results:\n\n  ```\n  Please return the top 20 search results for each keyword\n  ```\n\n  This will set the `limit: 20` parameter.\n\n- **Increase Timeout Duration**: For slow loading situations:\n  ```\n  Please set the page loading timeout to 120 seconds\n  ```\n  This will adjust the `timeout` parameter to 120000 milliseconds.\n\n#### Locale Settings Adjustment\n\n- **Change Search Region**: Specify a different locale setting:\n  ```\n  Please use Chinese locale (zh-CN) for searching\n  ```\n  This will set the `locale: \"zh-CN\"` parameter.\n\n### Debugging and Troubleshooting\n\n#### Enable Debug Mode\n\n- **Dynamic Debug Activation**: To display the browser window during a specific search operation:\n  ```\n  Please enable debug mode for this search operation\n  ```\n  This sets `debug: true` even if the server was started without the `--debug` flag.\n\n## Installation\n\n### Prerequisites\n\n- Node.js 18 or higher\n- NPM or Yarn\n\n### Install from Source\n\n1. Clone the repository:\n\n```bash\ngit clone https://github.com/jae-jae/g-search-mcp.git\ncd g-search-mcp\n```\n\n2. Install dependencies:\n\n```bash\nnpm install\n```\n\n3. Install Playwright browser:\n\n```bash\nnpm run install-browser\n```\n\n4. Build the server:\n\n```bash\nnpm run build\n```\n\n## Development\n\n### Auto Rebuild (Development Mode)\n\n```bash\nnpm run watch\n```\n\n### Using MCP Inspector for Debugging\n\n```bash\nnpm run inspector\n```\n\n## Related Projects\n\n- [fetcher-mcp](https://github.com/jae-jae/fetcher-mcp): A powerful MCP server for fetching web page content using Playwright headless browser. Features intelligent content extraction, parallel processing, resource optimization, and more, making it an ideal tool for web content scraping.\n\n## License\n\nLicensed under the [MIT License](https://choosealicense.com/licenses/mit/)\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "searching",
        "search",
        "mcp",
        "search mcp",
        "parallel searching",
        "searching multiple"
      ],
      "category": "search--data-extraction"
    },
    "joelio--stocky": {
      "owner": "joelio",
      "name": "stocky",
      "url": "https://github.com/joelio/stocky",
      "imageUrl": "",
      "description": "An MCP server for searching and downloading royalty-free stock photography from Pexels and Unsplash. Features multi-provider search, rich metadata, pagination support, and async performance for AI assistants to find and access high-quality images.",
      "stars": 11,
      "forks": 6,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-23T11:42:37Z",
      "readme_content": "# <div align=\"center\"><br/>Stocky<br/>*Find beautiful royalty-free stock images* 📸</div>\n\n<div align=\"center\">\n\n[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)\n[![MCP Compatible](https://img.shields.io/badge/MCP-Compatible-green.svg)](https://github.com/modelcontextprotocol)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n\n</div>\n\n## ✨ Features\n\n- 🔍 **Multi-Provider Search** - Search across Pexels and Unsplash simultaneously\n- 📊 **Rich Metadata** - Get comprehensive image details including dimensions, photographer info, and licensing\n- 📄 **Pagination Support** - Browse through large result sets with ease\n- 🛡️ **Graceful Error Handling** - Robust error handling for API failures\n- ⚡ **Async Performance** - Lightning-fast concurrent API calls\n- 🎯 **Provider Flexibility** - Search specific providers or all at once\n\n\n\n**Beautiful stock photography at your fingertips**  \nExample image used for demonstration purposes\n\n\n*Stunning landscapes available through multiple providers*\n\nPhoto by [Simon Berger](https://unsplash.com/@simon_berger) on [Unsplash](https://unsplash.com/photos/twukN12EN7c)\n\n## 🚀 Quick Start\n\n### Installation\n\n1. Clone the repository:\n```bash\ngit clone https://github.com/yourusername/stocky-mcp.git\ncd stocky-mcp\n```\n\n2. Install dependencies:\n```bash\npip install -r requirements.txt\n```\n\n### API Key Setup\n\nYou'll need free API keys from each provider:\n\n1. **Pexels** 📷 - Get your key at [pexels.com/api](https://www.pexels.com/api/)\n2. **Unsplash** 🌅 - Sign up at [unsplash.com/developers](https://unsplash.com/developers)\n\n\n### API Key Configuration\n\nYou'll need to configure your API keys when setting up the MCP server. These keys are used to authenticate with the stock image providers.\n\n### Running as an MCP Server\n\nStocky is designed to be run as an MCP (Model Context Protocol) server, not as a standalone application. It should be configured in your MCP client configuration.\n\n## 🔧 MCP Client Configuration\n\nAdd Stocky to your MCP client configuration:\n\n```json\n{\n  \"mcpServers\": {\n    \"stocky\": {\n      \"command\": \"python\",\n      \"args\": [\"/path/to/stocky_mcp.py\"],\n      \"env\": {\n        \"PEXELS_API_KEY\": \"your_pexels_key\",\n        \"UNSPLASH_ACCESS_KEY\": \"your_unsplash_key\",\n\n      }\n    }\n  }\n}\n```\n\n## 📖 Usage Examples\n\n<div align=\"center\">\n\n<p><em>Find the perfect image for your project</em></p>\n</div>\n\n### Searching for Images\n\nSearch across all providers:\n```python\nresults = await search_stock_images(\"sunset beach\")\n```\n\nSearch specific providers:\n```python\nresults = await search_stock_images(\n    query=\"mountain landscape\",\n    providers=[\"pexels\", \"unsplash\"],\n    per_page=30,\n    page=1\n)\n```\n\n### Getting Image Details\n\n```python\ndetails = await get_image_details(\"unsplash_abc123xyz\")\n```\n\n### Downloading Images\n\n```python\n# Download and save to disk\nresult = await download_image(\n    image_id=\"pexels_123456\", \n    size=\"medium\", \n    output_path=\"/path/to/save.jpg\"\n)\n\n# Get base64-encoded image data\nresult = await download_image(\n    image_id=\"unsplash_abc123\", \n    size=\"original\"\n)\n```\n\n## 🛠️ Tools Documentation\n\n### `search_stock_images`\n\nSearch for royalty-free stock images across multiple providers.\n\n**Parameters:**\n- `query` (str, required) - Search terms for finding images\n- `providers` (list, optional) - List of providers to search: `[\"pexels\", \"unsplash\"]`\n- `per_page` (int, optional) - Results per page, max 50 (default: 20)\n- `page` (int, optional) - Page number for pagination (default: 1)\n- `sort_by` (str, optional) - Sort results by \"relevance\" or \"newest\"\n\n**Returns:** List of image results with metadata\n\n### `get_image_details`\n\nGet detailed information about a specific image.\n\n**Parameters:**\n- `image_id` (str, required) - Image ID in format `provider_id` (e.g., `pexels_123456`)\n\n**Returns:** Detailed image information including full metadata\n\n### `download_image`\n\nDownload an image to local storage or get base64 encoded data.\n\n**Parameters:**\n- `image_id` (str, required) - Image ID in format `provider_id` (e.g., `pexels_123456`)\n- `size` (str, optional) - Image size variant to download (default: \"original\")\n  - Options: thumbnail, small, medium, large, original\n- `output_path` (str, optional) - Path to save the image locally\n  - If not provided, returns base64 encoded image data\n\n**Returns:** Dictionary with download information or error\n\n## 📄 License Information\n\n<div align=\"center\">\n\n<p><em>Royalty-free images for your creative projects</em></p>\n</div>\n\nAll images returned by Stocky are free to use:\n\n- **Pexels** ✅ - Free for commercial and personal use, no attribution required\n- **Unsplash** ✅ - Free under the Unsplash License\n\n\nAlways check the specific license for each image before use in production.\n\n## 🤝 Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request. For major changes, please open an issue first to discuss what you would like to change.\n\n1. Fork the Project\n2. Create your Feature Branch (`git checkout -b feature/AmazingFeature`)\n3. Commit your Changes (`git commit -m 'Add some AmazingFeature'`)\n4. Push to the Branch (`git push origin feature/AmazingFeature`)\n5. Open a Pull Request\n\n## 🙏 Acknowledgments\n\n- Thanks to [Pexels](https://www.pexels.com) and [Unsplash](https://unsplash.com) for providing free APIs\n- Built with the [Model Context Protocol](https://github.com/modelcontextprotocol)\n- Created with ❤️ for the developer community\n\n## 🐛 Troubleshooting\n\n### Common Issues\n\n**\"API key not found\" error**\n- Ensure your `.env` file exists and contains valid API keys\n- Check that environment variables are properly loaded\n- Verify API key names match exactly (case-sensitive)\n\n**No results returned**\n- Try different search terms\n- Check your internet connection\n- Verify API keys are active and have not exceeded rate limits\n\n**Installation issues**\n- Ensure Python 3.8+ is installed\n- Try creating a virtual environment: `python -m venv venv`\n- Update pip: `pip install --upgrade pip`\n\n### Rate Limiting\n\nEach provider has different rate limits:\n- **Pexels**: 200 requests per hour\n- **Unsplash**: 50 requests per hour (demo), 5000 per hour (production)\n\n\n---\n\n<div align=\"center\">\nMade with 💜 by the Stocky Team\n</div>",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "pexels",
        "searching",
        "search",
        "stocky mcp",
        "stock photography",
        "search data"
      ],
      "category": "search--data-extraction"
    },
    "just-every--mcp-read-website-fast": {
      "owner": "just-every",
      "name": "mcp-read-website-fast",
      "url": "https://github.com/just-every/mcp-read-website-fast",
      "imageUrl": "",
      "description": "Fast, token-efficient web content extraction for AI agents - converts websites to clean Markdown while preserving links. Features Mozilla Readability, smart caching, polite crawling with robots.txt support, and concurrent fetching.",
      "stars": 93,
      "forks": 14,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-10-02T06:41:09Z",
      "readme_content": "# @just-every/mcp-read-website-fast\n\nFast, token-efficient web content extraction for AI agents - converts websites to clean Markdown.\n\n[![npm version](https://badge.fury.io/js/@just-every%2Fmcp-read-website-fast.svg)](https://www.npmjs.com/package/@just-every/mcp-read-website-fast)\n[![GitHub Actions](https://github.com/just-every/mcp-read-website-fast/workflows/Release/badge.svg)](https://github.com/just-every/mcp-read-website-fast/actions)\n\n<a href=\"https://glama.ai/mcp/servers/@just-every/mcp-read-website-fast\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@just-every/mcp-read-website-fast/badge\" alt=\"read-website-fast MCP server\" />\n</a>\n\n## Overview\n\nExisting MCP web crawlers are slow and consume large quantities of tokens. This pauses the development process and provides incomplete results as LLMs need to parse whole web pages.\n\nThis MCP package fetches web pages locally, strips noise, and converts content to clean Markdown while preserving links. Designed for Claude Code, IDEs and LLM pipelines with minimal token footprint. Crawl sites locally with minimal dependencies.\n\n**Note:** This package now uses [@just-every/crawl](https://www.npmjs.com/package/@just-every/crawl) for its core crawling and markdown conversion functionality.\n\n## Features\n\n- **Fast startup** using official MCP SDK with lazy loading for optimal performance\n- **Content extraction** using Mozilla Readability (same as Firefox Reader View)\n- **HTML to Markdown** conversion with Turndown + GFM support\n- **Smart caching** with SHA-256 hashed URLs\n- **Polite crawling** with robots.txt support and rate limiting\n- **Concurrent fetching** with configurable depth crawling\n- **Stream-first design** for low memory usage\n- **Link preservation** for knowledge graphs\n- **Optional chunking** for downstream processing\n\n## Installation\n\n### Claude Code\n\n```bash\nclaude mcp add read-website-fast -s user -- npx -y @just-every/mcp-read-website-fast\n```\n\n### VS Code\n\n```bash\ncode --add-mcp '{\"name\":\"read-website-fast\",\"command\":\"npx\",\"args\":[\"-y\",\"@just-every/mcp-read-website-fast\"]}'\n```\n\n### Cursor\n\n```bash\ncursor://anysphere.cursor-deeplink/mcp/install?name=read-website-fast&config=eyJyZWFkLXdlYnNpdGUtZmFzdCI6eyJjb21tYW5kIjoibnB4IiwiYXJncyI6WyIteSIsIkBqdXN0LWV2ZXJ5L21jcC1yZWFkLXdlYnNpdGUtZmFzdCJdfX0=\n```\n\n### JetBrains IDEs\n\nSettings → Tools → AI Assistant → Model Context Protocol (MCP) → Add\n\nChoose “As JSON” and paste:\n\n```json\n{\"command\":\"npx\",\"args\":[\"-y\",\"@just-every/mcp-read-website-fast\"]}\n```\n\nOr, in the chat window, type /add and fill in the same JSON—both paths land the server in a single step. ￼\n\n### Raw JSON (works in any MCP client)\n\n```json\n{\n  \"mcpServers\": {\n    \"read-website-fast\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@just-every/mcp-read-website-fast\"]\n    }\n  }\n}\n```\n\nDrop this into your client’s mcp.json (e.g. .vscode/mcp.json, ~/.cursor/mcp.json, or .mcp.json for Claude).\n\n\n\n## Features\n\n- **Fast startup** using official MCP SDK with lazy loading for optimal performance\n- **Content extraction** using Mozilla Readability (same as Firefox Reader View)\n- **HTML to Markdown** conversion with Turndown + GFM support\n- **Smart caching** with SHA-256 hashed URLs\n- **Polite crawling** with robots.txt support and rate limiting\n- **Concurrent fetching** with configurable depth crawling\n- **Stream-first design** for low memory usage\n- **Link preservation** for knowledge graphs\n- **Optional chunking** for downstream processing\n\n### Available Tools\n\n- `read_website` - Fetches a webpage and converts it to clean markdown\n  - Parameters:\n    - `url` (required): The HTTP/HTTPS URL to fetch\n    - `pages` (optional): Maximum number of pages to crawl (default: 1, max: 100)\n\n### Available Resources\n\n- `read-website-fast://status` - Get cache statistics\n- `read-website-fast://clear-cache` - Clear the cache directory\n\n## Development Usage\n\n### Install\n\n```bash\nnpm install\nnpm run build\n```\n\n### Single page fetch\n```bash\nnpm run dev fetch https://example.com/article\n```\n\n### Crawl with depth\n```bash\nnpm run dev fetch https://example.com --depth 2 --concurrency 5\n```\n\n### Output formats\n```bash\n# Markdown only (default)\nnpm run dev fetch https://example.com\n\n# JSON output with metadata\nnpm run dev fetch https://example.com --output json\n\n# Both URL and markdown\nnpm run dev fetch https://example.com --output both\n```\n\n### CLI Options\n\n- `-p, --pages <number>` - Maximum number of pages to crawl (default: 1)\n- `-c, --concurrency <number>` - Max concurrent requests (default: 3)\n- `--no-robots` - Ignore robots.txt\n- `--all-origins` - Allow cross-origin crawling\n- `-u, --user-agent <string>` - Custom user agent\n- `--cache-dir <path>` - Cache directory (default: .cache)\n- `-t, --timeout <ms>` - Request timeout in milliseconds (default: 30000)\n- `-o, --output <format>` - Output format: json, markdown, or both (default: markdown)\n\n### Clear cache\n```bash\nnpm run dev clear-cache\n```\n\n## Auto-Restart Feature\n\nThe MCP server includes automatic restart capability by default for improved reliability:\n\n- Automatically restarts the server if it crashes\n- Handles unhandled exceptions and promise rejections\n- Implements exponential backoff (max 10 attempts in 1 minute)\n- Logs all restart attempts for monitoring\n- Gracefully handles shutdown signals (SIGINT, SIGTERM)\n\nFor development/debugging without auto-restart:\n```bash\n# Run directly without restart wrapper\nnpm run serve:dev\n```\n\n## Architecture\n\n```\nmcp/\n├── src/\n│   ├── crawler/        # URL fetching, queue management, robots.txt\n│   ├── parser/         # DOM parsing, Readability, Turndown conversion\n│   ├── cache/          # Disk-based caching with SHA-256 keys\n│   ├── utils/          # Logger, chunker utilities\n│   ├── index.ts        # CLI entry point\n│   ├── serve.ts        # MCP server entry point\n│   └── serve-restart.ts # Auto-restart wrapper\n```\n\n## Development\n\n```bash\n# Run in development mode\nnpm run dev fetch https://example.com\n\n# Build for production\nnpm run build\n\n# Run tests\nnpm test\n\n# Type checking\nnpm run typecheck\n\n# Linting\nnpm run lint\n```\n\n## Contributing\n\nContributions are welcome! Please:\n\n1. Fork the repository\n2. Create a feature branch\n3. Add tests for new functionality\n4. Submit a pull request\n\n## Troubleshooting\n\n### Cache Issues\n```bash\nnpm run dev clear-cache\n```\n\n### Timeout Errors\n- Increase timeout with `-t` flag\n- Check network connectivity\n- Verify URL is accessible\n\n### Content Not Extracted\n- Some sites block automated access\n- Try custom user agent with `-u` flag\n- Check if site requires JavaScript (not supported)\n\n## License\n\nMIT",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "mozilla",
        "web",
        "search",
        "efficient web",
        "content extraction",
        "web content"
      ],
      "category": "search--data-extraction"
    },
    "just-every--mcp-screenshot-website-fast": {
      "owner": "just-every",
      "name": "mcp-screenshot-website-fast",
      "url": "https://github.com/just-every/mcp-screenshot-website-fast",
      "imageUrl": "",
      "description": "Fast screenshot capture tool optimized for Claude Vision API. Automatically tiles full pages into 1072x1072 chunks for optimal AI processing with configurable viewports and wait strategies for dynamic content.",
      "stars": 81,
      "forks": 8,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-09-17T10:42:32Z",
      "readme_content": "# @just-every/mcp-screenshot-website-fast\n\nFast, efficient screenshot capture of web pages - optimized for CLI coding tools. Automatically tiles full pages into 1072x1072 chunks for optimal processing.\n\n<a href=\"https://glama.ai/mcp/servers/@just-every/mcp-screenshot-website-fast\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@just-every/mcp-screenshot-website-fast/badge\" alt=\"Screenshot Website Fast MCP server\" />\n</a>\n\n[![npm version](https://badge.fury.io/js/@just-every%2Fmcp-screenshot-website-fast.svg)](https://www.npmjs.com/package/@just-every/mcp-screenshot-website-fast)\n[![GitHub Actions](https://github.com/just-every/mcp-screenshot-website-fast/workflows/Release/badge.svg)](https://github.com/just-every/mcp-screenshot-website-fast/actions)\n\n## Overview\n\nBuilt specifically for AI vision workflows, this tool captures high-quality screenshots with automatic resolution limiting and tiling for optimal processing by Claude Vision API and other AI models. It ensures screenshots are perfectly sized at 1072x1072 pixels (1.15 megapixels) for maximum compatibility.\n\n## Features\n\n- 📸 **Fast screenshot capture** using Puppeteer headless browser\n- 🎯 **Claude Vision optimized** with automatic resolution limiting (1072x1072 for optimal 1.15 megapixels)\n- 🔲 **Automatic tiling** - Full pages are automatically split into 1072x1072 tiles\n- 🎬 **Screencast capture** - Record series of screenshots over time with configurable intervals\n- 🔄 **Always fresh content** - No caching ensures up-to-date screenshots\n- 📱 **Configurable viewports** for responsive testing\n- ⏱️ **Wait strategies** for dynamic content (networkidle, custom delays)\n- 📄 **Full page capture** by default for complete page screenshots\n- 🎥 **Animated WebP export** - Save screencasts as high-quality animated WebP files\n- 💉 **JavaScript injection** - Execute custom JS before screencast capture\n- 📦 **Minimal dependencies** for fast npm installs\n- 🔌 **MCP integration** for seamless AI workflows\n- 🔋 **Resource efficient** - Automatic browser cleanup after 60 seconds of inactivity\n- 🧹 **Memory management** - Pages are closed after each screenshot to prevent leaks\n\n## Installation\n\n### Claude Code\n\n```bash\nclaude mcp add screenshot-website-fast -s user -- npx -y @just-every/mcp-screenshot-website-fast\n```\n\n### VS Code\n\n```bash\ncode --add-mcp '{\"name\":\"screenshot-website-fast\",\"command\":\"npx\",\"args\":[\"-y\",\"@just-every/mcp-screenshot-website-fast\"]}'\n```\n\n### Cursor\n\n```bash\ncursor://anysphere.cursor-deeplink/mcp/install?name=screenshot-website-fast&config=eyJzY3JlZW5zaG90LXdlYnNpdGUtZmFzdCI6eyJjb21tYW5kIjoibnB4IiwiYXJncyI6WyIteSIsIkBqdXN0LWV2ZXJ5L21jcC1zY3JlZW5zaG90LXdlYnNpdGUtZmFzdCJdfX0=\n```\n\n### JetBrains IDEs\n\nSettings → Tools → AI Assistant → Model Context Protocol (MCP) → Add\n\nChoose \"As JSON\" and paste:\n\n```json\n{\"command\":\"npx\",\"args\":[\"-y\",\"@just-every/mcp-screenshot-website-fast\"]}\n```\n\n### Raw JSON (works in any MCP client)\n\n```json\n{\n  \"mcpServers\": {\n    \"screenshot-website-fast\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@just-every/mcp-screenshot-website-fast\"]\n    }\n  }\n}\n```\n\nDrop this into your client's mcp.json (e.g. .vscode/mcp.json, ~/.cursor/mcp.json, or .mcp.json for Claude).\n\n## Prerequisites\n\n- Node.js 20.x or higher\n- npm or npx\n- Chrome/Chromium (automatically downloaded by Puppeteer)\n\n## Quick Start\n\n### MCP Server Usage\n\nOnce installed in your IDE, the following tools are available:\n\n#### Available Tools\n\n- `take_screenshot` - Captures a high-quality screenshot of a webpage\n  - Parameters:\n    - `url` (required): The HTTP/HTTPS URL to capture\n    - `width` (optional): Viewport width in pixels (max 1072, default: 1072)\n    - `height` (optional): Viewport height in pixels (max 1072, default: 1072)\n    - `fullPage` (optional): Capture full page screenshot with tiling (default: true)\n    - `waitUntil` (optional): Wait until event: load, domcontentloaded, networkidle0, networkidle2 (default: domcontentloaded)\n    - `waitFor` (optional): Additional wait time in milliseconds\n    - `directory` (optional): Directory to save screenshots - returns file paths instead of base64 images\n\n#### Usage Examples\n\n**Default usage (returns base64 images):**\n```\ntake_screenshot(url=\"https://example.com\")\n```\n\n**Save to directory (returns file paths):**\n```\ntake_screenshot(url=\"https://example.com\", directory=\"/path/to/screenshots\")\n```\n\nWhen using the `directory` parameter:\n- Screenshots are saved as PNG files with timestamps\n- File paths are returned instead of base64 data\n- For tiled screenshots, each tile is saved as a separate file\n- Directory is created automatically if it doesn't exist\n\n### take_screencast\n\nCaptures a series of screenshots over time to create a screencast. Only captures the top tile (1072x1072) of the viewport.\n\n#### Parameters\n- `url` (required): The URL to capture\n- `duration` (optional): Total duration in seconds (default: 10)\n- `interval` (optional): Interval between screenshots in seconds (default: 2)\n- `jsEvaluate` (optional): JavaScript code to execute at the start\n- `waitUntil` (optional): Wait strategy: 'load', 'domcontentloaded', 'networkidle0', 'networkidle2'\n- `waitForMS` (optional): Additional wait time before starting\n- `directory` (optional): Save as animated WebP to directory (captures every 1 second)\n\n#### Usage Examples\n\n**Basic screencast (5 frames over 10 seconds):**\n```\ntake_screencast(url=\"https://example.com\")\n```\n\n**Custom timing:**\n```\ntake_screencast(url=\"https://example.com\", duration=15, interval=3)\n```\n\n**With JavaScript execution:**\n```\ntake_screencast(\n  url=\"https://example.com\",\n  jsEvaluate=\"document.body.style.backgroundColor = 'red';\"\n)\n```\n\n**Save as animated WebP:**\n```\ntake_screencast(url=\"https://example.com\", directory=\"/path/to/output\")\n```\n\nWhen using the `directory` parameter:\n- An animated WebP is created with 1-second intervals\n- Individual frames are also saved as PNG files\n- The animation loops forever by default\n- WebP provides excellent quality:\n  - Full color support (no 256 color limitation)\n  - Efficient compression for web animations\n  - Perfect for gradient backgrounds and smooth animations\n  - Smaller file sizes compared to GIF with better quality\n\n## Development Usage\n\n### Install\n\n```bash\nnpm install\nnpm run build\n```\n\n### Capture screenshot\n```bash\n# Full page with automatic tiling (default)\nnpm run dev capture https://example.com -o screenshot.png\n\n# Viewport-only screenshot  \nnpm run dev capture https://example.com --no-full-page -o screenshot.png\n\n# Wait for specific conditions\nnpm run dev capture https://example.com --wait-until networkidle0 --wait-for 2000 -o screenshot.png\n```\n\n### CLI Options\n\n- `-w, --width <pixels>` - Viewport width (max 1072, default: 1072)\n- `-h, --height <pixels>` - Viewport height (max 1072, default: 1072)\n- `--no-full-page` - Disable full page capture and tiling\n- `--wait-until <event>` - Wait until event: load, domcontentloaded, networkidle0, networkidle2\n- `--wait-for <ms>` - Additional wait time in milliseconds\n- `-o, --output <path>` - Output file path (required for tiled output)\n\n## Auto-Restart Feature\n\nThe MCP server includes automatic restart capability by default for improved reliability:\n\n- Automatically restarts the server if it crashes\n- Handles unhandled exceptions and promise rejections\n- Implements exponential backoff (max 10 attempts in 1 minute)\n- Logs all restart attempts for monitoring\n- Gracefully handles shutdown signals (SIGINT, SIGTERM)\n\nFor development/debugging without auto-restart:\n```bash\n# Run directly without restart wrapper\nnpm run serve:dev\n```\n\n## Architecture\n\n```\nmcp-screenshot-website-fast/\n├── src/\n│   ├── internal/       # Core screenshot capture logic\n│   ├── utils/          # Logger and utilities\n│   ├── index.ts        # CLI entry point\n│   ├── serve.ts        # MCP server entry point\n│   └── serve-restart.ts # Auto-restart wrapper\n```\n\n## Development\n\n```bash\n# Run in development mode\nnpm run dev capture https://example.com -o screenshot.png\n\n# Build for production\nnpm run build\n\n# Run tests\nnpm test\n\n# Type checking\nnpm run typecheck\n\n# Linting\nnpm run lint\n```\n\n## Why This Tool?\n\nBuilt specifically for AI vision workflows:\n\n1. **Optimized for Claude Vision API** - Automatic resolution limiting to 1072x1072 pixels (1.15 megapixels)\n2. **Automatic tiling** - Full pages split into perfect chunks for AI processing\n3. **Always fresh** - No caching ensures you get the latest content\n4. **MCP native** - First-class integration with AI development tools\n5. **Simple API** - Clean, straightforward interface for capturing screenshots\n\n## Contributing\n\nContributions are welcome! Please:\n\n1. Fork the repository\n2. Create a feature branch\n3. Add tests for new functionality\n4. Submit a pull request\n\n## Troubleshooting\n\n### Puppeteer Issues\n- Ensure Chrome/Chromium can be downloaded\n- Check firewall settings\n- Try setting `PUPPETEER_SKIP_CHROMIUM_DOWNLOAD=true` and provide custom executable\n\n### Screenshot Quality\n- Adjust viewport dimensions\n- Use appropriate wait strategies\n- Check if site requires authentication\n\n### Timeout Errors\n- Increase wait time with `--wait-for` flag\n- Use different `--wait-until` strategies\n- Check if site is accessible\n\n## License\n\nMIT",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "capture",
        "vision",
        "optimized",
        "fast screenshot",
        "screenshot capture",
        "mcp screenshot"
      ],
      "category": "search--data-extraction"
    },
    "kagisearch--kagimcp": {
      "owner": "kagisearch",
      "name": "kagimcp",
      "url": "https://github.com/kagisearch/kagimcp",
      "imageUrl": "",
      "description": "[kehvinbehvin/json-mcp-filter](https://github.com/kehvinbehvin/json-mcp-filter) ️🏠 📇 – Stop bloating your LLM context. Query & Extract only what you need from your JSON files.",
      "stars": 197,
      "forks": 21,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-10-03T22:32:01Z",
      "readme_content": "# Kagi MCP server\n\n[![smithery badge](https://smithery.ai/badge/kagimcp)](https://smithery.ai/server/kagimcp)\n\n<a href=\"https://glama.ai/mcp/servers/xabrrs4bka\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/xabrrs4bka/badge\" alt=\"Kagi Server MCP server\" />\n</a>\n\n## Setup Intructions\n> Before anything, unless you are just using non-search tools, ensure you have access to the search API. It is currently in closed beta and available upon request. Please reach out to support@kagi.com for an invite.\n\nInstall uv first.\n\nMacOS/Linux:\n```bash\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n```\n\nWindows:\n```\npowershell -ExecutionPolicy ByPass -c \"irm https://astral.sh/uv/install.ps1 | iex\"\n```\n### Installing via Smithery\n\nAlternatively, you can install Kagi for Claude Desktop via [Smithery](https://smithery.ai/server/kagimcp):\n\n```bash\nnpx -y @smithery/cli install kagimcp --client claude\n```\n\n### Setup with Claude\n#### Claude Desktop\n```json\n// claude_desktop_config.json\n// Can find location through:\n// Hamburger Menu -> File -> Settings -> Developer -> Edit Config\n{\n  \"mcpServers\": {\n    \"kagi\": {\n      \"command\": \"uvx\",\n      \"args\": [\"kagimcp\"],\n      \"env\": {\n        \"KAGI_API_KEY\": \"YOUR_API_KEY_HERE\",\n        \"KAGI_SUMMARIZER_ENGINE\": \"YOUR_ENGINE_CHOICE_HERE\" // Defaults to \"cecil\" engine if env var not present\n      }\n    }\n  }\n}\n```\n#### Claude Code\nAdd the Kagi mcp server with the following command (setting summarizer engine optional):\n\n```bash\nclaude mcp add kagi -e KAGI_API_KEY=\"YOUR_API_KEY_HERE\" KAGI_SUMMARIZER_ENGINE=\"YOUR_ENGINE_CHOICE_HERE\" -- uvx kagimcp\n```\n\nNow claude code can use the Kagi mcp server. However, claude code comes with its own web search functionality by default, which may conflict with Kagi. You can disable claude's web search functionality with the following in your claude code settings file (`~/.claude/settings.json`):\n\n```json\n{\n  \"permissions\": {\n    \"deny\": [\n      \"WebSearch\"\n    ]\n  }\n}\n```\n\n### Pose query that requires use of a tool\ne.g. \"Who was time's 2024 person of the year?\" for search, or \"summarize this video: https://www.youtube.com/watch?v=jNQXAC9IVRw\" for summarizer.\n\n### Debugging\nRun:\n```bash\nnpx @modelcontextprotocol/inspector uvx kagimcp\n```\n\n## Local/Dev Setup Instructions\n\n### Clone repo\n`git clone https://github.com/kagisearch/kagimcp.git`\n\n### Install dependencies\nInstall uv first.\n\nMacOS/Linux:\n```bash\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n```\n\nWindows:\n```\npowershell -ExecutionPolicy ByPass -c \"irm https://astral.sh/uv/install.ps1 | iex\"\n```\n\nThen install MCP server dependencies:\n```bash\ncd kagimcp\n\n# Create virtual environment and activate it\nuv venv\n\nsource .venv/bin/activate # MacOS/Linux\n# OR\n.venv/Scripts/activate # Windows\n\n# Install dependencies\nuv sync\n```\n### Setup with Claude Desktop\n\n#### Using MCP CLI SDK\n```bash\n# `pip install mcp[cli]` if you haven't\nmcp install /ABSOLUTE/PATH/TO/PARENT/FOLDER/kagimcp/src/kagimcp/server.py -v \"KAGI_API_KEY=API_KEY_HERE\"\n```\n\n#### Manually\n```json\n# claude_desktop_config.json\n# Can find location through:\n# Hamburger Menu -> File -> Settings -> Developer -> Edit Config\n{\n  \"mcpServers\": {\n    \"kagi\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"--directory\",\n        \"/ABSOLUTE/PATH/TO/PARENT/FOLDER/kagimcp\",\n        \"run\",\n        \"kagimcp\"\n      ],\n      \"env\": {\n        \"KAGI_API_KEY\": \"YOUR_API_KEY_HERE\",\n        \"KAGI_SUMMARIZER_ENGINE\": \"YOUR_ENGINE_CHOICE_HERE\" // Defaults to \"cecil\" engine if env var not present\n      }\n    }\n  }\n}\n```\n\n### Pose query that requires use of a tool\ne.g. \"Who was time's 2024 person of the year?\" for search, or \"summarize this video: https://www.youtube.com/watch?v=jNQXAC9IVRw\" for summarizer.\n\n### Debugging\nRun:\n```bash\n# If mcp cli installed (`pip install mcp[cli]`)\nmcp dev /ABSOLUTE/PATH/TO/PARENT/FOLDER/kagimcp/src/kagimcp/server.py\n\n# If not\nnpx @modelcontextprotocol/inspector \\\n      uv \\\n      --directory /ABSOLUTE/PATH/TO/PARENT/FOLDER/kagimcp \\\n      run \\\n      kagimcp\n```\nThen access MCP Inspector at `http://localhost:5173`. You may need to add your Kagi API key in the environment variables in the inspector under `KAGI_API_KEY`.\n\n# Advanced Configuration\n- Level of logging is adjustable through the `FASTMCP_LOG_LEVEL` environment variable (e.g. `FASTMCP_LOG_LEVEL=\"ERROR\"`)\n  - Relevant issue: https://github.com/kagisearch/kagimcp/issues/4\n- Summarizer engine can be customized using the `KAGI_SUMMARIZER_ENGINE` environment variable (e.g. `KAGI_SUMMARIZER_ENGINE=\"daphne\"`)\n  - Learn about the different summarization engines [here](https://help.kagi.com/kagi/api/summarizer.html#summarization-engines)\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "kagisearch",
        "kagimcp",
        "search",
        "extraction kagisearch",
        "json mcp",
        "kagisearch kagimcp"
      ],
      "category": "search--data-extraction"
    },
    "kimdonghwi94--Web-Analyzer-MCP": {
      "owner": "kimdonghwi94",
      "name": "Web-Analyzer-MCP",
      "url": "https://github.com/kimdonghwi94/web-analyzer-mcp",
      "imageUrl": "",
      "description": "Extracts clean web content for RAG and provides Q&A about web pages.",
      "stars": 2,
      "forks": 3,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-15T08:50:54Z",
      "readme_content": "# 🔍 Web Analyzer MCP\n\n<a href=\"https://glama.ai/mcp/servers/@kimdonghwi94/web-analyzer-mcp\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@kimdonghwi94/web-analyzer-mcp/badge\" alt=\"WebAnalyzer MCP server\" />\n</a>\n\nA powerful MCP (Model Context Protocol) server for intelligent web content analysis and summarization. Built with FastMCP, this server provides smart web scraping, content extraction, and AI-powered question-answering capabilities.\n\n## ✨ Features\n\n### 🎯 Core Tools\n\n1. **`url_to_markdown`** - Extract and summarize key web page content\n   - Analyzes content importance using custom algorithms\n   - Removes ads, navigation, and irrelevant content\n   - Keeps only essential information (tables, images, key text)\n   - Outputs structured markdown optimized for analysis\n\n2. **`web_content_qna`** - AI-powered Q&A about web content\n   - Extracts relevant content sections from web pages\n   - Uses intelligent chunking and relevance matching\n   - Answers questions using OpenAI GPT models\n\n### 🚀 Key Features\n\n- **Smart Content Ranking**: Algorithm-based content importance scoring\n- **Essential Content Only**: Removes clutter, keeps what matters\n- **Multi-IDE Support**: Works with Claude Desktop, Cursor, VS Code, PyCharm\n- **Flexible Models**: Choose from GPT-3.5, GPT-4, GPT-4 Turbo, or GPT-5\n\n## 📦 Installation\n\n### Prerequisites\n- [uv](https://docs.astral.sh/uv/getting-started/installation/) (Python package manager)\n- Chrome/Chromium browser (for Selenium)\n- OpenAI API key (for Q&A functionality)\n\n### 🚀 Quick Start with uv (Recommended)\n\n```bash\n# Clone the repository\ngit clone https://github.com/kimdonghwi94/web-analyzer-mcp.git\ncd web-analyzer-mcp\n\n# Run directly with uv (auto-installs dependencies)\nuv run mcp-webanalyzer\n```\n\n### Installing via Smithery\n\nTo install web-analyzer-mcp for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@kimdonghwi94/web-analyzer-mcp):\n\n```bash\nnpx -y @smithery/cli install @kimdonghwi94/web-analyzer-mcp --client claude\n```\n\n# IDE/Editor Integration\n\n<details>\n<summary><b>Install Claude Desktop</b></summary>\n\nAdd to your Claude Desktop_config.json file. See [Claude Desktop MCP documentation](https://modelcontextprotocol.io/quickstart/user) for more details.\n\n```json\n{\n  \"mcpServers\": {\n    \"web-analyzer\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"--directory\",\n        \"/path/to/web-analyzer-mcp\",\n        \"run\", \n        \"mcp-webanalyzer\"\n      ],\n      \"env\": {\n        \"OPENAI_API_KEY\": \"your_openai_api_key_here\",\n        \"OPENAI_MODEL\": \"gpt-4\"\n      }\n    }\n  }\n}\n```\n\n</details>\n\n<details>\n<summary><b>Install Claude Code (VS Code Extension)</b></summary>\n\nAdd the server using Claude Code CLI:\n\n```bash\nclaude mcp add web-analyzer -e OPENAI_API_KEY=your_api_key_here -e OPENAI_MODEL=gpt-4 -- uv --directory /path/to/web-analyzer-mcp run mcp-webanalyzer\n```\n</details>\n\n<details>\n<summary><b>Install Cursor IDE</b></summary>\n\nAdd to your Cursor settings (`File > Preferences > Settings > Extensions > MCP`):\n\n```json\n{\n  \"mcpServers\": {\n    \"web-analyzer\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"--directory\",\n        \"/path/to/web-analyzer-mcp\",\n        \"run\", \n        \"mcp-webanalyzer\"\n      ],\n      \"env\": {\n        \"OPENAI_API_KEY\": \"your_openai_api_key_here\",\n        \"OPENAI_MODEL\": \"gpt-4\"\n      }\n    }\n  }\n}\n```\n</details>\n\n<details>\n<summary><b>Install JetBrains AI Assistant</b></summary>\n\nSee [JetBrains AI Assistant Documentation](https://www.jetbrains.com/help/idea/ai-assistant.html) for more details.\n\n1. In JetBrains IDEs go to **Settings** → **Tools** → **AI Assistant** → **Model Context Protocol (MCP)**\n2. Click **+ Add**\n3. Click on **Command** in the top-left corner of the dialog and select the **As JSON** option from the list\n4. Add this configuration and click **OK**:\n\n```json\n{\n  \"mcpServers\": {\n    \"web-analyzer\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"--directory\",\n        \"/path/to/web-analyzer-mcp\",\n        \"run\", \n        \"mcp-webanalyzer\"\n      ],\n      \"env\": {\n        \"OPENAI_API_KEY\": \"your_openai_api_key_here\",\n        \"OPENAI_MODEL\": \"gpt-4\"\n      }\n    }\n  }\n}\n```\n</details>\n\n## 🎛️ Tool Descriptions\n\n### `url_to_markdown`\nConverts web pages to clean markdown format with essential content extraction.\n\n**Parameters:**\n- `url` (string): The web page URL to analyze\n\n**Returns:** Clean markdown content with structured data preservation\n\n### `web_content_qna` \nAnswers questions about web page content using intelligent content analysis.\n\n**Parameters:**\n- `url` (string): The web page URL to analyze\n- `question` (string): Question about the page content\n\n**Returns:** AI-generated answer based on page content\n\n## 🏗️ Architecture\n\n### Content Extraction Pipeline\n\n1. **URL Validation** - Ensures proper URL format\n2. **HTML Fetching** - Uses Selenium for dynamic content\n3. **Content Parsing** - BeautifulSoup for HTML processing\n4. **Element Scoring** - Custom algorithm ranks content importance\n5. **Content Filtering** - Removes duplicates and low-value content\n6. **Markdown Conversion** - Structured output generation\n\n### Q&A Processing Pipeline\n\n1. **Content Chunking** - Intelligent text segmentation\n2. **Relevance Scoring** - Matches content to questions\n3. **Context Selection** - Picks most relevant chunks\n4. **Answer Generation** - OpenAI GPT integration\n\n## 🏗️ Project Structure\n\n```\nweb-analyzer-mcp/\n├── web_analyzer_mcp/          # Main Python package\n│   ├── __init__.py           # Package initialization\n│   ├── server.py             # FastMCP server with tools\n│   ├── web_extractor.py      # Web content extraction engine\n│   └── rag_processor.py      # RAG-based Q&A processor\n├── scripts/                   # Build and utility scripts\n│   └── build.js              # Node.js build script\n├── README.md                 # English documentation\n├── README.ko.md              # Korean documentation\n├── package.json              # npm configuration and scripts\n├── pyproject.toml            # Python package configuration\n├── .env.example              # Environment variables template\n└── dist-info.json            # Build information (generated)\n```\n\n## 🛠️ Development\n\n### Modern Development with uv\n\n```bash\n# Clone repository\ngit clone https://github.com/kimdonghwi94/web-analyzer-mcp.git\ncd web-analyzer-mcp\n\n# Development commands\nuv run mcp-webanalyzer     # Start development server\nuv run python -m pytest   # Run tests\nuv run ruff check .        # Lint code\nuv run ruff format .       # Format code\nuv sync                    # Sync dependencies\n\n# Install development dependencies\nuv add --dev pytest ruff mypy\n\n# Create production build\nnpm run build\n```\n\n### Alternative: Traditional Python Development\n\n```bash\n# Setup Python environment (if not using uv)\npip install -e .[dev]\n\n# Development commands\npython -m web_analyzer_mcp.server  # Start server\npython -m pytest tests/            # Run tests\npython -m ruff check .             # Lint code\npython -m ruff format .            # Format code\npython -m mypy web_analyzer_mcp/   # Type checking\n```\n\n## 🤝 Contributing\n\n1. Fork the repository\n2. Create a feature branch (`git checkout -b feature/amazing-feature`)\n3. Commit your changes (`git commit -m 'Add amazing feature'`)\n4. Push to the branch (`git push origin feature/amazing-feature`)\n5. Open a Pull Request\n\n## 📋 Roadmap\n\n- [ ] Support for more content types (PDFs, videos)\n- [ ] Multi-language content extraction\n- [ ] Custom extraction rules\n- [ ] Caching for frequently accessed content\n- [ ] Webhook support for real-time updates\n\n## ⚠️ Limitations\n\n- Requires Chrome/Chromium for JavaScript-heavy sites\n- OpenAI API key needed for Q&A functionality\n- Rate limited to prevent abuse\n- Some sites may block automated access\n\n## 📄 License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n\n## 🙋‍♂️ Support\n\n- Create an issue for bug reports or feature requests\n- Contribute to discussions in the GitHub repository\n- Check the [documentation](https://github.com/kimdonghwi94/web-analyzer-mcp) for detailed guides\n\n## 🌟 Acknowledgments\n\n- Built with [FastMCP](https://github.com/jlowin/fastmcp) framework\n- Inspired by [HTMLRAG](https://github.com/plageon/HtmlRAG) techniques for web content processing\n- Thanks to the MCP community for feedback and contributions\n\n---\n\n**Made with ❤️ for the MCP community**",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "extraction",
        "web",
        "analyzer",
        "web analyzer",
        "clean web",
        "kimdonghwi94 web"
      ],
      "category": "search--data-extraction"
    },
    "kshern--mcp-tavily": {
      "owner": "kshern",
      "name": "mcp-tavily",
      "url": "https://github.com/kshern/mcp-tavily.git",
      "imageUrl": "",
      "description": "[leehanchung/bing-search-mcp](https://github.com/leehanchung/bing-search-mcp) 📇 ☁️ - Web search capabilities using Microsoft Bing Search API",
      "stars": 6,
      "forks": 3,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-07-20T19:46:19Z",
      "readme_content": "# MCP Tavily\n\n[![smithery badge](https://smithery.ai/badge/@kshern/mcp-tavily)](https://smithery.ai/server/@kshern/mcp-tavily)\n\n[中文文档](./readme.zh-CN.md)\n\nA Model Context Protocol (MCP) server implementation for Tavily API, providing advanced search and content extraction capabilities.\n\n## Features\n\n- **Multiple Search Tools**:\n  - `search`: Basic search functionality with customizable options\n  - `searchContext`: Context-aware search for better relevance\n  - `searchQNA`: Question and answer focused search\n- **Content Extraction**: Extract content from URLs with configurable options\n- **Rich Configuration Options**: Extensive options for search depth, filtering, and content inclusion\n\n\n### Usage with MCP\n\nAdd the Tavily MCP server to your MCP configuration:\n\n```json\n{\n  \"mcpServers\": {\n    \"tavily\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@mcptools/mcp-tavily\"],\n      \"env\": {\n        \"TAVILY_API_KEY\": \"your-api-key\"\n      }\n    }\n  }\n}\n```\n\n> Note: Make sure to replace `your-api-key` with your actual Tavily API key. You can also set it as an environment variable `TAVILY_API_KEY` before running the server.\n\n## API Reference\n\n### Search Tools\n\nThe server provides three search tools that can be called through MCP:\n\n#### 1. Basic Search\n```typescript\n// Tool name: search\n{\n  query: \"artificial intelligence\",\n  options: {\n    searchDepth: \"advanced\",\n    topic: \"news\",\n    maxResults: 10\n  }\n}\n```\n\n#### 2. Context Search\n```typescript\n// Tool name: searchContext\n{\n  query: \"latest developments in AI\",\n  options: {\n    topic: \"news\",\n    timeRange: \"week\"\n  }\n}\n```\n\n#### 3. Q&A Search\n```typescript\n// Tool name: searchQNA\n{\n  query: \"What is quantum computing?\",\n  options: {\n    includeAnswer: true,\n    maxResults: 5\n  }\n}\n```\n\n### Extract Tool\n\n```typescript\n// Tool name: extract\n{\n  urls: [\"https://example.com/article1\", \"https://example.com/article2\"],\n  options: {\n    extractDepth: \"advanced\",\n    includeImages: true\n  }\n}\n```\n\n### Search Options\n\nAll search tools share these options:\n\n```typescript\ninterface SearchOptions {\n  searchDepth?: \"basic\" | \"advanced\";    // Search depth level\n  topic?: \"general\" | \"news\" | \"finance\"; // Search topic category\n  days?: number;                         // Number of days to search\n  maxResults?: number;                   // Maximum number of results\n  includeImages?: boolean;               // Include images in results\n  includeImageDescriptions?: boolean;    // Include image descriptions\n  includeAnswer?: boolean;               // Include answer in results\n  includeRawContent?: boolean;           // Include raw content\n  includeDomains?: string[];            // List of domains to include\n  excludeDomains?: string[];            // List of domains to exclude\n  maxTokens?: number;                    // Maximum number of tokens\n  timeRange?: \"year\" | \"month\" | \"week\" | \"day\" | \"y\" | \"m\" | \"w\" | \"d\"; // Time range for search\n}\n```\n\n### Extract Options\n\n```typescript\ninterface ExtractOptions {\n  extractDepth?: \"basic\" | \"advanced\";   // Extraction depth level\n  includeImages?: boolean;               // Include images in results\n}\n```\n\n## Response Format\n\nAll tools return responses in the following format:\n\n```typescript\n{\n  content: Array<{\n    type: \"text\",\n    text: string\n  }>\n}\n```\n\nFor search results, each item includes:\n- Title\n- Content\n- URL\n\nFor extracted content, each item includes:\n- URL\n- Raw content\n- Failed URLs list (if any)\n\n## Error Handling\n\nAll tools include proper error handling and will throw descriptive error messages if something goes wrong.\n\n## Installation\n\n### Installing via Smithery\n\nTo install Tavily API Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@kshern/mcp-tavily):\n\n```bash\nnpx -y @smithery/cli install @kshern/mcp-tavily --client claude\n```\n\n### Manual Installation\n```bash\nnpm install @mcptools/mcp-tavily\n```\n\nOr use it directly with npx:\n\n```bash\nnpx @mcptools/mcp-tavily\n```\n\n\n\n### Prerequisites\n\n- Node.js 16 or higher\n- npm or yarn\n- Tavily API key (get one from [Tavily](https://tavily.com))\n\n### Setup\n\n1. Clone the repository\n2. Install dependencies:\n```bash\nnpm install\n```\n3. Set your Tavily API key:\n```bash\nexport TAVILY_API_KEY=your_api_key\n```\n\n\n### Building\n\n```bash\nnpm run build\n```\n\n## Debugging with MCP Inspector\n\nFor development and debugging, we recommend using [MCP Inspector](https://github.com/modelcontextprotocol/inspector), a powerful development tool for MCP servers.\n\n\nThe Inspector provides a user interface for:\n- Testing tool calls\n- Viewing server responses\n- Debugging tool execution\n- Monitoring server state\n\n## Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n\n1. Fork the repository\n2. Create your feature branch (`git checkout -b feature/AmazingFeature`)\n3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)\n4. Push to the branch (`git push origin feature/AmazingFeature`)\n5. Open a Pull Request\n\n## License\n\nThis project is licensed under the MIT License.\n\n## Support\n\nFor any questions or issues:\n- Tavily API: refer to the [Tavily documentation](https://docs.tavily.com/)\n- MCP integration: refer to the [MCP documentation](https://modelcontextprotocol.io//)",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "bing",
        "search",
        "api",
        "bing search",
        "search api",
        "microsoft bing"
      ],
      "category": "search--data-extraction"
    },
    "lfnovo--content-core": {
      "owner": "lfnovo",
      "name": "content-core",
      "url": "https://github.com/lfnovo/content-core",
      "imageUrl": "",
      "description": "Extract content from URLs, documents, videos, and audio files using intelligent auto-engine selection. Supports web pages, PDFs, Word docs, YouTube transcripts, and more with structured JSON responses.",
      "stars": 66,
      "forks": 13,
      "license": "MIT License",
      "language": "Jupyter Notebook",
      "updated_at": "2025-10-03T23:55:22Z",
      "readme_content": "# Content Core\n\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![PyPI version](https://badge.fury.io/py/content-core.svg)](https://badge.fury.io/py/content-core)\n[![Downloads](https://pepy.tech/badge/content-core)](https://pepy.tech/project/content-core)\n[![Downloads](https://pepy.tech/badge/content-core/month)](https://pepy.tech/project/content-core)\n[![GitHub stars](https://img.shields.io/github/stars/lfnovo/content-core?style=social)](https://github.com/lfnovo/content-core)\n[![GitHub forks](https://img.shields.io/github/forks/lfnovo/content-core?style=social)](https://github.com/lfnovo/content-core)\n[![GitHub issues](https://img.shields.io/github/issues/lfnovo/content-core)](https://github.com/lfnovo/content-core/issues)\n[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)\n[![Ruff](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/ruff/main/assets/badge/v2.json)](https://github.com/astral-sh/ruff)\n\n**Content Core** is a powerful, AI-powered content extraction and processing platform that transforms any source into clean, structured content. Extract text from websites, transcribe videos, process documents, and generate AI summaries—all through a unified interface with multiple integration options.\n\n## 🚀 What You Can Do\n\n**Extract content from anywhere:**\n- 📄 **Documents** - PDF, Word, PowerPoint, Excel, Markdown, HTML, EPUB\n- 🎥 **Media** - Videos (MP4, AVI, MOV) with automatic transcription  \n- 🎵 **Audio** - MP3, WAV, M4A with speech-to-text conversion\n- 🌐 **Web** - Any URL with intelligent content extraction\n- 🖼️ **Images** - JPG, PNG, TIFF with OCR text recognition\n- 📦 **Archives** - ZIP, TAR, GZ with content analysis\n\n**Process with AI:**\n- ✨ **Clean & format** extracted content automatically\n- 📝 **Generate summaries** with customizable styles (bullet points, executive summary, etc.)\n- 🎯 **Context-aware processing** - explain to a child, technical summary, action items\n- 🔄 **Smart engine selection** - automatically chooses the best extraction method\n\n## 🛠️ Multiple Ways to Use\n\n### 🖥️ Command Line (Zero Install)\n```bash\n# Extract content from any source\nuvx --from \"content-core\" ccore https://example.com\nuvx --from \"content-core\" ccore document.pdf\n\n# Generate AI summaries  \nuvx --from \"content-core\" csum video.mp4 --context \"bullet points\"\n```\n\n### 🤖 Claude Desktop Integration\nOne-click setup with Model Context Protocol (MCP) - extract content directly in Claude conversations.\n\n### 🔍 Raycast Extension  \nSmart auto-detection commands:\n- **Extract Content** - Full interface with format options\n- **Summarize Content** - 9 summary styles available\n- **Quick Extract** - Instant clipboard extraction\n\n### 🖱️ macOS Right-Click Integration\nRight-click any file in Finder → Services → Extract or Summarize content instantly.\n\n### 🐍 Python Library\n```python\nimport content_core as cc\n\n# Extract from any source\nresult = await cc.extract(\"https://example.com/article\")\nsummary = await cc.summarize_content(result, context=\"explain to a child\")\n```\n\n## ⚡ Key Features\n\n*   **🎯 Intelligent Auto-Detection:** Automatically selects the best extraction method based on content type and available services\n*   **🔧 Smart Engine Selection:** \n    * **URLs:** Firecrawl → Jina → BeautifulSoup fallback chain\n    * **Documents:** Docling → Enhanced PyMuPDF → Simple extraction fallback  \n    * **Media:** OpenAI Whisper transcription\n    * **Images:** OCR with multiple engine support\n*   **📊 Enhanced PDF Processing:** Advanced PyMuPDF engine with quality flags, table detection, and optional OCR for mathematical formulas\n*   **🌍 Multiple Integrations:** CLI, Python library, MCP server, Raycast extension, macOS Services\n*   **⚡ Zero-Install Options:** Use `uvx` for instant access without installation\n*   **🧠 AI-Powered Processing:** LLM integration for content cleaning and summarization\n*   **🔄 Asynchronous:** Built with `asyncio` for efficient processing\n*   **🐍 Pure Python Implementation:** No system dependencies required - simplified installation across all platforms\n\n## Getting Started\n\n### Installation\n\nInstall Content Core using `pip` - **no system dependencies required!**\n\n```bash\n# Basic installation (PyMuPDF + BeautifulSoup/Jina extraction)\npip install content-core\n\n# With enhanced document processing (adds Docling)\npip install content-core[docling]\n\n# With MCP server support (now included by default)\npip install content-core\n\n# Full installation (with enhanced document processing)\npip install content-core[docling]\n```\n\n> **Note:** Unlike many content extraction tools, Content Core uses pure Python implementations and doesn't require system libraries like libmagic. This ensures consistent, hassle-free installation across Windows, macOS, and Linux.\n\nAlternatively, if you’re developing locally:\n\n```bash\n# Clone the repository\ngit clone https://github.com/lfnovo/content-core\ncd content-core\n\n# Install with uv\nuv sync\n```\n\n### Command-Line Interface\n\nContent Core provides three CLI commands for extracting, cleaning, and summarizing content: \nccore, cclean, and csum. These commands support input from text, URLs, files, or piped data (e.g., via cat file | command).\n\n**Zero-install usage with uvx:**\n```bash\n# Extract content\nuvx --from \"content-core\" ccore https://example.com\n\n# Clean content  \nuvx --from \"content-core\" cclean \"messy content\"\n\n# Summarize content\nuvx --from \"content-core\" csum \"long text\" --context \"bullet points\"\n```\n\n#### ccore - Extract Content\n\nExtracts content from text, URLs, or files, with optional formatting.\nUsage:\n```bash\nccore [-f|--format xml|json|text] [-d|--debug] [content]\n```\nOptions:\n- `-f`, `--format`: Output format (xml, json, or text). Default: text.\n- `-d`, `--debug`: Enable debug logging.\n- `content`: Input content (text, URL, or file path). If omitted, reads from stdin.\n\nExamples:\n\n```bash\n# Extract from a URL as text\nccore https://example.com\n\n# Extract from a file as JSON\nccore -f json document.pdf\n\n# Extract from piped text as XML\necho \"Sample text\" | ccore --format xml\n```\n\n#### cclean - Clean Content\nCleans content by removing unnecessary formatting, spaces, or artifacts. Accepts text, JSON, XML input, URLs, or file paths.\nUsage:\n\n```bash\ncclean [-d|--debug] [content]\n```\n\nOptions:\n- `-d`, `--debug`: Enable debug logging.\n- `content`: Input content to clean (text, URL, file path, JSON, or XML). If omitted, reads from stdin.\n\nExamples:\n\n```bash\n# Clean a text string\ncclean \"  messy   text   \"\n\n# Clean piped JSON\necho '{\"content\": \"  messy   text   \"}' | cclean\n\n# Clean content from a URL\ncclean https://example.com\n\n# Clean a file’s content\ncclean document.txt\n```\n\n### csum - Summarize Content\n\nSummarizes content with an optional context to guide the summary style. Accepts text, JSON, XML input, URLs, or file paths.\n\nUsage:\n\n```bash\ncsum [--context \"context text\"] [-d|--debug] [content]\n```\n\nOptions:\n- `--context`: Context for summarization (e.g., \"explain to a child\"). Default: none.\n- `-d`, `--debug`: Enable debug logging.\n- `content`: Input content to summarize (text, URL, file path, JSON, or XML). If omitted, reads from stdin.\n\nExamples:\n\n```bash\n# Summarize text\ncsum \"AI is transforming industries.\"\n\n# Summarize with context\ncsum --context \"in bullet points\" \"AI is transforming industries.\"\n\n# Summarize piped content\ncat article.txt | csum --context \"one sentence\"\n\n# Summarize content from URL\ncsum https://example.com\n\n# Summarize a file's content\ncsum document.txt\n```\n\n## Quick Start\n\nYou can quickly integrate `content-core` into your Python projects to extract, clean, and summarize content from various sources.\n\n```python\nimport content_core as cc\n\n# Extract content from a URL, file, or text\nresult = await cc.extract(\"https://example.com/article\")\n\n# Clean messy content\ncleaned_text = await cc.clean(\"...messy text with [brackets] and extra spaces...\")\n\n# Summarize content with optional context\nsummary = await cc.summarize_content(\"long article text\", context=\"explain to a child\")\n```\n\n## Documentation\n\nFor more information on how to use the Content Core library, including details on AI model configuration and customization, refer to our [Usage Documentation](docs/usage.md).\n\n## MCP Server Integration\n\nContent Core includes a Model Context Protocol (MCP) server that enables seamless integration with Claude Desktop and other MCP-compatible applications. The MCP server exposes Content Core's powerful extraction capabilities through a standardized protocol.\n\n<a href=\"https://glama.ai/mcp/servers/@lfnovo/content-core\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@lfnovo/content-core/badge\" />\n</a>\n\n### Quick Setup with Claude Desktop\n\n```bash\n# Install Content Core (MCP server included)\npip install content-core\n\n# Or use directly with uvx (no installation required)\nuvx --from \"content-core\" content-core-mcp\n```\n\nAdd to your `claude_desktop_config.json`:\n```json\n{\n  \"mcpServers\": {\n    \"content-core\": {\n      \"command\": \"uvx\",\n      \"args\": [\n        \"--from\",\n        \"content-core\",\n        \"content-core-mcp\"\n      ]\n    }\n  }\n}\n```\n\nFor detailed setup instructions, configuration options, and usage examples, see our [MCP Documentation](docs/mcp.md).\n\n## Enhanced PDF Processing\n\nContent Core features an optimized PyMuPDF extraction engine with significant improvements for scientific documents and complex PDFs.\n\n### Key Improvements\n\n- **🔬 Mathematical Formula Extraction**: Enhanced quality flags eliminate `<!-- formula-not-decoded -->` placeholders\n- **📊 Automatic Table Detection**: Tables converted to markdown format for LLM consumption\n- **🔧 Quality Text Rendering**: Better ligature, whitespace, and image-text integration\n- **⚡ Optional OCR Enhancement**: Selective OCR for formula-heavy pages (requires Tesseract)\n\n### Configuration for Scientific Documents\n\nFor documents with heavy mathematical content, enable OCR enhancement:\n\n```yaml\n# In cc_config.yaml\nextraction:\n  pymupdf:\n    enable_formula_ocr: true      # Enable OCR for formula-heavy pages\n    formula_threshold: 3          # Min formulas per page to trigger OCR\n    ocr_fallback: true           # Graceful fallback if OCR fails\n```\n\n```python\n# Runtime configuration\nfrom content_core.config import set_pymupdf_ocr_enabled\nset_pymupdf_ocr_enabled(True)\n```\n\n### Requirements for OCR Enhancement\n\n```bash\n# Install Tesseract OCR (optional, for formula enhancement)\n# macOS\nbrew install tesseract\n\n# Ubuntu/Debian\nsudo apt-get install tesseract-ocr\n```\n\n**Note**: OCR is optional - you get improved PDF extraction automatically without any additional setup.\n\n## macOS Services Integration\n\nContent Core provides powerful right-click integration with macOS Finder, allowing you to extract and summarize content from any file without installation. Choose between clipboard or TextEdit output for maximum flexibility.\n\n### Available Services\n\nCreate **4 convenient services** for different workflows:\n\n- **Extract Content → Clipboard** - Quick copy for immediate pasting\n- **Extract Content → TextEdit** - Review before using  \n- **Summarize Content → Clipboard** - Quick summary copying\n- **Summarize Content → TextEdit** - Formatted summary with headers\n\n### Quick Setup\n\n1. **Install uv** (if not already installed):\n   ```bash\n   curl -LsSf https://astral.sh/uv/install.sh | sh\n   ```\n\n2. **Create services manually** using Automator (5 minutes setup)\n\n### Usage\n\n**Right-click any supported file** in Finder → **Services** → Choose your option:\n\n- **PDFs, Word docs** - Instant text extraction\n- **Videos, audio files** - Automatic transcription  \n- **Images** - OCR text recognition\n- **Web content** - Clean text extraction\n- **Multiple files** - Batch processing support\n\n### Features\n\n- **Zero-install processing**: Uses `uvx` for isolated execution\n- **Multiple output options**: Clipboard or TextEdit display\n- **System notifications**: Visual feedback on completion\n- **Wide format support**: 20+ file types supported\n- **Batch processing**: Handle multiple files at once\n- **Keyboard shortcuts**: Assignable hotkeys for power users\n\nFor complete setup instructions with copy-paste scripts, see [macOS Services Documentation](docs/macos.md).\n\n## Raycast Extension\n\nContent Core provides a powerful Raycast extension with smart auto-detection that handles both URLs and file paths seamlessly. Extract and summarize content directly from your Raycast interface without switching applications.\n\n### Quick Setup\n\n**From Raycast Store** (coming soon):\n1. Open Raycast and search for \"Content Core\"\n2. Install the extension by `luis_novo`\n3. Configure API keys in preferences\n\n**Manual Installation**:\n1. Download the extension from the repository\n2. Open Raycast → \"Import Extension\"\n3. Select the `raycast-content-core` folder\n\n### Commands\n\n**🔍 Extract Content** - Smart URL/file detection with full interface\n- Auto-detects URLs vs file paths in real-time\n- Multiple output formats (Text, JSON, XML)\n- Drag & drop support for files\n- Rich results view with metadata\n\n**📝 Summarize Content** - AI-powered summaries with customizable styles  \n- 9 different summary styles (bullet points, executive summary, etc.)\n- Auto-detects source type with visual feedback\n- One-click snippet creation and quicklinks\n\n**⚡ Quick Extract** - Instant extraction to clipboard\n- Type → Tab → Paste source → Enter\n- No UI, works directly from command bar\n- Perfect for quick workflows\n\n### Features\n\n- **Smart Auto-Detection**: Instantly recognizes URLs vs file paths\n- **Zero Installation**: Uses `uvx` for Content Core execution\n- **Rich Integration**: Keyboard shortcuts, clipboard actions, Raycast snippets\n- **All File Types**: Documents, videos, audio, images, archives\n- **Visual Feedback**: Real-time type detection with icons\n\nFor detailed setup, configuration, and usage examples, see [Raycast Extension Documentation](docs/raycast.md).\n\n## Using with Langchain\n\nFor users integrating with the [Langchain](https://python.langchain.com/) framework, `content-core` exposes a set of compatible tools. These tools, located in the `src/content_core/tools` directory, allow you to leverage `content-core` extraction, cleaning, and summarization capabilities directly within your Langchain agents and chains.\n\nYou can import and use these tools like any other Langchain tool. For example:\n\n```python\nfrom content_core.tools import extract_content_tool, cleanup_content_tool, summarize_content_tool\nfrom langchain.agents import initialize_agent, AgentType\n\ntools = [extract_content_tool, cleanup_content_tool, summarize_content_tool]\nagent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\nagent.run(\"Extract the content from https://example.com and then summarize it.\") \n```\n\nRefer to the source code in `src/content_core/tools` for specific tool implementations and usage details.\n\n## Basic Usage\n\nThe core functionality revolves around the extract_content function.\n\n```python\nimport asyncio\nfrom content_core.extraction import extract_content\n\nasync def main():\n    # Extract from raw text\n    text_data = await extract_content({\"content\": \"This is my sample text content.\"})\n    print(text_data)\n\n    # Extract from a URL (uses 'auto' engine by default)\n    url_data = await extract_content({\"url\": \"https://www.example.com\"})\n    print(url_data)\n\n    # Extract from a local video file (gets transcript, engine='auto' by default)\n    video_data = await extract_content({\"file_path\": \"path/to/your/video.mp4\"})\n    print(video_data)\n\n    # Extract from a local markdown file (engine='auto' by default)\n    md_data = await extract_content({\"file_path\": \"path/to/your/document.md\"})\n    print(md_data)\n\n    # Per-execution override with Docling for documents\n    doc_data = await extract_content({\n        \"file_path\": \"path/to/your/document.pdf\",\n        \"document_engine\": \"docling\",\n        \"output_format\": \"html\"\n    })\n    \n    # Per-execution override with Firecrawl for URLs\n    url_data = await extract_content({\n        \"url\": \"https://www.example.com\",\n        \"url_engine\": \"firecrawl\"\n    })\n    print(doc_data)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n(See `src/content_core/notebooks/run.ipynb` for more detailed examples.)\n\n## Docling Integration\n\nContent Core supports an optional Docling-based extraction engine for rich document formats (PDF, DOCX, PPTX, XLSX, Markdown, AsciiDoc, HTML, CSV, Images).\n\n\n### Enabling Docling\n\nDocling is not the default engine when parsing documents. If you don't want to use it, you need to set engine to \"simple\". \n\n#### Via configuration file\n\nIn your `cc_config.yaml` or custom config, set:\n```yaml\nextraction:\n  document_engine: docling  # 'auto' (default), 'simple', or 'docling'\n  url_engine: auto          # 'auto' (default), 'simple', 'firecrawl', or 'jina'\n  docling:\n    output_format: markdown  # markdown | html | json\n```\n\n#### Programmatically in Python\n\n```python\nfrom content_core.config import set_document_engine, set_url_engine, set_docling_output_format\n\n# switch document engine to Docling\nset_document_engine(\"docling\")\n\n# switch URL engine to Firecrawl\nset_url_engine(\"firecrawl\")\n\n# choose output format: 'markdown', 'html', or 'json'\nset_docling_output_format(\"html\")\n\n# now use ccore.extract or ccore.ccore\nresult = await cc.extract(\"document.pdf\")\n```\n\n## Configuration\n\nConfiguration settings (like API keys for external services, logging levels) can be managed through environment variables or `.env` files, loaded automatically via `python-dotenv`.\n\nExample `.env`:\n\n```plaintext\nOPENAI_API_KEY=your-key-here\nGOOGLE_API_KEY=your-key-here\n\n# Engine Selection (optional)\nCCORE_DOCUMENT_ENGINE=auto  # auto, simple, docling\nCCORE_URL_ENGINE=auto       # auto, simple, firecrawl, jina\n```\n\n### Engine Selection via Environment Variables\n\nFor deployment scenarios like MCP servers or Raycast extensions, you can override the extraction engines using environment variables:\n\n- **`CCORE_DOCUMENT_ENGINE`**: Force document engine (`auto`, `simple`, `docling`)\n- **`CCORE_URL_ENGINE`**: Force URL engine (`auto`, `simple`, `firecrawl`, `jina`)\n\nThese variables take precedence over config file settings and provide explicit control for different deployment scenarios.\n\n### Custom Prompt Templates\n\nContent Core allows you to define custom prompt templates for content processing. By default, the library uses built-in prompts located in the `prompts` directory. However, you can create your own prompt templates and store them in a dedicated directory. To specify the location of your custom prompts, set the `PROMPT_PATH` environment variable in your `.env` file or system environment.\n\nExample `.env` with custom prompt path:\n\n```plaintext\nOPENAI_API_KEY=your-key-here\nGOOGLE_API_KEY=your-key-here\nPROMPT_PATH=/path/to/your/custom/prompts\n```\n\nWhen a prompt template is requested, Content Core will first look in the custom directory specified by `PROMPT_PATH` (if set and exists). If the template is not found there, it will fall back to the default built-in prompts. This allows you to override specific prompts while still using the default ones for others.\n\n## Development\n\nTo set up a development environment:\n\n```bash\n# Clone the repository\ngit clone <repository-url>\ncd content-core\n\n# Create virtual environment and install dependencies\nuv venv\nsource .venv/bin/activate\nuv sync --group dev\n\n# Run tests\nmake test\n\n# Lint code\nmake lint\n\n# See all commands\nmake help\n```\n\n## License\n\nThis project is licensed under the [MIT License](LICENSE). See the [LICENSE](LICENSE) file for details.\n\n## Contributing\n\nContributions are welcome! Please see our [Contributing Guide](CONTRIBUTING.md) for more details on how to get started.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "lfnovo",
        "content",
        "search",
        "lfnovo content",
        "extraction lfnovo",
        "core extract"
      ],
      "category": "search--data-extraction"
    },
    "luminati-io--brightdata-mcp": {
      "owner": "luminati-io",
      "name": "brightdata-mcp",
      "url": "https://github.com/luminati-io/brightdata-mcp",
      "imageUrl": "",
      "description": "Discover, extract, and interact with the web - one interface powering automated access across the public internet.",
      "stars": 1389,
      "forks": 187,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-10-04T00:42:56Z",
      "readme_content": "<div align=\"center\">\n  <a href=\"https://brightdata.com/ai/mcp-server\">\n    <img src=\"https://github.com/user-attachments/assets/c21b3f7b-7ff1-40c3-b3d8-66706913d62f\" alt=\"Bright Data Logo\">\n  </a>\n\n  <h1>The Web MCP</h1>\n  \n  <p>\n    <strong>🌐 Give your AI real-time web superpowers</strong><br/>\n    <i>Seamlessly connect LLMs to the live web without getting blocked</i>\n  </p>\n\n  <p>\n    <a href=\"https://www.npmjs.com/package/@brightdata/mcp\">\n      <img src=\"https://img.shields.io/npm/v/@brightdata/mcp?style=for-the-badge&color=blue\" alt=\"npm version\"/>\n    </a>\n    <a href=\"https://www.npmjs.com/package/@brightdata/mcp\">\n      <img src=\"https://img.shields.io/npm/dw/@brightdata/mcp?style=for-the-badge&color=green\" alt=\"npm downloads\"/>\n    </a>\n    <a href=\"https://github.com/brightdata-com/brightdata-mcp/blob/main/LICENSE\">\n      <img src=\"https://img.shields.io/badge/license-MIT-purple?style=for-the-badge\" alt=\"License\"/>\n    </a>\n  </p>\n\n  <p>\n    <a href=\"#-quick-start\">Quick Start</a> •\n    <a href=\"#-features\">Features</a> •\n    <a href=\"#-pricing--modes\">Pricing</a> •\n    <a href=\"#-demos\">Demos</a> •\n    <a href=\"#-documentation\">Docs</a> •\n    <a href=\"#-support\">Support</a>\n  </p>\n\n  <div>\n    <h3>🎉 <strong>Free Tier Available!</strong> 🎉</h3>\n    <p><strong>5,000 requests/month FREE</strong> <br/>\n    <sub>Perfect for prototyping and everyday AI workflows</sub></p>\n  </div>\n</div>\n\n---\n\n## 🌟 Overview\n\n**The Web MCP** is your gateway to giving AI assistants true web capabilities. No more outdated responses, no more \"I can't access real-time information\" - just seamless, reliable web access that actually works.\n\nBuilt by [Bright Data](https://brightdata.com), the world's #1 web data platform, this MCP server ensures your AI never gets blocked, rate-limited, or served CAPTCHAs.\n\n<div align=\"center\">\n  <table>\n    <tr>\n      <td align=\"center\">✅ <strong>Works with Any LLM</strong><br/><sub>Claude, GPT, Gemini, Llama</sub></td>\n      <td align=\"center\">🛡️ <strong>Never Gets Blocked</strong><br/><sub>Enterprise-grade unblocking</sub></td>\n      <td align=\"center\">🚀 <strong>5,000 Free Requests</strong><br/><sub>Monthly</sub></td>\n      <td align=\"center\">⚡ <strong>Zero Config</strong><br/><sub>Works out of the box</sub></td>\n    </tr>\n  </table>\n</div>\n\n---\n\n## 🎯 Perfect For\n\n- 🔍 **Real-time Research** - Get current prices, news, and live data\n- 🛍️ **E-commerce Intelligence** - Monitor products, prices, and availability  \n- 📊 **Market Analysis** - Track competitors and industry trends\n- 🤖 **AI Agents** - Build agents that can actually browse the web\n- 📝 **Content Creation** - Access up-to-date information for writing\n- 🎓 **Academic Research** - Gather data from multiple sources efficiently\n\n---\n\n## ⚡ Quick Start\n\n\n<summary><b>📡 Use our hosted server - No installation needed!</b></summary>\n\nPerfect for users who want zero setup. Just add this URL to your MCP client:\n\n```\nhttps://mcp.brightdata.com/mcp?token=YOUR_API_TOKEN_HERE\n```\n\n**Setup in Claude Desktop:**\n1. Go to: Settings → Connectors → Add custom connector\n2. Name: `Bright Data Web`\n3. URL: `https://mcp.brightdata.com/mcp?token=YOUR_API_TOKEN`\n4. Click \"Add\" and you're done! ✨\n\n\n<summary><b>Run locally on your machine</b></summary>\n\n```json\n{\n  \"mcpServers\": {\n    \"Bright Data\": {\n      \"command\": \"npx\",\n      \"args\": [\"@brightdata/mcp\"],\n      \"env\": {\n        \"API_TOKEN\": \"<your-api-token-here>\"\n      }\n    }\n  }\n}\n```\n\n\n---\n\n## 🚀 Pricing & Modes\n\n<div align=\"center\">\n  <table>\n    <tr>\n      <th width=\"33%\">⚡ Rapid Mode (Free tier)</th>\n      <th width=\"33%\">💎 Pro Mode</th>\n    </tr>\n    <tr>\n      <td align=\"center\">\n        <h3>$0/month</h3>\n        <p><strong>5,000 requests</strong></p>\n        <hr/>\n        <p>✅ Web Search<br/>\n        ✅ Scraping with Web unlocker<br/>\n        ❌ Browser Automation<br/>\n        ❌ Web data tools</p>\n        <br/>\n        <code>Default Mode</code>\n      </td>\n      <td align=\"center\">\n        <h3>Pay-as-you-go</h3>\n        <p><strong>Every thing in rapid and 60+ Advanced Tools</strong></p>\n        <hr/>\n        <p>✅ Browser Control<br/>\n        ✅ Web Data APIs<br/>\n        <br/>\n        <br/>\n        <br/>\n        <code>PRO_MODE=true</code>\n      </td>\n    </tr>\n  </table>\n</div>\n\n> **💡 Note:** Pro mode is **not included** in the free tier and incurs additional charges based on usage.\n\n---\n\n## ✨ Features\n\n### 🔥 Core Capabilities\n\n<table>\n  <tr>\n    <td>🔍 <b>Smart Web Search</b><br/>Google-quality results optimized for AI</td>\n    <td>📄 <b>Clean Markdown</b><br/>AI-ready content extraction</td>\n  </tr>\n  <tr>\n    <td>🌍 <b>Global Access</b><br/>Bypass geo-restrictions automatically</td>\n    <td>🛡️ <b>Anti-Bot Protection</b><br/>Never get blocked or rate-limited</td>\n  </tr>\n  <tr>\n    <td>🤖 <b>Browser Automation</b><br/>Control real browsers remotely (Pro)</td>\n    <td>⚡ <b>Lightning Fast</b><br/>Optimized for minimal latency</td>\n  </tr>\n</table>\n\n### 🎯 Example Queries That Just Work\n\n```yaml\n✅ \"What's Tesla's current stock price?\"\n✅ \"Find the best-rated restaurants in Tokyo right now\"\n✅ \"Get today's weather forecast for New York\"\n✅ \"What movies are releasing this week?\"\n✅ \"What are the trending topics on Twitter today?\"\n```\n\n---\n\n## 🎬 Demos\n\n> **Note:** These videos show earlier versions. New demos coming soon! 🎥\n\n<details>\n<summary><b>View Demo Videos</b></summary>\n\n### Basic Web Search Demo\nhttps://github.com/user-attachments/assets/59f6ebba-801a-49ab-8278-1b2120912e33\n\n### Advanced Scraping Demo\nhttps://github.com/user-attachments/assets/61ab0bee-fdfa-4d50-b0de-5fab96b4b91d\n\n[📺 More tutorials on YouTube →](https://github.com/brightdata-com/brightdata-mcp/blob/main/examples/README.md)\n\n</details>\n\n---\n\n## 🔧 Available Tools\n\n### ⚡ Rapid Mode Tools (Default - Free)\n\n| Tool | Description | Use Case |\n|------|-------------|----------|\n| 🔍 `search_engine` | Web search with AI-optimized results | Research, fact-checking, current events |\n| 📄 `scrape_as_markdown` | Convert any webpage to clean markdown | Content extraction, documentation |\n\n### 💎 Pro Mode Tools (60+ Tools)\n\n<details>\n<summary><b>Click to see all Pro tools</b></summary>\n\n| Category | Tools | Description |\n|----------|-------|-------------|\n| **Browser Control** | `scraping_browser.*` | Full browser automation |\n| **Web Data APIs** | `web_data_*` | Structured data extraction |\n| **E-commerce** | Product scrapers | Amazon, eBay, Walmart data |\n| **Social Media** | Social scrapers | Twitter, LinkedIn, Instagram |\n| **Maps & Local** | Location tools | Google Maps, business data |\n\n[📚 View complete tool documentation →](https://github.com/brightdata-com/brightdata-mcp/blob/main/assets/Tools.md)\n\n</details>\n\n---\n\n## 🎮 Try It Now!\n\n### 🧪 Online Playground\nTry the Web MCP without any setup:\n\n<div align=\"center\">\n  <a href=\"https://brightdata.com/ai/playground-chat\">\n    <img src=\"https://img.shields.io/badge/Try_on-Playground-00C7B7?style=for-the-badge&logo=data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjQiIGhlaWdodD0iMjQiIHZpZXdCb3g9IjAgMCAyNCAyNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPHBhdGggZD0iTTEyIDJMMyA3VjE3TDEyIDIyTDIxIDE3VjdMMTIgMloiIHN0cm9rZT0id2hpdGUiIHN0cm9rZS13aWR0aD0iMiIvPgo8L3N2Zz4=\" alt=\"Playground\"/>\n  </a>\n</div>\n\n---\n\n## 🔧 Configuration\n\n### Basic Setup\n```json\n{\n  \"mcpServers\": {\n    \"Bright Data\": {\n      \"command\": \"npx\",\n      \"args\": [\"@brightdata/mcp\"],\n      \"env\": {\n        \"API_TOKEN\": \"your-token-here\"\n      }\n    }\n  }\n}\n```\n\n### Advanced Configuration\n```json\n{\n  \"mcpServers\": {\n    \"Bright Data\": {\n      \"command\": \"npx\",\n      \"args\": [\"@brightdata/mcp\"],\n      \"env\": {\n        \"API_TOKEN\": \"your-token-here\",\n        \"PRO_MODE\": \"true\",              // Enable all 60+ tools\n        \"RATE_LIMIT\": \"100/1h\",          // Custom rate limiting\n        \"WEB_UNLOCKER_ZONE\": \"custom\",   // Custom unlocker zone\n        \"BROWSER_ZONE\": \"custom_browser\" // Custom browser zone\n      }\n    }\n  }\n}\n```\n\n---\n\n## 📚 Documentation\n\n<div align=\"center\">\n  <table>\n    <tr>\n      <td align=\"center\">\n        <a href=\"https://docs.brightdata.com/mcp-server/overview\">\n          <img src=\"https://img.shields.io/badge/📖-API_Docs-blue?style=for-the-badge\" alt=\"API Docs\"/>\n        </a>\n      </td>\n      <td align=\"center\">\n        <a href=\"https://github.com/brightdata-com/brightdata-mcp/blob/main/examples\">\n          <img src=\"https://img.shields.io/badge/💡-Examples-green?style=for-the-badge\" alt=\"Examples\"/>\n        </a>\n      </td>\n      <td align=\"center\">\n        <a href=\"https://github.com/brightdata-com/brightdata-mcp/blob/main/CHANGELOG.md\">\n          <img src=\"https://img.shields.io/badge/📝-Changelog-orange?style=for-the-badge\" alt=\"Changelog\"/>\n        </a>\n      </td>\n      <td align=\"center\">\n        <a href=\"https://brightdata.com/blog/ai/web-scraping-with-mcp\">\n          <img src=\"https://img.shields.io/badge/📚-Tutorial-purple?style=for-the-badge\" alt=\"Tutorial\"/>\n        </a>\n      </td>\n    </tr>\n  </table>\n</div>\n\n---\n\n## 🚨 Common Issues & Solutions\n\n<details>\n<summary><b>🔧 Troubleshooting Guide</b></summary>\n\n### ❌ \"spawn npx ENOENT\" Error\n**Solution:** Install Node.js or use the full path to node:\n```json\n\"command\": \"/usr/local/bin/node\"  // macOS/Linux\n\"command\": \"C:\\\\Program Files\\\\nodejs\\\\node.exe\"  // Windows\n```\n\n### ⏱️ Timeouts on Complex Sites\n**Solution:** Increase timeout in your client settings to 180s\n\n### 🔑 Authentication Issues\n**Solution:** Ensure your API token is valid and has proper permissions\n\n### 📡 Remote Server Connection\n**Solution:** Check your internet connection and firewall settings\n\n[More troubleshooting →](https://github.com/brightdata-com/brightdata-mcp#troubleshooting)\n\n</details>\n\n---\n\n## 🤝 Contributing\n\nWe love contributions! Here's how you can help:\n\n- 🐛 [Report bugs](https://github.com/brightdata-com/brightdata-mcp/issues)\n- 💡 [Suggest features](https://github.com/brightdata-com/brightdata-mcp/issues)\n- 🔧 [Submit PRs](https://github.com/brightdata-com/brightdata-mcp/pulls)\n- ⭐ Star this repo!\n\nPlease follow [Bright Data's coding standards](https://brightdata.com/dna/js_code).\n\n---\n\n## 📞 Support\n\n<div align=\"center\">\n  <table>\n    <tr>\n      <td align=\"center\">\n        <a href=\"https://github.com/brightdata-com/brightdata-mcp/issues\">\n          <strong>🐛 GitHub Issues</strong><br/>\n          <sub>Report bugs & features</sub>\n        </a>\n      </td>\n      <td align=\"center\">\n        <a href=\"https://docs.brightdata.com/mcp-server/overview\">\n          <strong>📚 Documentation</strong><br/>\n          <sub>Complete guides</sub>\n        </a>\n      </td>\n      <td align=\"center\">\n        <a href=\"mailto:support@brightdata.com\">\n          <strong>✉️ Email</strong><br/>\n          <sub>support@brightdata.com</sub>\n        </a>\n      </td>\n    </tr>\n  </table>\n</div>\n\n---\n\n## 📜 License\n\nMIT © [Bright Data Ltd.](https://brightdata.com)\n\n---\n\n<div align=\"center\">\n  <p>\n    <strong>Built with ❤️ by</strong><br/>\n    <a href=\"https://brightdata.com\">\n      <img src=\"https://idsai.net.technion.ac.il/files/2022/01/Logo-600.png\" alt=\"Bright Data\" height=\"30\"/>\n    </a>\n  </p>\n  <p>\n    <sub>The world's #1 web data platform</sub>\n  </p>\n  \n  <br/>\n  \n  <p>\n    <a href=\"https://github.com/brightdata-com/brightdata-mcp\">⭐ Star us on GitHub</a> • \n    <a href=\"https://brightdata.com/blog\">Read our Blog</a>\n  </p>\n</div>\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "brightdata",
        "io",
        "web",
        "io brightdata",
        "brightdata mcp",
        "data extraction"
      ],
      "category": "search--data-extraction"
    },
    "mikechao--brave-search-mcp": {
      "owner": "mikechao",
      "name": "brave-search-mcp",
      "url": "https://github.com/mikechao/brave-search-mcp",
      "imageUrl": "",
      "description": "Web, Image, News, Video, and Local Point of Interest search capabilities using Brave's Search API",
      "stars": 80,
      "forks": 15,
      "license": "GNU General Public License v3.0",
      "language": "TypeScript",
      "updated_at": "2025-09-18T13:13:13Z",
      "readme_content": "# Brave Search MCP Server\n\nAn MCP Server implementation that integrates the [Brave Search API](https://brave.com/search/api/), providing, Web Search, Local Points of Interest Search, Video Search, Image Search and News Search capabilities\n\n<a href=\"https://glama.ai/mcp/servers/@mikechao/brave-search-mcp\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@mikechao/brave-search-mcp/badge\" alt=\"Brave Search MCP server\" />\n</a>\n\n## Features\n\n- **Web Search**: Perform a regular search on the web\n- **Image Search**: Search the web for images. Image search results will be available as a Resource\n- **News Search**: Search the web for news\n- **Video Search**: Search the web for videos\n- **Local Points of Interest Search**: Search for local physical locations, businesses, restaurants, services, etc\n\n## Tools\n\n- **brave_web_search**\n\n  - Execute web searches using Brave's API\n  - Inputs:\n    - `query` (string): The term to search the internet for\n    - `count` (number, optional): The number of results to return (max 20, default 10)\n    - `offset` (number, optional, default 0): The offset for pagination\n    - `freshness` (enum, optional): Filters search results by when they were discovered\n      - The following values are supported\n        - pd: Discovered within the last 24 hours.\n        - pw: Discovered within the last 7 Days.\n        - pm: Discovered within the last 31 Days.\n        - py: Discovered within the last 365 Days\n        - YYYY-MM-DDtoYYYY-MM-DD: Custom date range (e.g., 2022-04-01to2022-07-30)\n\n- **brave_image_search**\n\n  - Get images from the web relevant to the query\n  - Inputs:\n    - `query` (string): The term to search the internet for images of\n    - `count` (number, optional): The number of images to return (max 3, default 1)\n\n- **brave_news_search**\n\n  - Searches the web for news\n  - Inputs:\n    - `query` (string): The term to search the internet for news articles, trending topics, or recent events\n    - `count` (number, optional): The number of results to return (max 20, default 10)\n    - `freshness` (enum, optional): Filters search results by when they were discovered\n      - The following values are supported\n        - pd: Discovered within the last 24 hours.\n        - pw: Discovered within the last 7 Days.\n        - pm: Discovered within the last 31 Days.\n        - py: Discovered within the last 365 Days\n        - YYYY-MM-DDtoYYYY-MM-DD: Custom date range (e.g., 2022-04-01to2022-07-30)\n\n- **brave_local_search**\n\n  - Search for local businesses, services and points of interest\n  - **REQUIRES** subscription to the Pro api plan for location results\n  - Falls back to brave_web_search if no location results are found\n  - Inputs:\n    - `query` (string): Local search term\n    - `count` (number, optional): The number of results to return (max 20, default 5)\n\n- **brave_video_search**\n\n  - Search the web for videos\n  - Inputs:\n    - `query`: (string): The term to search for videos\n    - `count`: (number, optional): The number of videos to return (max 20, default 10)\n    - `freshness` (enum, optional): Filters search results by when they were discovered\n      - The following values are supported\n        - pd: Discovered within the last 24 hours.\n        - pw: Discovered within the last 7 Days.\n        - pm: Discovered within the last 31 Days.\n        - py: Discovered within the last 365 Days\n        - YYYY-MM-DDtoYYYY-MM-DD: Custom date range (e.g., 2022-04-01to2022-07-30)\n\n## Configuration\n\n### Getting an API Key\n\n1. Sign up for a [Brave Search API account](https://brave.com/search/api/)\n2. Choose a plan (Free tier available with 2,000 queries/month)\n3. Generate your API key [from the developer dashboard](https://api.search.brave.com/app/keys)\n\n### Usage with Claude Code\n\nFor [Claude Code](https://claude.ai/code) users, run this command:\n\n**Windows:**\n\n```bash\nclaude mcp add-json brave-search '{\"command\":\"cmd\",\"args\":[\"/c\",\"npx\",\"-y\",\"brave-search-mcp\"],\"env\":{\"BRAVE_API_KEY\":\"YOUR_API_KEY_HERE\"}}'\n```\n\n**Linux/macOS:**\n\n```bash\nclaude mcp add-json brave-search '{\"command\":\"npx\",\"args\":[\"-y\",\"brave-search-mcp\"],\"env\":{\"BRAVE_API_KEY\":\"YOUR_API_KEY_HERE\"}}'\n```\n\nReplace `YOUR_API_KEY_HERE` with your actual Brave Search API key.\n\n### Usage with Claude Desktop\n\n## Desktop Extension (DXT)\n\n1. Download the `dxt` file from the [Releases](https://github.com/mikechao/brave-search-mcp/releases)\n2. Open it with Claude Desktop\n   or\n   Go to File -> Settings -> Extensions and drag the .DXT file to the window to install it\n\n## Docker\n\n1. Clone the repo\n2. Docker build\n\n```bash\ndocker build -t brave-search-mcp:latest -f ./Dockerfile .\n```\n\n3. Add this to your `claude_desktop_config.json`:\n\n```json\n{\n  \"mcp-servers\": {\n    \"brave-search\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"-i\",\n        \"--rm\",\n        \"-e\",\n        \"BRAVE_API_KEY\",\n        \"brave-search-mcp\"\n      ],\n      \"env\": {\n        \"BRAVE_API_KEY\": \"YOUR API KEY HERE\"\n      }\n    }\n  }\n}\n```\n\n### NPX\n\nAdd this to your `claude_desktop_config.json`:\n\n```json\n{\n  \"mcp-servers\": {\n    \"brave-search\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"brave-search-mcp\"\n      ],\n      \"env\": {\n        \"BRAVE_API_KEY\": \"YOUR API KEY HERE\"\n      }\n    }\n  }\n}\n```\n\n### Usage with LibreChat\n\nAdd this to librechat.yaml\n\n```yaml\nbrave-search:\n  command: sh\n  args:\n    - -c\n    - BRAVE_API_KEY=API KEY npx -y brave-search-mcp\n```\n\n## Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n\n## Desktop Extensions (DXT)\n\nAnthropic recently released [Desktop Extensions](https://github.com/anthropics/dxt) allowing installation of local MCP Servers with one click.\n\nInstall the CLI tool to help generate both `manifest.json` and final `.dxt` file.\n\n```sh\nnpm install -g @anthropic-ai/dxt\n```\n\n### Creating the manifest.json file\n\n1. In this folder/directory which contains the local MCP Server, run `dxt init`. The command will start an interactive CLI to help create the `manifest.json`.\n\n### Creating the `dxt` file\n\n1. First install dev dependencies and build\n\n```sh\nnpm install\nnpm run build\n```\n\n2. Then install only the production dependencies, generate a smaller nodule_modules directory\n\n```sh\nnpm install --omit=dev\n```\n\n3. Run `dxt pack` to create a `dxt` file. This will also validate the manifest.json that was created. The `dxt` is essentially a zip file and will contain everything in this directory.\n\n## Disclaimer\n\nThis library is not officially associated with Brave Software. It is a third-party implementation of the Brave Search API with a MCP Server.\n\n## License\n\nThis project is licensed under the GNU General Public License v3.0 - see the [LICENSE](LICENSE) file for details.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "search",
        "api",
        "web",
        "brave search",
        "search api",
        "search data"
      ],
      "category": "search--data-extraction"
    },
    "modelcontextprotocol--server-brave-search": {
      "owner": "modelcontextprotocol",
      "name": "server-brave-search",
      "url": "https://github.com/modelcontextprotocol/servers/tree/main/src/brave-search",
      "imageUrl": "",
      "description": "Web search capabilities using Brave's Search API",
      "stars": 0,
      "forks": 0,
      "license": "Unknown",
      "language": "Unknown",
      "updated_at": "",
      "readme_content": "",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "search",
        "modelcontextprotocol",
        "brave",
        "brave search",
        "search api",
        "search capabilities"
      ],
      "category": "search--data-extraction"
    },
    "modelcontextprotocol--server-fetch": {
      "owner": "modelcontextprotocol",
      "name": "server-fetch",
      "url": "https://github.com/modelcontextprotocol/servers/tree/main/src/fetch",
      "imageUrl": "",
      "description": "Efficient web content fetching and processing for AI consumption",
      "stars": 0,
      "forks": 0,
      "license": "Unknown",
      "language": "Unknown",
      "updated_at": "",
      "readme_content": "",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "fetching",
        "search",
        "ai",
        "content fetching",
        "search data",
        "fetching processing"
      ],
      "category": "search--data-extraction"
    },
    "mzxrai--mcp-webresearch": {
      "owner": "mzxrai",
      "name": "mcp-webresearch",
      "url": "https://github.com/mzxrai/mcp-webresearch",
      "imageUrl": "",
      "description": "Search Google and do deep web research on any topic",
      "stars": 281,
      "forks": 69,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-10-02T21:03:39Z",
      "readme_content": "# MCP Web Research Server\n\nA Model Context Protocol (MCP) server for web research. \n\nBring real-time info into Claude and easily research any topic.\n\n## Features\n\n- Google search integration\n- Webpage content extraction\n- Research session tracking (list of visited pages, search queries, etc.)\n- Screenshot capture\n\n## Prerequisites\n\n- [Node.js](https://nodejs.org/) >= 18 (includes `npm` and `npx`)\n- [Claude Desktop app](https://claude.ai/download)\n\n## Installation\n\nFirst, ensure you've downloaded and installed the [Claude Desktop app](https://claude.ai/download) and you have npm installed.\n\nNext, add this entry to your `claude_desktop_config.json` (on Mac, found at `~/Library/Application\\ Support/Claude/claude_desktop_config.json`):\n\n```json\n{\n  \"mcpServers\": {\n    \"webresearch\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@mzxrai/mcp-webresearch@latest\"]\n    }\n  }\n}\n```\n\nThis config allows Claude Desktop to automatically start the web research MCP server when needed.\n\n## Usage\n\nSimply start a chat with Claude and send a prompt that would benefit from web research. If you'd like a prebuilt prompt customized for deeper web research, you can use the `agentic-research` prompt that we provide through this package. Access that prompt in Claude Desktop by clicking the Paperclip icon in the chat input and then selecting `Choose an integration` → `webresearch` → `agentic-research`.\n\n<img src=\"https://i.ibb.co/N6Y3C0q/Screenshot-2024-12-05-at-11-01-27-PM.png\" alt=\"Example screenshot of web research\" width=\"400\"/>\n\n### Tools\n\n1. `search_google`\n   - Performs Google searches and extracts results\n   - Arguments: `{ query: string }`\n\n2. `visit_page`\n   - Visits a webpage and extracts its content\n   - Arguments: `{ url: string, takeScreenshot?: boolean }`\n\n3. `take_screenshot`\n   - Takes a screenshot of the current page\n   - No arguments required\n\n### Prompts\n\n#### `agentic-research`\nA guided research prompt that helps Claude conduct thorough web research. The prompt instructs Claude to:\n- Start with broad searches to understand the topic landscape\n- Prioritize high-quality, authoritative sources\n- Iteratively refine the research direction based on findings\n- Keep you informed and let you guide the research interactively\n- Always cite sources with URLs\n\n### Resources\n\nWe expose two things as MCP resources: (1) captured webpage screenshots, and (2) the research session.\n\n#### Screenshots\n\nWhen you take a screenshot, it's saved as an MCP resource. You can access captured screenshots in Claude Desktop via the Paperclip icon.\n\n#### Research Session\n\nThe server maintains a research session that includes:\n- Search queries\n- Visited pages\n- Extracted content\n- Screenshots\n- Timestamps\n\n### Suggestions\n\nFor the best results, if you choose not to use the `agentic-research` prompt when doing your research, it may be helpful to suggest high-quality sources for Claude to use when researching general topics. For example, you could prompt `news today from reuters or AP` instead of `news today`.\n\n## Problems\n\nThis is very much pre-alpha code. And it is also AIGC, so expect bugs.\n\nIf you run into issues, it may be helpful to check Claude Desktop's MCP logs:\n\n```bash\ntail -n 20 -f ~/Library/Logs/Claude/mcp*.log\n```\n\n## Development\n\n```bash\n# Install dependencies\npnpm install\n\n# Build the project\npnpm build\n\n# Watch for changes\npnpm watch\n\n# Run in development mode\npnpm dev\n```\n\n## Requirements\n\n- Node.js >= 18\n- Playwright (automatically installed as a dependency)\n\n## Verified Platforms\n\n- [x] macOS\n- [ ] Linux\n\n## License\n\nMIT\n\n## Author\n\n[mzxrai](https://github.com/mzxrai) ",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "webresearch",
        "search",
        "google",
        "mcp webresearch",
        "web research",
        "webresearch search"
      ],
      "category": "search--data-extraction"
    },
    "nickclyde--duckduckgo-mcp-server": {
      "owner": "nickclyde",
      "name": "duckduckgo-mcp-server",
      "url": "https://github.com/nickclyde/duckduckgo-mcp-server",
      "imageUrl": "",
      "description": "Web search using DuckDuckGo",
      "stars": 513,
      "forks": 110,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-10-03T15:24:39Z",
      "readme_content": "# DuckDuckGo Search MCP Server\n\n[![smithery badge](https://smithery.ai/badge/@nickclyde/duckduckgo-mcp-server)](https://smithery.ai/server/@nickclyde/duckduckgo-mcp-server)\n\nA Model Context Protocol (MCP) server that provides web search capabilities through DuckDuckGo, with additional features for content fetching and parsing.\n\n<a href=\"https://glama.ai/mcp/servers/phcus2gcpn\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/phcus2gcpn/badge\" alt=\"DuckDuckGo Server MCP server\" />\n</a>\n\n## Features\n\n- **Web Search**: Search DuckDuckGo with advanced rate limiting and result formatting\n- **Content Fetching**: Retrieve and parse webpage content with intelligent text extraction\n- **Rate Limiting**: Built-in protection against rate limits for both search and content fetching\n- **Error Handling**: Comprehensive error handling and logging\n- **LLM-Friendly Output**: Results formatted specifically for large language model consumption\n\n## Installation\n\n### Installing via Smithery\n\nTo install DuckDuckGo Search Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@nickclyde/duckduckgo-mcp-server):\n\n```bash\nnpx -y @smithery/cli install @nickclyde/duckduckgo-mcp-server --client claude\n```\n\n### Installing via `uv`\n\nInstall directly from PyPI using `uv`:\n\n```bash\nuv pip install duckduckgo-mcp-server\n```\n\n## Usage\n\n### Running with Claude Desktop\n\n1. Download [Claude Desktop](https://claude.ai/download)\n2. Create or edit your Claude Desktop configuration:\n   - On macOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\n   - On Windows: `%APPDATA%\\Claude\\claude_desktop_config.json`\n\nAdd the following configuration:\n\n```json\n{\n    \"mcpServers\": {\n        \"ddg-search\": {\n            \"command\": \"uvx\",\n            \"args\": [\"duckduckgo-mcp-server\"]\n        }\n    }\n}\n```\n\n3. Restart Claude Desktop\n\n### Development\n\nFor local development, you can use the MCP CLI:\n\n```bash\n# Run with the MCP Inspector\nmcp dev server.py\n\n# Install locally for testing with Claude Desktop\nmcp install server.py\n```\n## Available Tools\n\n### 1. Search Tool\n\n```python\nasync def search(query: str, max_results: int = 10) -> str\n```\n\nPerforms a web search on DuckDuckGo and returns formatted results.\n\n**Parameters:**\n- `query`: Search query string\n- `max_results`: Maximum number of results to return (default: 10)\n\n**Returns:**\nFormatted string containing search results with titles, URLs, and snippets.\n\n### 2. Content Fetching Tool\n\n```python\nasync def fetch_content(url: str) -> str\n```\n\nFetches and parses content from a webpage.\n\n**Parameters:**\n- `url`: The webpage URL to fetch content from\n\n**Returns:**\nCleaned and formatted text content from the webpage.\n\n## Features in Detail\n\n### Rate Limiting\n\n- Search: Limited to 30 requests per minute\n- Content Fetching: Limited to 20 requests per minute\n- Automatic queue management and wait times\n\n### Result Processing\n\n- Removes ads and irrelevant content\n- Cleans up DuckDuckGo redirect URLs\n- Formats results for optimal LLM consumption\n- Truncates long content appropriately\n\n### Error Handling\n\n- Comprehensive error catching and reporting\n- Detailed logging through MCP context\n- Graceful degradation on rate limits or timeouts\n\n## Contributing\n\nIssues and pull requests are welcome! Some areas for potential improvement:\n\n- Additional search parameters (region, language, etc.)\n- Enhanced content parsing options\n- Caching layer for frequently accessed content\n- Additional rate limiting strategies\n\n## License\n\nThis project is licensed under the MIT License.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "duckduckgo",
        "search",
        "extraction",
        "duckduckgo mcp",
        "using duckduckgo",
        "nickclyde duckduckgo"
      ],
      "category": "search--data-extraction"
    },
    "nkapila6--mcp-local-rag": {
      "owner": "nkapila6",
      "name": "mcp-local-rag",
      "url": "https://github.com/nkapila6/mcp-local-rag",
      "imageUrl": "",
      "description": "\"primitive\" RAG-like web search model context protocol (MCP) server that runs locally. No APIs needed.",
      "stars": 83,
      "forks": 16,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-10-01T12:41:08Z",
      "readme_content": "<a href='https://github.com/nkapila6/mcp-local-rag/'></a>\n\n<!-- omit from toc -->\n# mcp-local-rag\n\"primitive\" RAG-like web search model context protocol (MCP) server that runs locally. ✨ no APIs ✨\n\n```mermaid\n%%{init: {'theme': 'base'}}%%\nflowchart TD\n    A[User] -->|1.Submits LLM Query| B[Language Model]\n    B -->|2.Sends Query| C[mcp-local-rag Tool]\n    \n    subgraph mcp-local-rag Processing\n    C -->|Search DuckDuckGo| D[Fetch 10 search results]\n    D -->|Fetch Embeddings| E[Embeddings from Google's MediaPipe Text Embedder]\n    E -->|Compute Similarity| F[Rank Entries Against Query]\n    F -->|Select top k results| G[Context Extraction from URL]\n    end\n    \n    G -->|Returns Markdown from HTML content| B\n    B -->|3.Generated response with context| H[Final LLM Output]\n    H -->|5.Present result to user| A\n\n    classDef default stroke:#333,stroke-width:2px;\n    classDef process stroke:#333,stroke-width:2px;\n    classDef input stroke:#333,stroke-width:2px;\n    classDef output stroke:#333,stroke-width:2px;\n\n    class A input;\n    class B,C process;\n    class G output;\n```\n\n# Installation\nLocate your MCP config path [here](https://modelcontextprotocol.io/quickstart/user) or check your MCP client settings. \n\n### Run Directly via `uvx`\nThis is the easiest and quickest method. You need to install [uv](https://docs.astral.sh/uv/) for this to work. <br>\nAdd this to your MCP server configuration:\n```json\n{\n  \"mcpServers\": {\n    \"mcp-local-rag\":{\n      \"command\": \"uvx\",\n        \"args\": [\n          \"--python=3.10\",\n          \"--from\",\n          \"git+https://github.com/nkapila6/mcp-local-rag\",\n          \"mcp-local-rag\"\n        ]\n      }\n  }\n}\n```\n\n### Using Docker (recommended)\nEnsure you have [Docker](https://www.docker.com) installed.<br>\nAdd this to your MCP server configuration:\n```json\n{\n  \"mcpServers\": {\n    \"mcp-local-rag\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"--rm\",\n        \"-i\",\n        \"--init\",\n        \"-e\",\n        \"DOCKER_CONTAINER=true\",\n        \"ghcr.io/nkapila6/mcp-local-rag:latest\"\n      ]\n    }\n  }\n}\n```\n\n# Security audits\nMseeP does security audits on every MCP server, you can see the security audit of this MCP server by clicking [here](https://mseep.ai/app/nkapila6-mcp-local-rag).\n\n<a href='https://mseep.ai/app/nkapila6-mcp-local-rag'><img src='https://mseep.net/pr/nkapila6-mcp-local-rag-badge.png' width='auto' height='200'></a>\n\n# MCP Clients\nThe MCP server should work with any MCP client that supports tool calling. Has been tested on the below clients.\n\n- Claude Desktop\n- Cursor\n- Goose\n- Others? You try!\n\n# Examples on Claude Desktop\nWhen an LLM (like Claude) is asked a question requiring recent web information, it will trigger `mcp-local-rag`.\n\nWhen asked to fetch/lookup/search the web, the model prompts you to use MCP server for the chat.\n\nIn the example, have asked it about Google's latest Gemma models released yesterday. This is new info that Claude is not aware about.\n\n\n## Result\n`mcp-local-rag` performs a live web search, extracts context, and sends it back to the model—giving it fresh knowledge:\n\n\n\n# Contributing\nHave ideas or want to improve this project? Issues and pull requests are welcome!\n\n# License\nThis project is licensed under the MIT License.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "search",
        "apis",
        "nkapila6",
        "web search",
        "apis needed",
        "search data"
      ],
      "category": "search--data-extraction"
    },
    "nyxn-ai--NyxDocs": {
      "owner": "nyxn-ai",
      "name": "NyxDocs",
      "url": "https://github.com/nyxn-ai/NyxDocs",
      "imageUrl": "",
      "description": "Specialized MCP server for cryptocurrency project documentation management with multi-blockchain support (Ethereum, BSC, Polygon, Solana).",
      "stars": 3,
      "forks": 3,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-07-10T03:45:06Z",
      "readme_content": "# NyxDocs - Cryptocurrency Documentation MCP Server\n\n[![Python](https://img.shields.io/badge/python-3.11+-blue.svg)](https://www.python.org/downloads/)\n[![MCP](https://img.shields.io/badge/MCP-compatible-green.svg)](https://modelcontextprotocol.io)\n[![License](https://img.shields.io/badge/license-MIT-blue.svg)](LICENSE)\n\nNyxDocs is a specialized Model Context Protocol (MCP) server that provides comprehensive documentation management for cryptocurrency projects. Built with Python and inspired by Context7's architecture, it offers real-time access to crypto project documentation, blockchain information, and development resources.\n\n## 🚀 Features\n\n### Core Capabilities\n- **Multi-Blockchain Support**: Ethereum, BSC, Polygon, Solana, and more\n- **Real-time Documentation**: Automatically discovers and updates project docs\n- **Smart Search**: Find projects by name, category, or blockchain\n- **Content Extraction**: Supports GitHub, GitBook, Notion, and official websites\n- **Update Monitoring**: Tracks documentation changes automatically\n\n### MCP Tools\n- `search_crypto_projects`: Search cryptocurrency projects by various criteria\n- `get_project_info`: Detailed project information with blockchain context\n- `get_documentation`: Retrieve actual documentation content\n- `list_blockchains`: Available blockchain networks\n- `check_updates`: Recent documentation updates\n\n## 🏗️ Architecture\n\n```\n┌─────────────────┐    ┌──────────────────┐    ┌─────────────────┐\n│   Data Sources  │    │   NyxDocs Core   │    │   MCP Client    │\n│                 │    │                  │    │                 │\n│ • CoinGecko API │────│ • Project DB     │────│ • Claude        │\n│ • GitHub API    │    │ • Doc Scraper    │    │ • Cursor        │\n│ • GitBook       │    │ • Update Monitor │    │ • VS Code       │\n│ • Notion        │    │ • MCP Server     │    │ • Other Clients │\n└─────────────────┘    └──────────────────┘    └─────────────────┘\n```\n\n### Key Components\n\n1. **MCP Server Core**: FastMCP-based server handling protocol communication\n2. **Data Collectors**: Modules for gathering project information from various APIs\n3. **Documentation Scrapers**: Intelligent content extraction from different sources\n4. **Database Layer**: SQLite/PostgreSQL for storing projects and documentation\n5. **Update Monitors**: Background tasks for tracking documentation changes\n\n## 📦 Installation\n\n### Prerequisites\n- Python 3.11+\n- uv (recommended) or pip\n\n### Quick Start\n\n```bash\n# Clone the repository\ngit clone https://github.com/nyxn-ai/NyxDocs.git\ncd NyxDocs\n\n# Install with uv (recommended)\nuv sync\n\n# Or install with pip\npip install -e .\n\n# Set up environment\ncp .env.example .env\n# Edit .env with your API keys\n\n# Initialize database\nuv run python -m nyxdocs.database.init\n\n# Start the server\nuv run python -m nyxdocs.server\n```\n\n### MCP Client Configuration\n\n#### Cursor\n```json\n{\n  \"mcpServers\": {\n    \"nyxdocs\": {\n      \"command\": \"uv\",\n      \"args\": [\"run\", \"python\", \"-m\", \"nyxdocs.server\"]\n    }\n  }\n}\n```\n\n#### Claude Desktop\n```json\n{\n  \"mcpServers\": {\n    \"nyxdocs\": {\n      \"command\": \"uv\",\n      \"args\": [\"run\", \"python\", \"-m\", \"nyxdocs.server\"]\n    }\n  }\n}\n```\n\n## 🔧 Configuration\n\n### Environment Variables\n\n```env\n# API Keys\nCOINGECKO_API_KEY=your_coingecko_api_key\nGITHUB_TOKEN=your_github_token\n\n# Database\nDATABASE_URL=sqlite:///nyxdocs.db\n# Or for PostgreSQL: postgresql://user:pass@localhost/nyxdocs\n\n# Server Settings\nLOG_LEVEL=INFO\nUPDATE_INTERVAL=3600  # seconds\nMAX_CONCURRENT_SCRAPES=5\n```\n\n### Supported Data Sources\n\n- **CoinGecko**: Market data and project information\n- **GitHub**: Repository documentation and README files\n- **GitBook**: Hosted documentation platforms\n- **Notion**: Project documentation pages\n- **Official Websites**: Direct documentation scraping\n\n## 🛠️ Usage Examples\n\n### Search for DeFi Projects\n```python\n# In your MCP client\nsearch_crypto_projects(query=\"uniswap\", category=\"DeFi\", blockchain=\"ethereum\")\n```\n\n### Get Project Documentation\n```python\nget_documentation(project=\"uniswap\", format=\"markdown\")\n```\n\n### Monitor Updates\n```python\ncheck_updates(since=\"2024-01-01\", limit=10)\n```\n\n## 🧪 Development\n\n### Project Structure\n```\nNyxDocs/\n├── nyxdocs/\n│   ├── __init__.py\n│   ├── server.py              # Main MCP server\n│   ├── collectors/            # Data collection modules\n│   ├── scrapers/              # Documentation scrapers\n│   ├── database/              # Database models and operations\n│   ├── tools/                 # MCP tool implementations\n│   └── utils/                 # Utility functions\n├── tests/                     # Test suite\n├── docs/                      # Documentation\n├── pyproject.toml            # Project configuration\n└── README.md\n```\n\n### Running Tests\n```bash\nuv run pytest\n```\n\n### Code Quality\n```bash\nuv run ruff check\nuv run mypy nyxdocs\n```\n\n## 📚 Documentation\n\n- [API Reference](docs/api.md)\n- [Configuration Guide](docs/configuration.md)\n- [Development Setup](docs/development.md)\n- [Contributing Guidelines](CONTRIBUTING.md)\n\n## 🤝 Contributing\n\nWe welcome contributions! Please see our [Contributing Guidelines](CONTRIBUTING.md) for details.\n\n## 📄 License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n\n## 🙏 Acknowledgments\n\n- Inspired by [Context7](https://github.com/upstash/context7) by Upstash\n- Built with [Python MCP SDK](https://github.com/modelcontextprotocol/python-sdk)\n- Cryptocurrency data provided by CoinGecko API\n\n---\n\n**NyxDocs** - Making cryptocurrency project documentation accessible and up-to-date for AI assistants.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "nyxdocs",
        "nyxn",
        "ethereum",
        "extraction nyxn",
        "ai nyxdocs",
        "nyxn ai"
      ],
      "category": "search--data-extraction"
    },
    "pragmar--mcp-server-webcrawl": {
      "owner": "pragmar",
      "name": "mcp-server-webcrawl",
      "url": "https://github.com/pragmar/mcp-server-webcrawl",
      "imageUrl": "",
      "description": "Advanced search and retrieval for web crawler data. Supports WARC, wget, Katana, SiteOne, and InterroBot crawlers.",
      "stars": 25,
      "forks": 6,
      "license": "Other",
      "language": "HTML",
      "updated_at": "2025-10-03T22:32:25Z",
      "readme_content": "<p align=\"center\">\r\n  \r\n</p>\r\n\r\n<p align=\"center\">\r\n  <a href=\"https://pragmar.com/mcp-server-webcrawl/\" style=\"margin: 0 10px;\">Website</a> |\r\n  <a href=\"https://github.com/pragmar/mcp-server-webcrawl\" style=\"margin: 0 10px;\">GitHub</a> |\r\n  <a href=\"https://pragmar.github.io/mcp-server-webcrawl/\" style=\"margin: 0 10px;\">Docs</a> |\r\n  <a href=\"https://pypi.org/project/mcp-server-webcrawl/\" style=\"margin: 0 10px;\">PyPi</a>\r\n</p>\r\n\r\n# mcp-server-webcrawl\r\n\r\nAdvanced search and retrieval for web crawler data. With **mcp-server-webcrawl**, your AI client filters and analyzes web content under your direction or autonomously. The server includes a fulltext search interface with boolean support, and resource filtering by type, HTTP status, and more.\r\n\r\n**mcp-server-webcrawl** provides the LLM a complete menu with which to search, and works with a variety of web crawlers:\r\n\r\n| Crawler/Format | Description | Platforms | Setup Guide |\r\n|---|---|---|---|\r\n| [**ArchiveBox**][1] | Web archiving tool | macOS/Linux | [Setup Guide][8] |\r\n| [**HTTrack**][2] | GUI mirroring tool | macOS/Windows/Linux | [Setup Guide][9] |\r\n| [**InterroBot**][3] | GUI crawler and analyzer | macOS/Windows/Linux | [Setup Guide][10] |\r\n| [**Katana**][4] | CLI security-focused crawler | macOS/Windows/Linux | [Setup Guide][11] |\r\n| [**SiteOne**][5] | GUI crawler and analyzer | macOS/Windows/Linux | [Setup Guide][12] |\r\n| [**WARC**][6] | Standard web archive format | varies by client | [Setup Guide][13] |\r\n| [**wget**][7] | CLI website mirroring tool | macOS/Linux | [Setup Guide][14] |\r\n\r\n[1]: https://archivebox.io\r\n[2]: https://github.com/xroche/httrack\r\n[3]: https://interro.bot\r\n[4]: https://github.com/projectdiscovery/katana\r\n[5]: https://crawler.siteone.io\r\n[6]: https://en.wikipedia.org/wiki/WARC_(file_format)\r\n[7]: https://en.wikipedia.org/wiki/Wget\r\n[8]: https://pragmar.github.io/mcp-server-webcrawl/guides/archivebox.html\r\n[9]: https://pragmar.github.io/mcp-server-webcrawl/guides/httrack.html\r\n[10]: https://pragmar.github.io/mcp-server-webcrawl/guides/interrobot.html\r\n[11]: https://pragmar.github.io/mcp-server-webcrawl/guides/katana.html\r\n[12]: https://pragmar.github.io/mcp-server-webcrawl/guides/siteone.html\r\n[13]: https://pragmar.github.io/mcp-server-webcrawl/guides/warc.html\r\n[14]: https://pragmar.github.io/mcp-server-webcrawl/guides/wget.html\r\n\r\n**mcp-server-webcrawl** is free and open source, and requires Claude Desktop and Python (>=3.10). It is installed on the command line, via pip install:\r\n\r\n```bash\r\npip install mcp-server-webcrawl\r\n```\r\n\r\nFor step-by-step MCP server setup, refer to the [Setup Guides](https://pragmar.github.io/mcp-server-webcrawl/guides.html).\r\n\r\n## Features\r\n\r\n* Claude Desktop ready\r\n* Multi-crawler compatible\r\n* Filter by type, status, and more\r\n* Boolean search support\r\n* Support for Markdown and snippets\r\n* Roll your own website knowledgebase\r\n\r\n## Prompt Routines\r\n\r\n**mcp-server-webcrawl** provides the toolkit necessary to search web crawl data freestyle, figuring it out as you go, reacting to each query. This is what it was designed for.\r\n\r\nIt is also capable of running routines (as prompts). You can write these yourself, or use the ones provided. These prompts are **copy and paste**, and used as raw Markdown. They are enabled by the advanced search provided to the LLM; queries and logic can be embedded in a procedural set of instructions, or even an input loop as is the case with Gopher Service.\r\n\r\n| Prompt | Download | Category | Description |\r\n|--------|----------|----------|-------------|\r\n|🔍 **SEO Audit** | [`auditseo.md`](https://raw.githubusercontent.com/pragmar/mcp-server-webcrawl/master/prompts/auditseo.md) | audit | Technical SEO (search engine optimization) analysis. Covers the basics, with options to dive deeper. |\r\n|🔗 **404 Audit** | [`audit404.md`](https://raw.githubusercontent.com/pragmar/mcp-server-webcrawl/master/prompts/audit404.md) | audit | Broken link detection and pattern analysis. Not only finds issues, but suggests fixes. |\r\n|⚡&nbsp;**Performance&nbsp;Audit** | [`auditperf.md`](https://raw.githubusercontent.com/pragmar/mcp-server-webcrawl/master/prompts/auditperf.md) | audit | Website speed and optimization analysis. Real talk. |\r\n|📁 **File Audit** | [`auditfiles.md`](https://raw.githubusercontent.com/pragmar/mcp-server-webcrawl/master/prompts/auditfiles.md) | audit | File organization and asset analysis. Discover the composition of your website. |\r\n|🌐 **Gopher Interface** | [`gopher.md`](https://raw.githubusercontent.com/pragmar/mcp-server-webcrawl/master/prompts/gopher.md) | interface | An old-fashioned search interface inspired by the Gopher clients of yesteryear. |\r\n|⚙️ **Search Test** | [`testsearch.md`](https://raw.githubusercontent.com/pragmar/mcp-server-webcrawl/master/prompts/testsearch.md) | self-test | A battery of tests to check for Boolean logical inconsistencies in the search query parser and subsequent FTS5 conversion. |\r\n\r\nIf you want to shortcut the site selection (one less query), paste the markdown and in the same request, type \"run pasted for [site name or URL].\" It will figure it out. When pasted without additional context, you should be prompted to select from a list of crawled sites.\r\n\r\n## Boolean Search Syntax\r\n\r\nThe query engine supports field-specific (`field: value`) searches and complex boolean expressions. Fulltext is supported as a combination of the url, content, and headers fields.\r\n\r\nWhile the API interface is designed to be consumed by the LLM directly, it can be helpful to familiarize yourself with the search syntax. Searches generated by the LLM are inspectable, but generally collapsed in the UI. If you need to see the query, expand the MCP collapsible.\r\n\r\n**Example Queries**\r\n\r\n| Query Example | Description |\r\n|--------------|-------------|\r\n| privacy | fulltext single keyword match |\r\n| \"privacy policy\" | fulltext match exact phrase |\r\n| boundar* | fulltext wildcard matches results starting with *boundar* (boundary, boundaries) |\r\n| id: 12345 | id field matches a specific resource by ID |\r\n| url: example.com/somedir | url field matches results with URL containing example.com/somedir |\r\n| type: html | type field matches for HTML pages only |\r\n| status: 200 | status field matches specific HTTP status codes (equal to 200) |\r\n| status: >=400 | status field matches specific HTTP status code (greater than or equal to 400) |\r\n| content: h1 | content field matches content (HTTP response body, often, but not always HTML) |\r\n| headers: text/xml | headers field matches HTTP response headers |\r\n| privacy AND policy | fulltext matches both |\r\n| privacy OR policy | fulltext matches either |\r\n| policy NOT privacy | fulltext matches policies not containing privacy |\r\n| (login OR signin) AND form | fulltext matches fulltext login or signin with form |\r\n| type: html AND status: 200 | fulltext matches only HTML pages with HTTP success |\r\n\r\n## Field Search Definitions\r\n\r\nField search provides search precision, allowing you to specify which columns of the search index to filter. Rather than searching the entire content, you can restrict your query to specific attributes like URLs, headers, or content body. This approach improves efficiency when looking for specific attributes or patterns within crawl data.\r\n\r\n| Field | Description |\r\n|-------|-------------|\r\n| id | database ID |\r\n| url | resource URL |\r\n| type | enumerated list of types (see types table) |\r\n| size | file size in bytes |\r\n| status | HTTP response codes |\r\n| headers | HTTP response headers |\r\n| content | HTTP body—HTML, CSS, JS, and more |\r\n\r\n## Field Content\r\n\r\nA subset of fields can be independently requested with results, while core fields are always on. Use of headers and content can consume tokens quickly. Use judiciously, or use extras to crunch more results into the context window. Fields are a top level argument, independent of any field searching taking place in the query.\r\n\r\n| Field | Description |\r\n|-------|-------------|\r\n| id | always available |\r\n| url | always available |\r\n| type | always available |\r\n| status | always available |\r\n| created | on request |\r\n| modified | on request |\r\n| size | on request |\r\n| headers | on request |\r\n| content | on request |\r\n\r\n## Content Types\r\n\r\nCrawls contain resource types beyond HTML pages. The `type:` field search allows filtering by broad content type groups, particularly useful when filtering images without complex extension queries. For example, you might search for `type: html NOT content: login` to find pages without \"login,\" or `type: img` to analyze image resources. The table below lists all supported content types in the search system.\r\n\r\n| Type | Description |\r\n|------|-------------|\r\n| html | webpages |\r\n| iframe | iframes |\r\n| img | web images |\r\n| audio | web audio files |\r\n| video | web video files |\r\n| font | web font files |\r\n| style | CSS stylesheets |\r\n| script | JavaScript files |\r\n| rss | RSS syndication feeds |\r\n| text | plain text content |\r\n| pdf | PDF files |\r\n| doc | MS Word documents |\r\n| other | uncategorized |\r\n\r\n## Extras\r\n\r\nThe `extras` parameter provides additional processing options, transforming HTTP data (markdown, snippets, regex, xpath), or connecting the LLM to external data (thumbnails). These options can be combined as needed to achieve the desired result format.\r\n\r\n| Extra | Description |\r\n|-------|-------------|\r\n| thumbnails | Generates base64 encoded images to be viewed and analyzed by AI models. Enables image description, content analysis, and visual understanding while keeping token output minimal. Works with images, which can be filtered using `type: img` in queries. SVG is not supported. |\r\n| markdown | Provides the HTML content field as concise Markdown, reducing token usage and improving readability for LLMs. Works with HTML, which can be filtered using `type: html` in queries. |\r\n| regex | Extracts regular expression matches from crawled files such as HTML, CSS, JavaScript, etc. Not as precise a tool as XPath for HTML, but supports any text file as a data source. One or more regex patterns can be requested, using the `extrasRegex` argument. |\r\n| snippets | Matches fulltext queries to contextual keyword usage within the content. When used without requesting the content field (or markdown extra), it can provide an efficient means of refining a search without pulling down the complete page contents. Also great for rendering old school hit-highlighted results as a list, like Google search in 1999. Works with HTML, CSS, JS, or any text-based, crawled file. |\r\n| xpath | Extracts XPath selector data, used in scraping HTML content. Use XPath's text() selector for text-only, element selectors return outerHTML. Only supported with `type: html`, other types will be ignored. One or more XPath selectors (//h1, count(//h1), etc.) can be requested, using the `extrasXpath` argument. |\r\n\r\nExtras provide a means of producing token-efficient HTTP content responses. Markdown produces roughly 1/3 the bytes of the source HTML, snippets are generally 500 or so bytes per result, and XPath can be as specific or broad as you choose. The more focused your requests, the more results you can fit into your LLM session.\r\n\r\nThe idea, of course, is that the LLM takes care of this for you. If you notice your LLM developing an affinity to the \"content\" field (full HTML), a nudge in chat to budget tokens using the extras feature should be all that is needed.\r\n\r\n## Interactive Mode\r\n\r\n**No AI, just classic Boolean search of your web-archives in a terminal.**\r\n\r\nmcp-server-webcrawl can double as a terminal search for your web archives. You can run it against your local archives, but it gets more interesting when you realize you can ssh into any remote host and view archives sitting on that host. No downloads, syncs, multifactor logins, or other common drudgery required. With interactive mode, you can be in and searching a crawl sitting on a remote server in no time at all.\r\n\r\nLaunch with --crawler and --datasource to load into search immediately, or use setup datasrc and crawler in-app.\r\n\r\n```bash\r\nmcp-server-webcrawl --crawler wget --datasrc /path/to/datasrc --interactive\r\n# or manually enter crawler and datasrc in the UI\r\nmcp-server-webcrawl --interactive\r\n```\r\n\r\nInteractive mode is a way to search through tranches of crawled data, whenever, whereever... in a terminal.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "webcrawl",
        "crawler",
        "wget",
        "web crawler",
        "webcrawl advanced",
        "crawler data"
      ],
      "category": "search--data-extraction"
    },
    "r-huijts--opentk-mcp": {
      "owner": "r-huijts",
      "name": "opentk-mcp",
      "url": "https://github.com/r-huijts/opentk-mcp",
      "imageUrl": "",
      "description": "Access Dutch Parliament (Tweede Kamer) information including documents, debates, activities, and legislative cases through structured search capabilities (based on opentk project by Bert Hubert)",
      "stars": 15,
      "forks": 2,
      "license": "MIT License",
      "language": "HTML",
      "updated_at": "2025-09-24T07:23:36Z",
      "readme_content": "# OpenTK Model Context Protocol Server\n\n> **Important Attribution**: This MCP server is built as a wrapper around the excellent [OpenTK project](https://berthub.eu/tkconv/) created by [Bert Hubert](https://berthub.eu/). The OpenTK project provides unprecedented access to Dutch parliamentary data through a user-friendly interface. Learn more about the project in Bert's article: [Welkom bij OpenTK](https://berthub.eu/articles/posts/welkom-bij-opentk/). All credit for the underlying data access and processing goes to Bert Hubert and his contributions to open government data.\n\nA bridge between large language models (LLMs) and Dutch parliamentary data through a standardized interface. This MCP server provides access to Dutch parliamentary documents, debates, and member information from the Tweede Kamer.\n\n<a href=\"https://glama.ai/mcp/servers/@r-huijts/opentk-mcp\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@r-huijts/opentk-mcp/badge\" alt=\"OpenTK Model Context Protocol Server MCP server\" />\n</a>\n\n## Real-World Natural Language Interaction Examples\n\n## Example 1: Comparing Party Positions on AI Policies\nUser Query: \"When comparing the activities of opposition parties PvdA, GroenLinks, and Volt with government party BBB in the Dutch House of Representatives in the field of AI, what are actions they can undertake together in the short term that align with the positions and views they have demonstrated over the past year? Please use sources from OpenTK.\"\n\n## Example 2: Researching Parliamentary Discussions on Climate Policy\nUser Query: \"I'd like to analyze recent parliamentary debates on climate policy and emission reduction targets in the Netherlands. Can you help me identify key discussions and the main positions taken by different parties over the past six months?\"\n\n## Example 3: Information About a Specific MP's Voting Record\nUser Query: \"What is MP Pieter Omtzigt's voting record on healthcare reform legislation, and how does his position differ from other independent members? Has he introduced any motions on this topic?\"\n\n## Example 4: Finding Recent Housing Legislation Developments\nUser Query: \"What are the most significant parliamentary documents and debates about affordable housing legislation from the past year? I'm particularly interested in proposals addressing the rental market crisis.\"\n\n## Example 5: Finding MPs with Specific Committee Memberships\nUser Query: \"Which MPs currently serve on both the Finance Committee and the Economic Affairs Committee? What parties do they represent, and have they recently submitted any joint initiatives?\"\n\n## Example 6: Identifying Upcoming Parliamentary Activities on Digital Security\nUser Query: \"Are there any scheduled committee meetings or debates about cybersecurity and digital infrastructure planned for the next month? Which ministers will be participating and what specific topics will be addressed?\"\n\n## Project Concept\n\nThe OpenTK project is a Model Context Protocol (MCP) server that provides access to Dutch parliamentary data through a standardized interface. It serves as a bridge between large language models (LLMs) and the Dutch Parliament's information systems, allowing AI assistants to search, retrieve, and analyze parliamentary documents, debates, and member information.\n\nThe server uses the `@modelcontextprotocol/sdk` to implement the MCP specification, which enables structured communication between AI models and external data sources. By exposing parliamentary data through well-defined tools and endpoints, OpenTK makes it possible for AI assistants to:\n\n1. Search for parliamentary documents using complex queries\n2. Access information about Members of Parliament\n3. Retrieve official documents in various formats and read the full content of the documents\n4. Analyze parliamentary activities and proceedings\n5. Track legislative cases and government pledges\n\nThe project leverages Bert Hubert's tkconv service as its primary data source, which provides a more accessible API than the official Dutch Parliament APIs.\n\n## Installation\n\n### 1. Quick Start with NPM Package (Recommended)\n\nThe fastest way to get started is using the published npm package:\n\n```bash\nnpx @r-huijts/opentk-mcp\n```\n\n### 2. Using Claude Desktop with NPM Package\n\nUpdate your Claude configuration file (`~/Library/Application Support/Claude/claude_desktop_config.json`):\n\n```json\n{\n  \"mcpServers\": {\n    \"opentk\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"@r-huijts/opentk-mcp\"\n      ]\n    }\n  }\n}\n```\n\n**Alternative configurations:**\n\nFor MultiServerMCPClient (Python):\n```python\nmcp_client = MultiServerMCPClient({\n    \"opentk\": {\n        \"command\": \"npx\",\n        \"args\": [\"-y\", \"@r-huijts/opentk-mcp\"],\n        \"transport\": \"stdio\",\n    }\n})\n```\n\n### 3. From Source (Development)\n\nIf you want to modify the code or contribute to development:\n\n**Clone Repository:**\n```bash\ngit clone https://github.com/r-huijts/opentk-mcp.git\ncd opentk-mcp\n```\n\n**Install Dependencies:**\n```bash\nnpm install\n```\n\n**Build the Project:**\n```bash\nnpm run build\n```\n\n**Start the Server:**\n```bash\nnpm start\n```\n\n**Configure Claude Desktop for local development:**\n\nUpdate your Claude configuration file:\n\n```json\n{\n  \"mcpServers\": {\n    \"opentk-local\": {\n      \"command\": \"node\",\n      \"args\": [\n        \"/absolute/path/to/your/opentk-mcp/dist/index.js\"\n      ]\n    }\n  }\n}\n```\n\nMake sure to replace `/absolute/path/to/your/opentk-mcp/` with the actual path to your installation.\n\n### 4. Publishing (for maintainers)\n\nTo publish a new version of the scoped package:\n\n```bash\nnpm run build\nnpm publish --access=public\n```\n\nNote: Scoped packages require the `--access=public` flag to be publicly available.\n\n## Search Functionality\n\nThe search functionality is particularly sophisticated, supporting:\n\n- Simple keyword searches: `kunstmatige intelligentie`\n- Exact phrase searches: `\"kunstmatige intelligentie\"`\n- Exclusion searches: `Hubert NOT Bruls`\n- Boolean operators: `OR`, `NEAR()`\n\nThe implementation handles various edge cases:\n- Preserves quotes in search queries\n- Uses proper content type headers\n- Implements fallback mechanisms for API errors\n- Provides meaningful error messages\n\n## Error Handling\n\nThe API service includes robust error handling:\n- Graceful handling of API errors (4xx, 5xx)\n- Fallback to simplified queries when complex ones fail\n- Detailed error messages for debugging\n- Proper logging to stderr (not stdout, which would break the stdio transport)\n\n## Configuration\n\nThe server connects to Bert Hubert's [tkconv service](https://berthub.eu/tkconv/) as its primary data source, which provides a more accessible API than the official Dutch Parliament APIs. This service, created by Bert Hubert, does the heavy lifting of collecting, organizing, and making available Dutch parliamentary data in a developer-friendly format. Our MCP server builds upon this foundation to create a standardized interface for AI assistants to interact with this valuable data.\n\n## License\n\nMIT\n\n## Conclusion\n\nThe OpenTK MCP server provides a robust and well-structured interface to Dutch parliamentary data, making it accessible to AI assistants through the Model Context Protocol. Its modular design, comprehensive API, and thorough testing ensure reliable access to parliamentary information for AI-assisted research, analysis, and information retrieval.\n\nOnce configured, Claude will be able to access Dutch parliamentary data through the OpenTK MCP server. The server exposes all the tools described in the [Usage](#usage) section above.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "parliament",
        "search",
        "documents",
        "dutch parliament",
        "structured search",
        "search data"
      ],
      "category": "search--data-extraction"
    },
    "reading-plus-ai--mcp-server-deep-research": {
      "owner": "reading-plus-ai",
      "name": "mcp-server-deep-research",
      "url": "https://github.com/reading-plus-ai/mcp-server-deep-research",
      "imageUrl": "",
      "description": "MCP server providing OpenAI/Perplexity-like autonomous deep research, structured query elaboration, and concise reporting.",
      "stars": 181,
      "forks": 23,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-10-03T10:09:42Z",
      "readme_content": "# MCP Server for Deep Research\n\nMCP Server for Deep Research is a tool designed for conducting comprehensive research on complex topics. It helps you explore questions in depth, find relevant sources, and generate structured research reports.\n\nYour personal Research Assistant, turning research questions into comprehensive, well-cited reports.\n\n## 🚀 Try it Out\n\n[![Watch the demo](https://img.youtube.com/vi/_a7sfo5yxoI/maxresdefault.jpg)]([VIDEO_URL](https://youtu.be/_a7sfo5yxoI))\nYoutube: https://youtu.be/_a7sfo5yxoI\n\n1. **Download Claude Desktop**\n   - Get it [here](https://claude.ai/download)\n\n2. **Install and Set Up**\n   - On macOS, run the following command in your terminal:\n   ```bash\n   python setup.py\n   ```\n\n3. **Start Researching**\n   - Select the deep-research prompt template from MCP\n   - Begin your research by providing a research question\n\n## Features\n\nThe Deep Research MCP Server offers a complete research workflow:\n\n1. **Question Elaboration**\n   - Expands and clarifies your research question\n   - Identifies key terms and concepts\n   - Defines scope and parameters\n\n2. **Subquestion Generation**\n   - Creates focused subquestions that address different aspects\n   - Ensures comprehensive coverage of the main topic\n   - Provides structure for systematic research\n\n3. **Web Search Integration**\n   - Uses Claude's built-in web search capabilities\n   - Performs targeted searches for each subquestion\n   - Identifies relevant and authoritative sources\n   - Collects diverse perspectives on the topic\n\n4. **Content Analysis**\n   - Evaluates information quality and relevance\n   - Synthesizes findings from multiple sources\n   - Provides proper citations for all sources\n\n5. **Report Generation**\n   - Creates well-structured, comprehensive reports as artifacts\n   - Properly cites all sources used\n   - Presents a balanced view with evidence-based conclusions\n   - Uses appropriate formatting for clarity and readability\n\n## 📦 Components\n\n### Prompts\n- **deep-research**: Tailored for comprehensive research tasks with a structured approach\n\n## ⚙️ Modifying the Server\n\n### Claude Desktop Configurations\n- macOS: `~/Library/Application\\ Support/Claude/claude_desktop_config.json`\n- Windows: `%APPDATA%/Claude/claude_desktop_config.json`\n\n### Development (Unpublished Servers)\n```json\n\"mcpServers\": {\n  \"mcp-server-deep-research\": {\n    \"command\": \"uv\",\n    \"args\": [\n      \"--directory\",\n      \"/Users/username/repos/mcp-server-application/mcp-server-deep-research\",\n      \"run\",\n      \"mcp-server-deep-research\"\n    ]\n  }\n}\n```\n\n### Published Servers\n```json\n\"mcpServers\": {\n  \"mcp-server-deep-research\": {\n    \"command\": \"uvx\",\n    \"args\": [\n      \"mcp-server-deep-research\"\n    ]\n  }\n}\n```\n\n## 🛠️ Development\n\n### Building and Publishing\n1. **Sync Dependencies**\n   ```bash\n   uv sync\n   ```\n\n2. **Build Distributions**\n   ```bash\n   uv build\n   ```\n   Generates source and wheel distributions in the dist/ directory.\n\n3. **Publish to PyPI**\n   ```bash\n   uv publish\n   ```\n\n## 🤝 Contributing\n\nContributions are welcome! Whether you're fixing bugs, adding features, or improving documentation, your help makes this project better.\n\n## 📜 License\n\nThis project is licensed under the MIT License.\nSee the LICENSE file for details.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "openai",
        "search",
        "mcp",
        "openai perplexity",
        "providing openai",
        "server deep"
      ],
      "category": "search--data-extraction"
    },
    "ricocf--mcp-wolframalpha": {
      "owner": "ricocf",
      "name": "mcp-wolframalpha",
      "url": "https://github.com/ricocf/mcp-wolframalpha",
      "imageUrl": "",
      "description": "An MCP server lets AI assistants use the Wolfram Alpha API for real-time access to computational knowledge and data.",
      "stars": 44,
      "forks": 7,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-10-03T21:33:50Z",
      "readme_content": "# MCP Wolfram Alpha (Server + Client)\nSeamlessly integrate Wolfram Alpha into your chat applications.\n\nThis project implements an MCP (Model Context Protocol) server designed to interface with the Wolfram Alpha API. It enables chat-based applications to perform computational queries and retrieve structured knowledge, facilitating advanced conversational capabilities.\n\nIncluded is an MCP-Client example utilizing Gemini via LangChain, demonstrating how to connect large language models to the MCP server for real-time interactions with Wolfram Alpha’s knowledge engine.\n\n[![Ask DeepWiki](https://deepwiki.com/badge.svg)](https://deepwiki.com/akalaric/mcp-wolframalpha)\n---\n\n## Features\n\n-  **Wolfram|Alpha Integration** for math, science, and data queries.\n\n-  **Modular Architecture** Easily extendable to support additional APIs and functionalities.\n\n-  **Multi-Client Support** Seamlessly handle interactions from multiple clients or interfaces.\n\n-  **MCP-Client example** using Gemini (via LangChain).\n-  **UI Support** using Gradio for a user-friendly web interface to interact with Google AI and Wolfram Alpha MCP server.\n\n---\n\n##  Installation\n\n\n### Clone the Repo\n   ```bash\n   git clone https://github.com/ricocf/mcp-wolframalpha.git\n\n   cd mcp-wolframalpha\n   ```\n  \n\n### Set Up Environment Variables\n\nCreate a .env file based on the example:\n\n- WOLFRAM_API_KEY=your_wolframalpha_appid\n\n- GeminiAPI=your_google_gemini_api_key *(Optional if using Client method below.)*\n\n### Install Requirements\n   ```bash\n   pip install -r requirements.txt\n   ```\n\n  Install the required dependencies with uv:\n  Ensure [`uv`](https://github.com/astral-sh/uv) is installed.\n\n   ```bash\n   uv sync\n   ```\n\n### Configuration\n\nTo use with the VSCode MCP Server:\n1.  Create a configuration file at `.vscode/mcp.json` in your project root.\n2.  Use the example provided in `configs/vscode_mcp.json` as a template.\n3.  For more details, refer to the [VSCode MCP Server Guide](https://sebastian-petrus.medium.com/vscode-mcp-server-42286eed3ee7).\n\nTo use with Claude Desktop:\n```json\n{\n  \"mcpServers\": {\n    \"WolframAlphaServer\": {\n      \"command\": \"python3\",\n      \"args\": [\n        \"/path/to/src/core/server.py\"\n      ]\n    }\n  }\n}\n```\n## Client Usage Example\n\nThis project includes an LLM client that communicates with the MCP server.\n\n#### Run with Gradio UI\n- Required: GeminiAPI\n- Provides a local web interface to interact with Google AI and Wolfram Alpha.\n- To run the client directly from the command line:\n```bash\npython main.py --ui\n```\n#### Docker\nTo build and run the client inside a Docker container:\n```bash\ndocker build -t wolframalphaui -f .devops/ui.Dockerfile .\n\ndocker run wolframalphaui\n```\n#### UI\n- Intuitive interface built with Gradio to interact with both Google AI (Gemini) and the Wolfram Alpha MCP server.\n- Allows users to switch between Wolfram Alpha, Google AI (Gemini), and query history.\n  \n\n\n#### Run as CLI Tool\n- Required: GeminiAPI\n- To run the client directly from the command line:\n```bash\npython main.py\n```\n#### Docker\nTo build and run the client inside a Docker container:\n```bash\ndocker build -t wolframalpha -f .devops/llm.Dockerfile .\n\ndocker run -it wolframalpha\n```\n\n## Contact\n\nFeel free to give feedback. The e-mail address is shown if you execute this in a shell:\n\n```sh\nprintf \"\\x61\\x6b\\x61\\x6c\\x61\\x72\\x69\\x63\\x31\\x40\\x6f\\x75\\x74\\x6c\\x6f\\x6f\\x6b\\x2e\\x63\\x6f\\x6d\\x0a\"\n```",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "wolfram",
        "ai",
        "ricocf",
        "mcp wolframalpha",
        "ai assistants",
        "wolframalpha mcp"
      ],
      "category": "search--data-extraction"
    },
    "sascharo--gxtract": {
      "owner": "sascharo",
      "name": "gxtract",
      "url": "https://github.com/sascharo/gxtract",
      "imageUrl": "",
      "description": "GXtract is a MCP server designed to integrate with VS Code and other compatible editors (documentation: [sascharo.github.io/gxtract](https://sascharo.github.io/gxtract)). It provides a suite of tools for interacting with the GroundX platform, enabling you to leverage its powerful document understanding capabilities directly within your development environment.",
      "stars": 0,
      "forks": 2,
      "license": "Other",
      "language": "Python",
      "updated_at": "2025-05-20T04:18:08Z",
      "readme_content": "# GXtract MCP Server\n\n<div style=\"text-align: left;\">\n  \n</div>\n\n[![Documentation](https://img.shields.io/badge/docs-latest-brightgreen.svg)](https://sascharo.github.io/gxtract/)\n[![Python Version](https://img.shields.io/badge/python-3.12+-blue.svg)](https://www.python.org/)\n[![UV Version](https://img.shields.io/badge/uv-0.7.6+-green.svg)](https://github.com/astral-sh/uv)\n[![Ruff](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/ruff/main/assets/badge/v2.json)](https://github.com/astral-sh/ruff)\n[![License: GPL v3](https://img.shields.io/badge/License-GPLv3-blue.svg)](https://www.gnu.org/licenses/gpl-3.0)\n\nGXtract is a Model Context Protocol (MCP) server designed to integrate with VS Code and other compatible editors. It provides a suite of tools for interacting with the GroundX platform, enabling you to leverage its powerful document understanding capabilities directly within your development environment.\n\n## Table of Contents\n\n- [Features](#features)\n- [Architecture](#architecture)\n- [Prerequisites](#prerequisites)\n- [Installing UV](#installing-uv)\n- [Quick Start: VS Code Integration](#quick-start-vs-code-integration)\n- [Available Tools](#available-tools)\n- [Configuration](#configuration)\n  - [API Key Security](#api-key-security)\n- [Development](#development)\n- [Documentation](#documentation)\n  - [Building Documentation Locally](#building-documentation-locally)\n  - [Building Documentation (Sphinx)](#building-documentation-sphinx)\n- [Cache Management](#cache-management)\n  - [When to Manually Refresh the Cache](#when-to-manually-refresh-the-cache)\n  - [How to Refresh the Cache](#how-to-refresh-the-cache)\n  - [Troubleshooting Common Cache Issues](#troubleshooting-common-cache-issues)\n  - [Checking Cache Status](#checking-cache-status)\n- [Dependency Management](#dependency-management)\n  - [Working with Dependencies](#working-with-dependencies)\n  - [The uv.lock File](#the-uvlock-file)\n- [Versioning](#versioning)\n- [License](#license)\n\n## Features\n\n*   **GroundX Integration:** Access GroundX functionalities like document search, querying, and semantic object explanation.\n*   **MCP Compliant:** Built for use with VS Code's MCP client and other MCP-compatible systems.\n*   **Efficient and Modern:** Developed with Python 3.12+ and FastMCP v2 for performance.\n*   **Easy to Configure:** Simple setup for VS Code.\n*   **Caching:** In-memory cache for GroundX metadata to improve performance and reduce API calls.\n\n## Architecture\n\nThe high-level system architecture of GXtract illustrates how the components interact:\n\n```mermaid\ngraph TB\n    subgraph \"Client\"\n        VSC[VS Code / Editor]\n    end\n\n    subgraph \"GXtract MCP Server\"\n        MCP[MCP Interface<br>stdio/http]\n        Server[GXtract Server]\n        Cache[Metadata Cache]\n        Tools[Tool Implementations]\n    end\n\n    subgraph \"External Services\"\n        GXAPI[GroundX API]\n    end\n\n    VSC -->|MCP Protocol| MCP\n    MCP --> Server\n    Server --> Tools\n    Tools -->|Query| GXAPI\n    Tools -->|Read/Write| Cache\n    Cache -.->|Refresh| GXAPI\n```\n\nThis diagram shows:\n\n1. **Client Integration**: VS Code communicates with GXtract using the MCP protocol\n2. **Transport Layer**: Supports both stdio (for direct VS Code integration) and HTTP transport\n3. **Core Components**: Server manages tool registration and requests\n4. **Caching Layer**: Maintains metadata to reduce API calls\n5. **Tool Implementation**: Provides specialized functions for interacting with GroundX\n6. **API Communication**: Secure connection to GroundX platform\n\nFor more detailed architecture information, see the [full documentation](https://sascharo.github.io/gxtract/architecture.html).\n\n## Prerequisites\n\n*   **Python 3.12 or higher.**\n*   **UV (Python package manager):** Version 0.7.6 or higher. You can install it from [astral.sh/uv](https://astral.sh/uv).\n*   **GroundX API Key:** You need a valid API key from the [GroundX Dashboard](https://dashboard.groundx.ai/).\n\n## Installing UV\n\nBefore you can use GXtract, you need to install UV (version 0.7.6 or higher), a modern Python package manager written in Rust that offers significant performance improvements over traditional tools.\n\n### Quick Installation Methods\n\n**Windows (PowerShell 7):**\n```powershell\npowershell -c \"irm https://astral.sh/uv/install.ps1 | iex\"\n```\n\n**macOS and Linux:**\n```bash\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n```\n\n### Alternative Installation Methods\n\n**Using pip:**\n```bash\npip install --upgrade uv\n```\n\n**Using Homebrew (macOS):**\n```bash\nbrew install uv\n```\n\n**Using pipx (isolated environment):**\n```bash\npipx install uv\n```\n\nAfter installation, verify that UV is working correctly:\n```bash\nuv --version\n```\n\nThis should display version 0.7.6 or higher. For more information about UV, visit the [official documentation](https://docs.astral.sh/uv/).\n\n## Quick Start: VS Code Integration\n\n1.  **Clone the GXtract Repository:**\n    ```bash\n    git clone <repository_url>  # Replace <repository_url> with the actual URL\n    cd gxtract\n    ```\n\n2.  **Install Dependencies using UV:**\n    Open a terminal in the `gxtract` project directory and run:\n    ```powershell\n    uv sync\n    ```\n    This command creates a virtual environment (if one doesn't exist or isn't active) and installs all necessary dependencies specified in `pyproject.toml` and `uv.lock`.\n\n3.  **Set GroundX API Key:**\n    The GXtract server requires your GroundX API key. You need to make this key available as an environment variable named `GROUNDX_API_KEY`.\n    VS Code will pass this environment variable to the server based on the configuration below. Ensure `GROUNDX_API_KEY` is set in the environment where VS Code is launched, or configure your shell profile (e.g., `.bashrc`, `.zshrc`, PowerShell Profile) to set it.\n\n    **Option 1: Using Environment Variables (as shown above)**\n    \n    This approach reads the API key from your system environment variables:\n    \n    ```json\n    \"env\": {\n        \"GROUNDX_API_KEY\": \"${env:GROUNDX_API_KEY}\"\n    }\n    ```\n    \n    **Option 2: Using VS Code's Secure Inputs**\n    \n    VS Code can prompt for your API key and store it securely. Add this to your `settings.json`:\n    \n    ```json\n    \"inputs\": [\n      {\n        \"type\": \"promptString\",\n        \"id\": \"groundx-api-key\",\n        \"description\": \"GroundX API Key\",\n        \"password\": true\n      }\n    ]\n    ```\n    \n    Then reference it in your server configuration:\n    \n    ```json\n    \"env\": {\n        \"GROUNDX_API_KEY\": \"${input:groundx-api-key}\"\n    }\n    ```\n    \n    With this approach, VS Code will prompt you for the API key the first time it launches the server, then store it securely in your system's credential manager (Windows Credential Manager, macOS Keychain, or similar).\n\n4.  **Configure VS Code `settings.json`:**\n    Open your VS Code `settings.json` file (Ctrl+Shift+P, then search for \"Preferences: Open User Settings (JSON)\"). Add or update the `mcp.servers` configuration:\n    ```jsonc\n    \"mcp\": {\n        \"servers\": {\n           \"gxtract\": { // You can name this server entry as you like, i.e. GXtract\n                \"command\": \"uv\",\n                \"type\": \"stdio\", // 💡 http is also supported but VS Code only supports stdio currently\n                \"args\": [\n                    // Adjust the path to your gxtract project directory if it's different\n                    \"--directory\", \n                    \"DRIVE:\\\\path\\\\to\\\\your\\\\gxtract\", // Example: C:\\\\Users\\\\yourname\\\\projects\\\\gxtract\n                    \"--project\",\n                    \"DRIVE:\\\\path\\\\to\\\\your\\\\gxtract\", // Example: C:\\\\Users\\\\yourname\\\\projects\\\\gxtract\n                    \"run\",\n                    \"gxtract\", // This matches the script name in pyproject.toml\n                    \"--transport\",\n                    \"stdio\" // 💡 Ensure this matches the \"type\" above\n                ],\n                \"env\": {\n                    // Option 1: Using environment variables (system-wide)\n                    \"GROUNDX_API_KEY\": \"${env:GROUNDX_API_KEY}\"\n\n                    // Option 2: Using secure VS Code input (uncomment to use)\n                    // \"GROUNDX_API_KEY\": \"${input:groundx-api-key}\"\n                }\n            }\n        }\n    }\n    ```\n    If using Option 2 (secure inputs), add this section (`settings.json`):\n    ```jsonc\n    // 💡 Only needed for Option 2 (secure inputs)\n    \"inputs\": [\n        {\n            \"type\": \"promptString\",\n            \"id\": \"groundx-api-key\",\n            \"description\": \"GroundX API Key\",\n            \"password\": true\n        }\n    ]\n    ```\n    **Important:**\n    *   Replace `\"DRIVE:\\\\path\\\\to\\\\your\\\\gxtract\"` with the **absolute path** to the `gxtract` directory on your system.\n    *   The `\"command\": \"uv\"` assumes `uv` is in your system's PATH. If not, you might need to provide the full path to the `uv` executable.\n    *   The server name `\"GXtract\"` in `settings.json` is how it will appear in VS Code's MCP interface.\n\n5.  **Reload VS Code:**\n    After saving `settings.json`, you might need to reload VS Code (Ctrl+Shift+P, \"Developer: Reload Window\") for the changes to take effect.\n\n6.  **Using GXtract Tools:**\n    Once configured, you can access GXtract's tools through VS Code's MCP features (e.g., via chat `@` mentions if your VS Code version supports it, or other MCP integrations).\n\n## Available Tools\n\nGXtract provides the following tools for interacting with GroundX:\n\n*   `groundx/searchDocuments`: Search for documents within your GroundX projects.\n*   `groundx/queryDocument`: Ask specific questions about a document in GroundX.\n*   `groundx/explainSemanticObject`: Get explanations for diagrams, tables, or other semantic objects within documents.\n*   `cache/refreshMetadataCache`: Manually refresh the GroundX metadata cache.\n*   `cache/refreshCachedResources`: Manually refresh the GroundX projects and buckets cache.\n*   `cache/getCacheStatistics`: Get statistics about the cached metadata.\n*   `cache/listCachedResources`: List all currently cached GroundX resources (projects, buckets).\n\n## Configuration\n\nThe server can be configured via command-line arguments when run directly. When used via VS Code, these are typically set in the `args` array in `settings.json`.\n\n*   `--transport {stdio|http}`: Communication transport type (default: `http`, but `stdio` is used for VS Code).\n*   `--host TEXT`: Host address for HTTP transport (default: `127.0.0.1`).\n*   `--port INTEGER`: Port for HTTP transport (default: `8080`).\n*   `--log-level {DEBUG|INFO|WARNING|ERROR|CRITICAL}`: Logging level (default: `INFO`).\n*   `--log-format {text|json}`: Log output format (default: `text`).\n*   `--disable-cache`: Disable the GroundX metadata cache.\n*   `--cache-ttl INTEGER`: Cache Time-To-Live in seconds (default: `3600`).\n\n### API Key Security\n\nThe GroundX API key is sensitive information that should be handled securely. GXtract supports several approaches to provide this key:\n\n1. **Environment Variables** (recommended for development):\n   - Set `GROUNDX_API_KEY` in your system or shell environment\n   - VS Code will pass it to the server using `${env:GROUNDX_API_KEY}` in settings.json\n\n2. **VS Code Secure Storage** (recommended for shared workstations):\n   - Configure VS Code to prompt for the key and store it securely\n   - Uses your system's credential manager (Windows Credential Manager, macOS Keychain)\n   - Setup using the `inputs` section in settings.json as shown in the Quick Start\n\n3. **Direct Environment Variable in VS Code settings** (not recommended):\n   - It's possible to set the key directly in settings.json: `\"GROUNDX_API_KEY\": \"your-api-key-here\"`\n   - This is not recommended as it stores the key in plaintext in your settings.json file\n\nAlways ensure your API key is not committed to source control or shared with unauthorized users.\n\n## Development\n\nTo set up for development:\n\n1.  Clone the repository.\n2.  Navigate to the `gxtract` directory.\n3.  Create and activate a virtual environment using `uv`:\n    ```powershell\n    uv venv # Create virtual environment in .venv\n    ```\n    * Activate with Windows PowerShell:\n      ```powershell\n      .\\.venv\\Scripts\\Activate.ps1\n      ```\n    * Activate with Linux/macOS bash/zsh:\n      ```bash\n      source .venv/bin/activate \n      ```\n4.  Install main project dependencies into the virtual environment:\n    ```powershell\n    uv sync # Install main dependencies from pyproject.toml\n    ```\n    Development tools (like Ruff, Pytest, Sphinx, etc.) are managed by Hatch and will be installed automatically\n    into a separate environment when you run Hatch scripts (see below).\n    Alternatively, to explicitly create or ensure the Hatch 'default' development environment is set up:\n    ```powershell\n    hatch env create default # Ensure your main .venv is active first\n    ```\n    If you need to force a complete refresh of this environment, you can remove it first \n    with 'hatch env remove default' before running 'hatch env create default'.\n\nRun linters/formatters (this will also install them via Hatch if not already present):\n```powershell\nuv run lint\nuv run format\n```\n\n## Documentation\n\nThe full documentation for GXtract is available at [https://sascharo.github.io/gxtract/](https://sascharo.github.io/gxtract/).\n\n### Building Documentation Locally\n\nIf you want to build and view the documentation locally:\n\n1. Ensure you have installed all development dependencies:\n   ```bash\n   uv sync\n   ```\n\n2. Build the documentation:\n   ```bash\n   uv run hatch -e default run docs-build\n   ```\n\n3. Serve the documentation locally:\n   ```bash\n   uv run hatch -e default run docs-serve\n   ```\n\n4. Open your browser and navigate to [http://127.0.0.1:8000](http://127.0.0.1:8000)\n\n### Building Documentation (Sphinx)\n\nThe project documentation is built using [Sphinx](https://www.sphinx-doc.org/). The following Hatch scripts are available to manage the documentation:\n\n*   **Build Documentation:**\n    ```bash\n    uv run docs-build\n    ```\n    This command generates the HTML documentation in the `docs/sphinx/build/html` directory.\n\n*   **Serve Documentation Locally:**\n    ```bash\n    uv run docs-serve\n    ```\n    This starts a local HTTP server (usually at `http://127.0.0.1:8000`) to preview the documentation. You can specify a different port if needed, e.g., `uv run docs-serve 8081`.\n\n*   **Clean Documentation Build:**\n    ```bash\n    uv run docs-clean\n    ```\n    This command removes the `docs/sphinx/build` directory, cleaning out old build artifacts.\n\nEnsure your virtual environment is active before running these commands.\n\n## Cache Management\n\nGXtract maintains an in-memory cache of GroundX metadata (projects and buckets) to improve performance and reduce API calls. While this cache is automatically populated during server startup and periodically refreshed, there are situations when you may need to manually refresh the cache.\n\n### When to Manually Refresh the Cache\n\nYou should manually refresh the cache when:\n\n1. You've recently created new projects or buckets in your GroundX account and want them to be immediately available in GXtract.\n2. You see warnings in the server logs about cache population failures.\n3. You're experiencing issues with project or bucket lookup when using GXtract tools.\n\n### How to Refresh the Cache\n\n#### Using VS Code's MCP Interface\n\nIf your VS Code version supports MCP chat interfaces:\n\n1. Open VS Code's chat interface.\n2. Use the `@GXtract` mention (or whatever name you assigned to the server in your settings).\n3. Type a command to refresh the cache:\n   ```\n   @GXtract Please refresh the GroundX metadata cache\n   ```\n4. The VS Code interface will use the appropriate cache refresh tool.\n\n#### Using Direct JSON-RPC Requests\n\nIf you have access to the server through HTTP (when not using stdio transport), you can make direct requests:\n\n```bash\ncurl -X POST http://127.0.0.1:8080/jsonrpc -H \"Content-Type: application/json\" -d '{\n  \"jsonrpc\": \"2.0\",\n  \"method\": \"cache/refreshMetadataCache\",\n  \"params\": {},\n  \"id\": \"refresh-req-001\"\n}'\n```\n\n### Troubleshooting Common Cache Issues\n\n#### Warning: \"No projects (groups) found or 'groups' attribute missing in API response\"\n\nThis warning indicates that:\n- Your API key might not have access to any projects, or\n- No projects have been created in your GroundX account yet, or\n- There might be an issue with the GroundX API or connectivity.\n\n**Solution**: \n1. Verify you have correctly set up your GroundX account with at least one project.\n2. Check that your API key has proper permissions.\n3. Try refreshing the cache manually after confirming your account setup.\n\n#### Warning: \"GroundX metadata cache population failed. Check logs for details\"\n\nThis warning appears during server startup if the initial cache population failed.\n\n**Solution**:\n1. Check the full server logs for more details about the error.\n2. Verify your API key is correctly set in the environment.\n3. Check your internet connection and GroundX API availability.\n4. Try using the `cache/refreshMetadataCache` tool to manually populate the cache.\n\n### Checking Cache Status\n\nYou can check the current status of the cache with:\n\n```json\n{\n  \"jsonrpc\": \"2.0\",\n  \"method\": \"cache/getCacheStatistics\",\n  \"params\": {},\n  \"id\": \"stats-req-001\"\n}\n```\n\nOr list the currently cached resources:\n\n```json\n{\n  \"jsonrpc\": \"2.0\",\n  \"method\": \"cache/listCachedResources\",\n  \"params\": {},\n  \"id\": \"list-req-001\"\n}\n```\n\n## Dependency Management\n\nGXtract uses [uv](https://github.com/astral-sh/uv) for dependency management. Dependencies are specified in `pyproject.toml` and locked in `uv.lock` to ensure reproducible installations.\n\n### Working with Dependencies\n\n- **Installing dependencies**: Run `uv sync` to install all dependencies according to the lockfile.\n- **Adding a new dependency**: Add the dependency to `pyproject.toml` and run `uv pip compile pyproject.toml -o uv.lock` to update the lockfile.\n- **Updating dependencies**: After manually changing versions in `pyproject.toml`, run `uv pip compile pyproject.toml -o uv.lock --upgrade` to update the lockfile with newest compatible versions.\n\n### The uv.lock File\n\nThe `uv.lock` file is committed to the repository to ensure that everyone working on the project uses exactly the same dependency versions. This prevents \"works on my machine\" problems and ensures consistent behavior across development environments and CI/CD pipelines.\n\nWhen making changes to dependencies, always commit both the updated `pyproject.toml` and the `uv.lock` file.\n\n## Versioning\n\nThis project adheres to [Semantic Versioning (SemVer 2.0.0)](https://semver.org/spec/v2.0.0.html).\n\n## License\n\nThis project is licensed under the GNU General Public License v3.0 - see the [LICENSE.md](LICENSE.md) file for details.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "gxtract",
        "sascharo",
        "suite",
        "gxtract provides",
        "sascharo gxtract",
        "io gxtract"
      ],
      "category": "search--data-extraction"
    },
    "scrapeless-ai--scrapeless-mcp-server": {
      "owner": "scrapeless-ai",
      "name": "scrapeless-mcp-server",
      "url": "https://github.com/scrapeless-ai/scrapeless-mcp-server",
      "imageUrl": "",
      "description": "The Scrapeless Model Context Protocol service acts as an MCP server connector to the Google SERP API, enabling web search within the MCP ecosystem without leaving it.",
      "stars": 53,
      "forks": 10,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-09-30T09:42:40Z",
      "readme_content": "# Scrapeless MCP Server\n\n**Welcome to the official Scrapeless Model Context Protocol (MCP) Server** — a powerful integration layer that empowers LLMs, AI Agents, and AI applications to interact with the web in real time.\n\nBuilt on the open MCP standard, Scrapeless MCP Server seamlessly connects models like **ChatGPT**, **Claude**, and tools like **Cursor** and **Windsurf** to a wide range of external capabilities, including:\n\n- **Google services integration** (Search, Trends)\n- **Browser automation** for page-level navigation and interaction\n- **Scrape** dynamic, JS-heavy sites—export as HTML, Markdown, or screenshots\n\nWhether you're building an AI research assistant, a coding copilot, or autonomous web agents, this server provides the dynamic context and real-world data your workflows need—**without getting blocked**.\n\n## Usage Examples\n\n1. Automated Web Interaction and Data Extraction with Claude\n\nUsing Scrapeless MCP Browser, Claude can perform complex tasks such as web navigation, clicking, scrolling, and scraping through conversational commands, with real-time preview of web interaction results via `live sessions`.\n\n\n\n2. Bypassing Cloudflare to Retrieve Target Page Content\n\nUsing the Scrapeless MCP Browser service, the Cloudflare page is automatically accessed, and after the process is completed, the page content is extracted and returned in Markdown format.\n\n\n\n3. Extracting Dynamically Rendered Page Content and Writing to File\n\nUsing the Scrapeless MCP Universal API, the JavaScript-rendered content of the target page above is scraped, exported in Markdown format, and finally written to a local file named **`text.md`**.\n\n\n\n4. Automated SERP Scraping\n\nUsing the Scrapeless MCP Server, query the keyword “web scraping” on Google Search, retrieve the first 10 search results (including title, link, and summary), and write the content to the file named `serp.text`.\n\n\n\nHere are some additional examples of how to use these servers:\n\n| Example                                                                                                                           |\n| --------------------------------------------------------------------------------------------------------------------------------- |\n| Search scrapeless by Google search.                                                                                               |\n| Find the search interest for \"AI\" over the last year.                                                                             |\n| Use a browser to visit [chatgpt.com](http://chatgpt.com), search for \"What's the weather like today?\", and summarize the results. |\n| Scrape the HTML content of [scrapeless.com](http://scrapeless.com) page.                                                          |\n| Scrape the Markdown content of [scrapeless.com](http://scrapeless.com) page.                                                      |\n| Get screenshots of [scrapeless.com](http://scrapeless.com).                                                                       |\n\n## Setup Guide\n\n1. Get Scrapeless Key\n\n- [Log in](https://app.scrapeless.com/passport/login?utm_source=github&utm_medium=github-mcp&utm_campaign=mcp) to the Scrapeless Dashboard（Free trial available）\n- Then click \"**Setting**\" on the left -> select \"**API Key Management**\" -> click \"**Create API Key**\". Finally, click the API Key you created to **copy** it.\n\n\n\n2. Configure Your MCP Client\n\nScrapeless MCP Server supports both **Stdio** and **Streamable HTTP** transport modes.\n\n🖥️ Stdio (Local Execution)\n\n```JSON\n{\n  \"mcpServers\": {\n    \"Scrapeless MCP Server\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"scrapeless-mcp-server\"],\n      \"env\": {\n        \"SCRAPELESS_KEY\": \"YOUR_SCRAPELESS_KEY\"\n      }\n    }\n  }\n}\n```\n\n🌐 Streamable HTTP (Hosted API Mode)\n\n```JSON\n{\n  \"mcpServers\": {\n    \"Scrapeless MCP Server\": {\n      \"type\": \"streamable-http\",\n      \"url\": \"https://api.scrapeless.com/mcp\",\n      \"headers\": {\n        \"x-api-token\": \"YOUR_SCRAPELESS_KEY\"\n      },\n      \"disabled\": false,\n      \"alwaysAllow\": []\n    }\n  }\n}\n```\n\n#### Advanced Options\n\nCustomize browser session behavior with optional parameters. These can be set via environment variables (for Stdio) or HTTP headers (for Streamable HTTP):\n\n| Stdio (Env Var)         | Streamable HTTP (HTTP Header) | Description                                                                                                                  |\n| ----------------------- | ----------------------------- | ---------------------------------------------------------------------------------------------------------------------------- |\n| BROWSER_PROFILE_ID      | x-browser-profile-id          | Specifies a reusable browser profile ID for session continuity.                                                              |\n| BROWSER_PROFILE_PERSIST | x-browser-profile-persist     | Enables persistent storage for cookies, local storage, etc.                                                                  |\n| BROWSER_SESSION_TTL     | x-browser-session-ttl         | Defines the **maximum session timeout** in seconds. The session will automatically expire after this duration of inactivity. |\n\n## Integration with Claude Desktop\n\n1. Open **Claude Desktop**\n2. Navigate to: `Settings` → `Tools` → `MCP Servers`\n3. Click **\"Add MCP Server\"**\n4. Paste either the `Stdio` or `Streamable HTTP` config above\n5. Save and enable the server\n6. Claude will now be able to issue web queries, extract content, and interact with pages using Scrapeless\n\n## Integration with Cursor IDE\n\n1. Open **Cursor**\n2. Press `Cmd + Shift + P` and search for: `Configure MCP Servers`\n3. Add the Scrapeless MCP config using the format above\n4. Save the file and restart Cursor (if needed)\n5. Now you can ask Cursor things like:\n   1. `\"Search StackOverflow for a solution to this error\"`\n   2. `\"Scrape the HTML from this page\"`\n6. And it will use Scrapeless in the background.\n\n## Supported MCP Tools\n\n| Name               | Description                                                    |\n| ------------------ | -------------------------------------------------------------- |\n| google_search      | Universal information search engine.                           |\n| google_trends      | Get trending search data from Google Trends.                   |\n| browser_create     | Create or reuse a cloud browser session using Scrapeless.      |\n| browser_close      | Closes the current session by disconnecting the cloud browser. |\n| browser_goto       | Navigate browser to a specified URL.                           |\n| browser_go_back    | Go back one step in browser history.                           |\n| browser_go_forward | Go forward one step in browser history.                        |\n| browser_click      | Click a specific element on the page.                          |\n| browser_type       | Type text into a specified input field.                        |\n| browser_press_key  | Simulate a key press.                                          |\n| browser_wait_for   | Wait for a specific page element to appear.                    |\n| browser_wait       | Pause execution for a fixed duration.                          |\n| browser_screenshot | Capture a screenshot of the current page.                      |\n| browser_get_html   | Get the full HTML of the current page.                         |\n| browser_get_text   | Get all visible text from the current page.                    |\n| browser_scroll     | Scroll to the bottom of the page.                              |\n| browser_scroll_to  | Scroll a specific element into view.                           |\n| scrape_html        | Scrape a URL and return its full HTML content.                 |\n| scrape_markdown    | Scrape a URL and return its content as Markdown.               |\n| scrape_screenshot  | Capture a high-quality screenshot of any webpage.              |\n\n## Security Best Practices\n\nWhen using Scrapeless MCP Server with LLMs (like ChatGPT, Claude, or Cursor), it's critical to handle all scraped or extracted web content with care. **Web data is untrusted by default**, and improper handling may expose your application to prompt injection or other security vulnerabilities.\n\n#### ✅ Recommended Practices\n\n- **Never pass raw scraped content directly into LLM prompts.** Raw HTML, JavaScript, or user-generated text may contain hidden injection payloads.\n- **Sanitize and validate all extracted content.** Strip or escape potentially harmful tags and scripts before using content in downstream logic or AI models.\n- **Prefer structured extraction over free-form text.** Use tools like `scrape_html`, `scrape_markdown`, or targeted `browser_get_text` with known-safe selectors to extract only the content you trust.\n- **Apply domain or selector whitelisting** when scraping dynamically generated pages, to restrict data flow to known and trusted sources.\n- **Log and monitor all outbound requests** made via browser or scraping tools, especially if you're handling sensitive data, tokens, or internal network access.\n\n#### 🚫 Avoid\n\n- Injecting scraped HTML directly into prompts\n- Letting users specify arbitrary URLs or CSS selectors without validation\n- Storing unfiltered scraped content for future prompt usage\n\n## Community\n\n- [MCP Server Discord](https://backend.scrapeless.com/app/api/v1/public/links/discord)\n\n## Contact Us\n\nFor questions, suggestions, or collaboration inquiries, feel free to contact us via:\n\n- Email: [market@scrapeless.com](mailto:market@scrapeless.com)\n- Official Website: [https://www.scrapeless.com](https://www.scrapeless.com/)\n- Community Forum: https://discord.gg/Np4CAHxB9a",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "scrapeless",
        "serp",
        "mcp",
        "scrapeless mcp",
        "google serp",
        "serp api"
      ],
      "category": "search--data-extraction"
    },
    "serkan-ozal--driflyte-mcp-server": {
      "owner": "serkan-ozal",
      "name": "driflyte-mcp-server",
      "url": "https://github.com/serkan-ozal/driflyte-mcp-server",
      "imageUrl": "",
      "description": "The Driflyte MCP Server exposes tools that allow AI assistants to query and retrieve topic-specific knowledge from recursively crawled and indexed web pages.",
      "stars": 6,
      "forks": 2,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-10-02T05:26:38Z",
      "readme_content": "# Driflyte MCP Server\n\n![Build Status](https://github.com/serkan-ozal/driflyte-mcp-server/actions/workflows/build.yml/badge.svg)\n![NPM Version](https://badge.fury.io/js/%40driflyte%2Fmcp-server.svg)\n![License](https://img.shields.io/badge/license-MIT-blue)\n[![MCP Badge](https://lobehub.com/badge/mcp/serkan-ozal-driflyte-mcp-server)](https://lobehub.com/mcp/serkan-ozal-driflyte-mcp-server)\n\nMCP Server for [Driflyte](http://console.driflyte.com).\n\nThe Driflyte MCP Server exposes tools that allow AI assistants to query and retrieve topic-specific knowledge from recursively crawled and indexed web pages.\nWith this MCP server, Driflyte acts as a bridge between diverse, topic-aware content sources (web, GitHub, and more) and AI-powered reasoning, enabling richer, more accurate answers.\n\n\n## What It Does\n\n- **Deep Web Crawling**: Recursively follows links to crawl and index web pages.\n- **GitHub Integration**: Crawls repositories, issues, and discussions.\n- **Extensible Resource Support**: Future support planned for Slack, Microsoft Teams, Google Docs/Drive, Confluence, JIRA, Zendesk, Salesforce, and more.\n- **Topic-Aware Indexing**: Each document is tagged with one or more topics, enabling targeted, topic-specific retrieval.\n- **Designed for RAG with RAG**: The server itself is built with Retrieval-Augmented Generation (RAG) in mind, and it powers RAG workflows by providing assistants with high-quality, topic-specific documents as grounding context.\n- **Designed for AI with AI**: The system is not just for AI assistants — it is also designed and evolved using AI itself, making it an AI-native component for intelligent knowledge retrieval.\n\n\n## Usage & Limits\n\n- **Free Access**: Driflyte is currently free to use.\n- **No Signup Required**: You can start using it immediately — no registration or subscription needed.\n- **Rate Limits**: To ensure fair usage, requests are limited by IP:\n  - **`100` API requests** per **`5` minutes** per **IP address**.\n- Future changes to usage policies and limits may be introduced as new features and resource integrations become available.\n\n\n## Prerequisites\n- Node.js 18+\n- An AI assistant (with MCP client) like Cursor, Claude (Desktop or Code), VS Code, Windsurf, etc ...\n\n## Configurations\n\n### CLI Arguments\n\nDriflyte MCP server supports the following CLI arguments for configuration:\n- `--transport <stdio|streamable-http>` - Configures the transport protocol (defaults to `stdio`).\n- `--port <number>` – Configures the port number to listen on when using `streamable-http` transport (defaults to `3000`).\n\n\n## Quick Start\n\nThis MCP server (using `STDIO` or `Streamable HTTP` transport) can be added to any MCP Client \nlike VS Code, Claude, Cursor, Windsurf Github Copilot via the `@driflyte/mcp-server` NPM package.\n\n### ChatGPT\n\n- Navigate to `Settings` under your profile and enable `Developer Mode` under the `Connectors` option.\n- In the chat panel, click the `+` icon, and from the dropdown, select `Developer Mode`. \n  You’ll see an option to add sources/connectors.\n- Enter the following MCP Server details and then click `Create`:\n  - `Name`: `Driflyte`\n  - `MCP Server URL`: `https://mcp.driflyte.com/openai`\n  - `Authentication`: `No authentication`\n  - `Trust Setting`: Check `I trust this application`\n\nSee [How to set up a remote MCP server and connect it to ChatGPT deep research](https://community.openai.com/t/how-to-set-up-a-remote-mcp-server-and-connect-it-to-chatgpt-deep-research/1278375) \nand [MCP server tools now in ChatGPT – developer mode](https://community.openai.com/t/mcp-server-tools-now-in-chatgpt-developer-mode/1357233) for more info.\n\n### Claude Code\n\nRun the following command.\nSee [Claude Code MCP docs](https://docs.anthropic.com/en/docs/claude-code/mcp) for more info.\n\n#### Local Server\n```bash\nclaude mcp add driflyte -- npx -y @driflye/mcp-server\n```\n\n#### Remote Server\n```bash\nclaude mcp add --transport http driflyte https://mcp.driflyte.com/mcp\n```\n\n### Claude Desktop\n\n#### Local Server\nAdd the following configuration into the `claude_desktop_config.json` file.\nSee the [Claude Desktop MCP docs](https://modelcontextprotocol.io/docs/develop/connect-local-servers) for more info.\n```json\n{\n  \"mcpServers\": {\n    \"driflyte\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@driflyte/mcp-server\"]\n    }\n  }\n}\n```\n\n#### Remote Server\nGo to the `Settings` > `Connectors` > `Add Custom Connector` in the Claude Desktop and add the new MCP server with the following fields: \n- Name: `Driflyte` \n- Remote MCP server URL: `https://mcp.driflyte.com/mcp`\n\n### Copilot Coding Agent\n\nAdd the following configuration to the `mcpServers` section of your Copilot Coding Agent configuration through \n`Repository` > `Settings` > `Copilot` > `Coding agent` > `MCP configuration`.\nSee the [Copilot Coding Agent MCP docs](https://docs.github.com/en/enterprise-cloud@latest/copilot/how-tos/agents/copilot-coding-agent/extending-copilot-coding-agent-with-mcp) for more info.\n\n#### Local Server\n```json\n{\n  \"mcpServers\": {\n    \"driflyte\": {\n      \"type\": \"local\",\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@driflyte/mcp-server\"]\n    }\n  }\n}\n```\n\n#### Remote Server\n```json\n{\n  \"mcpServers\": {\n    \"driflyte\": {\n      \"type\": \"http\",\n      \"url\": \"https://mcp.driflyte.com/mcp\"\n    }\n  }\n}\n```\n\n### Cursor\n\nAdd the following configuration into the `~/.cursor/mcp.json` file (or `.cursor/mcp.json` in your project folder).\nOr setup by 🖱️[One Click Installation](https://cursor.com/en/install-mcp?name=driflyte&config=eyJjb21tYW5kIjoibnB4IiwiYXJncyI6WyIteSIsIkBkcmlmbHl0ZS9tY3Atc2VydmVyIl19).\nSee the [Cursor MCP docs](https://docs.cursor.com/context/model-context-protocol) for more info.\n\n#### Local Server\n```json\n{\n  \"mcpServers\": {\n    \"driflyte\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@driflyte/mcp-server\"]\n    }\n  }\n}\n```\n\n#### Remote Server\n```json\n{\n  \"mcpServers\": {\n    \"driflyte\": {\n      \"url\": \"https://mcp.driflyte.com/mcp\"\n    }\n  }\n}\n```\n\n### Gemini CLI\n\nAdd the following configuration into the `~/.gemini/settings.json` file.\nSee the [Gemini CLI MCP docs](https://google-gemini.github.io/gemini-cli/docs/tools/mcp-server.html) for more info.\n\n#### Local Server\n```json\n{\n  \"mcpServers\": {\n    \"driflyte\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@driflyte/mcp-server\"]\n    }\n  }\n}\n```\n\n#### Remote Server\n```json\n{\n  \"mcpServers\": {\n    \"driflyte\": {\n      \"httpUrl\": \"https://mcp.driflyte.com/mcp\"\n    }\n  }\n}\n```\n\n### Smithery\n\nRun the following command.\nYou can find your Smithery API key [here](https://smithery.ai/account/api-keys).\nSee the [Smithery CLI docs](https://smithery.ai/docs/concepts/cli) for more info.\n```bash\nnpx -y @smithery/cli install @serkan-ozal/driflyte-mcp-server --client <SMITHERY-CLIENT-NAME> --key <SMITHERY-API-KEY>\n```\n\n### VS Code\n\nAdd the following configuration into the `.vscode/mcp.json` file.\nOr setup by 🖱️[One Click Installation](https://insiders.vscode.dev/redirect?url=vscode%3Amcp%2Finstall%3F%7B%22name%22%3A%22driflyte%22%2C%22command%22%3A%22npx%22%2C%22args%22%3A%5B%22-y%22%2C%22%40driflyte%2Fmcp-server%22%5D%7D).\nSee the [VS Code MCP docs](https://code.visualstudio.com/docs/copilot/chat/mcp-servers) for more info.\n\n#### Local Server\n```json\n{\n  \"mcp\": {\n    \"servers\": {\n      \"driflyte\": {\n        \"type\": \"stdio\",\n        \"command\": \"npx\",\n        \"args\": [\"-y\", \"@driflyte/mcp-server\"]\n      }\n    }\n  }\n}\n```\n\n#### Remote Server\n```json\n{\n  \"mcp\": {\n    \"servers\": {\n      \"driflyte\": {\n        \"type\": \"http\",\n        \"url\": \"https://mcp.driflyte.com/mcp\"\n      }\n    }\n  }\n}\n```\n\n### Windsurf\n\nAdd the following configuration into the `~/.codeium/windsurf/mcp_config.json` file. \nSee the [Windsurf MCP docs](https://docs.windsurf.com/windsurf/cascade/mcp) for more info.\n\n#### Local Server\n```json\n{\n  \"mcpServers\": {\n    \"driflyte\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@driflyte/mcp-server\"]\n    }\n  }\n}\n```\n\n#### Remote Server\n```json\n{\n  \"mcpServers\": {\n    \"driflyte\": {\n      \"serverUrl\": \"https://mcp.driflyte.com/mcp\"\n    }\n  }\n}\n```\n\n\n## Components\n\n### Tools\n\n- `list-topics`: Returns a list of topics for which resources (web pages, etc ...) have been crawled and content is available. \n                 This allows AI assistants to discover the most relevant and up-to-date subject areas currently indexed by the crawler.\n  - **Input Schema**: No input parameter supported.\n  - **Output Schema**:\n    - `topics`:\n      - `Optinal`: `false`\n      - `Type`: `Array<string>`\n      - `Description`: List of the supported topics.\n- `search`: Given a list of topics and a user question, this tool retrieves the top-K most relevant documents from the crawled content. \n            It is designed to help AI assistants surface the most contextually appropriate and up-to-date information for a specific topic and query.\n            This enables more informed and accurate responses based on real-world, topic-tagged web content.\n  - **Input Schema**:\n    - `topics`\n      - `Optinal`: `false`\n      - `Type`: `Array<string>`\n      - `Description`: A list of one or more topic identifiers to constrain the search space.\n                       Only documents tagged with at least one of these topics will be considered.\n    - `query`\n      - `Optinal`: `false`\n      - `Type`: `string`\n      - `Description`: The natural language query or question for which relevant information is being sought.\n                       This will be used to rank documents by semantic relevance. \n    - `topK`\n      - `Optinal`: `true`\n      - `Type`: `number`\n      - `Default Value`: `10`\n      - `Min Value`: `1`\n      - `Max Value`: `30`\n      - `Description`: The maximum number of relevant documents to return.\n                       Results are sorted by descending relevance score.\n  - **Output Schema**:\n    - `documents`:\n      - `Optional`: `false`\n      - `Type`: `Array<Document>`\n      - `Description`: Matched documents to the search query.\n      - **Type**: `Document`:\n        - `content`\n          - `Optinal`: `false`\n          - `Type`: `string`\n          - `Description`: Related content (full or partial) of the matched document.\n        - `metadata`\n          - `Optinal`: `false`\n          - `Type`: `Map<string, any>`\n          - `Description`: Metadata of the document and related content in key-value format.\n        - `score`\n          - `Optinal`: `false`\n          - `Type`: `number`\n          - `Min Value`: `0`\n          - `Max Value`: `1`\n          - `Description`: Similarity score (between `0` and `1`) for the content of the document.\n\n### Resources\n\nN/A\n\n\n## Roadmap\n\n- Support more content types (`.pdf`, `.ppt`/`.pptx`, `.doc`/`.docx`, and many others applicable including audio and video file formats ...)\n- Integrate with more data sources (Slack, Teams, Google Docs/Drive, Confluence, JIRA, Zendesk, Salesforce, etc ...))\n- And more topics with their resources\n\n\n## Issues and Feedback\n\n[![Issues](https://img.shields.io/github/issues/serkan-ozal/driflyte-mcp-server.svg)](https://github.com/serkan-ozal/driflyte-mcp-server/issues?q=is%3Aopen+is%3Aissue)\n[![Closed issues](https://img.shields.io/github/issues-closed/serkan-ozal/driflyte-mcp-server.svg)](https://github.com/serkan-ozal/driflyte-mcp-server/issues?q=is%3Aissue+is%3Aclosed)\n\nPlease use [GitHub Issues](https://github.com/serkan-ozal/driflyte-mcp-server/issues) for any bug report, feature request and support.\n\n\n## Contribution\n\n[![Pull requests](https://img.shields.io/github/issues-pr/serkan-ozal/driflyte-mcp-server.svg)](https://github.com/serkan-ozal/driflyte-mcp-server/pulls?q=is%3Aopen+is%3Apr)\n[![Closed pull requests](https://img.shields.io/github/issues-pr-closed/serkan-ozal/driflyte-mcp-server.svg)](https://github.com/serkan-ozal/driflyte-mcp-server/pulls?q=is%3Apr+is%3Aclosed)\n[![Contributors](https://img.shields.io/github/contributors/serkan-ozal/driflyte-mcp-server.svg)]()\n\nIf you would like to contribute, please\n- Fork the repository on GitHub and clone your fork.\n- Create a branch for your changes and make your changes on it.\n- Send a pull request by explaining clearly what is your contribution.\n\n> Tip:\n> Please check the existing pull requests for similar contributions and\n> consider submit an issue to discuss the proposed feature before writing code.\n\n## License\n\nLicensed under [MIT](LICENSE).\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "crawled",
        "search",
        "indexed",
        "search data",
        "data extraction",
        "driflyte mcp"
      ],
      "category": "search--data-extraction"
    },
    "shopsavvy--shopsavvy-mcp-server": {
      "owner": "shopsavvy",
      "name": "shopsavvy-mcp-server",
      "url": "https://github.com/shopsavvy/shopsavvy-mcp-server",
      "imageUrl": "",
      "description": "Complete product and pricing data solution for AI assistants. Search for products by barcode/ASIN/URL, access detailed product metadata, access comprehensive pricing data from thousands of retailers, view and track price history, and more.",
      "stars": 3,
      "forks": 2,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-09-15T17:16:59Z",
      "readme_content": "# ShopSavvy Data API MCP Server\n\nA [Model Context Protocol](https://modelcontextprotocol.io/) (MCP) server that provides AI assistants with access to ShopSavvy's comprehensive product data, pricing information, and historical price tracking.\n\n## Overview\n\nThis MCP server enables AI assistants to:\n- **Look up products** by barcode, ASIN, URL, model number, or ShopSavvy ID\n- **Get current pricing** from multiple retailers\n- **Access historical pricing data** with date ranges\n- **Schedule products** for automatic price monitoring\n- **Track API usage** and credit consumption\n\n## Features\n\n### 🔍 Product Lookup Tools\n- `product_lookup` - Find products by various identifiers (barcode, ASIN, URL, etc.)\n- `product_lookup_batch` - Look up multiple products at once\n\n### 💰 Pricing Tools\n- `product_offers` - Get current offers from all retailers\n- `product_offers_retailer` - Get offers from a specific retailer\n- `product_price_history` - Get historical pricing data with date ranges\n\n### 📅 Scheduling Tools\n- `product_schedule` - Schedule products for automatic refresh (hourly/daily/weekly)\n- `product_unschedule` - Remove products from refresh schedule\n- `scheduled_products_list` - View all scheduled products\n\n### 📊 Analytics Tools\n- `api_usage` - View current API usage and credit consumption\n\n## Installation\n\n```bash\nnpm install @shopsavvy/mcp-server\n```\n\n## Configuration\n\n### 1. Get API Key\n\nFirst, get your ShopSavvy Data API key:\n\n1. Visit [https://shopsavvy.com/data](https://shopsavvy.com/data)\n2. Sign up and choose a subscription plan\n3. Create an API key in your dashboard\n4. Copy your API key (starts with `ss_live_` or `ss_test_`)\n\n### 2. Claude Desktop Setup\n\nAdd this to your Claude Desktop configuration file:\n\n**macOS**: `~/Library/Application Support/Claude/claude_desktop_config.json`\n**Windows**: `%APPDATA%\\\\Claude\\\\claude_desktop_config.json`\n\n```json\n{\n  \"mcpServers\": {\n    \"shopsavvy\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"@shopsavvy/mcp-server\"\n      ],\n      \"env\": {\n        \"SHOPSAVVY_API_KEY\": \"your_api_key_here\"\n      }\n    }\n  }\n}\n```\n\n### 3. Environment Variables\n\nSet your API key as an environment variable:\n\n```bash\nexport SHOPSAVVY_API_KEY=\"ss_live_your_key_here\"\n```\n\nOr create a `.env` file:\n```\nSHOPSAVVY_API_KEY=ss_live_your_key_here\n```\n\n## Usage Examples\n\n### Product Lookup\n```\nLook up the product with barcode 012345678901\n```\n\n### Current Pricing\n```\nGet current prices for ASIN B08N5WRWNW from all retailers\n```\n\n### Price History\n```\nGet price history for product 012345678901 from January 1-15, 2024\n```\n\n### Schedule Monitoring\n```\nSchedule daily price monitoring for products: 012345678901, B08N5WRWNW\n```\n\n## API Limits & Pricing\n\n- **Starter Plan**: 1,000 credits/month - $49/month\n- **Professional Plan**: 10,000 credits/month - $199/month\n- **Enterprise Plan**: 100,000 credits/month - $499/month\n\n### Credit Usage:\n- Product lookup: 1 credit per product found\n- Current offers (all retailers): 3 credits per product\n- Current offers (single retailer): 2 credits per product\n- Historical data: 3 credits + 1 credit per day of history\n- Scheduling: 1 credit per product scheduled\n\n## Development\n\n### Running Locally\n\n```bash\n# Clone the repository\ngit clone https://github.com/shopsavvy/shopsavvy-mcp-server\ncd shopsavvy-mcp-server\n\n# Install dependencies\nnpm install\n\n# Set your API key\nexport SHOPSAVVY_API_KEY=\"your_key_here\"\n\n# Test with MCP CLI\nnpm run dev\n\n# Or inspect with MCP Inspector\nnpm run inspect\n```\n\n### Building\n\n```bash\nnpm run build\n```\n\n## Error Handling\n\nThe server provides detailed error messages for:\n- Invalid API keys\n- Insufficient credits\n- Rate limiting\n- Invalid product identifiers\n- API service issues\n\n## Support\n\n- **Documentation**: [https://shopsavvy.com/data/documentation](https://shopsavvy.com/data/documentation)\n- **Dashboard**: [https://shopsavvy.com/data/dashboard](https://shopsavvy.com/data/dashboard)\n- **Issues**: [https://github.com/shopsavvy/shopsavvy-mcp-server/issues](https://github.com/shopsavvy/shopsavvy-mcp-server/issues)\n\n## Changelog\n\n### v1.0.0 (2025-07-28)\n\n🎉 **Initial Release**\n\n- **Features**: Complete ShopSavvy Data API integration with MCP support\n- **Product Tools**: Lookup by barcode, ASIN, URL, model number, or ShopSavvy ID\n- **Pricing Tools**: Current offers from all/specific retailers, historical pricing data\n- **Scheduling Tools**: Automatic product monitoring (hourly/daily/weekly)\n- **Analytics Tools**: API usage tracking and credit consumption monitoring\n- **npm Package**: Published as `@shopsavvy/mcp-server` under ShopSavvy organization\n- **TypeScript**: Full TypeScript support with proper error handling\n- **Documentation**: Comprehensive README with examples and configuration guides\n\n## License\n\nMIT License - see [LICENSE](LICENSE) file for details.\n\n---\n\nMade with ❤️ by [ShopSavvy](https://shopsavvy.com) - Empowering everyone to always get the best deal, every time.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "shopsavvy",
        "retailers",
        "search",
        "pricing data",
        "shopsavvy mcp",
        "search products"
      ],
      "category": "search--data-extraction"
    },
    "takashiishida--arxiv-latex-mcp": {
      "owner": "takashiishida",
      "name": "arxiv-latex-mcp",
      "url": "https://github.com/takashiishida/arxiv-latex-mcp",
      "imageUrl": "",
      "description": "Get the LaTeX source of arXiv papers to handle mathematical content and equations",
      "stars": 66,
      "forks": 7,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-10-04T06:58:54Z",
      "readme_content": "# arxiv-latex MCP Server\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![GitHub Release](https://img.shields.io/github/v/release/takashiishida/arxiv-latex-mcp)](https://github.com/takashiishida/arxiv-latex-mcp/releases)\n\n\nAn MCP server that enables [Claude Desktop](https://claude.ai/download), [Cursor](https://www.cursor.com/), or other MCP clients to directly access and process arXiv papers by fetching the LaTeX source. It uses [arxiv-to-prompt](https://github.com/takashiishida/arxiv-to-prompt) under the hood to handle downloading and processing the LaTeX.\n\nWhy use the LaTeX source instead of uploading PDFs? Many PDF chat applications often struggle with mathematical content and equation-heavy papers. By utilizing the original LaTeX source code from arXiv papers, the LLM can accurately understand and handle equations and notations. This approach is particularly valuable for fields like computer science, mathematics, and engineering where precise interpretation of mathematical expressions is crucial.\n\n## Installation\n\nIf you are using Claude Desktop and MacOS, you can utilize Desktop Extensions by double-clicking on the .dxt file to install.\nDownload the .dxt file from [here](https://github.com/takashiishida/arxiv-latex-mcp/releases/).\n\nOtherwise, you can manually add the following configuration to your config file:\n```json\n{\n  \"mcpServers\": {\n      \"arxiv-latex-mcp\": {\n          \"command\": \"uv\",\n          \"args\": [\n              \"--directory\",\n              \"/ABSOLUTE/PATH/TO/arxiv-latex-mcp\",\n              \"run\",\n              \"server/main.py\"\n          ]\n      }\n  }\n}\n```\n\nYou may need to replace the `command` field with the full path of `uv`: check this by running `which uv` (MacOS/Linux) or `where uv` (Windows).\n\nRestart the application after saving the above.\n\nFor Claude Desktop, click on the hammer icon, and you should see `get_paper_prompt` in the list of \"Available MCP tools\".\n\n## Example\nTry asking questions about a paper from arXiv, e.g., \"Explain the first theorem in 2202.00395\"\n\n<div align=\"center\">\n  \n</div>",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "latex",
        "arxiv",
        "equations",
        "arxiv latex",
        "arxiv papers",
        "latex source"
      ],
      "category": "search--data-extraction"
    },
    "the0807--GeekNews-MCP-Server": {
      "owner": "the0807",
      "name": "GeekNews-MCP-Server",
      "url": "https://github.com/the0807/GeekNews-MCP-Server",
      "imageUrl": "",
      "description": "An MCP Server that retrieves and processes news data from the GeekNews site.",
      "stars": 16,
      "forks": 5,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-01T05:44:01Z",
      "readme_content": "# GeekNews MCP Server\n\n[![smithery badge](https://smithery.ai/badge/@the0807/geeknews-mcp-server)](https://smithery.ai/server/@the0807/geeknews-mcp-server)\n\n이 프로젝트는 [GeekNews](https://news.hada.io)에서 아티클을 가져오는 Model Context Protocol(MCP) 서버입니다. Python으로 구현되었으며, BeautifulSoup을 사용하여 웹 스크래핑을 수행합니다. 서버 부하를 줄이기 위해 하루에 한 번 자동으로 데이터를 파싱하고 캐시에 저장하는 기능을 제공합니다.\n\n## 기능\n|    |    |\n|:-------------------------:|:-------------------------:|\n|||\n\n### 도구 (Tools)\n\n- `get_articles` 도구: GeekNews에서 아티클을 가져오는 기능\n  - 아티클 유형(top, new, ask, show)과 반환할 아티클 수를 지정할 수 있음\n  - 각 응답에는 제목, URL, 포인트, 작성자, 시간, 댓글 수, 순위 정보가 포함됨\n  - 캐시된 데이터를 사용하여 서버 부하 감소\n\n- `get_weekly_news` 도구: GeekNews에서 주간 뉴스를 가져오는 기능\n  - 특정 주간 뉴스 ID를 지정하거나 가장 최근 주간 뉴스를 가져올 수 있음\n  - 주간 뉴스의 제목, 번호, ID, 내용, URL, 아이템 목록 등의 정보를 제공\n  - 각 아이템에는 제목, URL, 순위 정보가 포함됨\n  - 캐시된 데이터를 사용하여 서버 부하 감소\n\n### 캐시 기능\n\n- 하루에 한 번 자동으로 데이터를 파싱하고 캐시에 저장\n- 캐시된 데이터가 유효한 경우 GeekNews 서버에 요청하지 않고 캐시된 데이터 사용\n- 캐시 데이터는 24시간 동안 유효하며, 이후 자동으로 갱신\n- 스케줄러가 주기적으로 캐시 유효성을 검사하고 필요시 갱신\n\n## 사용법\n\n- Smithery를 이용한 설치\n\n   🚀 [geeknews-mcp-server](https://smithery.ai/server/@the0807/geeknews-mcp-server)\n\n- MCP 설정 파일에 서버 정보를 추가\n\n   ```json\n   {\n   \"mcpServers\": {\n      \"geeknews-mcp-server\": {\n         \"command\": \"npx\",\n         \"args\": [\n         \"-y\",\n         \"@smithery/cli@latest\",\n         \"run\",\n         \"@the0807/geeknews-mcp-server\",\n         \"--key\",\n         \"smithery에서 발급 받은 키\"\n         ]\n      }\n   }\n   }\n   ```\n\n## 로컬 설치 방법\n\n1. Git Clone\n\n   ```bash\n   git clone https://github.com/the0807/GeekNews-MCP-Server\n   cd GeekNews-MCP-Server\n   ```\n\n2. uv로 환경 세팅\n\n   ```bash\n   uv sync\n   ```\n\n3. 가상환경 실행\n\n   ```bash\n   uv venv\n   source .venv/bin/activate\n   ```\n\n4. MCP Inspector로 서버 테스트\n\n   ```bash\n   uv run mcp\n   mcp dev main.py\n\n   # 터미널에 나오는 URL(MCP Inspector)로 접속하여 서버 테스트\n   ```\n\n## 코드 구조\n\n- `src/models.py`: 아티클 정보를 저장하는 데이터 클래스 정의\n- `src/parser.py`: GeekNews 웹사이트의 HTML을 파싱하여 아티클 정보를 추출\n- `src/client.py`: GeekNews 웹사이트에서 데이터를 가져오는 HTTP 클라이언트\n- `src/config.py`: 설정과 상수 정의\n- `src/cache.py`: 캐시 관리 기능 제공\n- `src/scheduler.py`: 주기적인 데이터 갱신 스케줄러\n- `src/server.py`: MCP 서버 구현\n- `main.py`: 서버 실행 진입점\n\n> [!Note]\n> - 이 서버는 GeekNews 웹사이트의 HTML 구조에 의존합니다. 웹사이트 구조가 변경되면 파싱 로직을 업데이트해야 할 수 있습니다.\n> - 캐시 데이터는 사용자의 홈 디렉토리 아래 `.cache/geeknews-mcp` 폴더에 저장됩니다.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "geeknews",
        "mcp",
        "retrieves",
        "geeknews mcp",
        "mcp server",
        "data geeknews"
      ],
      "category": "search--data-extraction"
    },
    "tianqitang1--enrichr-mcp-server": {
      "owner": "tianqitang1",
      "name": "enrichr-mcp-server",
      "url": "https://github.com/tianqitang1/enrichr-mcp-server",
      "imageUrl": "",
      "description": "A MCP server that provides gene set enrichment analysis using the Enrichr API",
      "stars": 8,
      "forks": 3,
      "license": "No License",
      "language": "JavaScript",
      "updated_at": "2025-09-17T19:49:19Z",
      "readme_content": "<!--\n * @Author: tianqitang1 Tianqi.Tang@ucsf.edu\n * @Date: 2025-06-03 14:18:58\n * @LastEditors: tianqitang1 Tianqi.Tang@ucsf.edu\n * @LastEditTime: 2025-06-29 06:46:59\n * @FilePath: /enrichr-mcp-server/README.md\n-->\n# Enrichr MCP Server\n\n<div align=\"center\">\n  \n</div>\n\nA Model Context Protocol (MCP) server that provides gene set enrichment analysis using the [Enrichr](https://maayanlab.cloud/Enrichr/) API. This server supports all available gene set libraries from Enrichr and returns only statistically significant results (corrected-$p$ < 0.05) for LLM tools to interpret.\n\n## Installation\n\nFor Claude Desktop, please download the [Desktop Extension](https://github.com/tianqitang1/enrichr-mcp-server/releases/latest) and install it by clicking `☰ (top left) -> File -> Settings` and drag and drop the downloaded file into the `Settings` window.\n\nUse the button below to install the MCP server to Cursor, VS Code, or VS Code Insiders with default settings.\n\n[![Install MCP Server](https://cursor.com/deeplink/mcp-install-dark.svg)](https://cursor.com/install-mcp?name=enrichr-mcp-server&config=eyJjb21tYW5kIjoibnB4IC15IGVucmljaHItbWNwLXNlcnZlciAtLWNvbXBhY3QgLS1tYXgtdGVybXMgMTAwIn0%3D)\n[![Add to VS Code](https://img.shields.io/badge/Add_to_VS_Code-007ACC?style=for-the-badge&logo=visualstudiocode&logoColor=white)](https://vscode.dev/redirect?url=vscode%3Amcp/install%3F%257B%2522name%2522%253A%2522enrichr-mcp-server%2522%252C%2522command%2522%253A%2522npx%2522%252C%2522args%2522%253A%255B%2522-y%2522%252C%2522enrichr-mcp-server%2522%252C%2522--compact%2522%252C%2522--max-terms%2522%252C%2522100%2522%255D%257D)\n[![Add to VS Code Insiders](https://img.shields.io/badge/Add_to_VS_Code_Insiders-24bfa5?style=for-the-badge&logo=visualstudiocode&logoColor=white)](https://vscode.dev/redirect?url=vscode-insiders%3Amcp/install%3F%257B%2522name%2522%253A%2522enrichr-mcp-server%2522%252C%2522command%2522%253A%2522npx%2522%252C%2522args%2522%253A%255B%2522-y%2522%252C%2522enrichr-mcp-server%2522%252C%2522--compact%2522%252C%2522--max-terms%2522%252C%2522100%2522%255D%257D)\n\n\nFor Claude Code, use the following command:\n```bash\nclaude mcp add enrichr-mcp-server -- npx -y enrichr-mcp-server\n```\n\n## Features\n\n- **Multi-Library Enrichment Analysis**: Query multiple Enrichr libraries simultaneously (GO, pathways, diseases, tissues, drugs, etc.)\n- **Comprehensive Library Support**: Access to hundreds of gene set libraries from Enrichr including:\n  - Gene Ontology (Biological Process, Molecular Function, Cellular Component)\n  - Pathway databases (KEGG, Reactome, WikiPathways, BioCarta, MSigDB)\n  - Disease/Phenotype databases (Human Phenotype Ontology, GWAS Catalog)\n  - Tissue/Cell type libraries (GTEx, Human Cell Atlas, ARCHS4)\n  - Drug/Chemical libraries (DrugMatrix, L1000, TG-GATEs)\n  - Transcription Factor targets (ChEA, ENCODE)\n  - MicroRNA targets (TargetScan, miRTarBase)\n- **GO Enrichment Analysis**: Specialized tool for GO Biological Process enrichment analysis (I use this a lot, so I made it a tool)\n\n\n## Configuration\n\n### MCP Client Configuration\n\nAdd this server to your MCP client configuration (e.g., `.cursor/mcp.json`):\n\n#### Basic Configuration (Popular Libraries by Default)\nWith the default configuration the server will query a curated list of popular libraries.\n```json\n{\n  \"mcpServers\": {\n    \"enrichr-server\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"enrichr-mcp-server\"]\n    }\n  }\n}\n```\n\n#### Custom Available Libraries Configuration\n\nYou can configure libraries that are available for the LLM to use using CLI arguments in your MCP configuration:\n\n```json\n{\n  \"mcpServers\": {\n    \"enrichr-popular\": {\n      \"command\": \"npx\", \n      \"args\": [\"-y\", \"enrichr-mcp-server\", \"--libraries\", \"pop\"]\n    }, // This will make the most popular libraries available to the LLM, namely GO_Biological_Process_2025, KEGG_2021_Human, Reactome_2022, MSigDB_Hallmark_2020, ChEA_2022, GWAS_Catalog_2023, Human_Phenotype_Ontology, STRING_Interactions_2023, DrugBank_2022, CellMarker_2024\n    \"enrichr-pathways\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"enrichr-mcp-server\", \"-l\", \"GO_Biological_Process_2025,KEGG_2021_Human,Reactome_2022\"]\n    },\n    \"enrichr-disease\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"enrichr-mcp-server\", \"--libraries\", \"Human_Phenotype_Ontology,OMIM_Disease,ClinVar_2019\"]\n    }\n  }\n}\n```\n\n### Command Line Options\n\nAdjust the CLI options to your needs, unreasonable settings might exceed the context window of the LLM and confuse it, so choose wisely:\n\n| Option | Short | Description | Default |\n|--------|-------|-------------|---------|\n| `--libraries <libs>` | `-l` | Comma-separated list of Enrichr libraries to query | `pop` |\n| `--max-terms <num>` | `-m` | Maximum terms to show per library | `50` |\n| `--format <format>` | `-f` | Output format: `detailed`, `compact`, `minimal` | `detailed` |\n| `--output <file>` | `-o` | Save complete results to TSV file | _(none)_ |\n| `--compact` | `-c` | Use compact format (same as `--format compact`) | _(flag)_ |\n| `--minimal` | | Use minimal format (same as `--format minimal`) | _(flag)_ |\n| `--help` | `-h` | Show help message | _(flag)_ |\n\n#### Format Options\n- **`detailed`**: Full details including p-values, odds ratios, and gene lists (default)\n- **`compact`**: Term name + p-value + gene count (saves ~50% tokens)\n- **`minimal`**: Just term name + p-value (saves ~80% tokens)\n\n#### Examples\n\nFor a full list of commands, options, and usage examples, run the server with the `--help` flag. This is the most up-to-date source of information.\n\n```bash\n# Show the help message\nnpx enrichr-mcp-server --help\n```\n\n### Environment Variables\n\nYou can also configure the server via environment variables:\n\n| Variable | Description | Example |\n|----------|-------------|---------|\n| `ENRICHR_LIBRARIES` | Comma-separated list of libraries to query | `GO_Biological_Process_2025,KEGG_2021_Human` |\n| `ENRICHR_MAX_TERMS` | Maximum terms per library | `20` |\n| `ENRICHR_FORMAT` | Output format (`detailed`/`compact`/`minimal`) | `compact` |\n| `ENRICHR_OUTPUT_FILE` | TSV output file path | `/tmp/enrichr_results.tsv` |\n\n**Note**: CLI arguments take precedence over environment variables when both are specified.\n\n\n### Popular Libraries\n\nThis table lists the libraries included when using the `-l pop` flag.\n\n| Library | Description |\n|---------|-------------|\n| `GO_Biological_Process_2025` | Gene Ontology terms describing biological objectives accomplished by gene products. |\n| `KEGG_2021_Human` | Metabolic and signaling pathways from Kyoto Encyclopedia of Genes and Genomes for human. |\n| `Reactome_2022` | Curated and peer-reviewed pathways from Reactome covering signaling, metabolism, gene expression, and disease. |\n| `MSigDB_Hallmark_2020` | Hallmark gene sets representing well-defined biological states and processes from MSigDB. |\n| `ChEA_2022` | ChIP-seq experiments from GEO, ENCODE, and publications identifying transcription factor-gene interactions from human and mouse. |\n| `GWAS_Catalog_2023` | Genome-wide association study results from NHGRI-EBI GWAS Catalog linking genes to traits. |\n| `Human_Phenotype_Ontology` | Standardized vocabulary of phenotypic abnormalities associated with human diseases. |\n| `STRING_Interactions_2023` | Protein interactions from STRING database including experimental and predicted. |\n| `DrugBank_2022` | Drug targets from DrugBank including approved drugs and experimental compounds. |\n| `CellMarker_2024` | Manually curated cell type markers from CellMarker database for human and mouse. |\n\nFor a complete list of available libraries, visit the [Enrichr Libraries page](https://maayanlab.cloud/Enrichr/#libraries).\n\n### Benefits of Library Configuration\n\n1. **Simplified Tool Calls**: When libraries aren't specified in tool calls, your configured libraries are used\n2. **Consistent Results**: Ensures consistent library usage across different queries  \n3. **Multiple Configurations**: Set up different MCP server instances for different research contexts\n4. **Override Capability**: Individual tool calls can still specify different libraries when needed\n\n## Usage\n\nThe server provides two tools:\n\n### `enrichr_analysis` (Recommended for multi-library analysis)\n\nPerforms enrichment analysis across multiple specified Enrichr libraries.\n\n**Parameters:**\n- `genes` (required): Array of gene symbols (e.g., [\"TP53\", \"BRCA1\", \"EGFR\"])\n- `libraries` (optional): Array of Enrichr library names to query (defaults to configured libraries)\n- `description` (optional): Description for the gene list (default: \"Gene list for enrichment analysis\")\n- `maxTerms` (optional): Maximum number of terms to show per library (default: 50)\n- `format` (optional): Output format: `detailed`, `compact`, `minimal` (default: `detailed`)\n- `outputFile` (optional): Path to save complete results as TSV file\n\n### `go_bp_enrichment` \n\nPerforms Gene Ontology (GO) Biological Process enrichment analysis to understand biological functions and processes overrepresented in your gene list. Perfect for interpreting gene expression data, identifying significant biological processes, and uncovering functional implications of genes from RNA-seq, microarray, or other high-throughput experiments.\n\n**Parameters:**\n- `genes` (required): Array of gene symbols (e.g., [\"TP53\", \"BRCA1\", \"EGFR\"])\n- `description` (optional): Description for the gene list (default: \"Gene list for GO BP enrichment\")\n- `outputFile` (optional): Path to save complete results as TSV file\n\n**Returns:**\nAll tools return formatted text with significant terms including:\n- Library name and summary statistics\n- Term name and identifier\n- Adjusted P-value and raw P-value (scientific notation)\n- Odds ratio and combined score\n- Overlapping genes with counts\n\n## Available Library Categories\n\nEnrichr contains hundreds of gene set libraries organized into categories:\n\n- **Gene Ontology**: Biological processes, molecular functions, cellular components\n- **Pathways**: KEGG, Reactome, WikiPathways, BioCarta, NCI, HumanCyc, Panther\n- **Disease/Phenotype**: HPO, OMIM, ClinVar, GWAS Catalog, DisGeNET\n- **Tissues/Cell Types**: GTEx, Human Cell Atlas, ARCHS4, Mouse Gene Atlas\n- **Transcription Factors**: ChEA, ENCODE, TRANSFAC, JASPAR\n- **MicroRNA Targets**: TargetScan, miRTarBase, microRNA.org\n- **Drug/Chemical**: DrugMatrix, L1000, TG-GATEs, CTD\n- **Protein Interactions**: BioGRID, STRING, hu.MAP\n- **Literature Mining**: PubMed, Geneshot, Co-expression\n- **Evolutionary**: Cross-species homologs, phylogenetic profiles\n\nFor a complete list of available libraries, visit the [Enrichr Libraries page](https://maayanlab.cloud/Enrichr/#libraries).\n\n## API Details\n\nThis server uses the Enrichr API:\n- **Add List Endpoint**: `https://maayanlab.cloud/Enrichr/addList`\n- **Enrichment Endpoint**: `https://maayanlab.cloud/Enrichr/enrich`\n- **Supported Libraries**: All libraries available through Enrichr web interface\n\n## Development\n\n- **Build**: `npm run build`\n- **Watch**: `npm run watch` (rebuilds on file changes)\n- **Inspector**: `npm run inspector` (debug with MCP inspector)\n\n## Requirements\n\n- Node.js 18+\n- TypeScript 5.3+\n- Internet connection for Enrichr API access\n\n## License\n\nThis project follows the same license as the MCP TypeScript SDK.\n\n## References\n\n- Chen EY, Tan CM, Kou Y, Duan Q, Wang Z, Meirelles GV, Clark NR, Ma'ayan A. Enrichr: interactive and collaborative HTML5 gene list enrichment analysis tool. BMC Bioinformatics. 2013; 128(14).\n\n- Kuleshov MV, Jones MR, Rouillard AD, Fernandez NF, Duan Q, Wang Z, Koplev S, Jenkins SL, Jagodnik KM, Lachmann A, McDermott MG, Monteiro CD, Gundersen GW, Ma'ayan A. Enrichr: a comprehensive gene set enrichment analysis web server 2016 update. Nucleic Acids Research. 2016; gkw377.\n\n- Xie Z, Bailey A, Kuleshov MV, Clarke DJB., Evangelista JE, Jenkins SL, Lachmann A, Wojciechowicz ML, Kropiwnicki E, Jagodnik KM, Jeon M, & Ma'ayan A. Gene set knowledge discovery with Enrichr. Current Protocols, 1, e90. 2021. doi: 10.1002/cpz1.90\n\n- [Enrichr](https://maayanlab.cloud/Enrichr/)\n- [Model Context Protocol](https://modelcontextprotocol.io/)\n- [MCP TypeScript SDK](https://github.com/modelcontextprotocol/typescript-sdk)",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "gene",
        "enrichment",
        "enrichr",
        "enrichment analysis",
        "extraction tianqitang1",
        "using enrichr"
      ],
      "category": "search--data-extraction"
    },
    "tinyfish-io--agentql-mcp": {
      "owner": "tinyfish-io",
      "name": "agentql-mcp",
      "url": "https://github.com/tinyfish-io/agentql-mcp",
      "imageUrl": "",
      "description": "MCP server that provides [AgentQL](https://agentql.com)'s data extraction capabilities.",
      "stars": 115,
      "forks": 27,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-10-03T15:53:06Z",
      "readme_content": "# AgentQL MCP Server\n\nThis is a Model Context Protocol (MCP) server that integrates [AgentQL](https://agentql.com)'s data extraction capabilities.\n\n## Features\n\n### Tools\n\n- `extract-web-data` - extract structured data from a given 'url', using 'prompt' as a description of actual data and its fields to extract.\n\n## Installation\n\nTo use AgentQL MCP Server to extract data from web pages, you need to install it via npm, get an API key from our [Dev Portal](https://dev.agentql.com), and configure it in your favorite app that supports MCP.\n\n### Install the package\n\n```bash\nnpm install -g agentql-mcp\n```\n\n### Configure Claude\n\n- Open Claude Desktop **Settings** via `⌘`+`,` (don't confuse with Claude Account Settings)\n- Go to **Developer** sidebar section\n- Click **Edit Config** and open `claude_desktop_config.json` file\n- Add `agentql` server inside `mcpServers` dictionary in the config file\n- Restart the app\n\n```json title=\"claude_desktop_config.json\"\n{\n  \"mcpServers\": {\n    \"agentql\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"agentql-mcp\"],\n      \"env\": {\n        \"AGENTQL_API_KEY\": \"YOUR_API_KEY\"\n      }\n    }\n  }\n}\n```\n\nRead more about MCP configuration in Claude [here](https://modelcontextprotocol.io/quickstart/user).\n\n### Configure VS Code\n\nFor one-click installation, click one of the install buttons below:\n\n[![Install with NPX in VS Code](https://img.shields.io/badge/VS_Code-NPM-0098FF?style=flat-square&logo=visualstudiocode&logoColor=white)](https://insiders.vscode.dev/redirect/mcp/install?name=agentql&config=%7B%22command%22%3A%22npx%22%2C%22args%22%3A%5B%22-y%22%2C%22agentql-mcp%22%5D%2C%22env%22%3A%7B%22AGENTQL_API_KEY%22%3A%22%24%7Binput%3AapiKey%7D%22%7D%7D&inputs=%5B%7B%22type%22%3A%22promptString%22%2C%22id%22%3A%22apiKey%22%2C%22description%22%3A%22AgentQL+API+Key%22%2C%22password%22%3Atrue%7D%5D) [![Install with NPX in VS Code Insiders](https://img.shields.io/badge/VS_Code_Insiders-NPM-24bfa5?style=flat-square&logo=visualstudiocode&logoColor=white)](https://insiders.vscode.dev/redirect/mcp/install?name=agentql&config=%7B%22command%22%3A%22npx%22%2C%22args%22%3A%5B%22-y%22%2C%22agentql-mcp%22%5D%2C%22env%22%3A%7B%22AGENTQL_API_KEY%22%3A%22%24%7Binput%3AapiKey%7D%22%7D%7D&inputs=%5B%7B%22type%22%3A%22promptString%22%2C%22id%22%3A%22apiKey%22%2C%22description%22%3A%22AgentQL+API+Key%22%2C%22password%22%3Atrue%7D%5D&quality=insiders)\n\n#### Manual Installation\n\nClick the install buttons at the top of this section for the quickest installation method. For manual installation, follow these steps:\n\nAdd the following JSON block to your User Settings (JSON) file in VS Code. You can do this by pressing `Ctrl + Shift + P` and typing `Preferences: Open User Settings (JSON)`.\n\n```json\n{\n  \"mcp\": {\n    \"inputs\": [\n      {\n        \"type\": \"promptString\",\n        \"id\": \"apiKey\",\n        \"description\": \"AgentQL API Key\",\n        \"password\": true\n      }\n    ],\n    \"servers\": {\n      \"agentql\": {\n        \"command\": \"npx\",\n        \"args\": [\"-y\", \"agentql-mcp\"],\n        \"env\": {\n          \"AGENTQL_API_KEY\": \"${input:apiKey}\"\n        }\n      }\n    }\n  }\n}\n```\n\nOptionally, you can add it to a file called `.vscode/mcp.json` in your workspace. This will allow you to share the configuration with others.\n\n```json\n{\n  \"inputs\": [\n    {\n      \"type\": \"promptString\",\n      \"id\": \"apiKey\",\n      \"description\": \"AgentQL API Key\",\n      \"password\": true\n    }\n  ],\n  \"servers\": {\n    \"agentql\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"agentql-mcp\"],\n      \"env\": {\n        \"AGENTQL_API_KEY\": \"${input:apiKey}\"\n      }\n    }\n  }\n}\n```\n\n### Configure Cursor\n\n- Open **Cursor Settings**\n- Go to **MCP > MCP Servers**\n- Click **+ Add new MCP Server**\n- Enter the following:\n  - Name: \"agentql\" (or your preferred name)\n  - Type: \"command\"\n  - Command: `env AGENTQL_API_KEY=YOUR_API_KEY npx -y agentql-mcp`\n\nRead more about MCP configuration in Cursor [here](https://docs.cursor.com/context/model-context-protocol).\n\n### Configure Windsurf\n\n- Open **Windsurf: MCP Configuration Panel**\n- Click **Add custom server+**\n- Alternatively you can open `~/.codeium/windsurf/mcp_config.json` directly\n- Add `agentql` server inside `mcpServers` dictionary in the config file\n\n```json title=\"mcp_config.json\"\n{\n  \"mcpServers\": {\n    \"agentql\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"agentql-mcp\"],\n      \"env\": {\n        \"AGENTQL_API_KEY\": \"YOUR_API_KEY\"\n      }\n    }\n  }\n}\n```\n\nRead more about MCP configuration in Windsurf [here](https://docs.codeium.com/windsurf/mcp).\n\n### Validate MCP integration\n\nGive your agent a task that will require extracting data from the web. For example:\n\n```text\nExtract the list of videos from the page https://www.youtube.com/results?search_query=agentql, every video should have a title, an author name, a number of views and a url to the video. Make sure to exclude ads items. Format this as a markdown table.\n```\n\n> [!TIP]\n> In case your agent complains that it can't open urls or load content from the web instead of using AgentQL, try adding \"use tools\" or \"use agentql tool\" hint.\n\n## Development\n\nInstall dependencies:\n\n```bash\nnpm install\n```\n\nBuild the server:\n\n```bash\nnpm run build\n```\n\nFor development with auto-rebuild:\n\n```bash\nnpm run watch\n```\n\nIf you want to try out development version, you can use the following config instead of the default one:\n\n```json\n{\n  \"mcpServers\": {\n    \"agentql\": {\n      \"command\": \"/path/to/agentql-mcp/dist/index.js\",\n      \"env\": {\n        \"AGENTQL_API_KEY\": \"YOUR_API_KEY\"\n      }\n    }\n  }\n}\n```\n\n> [!NOTE]\n> Don't forget to remove the default AgentQL MCP server config to not confuse Claude with two similar servers.\n\n## Debugging\n\nSince MCP servers communicate over stdio, debugging can be challenging. We recommend using the [MCP Inspector](https://github.com/modelcontextprotocol/inspector), which is available as a package script:\n\n```bash\nnpm run inspector\n```\n\nThe Inspector will provide a URL to access debugging tools in your browser.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "agentql",
        "tinyfish",
        "mcp",
        "io agentql",
        "agentql mcp",
        "provides agentql"
      ],
      "category": "search--data-extraction"
    },
    "vitorpavinato--ncbi-mcp-server": {
      "owner": "vitorpavinato",
      "name": "ncbi-mcp-server",
      "url": "https://github.com/vitorpavinato/ncbi-mcp-server",
      "imageUrl": "",
      "description": "Comprehensive NCBI/PubMed literature search server with advanced analytics, caching, MeSH integration, related articles discovery, and batch processing for all life sciences and biomedical research.",
      "stars": 5,
      "forks": 1,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-09-27T13:58:49Z",
      "readme_content": "# NCBI Literature Search MCP Server\n\nA Model Context Protocol (MCP) server for searching NCBI databases, designed for researchers across all life sciences and biomedical fields. This server provides seamless access to PubMed's vast collection of 35+ million scientific articles through natural language queries, enabling AI assistants to help with literature reviews, research discovery, and scientific analysis.\n\n## Features\n\n🔬 **Comprehensive Search**: Search PubMed's 35+ million articles across all biological disciplines\n📊 **Advanced Queries**: Support for complex searches with boolean operators, field tags, and filters  \n🧬 **Life Sciences Research**: Covers all biological and biomedical fields including genetics, ecology, medicine, and biotechnology\n💻 **Computational Biology**: Perfect for finding bioinformatics methods, algorithms, and computational tools\n🔬 **Research Applications**: Literature reviews, hypothesis generation, method discovery, and staying current with scientific advances\n📚 **Full Article Details**: Get abstracts, author lists, MeSH terms, DOIs, and publication information\n🔗 **Related Articles**: Discover relevant research through NCBI's relationship algorithms\n📖 **MeSH Integration**: Search and utilize Medical Subject Headings for precise terminology\n\n## Quick Start\n\n### Prerequisites\n- Python 3.8 or higher\n- Poetry (recommended) - [Install Poetry](https://python-poetry.org/docs/#installation)\n\n### Setup (5 minutes)\n\n1. **Create and initialize project**\n   ```bash\n   mkdir ncbi-mcp-server && cd ncbi-mcp-server\n   poetry init\n   ```\n   During init, add dependencies: `mcp`, `httpx`, `typing-extensions`\n\n2. **Create project structure**\n   ```bash\n   mkdir -p src/ncbi_mcp_server\n   # Save server.py code as src/ncbi_mcp_server/server.py\n   ```\n\n3. **Install dependencies**\n   ```bash\n   poetry install\n   ```\n\n4. **Test the server**\n   ```bash\n   poetry run python src/ncbi_mcp_server/server.py\n   ```\n\n5. **Configure Claude Desktop**\n   \n   Edit your Claude Desktop config file:\n   - **macOS**: `~/Library/Application Support/Claude/claude_desktop_config.json`\n   - **Windows**: `%APPDATA%/Claude/claude_desktop_config.json`\n   - **Linux**: `~/.config/claude/claude_desktop_config.json`\n\n   Add this configuration:\n   ```json\n   {\n     \"mcpServers\": {\n       \"ncbi-literature\": {\n         \"command\": \"poetry\",\n         \"args\": [\"run\", \"python\", \"src/ncbi_mcp_server/server.py\"],\n         \"cwd\": \"/FULL/PATH/TO/YOUR/ncbi-mcp-server\"\n       }\n     }\n   }\n   ```\n\n6. **Restart Claude Desktop** and start searching!\n\n### Alternative Setup Methods\n\n<details>\n<summary>Click to expand alternative installation methods</summary>\n\n#### **Conda Environment**\n```bash\nconda env create -f environment.yml\nconda activate ncbi-mcp\npython server.py\n```\n\n#### **Standard pip + venv**\n```bash\npython -m venv venv\nsource venv/bin/activate  # Linux/macOS\npip install -r requirements.txt\npython server.py\n```\n</details>\n\n## Usage Examples\n\n### For Evolutionary Biology Research\n\n**Search for phylogenetic studies:**\n```\n\"Search for recent phylogenetic analysis papers on mammalian evolution\"\n→ Uses: search_pubmed with query \"phylogenetic analysis[ti] AND mammalian[ti] AND evolution\"\n```\n\n**Find computational phylogenetics methods:**\n```\n\"Find papers about maximum likelihood methods for phylogenetic reconstruction\"\n→ Uses: search_pubmed with query \"maximum likelihood[ti] AND phylogenetic reconstruction\"\n```\n\n**Search by specific organism:**\n```\n\"Find recent papers on Drosophila comparative genomics\"\n→ Uses: search_pubmed with query \"Drosophila[ti] AND comparative genomics[ti]\"\n```\n\n### For Computational Biology Research\n\n**Algorithm and method papers:**\n```\n\"Search for machine learning applications in genomics from the last 2 years\"\n→ Uses: search_pubmed with date_range=\"730\" and query \"machine learning AND genomics\"\n```\n\n**Software and database papers:**\n```\n\"Find papers about new bioinformatics tools for sequence analysis\"\n→ Uses: search_pubmed with query \"bioinformatics[ti] AND software[ti] AND sequence analysis\"\n```\n\n### Advanced Search Examples\n\n**Multi-criteria search:**\n```\n\"Find review articles about CRISPR applications in evolutionary studies published in Nature or Science\"\n→ Uses: advanced_search with terms=[\"CRISPR\", \"evolution\"], publication_types=[\"Review\"], journals=[\"Nature\", \"Science\"]\n```\n\n**Author-specific searches:**\n```\n\"Find recent papers by researchers working on ancient DNA and phylogenomics\"\n→ Uses: search_pubmed with query \"ancient DNA[ti] AND phylogenomics[ti]\"\n```\n\n## Tool Reference\n\n### `search_pubmed`\nPrimary search tool for PubMed database\n- **query**: Search terms (supports field tags like `[ti]` for title, `[au]` for author, `[mh]` for MeSH terms)\n- **max_results**: Number of results (1-100, default: 20)\n- **sort**: Sort by \"relevance\", \"pub_date\", \"author\", or \"journal\"\n- **date_range**: Limit to recent articles (\"30\", \"90\", \"365\", \"1095\" days)\n\n**Examples:**\n- `\"CRISPR[ti] AND evolution\"` - CRISPR in title AND evolution anywhere\n- `\"phylogenetic analysis[mh]\"` - Using MeSH term for phylogenetic analysis\n- `\"computational biology AND machine learning\"` - Boolean search\n\n### `get_article_details`\nFetch complete information for specific articles\n- **pmids**: List of PubMed IDs (up to 50)\n\nReturns full abstracts, author lists, MeSH terms, DOI, publication details\n\n### `search_mesh_terms`\nFind standardized Medical Subject Headings\n- **term**: Term to search in MeSH database\n\nHelps discover related concepts and improve search precision\n\n### `get_related_articles`\nDiscover articles related to a specific paper\n- **pmid**: PubMed ID of reference article\n- **max_results**: Number of related articles (1-50, default: 10)\n\nPerfect for literature reviews and finding relevant research\n\n### `advanced_search`\nComplex searches with multiple criteria\n- **terms**: List of search terms to combine\n- **operator**: \"AND\", \"OR\", or \"NOT\" to combine terms\n- **authors**: List of author names\n- **journals**: List of journal names\n- **publication_types**: \"Research Article\", \"Review\", \"Meta-Analysis\", etc.\n- **date_from/date_to**: Date range in YYYY/MM/DD format\n- **max_results**: Number of results (1-100, default: 20)\n\n## Analytics & Performance Monitoring\n\nThe NCBI MCP Server includes comprehensive analytics to help you understand your research patterns and optimize performance.\n\n### Analytics Tools\n\n#### `get_analytics_summary`\nGet comprehensive analytics overview\n```\n\"Show me my research analytics summary\"\n```\nReturns:\n- Total requests and uptime\n- Operation breakdown (searches, fetches, etc.)\n- Cache performance metrics\n- Recent activity and error rates\n- System health indicators\n\n#### `get_detailed_metrics`\nDetailed performance metrics for specific time periods\n```\n\"Get detailed metrics for the last 24 hours\"\n```\n- **hours**: Time period to analyze (default: 24)\n- Operation-specific performance data\n- Timeline analysis with hourly breakdowns\n- Error rates and response times per operation\n\n#### `reset_analytics`\nReset analytics data (use with caution)\n```\n\"Reset all analytics data\"\n```\n**Note**: This permanently clears all collected metrics.\n\n### What's Tracked\n\n**Usage Patterns:**\n- Search queries and frequency\n- Most used operations\n- Unique vs. repeated queries\n- Peak usage periods\n\n**Performance Metrics:**\n- Response times for each operation\n- Cache hit/miss rates\n- Error rates and types\n- Rate limiting efficiency\n\n**Research Insights:**\n- Popular search terms and patterns\n- Research workflow analysis\n- Literature access patterns\n- Most accessed journals and topics\n\n## Deployment\n\n### Quick Start\n\n1. **Configure credentials:**\n   ```bash\n   cp .env.example .env\n   # Edit .env with your NCBI email and API key\n   ```\n\n2. **Choose deployment method:**\n   ```bash\n   # Local development\n   ./deploy.sh local\n   \n   # Docker deployment\n   ./deploy.sh docker\n   \n   # Production deployment\n   ./deploy.sh production\n   ```\n\n### Deployment Options\n\n#### 1. Local Development\nPerfect for development and testing:\n```bash\npoetry install\npoetry run python -m src.ncbi_mcp_server.server\n```\n\n#### 2. Docker Deployment\nRecommended for most users with two options:\n\n**Full setup with Redis (recommended):**\n```bash\n# Copy and configure environment\ncp .env.example .env\n# Edit .env with your NCBI email and API key\n\n# Start all services\ndocker-compose up -d\n```\n\n**Simple setup without Redis:**\n```bash\n# For basic usage without Redis dependencies\ncp .env.example .env\n# Edit .env with your NCBI email\n\ndocker-compose -f docker-compose.simple.yml up -d\n```\n\n**Full setup includes:**\n- NCBI MCP Server container\n- Redis cache for performance\n- Redis Commander UI (http://localhost:8081)\n\n**Simple setup includes:**\n- NCBI MCP Server container only\n- In-memory caching (no persistence)\n\n#### 3. Production Deployment\nFor production environments:\n```bash\n# Configure production settings\ncp .env.production .env\n# Edit with production values\n\n# Deploy\n./deploy.sh production\n```\n\n### Monitoring\n\n**Docker logs:**\n```bash\ndocker-compose logs -f ncbi-mcp-server\n```\n\n**Cache monitoring:**\n- Redis Commander: http://localhost:8081\n- Cache stats via MCP tool: `cache_stats()`\n\n**Health checks:**\n```bash\n# Test server health\ncurl http://localhost:8000/health\n\n# Test via MCP\npython -c \"from src.ncbi_mcp_server.server import cache_stats; import asyncio; print(asyncio.run(cache_stats()))\"\n```\n\n## Configuration\n\n### NCBI API Key (Optional but Recommended)\nFor higher rate limits and better performance:\n\n1. **Register at NCBI**: https://www.ncbi.nlm.nih.gov/account/\n2. **Get API key**: https://www.ncbi.nlm.nih.gov/account/settings/\n3. **Add to server code** in `src/ncbi_mcp_server/server.py`:\n\n```python\n# Replace the line: ncbi_client = NCBIClient()\n# With:\nncbi_client = NCBIClient(\n    email=\"your.email@university.edu\",\n    api_key=\"your_api_key_here\"\n)\n```\n\n### Rate Limits\n- **Without API key**: 3 requests/second\n- **With API key**: 10 requests/second  \n- **With API key + email**: Higher limits for bulk requests\n\n## Development Workflow\n\n### Poetry Commands\n```bash\npoetry shell              # Activate virtual environment\npoetry add package        # Add new dependency\npoetry remove package     # Remove dependency\npoetry update            # Update all dependencies\npoetry run python ...    # Run commands in environment\npoetry build             # Create distribution packages\n```\n\n### Code Quality (if you added dev dependencies)\n```bash\npoetry add --group dev black mypy pytest isort flake8\npoetry run black .       # Format code\npoetry run mypy .        # Type checking  \npoetry run pytest       # Run tests\npoetry run isort .       # Sort imports\n```\n\n### Sharing with Colleagues\n```bash\n# They just need:\ngit clone your-repo\ncd ncbi-mcp-server  \npoetry install\n# Everything works identically!\n```\n\n## Field Tags for Advanced Searches\n\nPubMed supports many field tags for precise searching:\n\n- `[ti]` - Title\n- `[tiab]` - Title and Abstract  \n- `[au]` - Author\n- `[mh]` - MeSH Terms\n- `[journal]` - Journal Name\n- `[pdat]` - Publication Date\n- `[pt]` - Publication Type\n- `[lang]` - Language\n- `[sb]` - Subset (e.g., medline, pubmed)\n\n**Example Advanced Queries:**\n```\n\"machine learning\"[ti] AND \"phylogen*\"[tiab] AND \"2020\"[pdat]:\"2024\"[pdat]\nevolutionary[mh] AND computational[ti] AND (genomics[tiab] OR proteomics[tiab])\n\"ancient DNA\"[ti] AND (paleogenomics[mh] OR phylogenomics[tiab])\n```\n\n## Research Workflow Examples\n\n### Literature Review Workflow\n1. **Start broad**: `search_pubmed(\"computational phylogenetics\")`\n2. **Refine with MeSH**: `search_mesh_terms(\"phylogenetics\")` \n3. **Find key papers**: Use publication dates and journal filters\n4. **Explore connections**: `get_related_articles(pmid=\"key_paper_id\")`\n5. **Deep dive**: `get_article_details(pmids=[\"12345\", \"67890\"])`\n\n### Staying Current\n1. **Recent methods**: `search_pubmed(\"new methods\", date_range=\"90\")`\n2. **Follow key authors**: `search_pubmed(\"author_name[au]\", sort=\"pub_date\")`\n3. **Track specific topics**: `advanced_search` with your research keywords\n\n### Method Discovery\n1. **Algorithm papers**: `search_pubmed(\"algorithm[ti] AND your_field\")`\n2. **Software tools**: `search_pubmed(\"software[ti] OR tool[ti] AND bioinformatics\")`\n3. **Benchmarking**: `search_pubmed(\"comparison[ti] OR benchmark[ti]\")`\n\n## Troubleshooting\n\n### Common Issues\n\n**Server won't start:**\n- Check Python version (3.8+ required)\n- Install dependencies: `pip install -r requirements.txt`\n- Verify file permissions\n\n**No search results:**\n- Check query syntax (use proper field tags)\n- Try broader search terms\n- Verify internet connection\n\n**Rate limit errors:**\n- Add delays between requests\n- Get NCBI API key for higher limits\n- Consider searching fewer results per query\n\n**XML parsing errors:**\n- Usually temporary NCBI server issues\n- Retry after a few seconds\n- Check NCBI status: https://www.ncbi.nlm.nih.gov/\n\n### Getting Help\n\n- **NCBI E-utilities documentation**: https://www.ncbi.nlm.nih.gov/books/NBK25499/\n- **PubMed search tips**: https://pubmed.ncbi.nlm.nih.gov/help/\n- **MeSH database**: https://www.ncbi.nlm.nih.gov/mesh/\n\n## Contributing\n\nThis MCP server is designed to grow with the research community. Ideas for enhancement:\n\n- **Additional databases**: PMC, BioRxiv, databases beyond NCBI\n- **Citation analysis**: Track paper impact and citation networks  \n- **Export formats**: BibTeX, EndNote, RIS for reference managers\n- **Saved searches**: Persistent search profiles and alerts\n- **Full-text integration**: When available through PMC\n\n## License\n\nThis project is open source. Feel free to modify and distribute according to your institution's policies.\n\n---\n\n**Perfect for researchers in:**\n- Evolutionary Biology & Phylogenetics\n- Computational Biology & Bioinformatics  \n- Molecular Evolution & Population Genetics\n- Comparative Genomics & Proteomics\n- Systems Biology & Network Analysis\n- Biostatistics & Mathematical Biology\n- Ancient DNA & Paleogenomics\n- Conservation Genetics & Ecology\n\nStart exploring the vast world of biological literature with powerful, precise searches!",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "ncbi",
        "biomedical",
        "pubmed",
        "ncbi pubmed",
        "comprehensive ncbi",
        "biomedical research"
      ],
      "category": "search--data-extraction"
    },
    "webscraping-ai--webscraping-ai-mcp-server": {
      "owner": "webscraping-ai",
      "name": "webscraping-ai-mcp-server",
      "url": "https://github.com/webscraping-ai/webscraping-ai-mcp-server",
      "imageUrl": "",
      "description": "Interact with [WebScraping.ai](https://webscraping.ai) for web data extraction and scraping.",
      "stars": 31,
      "forks": 11,
      "license": "No License",
      "language": "JavaScript",
      "updated_at": "2025-09-27T23:32:52Z",
      "readme_content": "# WebScraping.AI MCP Server\n\nA Model Context Protocol (MCP) server implementation that integrates with [WebScraping.AI](https://webscraping.ai) for web data extraction capabilities.\n\n## Features\n\n- Question answering about web page content\n- Structured data extraction from web pages\n- HTML content retrieval with JavaScript rendering\n- Plain text extraction from web pages\n- CSS selector-based content extraction\n- Multiple proxy types (datacenter, residential) with country selection\n- JavaScript rendering using headless Chrome/Chromium\n- Concurrent request management with rate limiting\n- Custom JavaScript execution on target pages\n- Device emulation (desktop, mobile, tablet)\n- Account usage monitoring\n\n## Installation\n\n### Running with npx\n\n```bash\nenv WEBSCRAPING_AI_API_KEY=your_api_key npx -y webscraping-ai-mcp\n```\n\n### Manual Installation\n\n```bash\n# Clone the repository\ngit clone https://github.com/webscraping-ai/webscraping-ai-mcp-server.git\ncd webscraping-ai-mcp-server\n\n# Install dependencies\nnpm install\n\n# Run\nnpm start\n```\n\n### Configuring in Cursor\nNote: Requires Cursor version 0.45.6+\n\nThe WebScraping.AI MCP server can be configured in two ways in Cursor:\n\n1. **Project-specific Configuration** (recommended for team projects):\n   Create a `.cursor/mcp.json` file in your project directory:\n   ```json\n   {\n     \"servers\": {\n       \"webscraping-ai\": {\n         \"type\": \"command\",\n         \"command\": \"npx -y webscraping-ai-mcp\",\n         \"env\": {\n           \"WEBSCRAPING_AI_API_KEY\": \"your-api-key\",\n           \"WEBSCRAPING_AI_CONCURRENCY_LIMIT\": \"5\"\n         }\n       }\n     }\n   }\n   ```\n\n2. **Global Configuration** (for personal use across all projects):\n   Create a `~/.cursor/mcp.json` file in your home directory with the same configuration format as above.\n\n> If you are using Windows and are running into issues, try using `cmd /c \"set WEBSCRAPING_AI_API_KEY=your-api-key && npx -y webscraping-ai-mcp\"` as the command.\n\nThis configuration will make the WebScraping.AI tools available to Cursor's AI agent automatically when relevant for web scraping tasks.\n\n### Running on Claude Desktop\n\nAdd this to your `claude_desktop_config.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"mcp-server-webscraping-ai\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"webscraping-ai-mcp\"],\n      \"env\": {\n        \"WEBSCRAPING_AI_API_KEY\": \"YOUR_API_KEY_HERE\",\n        \"WEBSCRAPING_AI_CONCURRENCY_LIMIT\": \"5\"\n      }\n    }\n  }\n}\n```\n\n## Configuration\n\n### Environment Variables\n\n#### Required\n\n- `WEBSCRAPING_AI_API_KEY`: Your WebScraping.AI API key\n  - Required for all operations\n  - Get your API key from [WebScraping.AI](https://webscraping.ai)\n\n#### Optional Configuration\n- `WEBSCRAPING_AI_CONCURRENCY_LIMIT`: Maximum number of concurrent requests (default: `5`)\n- `WEBSCRAPING_AI_DEFAULT_PROXY_TYPE`: Type of proxy to use (default: `residential`)\n- `WEBSCRAPING_AI_DEFAULT_JS_RENDERING`: Enable/disable JavaScript rendering (default: `true`)\n- `WEBSCRAPING_AI_DEFAULT_TIMEOUT`: Maximum web page retrieval time in ms (default: `15000`, max: `30000`)\n- `WEBSCRAPING_AI_DEFAULT_JS_TIMEOUT`: Maximum JavaScript rendering time in ms (default: `2000`)\n\n### Configuration Examples\n\nFor standard usage:\n```bash\n# Required\nexport WEBSCRAPING_AI_API_KEY=your-api-key\n\n# Optional - customize behavior (default values)\nexport WEBSCRAPING_AI_CONCURRENCY_LIMIT=5\nexport WEBSCRAPING_AI_DEFAULT_PROXY_TYPE=residential # datacenter or residential\nexport WEBSCRAPING_AI_DEFAULT_JS_RENDERING=true\nexport WEBSCRAPING_AI_DEFAULT_TIMEOUT=15000\nexport WEBSCRAPING_AI_DEFAULT_JS_TIMEOUT=2000\n```\n\n## Available Tools\n\n### 1. Question Tool (`webscraping_ai_question`)\n\nAsk questions about web page content.\n\n```json\n{\n  \"name\": \"webscraping_ai_question\",\n  \"arguments\": {\n    \"url\": \"https://example.com\",\n    \"question\": \"What is the main topic of this page?\",\n    \"timeout\": 30000,\n    \"js\": true,\n    \"js_timeout\": 2000,\n    \"wait_for\": \".content-loaded\",\n    \"proxy\": \"datacenter\",\n    \"country\": \"us\"\n  }\n}\n```\n\nExample response:\n\n```json\n{\n  \"content\": [\n    {\n      \"type\": \"text\",\n      \"text\": \"The main topic of this page is examples and documentation for HTML and web standards.\"\n    }\n  ],\n  \"isError\": false\n}\n```\n\n### 2. Fields Tool (`webscraping_ai_fields`)\n\nExtract structured data from web pages based on instructions.\n\n```json\n{\n  \"name\": \"webscraping_ai_fields\",\n  \"arguments\": {\n    \"url\": \"https://example.com/product\",\n    \"fields\": {\n      \"title\": \"Extract the product title\",\n      \"price\": \"Extract the product price\",\n      \"description\": \"Extract the product description\"\n    },\n    \"js\": true,\n    \"timeout\": 30000\n  }\n}\n```\n\nExample response:\n\n```json\n{\n  \"content\": [\n    {\n      \"type\": \"text\",\n      \"text\": {\n        \"title\": \"Example Product\",\n        \"price\": \"$99.99\",\n        \"description\": \"This is an example product description.\"\n      }\n    }\n  ],\n  \"isError\": false\n}\n```\n\n### 3. HTML Tool (`webscraping_ai_html`)\n\nGet the full HTML of a web page with JavaScript rendering.\n\n```json\n{\n  \"name\": \"webscraping_ai_html\",\n  \"arguments\": {\n    \"url\": \"https://example.com\",\n    \"js\": true,\n    \"timeout\": 30000,\n    \"wait_for\": \"#content-loaded\"\n  }\n}\n```\n\nExample response:\n\n```json\n{\n  \"content\": [\n    {\n      \"type\": \"text\",\n      \"text\": \"<html>...[full HTML content]...</html>\"\n    }\n  ],\n  \"isError\": false\n}\n```\n\n### 4. Text Tool (`webscraping_ai_text`)\n\nExtract the visible text content from a web page.\n\n```json\n{\n  \"name\": \"webscraping_ai_text\",\n  \"arguments\": {\n    \"url\": \"https://example.com\",\n    \"js\": true,\n    \"timeout\": 30000\n  }\n}\n```\n\nExample response:\n\n```json\n{\n  \"content\": [\n    {\n      \"type\": \"text\",\n      \"text\": \"Example Domain\\nThis domain is for use in illustrative examples in documents...\"\n    }\n  ],\n  \"isError\": false\n}\n```\n\n### 5. Selected Tool (`webscraping_ai_selected`)\n\nExtract content from a specific element using a CSS selector.\n\n```json\n{\n  \"name\": \"webscraping_ai_selected\",\n  \"arguments\": {\n    \"url\": \"https://example.com\",\n    \"selector\": \"div.main-content\",\n    \"js\": true,\n    \"timeout\": 30000\n  }\n}\n```\n\nExample response:\n\n```json\n{\n  \"content\": [\n    {\n      \"type\": \"text\",\n      \"text\": \"<div class=\\\"main-content\\\">This is the main content of the page.</div>\"\n    }\n  ],\n  \"isError\": false\n}\n```\n\n### 6. Selected Multiple Tool (`webscraping_ai_selected_multiple`)\n\nExtract content from multiple elements using CSS selectors.\n\n```json\n{\n  \"name\": \"webscraping_ai_selected_multiple\",\n  \"arguments\": {\n    \"url\": \"https://example.com\",\n    \"selectors\": [\"div.header\", \"div.product-list\", \"div.footer\"],\n    \"js\": true,\n    \"timeout\": 30000\n  }\n}\n```\n\nExample response:\n\n```json\n{\n  \"content\": [\n    {\n      \"type\": \"text\",\n      \"text\": [\n        \"<div class=\\\"header\\\">Header content</div>\",\n        \"<div class=\\\"product-list\\\">Product list content</div>\",\n        \"<div class=\\\"footer\\\">Footer content</div>\"\n      ]\n    }\n  ],\n  \"isError\": false\n}\n```\n\n### 7. Account Tool (`webscraping_ai_account`)\n\nGet information about your WebScraping.AI account.\n\n```json\n{\n  \"name\": \"webscraping_ai_account\",\n  \"arguments\": {}\n}\n```\n\nExample response:\n\n```json\n{\n  \"content\": [\n    {\n      \"type\": \"text\",\n      \"text\": {\n        \"requests\": 5000,\n        \"remaining\": 4500,\n        \"limit\": 10000,\n        \"resets_at\": \"2023-12-31T23:59:59Z\"\n      }\n    }\n  ],\n  \"isError\": false\n}\n```\n\n## Common Options for All Tools\n\nThe following options can be used with all scraping tools:\n\n- `timeout`: Maximum web page retrieval time in ms (15000 by default, maximum is 30000)\n- `js`: Execute on-page JavaScript using a headless browser (true by default)\n- `js_timeout`: Maximum JavaScript rendering time in ms (2000 by default)\n- `wait_for`: CSS selector to wait for before returning the page content\n- `proxy`: Type of proxy, datacenter or residential (residential by default)\n- `country`: Country of the proxy to use (US by default). Supported countries: us, gb, de, it, fr, ca, es, ru, jp, kr, in\n- `custom_proxy`: Your own proxy URL in \"http://user:password@host:port\" format\n- `device`: Type of device emulation. Supported values: desktop, mobile, tablet\n- `error_on_404`: Return error on 404 HTTP status on the target page (false by default)\n- `error_on_redirect`: Return error on redirect on the target page (false by default)\n- `js_script`: Custom JavaScript code to execute on the target page\n\n## Error Handling\n\nThe server provides robust error handling:\n\n- Automatic retries for transient errors\n- Rate limit handling with backoff\n- Detailed error messages\n- Network resilience\n\nExample error response:\n\n```json\n{\n  \"content\": [\n    {\n      \"type\": \"text\",\n      \"text\": \"API Error: 429 Too Many Requests\"\n    }\n  ],\n  \"isError\": true\n}\n```\n\n## Integration with LLMs\n\nThis server implements the [Model Context Protocol](https://github.com/facebookresearch/modelcontextprotocol), making it compatible with any MCP-enabled LLM platforms. You can configure your LLM to use these tools for web scraping tasks.\n\n### Example: Configuring Claude with MCP\n\n```javascript\nconst { Claude } = require('@anthropic-ai/sdk');\nconst { Client } = require('@modelcontextprotocol/sdk/client/index.js');\nconst { StdioClientTransport } = require('@modelcontextprotocol/sdk/client/stdio.js');\n\nconst claude = new Claude({\n  apiKey: process.env.ANTHROPIC_API_KEY\n});\n\nconst transport = new StdioClientTransport({\n  command: 'npx',\n  args: ['-y', 'webscraping-ai-mcp'],\n  env: {\n    WEBSCRAPING_AI_API_KEY: 'your-api-key'\n  }\n});\n\nconst client = new Client({\n  name: 'claude-client',\n  version: '1.0.0'\n});\n\nawait client.connect(transport);\n\n// Now you can use Claude with WebScraping.AI tools\nconst tools = await client.listTools();\nconst response = await claude.complete({\n  prompt: 'What is the main topic of example.com?',\n  tools: tools\n});\n```\n\n## Development\n\n```bash\n# Clone the repository\ngit clone https://github.com/webscraping-ai/webscraping-ai-mcp-server.git\ncd webscraping-ai-mcp-server\n\n# Install dependencies\nnpm install\n\n# Run tests\nnpm test\n\n# Add your .env file\ncp .env.example .env\n\n# Start the inspector\nnpx @modelcontextprotocol/inspector node src/index.js\n```\n\n### Contributing\n\n1. Fork the repository\n2. Create your feature branch\n3. Run tests: `npm test`\n4. Submit a pull request\n\n## License\n\nMIT License - see LICENSE file for details \n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "webscraping",
        "scraping",
        "web",
        "ai webscraping",
        "extraction webscraping",
        "webscraping ai"
      ],
      "category": "search--data-extraction"
    },
    "yamanoku--baseline-mcp-server": {
      "owner": "yamanoku",
      "name": "baseline-mcp-server",
      "url": "https://github.com/yamanoku/baseline-mcp-server",
      "imageUrl": "",
      "description": "MCP server that searches Baseline status using Web Platform API",
      "stars": 35,
      "forks": 2,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-09-12T01:47:35Z",
      "readme_content": "<p align=\"center\">\n\t\n</p>\n\n<h1 align=\"center\">Baseline MCP Server</h1>\n\n[日本語版](./README.md) | [English Version](./README_EN.md)\n\nWeb Platform APIのサポート状況を提供するModel Context Protocolサーバーです。\n\n[![JSR Version](https://jsr.io/badges/@yamanoku/baseline-mcp-server)](https://jsr.io/@yamanoku/baseline-mcp-server)\n\n## 概要\n\nこのサーバーは、[Web Platform Dashboard](https://webstatus.dev/)のAPIを使用して、WebのAPI機能のBaselineステータス（サポート状況）を取得できるMCPサーバーを実装しています。クエリに基づいてWeb機能の情報を取得し、その結果をMCPクライアントに返します。\n\n\n\n## 機能\n\n- Web Platform DashboardのAPIを使用した機能検索\n- 機能のBaselineステータス（`widely`、`newly`、`limited`、`no_data`）の提供\n- ブラウザ実装状況（バージョンと実装日）の提供\n- 機能の使用状況データの提供\n- 特定のブラウザを除外した機能検索（`chrome`, `edge`, `firefox`, `safari`）\n- MCPを介した各種AIモデルとの連携\n\n## Baselineステータスについて\n\nBaselineステータスは、Web機能のブラウザサポート状況を示します：\n\n- **widely**:\n  広くサポートされているWeb標準機能。ほとんどのブラウザで安全に使用できます。\n- **newly**:\n  新しく標準化されたWeb機能。主要なブラウザでサポートされ始めていますが、まだ普及途上です。\n- **limited**:\n  限定的にサポートされているWeb機能。一部のブラウザでは使用できないか、フラグが必要な場合があります。\n- **no_data**:\n  現時点ではBaselineに含まれていないWeb機能。ブラウザのサポート状況を個別に確認する必要があります。\n\nBaselineについての詳細については「[Baseline (互換性) - MDN Web Docs 用語集](https://developer.mozilla.org/ja/docs/Glossary/Baseline/Compatibility)」を参照してください。\n\n## MCPクライアントでの設定\n\n- サーバーを起動するにあたり、Denoの使用を推奨します\n  - パーミッションとして`api.webstatus.dev`のみのアクセスを許可してください\n- [`@yamanoku/baseline-mcp-server`](https://jsr.io/@yamanoku/baseline-mcp-server)を指定するか、お手元のローカル環境にbaseline-mcp-server.tsを設置して読み取るように設定してください\n\n### Claude Desktop\n\nClaude\nDesktopのMCPクライアントで使用するには、以下のように`cline_mcp_settings.json`に設定を追加します。\n\n```json\n{\n  \"mcpServers\": {\n    \"baseline-mcp-server\": {\n      \"command\": \"deno\",\n      \"args\": [\n        \"run\",\n        \"--allow-net=api.webstatus.dev\",\n        \"jsr:@yamanoku/baseline-mcp-server\"\n      ]\n    }\n  }\n}\n```\n\n### Visual Studio Code\n\nVisual Studio\nCodeのMCPクライアントで使用するには、以下のように`settings.json`に設定を追加します。\n\n```json\n{\n  \"mcp\": {\n    \"servers\": {\n      \"baseline-mcp-server\": {\n        \"command\": \"deno\",\n        \"args\": [\n          \"run\",\n          \"--allow-net=api.webstatus.dev\",\n          \"jsr:@yamanoku/baseline-mcp-server\"\n        ]\n      }\n    }\n  }\n}\n```\n\n## Dockerによる起動\n\n最初にDockerイメージをビルドします。\n\n```shell\ndocker build -t baseline-mcp-server .\n```\n\nMCPクライアントの設定でDockerコンテナを実行するようにします。\n\n```json\n{\n  \"mcpServers\": {\n    \"baseline-mcp-server\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"-i\",\n        \"baseline-mcp-server:latest\"\n      ]\n    }\n  }\n}\n```\n\n## 謝辞\n\nこのOSSはGPT-4o Image Generationによってロゴを製作、Claude 3.7\nSonnetによって実装、ドキュメントのサンプルを提案いただきました。感謝申し上げます。\n\n## ライセンス\n\n[MIT License](./LICENSE)",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "searches",
        "mcp",
        "search",
        "searches baseline",
        "server searches",
        "server mcp"
      ],
      "category": "search--data-extraction"
    },
    "zhsama--duckduckgo-mcp-server": {
      "owner": "zhsama",
      "name": "duckduckgo-mcp-server",
      "url": "https://github.com/zhsama/duckduckgo-mpc-server/",
      "imageUrl": "",
      "description": "This is a TypeScript-based MCP server that provides DuckDuckGo search functionality.",
      "stars": 62,
      "forks": 7,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-10-03T13:03:47Z",
      "readme_content": "# duckduckgo-search MCP Server\n\nEnglish | [中文](README_zh.md)\n\nA Model Context Protocol server for DuckDuckGo Search\n\nThis is a TypeScript-based MCP server that provides DuckDuckGo search functionality. It demonstrates core MCP concepts through:\n\n- Integration with DuckDuckGo Search\n- Easy-to-use search tool interface\n- Rate limiting and error handling support\n\n<a href=\"https://glama.ai/mcp/servers/34fhy9xb9w\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/34fhy9xb9w/badge\" alt=\"DuckDuckGo Server MCP server\" />\n</a>\n\n## Features\n\n### Search Tool\n\n- `duckduckgo_search` - Perform web searches using DuckDuckGo API\n  - Required parameter: `query` (search query, max 400 characters)\n  - Optional parameter: `count` (number of results, 1-20, default 10)\n  - Optional parameter: `safeSearch` (safety level: strict/moderate/off, default moderate)\n  - Returns formatted Markdown search results\n\n### Rate Limits\n\n- Maximum 1 request per second\n- Maximum 15000 requests per month\n\n## Development\n\n### Prerequisites\n\n- Node.js >= 18\n- pnpm >= 8.0.0\n\n### Installation\n\n```bash\n# Install pnpm if not already installed\nnpm install -g pnpm\n\n# Install project dependencies\npnpm install\n```\n\n### Build and Run\n\nBuild the server:\n\n```bash\npnpm run build\n```\n\nFor development with auto-rebuild:\n\n```bash\npnpm run watch\n```\n\n## Setup in Claude Desktop\n\nTo use with Claude Desktop, add the server config:\n\nOn MacOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\nOn Windows: `%APPDATA%/Claude/claude_desktop_config.json`\n\n```json\n# online\n{\n  \"mcpServers\": {\n    \"duckduckgo-search\": {\n        \"command\": \"npx\",\n        \"args\": [\n          \"-y\",\n          \"duckduckgo-mcp-server\"\n        ]\n    }\n  }\n}\n\n# local\n{\n  \"mcpServers\": {\n    \"duckduckgo-search\": {\n      \"command\": \"node\",\n      \"args\": [\n        \"/path/to/duckduckgo-search/build/index.js\"\n      ]\n    }\n  }\n}\n```\n![image](https://github.com/user-attachments/assets/6906e280-9dbb-4bb5-a537-d9e45e666084)\n![image](https://github.com/user-attachments/assets/867a70ae-082f-45ab-a623-869bfd6c31eb)\n\n### Debugging\n\nSince MCP servers communicate over stdio, debugging can be challenging. We recommend using the [MCP Inspector](https://github.com/modelcontextprotocol/inspector), which is available as a package script:\n\n```bash\npnpm run inspector\n```\n\nThe Inspector will provide a URL to access debugging tools in your browser.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "duckduckgo",
        "typescript",
        "mcp",
        "duckduckgo search",
        "duckduckgo mcp",
        "provides duckduckgo"
      ],
      "category": "search--data-extraction"
    },
    "zoomeye-ai--mcp_zoomeye": {
      "owner": "zoomeye-ai",
      "name": "mcp_zoomeye",
      "url": "https://github.com/zoomeye-ai/mcp_zoomeye",
      "imageUrl": "",
      "description": "Querying network asset information by ZoomEye MCP Server",
      "stars": 48,
      "forks": 13,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-10-03T03:44:56Z",
      "readme_content": "# 🚀 ZoomEye MCP Server\n\nA Model Context Protocol (MCP) server that provides network asset information based on query conditions. This server allows Large Language Models (LLMs) to obtain network asset information by querying ZoomEye using dorks and other search parameters.\n\n## 🔔 Announcement\n\n🎉 We are excited to announce the official open-source release of **ZoomEye MCP Server** — a powerful Model Context Protocol (MCP) server that brings real-time cyber asset intelligence to AI assistants and development environments.\n\n🚀 Free Trial: 7-Day FREE Access to ZoomEye MCP!\nExperience ZoomEye MCP — the AI-powered cyberspace asset search engine — absolutely free for 7 days!\n\n🔍 Search global internet assets, track real-time changes, and unlock AI-driven insights — all in one place.\n\n👉 How to claim:\n\n1. Follow us on Twitter: [@zoomeye_team](https://x.com/zoomeye_team)\n2. DM us \"MCP\" and your MCP setup screenshot\n3. Get instant access to your 7-day membership\n\n🎁 Limited-time free trial — explore the power of AI asset search today!\n\n💡 Provide insightful feedback that gets officially adopted, and you'll unlock **even more rewards**!\n\n🔧 Fully compatible with leading MCP environments:\n\n- Claude Desktop\n- Cursor\n- Windsurf\n- Cline\n- Continue\n- Zed\n- Cherry Studio\n- Chatbox\n\n🔗 Explore ZoomEye MCP Server on:\n\n- GitHub: [zoomeye-ai/mcp_zoomeye](https://github.com/zoomeye-ai/mcp_zoomeye)\n- MCP.so: [mcp.so/server/mcp_zoomeye](https://mcp.so/server/mcp_zoomeye/zoomeye-ai)\n- Smithery: [smithery.ai/server/@zoomeye-ai/mcp_zoomeye](https://smithery.ai/server/@zoomeye-ai/mcp_zoomeye)\n- Cursor Directory: [cursor.directory/mcp/zoomeye](https://cursor.directory/mcp/zoomeye)\n- Pulse MCP: [pulsemcp.com/servers/zoomeye](https://www.pulsemcp.com/servers/zoomeye)\n- Glama MCP: [glama.ai/mcp/servers](https://glama.ai/mcp/servers)\n\nWe welcome everyone to use, explore, and contribute!\n\n## 🔑 How can I get a ZoomEye API key?\n\nTo use this MCP server, you’ll need a ZoomEye API key.\n\n1. Go to https://www.zoomeye.ai\n2. Register or log in\n3. Click your avatar → **Profile**\n4. Copy your **API-KEY**\n5. Set the environment variable:\n   \n   `export ZOOMEYE_API_KEY=\"your_api_key_here\"`\n\n\n\n\n\n## Features\n\n- Query ZoomEye for network asset information using dorks\n- Caching mechanism to improve performance and reduce API calls\n- Automatic retry mechanism for failed API requests\n- Comprehensive error handling and logging\n\n## Available Tools\n\n- `zoomeye_search` - Get network asset information based on query conditions.\n  - Required parameters:\n    - `qbase64` (string): Base64 encoded query string for ZoomEye search\n  - Optional parameters:\n    - `page` (integer): View asset page number, default is 1\n    - `pagesize` (integer): Number of records per page, default is 10, maximum is 1000\n    - `fields` (string): The fields to return, separated by commas\n    - `sub_type` (string): Data type, supports v4, v6, and web. Default is v4\n    - `facets` (string): Statistical items, separated by commas if there are multiple\n    - `ignore_cache` (boolean): Whether to ignore the cache\n\n## Usage Guide\n\n### Basic Usage\n\nOnce the server is running, you can interact with it through your AI assistant or development environment. Here's how to use it:\n\n1. **Start the server** using one of the installation methods above\n2. **Configure your AI assistant** (Claude Desktop, Cursor, Windsurf, Cline, Continue, Zed, etc.) to use the server\n3. **Query network information** using natural language\n\n\n\n### Search Syntax Guide\n\n- Search Scope covers devices (IPv4, IPv6) and websites (domains).\n- When entering a search string, the system will match keywords in \"global\" mode, including content from various\n  protocols such as HTTP, SSH, FTP, etc. (e.g., HTTP/HTTPS protocol headers, body, SSL, title, and other protocol\n  banners).\n- Search strings are case-insensitive and will be segmented for matching (the search results page provides a \"\n  segmentation\" test feature). When using == for search, it enforces exact case-sensitive matching with strict syntax.\n- Please use quotes for search strings (e.g., \"Cisco System\" or 'Cisco System'). If the search string contains quotes,\n  use the escape character, e.g.,\"a\\\"b\". If the search string contains parentheses, use the escape character, e.g.,\n  portinfo\\(\\).\n\nYou can see more detailed search syntax rules in [prompts.py](./src/mcp_server_zoomeye/prompts.py).\n\nFor more information on the ZoomEye Search API, refer to the [ZoomEye API v2 documentation](https://www.zoomeye.ai/doc).\n\n## Getting Started\n\n### Prerequisites\n\n1. **ZoomEye API Key**\n   \n   - Register for an account at [ZoomEye](https://www.zoomeye.ai/)\n   - Obtain your API key from your account settings\n   - The API key will be used to authenticate your requests to the ZoomEye API\n2. **Python Environment**\n   \n   - Python 3.10 or higher is required\n   - Alternatively, you can use Docker to run the server without installing Python\n\n## Installation\n\n### Using PIP\n\nAlternatively, you can install `mcp-server-zoomeye` via pip:\n\n```bash\npip install mcp-server-zoomeye\n```\n\nAfter installation, you can run it as a script using the following command:\n\n```bash\npython -m mcp_server_zoomeye\n```\n\n### Using Docker\n\nYou can also run the ZoomEye MCP server using Docker:\n\n#### Pull from Docker Hub\n\n```bash\n# Pull the latest image\ndocker pull zoomeyeteam/mcp-server-zoomeye:latest\n\n# Run the container with your API key\ndocker run -i --rm -e ZOOMEYE_API_KEY=your_api_key_here zoomeyeteam/mcp-server-zoomeye:latest\n```\n\n> **Note**: We provide multi-architecture Docker images that support `linux/amd64` and `linux/arm64` platforms and can run on Intel/AMD and ARM (such as Apple Silicon) processors.\n\n#### Build from Source\n\nAlternatively, you can build the Docker image from source:\n\n```bash\n# Clone the repository\ngit clone https://github.com/zoomeye-ai/mcp_zoomeye.git\ncd mcp_zoomeye\n\n# Build the Docker image\ndocker build -t zoomeyeteam/mcp-server-zoomeye:local .\n\n# Run the container\ndocker run -i --rm -e ZOOMEYE_API_KEY=your_api_key_here zoomeyeteam/mcp-server-zoomeye:local\n```\n\n### Using uv\n\n[`uv`](https://docs.astral.sh/uv/) is a fast Python package installer and resolver written in Rust. It's a modern alternative to pip that offers significant performance improvements.\n\n#### Installation of uv\n\n```bash\n# Install uv using curl (macOS/Linux)\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n\n# Or using PowerShell (Windows)\nirm https://astral.sh/uv/install.ps1 | iex\n\n# Or using Homebrew (macOS)\nbrew install uv\n```\n\n#### Using uvx to run mcp-server-zoomeye\n\nNo specific installation is required when using [`uvx`](https://docs.astral.sh/uv/guides/tools/), which allows you to run Python packages directly:\n\n#### Installing with uv\n\nAlternatively, you can install the package using uv:\n\n```bash\n# Install in the current environment\nuv pip install mcp-server-zoomeye\n\n# Or create and install in a new virtual environment\nuv venv\nuv pip install mcp-server-zoomeye\n```\n\n## Configuration\n\n### Environment Variables\n\nThe ZoomEye MCP server requires the following environment variable:\n\n- `ZOOMEYE_API_KEY`: Your ZoomEye API key for authentication\n\nYou can set this environment variable in several ways:\n\n1. **Export in your shell session**:\n   \n   ```bash\n   export ZOOMEYE_API_KEY=\"your_api_key_here\"\n   ```\n2. **Pass directly when running the container** (for Docker):\n   \n   ```bash\n   docker run -i --rm -e ZOOMEYE_API_KEY=your_api_key_here zoomeyeteam/mcp-server-zoomeye:latest\n   ```\n\n### Configure Claude.app\n\nAdd the following in Claude settings:\n\n<details>\n<summary>Using uvx</summary>\n\n```json\n\"mcpServers\": {\n  \"zoomeye\": {\n    \"command\": \"uvx\",\n    \"args\": [\"mcp-server-zoomeye\"],\n    \"env\": {\n        \"ZOOMEYE_API_KEY\": \"your_api_key_here\"\n    }\n  }\n}\n```\n\n</details>\n\n<details>\n<summary>Using docker</summary>\n\n```json\n\"mcpServers\": {\n  \"zoomeye\": {\n    \"command\": \"docker\",\n    \"args\": [\"run\", \"-i\", \"--rm\", \"-e\", \"ZOOMEYE_API_KEY=your_api_key_here\", \"zoomeyeteam/mcp-server-zoomeye:latest\"],\n    \"env\": {\n      \"ZOOMEYE_API_KEY\": \"your_api_key_here\"\n    }\n  }\n}\n```\n\n</details>\n\n<details>\n<summary>Installed via pip</summary>\n\n```json\n\"mcpServers\": {\n  \"zoomeye\": {\n    \"command\": \"python\",\n    \"args\": [\"-m\", \"mcp_server_zoomeye\"],\n    \"env\": {\n        \"ZOOMEYE_API_KEY\": \"your_api_key_here\"\n    }\n  }\n}\n```\n\n</details>\n\n### Configure Zed\n\nAdd the following in Zed's settings.json:\n\n<details>\n<summary>Using uvx</summary>\n\n```json\n\"context_servers\": [\n  \"mcp-server-zoomeye\": {\n    \"command\": \"uvx\",\n    \"args\": [\"mcp-server-zoomeye\"],\n    \"env\": {\n        \"ZOOMEYE_API_KEY\": \"your_api_key_here\"\n    }\n  }\n],\n```\n\n</details>\n\n<details>\n<summary>Installed via pip</summary>\n\n```json\n\"context_servers\": {\n  \"mcp-server-zoomeye\": {\n    \"command\": \"python\",\n    \"args\": [\"-m\", \"mcp_server_zoomeye\"],\n    \"env\": {\n        \"ZOOMEYE_API_KEY\": \"your_api_key_here\"\n    }\n  }\n},\n```\n\n</details>\n\n## Example Interactions\n\n### Example 1: Retrieve global Apache Tomcat assets\n\n```json\n{\n  \"name\": \"zoomeye_search\",\n  \"arguments\": {\n    \"qbase64\": \"app=\\\"Apache Tomcat\\\"\"\n  }\n}\n```\n\nResponse:\n\n```json\n{\n  \"code\": 60000,\n  \"message\": \"success\",\n  \"total\": 163139107,\n  \"query\": \"app=\\\"Apache Tomcat\\\"\",\n  \"data\": [\n    {\n      \"url\": \"https://1.1.1.1:443\",\n      \"ssl.jarm\": \"29d29d15d29d29d00029d29d29d29dea0f89a2e5fb09e4d8e099befed92cfa\",\n      \"ssl.ja3s\": \"45094d08156d110d8ee97b204143db14\",\n      \"iconhash_md5\": \"f3418a443e7d841097c714d69ec4bcb8\",\n      \"robots_md5\": \"0b5ce08db7fb8fffe4e14d05588d49d9\",\n      \"security_md5\": \"0b5ce08db7fb8fffe4e14d05588d49d9\",\n      \"ip\": \"1.1.1.1\",\n      \"domain\": \"www.google.com\",\n      \"hostname\": \"SPACEX\",\n      \"os\": \"windows\",\n      \"port\": 443,\n      \"service\": \"https\",\n      \"title\": [\"GoogleGoogle appsGoogle Search\"],\n      \"version\": \"1.1.0\",\n      \"device\": \"webcam\",\n      \"rdns\": \"c01031-001.cust.wallcloud.ch\",\n      \"product\": \"OpenSSD\",\n      \"header\": \"HTTP/1.1 302 Found Location: https://www.google.com/?gws_rd=ssl Cache-Control: private...\",\n      \"header_hash\": \"27f9973fe57298c3b63919259877a84d\",\n      \"body\": \"HTTP/1.1 302 Found Location: https://www.google.com/?gws_rd=ssl Cache-Control: private...\",\n      \"body_hash\": \"84a18166fde3ee7e7c974b8d1e7e21b4\",\n      \"banner\": \"SSH-2.0-OpenSSH_7.6p1 Ubuntu-4ubuntu0.3\",\n      \"update_time\": \"2024-07-03T14:34:10\",\n      \"header.server.name\": \"nginx\",\n      \"header.server.version\": \"1.8.1\",\n      \"continent.name\": \"Europe\",\n      \"country.name\": \"Germany\",\n      \"province.name\": \"Hesse\",\n      \"city.name\": \"Frankfurt\",\n      \"lon\": \"118.753262\",\n      \"lat\": \"32.064838\",\n      \"isp.name\": \"aviel.ru\",\n      \"organization.name\": \"SERVISFIRST BANK\",\n      \"zipcode\": \"210003\",\n      \"idc\": 0,\n      \"honeypot\": 0,\n      \"asn\": 4837,\n      \"protocol\": \"tcp\",\n      \"ssl\": \"SSL Certificate Version: TLS 1.2 CipherSuit: TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256...\",\n      \"primary_industry\": \"Finance\",\n      \"sub_industry\": \"bank\",\n      \"rank\": 60\n    }\n  ]\n}\n```\n\n## Debugging and Troubleshooting\n\n### Using MCP Inspector\n\nThe Model Context Protocol Inspector is a tool that helps debug MCP servers by simulating client interactions. You can use it to test your ZoomEye MCP server:\n\n```bash\n# For uvx installation\nnpx @modelcontextprotocol/inspector uvx mcp-server-zoomeye\n\n# If developing locally\ncd path/to/servers/src/mcp_server_zoomeye\nnpx @modelcontextprotocol/inspector uv run mcp-server-zoomeye\n```\n\n### Common Issues\n\n1. **Authentication Errors**\n   \n   - Ensure your ZoomEye API key is correct and properly set as an environment variable\n   - Check that your API key has not expired or been revoked\n2. **Connection Issues**\n   \n   - Verify your internet connection\n   - Check if the ZoomEye API is experiencing downtime\n3. **No Results**\n   \n   - Your query might be too specific or contain syntax errors\n   - Try simplifying your query or using different search terms\n4. **Rate Limiting**\n   \n   - ZoomEye API has rate limits based on your account type\n   - Space out your requests or upgrade your account for higher limits\n\n## Advanced Usage\n\n### Caching\n\nThe ZoomEye MCP server implements caching to improve performance and reduce API calls:\n\n- Responses are cached based on the query parameters\n- Cache duration is configurable (default: 1 hour)\n- You can bypass the cache by setting `ignore_cache` to `true` in your query\n\n### Custom Fields\n\nYou can request specific fields in your query results by using the `fields` parameter:\n\n```json\n{\n  \"name\": \"zoomeye_search\",\n  \"arguments\": {\n    \"qbase64\": \"app=\\\"Apache\\\"\",\n    \"fields\": \"ip,port,domain,service,os,country,city\"\n  }\n}\n```\n\n### Pagination\n\nFor queries that return many results, you can paginate through them:\n\n```json\n{\n  \"name\": \"zoomeye_search\",\n  \"arguments\": {\n    \"qbase64\": \"app=\\\"Apache\\\"\",\n    \"page\": 2,\n    \"pagesize\": 20\n  }\n}\n```\n\n## Contributing\n\nWe encourage contributions to mcp-server-zoomeye to help expand and improve its functionality. Whether it's adding new related tools, enhancing existing features, or improving documentation, your input is valuable.\n\nFor examples of other MCP servers and implementation patterns, see:\nhttps://github.com/modelcontextprotocol/servers\n\nPull requests are welcome! Feel free to contribute new ideas, bug fixes, or enhancements to make mcp-server-zoomeye more robust and practical.\n\n## License\n\nmcp-server-zoomeye is licensed under the MIT License. This means you are free to use, modify, and distribute the software, subject to the terms and conditions of the MIT License. For more information, see the LICENSE file in the project repository.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "mcp_zoomeye",
        "zoomeye",
        "mcp",
        "mcp_zoomeye querying",
        "ai mcp_zoomeye",
        "zoomeye mcp"
      ],
      "category": "search--data-extraction"
    }
  }
}