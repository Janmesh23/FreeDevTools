{
  "category": "web-search",
  "categoryDisplay": "Web Search",
  "description": "",
  "totalRepositories": 469,
  "repositories": {
    "BjornMelin--crawl4ai-mcp-server": {
      "owner": "BjornMelin",
      "name": "crawl4ai-mcp-server",
      "url": "https://github.com/BjornMelin/crawl4ai-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/BjornMelin.webp",
      "description": "Enables high-performance web scraping, crawling, and deep research through a Model Context Protocol server, offering structured data extraction and asynchronous crawling capabilities. Features secure authentication methods and integration with web research tools via CloudFlare Workers for AI-driven data gathering.",
      "stars": 18,
      "forks": 6,
      "license": "No License",
      "language": "TypeScript",
      "updated_at": "2025-09-30T08:01:18Z",
      "readme_content": "# ⚠️ NOTICE\n\n> **MCP SERVER CURRENTLY UNDER DEVELOPMENT**  \n> **NOT READY FOR PRODUCTION USE**  \n> **WILL UPDATE WHEN OPERATIONAL**\n\n# Crawl4AI MCP Server\n\n🚀 High-performance MCP Server for Crawl4AI - Enable AI assistants to access web scraping, crawling, and deep research via Model Context Protocol. Faster and more efficient than FireCrawl!\n\n## Overview\n\nThis project implements a custom Model Context Protocol (MCP) Server that integrates with Crawl4AI, an open-source web scraping and crawling library. The server is deployed as a remote MCP server on CloudFlare Workers, allowing AI assistants like Claude to access Crawl4AI's powerful web scraping capabilities.\n\n## Documentation\n\nFor comprehensive details about this project, please refer to the following documentation:\n\n- [Migration Plan](docs/MIGRATION_PLAN.md) - Detailed plan for migrating from Firecrawl to Crawl4AI\n- [Enhanced Architecture](docs/ENHANCED_ARCHITECTURE.md) - Multi-tenant architecture with cloud provider flexibility\n- [Implementation Guide](docs/IMPLEMENTATION_GUIDE.md) - Technical implementation details and code examples\n- [Codebase Simplification](docs/SIMPLIFICATION.md) - Details on code simplification and best practices implemented\n- [Docker Setup Guide](docs/DOCKER.md) - Instructions for Docker setup for local development and production\n\n## Features\n\n### Web Data Acquisition\n\n- 🌐 **Single Webpage Scraping**: Extract content from individual webpages\n- 🕸️ **Web Crawling**: Crawl websites with configurable depth and page limits\n- 🗺️ **URL Discovery**: Map and discover URLs from a starting point\n- 🕸️ **Asynchronous Crawling**: Crawl entire websites efficiently\n\n### Content Processing\n\n- 🔍 **Deep Research**: Conduct comprehensive research across multiple pages\n- 📊 **Structured Data Extraction**: Extract specific data using CSS selectors or LLM-based extraction\n- 🔎 **Content Search**: Search through previously crawled content\n\n### Integration & Security\n\n- 🔄 **MCP Integration**: Seamless integration with MCP clients (Claude Desktop, etc.)\n- 🔒 **OAuth Authentication**: Secure access with proper authorization\n- 🔒 **Authentication Options**: Secure access via OAuth or API key (Bearer token)\n- ⚡ **High Performance**: Optimized for speed and efficiency\n\n## Project Structure\n\n```plaintext\ncrawl4ai-mcp/\n├── src/\n│   ├── index.ts               # Main entry point with OAuth provider setup\n│   ├── auth-handler.ts        # Authentication handler\n│   ├── mcp-server.ts          # MCP server implementation\n│   ├── crawl4ai-adapter.ts    # Adapter for Crawl4AI API\n│   ├── tool-schemas/          # MCP tool schema definitions\n│   │   └── [...].ts           # Tool schemas\n│   ├── handlers/\n│   │   ├── crawl.ts           # Web crawling implementation\n│   │   ├── search.ts          # Search functionality\n│   │   └── extract.ts         # Content extraction\n│   └── utils/                 # Utility functions\n├── tests/                     # Test cases\n├── .github/                   # GitHub configuration\n├── wrangler.toml              # CloudFlare Workers configuration\n├── tsconfig.json              # TypeScript configuration\n├── package.json               # Node.js dependencies\n└── README.md                  # Project documentation\n```\n\n## Getting Started\n\n### Prerequisites\n\n- [Node.js](https://nodejs.org/) (v18 or higher)\n- [npm](https://www.npmjs.com/)\n- [Wrangler](https://developers.cloudflare.com/workers/wrangler/install-and-update/) (CloudFlare Workers CLI)\n- A CloudFlare account\n\n### Installation\n\n1. Clone the repository:\n\n   ```bash\n   git clone https://github.com/BjornMelin/crawl4ai-mcp-server.git\n   cd crawl4ai-mcp-server\n   ```\n\n2. Install dependencies:\n\n   ```bash\n   npm install\n   ```\n\n3. Set up CloudFlare KV namespace:\n\n   ```bash\n   wrangler kv:namespace create CRAWL_DATA\n   ```\n\n4. Update `wrangler.toml` with the KV namespace ID:\n\n   ```toml\n   kv_namespaces = [\n     { binding = \"CRAWL_DATA\", id = \"your-namespace-id\" }\n   ]\n   ```\n\n## Development\n\n### Local Development\n\n#### Using NPM\n\n1. Start the development server:\n\n   ```bash\n   npm run dev\n   ```\n\n2. The server will be available at <http://localhost:8787>\n\n#### Using Docker\n\nYou can also use Docker for local development, which includes the Crawl4AI API and a debug UI:\n\n1. Set up environment variables:\n\n   ```bash\n   cp .env.example .env\n   # Edit .env file with your API key\n   ```\n\n2. Start the Docker development environment:\n\n   ```bash\n   docker-compose up -d\n   ```\n\n3. Access the services:\n   - MCP Server: <http://localhost:8787>\n   - Crawl4AI UI: <http://localhost:3000>\n\nSee the [Docker Setup Guide](docs/DOCKER.md) for more details.\n\n### Testing\n\nThe project includes a comprehensive test suite using Jest. To run tests:\n\n```bash\n# Run all tests\nnpm test\n\n# Run tests with watch mode during development\nnpm run test:watch\n\n# Run tests with coverage report\nnpm run test:coverage\n\n# Run only unit tests\nnpm run test:unit\n\n# Run only integration tests\nnpm run test:integration\n```\n\nWhen running in Docker:\n\n```bash\ndocker-compose exec mcp-server npm test\n```\n\n## Deployment\n\n1. Deploy to CloudFlare Workers:\n\n   ```bash\n   npm run deploy\n   ```\n\n2. Your server will be available at the CloudFlare Workers URL assigned to your deployed worker.\n\n## Usage with MCP Clients\n\nThis server implements the Model Context Protocol, allowing AI assistants to access its tools.\n\n### Authentication\n\n- Implement OAuth authentication with workers-oauth-provider\n- Add API key authentication using Bearer tokens\n- Create login page and token management\n\n### Connecting to an MCP Client\n\n1. Use the CloudFlare Workers URL assigned to your deployed worker\n2. In Claude Desktop or other MCP clients, add this server as a tool source\n\n### Available Tools\n\n- `crawl`: Crawl web pages from a starting URL\n- `getCrawl`: Retrieve crawl data by ID\n- `listCrawls`: List all crawls or filter by domain\n- `search`: Search indexed documents by query\n- `extract`: Extract structured content from a URL\n\n## Configuration\n\nThe server can be configured by modifying environment variables in `wrangler.toml`:\n\n- `MAX_CRAWL_DEPTH`: Maximum depth for web crawling (default: 3)\n- `MAX_CRAWL_PAGES`: Maximum pages to crawl (default: 100)\n- `API_VERSION`: API version string (default: \"v1\")\n- `OAUTH_CLIENT_ID`: OAuth client ID for authentication\n- `OAUTH_CLIENT_SECRET`: OAuth client secret for authentication\n\n## Roadmap\n\nThe project is being developed with these components in mind:\n\n1. **Project Setup and Configuration**: CloudFlare Worker setup, TypeScript configuration\n2. **MCP Server and Tool Schemas**: Implementation of MCP server with tool definitions\n3. **Crawl4AI Adapter**: Integration with the Crawl4AI functionality\n4. **OAuth Authentication**: Secure authentication implementation\n5. **Performance Optimizations**: Enhancing speed and reliability\n6. **Advanced Extraction Features**: Improving structured data extraction capabilities\n\n## Contributing\n\nContributions are welcome! Please check the open issues or create a new one before starting work on a feature or bug fix. See [Contributing Guidelines](CONTRIBUTING.md) for detailed guidelines.\n\n## Support\n\nIf you encounter issues or have questions:\n\n- Open an issue on the GitHub repository\n- Check the [Crawl4AI documentation](https://crawl4ai.com/docs)\n- Refer to the [Model Context Protocol specification](https://github.com/anthropics/model-context-protocol)\n\n## How to Cite\n\nIf you use Crawl4AI MCP Server in your research or projects, please cite it using the following BibTeX entry:\n\n```bibtex\n@software{crawl4ai_mcp_2025,\n  author = {Melin, Bjorn},\n  title = {Crawl4AI MCP Server: High-performance Web Crawling for AI Assistants},\n  url = {https://github.com/BjornMelin/crawl4ai-mcp-server},\n  version = {1.0.0},\n  year = {2025},\n  month = {5}\n}\n```\n\n## License\n\n[MIT](LICENSE)",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "crawl4ai",
        "scraping",
        "crawling",
        "crawling capabilities",
        "scraping crawling",
        "crawl4ai mcp"
      ],
      "category": "web-search"
    },
    "BochaAI--bocha-search-mcp": {
      "owner": "BochaAI",
      "name": "bocha-search-mcp",
      "url": "https://github.com/BochaAI/bocha-search-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/BochaAI.webp",
      "description": "Provides access to high-quality world knowledge from billions of web pages and various ecological content sources through a powerful search engine interface. Supports natural language semantic search, offering rich, structured results and vertical domain cards for enhanced information retrieval across multiple domains.",
      "stars": 26,
      "forks": 7,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-09-26T07:24:49Z",
      "readme_content": "# MCP Server 产品名称: 博查\n\n\n\n## 版本信息\nv1\n\n## 产品描述\n### 短描述\n博查是一个给AI用的搜索引擎，让你的AI应用从近百亿网页和生态内容源中获取高质量的世界知识，涵盖天气、新闻、百科、医疗、火车票、图片等多种领域。\n\n### 长描述\n博查是一个给AI用的搜索引擎，让你的AI应用从近百亿网页和生态内容源中获取高质量的世界知识，涵盖天气、新闻、百科、医疗、火车票、图片等多种领域。\n\n## 分类\n网页搜索\n\n## 标签\n搜索, 新闻, 天气, 百科\n\n## Tools\n### Tool1: Bocha Web Search\n#### 详细描述\n从博查搜索全网信息和网页链接，返回结果包括网页标题、网页URL、网页摘要、网站名称、网站图标、发布时间、图片链接等。\n\n#### 调试所需要的参数\n输入:\n  - query: 搜索词(必填)\n  - freshness: 搜索指定时间范围内的网页 (可选值 YYYY-MM-DD, YYYY-MM-DD..YYYY-MM-DD, noLimit, oneYear, oneMonth, oneWeek, oneDay. 默认为 noLimit)\n  - count: 返回结果的条数 (1-50, 默认为 10)\n\n输出:\n  - 网页标题、网页链接、网页摘要、发布时间、网站名称\n\n### Tool2: Bocha AI Search\n#### 详细描述\n在博查网页搜索的基础上，AI识别搜索词语义并额外返回垂直领域内容的结构化模态卡，例如天气卡、日历卡、百科卡等几十种模态卡，在语义识别、搜索结果时效性、内容丰富性等方面更好。\n\n#### 调试所需要的参数\n输入:\n  - query: 搜索词(必填)\n  - freshness: 搜索指定时间范围内的网页 (可选值 YYYY-MM-DD, YYYY-MM-DD..YYYY-MM-DD, noLimit, oneYear, oneMonth, oneWeek, oneDay. 默认为 noLimit)\n  - count: 返回结果的条数 (1-50, 默认为 10)\n\n输出:\n  - 网页标题、网页链接、网页摘要、发布时间、网站名称、模态卡\n\n## 可适配平台\n方舟, python, Claude, Cursor等\n\n## 服务开通链接\n您需要前往 [博查AI开放平台](https://open.bochaai.com)，登陆后获取 API KEY。\n\n## 鉴权方式\nAPI Key\n\n## 安装部署\n### 步骤一：下载代码至本地\n```bash\ngit clone git@github.com:BochaAI/bocha-search-mcp.git\n```\n\n### 步骤二: 在客户端中配置\n#### Claude Desktop\nOn MacOS: `~/Library/Application\\ Support/Claude/claude_desktop_config.json`\n\nOn Windows: `%APPDATA%/Claude/claude_desktop_config.json`\n\n```json\n  \"mcpServers\": {\n    \"bocha-search-mcp\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"--directory\",\n        \"/path/to/bocha-search-mcp\",\n        \"run\",\n        \"bocha-search-mcp\"\n      ],\n      \"env\": {\n        \"BOCHA_API_KEY\": \"sk-****\"\n      }\n    }\n  }\n  ```\n\n### 步骤三: 在客户端中使用\n\n\n### 步骤四: 调试本地服务（可选）\n```bash\nnpx @modelcontextprotocol/inspector uv --directory /path/to/bocha-search-mcp run bocha-search-mcp\n```\n\n## 客户案例\n\n目前博查已经累计服务**3000+企业用户**和**20000+开发者用户**，并且成为**DeepSeek官方联网搜索供应方**以及**阿里、腾讯、字节官方推荐的搜索API**，目前**承接着国内60%以上AI应用的联网搜索请求**。\n\n博查的搜索内容源包括全网近百亿个网页，以及生态合作内容（含短视频、新闻、百科、天气、医疗、火车票、酒店、餐厅、景点、企业、学术等）。博查后续将会继续与各个平台在内容生态、智能体创作等方面开展共创合作，为博查用户的搜索问题提供丰富多彩的答案。\n\n## 常见问题\n\n### Bocha Web Search API服务可以提供什么样的能力?\nBocha Web Search 提供全网通用搜索能力。您可以从博查搜索全网信息和网页链接，返回结果包括网页标题、网页URL、网页摘要、网站名称、网站图标、发布时间、图片链接等，每次搜索结果返回的网页最多支持50条（count50）。\n\n传统搜索引擎使用的是关键字+竞价排名机制的搜索算法，搜索结果的目标不是直接为用户提供正确的答案，而是吸引用户点击以获得广告收入。\n\n博查是基于多模态混合搜索与语义排序技术的新一代搜索引擎，支持AI应用场景的自然语言搜索方式，同时搜索结果目标是提供干净、准确、高质量的答案。\n\n博查的语义排序技术基于Transformer架构，会根据搜索结果与用户问题的语义相关性进行排序。由于大模型同样是Transformer架构，通过判断上下文与用户问题的语义相关性进行取舍，因此最终大模型更加喜欢博查提供的搜索结果。\n\n目前博查的搜索效果是国内最接近Bing Search API的搜索引擎，由于Bing Search API数据会出海（无国内Region）、价格昂贵（15美元/千次）且不提供文本摘要（只有50-100字的snippet），国内很多企业客户都已经从Bing切换至博查。\n\n### Bocha AI Search API 服务可以提供什么样的能力？\nBocha AI Search 在博查 Web Search 的基础上，AI识别搜索词语义并额外返回垂直领域内容的结构化模态卡，例如天气卡、日历卡、百科卡等几十种模态卡，在语义识别、搜索结果时效性、内容丰富性等方面更好。\n\n目前支持的模态卡类型包括：天气、百科、医疗、万年历、火车、星座属相、贵金属、汇率、油价、手机、股票、汽车等。\n\n以股票信息为例，网页中一般无法获取到实时的股票数据，需要结构化模态卡来支撑。博查AI Search API可以在提供网页信息的基础上，额外输出股价的结构化数据模态卡，通过模态卡提供的结构化数据，可以进一步增强AI应用中用户对于时效性问题的回答准确性。",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "retrieval",
        "search",
        "semantic",
        "semantic search",
        "bocha search",
        "search bochaai"
      ],
      "category": "web-search"
    },
    "BurtTheCoder--mcp-dnstwist": {
      "owner": "BurtTheCoder",
      "name": "mcp-dnstwist",
      "url": "https://github.com/BurtTheCoder/mcp-dnstwist",
      "imageUrl": "/freedevtools/mcp/pfp/BurtTheCoder.webp",
      "description": "An MCP server for conducting DNS fuzzing to identify typosquatting, phishing, and potential corporate espionage by analyzing domain permutations.",
      "stars": 36,
      "forks": 7,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-09-28T06:09:21Z",
      "readme_content": "# DNStwist MCP Server\n[![smithery badge](https://smithery.ai/badge/@burtthecoder/mcp-dnstwist)](https://smithery.ai/server/@burtthecoder/mcp-dnstwist)\n\nA Model Context Protocol (MCP) server for [dnstwist](https://github.com/elceef/dnstwist), a powerful DNS fuzzing tool that helps detect typosquatting, phishing, and corporate espionage. This server provides tools for analyzing domain permutations and identifying potentially malicious domains. It is designed to integrate seamlessly with MCP-compatible applications like [Claude Desktop](https://claude.ai).\n\n<a href=\"https://glama.ai/mcp/servers/it7izu3ufb\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/it7izu3ufb/badge\" alt=\"mcp-dnstwist MCP server\" /></a>\n\n\n## ⚠️ Warning\n\nThis tool is designed for legitimate security research purposes. Please:\n- Only analyze domains you own or have permission to test\n- Respect rate limits and DNS server policies\n- Use responsibly and ethically\n- Be aware that some DNS servers may rate-limit or block automated queries\n- Consider the impact on DNS infrastructure when running large scans\n\n## Requirements\n\n- Node.js (v18 or later)\n- Docker\n- macOS, Linux, or Windows with Docker Desktop installed\n\n## Quick Start\n\n### Installing via Smithery\n\nTo install DNStwist for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@burtthecoder/mcp-dnstwist):\n\n```bash\nnpx -y @smithery/cli install @burtthecoder/mcp-dnstwist --client claude\n```\n\n### Installing Manually\n1. Install Docker:\n   - macOS: Install [Docker Desktop](https://www.docker.com/products/docker-desktop)\n   - Linux: Follow the [Docker Engine installation guide](https://docs.docker.com/engine/install/)\n\n2. Install the server globally via npm:\n```bash\nnpm install -g mcp-dnstwist\n```\n\n3. Add to your Claude Desktop configuration file:\n```json\n{\n  \"mcpServers\": {\n    \"dnstwist\": {\n      \"command\": \"mcp-dnstwist\"\n    }\n  }\n}\n```\n\nConfiguration file location:\n- macOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\n- Windows: `%APPDATA%\\Claude\\claude_desktop_config.json`\n\n4. Restart Claude Desktop\n\n## Alternative Setup (From Source)\n\nIf you prefer to run from source or need to modify the code:\n\n1. Clone and build:\n```bash\ngit clone <repository_url>\ncd mcp-dnstwist\nnpm install\nnpm run build\n```\n\n2. Add to your Claude Desktop configuration:\n```json\n{\n  \"mcpServers\": {\n    \"dnstwist\": {\n      \"command\": \"node\",\n      \"args\": [\"/absolute/path/to/mcp-dnstwist/build/index.js\"]\n    }\n  }\n}\n```\n\n## Features\n\n- **Domain Fuzzing**: Generate domain permutations using various algorithms\n- **Registration Check**: Verify if permutated domains are registered\n- **DNS Analysis**: Check A, AAAA, MX, and NS records\n- **Web Presence**: Capture HTTP banner information\n- **WHOIS Data**: Retrieve registration dates and registrar information\n- **Phishing Detection**: Generate fuzzy hashes of web pages\n- **Configurable**: Custom DNS servers and parallel processing\n- **Multiple Formats**: Support for json, csv, and list output formats\n\n## Tools\n\n### Domain Fuzzing Tool\n- Name: `fuzz_domain`\n- Description: Generate and analyze domain permutations to detect potential typosquatting, phishing, and brand impersonation\n- Parameters:\n  * `domain` (required): Domain name to analyze (e.g., example.com)\n  * `nameservers` (optional, default: \"1.1.1.1\"): Comma-separated list of DNS servers\n  * `threads` (optional, default: 50): Number of threads for parallel processing\n  * `format` (optional, default: \"json\"): Output format (json, csv, list)\n  * `registered_only` (optional, default: true): Show only registered domains\n  * `mxcheck` (optional, default: true): Check for MX records\n  * `ssdeep` (optional, default: false): Generate fuzzy hashes of web pages\n  * `banners` (optional, default: true): Capture HTTP banner information\n\nExample:\n```json\n{\n  \"domain\": \"example.com\",\n  \"nameservers\": \"1.1.1.1,8.8.8.8\",\n  \"threads\": 50,\n  \"format\": \"json\",\n  \"registered_only\": true,\n  \"mxcheck\": true,\n  \"banners\": true\n}\n```\n\n## Troubleshooting\n\n### Docker Issues\n\n1. Verify Docker is installed and running:\n```bash\ndocker --version\ndocker ps\n```\n\n2. Check Docker permissions:\n   - Ensure your user has permissions to run Docker commands\n   - On Linux, add your user to the docker group: `sudo usermod -aG docker $USER`\n\n### Common Issues\n\n1. DNS resolution problems:\n   - Verify DNS servers are accessible\n   - Try alternative DNS servers (e.g., 8.8.8.8)\n   - Check for rate limiting or blocking\n\n2. Performance issues:\n   - Adjust thread count based on system capabilities\n   - Consider network bandwidth and latency\n   - Monitor DNS server response times\n\n3. After fixing any issues:\n   - Save the configuration file\n   - Restart Claude Desktop\n\n## Error Messages\n\n- \"Docker is not installed or not running\": Install Docker and start the Docker daemon\n- \"Failed to parse dnstwist output\": Check if the domain is valid and the format is correct\n- \"Error executing dnstwist\": Check Docker logs and ensure proper permissions\n- \"DNS server not responding\": Verify DNS server accessibility and try alternative servers\n\n## Contributing\n\n1. Fork the repository\n2. Create a feature branch (`git checkout -b feature/amazing-feature`)\n3. Commit your changes (`git commit -m 'Add amazing feature'`)\n4. Push to the branch (`git push origin feature/amazing-feature`)\n5. Open a Pull Request\n\n## License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n",
      "npm_url": "https://www.npmjs.com/package/@burtthecoder/mcp-dnstwist",
      "npm_downloads": 4710,
      "keywords": [
        "dns",
        "phishing",
        "domain",
        "dns fuzzing",
        "mcp dnstwist",
        "dnstwist mcp"
      ],
      "category": "web-search"
    },
    "BurtTheCoder--mcp-maigret": {
      "owner": "BurtTheCoder",
      "name": "mcp-maigret",
      "url": "https://github.com/BurtTheCoder/mcp-maigret",
      "imageUrl": "/freedevtools/mcp/pfp/BurtTheCoder.webp",
      "description": "MCP server for conducting OSINT username searches across various social networks and analyzing URLs to collect user account information from public sources.",
      "stars": 192,
      "forks": 28,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-10-01T03:23:35Z",
      "readme_content": "# Maigret MCP Server\n[![smithery badge](https://smithery.ai/badge/mcp-maigret)](https://smithery.ai/server/mcp-maigret)\n\nA Model Context Protocol (MCP) server for [maigret](https://github.com/soxoj/maigret), a powerful OSINT tool that collects user account information from various public sources. This server provides tools for searching usernames across social networks and analyzing URLs. It is designed to integrate seamlessly with MCP-compatible applications like [Claude Desktop](https://claude.ai).\n\n<a href=\"https://glama.ai/mcp/servers/knnpcz651x\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/knnpcz651x/badge\" alt=\"mcp-maigret MCP server\" /></a>\n\n\n## ⚠️ Warning\n\nThis tool is designed for legitimate OSINT research purposes. Please:\n- Only search for information that is publicly available\n- Respect privacy and data protection laws\n- Follow the terms of service of the platforms being searched\n- Use responsibly and ethically\n- Be aware that some sites may rate-limit or block automated searches\n\n## Requirements\n\n- Node.js (v18 or later)\n- Docker\n- macOS, Linux, or Windows with Docker Desktop installed\n- Write access to the reports directory\n\n## Quick Start\n\n### Installing via Smithery\n\nTo install Maigret for Claude Desktop automatically via [Smithery](https://smithery.ai/server/mcp-maigret):\n\n```bash\nnpx -y @smithery/cli install mcp-maigret --client claude\n```\n\n### Installing Manually\n1. Install Docker:\n   - macOS: Install [Docker Desktop](https://www.docker.com/products/docker-desktop)\n   - Linux: Follow the [Docker Engine installation guide](https://docs.docker.com/engine/install/)\n\n2. Install the server globally via npm:\n```bash\nnpm install -g mcp-maigret\n```\n\n3. Create a reports directory:\n```bash\nmkdir -p /path/to/reports/directory\n```\n\n4. Add to your Claude Desktop configuration file:\n```json\n{\n  \"mcpServers\": {\n    \"maigret\": {\n      \"command\": \"mcp-maigret\",\n      \"env\": {\n        \"MAIGRET_REPORTS_DIR\": \"/path/to/reports/directory\"\n      }\n    }\n  }\n}\n```\n\nConfiguration file location:\n- macOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\n- Windows: `%APPDATA%\\Claude\\claude_desktop_config.json`\n\n5. Restart Claude Desktop\n\n## Alternative Setup (From Source)\n\nIf you prefer to run from source or need to modify the code:\n\n1. Clone and build:\n```bash\ngit clone <repository_url>\ncd mcp-maigret\nnpm install\nnpm run build\n```\n\n2. Add to your Claude Desktop configuration:\n```json\n{\n  \"mcpServers\": {\n    \"maigret\": {\n      \"command\": \"node\",\n      \"args\": [\"/absolute/path/to/mcp-maigret/build/index.js\"],\n      \"env\": {\n        \"MAIGRET_REPORTS_DIR\": \"/path/to/reports/directory\"\n      }\n    }\n  }\n}\n```\n\n## Features\n\n- **Username Search**: Search for a username across hundreds of social networks and websites\n- **URL Analysis**: Parse URLs to extract information and search for associated usernames\n- **Multiple Output Formats**: Support for txt, html, pdf, json, csv, and xmind formats\n- **Site Filtering**: Filter searches by site tags (e.g., photo, dating, us)\n- **Docker-based**: Reliable and consistent execution across environments\n\n## Tools\n\n### 1. Username Search Tool\n- Name: `search_username`\n- Description: Search for a username across social networks and sites\n- Parameters:\n  * `username` (required): Username to search for\n  * `format` (optional, default: \"pdf\"): Output format (txt, html, pdf, json, csv, xmind)\n  * `use_all_sites` (optional, default: false): Use all available sites instead of top 500\n  * `tags` (optional): Array of tags to filter sites (e.g., [\"photo\", \"dating\"])\n\nExample:\n```json\n{\n  \"username\": \"test_user123\",\n  \"format\": \"html\",\n  \"use_all_sites\": false,\n  \"tags\": [\"photo\"]\n}\n```\n\n### 2. URL Analysis Tool\n- Name: `parse_url`\n- Description: Parse a URL to extract information and search for associated usernames\n- Parameters:\n  * `url` (required): URL to analyze\n  * `format` (optional, default: \"pdf\"): Output format (txt, html, pdf, json, csv, xmind)\n\nExample:\n```json\n{\n  \"url\": \"https://example.com/profile\",\n  \"format\": \"txt\"\n}\n```\n\n## Troubleshooting\n\n### Docker Issues\n\n1. Verify Docker is installed and running:\n```bash\ndocker --version\ndocker ps\n```\n\n2. Check Docker permissions:\n   - Ensure your user has permissions to run Docker commands\n   - On Linux, add your user to the docker group: `sudo usermod -aG docker $USER`\n\n### Reports Directory Issues\n\n1. Verify the reports directory:\n   - The directory specified in MAIGRET_REPORTS_DIR must exist\n   - Your user must have write permissions to this directory\n   - Check permissions: `ls -la /path/to/reports/directory`\n\n2. Common configuration mistakes:\n   - Missing MAIGRET_REPORTS_DIR environment variable\n   - Directory doesn't exist\n   - Incorrect permissions\n   - Trailing slashes in the path\n\n3. After fixing any issues:\n   - Save the configuration file\n   - Restart Claude Desktop\n\n## Error Messages\n\n- \"Docker is not installed or not running\": Install Docker and start the Docker daemon\n- \"MAIGRET_REPORTS_DIR environment variable must be set\": Add the environment variable to your configuration\n- \"Error creating reports directory\": Check directory permissions and path\n- \"Error executing maigret\": Check Docker logs and ensure the container has proper permissions\n\n## Contributing\n\n1. Fork the repository\n2. Create a feature branch (`git checkout -b feature/amazing-feature`)\n3. Commit your changes (`git commit -m 'Add amazing feature'`)\n4. Push to the branch (`git push origin feature/amazing-feature`)\n5. Open a Pull Request\n\n## License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n",
      "npm_url": "https://www.npmjs.com/package/mcp-maigret",
      "npm_downloads": 5622,
      "keywords": [
        "searches",
        "search",
        "mcp",
        "username searches",
        "searches various",
        "web search"
      ],
      "category": "web-search"
    },
    "CaChiJ--kakao-navigation-mcp-server": {
      "owner": "CaChiJ",
      "name": "kakao-navigation-mcp-server",
      "url": "https://github.com/CaChiJ/kakao-navigation-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/CaChiJ.webp",
      "description": "Provides location search and route finding services using Kakao's mapping API, enabling geocoding of addresses and optimal path calculation across various transportation modes with consideration for real-time traffic data.",
      "stars": 9,
      "forks": 2,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-09-25T00:00:12Z",
      "readme_content": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/cachij-kakao-navigation-mcp-server-badge.png)](https://mseep.ai/app/cachij-kakao-navigation-mcp-server)\n\n# Kakao Mobility & Kakao Map MCP Server\n\n[![MCP](https://img.shields.io/badge/MCP-Compliant-blue)](https://github.com/cursor-ai/model-context-protocol)\n[![smithery badge](https://smithery.ai/badge/@CaChiJ/kakao-mobility-mcp-server)](https://smithery.ai/server/@CaChiJ/kakao-mobility-mcp-server)\n\n## 소개\n\n본 레포지토리는 Model Context Protocol (MCP)을 준수하여 카카오 모빌리티 및 카카오맵 API와 연동되는 서버를 제공합니다. 국내 환경에 적합한 길찾기 서비스를 제공하는 것을 목표로 합니다.\n\n## 주요 기능\n\n### 1. 위치 검색 (지오코딩)\n- 주소나 장소명을 좌표(위도/경도)로 변환\n- 정확한 위치 정보 제공\n\n### 2. 길찾기 서비스\n- 출발지에서 목적지까지의 최적 경로 검색\n- 도보, 자동차 등 다양한 이동 수단 지원\n- 실시간 교통 정보 반영\n\n## 시작하기\n1. [kakao developers](https://developers.kakao.com/)에 로그인합니다.\n2. 애플리케이션 생성\n  - '내 애플리케이션' > '애플리케이션 추가하기' > 애플리케이션 정보 입력 후 '저장'\n3. 카카오 맵 API 활성화\n  - 사이드바에서 '카카오 맵' 선택 > '활성화 설정' ON\n4. REST API Key 발급\n  - 사이드바에서 '앱 키' 선택 > 'REST API 키' 복사해 사용\n\n## 제공 도구\n\n본 서버는 다음과 같은 MCP 도구들을 제공합니다:\n\n- `geocode`: 주소를 좌표 정보로 지오코딩\n- `direction_search_by_names`: 출발지와 목적지 주소로 길찾기\n- `direction_search_by_coordinates`: 출발지와 목적지 좌표로 길찾기\n- `future_direction_search_by_coordinates`: 출발지와 목적지 좌표로 미래 특정 시점의 길찾기\n- `address_search_by_place_name`: 장소 이름으로 주소 찾기\n\n## 배포 정보\n\n본 서버의 배포 정보 및 최신 업데이트는 Smithery에서 확인하실 수 있습니다.\n\n---\nMade with ❤️ using Model Context Protocol\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "kakao",
        "geocoding",
        "cachij",
        "kakao navigation",
        "kakao mapping",
        "using kakao"
      ],
      "category": "web-search"
    },
    "Cam10001110101--mcp-server-ollama-deep-researcher": {
      "owner": "Cam10001110101",
      "name": "mcp-server-ollama-deep-researcher",
      "url": "https://github.com/Cam10001110101/mcp-server-ollama-deep-researcher",
      "imageUrl": "/freedevtools/mcp/pfp/Cam10001110101.webp",
      "description": "Conduct in-depth research on any topic by generating web search queries, gathering results, summarizing findings, and providing a comprehensive markdown summary of all sources used.",
      "stars": 14,
      "forks": 7,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-24T09:03:38Z",
      "readme_content": "# Ollama Deep Researcher DXT Extension\n\n## Overview\n\n**Ollama Deep Researcher** is a Desktop Extension (DXT) that enables advanced topic research using web search and LLM synthesis, powered by a local MCP server. It supports configurable research parameters, status tracking, and resource access, and is designed for seamless integration with the DXT ecosystem.\n\n- **Research any topic** using web search APIs and LLMs (Ollama, DeepSeek, etc.)\n- **Configure** max research loops, LLM model, and search API\n- **Track status** of ongoing research\n- **Access research results** as resources via MCP protocol\n\n## Features\n\n- Implements the MCP protocol over stdio for local, secure operation\n- Defensive programming: error handling, timeouts, and validation\n- Logging and debugging via stderr\n- Compatible with DXT host environments\n\n## Directory Structure\n\n```\n.\n├── manifest.json         # DXT manifest (see MANIFEST.md for spec)\n├── src/\n│   ├── index.ts         # MCP server entrypoint (Node.js, stdio transport)\n│   └── assistant/       # Python research logic\n│       └── run_research.py\n├── README.md            # This documentation\n└── ...\n```\n\n## Installation & Setup\n\n1. **Clone the repository** and install dependencies:\n   ```sh\n   git clone <your-repo-url>\n   cd mcp-server-ollama-deep-researcher\n   npm install\n   ```\n\n2. **Install Python dependencies** for the assistant:\n   ```sh\n   cd src/assistant\n   pip install -r requirements.txt\n   # or use pyproject.toml/uv if preferred\n   ```\n\n3. **Set required environment variables** for web search APIs:\n   - For Tavily: `TAVILY_API_KEY`\n   - For Perplexity: `PERPLEXITY_API_KEY`\n   - Example:\n     ```sh\n     export TAVILY_API_KEY=your_tavily_key\n     export PERPLEXITY_API_KEY=your_perplexity_key\n     ```\n\n4. **Build the TypeScript server** (if needed):\n   ```sh\n   npm run build\n   ```\n\n5. **Run the extension locally for testing:**\n   ```sh\n   node dist/index.js\n   # Or use the DXT host to load the extension per DXT documentation\n   ```\n\n## Usage\n\n- **Research a topic:**\n  - Use the `research` tool with `{ \"topic\": \"Your subject\" }`\n- **Get research status:**\n  - Use the `get_status` tool\n- **Configure research parameters:**\n  - Use the `configure` tool with any of: `maxLoops`, `llmModel`, `searchApi`\n\n## Manifest\n\nSee `manifest.json` for the full DXT manifest, including tool schemas and resource templates. Follows [DXT MANIFEST.md](https://github.com/anthropics/dxt/blob/main/MANIFEST.md).\n\n## Logging & Debugging\n\n- All server logs and errors are output to `stderr` for debugging.\n- Research subprocesses are killed after 5 minutes to prevent hangs.\n- Invalid requests and configuration errors return clear, structured error messages.\n\n## Security & Best Practices\n\n- All tool schemas are validated before execution.\n- API keys are required for web search APIs and are never logged.\n- MCP protocol is used over stdio for local, secure communication.\n\n## Testing & Validation\n\n- Validate the extension by loading it in a DXT-compatible host.\n- Ensure all tool calls return valid, structured JSON responses.\n- Check that the manifest loads and the extension registers as a DXT.\n\n## Troubleshooting\n\n- **Missing API key:** Ensure `TAVILY_API_KEY` or `PERPLEXITY_API_KEY` is set in your environment.\n- **Python errors:** Check Python dependencies and logs in `stderr`.\n- **Timeouts:** Research subprocesses are limited to 5 minutes.\n\n## References\n\n- [DXT Architecture Overview](https://github.com/anthropics/dxt/blob/main/README.md)\n- [DXT Manifest Spec](https://github.com/anthropics/dxt/blob/main/MANIFEST.md)\n- [DXT Example Extensions](https://github.com/anthropics/dxt/tree/main/examples)\n- [Model Context Protocol SDK](https://github.com/modelcontextprotocol/sdk)\n\n---\n\n© 2025 Your Name or Organization. Licensed under MIT.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "research",
        "search",
        "researcher",
        "web search",
        "deep researcher",
        "research topic"
      ],
      "category": "web-search"
    },
    "ChanMeng666--server-google-jobs": {
      "owner": "ChanMeng666",
      "name": "server-google-jobs",
      "url": "https://github.com/ChanMeng666/server-google-jobs",
      "imageUrl": "/freedevtools/mcp/pfp/ChanMeng666.webp",
      "description": "Provides capabilities to search for job listings using Google Jobs via SerpAPI integration, supporting multiple languages.",
      "stars": 13,
      "forks": 8,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-09-24T20:57:09Z",
      "readme_content": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/chanmeng666-server-google-jobs-badge.png)](https://mseep.ai/app/chanmeng666-server-google-jobs)\n\n<div align=\"center\">\n <h1><br/>Google Jobs MCP Server</h1>\n <img alt=\"TypeScript_007ACC_style_flat_logo_typescript_logoColor_white\" src=\"https://img.shields.io/badge/TypeScript-007ACC?style=flat&logo=typescript&logoColor=white\"/>\n <img alt=\"Node_js_43853D_style_flat_logo_node_js_logoColor_white\" src=\"https://img.shields.io/badge/Node.js-43853D?style=flat&logo=node.js&logoColor=white\"/>\n <img alt=\"MCP_Server_blue_style_flat\" src=\"https://img.shields.io/badge/MCP-Server-blue?style=flat\"/>\n <img alt=\"License_MIT_brightgreen_style_flat\" src=\"https://img.shields.io/badge/License-MIT-brightgreen?style=flat\"/>\n<a href=\"https://smithery.ai/server/@chanmeng666/google-jobs-server\"><img alt=\"Smithery Badge\" src=\"https://smithery.ai/badge/@chanmeng666/google-jobs-server\"></a>\n</div>\n\n<br/>\n\nA Model Context Protocol (MCP) server implementation that provides Google Jobs search capabilities via SerpAPI integration. Features multi-language support, flexible search parameters, and smart error handling.\n\n[![Ask DeepWiki](https://deepwiki.com/badge.svg)](https://deepwiki.com/ChanMeng666/server-google-jobs)\n\n[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/chanmeng666-server-google-jobs-badge.png)](https://mseep.ai/app/chanmeng666-server-google-jobs)\n[![Verified on MseeP](https://mseep.ai/badge.svg)](https://mseep.ai/app/df679562-50ad-4615-ac3c-a509c27583f0)\n\n<a href=\"https://glama.ai/mcp/servers/bijbpfhrbx\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/bijbpfhrbx/badge\" alt=\"Google Jobs Server MCP server\" />\n</a>\n\n<br/>\n\n[![👉Try It Now!👈](https://gradient-svg-generator.vercel.app/api/svg?text=%F0%9F%91%89Try%20It%20Now!%F0%9F%91%88&color=000000&height=60&gradientType=radial&duration=6s&color0=ffffff&template=pride-rainbow)](https://smithery.ai/server/@chanmeng666/google-jobs-server)\n\n<br/>\n\nhttps://github.com/user-attachments/assets/8f6739e1-7db7-4171-88b4-59c6290a4c72\n\n![屏幕截图 2024-12-31 183813](https://github.com/user-attachments/assets/fd02f916-7ba0-4d92-8970-79ccecdb1115)\n\n![屏幕截图 2024-12-31 183754](https://github.com/user-attachments/assets/22f497f5-381e-40d1-b082-d13d13239677)\n\n![屏幕截图 2024-12-31 180734](https://github.com/user-attachments/assets/19f74219-5059-4c49-95e9-3a1741d866d2)\n\n![屏幕截图 2024-12-31 182106](https://github.com/user-attachments/assets/5e88ec38-66cd-4f02-95b3-118007736dbd)\n\n\n# ✨ Features\n\n### 🌍 Multi-Language Support\nFull localization support for English, Chinese, Japanese and Korean with automatic language detection and fallback.\n\n### 🔍 Flexible Search Options\nComprehensive search parameters including:\n- Job title and keywords\n- Location with radius filtering\n- Employment type (full-time, part-time, etc.)\n- Salary range filters\n- Post date filtering\n- Results sorting\n\n### 💡 Smart Error Handling\n- Comprehensive input validation\n- Helpful error messages and suggestions\n- Automatic search refinement suggestions\n- Rate limit handling\n\n### 📊 Rich Job Details\n- Detailed job information formatting\n- Company benefits and highlights\n- Salary information when available\n- Direct application links\n- Job posting timestamps\n\n### 🔄 Advanced Features\n- Pagination support\n- Multiple sorting options\n- Geographic radius search\n- Employment type filtering\n\n# 🔑 SERP API Setup Guide\n\nBefore getting started, you'll need to obtain a SERP API key:\n\n1. Visit [SERP API website](https://serpapi.com/) and create an account\n\n2. After registration, go to your Dashboard:\n   - Locate the \"API Key\" section\n   - Copy your API key\n   - New users get 100 free API calls\n\n3. API Usage Details:\n   - Free tier: 100 searches per month\n   - Paid plans start at $50/month for 5000 searches\n   - Billing based on successful API calls\n   - Multiple payment methods: Credit Card, PayPal, etc.\n\n4. Usage Limits:\n   - Request Rate: 2 requests/second\n   - IP Restrictions: None\n   - Concurrent Requests: 5\n   - Response Cache Time: 1 hour\n\n# 👩‍🔧 Solution for MCP Servers Connection Issues with NVM/NPM\n\nClick to view my configuration solution 👉 https://github.com/modelcontextprotocol/servers/issues/76\n\n# 🚀 Quick Start\n\n1. Install dependencies:\n```bash\nnpm install\n```\n\n2. Configure environment:\nModify your `claude_desktop_config.json` with the following content (adjust paths according to your system):\n```json\n{\n  \"google-jobs\": {\n    \"command\": \"D:\\\\Program\\\\nvm\\\\node.exe\",\n    \"args\": [\"D:\\\\github_repository\\\\path_to\\\\dist\\\\index.js\"],\n    \"env\": {\n      \"SERP_API_KEY\": \"your-api-key\"\n    }\n  }\n}\n```\n\n3. Build the server:\n```bash\nnpm run build\n```\n\n4. Start the server:\n```bash\nnpm start\n```\n\n## Troubleshooting\n\n1. API Key Issues:\n- Verify key in configuration\n- Check key status in SERP API dashboard\n- Confirm key has remaining quota\n\n2. Search Issues:\n- Validate search parameters format\n- Check network connectivity\n- Verify country/language code support\n\n# 📦 Installation\n\n## Installing via Smithery\n\nTo install Google Jobs for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@chanmeng666/google-jobs-server):\n\n```bash\nnpx -y @smithery/cli install @chanmeng666/google-jobs-server --client claude\n```\n\n## Manual Installation\n\n<img alt=\"CB3837\" src=\"https://cdn.simpleicons.org/npm/CB3837\" height=\"14\"/> <a href=\"https://www.npmjs.com/package/@chanmeng666/google-jobs-server\">@chanmeng666/google-jobs-server</a>\n\n```bash\n# Using npm\nnpm i @chanmeng666/google-jobs-server\n# or\nnpm install @chanmeng666/google-jobs-server\n\n# Using yarn\nyarn add @chanmeng666/google-jobs-server\n\n# Using pnpm\npnpm add @chanmeng666/google-jobs-server\n```\n\n\n\n## Running evals\n\nThe evals package loads an mcp client that then runs the index.ts file, so there is no need to rebuild between tests. You can load environment variables by prefixing the npx command. Full documentation can be found [here](https://www.mcpevals.io/docs).\n\n```bash\nOPENAI_API_KEY=your-key  npx mcp-eval src/evals/evals.ts src/index.ts\n```\n# 💻 Tech Stack\n\n![TypeScript](https://img.shields.io/badge/typescript-%23007ACC.svg?style=for-the-badge&logo=typescript&logoColor=white)\n![NodeJS](https://img.shields.io/badge/node.js-6DA55F?style=for-the-badge&logo=node.js&logoColor=white)\n![MCP](https://img.shields.io/badge/MCP-SDK-blue?style=for-the-badge)\n\n# 📖 API Documentation\n\nThe server implements the Model Context Protocol and exposes a job search tool with the following parameters:\n\n- `query`: Search query string (required)\n- `location`: Job location (optional)\n- `posted_age`: Post date filter (optional)\n- `employment_type`: Job type filter (optional)\n- `salary`: Salary range filter (optional)\n- `radius`: Geographic search radius (optional)\n- `hl`: Language code (optional)\n- `page`: Pagination number (optional)\n- `sort_by`: Sort order (optional)\n\n# 🔧 Development\n\n```bash\n# Run in development mode\nnpm run dev\n\n# Run type checking\nnpm run typecheck\n\n# Build for production\nnpm run build\n```\n\n# 📝 License\n\nThis project is [MIT licensed](./LICENSE).\n\n# 🙋‍♀ Author\n\nCreated and maintained by [Chan Meng](https://chanmeng.org/).\n[![GitHub](https://img.shields.io/badge/github-%23121011.svg?style=normal&logo=github&logoColor=white)](https://github.com/ChanMeng666)\n[![LinkedIn](https://img.shields.io/badge/linkedin-%230077B5.svg?style=normal&logo=linkedin&logoColor=white)](https://www.linkedin.com/in/chanmeng666/)\n\n<details>\n<summary>🤖 AI Assistant Integration & GEO Optimization</summary>\n\n## AI Assistant Usage Guide\n\nThis MCP server is optimized for AI assistant integration. AI assistants can use this tool through the following methods:\n\n### Quick Start for AI\n1. **Installation**: `npm install @chanmeng666/google-jobs-server`\n2. **Configuration**: Add server configuration to MCP client\n3. **Usage**: Call the `search_jobs` tool for job searching\n\n### Search Parameters for AI\n- `query`: Search keywords (required)\n- `location`: Job location\n- `posted_age`: Post date filter (today, 3days, week, month)\n- `employment_type`: Job type (FULLTIME, PARTTIME, CONTRACTOR, INTERN)\n- `salary`: Salary range (e.g., $50K+, $100K+)\n- `radius`: Search radius (e.g., 10mi, 20mi, 50mi)\n- `hl`: Language code (en, zh-CN, ja, ko)\n- `page`: Page number (10 results per page)\n- `sort_by`: Sort order (date, relevance, salary)\n\n### AI Use Cases\n- Help users find specific job positions\n- Provide multi-language job search capabilities\n- Filter job opportunities based on criteria\n- Retrieve detailed job information and application links\n\n### Error Handling for AI\nThe system provides intelligent error prompts and search suggestions to help optimize search parameters.\n\n## Structured Data for AI Crawlers\n\n<script type=\"application/ld+json\">\n{\n  \"@context\": \"https://schema.org\",\n  \"@type\": \"SoftwareApplication\",\n  \"name\": \"Google Jobs MCP Server\",\n  \"description\": \"A powerful MCP server for searching Google Jobs via SerpAPI with multi-language support\",\n  \"url\": \"https://github.com/ChanMeng666/server-google-jobs\",\n  \"applicationCategory\": \"DeveloperApplication\",\n  \"operatingSystem\": \"Node.js\",\n  \"programmingLanguage\": \"TypeScript\",\n  \"author\": {\n    \"@type\": \"Person\",\n    \"name\": \"Chan Meng\",\n    \"url\": \"https://chanmeng.org/\"\n  },\n  \"offers\": {\n    \"@type\": \"Offer\",\n    \"price\": \"0\",\n    \"priceCurrency\": \"USD\"\n  },\n  \"featureList\": [\n    \"Multi-language support (English, Chinese, Japanese, Korean)\",\n    \"Flexible search parameters\",\n    \"Smart error handling\",\n    \"Rich job details formatting\",\n    \"Pagination support\",\n    \"Geographic radius search\"\n  ],\n  \"softwareVersion\": \"1.0.0\",\n  \"datePublished\": \"2024-12-31\",\n  \"license\": \"MIT\",\n  \"keywords\": [\n    \"google-jobs\",\n    \"job-search\",\n    \"mcp-server\",\n    \"serpapi\",\n    \"typescript\",\n    \"careers\",\n    \"employment\"\n  ]\n}\n</script>\n\n## AI Crawler Instructions\n\n<script type=\"text/llms.txt\">\n## AI Assistant Usage Guide\n\nThis is a Google Jobs MCP Server project. AI assistants can use it in the following ways:\n\n### Quick Start\n1. Install: npm install @chanmeng666/google-jobs-server\n2. Configure: Add server configuration to MCP client\n3. Use: Call search_jobs tool for job searching\n\n### Search Parameter Guide\n- query: Search keywords (required)\n- location: Job location\n- posted_age: Post date filter (today, 3days, week, month)\n- employment_type: Job type (FULLTIME, PARTTIME, CONTRACTOR, INTERN)\n- salary: Salary range (e.g., $50K+, $100K+)\n- radius: Search radius (e.g., 10mi, 20mi, 50mi)\n- hl: Language code (en, zh-CN, ja, ko)\n- page: Page number (10 results per page)\n- sort_by: Sort order (date, relevance, salary)\n\n### Use Cases\n- Help users find specific job positions\n- Provide multi-language job search capabilities\n- Filter job opportunities based on criteria\n- Retrieve detailed job information and application links\n\n### Error Handling\nThe system provides intelligent error prompts and search suggestions to help optimize search parameters.\n</script>\n\n</details>",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "google",
        "serpapi",
        "jobs",
        "google jobs",
        "jobs serpapi",
        "search job"
      ],
      "category": "web-search"
    },
    "Claw256--mcp-web-search": {
      "owner": "Claw256",
      "name": "mcp-web-search",
      "url": "https://github.com/Claw256/mcp-web-search",
      "imageUrl": "/freedevtools/mcp/pfp/Claw256.webp",
      "description": "Provides Google search capabilities with advanced filtering and allows viewing of web content, including markdown conversion. Features include rate limiting, caching, browser instance pooling, and bot detection avoidance.",
      "stars": 5,
      "forks": 5,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-04-15T16:27:05Z",
      "readme_content": "# Web Search MCP Server\n\nAn MCP server that provides Google search capabilities and web content viewing with advanced bot detection avoidance.\n\n## Features\n\n- Google Custom Search with advanced filtering\n- Web content viewing with markdown conversion\n- Rate limiting and caching\n- Browser instance pooling\n- Bot detection avoidance using rebrowser-puppeteer\n\n## Prerequisites\n\n- Bun runtime v1.0 or higher\n- Google API credentials (API key and Search Engine ID)\n\n## Installation\n\n```bash\n# Install dependencies\nbun install\n\n# Build the TypeScript files\nbun run build\n```\n\n## Configuration\n\n### Cookie Setup\n\nFor authenticated site access, you'll need to:\n\n1. Install the [Get cookies.txt LOCALLY](https://chromewebstore.google.com/detail/get-cookiestxt-locally/cclelndahbckbenkjhflpdbgdldlbecc) Chrome extension\n2. Visit the sites you want to authenticate with and log in\n3. Use the extension to export your cookies in JSON format\n4. Store the exported cookies file in a secure location\n5. Set the `BROWSER_COOKIES_PATH` environment variable to the absolute path of your cookies file\n\n### MCP Server Configuration\n\nAdd the server configuration to your MCP settings file:\n\n- For Cline: `%APPDATA%\\Code\\User\\globalStorage\\rooveterinaryinc.roo-cline\\settings\\cline_mcp_settings.json`\n- For Claude Desktop:\n  - MacOS/Linux: `~/Library/Application Support/Claude/claude_desktop_config.json`\n  - Windows: `%APPDATA%\\Claude\\claude_desktop_config.json`\n\n```json\n{\n  \"mcpServers\": {\n    \"web-search\": {\n      \"command\": \"bun\",\n      \"args\": [\n        \"run\",\n        \"/ABSOLUTE/PATH/TO/web_search_mcp/dist/index.js\"\n      ],\n      \"env\": {\n        \"GOOGLE_API_KEY\": \"your_api_key\",\n        \"GOOGLE_SEARCH_ENGINE_ID\": \"your_search_engine_id\",\n        \"MAX_CONCURRENT_BROWSERS\": \"3\",\n        \"BROWSER_TIMEOUT\": \"30000\",\n        \"RATE_LIMIT_WINDOW\": \"60000\",\n        \"RATE_LIMIT_MAX_REQUESTS\": \"60\",\n        \"SEARCH_CACHE_TTL\": \"3600\",\n        \"VIEW_URL_CACHE_TTL\": \"7200\",\n        \"MAX_CACHE_ITEMS\": \"1000\",\n        \"BROWSER_POOL_MIN\": \"1\",\n        \"BROWSER_POOL_MAX\": \"5\",\n        \"BROWSER_POOL_IDLE_TIMEOUT\": \"30000\",\n        \"REBROWSER_PATCHES_RUNTIME_FIX_MODE\": \"addBinding\",\n        \"REBROWSER_PATCHES_SOURCE_URL\": \"jquery.min.js\",\n        \"REBROWSER_PATCHES_UTILITY_WORLD_NAME\": \"util\",\n        \"REBROWSER_PATCHES_DEBUG\": \"0\",\n        \"BROWSER_COOKIES_PATH\": \"C:\\\\path\\\\to\\\\cookies.json\",\n        \"LOG_LEVEL\": \"info\",\n        \"NO_COLOR\": \"0\",\n        \"BUN_FORCE_COLOR\": \"1\",\n        \"FORCE_COLOR\": \"1\"\n      }\n    }\n  }\n}\n```\n\nReplace `/ABSOLUTE/PATH/TO/web_search_mcp` with the absolute path to your server directory.\n\n### Logging Configuration\n\nThe following environment variables control logging behavior:\n\n- `LOG_LEVEL`: Sets the logging level (error, warn, info, debug). Default: info\n- `NO_COLOR`: Disables colored output when set to \"1\"\n- `BUN_FORCE_COLOR`: Controls colored output in Bun runtime (set to \"0\" to disable)\n- `FORCE_COLOR`: Controls colored output globally (set to \"0\" to disable)\n\n## Bot Detection Avoidance\n\nThis server uses rebrowser-puppeteer to avoid bot detection:\n\n1. Runtime.Enable Leak Prevention:\n   - Uses the addBinding technique to avoid Runtime.Enable detection\n   - Works with web workers and iframes\n   - Maintains access to the main world context\n\n2. Source URL Masking:\n   - Changes Puppeteer's sourceURL to look like a legitimate script\n   - Helps avoid detection of automation tools\n\n3. Utility World Name:\n   - Uses a generic utility world name\n   - Prevents detection through world name patterns\n\n4. Browser Launch Configuration:\n   - Disables automation flags\n   - Uses optimized Chrome arguments\n   - Configures viewport and window settings\n\n## Using with Claude Desktop\n\n1. Make sure you have Claude Desktop installed and updated to the latest version\n2. Open your Claude Desktop configuration file:\n   - MacOS/Linux: `~/Library/Application Support/Claude/claude_desktop_config.json`\n   - Windows: `%APPDATA%\\Claude\\claude_desktop_config.json`\n\n3. Add the server configuration as shown in the [Configuration](#configuration) section above.\n\n4. Restart Claude Desktop\n5. Look for the hammer icon ![](https://mintlify.s3.us-west-1.amazonaws.com/mcp/images/claude-desktop-mcp-hammer-icon.svg) to confirm the tools are available\n\n## Available Tools\n\n### 1. Search Tool\n```typescript\n{\n  name: \"search\",\n  params: {\n    query: string;\n    trustedDomains?: string[];\n    excludedDomains?: string[];\n    resultCount?: number;\n    safeSearch?: boolean;\n    dateRestrict?: string;\n  }\n}\n```\n\n### 2. View URL Tool\n```typescript\n{\n  name: \"view_url\",\n  params: {\n    url: string;\n    includeImages?: boolean;\n    includeVideos?: boolean;\n    preserveLinks?: boolean;\n    formatCode?: boolean;\n  }\n}\n```\n\n## Troubleshooting\n\n### Claude Desktop Integration Issues\n\n1. Check the logs:\n   ```bash\n   # MacOS/Linux\n   tail -n 20 -f ~/Library/Logs/Claude/mcp*.log\n   \n   # Windows\n   type %APPDATA%\\Claude\\Logs\\mcp*.log\n   ```\n\n2. Common issues:\n   - Server not showing up: Check configuration file syntax and paths\n   - Tool calls failing: Check server logs and restart Claude Desktop\n   - Path issues: Ensure you're using absolute paths\n\nFor more detailed troubleshooting, refer to the [MCP debugging guide](https://modelcontextprotocol.io/docs/tools/debugging).\n\n## Development\n\n```bash\n# Run in development mode with watch\nbun --watch run dev\n\n# Run tests\nbun run test\n\n# Run linter\nbun run lint\n```\n\n## Important Notes\n\n1. Bot Detection:\n   - The bot detection avoidance features help prevent most common detection methods\n   - However, additional measures like proper proxies and user agents may be needed\n   - Some websites may still detect automation through other means\n\n2. Performance:\n   - Browser instances are pooled and reused\n   - Idle browsers are automatically cleaned up\n   - Resource limits prevent overloading\n\n## License\n\nMIT",
      "npm_url": "https://www.npmjs.com/package/mcp-web-search",
      "npm_downloads": 0,
      "keywords": [
        "markdown",
        "search",
        "claw256",
        "mcp web",
        "web search",
        "search claw256"
      ],
      "category": "web-search"
    },
    "Cleversoft-IT--drupal-modules-mcp": {
      "owner": "Cleversoft-IT",
      "name": "drupal-modules-mcp",
      "url": "https://github.com/Cleversoft-IT/drupal-modules-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/Cleversoft-IT.webp",
      "description": "Retrieve detailed information about Drupal modules directly from drupal.org, including version compatibility and installation instructions.",
      "stars": 0,
      "forks": 2,
      "license": "No License",
      "language": "JavaScript",
      "updated_at": "2025-02-15T13:55:53Z",
      "readme_content": "# drupal-modules-mcp MCP Server\n\nA Model Context Protocol server for retrieving Drupal module information from drupal.org.\n\nThis TypeScript-based MCP server provides tools to fetch detailed information about Drupal modules directly from drupal.org. It helps AI assistants and other tools to get accurate, up-to-date information about Drupal modules including version compatibility, installation instructions, and documentation.\n\n## Features\n\n### Tools\n- `get_module_info` - Fetch comprehensive information about a Drupal module\n  - Requires the module's machine name as parameter\n  - Returns detailed module information including:\n    - Name and description\n    - Latest recommended version\n    - Download statistics\n    - Module status\n    - Composer installation command\n    - Drupal version compatibility\n    - Project URL\n    - Module documentation/README\n\n## Development\n\nInstall dependencies:\n```bash\nnpm install\n```\n\nBuild the server:\n```bash\nnpm run build\n```\n\nFor development with auto-rebuild:\n```bash\nnpm run watch\n```\n\n## Installation\n\n### Claude Desktop\n\nAdd the server config to your Claude Desktop configuration:\n\nOn MacOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\nOn Windows: `%APPDATA%/Claude/claude_desktop_config.json`\n\n```json\n{\n  \"mcpServers\": {\n    \"drupal-modules-mcp\": {\n      \"command\": \"/path/to/drupal-modules-mcp/build/index.js\"\n    }\n  }\n}\n```\n\n### Cline, Roo-Cline, and Windsurf\n\nAdd the server configuration to your IDE's settings:\n\n1. Open the IDE settings\n2. Navigate to the MCP Servers section\n3. Add a new server with the following configuration:\n   ```json\n   {\n     \"drupal-modules-mcp\": {\n       \"command\": \"/path/to/drupal-modules-mcp/build/index.js\"\n     }\n   }\n   ```\n\nMake sure to replace `/path/to/drupal-modules-mcp` with the actual path where you installed the server.\n\n### Debugging\n\nSince MCP servers communicate over stdio, debugging can be challenging. We recommend using the [MCP Inspector](https://github.com/modelcontextprotocol/inspector), which is available as a package script:\n\n```bash\nnpm run inspector\n```\n\nThe Inspector will provide a URL to access debugging tools in your browser.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "modules",
        "drupal",
        "mcp",
        "modules mcp",
        "drupal modules",
        "cleversoft drupal"
      ],
      "category": "web-search"
    },
    "Codeshark-NET--climate-triage-mcp": {
      "owner": "Codeshark-NET",
      "name": "climate-triage-mcp",
      "url": "https://github.com/Codeshark-NET/climate-triage-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/Codeshark-NET.webp",
      "description": "Integrates with the ClimateTriage API to facilitate searching for open source issues related to climate change and sustainability. Users can filter and sort these issues based on various criteria to find relevant projects for contribution.",
      "stars": 1,
      "forks": 4,
      "license": "No License",
      "language": "TypeScript",
      "updated_at": "2025-04-09T07:17:11Z",
      "readme_content": "# ClimateTriage MCP Server\n\nAn MCP server implementation that integrates with the ClimateTriage API, providing tools to search for open source issues related to climate change and sustainability.\n\n## Features\n\n- **Issue Search**: Find open source issues in climate-related projects\n- **Multiple Filters**: Filter by category, programming language, keywords, and more\n- **Sorting Options**: Sort issues by creation date, update date, or stars\n\n## Tools\n\n### search_climate_issues\n\nSearches for open source issues related to climate change and sustainability.\n\n**Inputs:**\n- `category` (string, optional): Filter issues by project category (e.g., 'Climate Change', 'Energy Systems')\n- `language` (string, optional): Filter issues by programming language (e.g., 'JavaScript', 'Python')\n- `keyword` (string, optional): Filter issues by project keyword (e.g., 'good first issue', 'help wanted')\n- `page` (number, optional): Pagination page number (starts at 1)\n- `per_page` (number, optional): Number of records per page (default: 10)\n- `sort` (string, optional): Field to sort by ('created_at', 'updated_at', 'stars', default: 'created_at')\n- `order` (string, optional): Sort order ('asc' or 'desc', default: 'desc' for most recent first)\n\n## Configuration\n\nBy default, the server connects to the ClimateTriage API at `https://ost.ecosyste.ms/api/v1`.\n\n## Usage with Claude Desktop\n\nAdd this to your `claude_desktop_config.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"climate-triage\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"git+https://github.com/Codeshark-NET/climate-triage-mcp.git\"]\n    }\n  }\n}\n```\n\n## License\nThis MCP server is licensed under the MIT License. This means you are free to use, modify, and distribute the software, subject to the terms and conditions of the MIT License.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "climatetriage",
        "climate",
        "codeshark",
        "climatetriage api",
        "climate triage",
        "integrates climatetriage"
      ],
      "category": "web-search"
    },
    "Cognitive-Stack--search-stock-news-mcp": {
      "owner": "Cognitive-Stack",
      "name": "search-stock-news-mcp",
      "url": "https://github.com/Cognitive-Stack/search-stock-news-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/Cognitive-Stack.webp",
      "description": "Search real-time stock news with customizable queries and domain-specific filters using the Tavily API. Provides type-safe, configurable search operations for AI workflows.",
      "stars": 6,
      "forks": 4,
      "license": "No License",
      "language": "TypeScript",
      "updated_at": "2025-07-15T00:57:49Z",
      "readme_content": "<h1 align=\"center\">Search Stock News MCP Server 🚀</h1>\n\n<p align=\"center\">\n  \n</p>\n\n> 🔌 **Compatible with Cline, Cursor, Claude Desktop, and any other MCP Clients!**\n> \n> Search Stock News MCP works seamlessly with any MCP client\n\nThe Model Context Protocol (MCP) is an open standard that enables AI systems to interact seamlessly with various data sources and tools, facilitating secure, two-way connections.\n\nThe Search Stock News MCP server provides:\n\n* Real-time stock news search capabilities via Tavily API\n* Multiple customizable search query templates\n* Configurable search parameters and filtering\n* Domain-specific content filtering\n* Type-safe operations with TypeScript\n\n## Prerequisites 🔧\n\nBefore you begin, ensure you have:\n\n* Tavily API Key\n* Claude Desktop, Cursor, or any MCP-compatible client\n* Node.js (v16 or higher)\n* Git installed (only needed if using Git installation method)\n\n## Search Stock News MCP Server Installation ⚡\n\n### Running with NPX\n\n```bash\nnpx -y search-stock-news-mcp@latest\n```\n\n### Installing via Smithery\n\nTo install Search Stock News MCP Server for Claude Desktop automatically via Smithery:\n\n```bash\nnpx -y @smithery/cli install search-stock-news-mcp --client claude\n```\n\n## Configuring MCP Clients ⚙️\n\n### Configuring Cline 🤖\n\nThe easiest way to set up the Search Stock News MCP server in Cline is through the marketplace:\n\n1. Open Cline in VS Code\n2. Click on the Cline icon in the sidebar\n3. Navigate to the \"MCP Servers\" tab\n4. Search \"Search Stock News\" and click \"install\"\n5. When prompted, enter your Tavily API key\n\nAlternatively, manually configure the server in Cline:\n\n1. Open the Cline MCP settings file:\n```bash\n# For macOS:\ncode ~/Library/Application\\ Support/Code/User/globalStorage/saoudrizwan.claude-dev/settings/cline_mcp_settings.json\n\n# For Windows:\ncode %APPDATA%\\Code\\User\\globalStorage\\saoudrizwan.claude-dev\\settings\\cline_mcp_settings.json\n```\n\n2. Add the Search Stock News server configuration:\n```json\n{\n  \"mcpServers\": {\n    \"search-stock-news-mcp\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"search-stock-news-mcp@latest\"],\n      \"env\": {\n        \"TAVILY_API_KEY\": \"your-api-key-here\"\n      },\n      \"disabled\": false,\n      \"autoApprove\": []\n    }\n  }\n}\n```\n\n### Configuring Cursor 🖥️\n\nTo set up the Search Stock News MCP server in Cursor:\n\n1. Open Cursor Settings\n2. Navigate to Features > MCP Servers\n3. Click on the \"+ Add New MCP Server\" button\n4. Fill out the following information:\n   * **Name**: \"search-stock-news-mcp\"\n   * **Type**: \"command\"\n   * **Command**:\n   ```bash\n   env TAVILY_API_KEY=your-api-key-here npx -y search-stock-news-mcp@latest\n   ```\n\n### Configuring Claude Desktop 🖥️\n\n#### For macOS:\n```bash\ntouch \"$HOME/Library/Application Support/Claude/claude_desktop_config.json\"\nopen -e \"$HOME/Library/Application Support/Claude/claude_desktop_config.json\"\n```\n\n#### For Windows:\n```bash\ncode %APPDATA%\\Claude\\claude_desktop_config.json\n```\n\nAdd the server configuration:\n```json\n{\n  \"mcpServers\": {\n    \"search-stock-news-mcp\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"search-stock-news-mcp@latest\"],\n      \"env\": {\n        \"TAVILY_API_KEY\": \"your-api-key-here\"\n      }\n    }\n  }\n}\n```\n\n## Usage Examples 🎯\n\n1. **Basic Stock News Search**:\n```json\n{\n  \"symbol\": \"AAPL\",\n  \"companyName\": \"Apple Inc.\",\n  \"maxResults\": 10\n}\n```\n\n2. **Advanced Search with Filters**:\n```json\n{\n  \"symbol\": \"TSLA\",\n  \"companyName\": \"Tesla Inc.\",\n  \"maxResults\": 20,\n  \"searchDepth\": \"advanced\",\n  \"minScore\": 0.6\n}\n```\n\n3. **Custom Domain Search**:\n```json\n{\n  \"symbol\": \"MSFT\",\n  \"companyName\": \"Microsoft Corporation\",\n  \"includeDomains\": [\"reuters.com\", \"bloomberg.com\"]\n}\n```\n\n## Troubleshooting 🛠️\n\n### Common Issues\n\n1. **Server Not Found**\n   * Verify npm installation\n   * Check configuration syntax\n   * Ensure Node.js is properly installed\n\n2. **API Key Issues**\n   * Verify your Tavily API key is valid\n   * Check the API key is correctly set in config\n   * Ensure no spaces or quotes around the API key\n\n3. **Search Results Issues**\n   * Check search parameters are within valid ranges\n   * Verify domain filters are correctly formatted\n   * Ensure company name and symbol are accurate\n\n## Acknowledgments ✨\n\n* Model Context Protocol for the MCP specification\n* Anthropic for Claude Desktop\n* Tavily for the News Search API\n\n## License\n\nMIT",
      "npm_url": "https://www.npmjs.com/package/search-stock-news-mcp",
      "npm_downloads": 5035,
      "keywords": [
        "search",
        "api",
        "queries",
        "search operations",
        "search stock",
        "web search"
      ],
      "category": "web-search"
    },
    "ConechoAI--openai-websearch-mcp": {
      "owner": "ConechoAI",
      "name": "openai-websearch-mcp",
      "url": "https://github.com/ConechoAI/openai-websearch-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/ConechoAI.webp",
      "description": "Provides real-time web search capabilities for AI assistants, enhancing their responses with current information from the web. It integrates with applications like Claude.app and Zed editor for streamlined usage during conversations.",
      "stars": 65,
      "forks": 13,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-29T16:14:37Z",
      "readme_content": "# OpenAI WebSearch MCP Server 🔍\n\n[![PyPI version](https://badge.fury.io/py/openai-websearch-mcp.svg)](https://badge.fury.io/py/openai-websearch-mcp)\n[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)\n[![MCP Compatible](https://img.shields.io/badge/MCP-Compatible-green.svg)](https://modelcontextprotocol.io/)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n\nAn advanced MCP server that provides intelligent web search capabilities using OpenAI's reasoning models. Perfect for AI assistants that need up-to-date information with smart reasoning capabilities.\n\n## ✨ Features\n\n- **🧠 Reasoning Model Support**: Full compatibility with OpenAI's latest reasoning models (gpt-5, gpt-5-mini, gpt-5-nano, o3, o4-mini)\n- **⚡ Smart Effort Control**: Intelligent `reasoning_effort` defaults based on use case\n- **🔄 Multi-Mode Search**: Fast iterations with gpt-5-mini or deep research with gpt-5\n- **🌍 Localized Results**: Support for location-based search customization\n- **📝 Rich Descriptions**: Complete parameter documentation for easy integration\n- **🔧 Flexible Configuration**: Environment variable support for easy deployment\n\n## 🚀 Quick Start\n\n### One-Click Installation for Claude Desktop\n\n```bash\nOPENAI_API_KEY=sk-xxxx uvx --with openai-websearch-mcp openai-websearch-mcp-install\n```\n\nReplace `sk-xxxx` with your OpenAI API key from the [OpenAI Platform](https://platform.openai.com/).\n\n## ⚙️ Configuration\n\n### Claude Desktop\n\nAdd to your `claude_desktop_config.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"openai-websearch-mcp\": {\n      \"command\": \"uvx\",\n      \"args\": [\"openai-websearch-mcp\"],\n      \"env\": {\n        \"OPENAI_API_KEY\": \"your-api-key-here\",\n        \"OPENAI_DEFAULT_MODEL\": \"gpt-5-mini\"\n      }\n    }\n  }\n}\n```\n\n### Cursor\n\nAdd to your MCP settings in Cursor:\n\n1. Open Cursor Settings (`Cmd/Ctrl + ,`)\n2. Search for \"MCP\" or go to Extensions → MCP\n3. Add server configuration:\n\n```json\n{\n  \"mcpServers\": {\n    \"openai-websearch-mcp\": {\n      \"command\": \"uvx\",\n      \"args\": [\"openai-websearch-mcp\"],\n      \"env\": {\n        \"OPENAI_API_KEY\": \"your-api-key-here\",\n        \"OPENAI_DEFAULT_MODEL\": \"gpt-5-mini\"\n      }\n    }\n  }\n}\n```\n\n### Claude Code\n\nClaude Code automatically detects MCP servers configured for Claude Desktop. Use the same configuration as above for Claude Desktop.\n\n### Local Development\n\nFor local testing, use the absolute path to your virtual environment:\n\n```json\n{\n  \"mcpServers\": {\n    \"openai-websearch-mcp\": {\n      \"command\": \"/path/to/your/project/.venv/bin/python\",\n      \"args\": [\"-m\", \"openai_websearch_mcp\"],\n      \"env\": {\n        \"OPENAI_API_KEY\": \"your-api-key-here\",\n        \"OPENAI_DEFAULT_MODEL\": \"gpt-5-mini\",\n        \"PYTHONPATH\": \"/path/to/your/project/src\"\n      }\n    }\n  }\n}\n```\n\n## 🛠️ Available Tools\n\n### `openai_web_search`\n\nIntelligent web search with reasoning model support.\n\n#### Parameters\n\n| Parameter | Type | Description | Default |\n|-----------|------|-------------|---------|\n| `input` | `string` | The search query or question to search for | *Required* |\n| `model` | `string` | AI model to use. Supports gpt-4o, gpt-4o-mini, gpt-5, gpt-5-mini, gpt-5-nano, o3, o4-mini | `gpt-5-mini` |\n| `reasoning_effort` | `string` | Reasoning effort level: low, medium, high, minimal | Smart default |\n| `type` | `string` | Web search API version | `web_search_preview` |\n| `search_context_size` | `string` | Context amount: low, medium, high | `medium` |\n| `user_location` | `object` | Optional location for localized results | `null` |\n\n## 💬 Usage Examples\n\nOnce configured, simply ask your AI assistant to search for information using natural language:\n\n### Quick Search\n> \"Search for the latest developments in AI reasoning models using openai_web_search\"\n\n### Deep Research  \n> \"Use openai_web_search with gpt-5 and high reasoning effort to provide a comprehensive analysis of quantum computing breakthroughs\"\n\n### Localized Search\n> \"Search for local tech meetups in San Francisco this week using openai_web_search\"\n\nThe AI assistant will automatically use the `openai_web_search` tool with appropriate parameters based on your request.\n\n## 🤖 Model Selection Guide\n\n### Quick Multi-Round Searches 🚀\n- **Recommended**: `gpt-5-mini` with `reasoning_effort: \"low\"`\n- **Use Case**: Fast iterations, real-time information, multiple quick queries\n- **Benefits**: Lower latency, cost-effective for frequent searches\n\n### Deep Research 🔬\n- **Recommended**: `gpt-5` with `reasoning_effort: \"medium\"` or `\"high\"`\n- **Use Case**: Comprehensive analysis, complex topics, detailed investigation\n- **Benefits**: Multi-round reasoned results, no need for agent iterations\n\n### Model Comparison\n\n| Model | Reasoning | Default Effort | Best For |\n|-------|-----------|----------------|----------|\n| `gpt-4o` | ❌ | N/A | Standard search |\n| `gpt-4o-mini` | ❌ | N/A | Basic queries |\n| `gpt-5-mini` | ✅ | `low` | Fast iterations |\n| `gpt-5` | ✅ | `medium` | Deep research |\n| `gpt-5-nano` | ✅ | `medium` | Balanced approach |\n| `o3` | ✅ | `medium` | Advanced reasoning |\n| `o4-mini` | ✅ | `medium` | Efficient reasoning |\n\n## 📦 Installation\n\n### Using uvx (Recommended)\n\n```bash\n# Install and run directly\nuvx openai-websearch-mcp\n\n# Or install globally\nuvx install openai-websearch-mcp\n```\n\n### Using pip\n\n```bash\n# Install from PyPI\npip install openai-websearch-mcp\n\n# Run the server\npython -m openai_websearch_mcp\n```\n\n### From Source\n\n```bash\n# Clone the repository\ngit clone https://github.com/yourusername/openai-websearch-mcp.git\ncd openai-websearch-mcp\n\n# Install dependencies\nuv sync\n\n# Run in development mode\nuv run python -m openai_websearch_mcp\n```\n\n## 👩‍💻 Development\n\n### Setup Development Environment\n\n```bash\n# Clone and setup\ngit clone https://github.com/yourusername/openai-websearch-mcp.git\ncd openai-websearch-mcp\n\n# Create virtual environment and install dependencies\nuv sync\n\n# Run tests\nuv run python -m pytest\n\n# Install in development mode\nuv pip install -e .\n```\n\n### Environment Variables\n\n| Variable | Description | Default |\n|----------|-------------|---------|\n| `OPENAI_API_KEY` | Your OpenAI API key | *Required* |\n| `OPENAI_DEFAULT_MODEL` | Default model to use | `gpt-5-mini` |\n\n## 🐛 Debugging\n\n### Using MCP Inspector\n\n```bash\n# For uvx installations\nnpx @modelcontextprotocol/inspector uvx openai-websearch-mcp\n\n# For pip installations\nnpx @modelcontextprotocol/inspector python -m openai_websearch_mcp\n```\n\n### Common Issues\n\n**Issue**: \"Unsupported parameter: 'reasoning.effort'\"\n**Solution**: This occurs when using non-reasoning models (gpt-4o, gpt-4o-mini) with reasoning_effort parameter. The server automatically handles this by only applying reasoning parameters to compatible models.\n\n**Issue**: \"No module named 'openai_websearch_mcp'\"\n**Solution**: Ensure you've installed the package correctly and your Python path includes the package location.\n\n## 📄 License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n\n## 🙏 Acknowledgments\n\n- 🤖 Generated with [Claude Code](https://claude.ai/code)\n- 🔥 Powered by [OpenAI's Web Search API](https://openai.com)\n- 🛠️ Built on the [Model Context Protocol](https://modelcontextprotocol.io/)\n\n---\n\n**Co-Authored-By**: Claude <noreply@anthropic.com>",
      "npm_url": "https://www.npmjs.com/package/openai-websearch-mcp",
      "npm_downloads": 821,
      "keywords": [
        "websearch",
        "openai",
        "search",
        "openai websearch",
        "websearch mcp",
        "search conechoai"
      ],
      "category": "web-search"
    },
    "CooKey-Monster--EbayMcpServer": {
      "owner": "CooKey-Monster",
      "name": "EbayMcpServer",
      "url": "https://github.com/CooKey-Monster/EbayMcpServer",
      "imageUrl": "/freedevtools/mcp/pfp/CooKey-Monster.webp",
      "description": "Fetches auction listings from Ebay.com using simple prompts to discover various items, like comics or collectibles, by leveraging Ebay's REST API for real-time auction data.",
      "stars": 6,
      "forks": 5,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-10-02T05:23:12Z",
      "readme_content": "# Ebay MCP server\n\nSimple Ebay server that lets you fetch auctions from Ebay.com\n\nUses the official [MCP Python SDK](https://github.com/modelcontextprotocol/python-sdk) to handle protocol communication and server interactions.\n\n## Example\n\nLet's you use prompts like, \"Find me 10 auctions for batman comics\"\n\n## Components\n\n### Tools\n\nThe server provides a single tool:\n\n- list_auction: Scan ebay for auctions. This tool is helpful for finding auctions on ebay.\n  - Required \"query\" argument for the search query\n  - Optional \"ammount\" argument for ammount of results\n    - defaults to 0\n  - Returns result from Ebay's REST API\n\n## Installation\n\n### Requires [UV](https://github.com/astral-sh/uv) (Fast Python package and project manager)\n\nIf uv isn't installed.\n\n```bash\n# Using Homebrew on macOS\nbrew install uv\n```\n\nor\n\n```bash\n# On macOS and Linux.\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n\n# On Windows.\npowershell -ExecutionPolicy ByPass -c \"irm https://astral.sh/uv/install.ps1 | iex\"\n```\n\nNext, install the MCP server\n\n```bash\n# Install from source\nuv pip install git+https://github.com/CooKey-Monster/EbayMcpServer.git\n```\n\n### Environment Variables\n\nThe following environment variable is required; you can find them on the [Ebay developer portal](https://developer.ebay.com/develop)\n\n- `CLIENT_ID`: Your Ebay client ID\n- `CLIENT_SECRET`: Your Ebay client secret\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "ebaymcpserver",
        "auction",
        "monster",
        "monster ebaymcpserver",
        "ebaymcpserver fetches",
        "leveraging ebay"
      ],
      "category": "web-search"
    },
    "Cooooper--KKJSBridge": {
      "owner": "Cooooper",
      "name": "KKJSBridge",
      "url": "https://github.com/Cooooper/KKJSBridge",
      "imageUrl": "/freedevtools/mcp/pfp/Cooooper.webp",
      "description": "KKJSBridge enhances the WKWebView experience by providing offline packages, enabling Ajax requests, and synchronizing cookies. It supports modular JSAPI management and allows control over Ajax hooks from both native and HTML5 frameworks.",
      "stars": 0,
      "forks": 0,
      "license": "MIT License",
      "language": "",
      "updated_at": "2024-07-30T05:50:01Z",
      "readme_content": "# KKJSBridge\n\n一站式解决 WKWebView 支持离线包，Ajax 请求和 Cookie 同步的问题 (基于 Ajax Hook 和 Cookie Hook)\n\n[更详细的介绍](http://karosli.com/2019/08/30/%E4%B8%80%E7%AB%99%E5%BC%8F%E8%A7%A3%E5%86%B3WKWebView%E5%90%84%E7%B1%BB%E9%97%AE%E9%A2%98/)\n\n## KKJSBridge 支持的功能\n\n- 基于 MessageHandler 搭建通信层\n\n- 支持模块化的管理 JSAPI\n\n- 支持模块共享上下文信息\n\n- 支持模块消息转发\n\n- 支持离线资源\n\n- 支持 ajax hook 避免 body 丢失\n\n- Native 和 H5 侧都可以控制 ajax hook 开关\n\n- Cookie 统一管理\n\n- WKWebView 复用\n\n- 兼容 WebViewJavascriptBridge\n\n\n\n## Demo\n\n模块化调用 JSAPI\n\n![模块化调用 JSAPI](https://github.com/karosLi/KKJSBridge/blob/master/Demo1.gif)\n\n\n\najax hook 演示\n\n![ajax hook 演示](https://github.com/karosLi/KKJSBridge/blob/master/Demo2.gif)\n\n\n\n淘宝 ajax hook 演示\n\n![淘宝 ajax hook 演示](https://github.com/karosLi/KKJSBridge/blob/master/Demo3.gif)\n\n\n\n\n\n## 用法\n\n从复用池取出缓存的 WKWebView，并开启 ajax hook\n\n```objectivec\n+ (void)load {\n    __block id observer = [[NSNotificationCenter defaultCenter] addObserverForName:UIApplicationDidFinishLaunchingNotification object:nil queue:nil usingBlock:^(NSNotification * _Nonnull note) {\n        [self prepareWebView];\n        [[NSNotificationCenter defaultCenter] removeObserver:observer];\n    }];\n}\n\n+ (void)prepareWebView {\n    // 预先缓存一个 webView\n    [KKWebView configCustomUAWithType:KKWebViewConfigUATypeAppend UAString:@\"KKJSBridge/1.0.0\"];\n    [[KKWebViewPool sharedInstance] enqueueWebViewWithClass:KKWebView.class];\n}\n\n- (void)dealloc {\n    // 回收到复用池\n    [[KKWebViewPool sharedInstance] enqueueWebView:self.webView];\n}\n\n- (void)commonInit {\n    _webView = [[KKWebViewPool sharedInstance] dequeueWebViewWithClass:KKWebView.class webViewHolder:self];\n    _webView.configuration.allowsInlineMediaPlayback = YES;\n    _webView.configuration.preferences.minimumFontSize = 12;\n    _webView.hybirdDelegate = self;\n    _jsBridgeEngine = [KKJSBridgeEngine bridgeForWebView:self.webView];\n    _jsBridgeEngine.config.enableAjaxHook = YES;\n\n    [self registerModule];\n}\n```\n\n注册模块\n\n```objectivec\n- (void)registerModule {\n ModuleContext *context = [ModuleContext new];\n context.vc = self;\n context.scrollView = self.webView.scrollView;\n context.name = @\"上下文\";\n // 注册 默认模块\n [self.jsBridgeEngine.moduleRegister registerModuleClass:ModuleDefault.class];\n // 注册 模块A\n [self.jsBridgeEngine.moduleRegister registerModuleClass:ModuleA.class];\n // 注册 模块B 并带入上下文\n [self.jsBridgeEngine.moduleRegister registerModuleClass:ModuleB.class withContext:context];\n // 注册 模块C\n [self.jsBridgeEngine.moduleRegister registerModuleClass:ModuleC.class];\n}\n```\n\n模块定义\n\n```objectivec\n@interface ModuleB()<KKJSBridgeModule>\n\n@property (nonatomic, weak) ModuleContext *context;\n\n@end\n\n@implementation ModuleB\n\n// 模块名称\n+ (nonnull NSString *)moduleName {\n    return @\"b\";\n}\n\n// 单例模块\n+ (BOOL)isSingleton {\n    return YES;\n}\n\n// 模块初始化方法，支持上下文带入\n- (instancetype)initWithEngine:(KKJSBridgeEngine *)engine context:(id)context {\n    if (self = [super init]) {\n        _context = context;\n        NSLog(@\"ModuleB 初始化并带上 %@\", self.context.name);\n    }\n\n    return self;\n}\n\n// 模块提供的方法\n- (void)callToGetVCTitle:(KKJSBridgeEngine *)engine params:(NSDictionary *)params responseCallback:(void (^)(NSDictionary *responseData))responseCallback {\n    responseCallback ? responseCallback(@{@\"title\": self.context.vc.navigationItem.title ? self.context.vc.navigationItem.title : @\"\"}) : nil;\n}\n```\n\nJS 侧调用方式\n\n```javascript\nwindow.KKJSBridge.call('b', 'callToGetVCTitle', {}, function(res) {\n    console.log('receive vc title：', res.title);\n});\n```\n\n## TODO\n\n- [ ] Fetch hook。 虽然现在大多数 H5 页面的异步请求都是基于 ajax 实现的，随着 Fetch 的慢慢普及，后面也会多起来。\n\n## 参考\n\n- [Ajax-hook](https://github.com/wendux/Ajax-hook)\n\n- [HybridPageKit](https://github.com/dequan1331/HybridPageKit)\n\n- [kerkee_ios](https://github.com/kercer/kerkee_ios)\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "wkwebview",
        "kkjsbridge",
        "jsapi",
        "wkwebview experience",
        "enhances wkwebview",
        "cooooper kkjsbridge"
      ],
      "category": "web-search"
    },
    "DARPAI--darp_engine": {
      "owner": "DARPAI",
      "name": "darp_engine",
      "url": "https://github.com/DARPAI/darp_engine",
      "imageUrl": "/freedevtools/mcp/pfp/DARPAI.webp",
      "description": "DARPEngine provides smart search capabilities by storing metadata for MCP servers online, enabling efficient discovery and connection to relevant MCP tools. It offers a CLI and API for users to search and route requests based on user queries.",
      "stars": 10,
      "forks": 2,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-08-15T09:08:32Z",
      "readme_content": "# DARPEngine\nThe MCP searchengine for DARP.\n\n[![X][x-image]][x-url]\n[![Code style: black][black-image]][black-url]\n[![Imports: reorder-python-imports][imports-image]][imports-url]\n[![Pydantic v2][pydantic-image]][pydantic-url]\n[![pre-commit][pre-commit-image]][pre-commit-url]\n[![License MIT][license-image]][license-url]\n\nDARPEngine stores metadata for MCP servers hosted online and provides smart search capabilites.\n\n## Features\n\n* Simple CLI\n* API access to search\n* MCP tool to retrieve search results for connecting manually\n* Routing MCP tool based on the server: answer any question using the tools found for the user's request\n\n### Coming soon\n\n* Support for `.well-known/mcp.json`\n* Crawler\n* Nice frontend\n* Hosted version\n* Validate different levels of SSL certificates and integrate this info smarly to make sensitive MCP servers difficult to spoof\n\n## Installation\n\n```\nexport OPENAI_API_KEY=sk-...\ndocker network create highkey_network\ndocker compose build\ndocker compose -f docker-compose.yaml -f docker-compose-debug.yaml up --build --wait\n```\n\n## Getting started\n\nYou can connect the DARPEngine to an MCP Client (e.g. Claude Desktop or Cursor) using mcp tools provided. Just select SSE mode & specify `http://localhost:4689/sse` as the endpoint.\n\n\n### Direct CLI use\n\nAnother way is to use CLI. Most of the scripts work with just standard Python libraries, but routing tool requires mcp package, you can install script requirements like this:\n\n```\nconda create -n darp 'python>=3.10'\nconda activate darp\npip install -r mcp_server/requirements.txt\n```\n\nWhen installation is over we can use the scripts.\n\nTo begin with it we need to add some MCP servers to the engine, e.g:\n\n```\n$ python scripts/darp-add.py --url http://memelabs.ai:3006/sse --name code_analysis --description \"Analyze gitlab repo for quality, topics, packages use\"\n```\n\nThen we can make the requests:\n\n```\n$ python scripts/darp-search.py \"Analyze https://github.com/BenderV/autochat\"\nFound 1 servers:\ncode_analysis\n```\n\nYou can get more useful results with the routing tool:\n\n```\n$ python scripts/darp-router.py \"Analyze https://github.com/BenderV/autochat\"\nassistant: [tool_calls]\n[tool] ...\n[tool] ...\n[tool] ...\nassistant:\n  ### Code Quality\n  The code in the AutoChat repository demonstrates a good level of quality concerning **readability**, **maintainability**, and adherence to best practices:\n\n  - **Readability**: Consistent naming conventions are evident, with adherence to Python's PEP 8 styling guidelines. Descriptive function and variable names enhance the understanding of the code's purpose.\n\n  - **Maintainability**: The code is structured to allow easy updates and modifications, with a clear separation of concerns observed through the use of classes and methods handling distinct functionalities.\n\n  - **Best Practices**: Extensive use of exception handling, type annotations, and docstrings reflect best practices in Python development. Some TODO comments suggest areas for improvement, indicating that further attention is needed.\n\n  ### Code Structure\n  The code is organized into multiple files and modules, each serving a distinct purpose:\n\n  - **Modular Design**: Various classes (e.g., `Autochat`, `Image`, `Message`) indicate a well-structured object-oriented design that promotes separation of concerns, making the code easier to navigate.\n\n  - **Logical Organization**: Files are logically separated based on functionality. For example, `chat.py` focuses on chat-related logic, while `model.py` handles message and image processing. The utility functions in `utils.py` enhance reusability.\n\n  - **Testing**: The presence of a test file (`tests/test_utils.py`) shows commitment to testing, crucial for code reliability. The use of `unittest` indicates a structured approach to testing individual components.\n\n  ### Main Functionality\n  The code appears to be part of an **AutoChat package**, providing a framework for building conversational agents. Key functionalities include:\n\n  - **Chat Management**: The `Autochat` class acts as the main interface for managing conversations, handling message history, context, and interaction limits.\n\n  - **Message Handling**: Classes like `Message` and `MessagePart` enable structured message creation and processing, accommodating different message types, including text and images.\n\n  - **Functionality Extensions**: Methods like `add_tool` and `add_function` allow dynamic addition of tools and functions, facilitating customization of the chat experience.\n\n  - **Provider Integration**: Different API provider integrations (e.g., OpenAI, Anthropic) are encapsulated within respective classes, allowing flexibility in backend communication.\n\n  - **Utilities**: Utility functions offer additional capabilities such as CSV formatting and function parsing that support main chat operations.\n\n  Overall, the codebase is well-organized and showcases a thoughtful approach to developing a conversational AI framework. There is room for further refinement and enhancement, particularly in documentation and clarity of variable names.\n\n  ### Library Usage\n  The project makes use of **AI libraries**, indicated by its functionality related to conversational agents and integration with AI service providers. This supports its ability to manage interactions with AI models efficiently.\n\n  ### Summary\n  The AutoChat project is a chat system designed for communication with various AI models, primarily through the `Autochat` class, which manages conversations and supports complex message types, including text and images. The code is moderately complex due to its integration with external APIs and its ability to handle diverse interactions through extensible methods like `add_tool` and `add_function`. The quality of code is commendable, featuring a well-structured modular design that promotes readability and maintainability, although some areas require further documentation and refinement, such as clarifying variable names and enhancing comments. The organization into separate files for models, utilities, and tests aids development, but the utility functions could benefit from better categorization for improved clarity.\n```\n\nOf course, the usefulness of the result depends on the MCP servers you connect to the engine.\n\n\n## Get help and support\n\nPlease feel free to connect with us using the [discussion section](https://github.com/hipasus/darp_engine/discussions).\n\n## Contributing\n\nFollow us on X: https://x.com/DARP_AI\n\n## License\n\nThe DARPEngine codebase is under MIT license.\n\n<br>\n\n[x-image]: https://img.shields.io/twitter/follow/DARP_AI?style=social\n[x-url]: https://x.com/DARP_AI\n[black-image]: https://img.shields.io/badge/code%20style-black-000000.svg\n[black-url]: https://github.com/psf/black\n[imports-image]: https://img.shields.io/badge/%20imports-reorder_python_imports-%231674b1?style=flat&labelColor=ef8336\n[imports-url]: https://github.com/asottile/reorder-python-imports/\n[pydantic-image]: https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/pydantic/pydantic/main/docs/badge/v2.json\n[pydantic-url]: https://pydantic.dev\n[pre-commit-image]: https://img.shields.io/badge/pre--commit-enabled-brightgreen?logo=pre-commit&logoColor=white\n[pre-commit-url]: https://github.com/pre-commit/pre-commit\n[license-image]: https://img.shields.io/github/license/DARPAI/darp_engine\n[license-url]: https://opensource.org/licenses/MIT\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "darp_engine",
        "darpai",
        "darpengine",
        "search darpai",
        "darp_engine darpengine",
        "darpengine provides"
      ],
      "category": "web-search"
    },
    "DaInfernalCoder--perplexity-mcp": {
      "owner": "DaInfernalCoder",
      "name": "perplexity-mcp",
      "url": "https://github.com/DaInfernalCoder/perplexity-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/DaInfernalCoder.webp",
      "description": "A simple notes system designed to manage and summarize text notes, incorporating an intelligent research assistant with search capabilities for handling various queries efficiently.",
      "stars": 252,
      "forks": 27,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-09-28T20:54:17Z",
      "readme_content": "# Perplexity MCP Server\n\nAn intelligent research assistant powered by Perplexity's specialized AI models. Features automatic query complexity detection to route requests to the most appropriate model for optimal results. Unlike the Official server, it has search capabilities FOR EVERY TASK, essentially \n\n## Tools\n\n**Quick Note: The Deep Research tool is going to timeout with some tools like cline, but not with others like cursor due to implementation differences, but the reason tool makes up for it.**\n\n### 1. Search (Sonar Pro)\nQuick search for simple queries and basic information lookup. Best for straightforward questions that need concise, direct answers.\n\n```javascript\nconst result = await use_mcp_tool({\n  server_name: \"perplexity\",\n  tool_name: \"search\",\n  arguments: {\n    query: \"What is the capital of France?\",\n    force_model: false // Optional: force using this model even if query seems complex\n  }\n});\n```\n\n### 2. Reason (Sonar Reasoning Pro)\nHandles complex, multi-step tasks requiring detailed analysis. Perfect for explanations, comparisons, and problem-solving.\n\n```javascript\nconst result = await use_mcp_tool({\n  server_name: \"perplexity\",\n  tool_name: \"reason\",\n  arguments: {\n    query: \"Compare and contrast REST and GraphQL APIs, explaining their pros and cons\",\n    force_model: false // Optional: force using this model even if query seems simple\n  }\n});\n```\n\n### 3. Deep Research (Sonar Deep Research)\nConducts comprehensive research and generates detailed reports. Ideal for in-depth analysis of complex topics.\n\n```javascript\nconst result = await use_mcp_tool({\n  server_name: \"perplexity\",\n  tool_name: \"deep_research\",\n  arguments: {\n    query: \"The impact of quantum computing on cryptography\",\n    focus_areas: [\n      \"Post-quantum cryptographic algorithms\",\n      \"Timeline for quantum threats\",\n      \"Practical mitigation strategies\"\n    ],\n    force_model: false // Optional: force using this model even if query seems simple\n  }\n});\n```\n\n## Intelligent Model Selection\n\nThe server automatically analyzes query complexity to route requests to the most appropriate model:\n\n1. **Simple Queries** → Sonar Pro\n   - Basic information lookup\n   - Straightforward questions\n   - Quick facts\n\n2. **Complex Queries** → Sonar Reasoning Pro\n   - How/why questions\n   - Comparisons\n   - Step-by-step explanations\n   - Problem-solving tasks\n\n3. **Research Queries** → Sonar Deep Research\n   - In-depth analysis\n   - Comprehensive research\n   - Detailed investigations\n   - Multi-faceted topics\n\nYou can override the automatic selection using `force_model: true` in any tool's arguments.\n\n## Setup\n\n1. **Prerequisites**\n   - Node.js (from [nodejs.org](https://nodejs.org))\n   - Perplexity API key (from [perplexity.ai/settings/api](https://www.perplexity.ai/settings/api))\n   - clone the repo somewhere\n\n2. **Configure MCP Settings**\n\nAdd to your MCP settings file (location varies by platform):\n\n```json\n{\n  \"mcpServers\": {\n    \"perplexity\": {\n      \"command\": \"node\",\n      \"args\": [\"/path/to/perplexity-server/build/index.js\"],\n      \"env\": {\n        \"PERPLEXITY_API_KEY\": \"YOUR_API_KEY_HERE\"\n      },\n      \"disabled\": false,\n      \"autoApprove\": []\n    }\n  }\n}\n```\n\n\nOr use NPX to not have to install it locally (recommended for macos): \n\n```json\n{\n  \"mcpServers\": {\n    \"perplexity\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"perplexity-mcp\"\n      ],\n      \"env\": {\n        \"PERPLEXITY_API_KEY\": \"your_api_key\"\n      }\n    }\n  }\n}\n```\n## Star History\n\n[![Star History Chart](https://api.star-history.com/svg?repos=DaInfernalCoder/perplexity-mcp&type=Timeline)](https://www.star-history.com/#DaInfernalCoder/perplexity-mcp&Timeline)\n",
      "npm_url": "https://www.npmjs.com/package/perplexity-mcp",
      "npm_downloads": 8172,
      "keywords": [
        "notes",
        "text",
        "search",
        "text notes",
        "notes incorporating",
        "assistant search"
      ],
      "category": "web-search"
    },
    "DappierAI--dappier-mcp": {
      "owner": "DappierAI",
      "name": "dappier-mcp",
      "url": "https://github.com/DappierAI/dappier-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/DappierAI.webp",
      "description": "Provides real-time web search capabilities and access to premium data sources, including news, financial markets, sports, entertainment, and weather information. Facilitates the development of AI agents leveraging diverse data models.",
      "stars": 32,
      "forks": 10,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-28T11:52:36Z",
      "readme_content": "## 📽️ Watch the Demo Video (Live!)\n\n> 📌 Click the image below — use **Ctrl+Click** (or **Cmd+Click on Mac**) to open in a new tab.\n\n<a href=\"https://youtu.be/2Q_PwLFkYTQ\">\n  \n</a>\n\n# Dappier MCP Server\n\nEnable fast, free real-time web search and access premium data from trusted media brands—news, financial markets, sports, entertainment, weather, and more. Build powerful AI agents with Dappier.\n\n> Explore a wide range of data models in our marketplace at [marketplace.dappier.com](https://marketplace.dappier.com/marketplace).\n\n<br>\n\n<a href=\"https://smithery.ai/server/@DappierAI/dappier-mcp\" target=\"_blank\"><img alt=\"Smithery Badge\" src=\"https://smithery.ai/badge/@DappierAI/dappier-mcp\"></a>\n\n<br>\n\n<a href=\"https://glama.ai/mcp/servers/@DappierAI/dappier-mcp\" target=\"_blank\">\n  <img alt=\"badge\" width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@DappierAI/dappier-mcp/badge\" />\n</a>\n\n<br>\n\n[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/dappierai-dappier-mcp-badge.png)](https://mseep.ai/app/dappierai-dappier-mcp)\n\n<br>\n\n## Getting Started\n\nGet Dappier API Key. Head to [Dappier](https://platform.dappier.com/profile/api-keys) to sign up and generate an API key.\n\n\n## Installing via Smithery\n\nTo install dappier-mcp for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@DappierAI/dappier-mcp):\n\n```bash\nnpx -y @smithery/cli install @DappierAI/dappier-mcp --client claude\n```\n\n## Installation\n\nInstall `uv` first.\n\n**MacOS/Linux**:\n```bash\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n```\n\n**Windows**:\n```bash\npowershell -ExecutionPolicy ByPass -c \"irm https://astral.sh/uv/install.ps1 | iex\"\n```\n\n## Usage\n\n### Claude Desktop\n\nUpdate your Claude configuration file (`claude_desktop_config.json`) with the following content:\n\n```json\n{\n  \"mcpServers\": {\n    \"dappier\": {\n      \"command\": \"uvx\",\n      \"args\": [\"dappier-mcp\"],\n      \"env\": {\n        \"DAPPIER_API_KEY\": \"YOUR_API_KEY_HERE\"\n      }\n    }\n  }\n}\n```\n\n> **Hint**: You may need to provide the full path to the `uvx` executable in the `command` field. You can obtain this by running `which uvx` on macOS/Linux or `where uvx` on Windows.\n\n**Configuration file location:**\n- **macOS**: `~/Library/Application Support/Claude/claude_desktop_config.json`\n- **Windows**: `%APPDATA%\\Claude\\claude_desktop_config.json`\n\n**Accessing via application:**\n- **macOS**:\n  1. Open the Claude Desktop application.\n  2. In the menu bar, click on `Claude` > `Settings`.\n  3. Navigate to the `Developer` tab.\n  4. Click on `Edit Config` to open the configuration file in your default text editor.\n- **Windows**:\n  1. Open the Claude Desktop application.\n  2. Click on the gear icon to access `Settings`.\n  3. Navigate to the `Developer` tab.\n  4. Click on `Edit Config` to open the configuration file in your default text editor.\n\n> **Note**: If the `Developer` tab is not visible, ensure you're using the latest version of Claude Desktop. \n\n---\n\n### Cursor\n\nUpdate your Cursor configuration file (`mcp.json`) with the following content:\n\n```json\n{\n  \"mcpServers\": {\n    \"dappier\": {\n      \"command\": \"uvx\",\n      \"args\": [\"dappier-mcp\"],\n      \"env\": {\n        \"DAPPIER_API_KEY\": \"YOUR_API_KEY_HERE\"\n      }\n    }\n  }\n}\n```\n\n> **Hint**: You may need to provide the full path to the `uvx` executable in the `command` field. You can obtain this by running `which uvx` on macOS/Linux or `where uvx` on Windows.\n\n**Configuration file location:**\n- **Global Configuration**:\n  - **macOS**: `~/.cursor/mcp.json`\n  - **Windows**: `%USERPROFILE%\\.cursor\\mcp.json`\n- **Project-Specific Configuration**:\n  - Place the `mcp.json` file inside the `.cursor` directory within your project folder: `<project-root>/.cursor/mcp.json`\n\n**Accessing via application:**\n1. Open the Cursor application.\n2. Navigate to `Settings` > `MCP`.\n3. Click on `Add New Global MCP Server`.\n4. The application will open the `mcp.json` file in your default text editor for editing.\n\n> **Note**: On Windows, if the project-level configuration is not recognized, consider adding the MCP server through the Cursor settings interface. \n\n---\n\n### Windsurf\n\nUpdate your Windsurf configuration file (`mcp_config.json`) with the following content:\n\n```json\n{\n  \"mcpServers\": {\n    \"dappier\": {\n      \"command\": \"uvx\",\n      \"args\": [\"dappier-mcp\"],\n      \"env\": {\n        \"DAPPIER_API_KEY\": \"YOUR_API_KEY_HERE\"\n      }\n    }\n  }\n}\n```\n\n> **Hint**: You may need to provide the full path to the `uvx` executable in the `command` field. You can obtain this by running `which uvx` on macOS/Linux or `where uvx` on Windows.\n\n**Configuration file location:**\n- **macOS**: `~/.codeium/windsurf/mcp_config.json`\n- **Windows**: `%USERPROFILE%\\.codeium\\windsurf\\mcp_config.json`\n\n**Accessing via application:**\n1. Open the Windsurf application.\n2. Navigate to `Settings` > `Cascade`.\n3. Scroll down to the `Model Context Protocol (MCP) Servers` section.\n4. Click on `View raw config` to open the `mcp_config.json` file in your default text editor.\n\n> **Note**: After editing the configuration file, click the `Refresh` button in the MCP Servers section to apply the changes. \n\n## Features\n\nThe Dappier MCP Remote Server provides powerful real-time capabilities out of the box — no training or fine-tuning needed. Use it to build live, interactive tools powered by the latest web data, financial markets, or AI-curated content.\n\n### Real-Time Web Search  \n**Model ID:** `am_01j06ytn18ejftedz6dyhz2b15`  \n\nSearch the live web using Dappier’s AI-powered index. Get real-time access to:\n\n- Breaking news from across the globe  \n- Weather forecasts and local updates  \n- Travel alerts and flight info  \n- Trending topics and viral content  \n- Online deals and shopping highlights  \n\nIdeal for use cases like news agents, travel planners, alert bots, and more.\n\n### Stock Market Insights  \n**Model ID:** `am_01j749h8pbf7ns8r1bq9s2evrh`  \n\nThis model delivers instant access to market data, financial headlines, and trade insights. Perfect for portfolio dashboards, trading copilots, and investment tools.\n\nIt provides:\n\n- Real-time stock prices  \n- Financial news and company updates  \n- Trade signals and trends  \n- Market movement summaries  \n- AI-curated analysis using live data from Polygon.io  \n\n### AI-Powered Content Recommendations  \n\nChoose from several domain-specific AI models tailored for content discovery, summarization, and feed generation.\n\n#### Sports News  \n**Model ID:** `dm_01j0pb465keqmatq9k83dthx34`  \nStay updated with real-time sports headlines, game recaps, and expert analysis.\n\n#### Lifestyle Updates  \n**Model ID:** `dm_01j0q82s4bfjmsqkhs3ywm3x6y`  \nExplore curated lifestyle content — covering wellness, entertainment, and everyday inspiration.\n\n#### iHeartDogs AI  \n**Model ID:** `dm_01j1sz8t3qe6v9g8ad102kvmqn`  \nYour intelligent dog care assistant — access training tips, health advice, and behavior insights.\n\n#### iHeartCats AI  \n**Model ID:** `dm_01j1sza0h7ekhaecys2p3y0vmj`  \nAn expert AI for all things feline — from nutrition to playtime to grooming routines.\n\n#### GreenMonster  \n**Model ID:** `dm_01j5xy9w5sf49bm6b1prm80m27`  \nDiscover sustainable lifestyle ideas, ethical choices, and green innovations.\n\n#### WISH-TV AI  \n**Model ID:** `dm_01jagy9nqaeer9hxx8z1sk1jx6`  \nTap into hyperlocal news, politics, culture, health, and multicultural updates.\n\nEach recommendation includes:\n\n- A clear title and concise summary  \n- The original publication date  \n- The trusted source and domain  \n- Image preview (if available)  \n- A relevance score for prioritization\n\nAdvanced options let you:\n\n- Tune the search algorithm (`semantic`, `most_recent`, `trending`, etc.)  \n- Focus results on a specific domain (`ref`)  \n- Adjust how many results you want (`similarity_top_k`, `num_articles_ref`)  \n\n## Debugging\n\nRun the MCP inspector to debug the server:\n```bash\nnpx @modelcontextprotocol/inspector uvx dappier-mcp\n```\n\n## Contributing\n\nWe welcome contributions to expand and improve the Dappier MCP Server. Whether you want to add new search capabilities, enhance existing functionality, or improve documentation, your input is valuable.\n\nFor examples of other MCP servers and implementation patterns, see:\n[https://github.com/modelcontextprotocol/servers](https://github.com/modelcontextprotocol/servers)\n\nPull requests are welcome! Feel free to contribute new ideas, bug fixes, or enhancements.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "dappierai",
        "search",
        "ai",
        "search dappierai",
        "web search",
        "ai agents"
      ],
      "category": "web-search"
    },
    "DealExpress--mcp-server": {
      "owner": "DealExpress",
      "name": "mcp-server",
      "url": "https://github.com/DealExpress/mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/DealExpress.webp",
      "description": "Enables interaction with ads on the DealX platform through natural language queries, supporting search, sorting, and pagination of ads. The framework is easily extendable for future functionalities related to the DealX platform.",
      "stars": 0,
      "forks": 2,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-04-01T17:41:14Z",
      "readme_content": "# @dealx/mcp-server\n\nThis is a Model Context Protocol (MCP) server for the [DealX platform](https://dealx.com.ua). It allows LLMs to interact with the DealX platform, specifically to search for ads.\n\n## Table of Contents\n\n- [Overview](#overview)\n- [Installation](#installation)\n- [Usage](#usage)\n- [Available Tools](#available-tools)\n- [Extending the Server](#extending-the-server)\n- [Development](#development)\n- [Troubleshooting](#troubleshooting)\n\n## Overview\n\nThe DealX MCP Server implements the [Model Context Protocol](https://github.com/modelcontextprotocol/typescript-sdk) to provide a standardized way for LLMs to interact with the [DealX platform](https://dealx.com.ua). Currently, it supports searching for ads, with plans to add more functionality in the future.\n\n### What is MCP?\n\nThe Model Context Protocol (MCP) is a standardized way for LLMs to interact with external systems. It provides a structured interface for LLMs to access data and perform actions in the real world. This server implements the MCP specification to allow LLMs to interact with the DealX platform.\n\n## Installation\n\n### Prerequisites\n\n- Node.js (v20 or later)\n- npm (v11 or later)\n\n### MCP Configuration\n\nTo use this server with an LLM like Claude, you need to add it to your LLM's MCP configuration:\n\n1. Open your LLM's MCP configuration file:\n\n   - **Claude Desktop App**:\n     - macOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\n     - Windows: `%APPDATA%\\Claude\\claude_desktop_config.json`\n     - Linux: `~/.config/Claude/claude_desktop_config.json`\n   - **Cline (VS Code Extension)**:\n     - `~/Library/Application Support/Code/User/globalStorage/saoudrizwan.claude-dev/settings/cline_mcp_settings.json`\n\n2. Add the DealX MCP server to the `mcpServers` section:\n\n   ```json\n   {\n     \"mcpServers\": {\n       \"dealx\": {\n         \"command\": \"npx\",\n         \"args\": [\"-y\", \"@dealx/mcp-server\"],\n         \"env\": {\n           \"DEALX_API_URL\": \"https://dealx.com.ua\"\n         },\n         \"disabled\": false,\n         \"autoApprove\": []\n       }\n     }\n   }\n   ```\n\n### Installation via npm\n\nThe easiest way to install the DealX MCP Server is via npm:\n\n```shell\nnpm install -g @dealx/mcp-server\n```\n\n### Installation for Development\n\nIf you want to modify the server or contribute to its development:\n\n1. Clone the repository:\n\n   ```shell\n   git clone <repository-url>\n   cd dealx/mcp\n   ```\n\n2. Install dependencies:\n\n   ```shell\n   npm install\n   ```\n\n3. Create a `.env` file based on the `.env.example` file:\n\n   ```shell\n   cp .env.example .env\n   ```\n\n4. Edit the `.env` file to set the appropriate values:\n\n   ```shell\n   # DealX API URL\n   DEALX_API_URL=http://localhost:3001\n\n   # Optional: Specify the port for the MCP server\n   MCP_SERVER_PORT=3100\n\n   # Optional: Log level (debug, info, warn, error)\n   LOG_LEVEL=info\n   ```\n\n5. Build the server:\n\n   ```shell\n   npm run build\n   ```\n\n## Usage\n\n### Starting the Server\n\nYou can run the server in several ways:\n\n1. If installed globally:\n\n   ```shell\n   node node_modules/@dealx/mcp-server/build/index.js\n   ```\n\n2. Using npx without installation:\n\n   ```shell\n   npx -y @dealx/mcp-server\n   ```\n\n3. With environment variables:\n\n   ```shell\n   DEALX_API_URL=https://dealx.com.ua npx -y @dealx/mcp-server\n   ```\n\n4. For development:\n\n   ```shell\n   npm start\n   ```\n\n### Using with an LLM\n\nOnce configured in your LLM's MCP settings, you can use natural language to interact with the DealX platform.\n\nExample prompts:\n\n- \"Search for ads on DealX with the query 'laptop'\"\n- \"Find the newest 5 ads for 'iPhone' on DealX\"\n- \"Search DealX for apartments in Kyiv\"\n\n## Available Tools\n\n### search_ads\n\nSearch for ads on the DealX platform.\n\n**Parameters:**\n\n- `query` (string, optional): Search query string\n- `sort` (string, optional): Sort order (e.g., \"-created\" for newest first)\n- `offset` (number, optional): Pagination offset (starts at 1, default: 1)\n- `limit` (number, optional): Number of results per page (max 100, default: 30)\n\n**Example Usage:**\n\n```json\n{\n  \"query\": \"laptop\",\n  \"sort\": \"-created\",\n  \"offset\": 1,\n  \"limit\": 10\n}\n```\n\n## Extending the Server\n\nThe server is designed to be easily extended with additional tools. Here's how to add a new tool:\n\n- Define the tool in the `TOOLS` object in `src/index.ts`:\n\n  ```typescript\n  const TOOLS = {\n    SEARCH_ADS: \"search_ads\",\n    NEW_TOOL: \"new_tool\", // Add your new tool here\n  };\n  ```\n\n- Create a new file in the `src/tools` directory for your tool implementation:\n\n  ```typescript\n  // src/tools/new-tool.ts\n  import { ErrorCode, McpError } from \"@modelcontextprotocol/sdk/types.js\";\n\n  interface NewToolParams {\n    // Define your tool parameters here\n  }\n\n  export async function newTool(params: NewToolParams) {\n    try {\n      // Implement your tool logic here\n\n      return {\n        content: [\n          {\n            type: \"text\",\n            text: JSON.stringify(result, null, 2),\n          },\n        ],\n      };\n    } catch (error) {\n      // Handle errors\n      // ...\n    }\n  }\n  ```\n\n- Add the tool to the `ListToolsRequestSchema` handler in `src/index.ts`:\n\n  ```typescript\n  this.server.setRequestHandler(ListToolsRequestSchema, async () => ({\n    tools: [\n      // Existing tools...\n      {\n        name: TOOLS.NEW_TOOL,\n        description: \"Description of your new tool\",\n        inputSchema: {\n          type: \"object\",\n          properties: {\n            // Define your tool parameters here\n          },\n          required: [], // List required parameters\n        },\n      },\n    ],\n  }));\n  ```\n\n- Add the tool to the `CallToolRequestSchema` handler in `src/index.ts`:\n\n  ```typescript\n  this.server.setRequestHandler(CallToolRequestSchema, async (request) => {\n    const { name, arguments: args } = request.params;\n\n    switch (name) {\n      // Existing cases...\n      case TOOLS.NEW_TOOL:\n        return await newTool(args);\n      default:\n        throw new McpError(ErrorCode.MethodNotFound, `Unknown tool: ${name}`);\n    }\n  });\n  ```\n\n- Import your new tool in `src/index.ts`:\n\n  ```typescript\n  import { newTool } from \"./tools/new-tool.js\";\n  ```\n\n### Planned Future Tools\n\nThe following tools are planned for future implementation:\n\n- `create_ad`: Create a new ad on the [DealX platform](https://dealx.com.ua)\n- `edit_ad`: Edit an existing ad\n- `delete_ad`: Delete an ad\n- `get_threads`: Get discussion threads for an ad\n- `create_thread`: Create a new discussion thread\n\n## Development\n\n### Project Structure\n\n```shell\nmcp/\n├── build/              # Compiled JavaScript files\n├── src/                # TypeScript source files\n│   ├── tools/          # Tool implementations\n│   │   └── search-ads.ts\n│   └── index.ts        # Main server implementation\n├── .env                # Environment variables (not in git)\n├── .env.example        # Example environment variables\n├── package.json        # Project dependencies and scripts\n├── tsconfig.json       # TypeScript configuration\n└── README.md           # This file\n```\n\n### npm Scripts\n\n- `npm run build` - Compile TypeScript to JavaScript\n- `npm start` - Start the server using the compiled JavaScript\n- `npm run dev` - Start the server in development mode with hot reloading\n- `npm run lint` - Lint the code using ESLint\n- `npm run format` - Format the code using Prettier\n- `npm test` - Run tests\n\n## Troubleshooting\n\n### Common Issues\n\n#### Server Not Starting\n\nIf the server fails to start, check the following:\n\n- Make sure you have the correct Node.js version installed\n- Check that all dependencies are installed\n- Verify that the `.env` file exists and has the correct values\n- Check the console output for error messages\n\n#### Connection Issues\n\nIf the LLM can't connect to the server:\n\n- Make sure the server is running\n- Check that the MCP configuration in the LLM's settings is correct\n- Verify that the path to the server executable is correct\n- Check that the environment variables are set correctly\n\n#### API Connection Issues\n\nIf the server can't connect to the DealX API:\n\n- Make sure the DealX API is running\n- Check that the `DEALX_API_URL` environment variable is set correctly\n- Verify that the API endpoint is accessible from the server\n\n### Getting Help\n\nIf you encounter issues not covered here, please open an issue against this GitHub repository.\n",
      "npm_url": "https://www.npmjs.com/package/mcp-server",
      "npm_downloads": 29732,
      "keywords": [
        "dealexpress",
        "dealx",
        "queries",
        "search dealexpress",
        "ads dealx",
        "dealx platform"
      ],
      "category": "web-search"
    },
    "DeepSpringAI--search_mcp_server": {
      "owner": "DeepSpringAI",
      "name": "search_mcp_server",
      "url": "https://github.com/DeepSpringAI/search_mcp_server",
      "imageUrl": "/freedevtools/mcp/pfp/DeepSpringAI.webp",
      "description": "Enables applications to perform web searches and extract relevant information from search results, while also allowing for similarity searches to identify related content. This functionality simplifies access to web data and enhances content discovery.",
      "stars": 2,
      "forks": 1,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-04-23T14:58:59Z",
      "readme_content": "# parquet_mcp_server\n[![smithery badge](https://smithery.ai/badge/@DeepSpringAI/parquet_mcp_server)](https://smithery.ai/server/@DeepSpringAI/parquet_mcp_server)\n\nA powerful MCP (Model Control Protocol) server that provides tools for performing web searches and finding similar content. This server is designed to work with Claude Desktop and offers two main functionalities:\n\n1. **Web Search**: Perform a web search and scrape results\n2. **Similarity Search**: Extract relevant information from previous searches\n\nThis server is particularly useful for:\n- Applications requiring web search capabilities\n- Projects needing to find similar content based on search queries\n\n## Installation\n\n### Installing via Smithery\n\nTo install Parquet MCP Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@DeepSpringAI/parquet_mcp_server):\n\n```bash\nnpx -y @smithery/cli install @DeepSpringAI/parquet_mcp_server --client claude\n```\n\n### Clone this repository\n\n```bash\ngit clone ...\ncd parquet_mcp_server\n```\n\n### Create and activate virtual environment\n\n```bash\nuv venv\n.venv\\Scripts\\activate  # On Windows\nsource .venv/bin/activate  # On macOS/Linux\n```\n\n### Install the package\n\n```bash\nuv pip install -e .\n```\n\n### Environment\n\nCreate a `.env` file with the following variables:\n\n```bash\nEMBEDDING_URL=http://sample-url.com/api/embed  # URL for the embedding service\nOLLAMA_URL=http://sample-url.com/  # URL for Ollama server\nEMBEDDING_MODEL=sample-model  # Model to use for generating embeddings\nSEARCHAPI_API_KEY=your_searchapi_api_key\nFIRECRAWL_API_KEY=your_firecrawl_api_key\nVOYAGE_API_KEY=your_voyage_api_key\nAZURE_OPENAI_ENDPOINT=http://sample-url.com/azure_openai\nAZURE_OPENAI_API_KEY=your_azure_openai_api_key\n```\n\n## Usage with Claude Desktop\n\nAdd this to your Claude Desktop configuration file (`claude_desktop_config.json`):\n\n```json\n{\n  \"mcpServers\": {\n    \"parquet-mcp-server\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"--directory\",\n        \"/home/${USER}/workspace/parquet_mcp_server/src/parquet_mcp_server\",\n        \"run\",\n        \"main.py\"\n      ]\n    }\n  }\n}\n```\n\n## Available Tools\n\nThe server provides two main tools:\n\n1. **Search Web**: Perform a web search and scrape results\n   - Required parameters:\n     - `queries`: List of search queries\n   - Optional parameters:\n     - `page_number`: Page number for the search results (defaults to 1)\n\n2. **Extract Info from Search**: Extract relevant information from previous searches\n   - Required parameters:\n     - `queries`: List of search queries to merge\n\n## Example Prompts\n\nHere are some example prompts you can use with the agent:\n\n### For Web Search:\n```\n\"Please perform a web search for 'macbook' and 'laptop' and scrape the results from page 1\"\n```\n\n### For Extracting Info from Search:\n```\n\"Please extract relevant information from the previous searches for 'macbook'\"\n```\n\n## Testing the MCP Server\n\nThe project includes a comprehensive test suite in the `src/tests` directory. You can run all tests using:\n\n```bash\npython src/tests/run_tests.py\n```\n\nOr run individual tests:\n\n```bash\n# Test Web Search\npython src/tests/test_search_web.py\n\n# Test Extract Info from Search\npython src/tests/test_extract_info_from_search.py\n```\n\nYou can also test the server using the client directly:\n\n```python\nfrom parquet_mcp_server.client import (\n    perform_search_and_scrape,  # New web search function\n    find_similar_chunks  # New extract info function\n)\n\n# Perform a web search\nperform_search_and_scrape([\"macbook\", \"laptop\"], page_number=1)\n\n# Extract information from the search results\nfind_similar_chunks([\"macbook\"])\n```\n\n### Troubleshooting\n\n1. If you get SSL verification errors, make sure the SSL settings in your `.env` file are correct\n2. If embeddings are not generated, check:\n   - The Ollama server is running and accessible\n   - The model specified is available on your Ollama server\n   - The text column exists in your input Parquet file\n3. If DuckDB conversion fails, check:\n   - The input Parquet file exists and is readable\n   - You have write permissions in the output directory\n   - The Parquet file is not corrupted\n4. If PostgreSQL conversion fails, check:\n   - The PostgreSQL connection settings in your `.env` file are correct\n   - The PostgreSQL server is running and accessible\n   - You have the necessary permissions to create/modify tables\n   - The pgvector extension is installed in your database\n\n## PostgreSQL Function for Vector Similarity Search\n\nTo perform vector similarity searches in PostgreSQL, you can use the following function:\n\n```sql\n-- Create the function for vector similarity search\nCREATE OR REPLACE FUNCTION match_web_search(\n  query_embedding vector(1024),  -- Adjusted vector size\n  match_threshold float,\n  match_count int  -- User-defined limit for number of results\n)\nRETURNS TABLE (\n  id bigint,\n  metadata jsonb,\n  text TEXT,  -- Added text column to the result\n  date TIMESTAMP,  -- Using the date column instead of created_at\n  similarity float\n)\nLANGUAGE plpgsql\nAS $$\nBEGIN\n  RETURN QUERY\n  SELECT\n    web_search.id,\n    web_search.metadata,\n    web_search.text,  -- Returning the full text of the chunk\n    web_search.date,  -- Returning the date timestamp\n    1 - (web_search.embedding <=> query_embedding) as similarity\n  FROM web_search\n  WHERE 1 - (web_search.embedding <=> query_embedding) > match_threshold\n  ORDER BY web_search.date DESC,  -- Sort by date in descending order (newest first)\n           web_search.embedding <=> query_embedding  -- Sort by similarity\n  LIMIT match_count;  -- Limit the results to the match_count specified by the user\nEND;\n$$;\n```\n\nThis function allows you to perform similarity searches on vector embeddings stored in a PostgreSQL database, returning results that meet a specified similarity threshold and limiting the number of results based on user input. The results are sorted by date and similarity.\n\n\n\n## Postgres table creation\n```\nCREATE TABLE web_search (\n    id SERIAL PRIMARY KEY,\n    text TEXT,\n    metadata JSONB,\n    embedding VECTOR(1024),\n\n    -- This will be auto-updated\n    date TIMESTAMP DEFAULT NOW()\n);\n```",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "search_mcp_server",
        "searches",
        "search",
        "deepspringai search_mcp_server",
        "search deepspringai",
        "web search"
      ],
      "category": "web-search"
    },
    "DevEnterpriseSoftware--scrapi-mcp": {
      "owner": "DevEnterpriseSoftware",
      "name": "scrapi-mcp",
      "url": "https://github.com/DevEnterpriseSoftware/scrapi-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/DevEnterpriseSoftware.webp",
      "description": "ScrAPI MCP Server facilitates the extraction of data from any website, handling bot detection and geolocation restrictions efficiently. It provides an intuitive interface for web scraping tasks.",
      "stars": 13,
      "forks": 3,
      "license": "No License",
      "language": "JavaScript",
      "updated_at": "2025-09-07T18:02:21Z",
      "readme_content": "![ScrAPI logo](https://raw.githubusercontent.com/DevEnterpriseSoftware/scrapi-sdk-dotnet/master/icon_small.png)\n\n# ScrAPI MCP Server\n\n[![License: MIT](https://img.shields.io/badge/license-MIT-blue.svg)](https://opensource.org/licenses/MIT)\n[![NPM Downloads](https://img.shields.io/npm/dm/@deventerprisesoftware/scrapi-mcp)](https://www.npmjs.com/package/@deventerprisesoftware/scrapi-mcp)\n[![Docker Pulls](https://img.shields.io/docker/pulls/deventerprisesoftware/scrapi-mcp)](https://hub.docker.com/r/deventerprisesoftware/scrapi-mcp)\n[![smithery badge](https://smithery.ai/badge/@DevEnterpriseSoftware/scrapi-mcp)](https://smithery.ai/server/@DevEnterpriseSoftware/scrapi-mcp)\n\nMCP server for using [ScrAPI](https://scrapi.tech) to scrape web pages.\n\nScrAPI is your ultimate web scraping solution, offering powerful, reliable, and easy-to-use features to extract data from any website effortlessly.\n\n## Tools\n\n1. `scrape_url_html`\n   - Use a URL to scrape a website using the ScrAPI service and retrieve the result as HTML.\n     Use this for scraping website content that is difficult to access because of bot detection, captchas or even geolocation restrictions.\n     The result will be in HTML which is preferable if advanced parsing is required.\n   - Input: `url` (string)\n   - Returns: HTML content of the URL\n\n2. `scrape_url_markdown`\n   - Use a URL to scrape a website using the ScrAPI service and retrieve the result as Markdown.\n     Use this for scraping website content that is difficult to access because of bot detection, captchas or even geolocation restrictions.\n     The result will be in Markdown which is preferable if the text content of the webpage is important and not the structural information of the page.\n   - Input: `url` (string)\n   - Returns: Markdown content of the URL\n\n## Setup\n\n### API Key (optional)\n\nOptionally get an API key from the [ScrAPI website](https://scrapi.tech).\n\nWithout an API key you will be limited to one concurrent call and twenty free calls per day with minimal queuing capabilities.\n\n### Cloud Server\n\nThe ScrAPI MCP Server is also available in the cloud over SSE at https://api.scrapi.tech/mcp/sse and streamable HTTP at https://api.scrapi.tech/mcp\n\nCloud MCP servers are not widely supported yet but you can access this directly from your own custom clients or use [MCP Inspector](https://github.com/modelcontextprotocol/inspector) to test it. There is currently no facility to pass through your API key when connecting to the cloud MCP server.\n\n![MCP-Inspector](https://raw.githubusercontent.com/DevEnterpriseSoftware/scrapi-mcp/master/img/mcp-inspector.jpg)\n\n### Usage with Claude Desktop\n\nAdd the following to your `claude_desktop_config.json`:\n\n#### Docker\n\n```json\n{\n  \"mcpServers\": {\n    \"ScrAPI\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"-i\",\n        \"--rm\",\n        \"-e\",\n        \"SCRAPI_API_KEY\",\n        \"deventerprisesoftware/scrapi-mcp\"\n      ],\n      \"env\": {\n        \"SCRAPI_API_KEY\": \"<YOUR_API_KEY>\"\n      }\n    }\n  }\n}\n```\n\n#### NPX\n\n```json\n{\n  \"mcpServers\": {\n    \"ScrAPI\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"@deventerprisesoftware/scrapi-mcp\"\n      ],\n      \"env\": {\n        \"SCRAPI_API_KEY\": \"<YOUR_API_KEY>\"\n      }\n    }\n  }\n}\n```\n\n![Claude-Desktop](https://raw.githubusercontent.com/DevEnterpriseSoftware/scrapi-mcp/master/img/claude-desktop.jpg)\n\n## Build\n\nDocker build:\n\n```bash\ndocker build -t deventerprisesoftware/scrapi-mcp -f Dockerfile .\n```\n\n## License\n\nThis MCP server is licensed under the MIT License. This means you are free to use, modify, and distribute the software, subject to the terms and conditions of the MIT License. For more details, please see the LICENSE file in the project repository.\n",
      "npm_url": "https://www.npmjs.com/package/@deventerprisesoftware/scrapi-mcp",
      "npm_downloads": 4420,
      "keywords": [
        "scraping",
        "scrapi",
        "mcp",
        "mcp scrapi",
        "scrapi mcp",
        "deventerprisesoftware scrapi"
      ],
      "category": "web-search"
    },
    "Dips1902--Trip_Advisor": {
      "owner": "Dips1902",
      "name": "Trip_Advisor",
      "url": "https://github.com/Dips1902/Trip_Advisor",
      "imageUrl": "/freedevtools/mcp/pfp/Dips1902.webp",
      "description": "AI-powered trip planning assistant that creates personalized itineraries collaboratively based on user preferences using various interfaces including CLI, FASTAPI, and Streamlit.",
      "stars": 0,
      "forks": 0,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-04-05T09:56:22Z",
      "readme_content": "![CrewAI](https://miro.medium.com/v2/resize:fit:1400/0*-7HC-GJCxjn-Dm7i.png)\n\n# 🏖️ Trip Planner: Streamlit with CrewAI\n\n\n\n## Introduction\n\nTrip Planner leverages the CrewAI framework to automate and enhance the trip planning experience, integrating a CLI, FASTAPI, and a user-friendly Streamlit interface.\n\n\n## CrewAI Framework\n\nCrewAI simplifies the orchestration of role-playing AI agents. In VacAIgent, these agents collaboratively decide on cities and craft a complete itinerary for your trip based on specified preferences, all accessible via a streamlined Streamlit user interface.\n\n\n## Running the Application\n\nTo experience the VacAIgent app:\n\n- **Configure Environment**: Set up the environment variables for [Browseless](https://www.browserless.io/), [Serper](https://serper.dev/), and [OpenAI](https://openai.com/). Use the `secrets.example` as a guide to add your keys then move that file (`secrets.toml`) to `.streamlit/secrets.toml`.\n\n- **Install Dependencies**: Execute `pip install -r requirements.txt` in your terminal.\n- **Launch the CLI Mode**: Run `python cli_app.py -o \"Bangalore, India\" -d \"Krabi, Thailand\" -s 2024-05-01 -e 2024-05-10 -i \"2 adults who love swimming, dancing, hiking, shopping, food, water sports adventures, rock climbing\"` to start the CLI Mode.\n- **Launch the FASTAPI**: Run `uvicorn api_app:app --reload` to start the FASTAPI server.\n- **Launch the Streamlit App**: Run `streamlit run streamlit_app.py` to start the Streamlit interface.\n\n★ **Disclaimer**: The application uses GEMINI by default. Ensure you have access to GEMINI's API and be aware of the associated costs.\n\n## Details & Explanation\n\n- **Streamlit UI**: The Streamlit interface is implemented in `streamlit_app.py`, where users can input their trip details.\n- **Components**:\n  - `./trip_tasks.py`: Contains task prompts for the agents.\n  - `./trip_agents.py`: Manages the creation of agents.\n  - `./tools directory`: Houses tool classes used by agents.\n  - `./streamlit_app.py`: The heart of the Streamlit app.\n\n## Using LLM Models\n\nTo switch LLMs from differnet Providers\n\n```python\nclass TripAgents():\n    def __init__(self, llm: BaseChatModel = None):\n        if llm is None:\n            #self.llm = LLM(model=\"groq/deepseek-r1-distill-llama-70b\")\n            self.llm = LLM(model=\"gemini/gemini-2.0-flash\")\n        else:\n            self.llm = llm\n\n```\n[Connect to LLMs](https://docs.crewai.com/how-to/llm-connections#connect-crewai-to-llms)\n\n\n\n### Integrating Ollama with CrewAI\n\nPass the Ollama model to agents in the CrewAI framework:\n\n```python\n    agent = Agent(\n        role='Local AI Expert',\n        goal='Process information using a local model',\n        backstory=\"An AI assistant running on local hardware.\",\n        llm=LLM(model=\"ollama/llama3.2\", base_url=\"http://localhost:11434\")\n    )\n```\n\n\n## License\n\nTrip Planner is open-sourced under the MIT License.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "trip_advisor",
        "itineraries",
        "trip",
        "trip_advisor ai",
        "personalized itineraries",
        "itineraries collaboratively"
      ],
      "category": "web-search"
    },
    "Domoteek--mcp-server-airbnb": {
      "owner": "Domoteek",
      "name": "mcp-server-airbnb",
      "url": "https://github.com/Domoteek/mcp-server-airbnb",
      "imageUrl": "/freedevtools/mcp/pfp/Domoteek.webp",
      "description": "Search for Airbnb listings based on various parameters such as location, check-in and check-out dates, and guest count. Retrieve detailed information and direct links to the listings for easy access and trip planning.",
      "stars": 0,
      "forks": 1,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-03-24T15:59:47Z",
      "readme_content": "# Serveur MCP Airbnb\n[![smithery badge](https://smithery.ai/badge/@Domoteek/mcp-server-airbnb)](https://smithery.ai/server/@Domoteek/mcp-server-airbnb)\n\nServeur MCP pour rechercher des annonces Airbnb et obtenir les détails des logements. Fournit des liens directs vers les annonces Airbnb dans les résultats de recherche.\n\n## Outils\n\n1. `airbnb_search`\n   - Recherche d'annonces Airbnb\n   - Entrée requise : `location` (chaîne de caractères)\n   - Entrées optionnelles :\n     - `placeId` (chaîne de caractères)\n     - `checkin` (chaîne de caractères, AAAA-MM-JJ)\n     - `checkout` (chaîne de caractères, AAAA-MM-JJ)\n     - `adults` (nombre)\n     - `children` (nombre)\n     - `infants` (nombre)\n     - `pets` (nombre)\n     - `minPrice` (nombre)\n     - `maxPrice` (nombre)\n     - `cursor` (chaîne de caractères)\n     - `ignoreRobotsText` (booléen)\n   - Retourne : Tableau d'annonces avec des détails comme le nom, le prix, l'emplacement, etc. Chaque annonce inclut une `url` directe vers la page Airbnb.\n\n2. `airbnb_listing_details`\n   - Obtenir des informations détaillées sur une annonce Airbnb spécifique\n   - Entrée requise : `id` (chaîne de caractères)\n   - Entrées optionnelles :\n     - `checkin` (chaîne de caractères, AAAA-MM-JJ)\n     - `checkout` (chaîne de caractères, AAAA-MM-JJ)\n     - `adults` (nombre)\n     - `children` (nombre)\n     - `infants` (nombre)\n     - `pets` (nombre)\n     - `ignoreRobotsText` (booléen)\n   - Retourne : Informations détaillées sur l'annonce, y compris la description, les détails de l'hôte, les équipements, les tarifs, etc. La réponse inclut une `url` directe vers la page de l'annonce Airbnb.\n\n## Fonctionnalités\n\n- Respecte les règles du fichier robots.txt d'Airbnb\n- Utilise cheerio pour l'analyse HTML\n- Aucune clé API requise\n- Retourne des données JSON structurées\n- Réduit la charge de contexte en aplatissant et en sélectionnant les données\n- Fournit des URL directes vers les annonces Airbnb\n\n## Installation\n\n### Installation sur Claude Desktop\nAvant de commencer, assurez-vous que [Node.js](https://nodejs.org/) est installé sur votre ordinateur pour que `npx` fonctionne.\n\n1. Allez dans : Paramètres > Développeur > Modifier la configuration\n\n2. Ajoutez ce qui suit à votre fichier `claude_desktop_config.json` :\n\n```json\n{\n  \"mcpServers\": {\n    \"airbnb\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"@Domoteek/mcp-server-airbnb\"\n      ]\n    }\n  }\n}\n```\n\nPour ignorer le fichier robots.txt pour toutes les requêtes, utilisez cette version avec l'argument `--ignore-robots-txt` :\n\n```json\n{\n  \"mcpServers\": {\n    \"airbnb\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"@Domoteek/mcp-server-airbnb\",\n        \"--ignore-robots-txt\"\n      ]\n    }\n  }\n}\n```\n3. Redémarrez Claude Desktop et planifiez votre prochain voyage incluant des Airbnbs !\n\n### Autre option : Installation via Smithery\n\nPour installer mcp-server-airbnb pour Claude Desktop automatiquement via [Smithery](https://smithery.ai/server/@Domoteek/mcp-server-airbnb) :\n\n```bash\nnpx -y @smithery/cli install @Domoteek/mcp-server-airbnb --client claude\n```\n\n## Compilation (pour les développeurs)\n\n```bash\nnpm install\nnpm run build\n```\n\n## Licence\n\nCe serveur MCP est sous licence MIT.\n\n## Avertissement\n\nAirbnb est une marque déposée d'Airbnb, Inc.\nDomoteek n'est pas lié à Airbnb, Inc. ou à ses filiales\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "airbnb",
        "listings",
        "domoteek",
        "airbnb search",
        "search airbnb",
        "airbnb listings"
      ],
      "category": "web-search"
    },
    "Dreamboat-Rachel--MCP-Server-For-Local": {
      "owner": "Dreamboat-Rachel",
      "name": "MCP-Server-For-Local",
      "url": "https://github.com/Dreamboat-Rachel/MCP-Server-For-Local",
      "imageUrl": "/freedevtools/mcp/pfp/Dreamboat-Rachel.webp",
      "description": "Connect AI models to real-time data and tools with features such as weather querying, Google search automation, camera control, and image generation. The server supports modular expansion and custom API integration for tailored functionalities.",
      "stars": 14,
      "forks": 2,
      "license": "Apache License 2.0",
      "language": "Python",
      "updated_at": "2025-09-21T02:08:22Z",
      "readme_content": "# MCP Server for Local\n\n一个基于 MCP (Multi-Component Platform) 的本地代理服务器和客户端实现，提供多种 AI 工具调用能力。\n\n## 功能特点\n\n### 核心功能\n- **天气查询**：实时获取全球任意位置的天气信息，支持温度、湿度、风速等详细数据\n- **谷歌搜索**：智能检索互联网信息，支持多语言和高级搜索语法\n- **摄像头控制**：支持拍照、视频流和微表情分析，可用于情绪识别\n- **图片生成**：集成 ComfyUI，支持文本到图像的 AI 生成\n- **智能对话**：基于 DashScope 的 AI 对话能力，支持上下文理解和多轮对话\n\n### 技术特性\n- 跨平台支持（Windows 和 Linux）\n- 模块化设计，易于扩展新功能\n- 完整的日志系统，便于调试和监控\n- 支持自定义工具和 API 集成\n- 高性能并发处理能力\n\n## 环境配置\n\n### 系统要求\n- Python 3.8+\n- Node.js (可选，用于运行 JavaScript 服务器)\n- Chrome 浏览器（用于谷歌搜索功能）\n- 摄像头（用于拍照功能）\n- 至少 4GB 内存\n- 支持 CUDA 的显卡（可选，用于加速 AI 计算）\n\n### 安装步骤\n\n1. 克隆仓库：\n```bash\ngit clone https://github.com/yourusername/mcp-server-for-local.git\ncd mcp-server-for-local\n```\n\n2. 创建并激活虚拟环境：\n```bash\n# Windows\npython -m venv .venv\n.venv\\Scripts\\activate\n\n# Linux\npython3 -m venv .venv\nsource .venv/bin/activate\n```\n\n3. 安装依赖：\n```bash\n# 使用 uv 安装依赖\nuv pip install -r requirements.txt\n\n# 如果遇到网络问题，可以使用国内镜像\nuv pip install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple\n```\n\n4. 配置环境变量：\n```bash\n# 复制环境变量模板\ncp .env.example .env\n\n# 编辑 .env 文件，设置你的配置\n```\n\n### 环境变量配置\n编辑 `.env` 文件，设置以下配置：\n\n- `DASHSCOPE_API_KEY`: DashScope API 密钥（必填）\n- `MODEL`: 使用的模型名称（默认：qwen-max）\n- `CONFIG_FILE`: 服务器配置文件路径\n- `GAODE_API_KEY`: 高德地图 API 密钥（用于天气查询）\n- `CHROME_PATH`: Chrome 浏览器路径\n- `CHROMEDRIVER_PATH`: ChromeDriver 路径\n- `BASE_URL`: ComfyUI 服务器地址\n- `SERVERS_DIR`: 服务器脚本目录\n- `LOG_LEVEL`: 日志级别（可选：DEBUG, INFO, WARNING, ERROR）\n\n## 使用方法\n\n### 基本使用\n\n1. 进入项目目录：\n```bash\ncd src/mcp\n```\n\n2. 运行客户端：\n```bash\nuv run .\\client\\mcp_client.py .\\proxy\\proxy_server.py\n```\n\n3. 在客户端中输入命令，例如：\n- \"北京的天气怎么样？\"\n- \"在谷歌上搜索 Python 教程\"\n- \"拍照\"\n- \"生成一张猫的图片\"\n\n### 高级功能\n\n1. **自定义工具**：\n   - 在 `src/mcp/tools` 目录下添加新的工具类\n   - 实现必要的接口方法\n   - 在配置文件中注册新工具\n\n2. **API 扩展**：\n   - 支持添加新的 API 服务\n   - 可配置 API 密钥和端点\n   - 支持自定义请求和响应处理\n\n3. **日志管理**：\n   - 支持多级别日志记录\n   - 可配置日志输出位置\n   - 支持日志轮转和归档\n\n## 常见问题\n\n### 安装问题\n\n1. 依赖安装失败：\n```bash\n# 尝试清理缓存后重新安装\nuv pip cache purge\nuv pip install -r requirements.txt\n```\n\n2. 虚拟环境问题：\n```bash\n# 如果激活失败，尝试重新创建虚拟环境\nrm -rf .venv\npython -m venv .venv\n```\n\n### 运行问题\n\n1. 权限问题：\n```bash\n# Linux\nchmod +x src/mcp/proxy/proxy_server.py\nchmod +x src/mcp/client/mcp_client.py\n```\n\n2. Chrome 相关问题：\n- 确保 Chrome 和 ChromeDriver 版本匹配\n- 检查 Chrome 路径是否正确\n- 确保有足够的权限运行 Chrome\n- 如果遇到驱动问题，可以手动下载对应版本的 ChromeDriver\n\n3. API 密钥问题：\n- 检查 `.env` 文件中的 API 密钥是否正确\n- 确保 API 密钥有足够的配额\n- 检查网络连接是否正常\n\n## 开发指南\n\n### 项目结构\n```\nsrc/mcp/\n├── client/          # 客户端代码\n├── proxy/           # 代理服务器代码\n├── tools/           # 工具实现\n├── utils/           # 工具函数\n└── config/          # 配置文件\n```\n\n### 添加新功能\n1. 在 `tools` 目录下创建新的工具类\n2. 实现必要的接口方法\n3. 在配置文件中注册新工具\n4. 编写测试用例\n5. 更新文档\n\n## 贡献指南\n\n欢迎提交 Issue 和 Pull Request！在提交之前，请确保：\n1. 代码符合项目规范\n2. 添加了必要的测试\n3. 更新了相关文档\n4. 通过了所有测试\n\n## 许可证\n\nMIT License\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "ai",
        "automation",
        "server",
        "connect ai",
        "ai models",
        "search dreamboat"
      ],
      "category": "web-search"
    },
    "DumplingAI--mcp-server-dumplingai": {
      "owner": "DumplingAI",
      "name": "mcp-server-dumplingai",
      "url": "https://github.com/DumplingAI/mcp-server-dumplingai",
      "imageUrl": "/freedevtools/mcp/pfp/DumplingAI.webp",
      "description": "Integrates data scraping, content processing, and AI capabilities, with features for document conversion, web scraping, and knowledge management. Supports real-time information access through various data APIs and secure code execution.",
      "stars": 27,
      "forks": 6,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-09-07T08:58:27Z",
      "readme_content": "# Dumpling AI MCP Server\n\nA Model Context Protocol (MCP) server implementation that integrates with Dumpling AI for data scraping, content processing, knowledge management, AI agents, and code execution capabilities.\n\n[![smithery badge](https://smithery.ai/badge/@Dumpling-AI/mcp-server-dumplingai)](https://smithery.ai/server/@Dumpling-AI/mcp-server-dumplingai)\n\n## Features\n\n- Complete integration with all Dumpling AI API endpoints\n- Data APIs for YouTube transcripts, search, autocomplete, maps, places, news, and reviews\n- Web scraping with support for scraping, crawling, screenshots, and structured data extraction\n- Document conversion tools for text extraction, PDF operations, video processing\n- Extract data from documents, images, audio, and video\n- AI capabilities including agent completions, knowledge base management, and image generation\n- Developer tools for running JavaScript and Python code in a secure environment\n- Automatic error handling and detailed response formatting\n\n## Installation\n\n### Installing via Smithery\n\nTo install mcp-server-dumplingai for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@Dumpling-AI/mcp-server-dumplingai):\n\n```bash\nnpx -y @smithery/cli install @Dumpling-AI/mcp-server-dumplingai --client claude\n```\n\n### Running with npx\n\n```bash\nenv DUMPLING_API_KEY=your_api_key npx -y mcp-server-dumplingai\n```\n\n### Manual Installation\n\n```bash\nnpm install -g mcp-server-dumplingai\n```\n\n### Running on Cursor\n\nConfiguring Cursor 🖥️ Note: Requires Cursor version 0.45.6+\n\nTo configure Dumpling AI MCP in Cursor:\n\n1. Open Cursor Settings\n2. Go to Features > MCP Servers\n3. Click \"+ Add New MCP Server\"\n4. Enter the following:\n\n```\n{\n  \"mcpServers\": {\n    \"dumplingai\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"mcp-server-dumplingai\"],\n      \"env\": {\n        \"DUMPLING_API_KEY\": \"<your-api-key>\"\n      }\n    }\n  }\n}\n```\n\n> If you are using Windows and are running into issues, try `cmd /c \"set DUMPLING_API_KEY=your-api-key && npx -y mcp-server-dumplingai\"`\n\nReplace `your-api-key` with your Dumpling AI API key.\n\n## Configuration\n\n### Environment Variables\n\n- `DUMPLING_API_KEY`: Your Dumpling AI API key (required)\n\n## Available Tools\n\n### Data APIs\n\n#### 1. Get YouTube Transcript (`get-youtube-transcript`)\n\nExtract transcripts from YouTube videos with optional timestamps.\n\n```json\n{\n  \"name\": \"get-youtube-transcript\",\n  \"arguments\": {\n    \"videoUrl\": \"https://www.youtube.com/watch?v=dQw4w9WgXcQ\",\n    \"includeTimestamps\": true,\n    \"timestampsToCombine\": 3,\n    \"preferredLanguage\": \"en\"\n  }\n}\n```\n\n#### 2. Search (`search`)\n\nPerform Google web searches and optionally scrape content from results.\n\n```json\n{\n  \"name\": \"search\",\n  \"arguments\": {\n    \"query\": \"machine learning basics\",\n    \"country\": \"us\",\n    \"language\": \"en\",\n    \"dateRange\": \"pastMonth\",\n    \"scrapeResults\": true,\n    \"numResultsToScrape\": 3,\n    \"scrapeOptions\": {\n      \"format\": \"markdown\",\n      \"cleaned\": true\n    }\n  }\n}\n```\n\n#### 3. Get Autocomplete (`get-autocomplete`)\n\nGet Google search autocomplete suggestions for a query.\n\n```json\n{\n  \"name\": \"get-autocomplete\",\n  \"arguments\": {\n    \"query\": \"how to learn\",\n    \"country\": \"us\",\n    \"language\": \"en\",\n    \"location\": \"New York\"\n  }\n}\n```\n\n#### 4. Search Maps (`search-maps`)\n\nSearch Google Maps for locations and businesses.\n\n```json\n{\n  \"name\": \"search-maps\",\n  \"arguments\": {\n    \"query\": \"coffee shops\",\n    \"gpsPositionZoom\": \"37.7749,-122.4194,14z\",\n    \"language\": \"en\",\n    \"page\": 1\n  }\n}\n```\n\n#### 5. Search Places (`search-places`)\n\nSearch for places with more detailed information.\n\n```json\n{\n  \"name\": \"search-places\",\n  \"arguments\": {\n    \"query\": \"hotels in paris\",\n    \"country\": \"fr\",\n    \"language\": \"en\",\n    \"page\": 1\n  }\n}\n```\n\n#### 6. Search News (`search-news`)\n\nSearch for news articles with customizable parameters.\n\n```json\n{\n  \"name\": \"search-news\",\n  \"arguments\": {\n    \"query\": \"climate change\",\n    \"country\": \"us\",\n    \"language\": \"en\",\n    \"dateRange\": \"pastWeek\"\n  }\n}\n```\n\n#### 7. Get Google Reviews (`get-google-reviews`)\n\nRetrieve Google reviews for businesses or places.\n\n```json\n{\n  \"name\": \"get-google-reviews\",\n  \"arguments\": {\n    \"businessName\": \"Eiffel Tower\",\n    \"location\": \"Paris, France\",\n    \"limit\": 10,\n    \"sortBy\": \"relevance\"\n  }\n}\n```\n\n### Web Scraping\n\n#### 8. Scrape (`scrape`)\n\nExtract content from a web page with formatting options.\n\n```json\n{\n  \"name\": \"scrape\",\n  \"arguments\": {\n    \"url\": \"https://example.com\",\n    \"format\": \"markdown\",\n    \"cleaned\": true,\n    \"renderJs\": true\n  }\n}\n```\n\n#### 9. Crawl (`crawl`)\n\nRecursively crawl websites and extract content with customizable parameters.\n\n```json\n{\n  \"name\": \"crawl\",\n  \"arguments\": {\n    \"baseUrl\": \"https://example.com\",\n    \"maxPages\": 10,\n    \"crawlBeyondBaseUrl\": false,\n    \"depth\": 2,\n    \"scrapeOptions\": {\n      \"format\": \"markdown\",\n      \"cleaned\": true,\n      \"renderJs\": true\n    }\n  }\n}\n```\n\n#### 10. Screenshot (`screenshot`)\n\nCapture screenshots of web pages with customizable viewport and format options.\n\n```json\n{\n  \"name\": \"screenshot\",\n  \"arguments\": {\n    \"url\": \"https://example.com\",\n    \"width\": 1280,\n    \"height\": 800,\n    \"fullPage\": true,\n    \"format\": \"png\",\n    \"waitFor\": 1000\n  }\n}\n```\n\n#### 11. Extract (`extract`)\n\nExtract structured data from web pages using AI-powered instructions.\n\n```json\n{\n  \"name\": \"extract\",\n  \"arguments\": {\n    \"url\": \"https://example.com/products\",\n    \"instructions\": \"Extract all product names, prices, and descriptions from this page\",\n    \"schema\": {\n      \"products\": [\n        {\n          \"name\": \"string\",\n          \"price\": \"number\",\n          \"description\": \"string\"\n        }\n      ]\n    },\n    \"renderJs\": true\n  }\n}\n```\n\n### Document Conversion\n\n#### 12. Doc to Text (`doc-to-text`)\n\nConvert documents to plaintext with optional OCR.\n\n```json\n{\n  \"name\": \"doc-to-text\",\n  \"arguments\": {\n    \"url\": \"https://example.com/document.pdf\",\n    \"options\": {\n      \"ocr\": true,\n      \"language\": \"en\"\n    }\n  }\n}\n```\n\n#### 13. Convert to PDF (`convert-to-pdf`)\n\nConvert various file formats to PDF.\n\n```json\n{\n  \"name\": \"convert-to-pdf\",\n  \"arguments\": {\n    \"url\": \"https://example.com/document.docx\",\n    \"format\": \"docx\",\n    \"options\": {\n      \"quality\": 90,\n      \"pageSize\": \"A4\",\n      \"margin\": 10\n    }\n  }\n}\n```\n\n#### 14. Merge PDFs (`merge-pdfs`)\n\nCombine multiple PDFs into a single document.\n\n```json\n{\n  \"name\": \"merge-pdfs\",\n  \"arguments\": {\n    \"urls\": [\"https://example.com/doc1.pdf\", \"https://example.com/doc2.pdf\"],\n    \"options\": {\n      \"addPageNumbers\": true,\n      \"addTableOfContents\": true\n    }\n  }\n}\n```\n\n#### 15. Trim Video (`trim-video`)\n\nExtract a specific clip from a video.\n\n```json\n{\n  \"name\": \"trim-video\",\n  \"arguments\": {\n    \"url\": \"https://example.com/video.mp4\",\n    \"startTime\": 30,\n    \"endTime\": 60,\n    \"output\": \"mp4\",\n    \"options\": {\n      \"quality\": 720,\n      \"fps\": 30\n    }\n  }\n}\n```\n\n#### 16. Extract Document (`extract-document`)\n\nExtract specific content from documents in various formats.\n\n```json\n{\n  \"name\": \"extract-document\",\n  \"arguments\": {\n    \"url\": \"https://example.com/document.pdf\",\n    \"format\": \"structured\",\n    \"options\": {\n      \"ocr\": true,\n      \"language\": \"en\",\n      \"includeMetadata\": true\n    }\n  }\n}\n```\n\n#### 17. Extract Image (`extract-image`)\n\nExtract text and information from images.\n\n```json\n{\n  \"name\": \"extract-image\",\n  \"arguments\": {\n    \"url\": \"https://example.com/image.jpg\",\n    \"extractionType\": \"text\",\n    \"options\": {\n      \"language\": \"en\",\n      \"detectOrientation\": true\n    }\n  }\n}\n```\n\n#### 18. Extract Audio (`extract-audio`)\n\nTranscribe and extract information from audio files.\n\n```json\n{\n  \"name\": \"extract-audio\",\n  \"arguments\": {\n    \"url\": \"https://example.com/audio.mp3\",\n    \"language\": \"en\",\n    \"options\": {\n      \"model\": \"enhanced\",\n      \"speakerDiarization\": true,\n      \"wordTimestamps\": true\n    }\n  }\n}\n```\n\n#### 19. Extract Video (`extract-video`)\n\nExtract content from videos including transcripts, scenes, and objects.\n\n```json\n{\n  \"name\": \"extract-video\",\n  \"arguments\": {\n    \"url\": \"https://example.com/video.mp4\",\n    \"extractionType\": \"transcript\",\n    \"options\": {\n      \"language\": \"en\",\n      \"speakerDiarization\": true\n    }\n  }\n}\n```\n\n#### 20. Read PDF Metadata (`read-pdf-metadata`)\n\nExtract metadata from PDF files.\n\n```json\n{\n  \"name\": \"read-pdf-metadata\",\n  \"arguments\": {\n    \"url\": \"https://example.com/document.pdf\",\n    \"includeExtended\": true\n  }\n}\n```\n\n#### 21. Write PDF Metadata (`write-pdf-metadata`)\n\nUpdate metadata in PDF files.\n\n```json\n{\n  \"name\": \"write-pdf-metadata\",\n  \"arguments\": {\n    \"url\": \"https://example.com/document.pdf\",\n    \"metadata\": {\n      \"title\": \"New Title\",\n      \"author\": \"John Doe\",\n      \"keywords\": [\"keyword1\", \"keyword2\"]\n    }\n  }\n}\n```\n\n### AI\n\n#### 22. Generate Agent Completion (`generate-agent-completion`)\n\nGet AI agent completions with optional tool definitions.\n\n```json\n{\n  \"name\": \"generate-agent-completion\",\n  \"arguments\": {\n    \"prompt\": \"How can I improve my website's SEO?\",\n    \"model\": \"gpt-4\",\n    \"temperature\": 0.7,\n    \"maxTokens\": 500,\n    \"context\": [\"The website is an e-commerce store selling handmade crafts.\"]\n  }\n}\n```\n\n#### 23. Search Knowledge Base (`search-knowledge-base`)\n\nSearch a knowledge base for relevant information.\n\n```json\n{\n  \"name\": \"search-knowledge-base\",\n  \"arguments\": {\n    \"kbId\": \"kb_12345\",\n    \"query\": \"How to optimize database performance\",\n    \"limit\": 5,\n    \"similarityThreshold\": 0.7\n  }\n}\n```\n\n#### 24. Add to Knowledge Base (`add-to-knowledge-base`)\n\nAdd entries to a knowledge base.\n\n```json\n{\n  \"name\": \"add-to-knowledge-base\",\n  \"arguments\": {\n    \"kbId\": \"kb_12345\",\n    \"entries\": [\n      {\n        \"text\": \"MongoDB is a document-based NoSQL database.\",\n        \"metadata\": {\n          \"source\": \"MongoDB documentation\",\n          \"category\": \"databases\"\n        }\n      }\n    ],\n    \"upsert\": true\n  }\n}\n```\n\n#### 25. Generate AI Image (`generate-ai-image`)\n\nGenerate images using AI models.\n\n```json\n{\n  \"name\": \"generate-ai-image\",\n  \"arguments\": {\n    \"prompt\": \"A futuristic city with flying cars and neon lights\",\n    \"width\": 1024,\n    \"height\": 1024,\n    \"numImages\": 1,\n    \"quality\": \"hd\",\n    \"style\": \"photorealistic\"\n  }\n}\n```\n\n#### 26. Generate Image (`generate-image`)\n\nGenerate images using various AI providers.\n\n```json\n{\n  \"name\": \"generate-image\",\n  \"arguments\": {\n    \"prompt\": \"A golden retriever in a meadow of wildflowers\",\n    \"provider\": \"dalle\",\n    \"width\": 1024,\n    \"height\": 1024,\n    \"numImages\": 1\n  }\n}\n```\n\n### Developer Tools\n\n#### 27. Run JavaScript Code (`run-js-code`)\n\nExecute JavaScript code with optional dependencies.\n\n```json\n{\n  \"name\": \"run-js-code\",\n  \"arguments\": {\n    \"code\": \"const result = [1, 2, 3, 4].reduce((sum, num) => sum + num, 0); console.log(`Sum: ${result}`); return result;\",\n    \"dependencies\": {\n      \"lodash\": \"^4.17.21\"\n    },\n    \"timeout\": 5000\n  }\n}\n```\n\n#### 28. Run Python Code (`run-python-code`)\n\nExecute Python code with optional dependencies.\n\n```json\n{\n  \"name\": \"run-python-code\",\n  \"arguments\": {\n    \"code\": \"import numpy as np\\narr = np.array([1, 2, 3, 4, 5])\\nmean = np.mean(arr)\\nprint(f'Mean: {mean}')\\nreturn mean\",\n    \"dependencies\": [\"numpy\", \"pandas\"],\n    \"timeout\": 10000,\n    \"saveOutputFiles\": true\n  }\n}\n```\n\n## Error Handling\n\nThe server provides robust error handling:\n\n- Detailed error messages with HTTP status codes\n- API key validation\n- Input validation using Zod schemas\n- Network error handling with descriptive messages\n\nExample error response:\n\n```json\n{\n  \"content\": [\n    {\n      \"type\": \"text\",\n      \"text\": \"Error: Failed to fetch YouTube transcript: 404 Not Found\"\n    }\n  ],\n  \"isError\": true\n}\n```\n\n## Development\n\n```bash\n# Install dependencies\nnpm install\n\n# Build\nnpm run build\n\n```\n\n## License\n\nMIT License - see LICENSE file for details\n",
      "npm_url": "https://www.npmjs.com/package/mcp-server-dumplingai",
      "npm_downloads": 4773,
      "keywords": [
        "scraping",
        "dumplingai",
        "apis",
        "search dumplingai",
        "server dumplingai",
        "data scraping"
      ],
      "category": "web-search"
    },
    "DynamicEndpoints--Autogen_MCP": {
      "owner": "DynamicEndpoints",
      "name": "Autogen_MCP",
      "url": "https://github.com/DynamicEndpoints/Autogen_MCP",
      "imageUrl": "/freedevtools/mcp/pfp/DynamicEndpoints.webp",
      "description": "Facilitates the creation and management of AI agents that engage in natural language interactions and collaborate to solve problems. Supports orchestration of both individual and group conversations with customizable configurations and built-in error handling.",
      "stars": 15,
      "forks": 3,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-17T01:15:10Z",
      "readme_content": "# Enhanced AutoGen MCP Server\n\n[![smithery badge](https://smithery.ai/badge/@DynamicEndpoints/autogen_mcp)](https://smithery.ai/server/@DynamicEndpoints/autogen_mcp)\n\nA comprehensive MCP server that provides deep integration with Microsoft's AutoGen framework v0.9+, featuring the latest capabilities including prompts, resources, advanced workflows, and enhanced agent types. This server enables sophisticated multi-agent conversations through a standardized Model Context Protocol interface.\n\n## 🚀 Latest Features (v0.2.0)\n\n### ✨ **Enhanced MCP Support**\n- **Prompts**: Pre-built templates for common workflows (code review, research, creative writing)\n- **Resources**: Real-time access to agent status, chat history, and configurations\n- **Dynamic Content**: Template-based prompts with arguments and embedded resources\n- **Latest MCP SDK**: Version 1.12.3 with full feature support\n\n### 🤖 **Advanced Agent Types**\n- **Assistant Agents**: Enhanced with latest LLM capabilities\n- **Conversable Agents**: Flexible conversation patterns\n- **Teachable Agents**: Learning and memory persistence\n- **Retrievable Agents**: Knowledge base integration\n- **Multimodal Agents**: Image and document processing (when available)\n\n### 🔄 **Sophisticated Workflows**\n- **Code Generation**: Architect → Developer → Reviewer → Executor pipeline\n- **Research Analysis**: Researcher → Analyst → Critic → Synthesizer workflow\n- **Creative Writing**: Multi-stage creative collaboration\n- **Problem Solving**: Structured approach to complex problems\n- **Code Review**: Security → Performance → Style review teams\n- **Custom Workflows**: Build your own agent collaboration patterns\n\n### 🎯 **Enhanced Chat Capabilities**\n- **Smart Speaker Selection**: Auto, manual, random, round-robin modes\n- **Nested Conversations**: Hierarchical agent interactions\n- **Swarm Intelligence**: Coordinated multi-agent problem solving\n- **Memory Management**: Persistent agent knowledge and preferences\n- **Quality Checks**: Built-in validation and improvement loops\n\n## 🛠️ Available Tools\n\n### Core Agent Management\n- `create_agent` - Create agents with advanced configurations\n- `create_workflow` - Build complete multi-agent workflows\n- `get_agent_status` - Detailed agent metrics and health monitoring\n\n### Conversation Execution\n- `execute_chat` - Enhanced two-agent conversations\n- `execute_group_chat` - Multi-agent group discussions\n- `execute_nested_chat` - Hierarchical conversation structures\n- `execute_swarm` - Swarm-based collaborative problem solving\n\n### Workflow Orchestration\n- `execute_workflow` - Run predefined workflow templates\n- `manage_agent_memory` - Handle agent learning and persistence\n- `configure_teachability` - Enable/configure agent learning capabilities\n\n## 📝 Available Prompts\n\n### `autogen-workflow`\nCreate sophisticated multi-agent workflows with customizable parameters:\n- **Arguments**: `task_description`, `agent_count`, `workflow_type`\n- **Use case**: Rapid workflow prototyping and deployment\n\n### `code-review`\nSet up collaborative code review with specialized agents:\n- **Arguments**: `code`, `language`, `focus_areas`\n- **Use case**: Comprehensive code quality assessment\n\n### `research-analysis`\nDeploy research teams for in-depth topic analysis:\n- **Arguments**: `topic`, `depth`\n- **Use case**: Academic research, market analysis, technical investigation\n\n## 📊 Available Resources\n\n### `autogen://agents/list`\nLive list of active agents with status and capabilities\n\n### `autogen://workflows/templates`\nAvailable workflow templates and configurations\n\n### `autogen://chat/history`\nRecent conversation history and interaction logs\n\n### `autogen://config/current`\nCurrent server configuration and settings\n\n## Installation\n\n### Installing via Smithery\n\nTo install AutoGen Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@DynamicEndpoints/autogen_mcp):\n\n```bash\nnpx -y @smithery/cli install @DynamicEndpoints/autogen_mcp --client claude\n```\n\n### Manual Installation\n\n1. **Clone the repository:**\n```bash\ngit clone https://github.com/yourusername/autogen-mcp.git\ncd autogen-mcp\n```\n\n2. **Install Node.js dependencies:**\n```bash\nnpm install\n```\n\n3. **Install Python dependencies:**\n```bash\npip install -r requirements.txt --user\n```\n\n4. **Build the TypeScript project:**\n```bash\nnpm run build\n```\n\n5. **Set up configuration:**\n```bash\ncp .env.example .env\ncp config.json.example config.json\n# Edit .env and config.json with your settings\n```\n\n## Configuration\n\n### Environment Variables\n\nCreate a `.env` file from the template:\n\n```bash\n# Required\nOPENAI_API_KEY=your-openai-api-key-here\n\n# Optional - Path to configuration file\nAUTOGEN_MCP_CONFIG=config.json\n\n# Enhanced Features\nENABLE_PROMPTS=true\nENABLE_RESOURCES=true\nENABLE_WORKFLOWS=true\nENABLE_TEACHABILITY=true\n\n# Performance Settings\nMAX_CHAT_TURNS=10\nDEFAULT_OUTPUT_FORMAT=json\n```\n\n### Configuration File\n\nUpdate `config.json` with your preferences:\n\n```json\n{\n  \"llm_config\": {\n    \"config_list\": [\n      {\n        \"model\": \"gpt-4o\",\n        \"api_key\": \"your-openai-api-key\"\n      }\n    ],\n    \"temperature\": 0.7\n  },\n  \"enhanced_features\": {\n    \"prompts\": { \"enabled\": true },\n    \"resources\": { \"enabled\": true },\n    \"workflows\": { \"enabled\": true }\n  }\n}\n```\n\n## Usage Examples\n\n### Using with Claude Desktop\n\nAdd to your `claude_desktop_config.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"autogen\": {\n      \"command\": \"node\",\n      \"args\": [\"path/to/autogen-mcp/build/index.js\"],\n      \"env\": {\n        \"OPENAI_API_KEY\": \"your-key-here\"\n      }\n    }\n  }\n}\n```\n\n### Command Line Testing\n\nTest the server functionality:\n\n```bash\n# Run comprehensive tests\npython test_server.py\n\n# Test CLI interface\npython cli_example.py create_agent \"researcher\" \"assistant\" \"You are a research specialist\"\npython cli_example.py execute_workflow \"code_generation\" '{\"task\":\"Hello world\",\"language\":\"python\"}'\n```\n\n### Using Prompts\n\nThe server provides several built-in prompts:\n\n1. **autogen-workflow** - Create multi-agent workflows\n2. **code-review** - Set up collaborative code review\n3. **research-analysis** - Deploy research teams\n\n### Accessing Resources\n\nAvailable resources provide real-time data:\n\n- `autogen://agents/list` - Current active agents\n- `autogen://workflows/templates` - Available workflow templates  \n- `autogen://chat/history` - Recent conversation history\n- `autogen://config/current` - Server configuration\n\n## Workflow Examples\n\n### Code Generation Workflow\n\n```json\n{\n  \"workflow_name\": \"code_generation\",\n  \"input_data\": {\n    \"task\": \"Create a REST API endpoint\",\n    \"language\": \"python\",\n    \"requirements\": [\"FastAPI\", \"Pydantic\", \"Error handling\"]\n  },\n  \"quality_checks\": true\n}\n```\n\n### Research Workflow\n\n```json\n{\n  \"workflow_name\": \"research\", \n  \"input_data\": {\n    \"topic\": \"AI Ethics in 2025\",\n    \"depth\": \"comprehensive\"\n  },\n  \"output_format\": \"markdown\"\n}\n```\n\n## Advanced Features\n\n### Agent Types\n\n- **Assistant Agents**: LLM-powered conversational agents\n- **User Proxy Agents**: Code execution and human interaction\n- **Conversable Agents**: Flexible conversation patterns\n- **Teachable Agents**: Learning and memory persistence (when available)\n- **Retrievable Agents**: Knowledge base integration (when available)\n\n### Chat Modes\n\n- **Two-Agent Chat**: Direct conversation between agents\n- **Group Chat**: Multi-agent discussions with smart speaker selection\n- **Nested Chat**: Hierarchical conversation structures  \n- **Swarm Intelligence**: Coordinated problem solving (experimental)\n\n### Memory Management\n\n- Persistent agent memory across sessions\n- Conversation history tracking\n- Learning from interactions (teachable agents)\n- Memory cleanup and optimization\n\n## Troubleshooting\n\n### Common Issues\n\n1. **API Key Errors**: Ensure your OpenAI API key is valid and has sufficient credits\n2. **Import Errors**: Install all dependencies with `pip install -r requirements.txt --user`\n3. **Build Failures**: Check Node.js version (>= 18) and run `npm install`\n4. **Chat Failures**: Verify agent creation succeeded before attempting conversations\n\n### Debug Mode\n\nEnable detailed logging:\n\n```bash\nexport LOG_LEVEL=DEBUG\npython test_server.py\n```\n\n### Performance Tips\n\n- Use `gpt-4o-mini` for faster, cost-effective operations\n- Enable caching for repeated operations\n- Set appropriate timeout values for long-running workflows\n- Use quality checks only when needed (increases execution time)\n\n## Development\n\n### Running Tests\n\n```bash\n# Full test suite\npython test_server.py\n\n# Individual workflow tests  \npython -c \"\nimport asyncio\nfrom src.autogen_mcp.workflows import WorkflowManager\nwm = WorkflowManager()\nprint(asyncio.run(wm.execute_workflow('code_generation', {'task': 'test'})))\n\"\n```\n\n### Building\n\n```bash\nnpm run build\nnpm run lint\n```\n\n### Contributing\n\n1. Fork the repository\n2. Create a feature branch\n3. Make your changes\n4. Add tests for new functionality\n5. Submit a pull request\n\n## Version History\n\n### v0.2.0 (Latest)\n- ✨ Enhanced MCP support with prompts and resources\n- 🤖 Advanced agent types (teachable, retrievable)\n- 🔄 Sophisticated workflows with quality checks\n- 🎯 Smart speaker selection and nested conversations\n- 📊 Real-time resource monitoring\n- 🧠 Memory management and persistence\n\n### v0.1.0\n- Basic AutoGen integration\n- Simple agent creation and chat execution\n- MCP tool interface\n\n## Support\n\nFor issues and questions:\n- Check the troubleshooting section above\n- Review the test examples in `test_server.py`\n- Open an issue on GitHub with detailed reproduction steps\n\n## License\n\nMIT License - see LICENSE file for details.\n\n# OpenAI API Key (optional, can also be set in config.json)\nOPENAI_API_KEY=your-openai-api-key\n```\n\n### Server Configuration\n\n1. Copy `config.json.example` to `config.json`:\n```bash\ncp config.json.example config.json\n```\n\n2. Configure the server settings:\n```json\n{\n  \"llm_config\": {\n    \"config_list\": [\n      {\n        \"model\": \"gpt-4\",\n        \"api_key\": \"your-openai-api-key\"\n      }\n    ],\n    \"temperature\": 0\n  },\n  \"code_execution_config\": {\n    \"work_dir\": \"workspace\",\n    \"use_docker\": false\n  }\n}\n```\n\n## Available Operations\n\nThe server supports three main operations:\n\n### 1. Creating Agents\n\n```json\n{\n  \"name\": \"create_agent\",\n  \"arguments\": {\n    \"name\": \"tech_lead\",\n    \"type\": \"assistant\",\n    \"system_message\": \"You are a technical lead with expertise in software architecture and design patterns.\"\n  }\n}\n```\n\n### 2. One-on-One Chat\n\n```json\n{\n  \"name\": \"execute_chat\",\n  \"arguments\": {\n    \"initiator\": \"agent1\",\n    \"responder\": \"agent2\",\n    \"message\": \"Let's discuss the system architecture.\"\n  }\n}\n```\n\n### 3. Group Chat\n\n```json\n{\n  \"name\": \"execute_group_chat\",\n  \"arguments\": {\n    \"agents\": [\"agent1\", \"agent2\", \"agent3\"],\n    \"message\": \"Let's review the proposed solution.\"\n  }\n}\n```\n\n## Error Handling\n\nCommon error scenarios include:\n\n1. Agent Creation Errors\n```json\n{\n  \"error\": \"Agent already exists\"\n}\n```\n\n2. Execution Errors\n```json\n{\n  \"error\": \"Agent not found\"\n}\n```\n\n3. Configuration Errors\n```json\n{\n  \"error\": \"AUTOGEN_MCP_CONFIG environment variable not set\"\n}\n```\n\n## Architecture\n\nThe server follows a modular architecture:\n\n```\nsrc/\n├── autogen_mcp/\n│   ├── __init__.py\n│   ├── agents.py      # Agent management and configuration\n│   ├── config.py      # Configuration handling and validation\n│   ├── server.py      # MCP server implementation\n│   └── workflows.py   # Conversation workflow management\n```\n\n## License\n\nMIT License - See LICENSE file for details\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "ai",
        "autogen_mcp",
        "search",
        "ai agents",
        "autogen_mcp facilitates",
        "dynamicendpoints autogen_mcp"
      ],
      "category": "web-search"
    },
    "Ejb503--systemprompt-mcp-reddit": {
      "owner": "Ejb503",
      "name": "systemprompt-mcp-reddit",
      "url": "https://github.com/Ejb503/systemprompt-mcp-reddit",
      "imageUrl": "/freedevtools/mcp/pfp/Ejb503.webp",
      "description": "Fetch posts and content from Reddit, manage subreddits, and customize parameters for enhanced social media interaction.",
      "stars": 8,
      "forks": 5,
      "license": "Other",
      "language": "JavaScript",
      "updated_at": "2025-09-10T22:39:42Z",
      "readme_content": "# systemprompt-mcp-reddit\n\n[![npm version](https://img.shields.io/npm/v/systemprompt-mcp-reddit.svg)](https://www.npmjs.com/package/systemprompt-mcp-reddit)\n[![smithery badge](https://smithery.ai/badge/systemprompt-mcp-reddit)](https://smithery.ai/server/systemprompt-mcp-reddit)\n[![License: Apache-2.0](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)\n[![Twitter Follow](https://img.shields.io/twitter/follow/tyingshoelaces_?style=social)](https://twitter.com/tyingshoelaces_)\n[![Discord](https://img.shields.io/discord/1255160891062620252?color=7289da&label=discord)](https://discord.com/invite/wkAbSuPWpr)\n\n[Website](https://systemprompt.io) | [Documentation](https://systemprompt.io/documentation) | [Blog](https://tyingshoelaces.com) | [Get API Key](https://systemprompt.io/console)\n\nA specialized Model Context Protocol (MCP) server that enables AI agents to interact with Reddit, including reading posts, creating content, and managing subreddit configurations. The server is designed to work specifically with [systemprompt.io](https://systemprompt.io) client that support sampling and notification features, and may not function properly with other MCP clients.\n\nAn API KEY is required to use this server. This is currently free, although this may change in the future. You can get one [here](https://systemprompt.io/console).\n\nThis server uses Sampling and Notification functionality from the [@modelcontextprotocol/sdk](https://github.com/modelcontextprotocol/sdk).\n\n## Features\n\n#### Core Functionality\n\n- **Subreddit Configuration**: Configure and manage multiple subreddits for the AI agent\n- **Content Retrieval**: Fetch hot, new, or controversial posts from configured subreddits\n- **Content Creation**: Generate AI-powered posts and replies with customizable parameters\n- **Writing Style Control**: Configure tone, vocabulary, and content guidelines for the AI\n\n#### Advanced Features\n\n- **Rule Compliance**: Automatic adherence to subreddit rules and requirements\n- **Content Guidelines**: Customizable instructions for content generation\n- **Flexible Post Types**: Support for both text and link posts\n- **Smart Replies**: Context-aware response generation\n\n#### Integration Features\n\n- **MCP Protocol Integration**: Full implementation of Model Context Protocol\n- **Type-Safe Implementation**: Complete TypeScript support\n- **Real-Time Processing**: Supports streaming responses\n- **Advanced Error Handling**: Comprehensive error management\n\n## System Architecture\n\nThis project follows a modular architecture designed to be adaptable for other MCP server implementations:\n\n### Core Components\n\n- **MCP Protocol Layer**: Implements the complete Model Context Protocol\n- **Service Layer**: Abstracts Reddit API interactions\n- **Handler Layer**: Routes and processes MCP requests\n- **Tool Layer**: Defines operations AI agents can perform\n- **Utility Layer**: Provides helpers for validation and data transformation\n\n### Directory Structure\n\n- `/src/config`: Server configuration and capabilities\n- `/src/constants`: Tool schemas, sampling templates, and system constants\n- `/src/handlers`: Request handlers and tool implementations\n- `/src/services`: API integration services\n- `/src/types`: TypeScript type definitions\n- `/src/utils`: Utility functions and helpers\n\nFor detailed documentation of the architecture and implementation patterns, see:\n- [ARCHITECTURE.md](./ARCHITECTURE.md): Complete system overview\n- [TEMPLATE_GUIDE.md](./TEMPLATE_GUIDE.md): Guide for creating new MCP servers\n\n## Using as a Template\n\nThis codebase is designed to serve as a template for creating other MCP servers. The modular architecture makes it straightforward to replace the Reddit integration with other APIs:\n\n1. Replace the service layer with your API implementation\n2. Define new tools appropriate for your domain\n3. Update type definitions and schemas\n4. Configure server capabilities\n\nSee [TEMPLATE_GUIDE.md](./TEMPLATE_GUIDE.md) for detailed step-by-step instructions.\n\n## 🎥 Demo & Showcase\n\nWatch our video demonstration to see Systemprompt MCP Reddit in action:\n\n[▶️ Watch Demo Video](https://www.youtube.com/watch?v=NyXkfVAv7OE)\n\n## Related Links\n\n- [Multimodal MCP Client](https://github.com/Ejb503/multimodal-mcp-client) - Voice-powered MCP client\n- [systemprompt.io Documentation](https://systemprompt.io/docs)\n",
      "npm_url": "https://www.npmjs.com/package/systemprompt-mcp-reddit",
      "npm_downloads": 2260,
      "keywords": [
        "ejb503",
        "reddit",
        "search",
        "reddit fetch",
        "mcp reddit",
        "search ejb503"
      ],
      "category": "web-search"
    },
    "ElfProxy--proxy-pool-mcp-server": {
      "owner": "ElfProxy",
      "name": "proxy-pool-mcp-server",
      "url": "https://github.com/ElfProxy/proxy-pool-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/ElfProxy.webp",
      "description": "Provides secure access to global residential proxies with dynamic IP rotation, enabling geo-targeted web data extraction and AI-optimized web interactions. Features include adaptive rate limiting and intelligent content sanitization for enhanced AI workflows.",
      "stars": 1,
      "forks": 0,
      "license": "MIT License",
      "language": "Java",
      "updated_at": "2025-04-18T10:32:08Z",
      "readme_content": "<p align=\"center\">\n  \n</p>\n<h1 align=\"center\" style=\"border-bottom: none;\">\n  ElfProxy MCP Server\n</h1>\n<p align=\"center\">\n  <p align=\"center\">Extract agent seamlessly with ElfProxy via the Model Context Protocol</p>\n</p>\n\n[![jdk version](https://img.shields.io/badge/java%20version-%2017-blue)](http://pyrn.2.vu/1)\n[![SpringBoot version](https://img.shields.io/badge/spring%20boot-%203-blue)](http://pyrn.2.vu/1)\n\n<br/>\n\n## 📖 Overview\n\nA privacy-first infrastructure solution combining dynamic IP rotation with AI-optimized web interaction capabilities. This integration enables secure, large-scale web data access for AI systems through two synergistic components:\n\n**1. ElfProxy Dynamic IP Network**  \nGlobal residential proxy infrastructure featuring:\n- 🌐 195+ country IP pools with city-level targeting\n- 🔄 Automatic IP rotation (1s-24h adjustable intervals)\n- 🛡️ Built-in TLS fingerprint masking\n- ⚡ Multi-protocol support (HTTP/SOCKS5/WebSocket)\n- 📊 Real-time traffic analytics dashboard\n\n**2. Enhanced Model Context Protocol (MCP)**  \nAI-specific web interaction layer with:\n- 🧠 Context-aware request scheduling\n- 🖥️ Headless browser rendering cluster\n- ✂️ Intelligent content sanitization (Ads/Trackers/Boilerplate removal)\n- 🔄 3-stage failover system (IP/Header/Protocol rotation)\n- ⏱️ Adaptive rate limiting (Requests/second auto-tuning)\n\n### Key Features\n| Category              | Capabilities                                                                 |\n|-----------------------|-----------------------------------------------------------------------------|\n| **Anonymity**         | IP/UserAgent/Canvas fingerprint rotation                                   |\n| **AI Optimization**   | Content extraction → Markdown/JSON/Plaintext conversion                    |\n| **Geo-Targeting**     | Country/City/ASN-level location simulation                                 |\n| **Security**          | End-to-end HTTPS encryption + Request timestamp obfuscation                |\n| **Reliability**       | 99.99% SLA with automatic proxy blacklisting                              |\n\n## 💡 Example Queries\nWhen you've set up the MCP server with **Claude**, you can make requests like:\n\n- Please extract a proxy IP address from the United States for me\n- Accessing through a US proxy IP using `puppeteer`  ` https://www.amazon.de `\n\n## ✅ Prerequisites\n\nBefore you begin, make sure you have:\n\n- **ElfProxy Account**: Please register an account in [ElfProxy](http://pyrn.2.vu/1/) first, contact ElfProxy customers, and obtain an apiKey (there will be 200MB of data available for trial use)\n\n### 🧩 Server Configuration\n\nThe apiKey can be configured using the `application.yml` system property. \n\n### Cursor Tool Configuration\n\nTo use this MCP server in Cursor, add the following configuration to your Cursor settings:\n\n```json\n{\n  \"proxy-pool-server\": {\n    \"url\": \"http://localhost:9000/sse\",\n    \"enabled\": true\n  }\n}\n```\n\n## Building\n\n```bash\nmvn clean package\n```\n\n## Running\n\n```bash\njava -jar target/proxy-pool-server-{version}.jar\n```\n\n## API Endpoints\n\nThe server exposes the following MCP tools:\n\n- `getProxy`:Retrieve an IP address from the proxy pool and use the 'code' field to tell me which country's code you need. I will extract the IP address from the code and provide it to you\n\n\n\n## License\n\nThis project is licensed under the [MIT License](LICENSE).\n\n## About ElfProxy\n\nWith over 100M IP nodes and global service coverage (excluding China), we provide service support for multiple million level products with stable and reliable IP quality. Our comprehensive services are trustworthy\n\n[](http://pyrn.2.vu/1/)",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "elfproxy",
        "proxy",
        "proxies",
        "search elfproxy",
        "proxy pool",
        "elfproxy proxy"
      ],
      "category": "web-search"
    },
    "ErickWendel--erickwendel-contributions-mcp": {
      "owner": "ErickWendel",
      "name": "erickwendel-contributions-mcp",
      "url": "https://github.com/ErickWendel/erickwendel-contributions-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/ErickWendel.webp",
      "description": "Queries Erick Wendel's contributions across various platforms, including talks, blog posts, and videos, using natural language. Integrates with tools like Claude and Cursor for seamless data access.",
      "stars": 100,
      "forks": 18,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-06-02T13:57:17Z",
      "readme_content": "# erickwendel-contributions-mcp\n\n![CI Status](https://github.com/ErickWendel/erickwendel-contributions-mcp/workflows/Test%20MCP%20Server/badge.svg)\n[![smithery badge](https://smithery.ai/badge/@ErickWendel/erickwendel-contributions-mcp)](https://smithery.ai/server/@ErickWendel/erickwendel-contributions-mcp)\n\nA Model Context Protocol (MCP) server that provides tools to query [Erick Wendel's contributions](https://erickwendel.com.br/) across different platforms. Query talks, blog posts, and videos using natural language through Claude, Cursor or similars. This project was built using [Cursor](https://cursor.sh) IDE with the default agent (trial version).\n\nThis MCP server is also available on [Smithery](https://smithery.ai/server/@ErickWendel/erickwendel-contributions-mcp) for direct integration.\n\n## Available Tools\n\nThis MCP server provides the following tools to interact with the API:\n\n- `get-talks`: Retrieves a paginated list of talks with optional filtering\n  - Supports filtering by ID, title, language, city, country, and year\n  - Can return counts grouped by language, country, or city\n\n- `get-posts`: Fetches posts with optional filtering and pagination\n  - Supports filtering by ID, title, language, and portal\n\n- `get-videos`: Retrieves videos with optional filtering and pagination\n  - Supports filtering by ID, title, and language\n\n- `check-status`: Verifies if the API is alive and responding\n\n# Integration with AI Tools\n\n## Inspect MCP Server Capabilities\n\nYou can inspect this MCP server's capabilities using Smithery:\n\n```bash\nnpx -y @smithery/cli@latest inspect @ErickWendel/erickwendel-contributions-mcp\n```\n\nThis will show you all available tools, their parameters, and how to use them.\n\n## Setup\n\n1. Make sure you're using Node.js v23+\n```bash\nnode -v\n#v23.9.0\n```\n\n2. Clone this repository:\n```bash\ngit clone https://github.com/erickwendel/erickwendel-contributions-mcp.git\ncd erickwendel-contributions-mcp\n```\n\n3. Restore dependencies:\n```bash\nnpm ci\n```\n\n## Integration with AI Tools\n\n### Cursor Setup\n\n1. Open Cursor Settings\n2. Navigate to MCP section\n3. Click \"Add new MCP server\"\n4. Configure the server:\n   ```\n   Name = erickwendel-contributions\n   Type = command\n   Command = node ABSOLUTE_PATH_TO_PROJECT/src/index.ts\n   ```\n\n   or if you prefer executing it from Smithery\n   ```\n   Name = erickwendel-contributions\n   Type = command\n   Command = npm exec -- @smithery/cli@latest run @ErickWendel/erickwendel-contributions-mcp\n   ```\n\n\nor configure directly from the Cursor's global MCP file located in `~/.cursor/mcp.json` and add the following:\n\n```json\n{\n  \"mcpServers\": {\n    \"erickwendel-contributions\": {\n      \"command\": \"node\",\n      \"args\": [\"ABSOLUTE_PATH_TO_PROJECT/src/index.ts\"]\n    }\n  }\n}\n```\nor if you prefer executing it from Smithery\n```json\n{\n  \"mcpServers\": {\n    \"erickwendel-contributions\": {\n      \"command\": \"npm\",\n      \"args\": [\n        \"exec\",\n        \"--\",\n        \"@smithery/cli@latest\",\n        \"run\",\n        \"@ErickWendel/erickwendel-contributions-mcp\"\n      ]\n    }\n  }\n}\n```\n\n5. Make sure Cursor chat is in Agent mode by selecting \"Agent\" in the lower left side dropdown\n\n6. Go to the chat an ask \"how many videos were published about JavaScript in 2024\"\n\n\n\n### Claude Desktop Setup\n\n#### Installing via Smithery\n\nTo install Erick Wendel Contributions for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@ErickWendel/erickwendel-contributions-mcp):\n\n```bash\nnpx -y @smithery/cli install @ErickWendel/erickwendel-contributions-mcp --client claude\n```\n\n> **Note**: The Smithery CLI installation for Claude is currently experiencing issues. Please use the manual installation method below until this is resolved.\n\n#### Manual Setup\n\n1. Go to Claude settings\n2. Click in the Developer tab\n3. Click in edit config\n4. Open the config in a code editor\n5. Add the following configuration to your Claude Desktop config:\n\n```json\n{\n  \"mcpServers\": {\n    \"erickwendel-contributions\": {\n      \"command\": \"node\",\n      \"args\": [\"ABSOLUTE_PATH_TO_PROJECT/src/index.ts\"]\n    }\n  }\n}\n```\nor if you prefer executing it from Smithery\n```json\n{\n  \"mcpServers\": {\n    \"erickwendel-contributions\": {\n      \"command\": \"npm\",\n      \"args\": [\n        \"exec\",\n        \"--\",\n        \"@smithery/cli@latest\",\n        \"run\",\n        \"@ErickWendel/erickwendel-contributions-mcp\"\n      ]\n    }\n  }\n}\n```\n\n6. Save file and Restart Claude Desktop\n7. Open the Developer tab again and check if it's in the \"running\" state as follows:\n\n\n\n8. Go to the chat and ask \"Are there videos about RAG?\"\n\n\n\n### Free Alternative Using MCPHost\n\nIf you don't have access to Claude Desktop nor Cursor, you can use [MCPHost](https://github.com/mark3labs/mcphost) with Ollama as a free alternative. MCPHost is a CLI tool that enables Large Language Models to interact with MCP servers.\n\n1. Install MCPHost:\n```bash\ngo install github.com/mark3labs/mcphost@latest\n```\n\n2. Create a config file (e.g. [./mcp.jsonc](./mcp.jsonc)):\n```json\n{\n  \"mcpServers\": {\n    \"erickwendel-contributions\": {\n      \"command\": \"node\",\n      \"args\": [\"ABSOLUTE_PATH_TO_PROJECT/src/index.ts\"]\n    }\n  }\n}\n```\nor if you prefer executing it from Smithery\n```json\n{\n  \"mcpServers\": {\n    \"erickwendel-contributions\": {\n      \"command\": \"npm\",\n      \"args\": [\n        \"exec\",\n        \"--\",\n        \"@smithery/cli@latest\",\n        \"run\",\n        \"@ErickWendel/erickwendel-contributions-mcp\"\n      ]\n    }\n  }\n}\n```\n3. Run MCPHost with your preferred Ollama model:\n```bash\nollama pull MODEL_NAME\nmcphost --config ./mcp.jsonc -m ollama:MODEL_NAME\n```\n\n## Example Queries\n\nHere are some examples of queries you can ask Claude, Cursor or any MCP Client:\n\n1. \"How many talks were given in 2023?\"\n\n\n\n2. \"Show me talks in Spanish\"\n\n\n\n3. \"Find posts about WebXR\"\n\n\n\n\n# Development\n## Features\n\n- Built with Model Context Protocol (MCP)\n- Type-safe with TypeScript and Zod schema validation\n- Native TypeScript support in Node.js without transpilation\n- Generated SDK using [GenQL](https://genql.dev)\n- Modular architecture with separation of concerns\n- Standard I/O transport for easy integration\n- Structured error handling\n- Compatible with Claude Desktop, Cursor, and [MCPHost](https://github.com/mark3labs/mcphost) (free alternative)\n\n> Note: This project requires Node.js v23+ as it uses the native TypeScript support added in the last year.\n\n## Architecture\n\nThe codebase follows a modular structure:\n\n```\nsrc/\n  ├── config/      # Configuration settings\n  ├── types/       # TypeScript interfaces and types\n  ├── tools/       # MCP tool implementations\n  ├── utils/       # Utility functions\n  ├── services/    # API service layer\n  └── index.ts     # Main entry point\n```\n\n## Testing\n\nTo run the test suite:\n\n```bash\nnpm test\n```\n\nFor development mode with watch:\n\n```bash\nnpm run test:dev\n```\n\n## Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n\n## Author\n\n[Erick Wendel](https://linktr.ee/erickwendel)\n\n## License\n\nThis project is licensed under the MIT License - see the [LICENSE](./LICENSE) file for details.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "erickwendel",
        "erick",
        "search",
        "search erickwendel",
        "erickwendel contributions",
        "queries erick"
      ],
      "category": "web-search"
    },
    "Evilran--baidu-mcp-server": {
      "owner": "Evilran",
      "name": "baidu-mcp-server",
      "url": "https://github.com/Evilran/baidu-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/Evilran.webp",
      "description": "Connects to Baidu to perform web searches and retrieve webpage content, incorporating intelligent text extraction and formatted results. Features built-in rate limiting and robust error handling for efficient content management.",
      "stars": 13,
      "forks": 4,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-28T06:14:41Z",
      "readme_content": "# Baidu Search MCP Server\n\n[![smithery badge](https://smithery.ai/badge/@Evilran/baidu-mcp-server)](https://smithery.ai/server/@Evilran/baidu-mcp-server)\n\nA Model Context Protocol (MCP) server that provides web search capabilities through Baidu, with additional features for content fetching and parsing.\n\n<a href=\"https://glama.ai/mcp/servers/phcus2gcpn\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/phcus2gcpn/badge\" alt=\"Baidu Server MCP server\" />\n</a>\n\n## Features\n\n- **Web Search**: Search Baidu with advanced rate limiting and result formatting\n- **Content Fetching**: Retrieve and parse webpage content with intelligent text extraction\n- **Rate Limiting**: Built-in protection against rate limits for both search and content fetching\n- **Error Handling**: Comprehensive error handling and logging\n- **LLM-Friendly Output**: Results formatted specifically for large language model consumption\n\n## Installation\n\n### Installing via Smithery\n\nTo install Baidu Search Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@Evilran/baidu-mcp-server):\n\n```bash\nnpx -y @smithery/cli install @Evilran/baidu-mcp-server --client claude\n```\n\n### Installing via `uv`\n\nInstall directly from PyPI using `uv`:\n\n```bash\nuv pip install baidu-mcp-server\n```\n\n## Usage\n\n### Running with Claude Desktop\n\n1. Download [Claude Desktop](https://claude.ai/download)\n2. Create or edit your Claude Desktop configuration:\n   - On macOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\n   - On Windows: `%APPDATA%\\Claude\\claude_desktop_config.json`\n\nAdd the following configuration:\n\n```json\n{\n    \"mcpServers\": {\n        \"baidu-search\": {\n            \"command\": \"uvx\",\n            \"args\": [\"baidu-mcp-server\"]\n        }\n    }\n}\n```\n\n3. Restart Claude Desktop\n\n### Development\n\nFor local development, you can use the MCP CLI:\n\n```bash\n# Run with the MCP Inspector\nmcp dev server.py\n\n# Install locally for testing with Claude Desktop\nmcp install server.py\n```\n## Available Tools\n\n### 1. Search Tool\n\n```python\nasync def search(query: str, max_results: int = 10) -> str\n```\n\nPerforms a web search on Baidu and returns formatted results.\n\n**Parameters:**\n- `query`: Search query string\n- `max_results`: Maximum number of results to return (default: 10)\n\n**Returns:**\nFormatted string containing search results with titles, URLs, and snippets.\n\n### 2. Content Fetching Tool\n\n```python\nasync def fetch_content(url: str) -> str\n```\n\nFetches and parses content from a webpage.\n\n**Parameters:**\n- `url`: The webpage URL to fetch content from\n\n**Returns:**\nCleaned and formatted text content from the webpage.\n\n## Features in Detail\n\n### Rate Limiting\n\n- Search: Limited to 30 requests per minute\n- Content Fetching: Limited to 20 requests per minute\n- Automatic queue management and wait times\n\n### Result Processing\n\n- Removes ads and irrelevant content\n- Cleans up Baidu redirect URLs\n- Formats results for optimal LLM consumption\n- Truncates long content appropriately\n\n### Error Handling\n\n- Comprehensive error catching and reporting\n- Detailed logging through MCP context\n- Graceful degradation on rate limits or timeouts\n\n## Contributing\n\nIssues and pull requests are welcome! Some areas for potential improvement:\n\n- Additional search parameters (region, language, etc.)\n- Enhanced content parsing options\n- Caching layer for frequently accessed content\n- Additional rate limiting strategies\n\n## License\n\nThis project is licensed under the MIT License.\n\n## Acknowledgments\n\nThe code in this project references the following repositories:\n\n- [nickclyde/duckduckgo-mcp-server](https://github.com/nickclyde/duckduckgo-mcp-server)\n- [BaiduSpider/BaiduSpider](https://github.com/BaiduSpider/BaiduSpider)\n\nThanks to the authors and contributors of these repositories for their efforts and contributions to the open-source community.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "baidu",
        "searches",
        "webpage",
        "baidu mcp",
        "web search",
        "search evilran"
      ],
      "category": "web-search"
    },
    "ExactDoug--mcp-fetch": {
      "owner": "ExactDoug",
      "name": "mcp-fetch",
      "url": "https://github.com/ExactDoug/mcp-fetch",
      "imageUrl": "/freedevtools/mcp/pfp/ExactDoug.webp",
      "description": "Fetches web content and converts it to markdown format for easy processing by LLMs. It retrieves content from specified URLs and allows for extraction in manageable chunks by specifying character indexes.",
      "stars": 2,
      "forks": 2,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-08-05T15:41:24Z",
      "readme_content": "# Fetch MCP Server\n\nA Model Context Protocol server that provides web content fetching capabilities. This server enables LLMs to retrieve and process content from web pages, converting HTML to markdown for easier consumption.\n\nThe fetch tool will truncate the response, but by using the `start_index` argument, you can specify where to start the content extraction. This lets models read a webpage in chunks, until they find the information they need.\n\n### Available Tools\n\n- `fetch` - Fetches a URL from the internet and extracts its contents as markdown.\n    - `url` (string, required): URL to fetch\n    - `max_length` (integer, optional): Maximum number of characters to return (default: 5000)\n    - `start_index` (integer, optional): Start content from this character index (default: 0)\n    - `raw` (boolean, optional): Get raw content without markdown conversion (default: false)\n\n### Prompts\n\n- **fetch**\n    - Fetch a URL and extract its contents as markdown\n    - Arguments:\n        - `url` (string, required): URL to fetch\n\n## Installation\n\nOptionally: Install node.js, this will cause the fetch server to use a different HTML simplifier that is more robust.\n\n### Using uv (recommended)\n\nWhen using [`uv`](https://docs.astral.sh/uv/) no specific installation is needed. We will\nuse [`uvx`](https://docs.astral.sh/uv/guides/tools/) to directly run `mcp-server-fetch`.\n\n### Using PIP\n\nAlternatively you can install `mcp-server-fetch` via pip:\n\n```\npip install mcp-server-fetch\n",
      "npm_url": "https://www.npmjs.com/package/mcp-fetch",
      "npm_downloads": 1627,
      "keywords": [
        "fetches",
        "mcp",
        "fetch",
        "mcp fetch",
        "search exactdoug",
        "exactdoug mcp"
      ],
      "category": "web-search"
    },
    "FradSer--mcp-server-local-web-search": {
      "owner": "FradSer",
      "name": "mcp-server-local-web-search",
      "url": "https://github.com/FradSer/mcp-server-local-web-search",
      "imageUrl": "/freedevtools/mcp/pfp/FradSer.webp",
      "description": "Perform local web searches and extract relevant content from web pages, returning structured results including titles, URLs, and descriptions. The server supports customizable result limits, content truncation, domain filtering, and utilizes headless browser operations for enhanced performance.",
      "stars": 9,
      "forks": 0,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-09-06T20:59:41Z",
      "readme_content": "# mcp-server-local-web-search\n\n![A_FRAD_PRODUCT_WIP_yellow](https://img.shields.io/badge/A%20FRAD%20PRODUCT-WIP-yellow) [![Twitter Follow](https://img.shields.io/twitter/follow/FradSer?style=social)](https://twitter.com/FradSer)\n\nAn MCP server for performing local web searches. This server provides tools to search and extract content from web pages through the Model Context Protocol.\n\n## Features\n\n- Perform web searches with customizable result limits\n- Extract and process content from web pages\n- Return structured results with titles, URLs, and descriptions\n- Support for content truncation and domain filtering\n- Clean content extraction using Readability\n- Headless browser operation for improved performance\n\n## Installation\n\nTo install dependencies:\n\n```bash\nbun install\n```\n\n## Setup\n\nRun the setup script to configure the MCP server:\n\n```bash\nbun run setup.ts\n```\n\nThis will add the server to your Claude MCP configuration.\n\n### Available Tools\n\n1. `local_web_search`\n   - Performs web search and returns results with title, URL and description\n   - Parameters:\n     - `query`: Search query to find relevant content (required)\n     - `excludeDomains`: List of domains to exclude from search results (default: [])\n     - `limit`: Maximum number of results to return (default: 5)\n     - `truncate`: Maximum length of content to return per result (default: 4000)\n     - `show`: Show browser window for debugging (default: false)\n     - `proxy`: Proxy server to use for requests (optional)\n\n## Requirements\n\n- [Bun](https://bun.sh) runtime\n- Node.js TypeScript support\n\n## Development\n\nThis project uses:\n\n- [Bun](https://bun.sh) as the JavaScript runtime\n- [TypeScript](https://www.typescriptlang.org/) for type safety\n- [Model Context Protocol SDK](https://github.com/modelcontextprotocol/sdk) for server implementation\n- [@egoist/local-web-search](https://github.com/egoist/local-web-search/) for web search (using playwright-core)\n- [Readability](https://github.com/mozilla/readability) for content extraction\n\n## Contributors\n- [egoist](https://github.com/egoist) - Original local web search author\n- [FradSer](https://github.com/FradSer) - Original author\n- [TheSethRose](https://github.com/TheSethRose) - Playwright integration and performance improvements\n\n## License\n\nMIT License\n\nThis project was created using `bun init` in bun v1.2.2. [Bun](https://bun.sh) is a fast all-in-one JavaScript runtime.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "searches",
        "search",
        "web",
        "search fradser",
        "web search",
        "web searches"
      ],
      "category": "web-search"
    },
    "GridfireAI--reddit-mcp": {
      "owner": "GridfireAI",
      "name": "reddit-mcp",
      "url": "https://github.com/GridfireAI/reddit-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/GridfireAI.webp",
      "description": "Browse, search, and read Reddit content and comments through a simple interface, enabling integration of Reddit data into applications or workflows. Currently supports only read features with built-in rate limiting using the PRAW library.",
      "stars": 16,
      "forks": 7,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-08-31T12:55:58Z",
      "readme_content": "# Reddit MCP\n\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n\nA plug-and-play [MCP](https://modelcontextprotocol.io) server to browse, search, and read Reddit.\n\n## Demo\nHere's a short video showing how to use this in Claude Desktop:\n\nhttps://github.com/user-attachments/assets/a2e9f2dd-a9ac-453f-acd9-1791380ebdad\n\n## Features\n\n- Detailed parameter validation with [pydantic](https://docs.pydantic.dev)\n- Uses the reliable [PRAW](https://praw.readthedocs.io/) library under the hood\n- Built-in rate limiting protection thanks to PRAW\n\n## Caveats\n- Only supports read features for now. If you want to use write features, upvote the [issue](https://github.com/GridfireAI/reddit-mcp/issues/1) or [send a PR](CONTRIBUTING.md)! 🙌\n- Tools use tokens. To use this with Claude, you may need to be a Pro user to use many tool calls. Free tier users should be fine with lighter tool usage. Your token usage is your responsibility.\n\n## Installation\n\n### Prerequisite: Reddit API credentials\n\nCreate a [developer app](https://www.reddit.com/prefs/apps) in your Reddit account if you don't already have one. This will give you a `client_id` and `client_secret` to use in the following steps. If you already have these, you can skip this step.\n\n### Claude Desktop\n\nTo install into Claude Desktop:\n\n- Follow the instructions [here](https://modelcontextprotocol.io/quickstart/user) until the section \"Open up the configuration file in any text editor.\"\n- Add the following to the file depending on your preferred installation method:\n\n### Using [uvx](https://docs.astral.sh/uv/guides/tools/) (recommended)\n\n```json\n\"mcpServers\": {\n  \"reddit\": {\n    \"command\": \"uvx\",\n    \"args\": [\"reddit-mcp\"],\n    \"env\": {\n      \"REDDIT_CLIENT_ID\": \"<client_id>\",\n      \"REDDIT_CLIENT_SECRET\": \"<client_secret>\"\n    }\n  }\n}\n```\n\n### Using PIP\n\nFirst install the package:\n\n```bash\npip install reddit-mcp\n```\n\nThen add the following to the configuration file:\n\n```json\n\"mcpServers\": {\n  \"reddit\": {\n    \"command\": \"python\",\n    \"args\": [\"-m\", \"reddit_mcp\"],\n    \"env\": {\n      \"REDDIT_CLIENT_ID\": \"<client_id>\",\n      \"REDDIT_CLIENT_SECRET\": \"<client_secret>\"\n    }\n  }\n}\n```\n\n### Others\n\nYou can use this server with any [MCP client](https://modelcontextprotocol.io/docs/clients), including agent frameworks (LangChain, LlamaIndex, AutoGen, etc). For an example AutoGen integration, check out the [example](examples/autogen).\n\n## Tools\n\nThe tools the server will expose are:\n\n| Name                         | Description                              |\n| ---------------------------- | ---------------------------------------- |\n| `get_comment`                | Access a comment                         |\n| `get_comments_by_submission` | Access comments of a submission          |\n| `get_submission`             | Access a submission                      |\n| `get_subreddit`              | Access a subreddit by name               |\n| `search_posts`               | Search posts in a subreddit              |\n| `search_subreddits`          | Search subreddits by name or description |\n\n## Contributing\n\nContributions are welcome! See [CONTRIBUTING.md](CONTRIBUTING.md) for more information.\n\n## Acknowledgments\n\n- [PRAW](https://praw.readthedocs.io/) for an amazingly reliable library 💙\n",
      "npm_url": "https://www.npmjs.com/package/reddit-mcp",
      "npm_downloads": 916,
      "keywords": [
        "gridfireai",
        "reddit",
        "browse",
        "search gridfireai",
        "gridfireai reddit",
        "reddit data"
      ],
      "category": "web-search"
    },
    "Hajime-Y--deep-research-mcp": {
      "owner": "Hajime-Y",
      "name": "deep-research-mcp",
      "url": "https://github.com/Hajime-Y/deep-research-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/Hajime-Y.webp",
      "description": "Provides advanced web search capabilities, document analysis, and image processing. Extracts information from various sources including PDFs and YouTube transcripts efficiently.",
      "stars": 12,
      "forks": 1,
      "license": "Apache License 2.0",
      "language": "Python",
      "updated_at": "2025-09-06T18:57:01Z",
      "readme_content": "# Deep Research MCP Server\n\nDeep Research is an agent-based tool that provides web search and advanced research capabilities. It leverages HuggingFace's `smolagents` and is implemented as an MCP server.\n\nThis project is based on [HuggingFace's open_deep_research example](https://github.com/huggingface/smolagents/tree/main/examples/open_deep_research).\n\n## Features\n\n- Web search and information gathering\n- PDF and document analysis\n- Image analysis and description\n- YouTube transcript retrieval\n- Archive site search\n\n## Requirements\n\n- Python 3.11 or higher\n- `uv` package manager\n- The following API keys:\n  - OpenAI API key\n  - HuggingFace token\n  - SerpAPI key\n\n## Installation\n\n1. Clone the repository:\n\n```bash\ngit clone https://github.com/Hajime-Y/deep-research-mcp.git\ncd deep-research-mcp\n```\n\n2. Create a virtual environment and install dependencies:\n\n```bash\nuv venv\nsource .venv/bin/activate # For Linux or Mac\n# .venv\\Scripts\\activate # For Windows\nuv sync\n```\n\n## Environment Variables\n\nCreate a `.env` file in the root directory of the project and set the following environment variables:\n\n```\nOPENAI_API_KEY=your_openai_api_key\nHF_TOKEN=your_huggingface_token\nSERPER_API_KEY=your_serper_api_key\n```\n\nYou can obtain a SERPER_API_KEY by signing up at [Serper.dev](https://serper.dev/signup).\n\n## Usage\n\nStart the MCP server:\n\n```bash\nuv run deep_research.py\n```\n\nThis will launch the `deep_research` agent as an MCP server.\n\n## Docker Usage\n\nYou can also run this MCP server in a Docker container:\n\n```bash\n# Build the Docker image\ndocker build -t deep-research-mcp .\n\n# Run with required API keys\ndocker run -p 8080:8080 \\\n  -e OPENAI_API_KEY=your_openai_api_key \\\n  -e HF_TOKEN=your_huggingface_token \\\n  -e SERPER_API_KEY=your_serper_api_key \\\n  deep-research-mcp\n```\n\n### Registering with MCP Clients\n\nTo register this Docker container as an MCP server in different clients:\n\n#### Claude Desktop\n\nAdd the following to your Claude Desktop configuration file (typically located at `~/.config/Claude/claude_desktop_config.json` on Linux, `~/Library/Application Support/Claude/claude_desktop_config.json` on macOS, or `%APPDATA%\\Claude\\claude_desktop_config.json` on Windows):\n\n```json\n{\n  \"mcpServers\": {\n    \"deep-research-mcp\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\", \n        \"-i\", \n        \"--rm\", \n        \"-e\", \"OPENAI_API_KEY=your_openai_api_key\",\n        \"-e\", \"HF_TOKEN=your_huggingface_token\", \n        \"-e\", \"SERPER_API_KEY=your_serper_api_key\",\n        \"deep-research-mcp\"\n      ]\n    }\n  }\n}\n```\n\n#### Cursor IDE\n\nFor Cursor IDE, add the following configuration:\n\n```json\n{\n  \"mcpServers\": {\n    \"deep-research-mcp\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\", \n        \"-i\", \n        \"--rm\", \n        \"-e\", \"OPENAI_API_KEY=your_openai_api_key\",\n        \"-e\", \"HF_TOKEN=your_huggingface_token\", \n        \"-e\", \"SERPER_API_KEY=your_serper_api_key\",\n        \"deep-research-mcp\"\n      ]\n    }\n  }\n}\n```\n\n#### Using with Remote MCP Server\n\nIf you're running the MCP server on a remote machine or exposing it as a service, you can use the URL-based configuration:\n\n```json\n{\n  \"mcpServers\": {\n    \"deep-research-mcp\": {\n      \"url\": \"http://your-server-address:8080/mcp\",\n      \"type\": \"sse\"\n    }\n  }\n}\n```\n\n## Key Components\n\n- `deep_research.py`: Entry point for the MCP server\n- `create_agent.py`: Agent creation and configuration\n- `scripts/`: Various tools and utilities\n  - `text_web_browser.py`: Text-based web browser\n  - `text_inspector_tool.py`: File inspection tool\n  - `visual_qa.py`: Image analysis tool\n  - `mdconvert.py`: Converts various file formats to Markdown\n\n## License\n\nThis project is provided under the Apache License 2.0.\n\n## Acknowledgements\n\nThis project uses code from HuggingFace's `smolagents` and Microsoft's `autogen` projects.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "search",
        "hajime",
        "mcp",
        "search hajime",
        "web search",
        "research mcp"
      ],
      "category": "web-search"
    },
    "Haonter--MCP-Servers": {
      "owner": "Haonter",
      "name": "MCP-Servers",
      "url": "https://github.com/Haonter/MCP-Servers",
      "imageUrl": "/freedevtools/mcp/pfp/null.webp",
      "description": "Access detailed information about One Piece characters and geolocate public IPv4 addresses. This server facilitates the seamless integration of anime character data and IP geolocation features.",
      "stars": 2,
      "forks": 2,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-05-07T23:58:31Z",
      "readme_content": "# MCP-Server (OnePieceServer & Geolocalizar)\n\nEste proyecto contiene dos servidores MCP desarrollados con [`@modelcontextprotocol/sdk`](https://www.npmjs.com/package/@modelcontextprotocol/sdk). Cada uno expone una herramienta útil que puede ser integrada por un cliente AI compatible con MCP.\n\n<a href=\"https://glama.ai/mcp/servers/@Haonter/MCP-Servers\">\n  \n</a>\n\n\n[![smithery badge](https://smithery.ai/badge/@Haonter/MCP-Servers)](https://smithery.ai/server/@Haonter/MCP-Servers)\n\n---\n\n## 📁 Contenido\n\n- onePiece.ts: Servidor MCP para consultar personajes de One Piece\n- geolocalizar.ts: Servidor MCP para geolocalizar direcciones IP públicas\n\n---\n\n## ⚙️ Requisitos\n\n- Node.js ≥ 18\n- npm ≥ 9\n\n### 📦 Instalar dependencias\n\n```bash\nnpm install @modelcontextprotocol/sdk axios zod\nnpm install -D tsx\n```\n\n---\n\n## 🏴‍☠️ OnePieceServer MCP\n\n### 📄 Descripción\n\n`OnePieceServer` permite consultar información de personajes del anime/manga One Piece, ya sea individualmente o toda la lista disponible.\n\n### 🚀 Ejecutar\n\n```bash\nnpx tsx onePiece.ts\n```\n\nTambién puedes usar el Inspector MCP:\n\n```bash\nnpx -y @modelcontextprotocol/inspector npx -y tsx onePiece.ts\n```\n\n### 🛠 Herramienta expuesta: `one_piece`\n\n- **Input**:\n  ```json\n  { \"id\": \"1\" }\n  ```\n\n  Para obtener la lista completa:\n  ```json\n  { \"id\": \"todos\" }\n  ```\n\n- **Output**:\n  ```json\n  {\n    \"content\": [\n      {\n        \"type\": \"text\",\n        \"text\": \"Información del personaje: { ... }\"\n      }\n    ]\n  }\n  ```\n\n### 🌐 API usada\n\n- `https://onepieceapi-50cm.onrender.com/personaje/{id}`\n- `https://onepieceapi-50cm.onrender.com/personajes`\n\n---\n\n## 🌍 Geolocalizar MCP\n\n### 📄 Descripción\n\n`Geolocalizar` permite obtener información geográfica aproximada de una dirección IP (IPv4) pública.\n\n### 🚀 Ejecutar\n\n```bash\nnpx tsx main.ts\n```\n\nO usar con el Inspector MCP:\n\n```bash\nnpx -y @modelcontextprotocol/inspector npx -y tsx main.ts\n```\n\n### 🛠 Herramienta expuesta: `geolocalizar`\n\n- **Input**:\n  ```json\n  { \"ip\": \"8.8.8.8\" }\n  ```\n\n- **Output**:\n  ```json\n  {\n    \"content\": [\n      {\n        \"type\": \"text\",\n        \"text\": \"{ \\\"ip\\\": \\\"8.8.8.8\\\", \\\"country\\\": \\\"United States\\\", ... }\"\n      }\n    ]\n  }\n  ```\n\n### 🔐 Validación\n\nUsa `zod` para validar que el input sea una IP válida en formato IPv4.\n\n---\n\n## 📁 Estructura del Proyecto\n\n```\n.\n├── geolocalizar.ts   # MCP Geolocalizar\n├── onePiece.ts       # MCP OnePieceServer\n├── package.json      # Archivo de configuracion de NPM\n└── README.md         # Este archivo\n```\n\n---\n\n## 🧪 Desarrollo\n\nAmbos servidores se comunican mediante `stdin` y `stdout` usando `StdioServerTransport`, lo que permite su ejecución fácil desde CLI o integración con clientes MCP.\n\n---\n\n## 🧑‍💻 Autor\n\nDesarrollado por **Diego Rodríguez**  \n✉️ contacto@diegorodriguez.dev\n\n---\n\n### Installing via Smithery\n\nTo install OnePiece & Geolocalizar MCP Servers for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@Haonter/MCP-Servers):\n\n```bash\nnpx -y @smithery/cli install @Haonter/MCP-Servers --client claude\n```",
      "npm_url": "https://www.npmjs.com/package/mcp-servers",
      "npm_downloads": 854,
      "keywords": [
        "geolocate",
        "geolocation",
        "ip",
        "search haonter",
        "mcp servers",
        "ip geolocation"
      ],
      "category": "web-search"
    },
    "Hawstein--mcp-server-reddit": {
      "owner": "Hawstein",
      "name": "mcp-server-reddit",
      "url": "https://github.com/Hawstein/mcp-server-reddit",
      "imageUrl": "/freedevtools/mcp/pfp/Hawstein.webp",
      "description": "Enables interaction with Reddit's public API to fetch and browse content such as frontpage posts, subreddit information, and post comments.",
      "stars": 106,
      "forks": 18,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-28T06:25:19Z",
      "readme_content": "# MCP Server Reddit\n[![smithery badge](https://smithery.ai/badge/@Hawstein/mcp-server-reddit)](https://smithery.ai/server/@Hawstein/mcp-server-reddit)\n\nA Model Context Protocol server providing access to Reddit public API for LLMs. This server enables LLMs to interact with Reddit's content, including browsing frontpage posts, accessing subreddit information, and reading post comments.\n\nThis server uses [redditwarp](https://github.com/Pyprohly/redditwarp) to interact with Reddit's public API and exposes the functionality through MCP protocol.\n\n<a href=\"https://glama.ai/mcp/servers/4032xr14pu\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/4032xr14pu/badge\" alt=\"Server Reddit MCP server\" /></a>\n\n## Video Demo (Click to Watch)\n\nA demo in Clinde 👇\n\n[![MCP Server Reddit - Clinde](https://img.youtube.com/vi/1Gdx1jWFbCM/maxresdefault.jpg)](https://youtu.be/1Gdx1jWFbCM)\n\n\n## Available Tools\n\n- `get_frontpage_posts` - Get hot posts from Reddit frontpage\n  - Optional arguments:\n    - `limit` (integer): Number of posts to return (default: 10, range: 1-100)\n\n- `get_subreddit_info` - Get information about a subreddit\n  - Required arguments:\n    - `subreddit_name` (string): Name of the subreddit (e.g. 'Python', 'news')\n\n- `get_subreddit_hot_posts` - Get hot posts from a specific subreddit\n  - Required arguments:\n    - `subreddit_name` (string): Name of the subreddit (e.g. 'Python', 'news')\n  - Optional arguments:\n    - `limit` (integer): Number of posts to return (default: 10, range: 1-100)\n\n- `get_subreddit_new_posts` - Get new posts from a specific subreddit\n  - Required arguments:\n    - `subreddit_name` (string): Name of the subreddit (e.g. 'Python', 'news')\n  - Optional arguments:\n    - `limit` (integer): Number of posts to return (default: 10, range: 1-100)\n\n- `get_subreddit_top_posts` - Get top posts from a specific subreddit\n  - Required arguments:\n    - `subreddit_name` (string): Name of the subreddit (e.g. 'Python', 'news')\n  - Optional arguments:\n    - `limit` (integer): Number of posts to return (default: 10, range: 1-100)\n    - `time` (string): Time filter for top posts (default: '', options: 'hour', 'day', 'week', 'month', 'year', 'all')\n\n- `get_subreddit_rising_posts` - Get rising posts from a specific subreddit\n  - Required arguments:\n    - `subreddit_name` (string): Name of the subreddit (e.g. 'Python', 'news')\n  - Optional arguments:\n    - `limit` (integer): Number of posts to return (default: 10, range: 1-100)\n\n- `get_post_content` - Get detailed content of a specific post\n  - Required arguments:\n    - `post_id` (string): ID of the post\n  - Optional arguments:\n    - `comment_limit` (integer): Number of top-level comments to return (default: 10, range: 1-100)\n    - `comment_depth` (integer): Maximum depth of comment tree (default: 3, range: 1-10)\n\n- `get_post_comments` - Get comments from a post\n  - Required arguments:\n    - `post_id` (string): ID of the post\n  - Optional arguments:\n    - `limit` (integer): Number of comments to return (default: 10, range: 1-100)\n\n\n## Installation\n\n### Using [Clinde](https://clinde.ai/) (recommended)\n\nThe easiest way to use MCP Server Reddit is through the Clinde desktop app. Simply download and install Clinde, then:\n\n1. Open the Clinde app\n2. Navigate to the Servers page\n3. Find mcp-server-reddit and click Install\n\nThat's it! No technical knowledge required - Clinde handles all the installation and configuration for you seamlessly.\n\n### Using uv (recommended)\n\nWhen using [`uv`](https://docs.astral.sh/uv/) no specific installation is needed. We will\nuse [`uvx`](https://docs.astral.sh/uv/guides/tools/) to directly run *mcp-server-reddit*.\n\n### Using PIP\n\nAlternatively you can install `mcp-server-reddit` via pip:\n\n```bash\npip install mcp-server-reddit\n```\n\nAfter installation, you can run it as a script using:\n\n```bash\npython -m mcp_server_reddit\n```\n\n### Installing via Smithery\n\nTo install MCP Server Reddit for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@Hawstein/mcp-server-reddit):\n\n```bash\nnpx -y @smithery/cli install @Hawstein/mcp-server-reddit --client claude\n```\n\n## Configuration\n\n### Configure for Claude.app\n\nAdd to your Claude settings:\n\n<details>\n<summary>Using uvx</summary>\n\n```json\n\"mcpServers\": {\n  \"reddit\": {\n    \"command\": \"uvx\",\n    \"args\": [\"mcp-server-reddit\"]\n  }\n}\n```\n</details>\n\n<details>\n<summary>Using pip installation</summary>\n\n```json\n\"mcpServers\": {\n  \"reddit\": {\n    \"command\": \"python\",\n    \"args\": [\"-m\", \"mcp_server_reddit\"]\n  }\n}\n```\n</details>\n\n### Configure for Zed\n\nAdd to your Zed settings.json:\n\n<details>\n<summary>Using uvx</summary>\n\n```json\n\"context_servers\": [\n  \"mcp-server-reddit\": {\n    \"command\": \"uvx\",\n    \"args\": [\"mcp-server-reddit\"]\n  }\n],\n```\n</details>\n\n<details>\n<summary>Using pip installation</summary>\n\n```json\n\"context_servers\": {\n  \"mcp-server-reddit\": {\n    \"command\": \"python\",\n    \"args\": [\"-m\", \"mcp_server_reddit\"]\n  }\n},\n```\n</details>\n\n## Examples of Questions\n\n- \"What are the current hot posts on Reddit's frontpage?\" (get_frontpage_posts)\n- \"Tell me about the r/ClaudeAI subreddit\" (get_subreddit_info)\n- \"What are the hot posts in the r/ClaudeAI subreddit?\" (get_subreddit_hot_posts)\n- \"Show me the newest posts from r/ClaudeAI\" (get_subreddit_new_posts)\n- \"What are the top posts of all time in r/ClaudeAI?\" (get_subreddit_top_posts)\n- \"What posts are trending in r/ClaudeAI right now?\" (get_subreddit_rising_posts)\n- \"Get the full content and comments of this Reddit post: [post_url]\" (get_post_content)\n- \"Summarize the comments on this Reddit post: [post_url]\" (get_post_comments)\n\n## Debugging\n\nYou can use the MCP inspector to debug the server. For uvx installations:\n\n```bash\nnpx @modelcontextprotocol/inspector uvx mcp-server-reddit\n```\n\nOr if you've installed the package in a specific directory or are developing on it:\n\n```bash\ncd path/to/mcp_server_reddit\nnpx @modelcontextprotocol/inspector uv run mcp-server-reddit\n```\n\n## License\n\nmcp-server-reddit is licensed under the MIT License. This means you are free to use, modify, and distribute the software, subject to the terms and conditions of the MIT License. For more details, please see the LICENSE file in the project repository.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "frontpage",
        "reddit",
        "hawstein",
        "server reddit",
        "search hawstein",
        "browse content"
      ],
      "category": "web-search"
    },
    "IA-Programming--Youtube-MCP": {
      "owner": "IA-Programming",
      "name": "Youtube-MCP",
      "url": "https://github.com/IA-Programming/Youtube-MCP",
      "imageUrl": "/freedevtools/mcp/pfp/IA-Programming.webp",
      "description": "Search for YouTube videos, retrieve their transcripts, and conduct semantic searches over video content to extract insights.",
      "stars": 6,
      "forks": 1,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-08-07T05:33:22Z",
      "readme_content": "# YouTube MCP Server\n\nA Model Context Protocol (MCP) server that provides tools for searching YouTube videos, retrieving transcripts, and performing semantic search over video content.\n\n## Support Us\n\nIf you find this project helpful and would like to support future projects, consider buying us a coffee! Your support helps us continue building innovative AI solutions.\n\n<a href=\"https://www.buymeacoffee.com/blazzmocompany\"><img alt=\"text_Buy_me_a_coffee_emoji_slug_blazzmocompany_button_colour_40DCA5_font_colour_ffffff_font_family_Cookie_outline_colour_000000_coffee_colour_FFDD00\" src=\"https://img.buymeacoffee.com/button-api/?text=Buy me a coffee&emoji=&slug=blazzmocompany&button_colour=40DCA5&font_colour=ffffff&font_family=Cookie&outline_colour=000000&coffee_colour=FFDD00\"></a>\n\nYour contributions go a long way in fueling our passion for creating intelligent and user-friendly applications.\n\n## Table of Contents\n\n- [YouTube MCP Server](#youtube-mcp-server)\n- [Features](#features)\n- [Prerequisites](#prerequisites)\n- [Installation](#installation)\n- [Running the Server](#running-the-server)\n  - [1. Direct Method](#1-direct-method)\n  - [2. Configure for Claude.app](#2-configure-for-claudeapp)\n- [Available Tools](#available-tools)\n- [Using with MCP Clients](#using-with-mcp-clients)\n  - [Example Usage](#example-usage)\n- [Debugging](#debugging)\n- [Contributing](#contributing)\n- [License](#license)\n\n## Features\n\n- Search YouTube videos without using the official API\n- Retrieve video transcripts\n- Store video information and transcripts in a vector database\n- Perform semantic search over stored video transcripts\n\n## Prerequisites\n\n- Python 3.8+\n- Google API key for embeddings\n- uv package manager\n\n## Installation\n\n1. Clone this repository\n\n2. Create and activate a virtual environment using uv:\n```bash\nuv venv\n# On Windows:\n.venv\\Scripts\\activate\n# On Unix/MacOS:\nsource .venv/bin/activate\n```\n\n3. Install dependencies using uv:\n```bash\nuv pip install -r requirements.txt\n```\n\n4. Create a `.env` file with your Google API key:\n```\nGOOGLE_API_KEY=your_api_key_here\n```\n\n## Running the Server\n\nThere are two ways to run the MCP server:\n\n### 1. Direct Method\n\nTo start the MCP server directly:\n\n```bash\nuv run python server.py\n```\n\n### 2. Configure for Claude.app\n\nAdd to your Claude settings without using any package manager this works for windows:\n```json\n\"mcpServers\": {\n  \"youtube\": {\n    \"command\": \"C:\\\\Path\\\\To\\\\Your\\\\Project\\\\.venv\\\\Scripts\\\\python.exe\",\n    \"args\": [\"C:\\\\Path\\\\To\\\\Your\\\\Project\\\\server.py\"],\n    \"env\": {\n      \"GOOGLE_API_KEY\": \"your_api_key_here\"\n    }\n  }\n}\n```\n\nUsing Uv package manager this works for windows:\n\n```json\n\"mcpServers\": {\n  \"youtube\": {\n    \"command\": \"uv\",\n    \"args\": [\"--directory\", \"C:\\\\Path\\\\To\\\\Your\\\\Project\", \"run\", \"server.py\"],\n    \"env\": {\n      \"GOOGLE_API_KEY\": \"your_api_key_here\"\n    }\n  }\n}\n```\n\n## Available Tools\n\nThe server provides the following tools:\n\n1. `search-youtube`: Search for YouTube videos based on a query\n   - Parameters:\n     - query: Search query string\n     - max_results: Maximum number of results to return (default: 5)\n\n2. `get-transcript`: Get the transcript of a YouTube video\n   - Parameters:\n     - video_url: URL of the YouTube video\n\n3. `store-video-info`: Store video information and transcript in the vector database\n   - Parameters:\n     - video_url: URL of the YouTube video\n     - metadata: Optional metadata about the video\n\n4. `search-transcripts`: Search stored video transcripts using semantic search\n   - Parameters:\n     - query: Search query\n     - limit: Maximum number of results to return (default: 3)\n\n## Using with MCP Clients\n\nThis server can be used with any MCP-compatible client, such as Claude Desktop App. The tools will be automatically discovered and made available to the client.\n\n### Example Usage\n\n1. Start the server using one of the methods described above\n2. Open Claude Desktop App\n3. Look for the hammer icon to verify that the YouTube tools are available\n4. You can now use commands like:\n   - \"Search for Python tutorial videos\"\n   - \"Get the transcript of this video: [video_url]\"\n   - \"Search through stored video transcripts about machine learning\"\n\n## Debugging\n\nIf you encounter any issues:\n\n1. Make sure your Google API key is correctly set in the `.env` file\n2. Check that all dependencies are installed correctly\n3. Verify that the server is running and listening for connections\n4. Look for any error messages in the server output\n\n## Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n\n## License\n\nThis project is licensed under the MIT License - see the [LICENSE](./LICENSE) file for details.",
      "npm_url": "https://www.npmjs.com/package/youtube-mcp",
      "npm_downloads": 590,
      "keywords": [
        "youtube",
        "searches",
        "search",
        "programming youtube",
        "search youtube",
        "searches video"
      ],
      "category": "web-search"
    },
    "IBK-Insurance--IBKI_LLM": {
      "owner": "IBK-Insurance",
      "name": "IBKI_LLM",
      "url": "https://github.com/IBK-Insurance/IBKI_LLM",
      "imageUrl": "/freedevtools/mcp/pfp/IBK-Insurance.webp",
      "description": "Provides a web interface for real-time interaction with LLM models using Ollama, enabling users to chat and engage with various LLM capabilities through a responsive design.",
      "stars": 0,
      "forks": 0,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-09-30T09:10:38Z",
      "readme_content": "# IBKI 개인정보 가드레일 시스템\n\n이 프로젝트는 개인정보 보호를 위한 가드레일 시스템으로, 텍스트와 이미지에서 개인정보를 탐지하고 ChatGPT와 연동하여 분석을 제공합니다.\n\n## 🏗️ 프로젝트 구조\n\n```\nibki_sys_guardrail/\n├── client/                    # UI 레이아웃 및 사용자 인터랙션\n│   ├── __init__.py\n│   ├── ui_components.py       # 재사용 가능한 UI 컴포넌트\n│   ├── text_input_section.py  # 텍스트 입력 섹션\n│   ├── image_input_section.py # 이미지 입력 섹션\n│   └── chat_section.py        # ChatGPT 대화 섹션\n├── server/                    # 비즈니스 로직 및 API 처리\n│   ├── __init__.py\n│   ├── llm_service.py         # LLM 서비스 (Ollama)\n│   ├── openai_service.py      # OpenAI 서비스 (ChatGPT)\n│   ├── file_service.py        # 파일 처리 서비스\n│   └── system_service.py      # 시스템 상태 관리\n└── personal_info_app.py       # 메인 애플리케이션\n```\n\n## 📋 Prerequisites\n\n- Python 3.8 이상\n- Ollama가 설치되어 있어야 합니다 (https://ollama.ai/)\n- qwen2.5vl:7b 모델이 Ollama에 설치되어 있어야 합니다\n- OpenAI API 키 (선택사항, ChatGPT 기능 사용 시)\n\n## 🚀 설치 방법\n\n1. 저장소를 클론합니다:\n```bash\ngit clone [repository-url]\ncd IBKI_LLM\n```\n\n2. 가상환경을 생성하고 활성화합니다:\n```bash\npython -m venv venv\nsource venv/bin/activate  # Windows: venv\\Scripts\\activate\n```\n\n3. 필요한 Python 패키지를 설치합니다:\n```bash\npip install -r requirements.txt\n```\n\n4. Ollama를 설치하고 qwen2.5vl 모델을 다운로드합니다:\n```bash\n# Ollama 설치 후\nollama pull qwen2.5vl:7b\n```\n\n5. 환경설정 파일을 생성합니다:\n```bash\n# .env 파일 생성\necho \"OPENAI_API_KEY=your_openai_api_key_here\" > .env\n```\n\n## 🎯 실행 방법\n\n1. Ollama 서버가 실행 중인지 확인합니다:\n```bash\nollama serve\n```\n\n2. Streamlit 애플리케이션을 실행합니다:\n```bash\nstreamlit run ibki_sys_guardrail/personal_info_app.py\n```\n\n3. 웹 브라우저에서 http://localhost:8501 으로 접속합니다.\n\n## ✨ 주요 기능\n\n### 🔍 개인정보 탐지\n- **텍스트 분석**: 직접 입력한 텍스트에서 개인정보 탐지\n- **이미지 분석**: 업로드한 이미지에서 텍스트 추출 후 개인정보 탐지\n- **멀티모달 LLM**: qwen2.5vl 모델을 활용한 이미지 텍스트 추출\n\n### 💬 ChatGPT 연동\n- 개인정보가 포함되지 않은 경우 자동으로 ChatGPT 분석 제공\n- 독립적인 ChatGPT 대화 기능\n- 대화 히스토리 관리\n\n### 🎨 사용자 친화적 UI\n- 모듈화된 UI 컴포넌트\n- 실시간 시스템 상태 확인\n- 반응형 디자인\n\n## 🔧 개발 및 수정\n\n### UI 레이아웃 수정\nUI 관련 수정은 `client/` 디렉토리에서 진행하세요:\n\n- **전체 UI 컴포넌트**: `client/ui_components.py`\n- **텍스트 입력 섹션**: `client/text_input_section.py`\n- **이미지 입력 섹션**: `client/image_input_section.py`\n- **ChatGPT 대화 섹션**: `client/chat_section.py`\n\n### 비즈니스 로직 수정\n비즈니스 로직 수정은 `server/` 디렉토리에서 진행하세요:\n\n- **LLM 서비스**: `server/llm_service.py`\n- **OpenAI 서비스**: `server/openai_service.py`\n- **파일 처리**: `server/file_service.py`\n- **시스템 관리**: `server/system_service.py`\n\n## ⚠️ 주의사항\n\n- Ollama 서버가 실행 중이어야 합니다.\n- 이 도구는 참고용이며, 법적 판단의 근거로 사용하지 마세요.\n- 실제 개인정보 처리 시에는 관련 법규를 준수하세요.\n- OpenAI API 키는 선택사항이지만, ChatGPT 기능 사용 시 필요합니다.\n\n## 🛠️ 기술 스택\n\n- **Frontend**: Streamlit\n- **LLM**: Ollama (qwen2.5vl:7b)\n- **AI Service**: OpenAI GPT-4o\n- **Image Processing**: PIL (Pillow)\n- **File Processing**: PyPDF2, python-docx\n- **Environment**: python-dotenv ",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "ibki_llm",
        "ibk",
        "llm",
        "ibki_llm provides",
        "insurance ibki_llm",
        "search ibk"
      ],
      "category": "web-search"
    },
    "JackKuo666--Crossref-MCP-Server": {
      "owner": "JackKuo666",
      "name": "Crossref-MCP-Server",
      "url": "https://github.com/JackKuo666/Crossref-MCP-Server",
      "imageUrl": "/freedevtools/mcp/pfp/JackKuo666.webp",
      "description": "Connect to Crossref's database to search and access academic paper metadata, including metadata retrieval for specific papers, journals, and funding organizations using various queries.",
      "stars": 3,
      "forks": 1,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-07-28T16:51:46Z",
      "readme_content": "# Crossref MCP Server\n\n🔍 Enable AI assistants to search and access academic paper metadata through Crossref using a simple MCP interface.\n\nThe Crossref MCP Server provides a bridge between AI assistants and Crossref's database of academic literature through the Model Context Protocol (MCP). It allows AI models to search for scientific articles by DOI, title, or keywords, access their metadata, and retrieve journal and funder information in a programmatic way.\n\n## ✨ Core Features\n\n- 🔎 Work Search by Query: Find papers using keywords, titles, or authors ✅\n- 📊 Metadata Access: Retrieve detailed metadata for specific papers by DOI ✅\n- 📚 Journal Search: Find journals in the Crossref database ✅\n- 💰 Funder Search: Discover funding organizations and their supported research ✅\n\n## 🚀 Quick Start\n\n### Prerequisites\n\n- Python 3.10+\n- FastMCP library\n\n### Installation\n\n1. Clone the repository:\n   ```\n   git clone https://github.com/JackKuo666/Crossref-MCP-Server.git\n   cd Crossref-MCP-Server\n   ```\n\n2. Install the required dependencies:\n   ```\n   pip install -r requirements.txt\n   ```\n\n## 📊 Usage\n\nStart the MCP server:\n\n```bash\npython crossref_server.py\n```\n\n## Usage with Claude Desktop or Cline\n\nAdd this configuration to your `cline_mcp_settings.json` or `claude_desktop_config.json`:\n\n(Mac OS)\n\n```json\n{\n  \"mcpServers\": {\n    \"crossref\": {\n      \"command\": \"python\",\n      \"args\": [\"-m\", \"crossref_server.py\"]\n      }\n  }\n}\n```\n\n(Windows version):\n\n```json\n{\n  \"mcpServers\": {\n    \"crossref\": {\n      \"command\": \"C:\\\\Users\\\\YOUR\\\\PATH\\\\miniconda3\\\\envs\\\\mcp_server\\\\python.exe\",\n      \"args\": [\n        \"D:\\\\code\\\\YOUR\\\\PATH\\\\Crossref-MCP-Server\\\\crossref_server.py\"\n      ],\n      \"env\": {},\n      \"disabled\": false,\n      \"autoApprove\": []\n    }\n  }\n}\n```\n\n## 🛠 MCP Tools\n\nThe Crossref MCP Server provides the following tools:\n\n1. `search_works_by_query`: Search for scholarly works using keywords, titles, or authors.\n2. `get_work_metadata`: Get detailed metadata for a specific work using its DOI.\n3. `search_journals`: Search for journals in the Crossref database.\n4. `search_funders`: Search for funding organizations in the Crossref database.\n\n### Searching Works by Query\n\nYou can ask the AI assistant to search for papers using keywords:\n```\nCan you search Crossref for papers about \"machine learning in healthcare\"?\n```\n\n### Getting Work Metadata by DOI\n\nYou can get detailed metadata for a specific paper using its DOI:\n```\nCan you show me the metadata for the paper with DOI 10.1038/nature14539?\n```\n\n### Searching Journals\n\nYou can search for journals in the Crossref database:\n```\nCan you find journals related to \"artificial intelligence\" in Crossref?\n```\n\n### Searching Funders\n\nYou can search for funding organizations:\n```\nCan you find information about the \"National Science Foundation\" in Crossref?\n```\n\n\n## 📁 Project Structure\n\n- `crossref_server.py`: The main MCP server implementation using FastMCP\n- `crossref_search.py`: Contains the logic for searching Crossref and retrieving metadata\n\n## 🔧 Dependencies\n\n- Python 3.10+\n- FastMCP (mcp)\n- requests\n- bs4\n- habanero\n\n## 🤝 Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n\n## 📄 License\n\nThis project is licensed under the MIT License.\n\n## ⚠️ Note\n\nThis tool uses the Crossref API to access publicly available metadata about academic works. For better API access priority, it's recommended to provide your email address when initializing the CrossrefSearch class.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "crossref",
        "journals",
        "search",
        "metadata retrieval",
        "crossref database",
        "paper metadata"
      ],
      "category": "web-search"
    },
    "JackKuo666--Google-Scholar-MCP-Server": {
      "owner": "JackKuo666",
      "name": "Google-Scholar-MCP-Server",
      "url": "https://github.com/JackKuo666/Google-Scholar-MCP-Server",
      "imageUrl": "/freedevtools/mcp/pfp/JackKuo666.webp",
      "description": "Enables AI assistants to perform searches on Google Scholar to access academic papers, retrieve paper metadata, and gather detailed author information. Facilitates efficient academic research through a seamless programmatic interface.",
      "stars": 138,
      "forks": 25,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-10-03T08:00:22Z",
      "readme_content": "# Google Scholar MCP Server\n[![smithery badge](https://smithery.ai/badge/@JackKuo666/google-scholar-mcp-server)](https://smithery.ai/server/@JackKuo666/google-scholar-mcp-server)\n\n🔍 Enable AI assistants to search and access Google Scholar papers through a simple MCP interface.\n\nThe Google Scholar MCP Server provides a bridge between AI assistants and Google Scholar through the Model Context Protocol (MCP). It allows AI models to search for academic papers and access their content in a programmatic way.\n\n## ✨ Core Features\n- 🔎 Paper Search: Query Google Scholar papers with custom search strings or advanced search parameters ✅\n- 🚀 Efficient Retrieval: Fast access to paper metadata ✅\n- 👤 Author Information: Retrieve detailed information about authors ✅\n- 📊 Research Support: Facilitate academic research and analysis ✅\n\n## 🚀 Quick Start\n\n### Installing Manually\n### Installing via Smithery\n\nTo install google-scholar Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@JackKuo666/google-scholar-mcp-server):\n\n#### claude\n\n```sh\nnpx -y @smithery/cli@latest install @JackKuo666/google-scholar-mcp-server --client claude --config \"{}\"\n```\n\n#### Cursor\n\nPaste the following into Settings → Cursor Settings → MCP → Add new server: \n- Mac/Linux  \n```s\nnpx -y @smithery/cli@latest run @JackKuo666/google-scholar-mcp-server --client cursor --config \"{}\" \n```\n#### Windsurf\n```sh\nnpx -y @smithery/cli@latest install @JackKuo666/google-scholar-mcp-server --client windsurf --config \"{}\"\n```\n### CLine\n```sh\nnpx -y @smithery/cli@latest install @JackKuo666/google-scholar-mcp-server --client cline --config \"{}\"\n```\n\n1. Clone the repository:\n   ```\n   git clone https://github.com/JackKuo666/google-scholar-MCP-Server.git\n   cd google-scholar-MCP-Server\n   ```\n\n2. Install the required dependencies:\n   ```\n   pip install -r requirements.txt\n   ```\n\n\nFor development:\n\n```bash\n# Clone and set up development environment\ngit clone https://github.com/JackKuo666/Google-Scholar-MCP-Server.git\ncd Google-Scholar-MCP-Server\n\n# Create and activate virtual environment\npython -m venv venv\nsource venv/bin/activate  # On Windows use `venv\\Scripts\\activate`\n\n# Install dependencies\npip install -r requirements.txt\n```\n\n## 📊 Usage\n\nStart the MCP server:\n\n```bash\npython google_scholar_server.py\n```\n\nOnce the server is running, you can use the provided MCP tools in your AI assistant or application. Here are some examples of how to use the tools:\n\n### Example 1: Search for papers using keywords\n\n```python\nresult = await mcp.use_tool(\"search_google_scholar_key_words\", {\n    \"query\": \"artificial intelligence ethics\",\n    \"num_results\": 5\n})\nprint(result)\n```\n\n### Example 2: Perform an advanced search\n\n```python\nresult = await mcp.use_tool(\"search_google_scholar_advanced\", {\n    \"query\": \"machine learning\",\n    \"author\": \"Hinton\",\n    \"year_range\": [2020, 2023],\n    \"num_results\": 3\n})\nprint(result)\n```\n\n### Example 3: Get author information\n\n```python\nresult = await mcp.use_tool(\"get_author_info\", {\n    \"author_name\": \"Geoffrey Hinton\"\n})\nprint(result)\n```\n\nThese examples demonstrate how to use the three main tools provided by the Google Scholar MCP Server. Adjust the parameters as needed for your specific use case.\n\n## Usage with Claude Desktop\n\nAdd this configuration to your `claude_desktop_config.json`:\n\n(Mac OS)\n\n```json\n{\n  \"mcpServers\": {\n    \"google-scholar\": {\n      \"command\": \"python\",\n      \"args\": [\"-m\", \"google_scholar_mcp_server\"]\n      }\n  }\n}\n```\n\n(Windows version):\n\n```json\n{\n  \"mcpServers\": {\n    \"google-scholar\": {\n      \"command\": \"C:\\\\Users\\\\YOUR\\\\PATH\\\\miniconda3\\\\envs\\\\mcp_server\\\\python.exe\",\n      \"args\": [\n        \"D:\\\\code\\\\YOUR\\\\PATH\\\\Google-Scholar-MCP-Server\\\\google_scholar_server.py\"\n      ],\n      \"env\": {},\n      \"disabled\": false,\n      \"autoApprove\": []\n    }\n  }\n}\n```\nUsing with Cline\n```json\n{\n  \"mcpServers\": {\n    \"google-scholar\": {\n      \"command\": \"bash\",\n      \"args\": [\n        \"-c\",\n        \"source /home/YOUR/PATH/.venv/bin/activate && python /home/YOUR/PATH/google_scholar_mcp_server.py\"\n      ],\n      \"env\": {},\n      \"disabled\": false,\n      \"autoApprove\": []\n    }\n  }\n}\n```\n\n\n## 🛠 MCP Tools\n\nThe Google Scholar MCP Server provides the following tools:\n\n### search_google_scholar_key_words\n\nSearch for articles on Google Scholar using key words.\n\n**Parameters:**\n- `query` (str): Search query string\n- `num_results` (int, optional): Number of results to return (default: 5)\n\n**Returns:** List of dictionaries containing article information\n\n### search_google_scholar_advanced\n\nPerform an advanced search for articles on Google Scholar.\n\n**Parameters:**\n- `query` (str): General search query\n- `author` (str, optional): Author name\n- `year_range` (tuple, optional): Tuple containing (start_year, end_year)\n- `num_results` (int, optional): Number of results to return (default: 5)\n\n**Returns:** List of dictionaries containing article information\n\n### get_author_info\n\nGet detailed information about an author from Google Scholar.\n\n**Parameters:**\n- `author_name` (str): Name of the author to search for\n\n**Returns:** Dictionary containing author information\n\n## 📁 Project Structure\n\n- `google_scholar_server.py`: The main MCP server implementation using FastMCP\n- `google_scholar_web_search.py`: Contains the web scraping logic for searching Google Scholar\n\n## 🔧 Dependencies\n\n- Python 3.10+\n- mcp[cli]>=1.4.1\n- scholarly>=1.7.0\n- asyncio>=3.4.3\n\nYou can install the required dependencies using:\n\n```bash\npip install -r requirements.txt\n```\n\n## 🤝 Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n\n## 📄 License\n\nThis project is licensed under the MIT License.\n\n## ⚠️ Disclaimer\n\nThis tool is for research purposes only. Please respect Google Scholar's terms of service and use this tool responsibly.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "scholar",
        "searches",
        "google",
        "google scholar",
        "scholar access",
        "scholar mcp"
      ],
      "category": "web-search"
    },
    "JackKuo666--PubMed-MCP-Server": {
      "owner": "JackKuo666",
      "name": "PubMed-MCP-Server",
      "url": "https://github.com/JackKuo666/PubMed-MCP-Server",
      "imageUrl": "/freedevtools/mcp/pfp/JackKuo666.webp",
      "description": "Connects AI models to PubMed for searching, accessing, and analyzing biomedical articles, allowing retrieval of detailed metadata and deep analysis of research papers. Facilitates access to full-text PDFs and supports specialized prompts for comprehensive paper examination.",
      "stars": 64,
      "forks": 26,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-29T17:15:22Z",
      "readme_content": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/jackkuo666-pubmed-mcp-server-badge.png)](https://mseep.ai/app/jackkuo666-pubmed-mcp-server)\n\n# PubMed MCP Server\n\n[![smithery badge](https://smithery.ai/badge/@JackKuo666/pubmed-mcp-server)](https://smithery.ai/server/@JackKuo666/pubmed-mcp-server)\n\n🔍 Enable AI assistants to search, access, and analyze PubMed articles through a simple MCP interface.\n\nThe PubMed MCP Server provides a bridge between AI assistants and PubMed's vast repository of biomedical literature through the Model Context Protocol (MCP). It allows AI models to search for scientific articles, access their metadata, and perform deep analysis in a programmatic way.\n\n🤝 Contribute • 📝 Report Bug\n\n## ✨ Core Features\n- 🔎 Paper Search: Query PubMed articles with keywords or advanced search ✅\n- 🚀 Efficient Retrieval: Fast access to paper metadata ✅\n- 📊 Metadata Access: Retrieve detailed metadata for specific papers ✅\n- 📊 Research Support: Facilitate biomedical sciences research and analysis ✅\n- 📄 Paper Access: Attempt to download full-text PDF content ✅\n- 🧠 Deep Analysis: Perform comprehensive analysis of papers ✅\n- 📝 Research Prompts: A set of specialized prompts for paper analysis ✅\n\n## 🚀 Quick Start\n\n### Prerequisites\n\n- Python 3.10+\n- FastMCP library\n\n### Installation\n### Installing via Smithery\n\nTo install pubmed-mcp-server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@JackKuo666/pubmed-mcp-server):\n\n#### claude\n\n```bash\nnpx -y @smithery/cli install @JackKuo666/pubmed-mcp-server --client claude\n```\n\n#### Cursor\n\nPaste the following into Settings → Cursor Settings → MCP → Add new server: \n- Mac/Linux  \n```s\nnpx -y @smithery/cli@latest run @JackKuo666/pubmed-mcp-server --client cursor --config \"{}\" \n```\n#### Windsurf\n```sh\nnpx -y @smithery/cli@latest install @JackKuo666/pubmed-mcp-server --client windsurf --config \"{}\"\n```\n### CLine\n```sh\nnpx -y @smithery/cli@latest install @JackKuo666/pubmed-mcp-server --client cline --config \"{}\"\n```\n\n1. Clone the repository:\n   ```\n   git clone https://github.com/JackKuo666/PubMed-MCP-Server.git\n   cd PubMed-MCP-Server\n   ```\n\n2. Install the required dependencies:\n   ```\n   pip install -r requirements.txt\n   ```\n\n## 📊 Usage\n\nStart the MCP server:\n\n```bash\npython pubmed_server.py\n```\n## Usage with Claude Desktop\n\nAdd this configuration to your `claude_desktop_config.json`:\n\n(Mac OS)\n\n```json\n{\n  \"mcpServers\": {\n    \"pubmed\": {\n      \"command\": \"python\",\n      \"args\": [\"-m\", \"pubmed-mcp-server\"]\n      }\n  }\n}\n```\n\n(Windows version):\n\n```json\n{\n  \"mcpServers\": {\n    \"pubmed\": {\n      \"command\": \"C:\\\\Users\\\\YOUR\\\\PATH\\\\miniconda3\\\\envs\\\\mcp_server\\\\python.exe\",\n      \"args\": [\n        \"D:\\\\code\\\\YOUR\\\\PATH\\\\PubMed-MCP-Server\\\\pubmed_server.py\"\n      ],\n      \"env\": {},\n      \"disabled\": false,\n      \"autoApprove\": []\n    }\n  }\n}\n```\nUsing with Cline\n```json\n{\n  \"mcpServers\": {\n    \"pubmed\": {\n      \"command\": \"bash\",\n      \"args\": [\n        \"-c\",\n        \"source /home/YOUR/PATH/mcp-server-pubmed/.venv/bin/activate && python /home/YOUR/PATH/pubmed-mcp-server.py\"\n      ],\n      \"env\": {},\n      \"disabled\": false,\n      \"autoApprove\": []\n    }\n  }\n}\n```\n\n## 🛠 MCP Tools\n\nThe PubMed MCP Server provides the following tools:\n\n1. `search_pubmed_key_words`: Search for articles on PubMed using keywords.\n2. `search_pubmed_advanced`: Perform an advanced search for articles on PubMed with multiple parameters.\n3. `get_pubmed_article_metadata`: Fetch metadata for a PubMed article using its PMID.\n4. `download_pubmed_pdf`: Attempt to download the full-text PDF for a PubMed article.\n5. `deep_paper_analysis`: Perform a comprehensive analysis of a PubMed article.\n\n### Searching Papers\n\nYou can ask the AI assistant to search for papers using queries like:\n```\nCan you search PubMed for recent papers about CRISPR?\n```\n\n### Getting Paper Details\n\nOnce you have a PMID, you can ask for more details:\n```\nCan you show me the metadata for the paper with PMID 12345678?\n```\n\n### Analyzing Papers\n\nYou can request a deep analysis of a paper:\n```\nCan you perform a deep analysis of the paper with PMID 12345678?\n```\n\n## 📁 Project Structure\n\n- `pubmed_server.py`: The main MCP server implementation using FastMCP\n- `pubmed_web_search.py`: Contains the logic for searching PubMed and retrieving article information\n\n## 🔧 Dependencies\n\n- Python 3.10+\n- FastMCP\n- asyncio\n- logging\n- requests\n- beautifulsoup4\n\n## 🤝 Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n\n## 📄 License\n\nThis project is licensed under the MIT License.\n\n## ⚠️ Disclaimer\n\nThis tool is for research purposes only. Please respect PubMed's terms of service and use this tool responsibly.\n",
      "npm_url": "https://www.npmjs.com/package/pubmed-mcp-server",
      "npm_downloads": 80,
      "keywords": [
        "pubmed",
        "search",
        "searching",
        "pubmed searching",
        "models pubmed",
        "biomedical articles"
      ],
      "category": "web-search"
    },
    "JackKuo666--Sci-Hub-MCP-Server": {
      "owner": "JackKuo666",
      "name": "Sci-Hub-MCP-Server",
      "url": "https://github.com/JackKuo666/Sci-Hub-MCP-Server",
      "imageUrl": "/freedevtools/mcp/pfp/JackKuo666.webp",
      "description": "Connects to Sci-Hub for searching, accessing, and analyzing academic papers. Supports retrieving metadata and downloading full-text PDFs programmatically.",
      "stars": 35,
      "forks": 13,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-09-13T00:11:18Z",
      "readme_content": "# Sci-Hub MCP Server\n\n[![smithery badge](https://smithery.ai/badge/@JackKuo666/sci-hub-mcp-server)](https://smithery.ai/server/@JackKuo666/sci-hub-mcp-server)\n\n🔍 Enable AI assistants to search, access, and analyze academic papers through Sci-Hub using a simple MCP interface.\n\nThe Sci-Hub MCP Server provides a bridge between AI assistants and Sci-Hub's repository of academic literature through the Model Context Protocol (MCP). It allows AI models to search for scientific articles by DOI, title, or keywords, access their metadata, and download PDFs in a programmatic way.\n\n## ✨ Core Features\n\n- 🔎 Paper Search by DOI: Find papers using their Digital Object Identifier ✅\n- 🔍 Paper Search by Title: Locate papers using their full or partial title ✅\n- 🔑 Paper Search by Keyword: Discover papers related to specific research areas ✅\n- 📊 Metadata Access: Retrieve detailed metadata for specific papers ✅\n- 📄 PDF Download: Download full-text PDF content when available ✅\n\n## 🚀 Quick Start\n\n### Prerequisites\n\n- Python 3.10+\n- FastMCP library\n\n### Installation\n\n1. Clone the repository:\n   ```\n   git clone https://github.com/JackKuo666/Sci-Hub-MCP-Server.git\n   cd Sci-Hub-MCP-Server\n   ```\n\n2. Install the required dependencies:\n   ```\n   pip install -r requirements.txt\n   ```\n\n## 📊 Usage\n\nStart the MCP server:\n\n```bash\npython sci_hub_server.py\n```\n\n## Usage with Claude Desktop\n\nAdd this configuration to your `claude_desktop_config.json`:\n\n(Mac OS)\n\n```json\n{\n  \"mcpServers\": {\n    \"scihub\": {\n      \"command\": \"python\",\n      \"args\": [\"-m\", \"sci_hub_server.py\"]\n      }\n  }\n}\n```\n\n(Windows version):\n\n```json\n{\n  \"mcpServers\": {\n    \"scihub\": {\n      \"command\": \"C:\\\\Users\\\\YOUR\\\\PATH\\\\miniconda3\\\\envs\\\\mcp_server\\\\python.exe\",\n      \"args\": [\n        \"D:\\\\code\\\\YOUR\\\\PATH\\\\Sci-Hub-MCP-Server\\\\sci_hub_server.py\"\n      ],\n      \"env\": {},\n      \"disabled\": false,\n      \"autoApprove\": []\n    }\n  }\n}\n```\n\n## 🛠 MCP Tools\n\nThe Sci-Hub MCP Server provides the following tools:\n\n1. `search_scihub_by_doi`: Search for a paper on Sci-Hub using its DOI (Digital Object Identifier).\n2. `search_scihub_by_title`: Search for a paper on Sci-Hub using its title.\n3. `search_scihub_by_keyword`: Search for papers on Sci-Hub using a keyword.\n4. `download_scihub_pdf`: Download a paper PDF from Sci-Hub.\n5. `get_paper_metadata`: Get metadata information for a paper using its DOI.\n\n### Searching Papers by DOI\n\nYou can ask the AI assistant to search for papers using DOI:\n```\nCan you search Sci-Hub for the paper with DOI 10.1038/nature09492?\n```\n\n### Searching Papers by Title\n\nYou can search for papers using their title:\n```\nCan you find the paper titled \"Choosing Assessment Instruments for Posttraumatic Stress Disorder Screening and Outcome Research\" on Sci-Hub?\n```\n\n### Searching Papers by Keyword\n\nYou can search for papers related to specific keywords:\n```\nCan you search Sci-Hub for recent papers about artificial intelligence in medicine?\n```\n\n### Downloading Papers\n\nOnce you have found a paper, you can download it:\n```\nCan you download the PDF for this paper to my_paper.pdf?\n```\n\n### Getting Paper Metadata\n\nYou can request metadata for a paper using its DOI:\n```\nCan you show me the metadata for the paper with DOI 10.1038/nature09492?\n```\n\n## 📁 Project Structure\n\n- `sci_hub_server.py`: The main MCP server implementation using FastMCP\n- `sci_hub_search.py`: Contains the logic for searching Sci-Hub and retrieving paper information\n\n## 🔧 Dependencies\n\n- Python 3.10+\n- FastMCP\n- requests\n- bs4\n- scihub\n\n## 🤝 Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n\n## 📄 License\n\nThis project is licensed under the MIT License.\n\n## ⚠️ Disclaimer\n\nThis tool is for research purposes only. Please respect copyright laws and use this tool responsibly. The authors do not endorse or encourage any copyright infringement.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "pdfs",
        "search",
        "searching",
        "hub searching",
        "sci hub",
        "metadata downloading"
      ],
      "category": "web-search"
    },
    "JackKuo666--bioRxiv-MCP-Server": {
      "owner": "JackKuo666",
      "name": "bioRxiv-MCP-Server",
      "url": "https://github.com/JackKuo666/bioRxiv-MCP-Server",
      "imageUrl": "/freedevtools/mcp/pfp/JackKuo666.webp",
      "description": "Search for bioRxiv papers, retrieve detailed metadata, and download papers to support biological sciences research and analysis.",
      "stars": 20,
      "forks": 6,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-09-04T18:06:12Z",
      "readme_content": "# bioRxiv MCP Server\n\n🔍 Enable AI assistants to search and access bioRxiv papers through a simple MCP interface.\n\nThe bioRxiv MCP Server provides a bridge between AI assistants and bioRxiv's preprint repository through the Model Context Protocol (MCP). It allows AI models to search for biology preprints and access their metadata in a programmatic way.\n\n🤝 Contribute • 📝 Report Bug\n\n## ✨ Core Features\n- 🔎 Paper Search: Query bioRxiv papers with keywords or advanced search ✅\n- 🚀 Efficient Retrieval: Fast access to paper metadata ✅\n- 📊 Metadata Access: Retrieve detailed metadata for specific papers ✅\n- 📊 Research Support: Facilitate biological sciences research and analysis ✅\n- 📄 Paper Access: Download and read paper content 📝\n- 📋 Paper Listing: View all downloaded papers 📝\n- 🗃️ Local Storage: Papers are saved locally for faster access 📝\n- 📝 Research Prompts: A set of specialized prompts for paper analysis 📝\n\n## 🚀 Quick Start\n\n### Prerequisites\n\n- Python 3.10+\n- FastMCP library\n\n### Installation\n\n1. Clone the repository:\n   ```\n   git clone https://github.com/JackKuo666/bioRxiv-MCP-Server.git\n   cd bioRxiv-MCP-Server\n   ```\n\n2. Install the required dependencies:\n   ```\n   pip install -r requirements.txt\n   ```\n\n### Installing via Smithery\n\nTo install bioRxiv Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@JackKuo666/biorxiv-mcp-server):\n\n#### claude\n\n```bash\nnpx -y @smithery/cli@latest install @JackKuo666/biorxiv-mcp-server --client claude --config \"{}\"\n```\n\n#### Cursor\n\nPaste the following into Settings → Cursor Settings → MCP → Add new server: \n- Mac/Linux  \n```s\nnpx -y @smithery/cli@latest run @JackKuo666/biorxiv-mcp-server --client cursor --config \"{}\" \n```\n#### Windsurf\n```sh\nnpx -y @smithery/cli@latest install @JackKuo666/biorxiv-mcp-server --client windsurf --config \"{}\"\n```\n#### CLine\n```sh\nnpx -y @smithery/cli@latest install @JackKuo666/biorxiv-mcp-server --client cline --config \"{}\"\n```\n\n#### Usage with Claude Desktop\n\nAdd this configuration to your `claude_desktop_config.json`:\n\n(Mac OS)\n\n```json\n{\n  \"mcpServers\": {\n    \"biorxiv\": {\n      \"command\": \"python\",\n      \"args\": [\"-m\", \"biorxiv-mcp-server\"]\n      }\n  }\n}\n```\n\n(Windows version):\n\n```json\n{\n  \"mcpServers\": {\n    \"biorxiv\": {\n      \"command\": \"C:\\\\Users\\\\YOUR_USERNAME\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\python.exe\",\n      \"args\": [\n        \"-m\",\n        \"biorxiv-mcp-server\"\n      ]\n    }\n  }\n}\n```\nUsing with Cline\n```json\n{\n  \"mcpServers\": {\n    \"biorxiv\": {\n      \"command\": \"bash\",\n      \"args\": [\n        \"-c\",\n        \"source /home/YOUR/PATH/mcp-server-bioRxiv/.venv/bin/activate && python /home/YOUR/PATH/mcp-server-bioRxiv/biorxiv_server.py\"\n      ],\n      \"env\": {},\n      \"disabled\": false,\n      \"autoApprove\": []\n    }\n  }\n}\n```\n\n\n## 📊 Usage\n\nStart the MCP server:\n\n```bash\npython biorxiv_server.py\n```\n\n## 🛠 MCP Tools\n\nThe bioRxiv MCP Server provides the following tools:\n\n1. `search_biorxiv_key_words`: Search for articles on bioRxiv using keywords.\n2. `search_biorxiv_advanced`: Perform an advanced search for articles on bioRxiv with multiple parameters.\n3. `get_biorxiv_metadata`: Fetch metadata for a bioRxiv article using its DOI.\n\n### Searching Papers\n\nYou can ask the AI assistant to search for papers using queries like:\n```\nCan you search bioRxiv for recent papers about genomics?\n```\n\n### Getting Paper Details\n\nOnce you have a DOI, you can ask for more details:\n```\nCan you show me the metadata for the paper with DOI 10.1101/123456?\n```\n\n## 📁 Project Structure\n\n- `biorxiv_server.py`: The main MCP server implementation using FastMCP\n- `biorxiv_web_search.py`: Contains the web scraping logic for searching bioRxiv\n\n## 🔧 Dependencies\n\n- Python 3.10+\n- FastMCP\n- asyncio\n- logging\n\n## 🤝 Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n\n## 📄 License\n\nThis project is licensed under the MIT License.\n\n## ⚠️ Disclaimer\n\nThis tool is for research purposes only. Please respect bioRxiv's terms of service and use this tool responsibly.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "biorxiv",
        "biological",
        "search",
        "search biorxiv",
        "biorxiv papers",
        "jackkuo666 biorxiv"
      ],
      "category": "web-search"
    },
    "JackKuo666--medRxiv-MCP-Server": {
      "owner": "JackKuo666",
      "name": "medRxiv-MCP-Server",
      "url": "https://github.com/JackKuo666/medRxiv-MCP-Server",
      "imageUrl": "/freedevtools/mcp/pfp/JackKuo666.webp",
      "description": "Integrate with medRxiv's preprint repository to search for and access health sciences research papers and their metadata via a standardized interface. Enables efficient querying and retrieval of paper details based on custom search parameters and DOI.",
      "stars": 6,
      "forks": 3,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-07-29T12:23:22Z",
      "readme_content": "# medRxiv MCP Server\n[![smithery badge](https://smithery.ai/badge/@JackKuo666/medrxiv-mcp-server)](https://smithery.ai/server/@JackKuo666/medrxiv-mcp-server)\n\n🔍 Enable AI assistants to search and access medRxiv papers through a simple MCP interface.\n\nThe medRxiv MCP Server provides a bridge between AI assistants and medRxiv's preprint repository through the Model Context Protocol (MCP). It allows AI models to search for health sciences preprints and access their content in a programmatic way.\n\n🤝 Contribute • 📝 Report Bug\n\n## ✨ Core Features\n- 🔎 Paper Search: Query medRxiv papers with custom search strings or advanced search parameters ✅\n- 🚀 Efficient Retrieval: Fast access to paper metadata ✅\n- 📊 Metadata Access: Retrieve detailed metadata for specific papers using DOI ✅\n- 📊 Research Support: Facilitate health sciences research and analysis ✅\n- 📄 Paper Access: Download and read paper content 📝\n- 📋 Paper Listing: View all downloaded papers 📝\n- 🗃️ Local Storage: Papers are saved locally for faster access 📝\n- 📝 Research Prompts: A set of specialized prompts for paper analysis 📝\n\n## 🚀 Quick Start\n\n### Installing via Smithery\n\nTo install medRxiv Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@JackKuo666/medrxiv-mcp-server):\n\n#### claude\n\n```bash\nnpx -y @smithery/cli@latest install @JackKuo666/medrxiv-mcp-server --client claude --config \"{}\"\n```\n\n#### Cursor\n\nPaste the following into Settings → Cursor Settings → MCP → Add new server: \n- Mac/Linux  \n```s\nnpx -y @smithery/cli@latest run @JackKuo666/medrxiv-mcp-server --client cursor --config \"{}\" \n```\n#### Windsurf\n```sh\nnpx -y @smithery/cli@latest install @JackKuo666/medrxiv-mcp-server --client windsurf --config \"{}\"\n```\n### CLine\n```sh\nnpx -y @smithery/cli@latest install @JackKuo666/medrxiv-mcp-server --client cline --config \"{}\"\n```\n\n\n### Installing Manually\nInstall using uv:\n\n```bash\nuv tool install medRxiv-mcp-server\n```\n\nFor development:\n\n```bash\n# Clone and set up development environment\ngit clone https://github.com/JackKuo666/medRxiv-MCP-Server.git\ncd medRxiv-MCP-Server\n\n# Create and activate virtual environment\nuv venv\nsource .venv/bin/activate\nuv pip install -r requirements.txt\n```\n\n## 📊 Usage\n\nStart the MCP server:\n\n```bash\npython medrxiv_server.py\n```\n\nOnce the server is running, you can use the provided MCP tools in your AI assistant or application. Here are some examples of how to use the tools:\n\n### Example 1: Search for papers using keywords\n\n```python\nresult = await mcp.use_tool(\"search_medrxiv_key_words\", {\n    \"key_words\": \"COVID-19 vaccine efficacy\",\n    \"num_results\": 5\n})\nprint(result)\n```\n\n### Example 2: Perform an advanced search\n\n```python\nresult = await mcp.use_tool(\"search_medrxiv_advanced\", {\n    \"term\": \"COVID-19\",\n    \"author1\": \"MacLachlan\",\n    \"start_date\": \"2020-01-01\",\n    \"end_date\": \"2023-12-31\",\n    \"num_results\": 3\n})\nprint(result)\n```\n\n### Example 3: Get metadata for a specific paper\n\n```python\nresult = await mcp.use_tool(\"get_medrxiv_metadata\", {\n    \"doi\": \"10.1101/2025.03.09.25323517\"\n})\nprint(result)\n```\n\nThese examples demonstrate how to use the three main tools provided by the medRxiv MCP Server. Adjust the parameters as needed for your specific use case.\n\n## 🛠 MCP Tools\n\nThe medRxiv MCP Server provides the following tools:\n\n### search_medrxiv_key_words\n\nSearch for articles on medRxiv using key words.\n\n**Parameters:**\n- `key_words` (str): Search query string\n- `num_results` (int, optional): Number of results to return (default: 10)\n\n**Returns:** List of dictionaries containing article information\n\n### search_medrxiv_advanced\n\nPerform an advanced search for articles on medRxiv.\n\n**Parameters:**\n- `term` (str, optional): General search term\n- `title` (str, optional): Search in title\n- `author1` (str, optional): First author\n- `author2` (str, optional): Second author\n- `abstract_title` (str, optional): Search in abstract and title\n- `text_abstract_title` (str, optional): Search in full text, abstract, and title\n- `section` (str, optional): Section of medRxiv\n- `start_date` (str, optional): Start date for search range (format: YYYY-MM-DD)\n- `end_date` (str, optional): End date for search range (format: YYYY-MM-DD)\n- `num_results` (int, optional): Number of results to return (default: 10)\n\n**Returns:** List of dictionaries containing article information\n\n### get_medrxiv_metadata\n\nFetch metadata for a medRxiv article using its DOI.\n\n**Parameters:**\n- `doi` (str): DOI of the article\n\n**Returns:** Dictionary containing article metadata\n\n## Usage with Claude Desktop\n\nAdd this configuration to your `claude_desktop_config.json`:\n\n(Mac OS)\n\n```json\n{\n  \"mcpServers\": {\n    \"medrxiv\": {\n      \"command\": \"python\",\n      \"args\": [\"-m\", \"medrxiv-mcp-server\"]\n      }\n  }\n}\n```\n\n(Windows version):\n\n```json\n{\n  \"mcpServers\": {\n    \"medrxiv\": {\n      \"command\": \"C:\\\\Users\\\\YOUR_USERNAME\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\python.exe\",\n      \"args\": [\n        \"-m\",\n        \"medrxiv-mcp-server\"\n      ]\n    }\n  }\n}\n```\nUsing with Cline\n```json\n{\n  \"mcpServers\": {\n    \"medrxiv\": {\n      \"command\": \"bash\",\n      \"args\": [\n        \"-c\",\n        \"source /home/YOUR/PATH/mcp-server-medRxiv/.venv/bin/activate && python /home/YOUR/PATH/mcp-server-medRxiv/medrxiv_server.py\"\n      ],\n      \"env\": {},\n      \"disabled\": false,\n      \"autoApprove\": []\n    }\n  }\n}\n```\n\nAfter restarting Claude Desktop, the following capabilities will be available:\n\n### Searching Papers\n\nYou can ask Claude to search for papers using queries like:\n```\nCan you search medRxiv for recent papers about genomics?\n```\n\nThe search will return basic information about matching papers including:\n\n• Paper title\n\n• Authors\n\n• DOI\n\n\n### Getting Paper Details\n\nOnce you have a DOI, you can ask for more details:\n```\nCan you show me the details for paper 10.1101/003541?\n```\n\nThis will return:\n\n• Full paper title\n\n• Authors\n\n• Publication date\n\n• Paper abstract\n\n• Links to available formats (PDF/HTML)\n\n\n\n## 📝 TODO\n\n### download_paper\n\nDownload a paper and save it locally.\n\n### read_paper\n\nRead the content of a downloaded paper.\n\n### list_papers\n\nList all downloaded papers.\n\n### 📝 Research Prompts\n\nThe server offers specialized prompts to help analyze academic papers:\n\n#### Paper Analysis Prompt\n\nA comprehensive workflow for analyzing academic papers that only requires a paper ID:\n\n```python\nresult = await call_prompt(\"deep-paper-analysis\", {\n    \"paper_id\": \"2401.12345\"\n})\n```\n\nThis prompt includes:\n\n- Detailed instructions for using available tools (list_papers, download_paper, read_paper, search_papers)\n- A systematic workflow for paper analysis\n- Comprehensive analysis structure covering:\n  - Executive summary\n  - Research context\n  - Methodology analysis\n  - Results evaluation\n  - Practical and theoretical implications\n  - Future research directions\n  - Broader impacts\n\n## 📁 Project Structure\n\n- `medrxiv_server.py`: The main MCP server implementation using FastMCP\n- `medrxiv_web_search.py`: Contains the web scraping logic for searching medRxiv\n\n## 🔧 Dependencies\n\n- Python 3.10+\n- FastMCP\n- asyncio\n- logging\n- requests (for web scraping, used in medrxiv_web_search.py)\n- beautifulsoup4 (for web scraping, used in medrxiv_web_search.py)\n\nYou can install the required dependencies using:\n\n```bash\npip install FastMCP requests beautifulsoup4\n```\n\n## 🤝 Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n\n## 📄 License\n\nThis project is licensed under the MIT License.\n\n## 🙏 Acknowledgements\n\nThis project is inspired by and built upon the work done in the [arxiv-mcp-server](https://github.com/blazickjp/arxiv-mcp-server) project.\n\n## ⚠️ Disclaimer\n\nThis tool is for research purposes only. Please respect medRxiv's terms of service and use this tool responsibly.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "medrxiv",
        "search",
        "retrieval",
        "papers metadata",
        "retrieval paper",
        "medrxiv mcp"
      ],
      "category": "web-search"
    },
    "JackKuo666--semanticscholar-MCP-Server": {
      "owner": "JackKuo666",
      "name": "semanticscholar-MCP-Server",
      "url": "https://github.com/JackKuo666/semanticscholar-MCP-Server",
      "imageUrl": "/freedevtools/mcp/pfp/JackKuo666.webp",
      "description": "Search for academic papers, retrieve detailed information about specific papers and authors, and access citations and references through the Semantic Scholar API.",
      "stars": 29,
      "forks": 6,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-09-26T19:47:33Z",
      "readme_content": "# 🎓 Semantic Scholar MCP Server\n\n[![smithery badge](https://smithery.ai/badge/@JackKuo666/semanticscholar-mcp-server)](https://smithery.ai/server/@JackKuo666/semanticscholar-mcp-server)\n\nThis project implements a Model Context Protocol (MCP) server for interacting with the Semantic Scholar API. It provides tools for searching papers, retrieving paper and author details, and fetching citations and references.\n\n## ✨ Features\n\n- 🔍 Search for papers on Semantic Scholar\n- 📄 Retrieve detailed information about specific papers\n- 👤 Get author details\n- 🔗 Fetch citations and references for a paper\n\n## 📋 Prerequisites\n\n- 🐍 Python 3.10+\n- 📚 `semanticscholar` Python package\n- 🔧 `mcp` Python package (Model Context Protocol)\n\n## 🚀 Installation\n### Installing via Smithery\n\nTo install semanticscholar Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@JackKuo666/semanticscholar-mcp-server):\n\n#### claude\n\n```sh\nnpx -y @smithery/cli@latest install @JackKuo666/semanticscholar-mcp-server --client claude --config \"{}\"\n```\n\n#### Cursor\n\nPaste the following into Settings → Cursor Settings → MCP → Add new server: \n- Mac/Linux  \n```s\nnpx -y @smithery/cli@latest run @JackKuo666/semanticscholar-mcp-server --client cursor --config \"{}\" \n```\n#### Windsurf\n```sh\nnpx -y @smithery/cli@latest install @JackKuo666/semanticscholar-mcp-server --client windsurf --config \"{}\"\n```\n### CLine\n```sh\nnpx -y @smithery/cli@latest install @JackKuo666/semanticscholar-mcp-server --client cline --config \"{}\"\n```\n\n\n1. Clone this repository:\n   ```\n   git clone https://github.com/JackKuo666/semanticscholar-MCP-Server.git\n   cd semanticscholar-mcp-server\n   ```\n\n2. Install the required packages:\n   ```\n   pip install semanticscholar mcp\n   ```\n\n## 🖥️ Usage\n\n1. Start the Semantic Scholar MCP server:\n   ```\n   python semantic_scholar_server.py\n   ```\n\n2. The server will start and listen for MCP requests.\n\n3. Use an MCP client to interact with the server and access the following tools:\n\n   - 🔍 `search_semantic_scholar`: Search for papers using a query string\n   - 📄 `get_semantic_scholar_paper_details`: Get details of a specific paper\n   - 👤 `get_semantic_scholar_author_details`: Get details of a specific author\n   - 🔗 `get_semantic_scholar_citations_and_references`: Get citations and references for a paper\n\n## Usage with Claude Desktop\n\nAdd this configuration to your `claude_desktop_config.json`:\n\n(Mac OS)\n\n```json\n{\n  \"mcpServers\": {\n    \"semanticscholar\": {\n      \"command\": \"python\",\n      \"args\": [\"-m\", \"semanticscholar_mcp_server\"]\n      }\n  }\n}\n```\n\n(Windows version):\n\n```json\n{\n  \"mcpServers\": {\n    \"semanticscholar\": {\n      \"command\": \"C:\\\\Users\\\\YOUR\\\\PATH\\\\miniconda3\\\\envs\\\\mcp_server\\\\python.exe\",\n      \"args\": [\n        \"D:\\\\code\\\\YOUR\\\\PATH\\\\semanticscholar-MCP-Server\\\\semanticscholar_server.py\"\n      ],\n      \"env\": {},\n      \"disabled\": false,\n      \"autoApprove\": []\n    }\n  }\n}\n```\nUsing with Cline\n```json\n{\n  \"mcpServers\": {\n    \"semanticscholar\": {\n      \"command\": \"bash\",\n      \"args\": [\n        \"-c\",\n        \"source /home/YOUR/PATH/.venv/bin/activate && python /home/YOUR/PATH/semanticscholar_mcp_server.py\"\n      ],\n      \"env\": {},\n      \"disabled\": false,\n      \"autoApprove\": []\n    }\n  }\n}\n```\n\n## 📁 File Structure\n\n- 📜 `semantic_scholar_search.py`: Contains functions for interacting with the Semantic Scholar API\n- 🖥️ `semantic_scholar_server.py`: Implements the MCP server and defines the available tools\n\n## 🤝 Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n\n## 📄 License\n\nThis project is licensed under the MIT License - see the LICENSE file for details.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "scholar",
        "citations",
        "search",
        "semantic scholar",
        "scholar api",
        "search academic"
      ],
      "category": "web-search"
    },
    "JavaProgrammerLB--unsplash-mcp-server": {
      "owner": "JavaProgrammerLB",
      "name": "unsplash-mcp-server",
      "url": "https://github.com/JavaProgrammerLB/unsplash-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/JavaProgrammerLB.webp",
      "description": "Search for high-quality images from Unsplash, enabling applications to integrate visual search capabilities. This server also serves as a learning resource for building MCP servers in Java.",
      "stars": 7,
      "forks": 0,
      "license": "MIT License",
      "language": "Java",
      "updated_at": "2025-07-15T07:26:39Z",
      "readme_content": "# Unsplash MCP Server\n- With this mcp server, you can easily search picture from **[unsplash](https://unsplash.com/)**\n- With this project, you can easily learn how to write MCP Server with JAVA\n> **search two house picture from unsplash** and write to the document\n\n## Feature\n\n\n\n## How to use\n\n1. Clone this project\n```\ngit clone https://github.com/JavaProgrammerLB/unsplash-mcp-server.git\n```\n\n2. Build\n```\ncd unsplash-mcp-server\nmvn clean package\n```\n3. Get Unsplash Access Key\n- visit [unsplash](https://unsplash.com/developers)\n- create an unsplash application\n- find out the access key in unsplash application detail page\n\n4. Config MCP Server\n```\n{\n  \"mcpServers\": {\n    \"unsplash\": {\n      \"command\": \"java\",\n      \"args\": [\n        \"-Dspring.ai.mcp.server.stdio=true\",\n        \"-Dspring.main.web-application-type=none\",\n        \"-Dlogging.pattern.console=\",\n        \"-jar\",\n        \"/ABSOLUTE/PATH/target/unsplash-mcp-server-1.0.jar\"\n      ],\n      \"env\": {\n        \"UNSPLASH_ACCESS_KEY\": \"${YOUR UNSPLASH ACCESS KEY}\"\n      }\n    }\n  }\n}\n```\n\n## Thanks\n- [Python Version Unsplash MCP Server](https://github.com/hellokaton/unsplash-mcp-server)",
      "npm_url": "https://www.npmjs.com/package/unsplash-mcp-server",
      "npm_downloads": 315,
      "keywords": [
        "javaprogrammerlb",
        "java",
        "mcp",
        "search javaprogrammerlb",
        "server search",
        "javaprogrammerlb unsplash"
      ],
      "category": "web-search"
    },
    "JayArrowz--mcp-osrs": {
      "owner": "JayArrowz",
      "name": "mcp-osrs",
      "url": "https://github.com/JayArrowz/mcp-osrs",
      "imageUrl": "/freedevtools/mcp/pfp/JayArrowz.webp",
      "description": "Seamless access to Old School RuneScape Wiki content and game data through a standardized protocol. Enables searching wiki pages, retrieving detailed page information, and querying various game data files to enhance applications with OSRS context.",
      "stars": 11,
      "forks": 9,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-09-27T20:23:42Z",
      "readme_content": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/jayarrowz-mcp-osrs-badge.png)](https://mseep.ai/app/jayarrowz-mcp-osrs)\n\n# OSRS MCP Server [![smithery badge](https://smithery.ai/badge/@jayarrowz/mcp-osrs)](https://smithery.ai/server/@jayarrowz/mcp-osrs)\n\nMCP Server for interacting with the Old School RuneScape (OSRS) Wiki API and data files. This server provides tools to search the OSRS Wiki and access game data definitions through the Model Context Protocol.\n\n<a href=\"https://glama.ai/mcp/servers/@JayArrowz/mcp-osrs\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@JayArrowz/mcp-osrs/badge\" alt=\"OSRS Server MCP server\" />\n</a>\n\n![image](https://github.com/user-attachments/assets/da9d1f48-513d-4a1b-a65b-56f8a012fa83)\n\n![image](https://github.com/user-attachments/assets/9e7e4e07-6e47-44f9-ab0c-b3835418bd37)\n\n![image](https://github.com/user-attachments/assets/628f35e1-2e85-42f4-8693-4ef4f16591d4)\n\n\n## Tools\n\nThis server implements the following tools:\n\n### OSRS Wiki Methods\n1. `osrs_wiki_search` - Search the OSRS Wiki for pages matching a search term\n2. `osrs_wiki_get_page_info` - Get information about specific pages on the OSRS Wiki\n3. `osrs_wiki_parse_page` - Get the parsed HTML content of a specific OSRS Wiki page\n\n### Game Data Search Methods\n4. `search_varptypes` - Search the varptypes.txt file for player variables (varps) that store player state and progress\n5. `search_varbittypes` - Search the varbittypes.txt file for variable bits (varbits) that store individual bits from varps\n6. `search_iftypes` - Search the iftypes.txt file for interface definitions used in the game's UI\n7. `search_invtypes` - Search the invtypes.txt file for inventory type definitions in the game\n8. `search_loctypes` - Search the loctypes.txt file for location/object type definitions in the game world\n9. `search_npctypes` - Search the npctypes.txt file for NPC (non-player character) definitions\n10. `search_objtypes` - Search the objtypes.txt file for object/item definitions in the game\n11. `search_rowtypes` - Search the rowtypes.txt file for row definitions used in various interfaces\n12. `search_seqtypes` - Search the seqtypes.txt file for animation sequence definitions\n13. `search_soundtypes` - Search the soundtypes.txt file for sound effect definitions in the game\n14. `search_spottypes` - Search the spottypes.txt file for spot animation (graphical effect) definitions\n15. `search_spritetypes` - Search the spritetypes.txt file for sprite image definitions used in the interface\n16. `search_tabletypes` - Search the tabletypes.txt file for interface tab definitions\n\n### Generic Data File Methods\n17. `search_data_file` - Search any file in the data directory for matching entries\n18. `get_file_details` - Get details about a file in the data directory\n19. `list_data_files` - List available data files in the data directory\n\n## Installation\n\n### Installing via Smithery\nTo install mcp-osrs for Claude Desktop automatically via [Smithery](https://smithery.ai/embed/@jayarrowz/mcp-osrs):\n\n```bash\nnpx @smithery/cli@latest install @jayarrowz/mcp-osrs --client claude\n```\n\n### Prerequisites\n- Node.js (v16 or later)\n- npm or yarn\n\n### Installing the package\n```bash\n# Clone the repository\ngit clone https://github.com/jayarrowz/mcp-osrs.git\ncd mcp-osrs\n\n# Install dependencies\nnpm install\n\n# Build the package\nnpm run build\n```\n\n## Usage with Claude Desktop\n\nAdd the following to your `claude_desktop_config.json`:\n\n### Using npx\n```json\n{\n  \"mcpServers\": {\n    \"osrs\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@jayarrowz/mcp-osrs\"]\n    }\n  }\n}\n```\n\n### Direct Node.js\n```json\n{\n  \"mcpServers\": {\n    \"osrs\": {\n      \"command\": \"node\",\n      \"args\": [\"/path/to/mcp-osrs/dist/index.js\"]\n    }\n  }\n}\n```\n\nReplace `/path/to/mcp-osrs` with the actual path to your repository.\n\n## Examples\n\n### Search the OSRS Wiki\n```javascript\n// Search for information about the Abyssal whip\nconst result = await callTool(\"osrs_wiki_search\", { \n  search: \"Abyssal whip\" \n});\n```\n\n### Get Page Information\n```javascript\n// Get information about a specific wiki page\nconst pageInfo = await callTool(\"osrs_wiki_get_page_info\", { \n  titles: \"Abyssal_whip\" \n});\n```\n\n### Search Game Data\n```javascript\n// Search for items in the object definitions\nconst items = await callTool(\"search_objtypes\", { \n  query: \"dragon\",\n  page: 1,\n  pageSize: 10\n});\n```\n\n### List Available Data Files\n```javascript\n// Get a list of all data files\nconst files = await callTool(\"list_data_files\", {});\n```\n\n## Development\n```bash\n# Install dependencies\nnpm install\n\n# Start the server in development mode\nnpm start\n\n# Build the server\nnpm run build\n```\n\n## License\nThis MCP server is licensed under the MIT License. This means you are free to use, modify, and distribute the software, subject to the terms and conditions of the MIT License. For more details, please see the LICENSE file in the project repository.",
      "npm_url": "https://www.npmjs.com/package/@jayarrowz/mcp-osrs",
      "npm_downloads": 3398,
      "keywords": [
        "runescape",
        "osrs",
        "wiki",
        "runescape wiki",
        "osrs context",
        "applications osrs"
      ],
      "category": "web-search"
    },
    "JeremyNixon--mcp-fetch": {
      "owner": "JeremyNixon",
      "name": "mcp-fetch",
      "url": "https://github.com/JeremyNixon/mcp-fetch",
      "imageUrl": "/freedevtools/mcp/pfp/JeremyNixon.webp",
      "description": "Fetches web content and processes images for integration with AI models, streamlining the retrieval and handling of online content in various applications.",
      "stars": 0,
      "forks": 0,
      "license": "MIT License",
      "language": "",
      "updated_at": "2025-03-15T18:56:32Z",
      "readme_content": "# MCP Fetch\n\n[![smithery badge](https://smithery.ai/badge/@kazuph/mcp-fetch)](https://smithery.ai/server/@kazuph/mcp-fetch)\n\nModel Context Protocol server for fetching web content and processing images. This allows Claude Desktop (or any MCP client) to fetch web content and handle images appropriately.\n\n## Quick Start (For Users)\n\nTo use this tool with Claude Desktop, simply add the following to your Claude Desktop configuration (`~/Library/Application Support/Claude/claude_desktop_config.json`):\n\n```json\n{\n  \"tools\": {\n    \"fetch\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@kazuph/mcp-fetch\"]\n    }\n  }\n}\n```\n\nThis will automatically download and run the latest version of the tool when needed.\n\n### Required Setup\n\n1. Enable Accessibility for Claude:\n   - Open System Settings\n   - Go to Privacy & Security > Accessibility\n   - Click the \"+\" button\n   - Add Claude from your Applications folder\n   - Turn ON the toggle for Claude\n\nThis accessibility setting is required for automated clipboard operations (Cmd+V) to work properly.\n\n## For Developers\n\nThe following sections are for those who want to develop or modify the tool.\n\n## Prerequisites\n\n- Node.js 18+\n- macOS (for clipboard operations)\n- Claude Desktop (install from https://claude.ai/desktop)\n- tsx (install via `npm install -g tsx`)\n\n## Installation\n\n### Installing via Smithery\n\nTo install MCP Fetch for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@kazuph/mcp-fetch):\n\n```bash\nnpx -y @smithery/cli install @kazuph/mcp-fetch --client claude\n```\n\n### Manual Installation\n```bash\ngit clone https://github.com/kazuph/mcp-fetch.git\ncd mcp-fetch\nnpm install\nnpm run build\n```\n\n## Image Processing Specifications\n\nWhen processing images from web content, the following limits are applied:\n\n- Maximum 6 images per group\n- Maximum height of 8000 pixels per group\n- Maximum size of 30MB per group\n\nIf content exceeds these limits, images will be automatically split into multiple groups, and you'll need to paste (Cmd+V) multiple times.\n\n## Configuration\n\n1. Make sure Claude Desktop is installed and running.\n\n2. Install tsx globally if you haven't:\n```bash\nnpm install -g tsx\n# or\npnpm add -g tsx\n```\n\n3. Modify your Claude Desktop config located at:\n`~/Library/Application Support/Claude/claude_desktop_config.json`\n\nYou can easily find this through the Claude Desktop menu:\n1. Open Claude Desktop\n2. Click Claude on the Mac menu bar\n3. Click \"Settings\"\n4. Click \"Developer\"\n\nAdd the following to your MCP client's configuration:\n\n```json\n{\n  \"tools\": {\n    \"fetch\": {\n      \"args\": [\"tsx\", \"/path/to/mcp-fetch/index.ts\"]\n    }\n  }\n}\n```\n\n## Available Tools\n\n- `fetch`: Retrieves URLs from the Internet and extracts their content as markdown. Images are automatically processed and prepared for clipboard operations.\n\n## Notes\n\n- This tool is designed for macOS only due to its dependency on macOS-specific clipboard operations.\n- Images are processed using Sharp for optimal performance and quality.\n- When multiple images are found, they are merged vertically with consideration for size limits.\n- Animated GIFs are automatically handled by extracting their first frame.\n",
      "npm_url": "https://www.npmjs.com/package/mcp-fetch",
      "npm_downloads": 1627,
      "keywords": [
        "retrieval",
        "ai",
        "search",
        "web search",
        "ai models",
        "fetches web"
      ],
      "category": "web-search"
    },
    "JiantaoFu--AppInsightMCP": {
      "owner": "JiantaoFu",
      "name": "AppInsightMCP",
      "url": "https://github.com/JiantaoFu/AppInsightMCP",
      "imageUrl": "/freedevtools/mcp/pfp/JiantaoFu.webp",
      "description": "Analyzes data from the Apple App Store and Google Play Store to provide insights on apps, market trends, and user feedback. Offers detailed information, app listings, and search capabilities for mobile applications in major app marketplaces.",
      "stars": 17,
      "forks": 8,
      "license": "No License",
      "language": "JavaScript",
      "updated_at": "2025-09-24T08:58:44Z",
      "readme_content": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/jiantaofu-appinsightmcp-badge.png)](https://mseep.ai/app/jiantaofu-appinsightmcp)\n\n# App Market Intelligence MCP\n\n[![smithery badge](https://smithery.ai/badge/@JiantaoFu/appinsightmcp)](https://smithery.ai/server/@JiantaoFu/appinsightmcp)\n\nAn MCP server that provides comprehensive market intelligence by analyzing data from both the Apple App Store and Google Play Store. Get insights about apps, market trends, competitors, and user feedback across the major mobile app marketplaces.\n\n<a href=\"https://glama.ai/mcp/servers/@JiantaoFu/AppInsightMCP\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@JiantaoFu/AppInsightMCP/badge\" alt=\"App Market Intelligence MCP server\" />\n</a>\n\n## API Coverage\n\n### App Store API Coverage\n\n| API Function | Implemented | MCP Tool Name | Description |\n|-------------|-------------|---------------|-------------|\n| app         | ✅ | app-store-details | Get detailed information about an App Store app |\n| list        | ✅ | app-store-list | Retrieve apps from iTunes collections |\n| search      | ✅ | app-store-search | Search for apps on the App Store |\n| developer   | ✅ | app-store-developer | Get apps by a developer |\n| privacy     | ✅ | app-store-privacy | Get privacy details for an app |\n| suggest     | ✅ | app-store-suggest | Get search suggestions |\n| similar     | ✅ | app-store-similar | Get similar apps |\n| reviews     | ✅ | app-store-reviews | Get app reviews |\n| ratings     | ✅ | app-store-ratings | Get app ratings |\n| versionHistory | ✅ | app-store-version-history | Get app version history |\n\n### Google Play API Coverage\n\n| API Function | Implemented | MCP Tool Name | Description |\n|-------------|-------------|---------------|-------------|\n| app         | ✅ | google-play-details | Get detailed app information |\n| list        | ✅ | google-play-list | Retrieve apps from collections |\n| search      | ✅ | google-play-search | Search for apps |\n| developer   | ✅ | google-play-developer | Get apps by developer |\n| suggest     | ✅ | google-play-suggest | Get search suggestions |\n| reviews     | ✅ | google-play-reviews | Get app reviews |\n| similar     | ✅ | google-play-similar | Get similar apps |\n| permissions | ✅ | google-play-permissions | Get app permissions |\n| datasafety  | ✅ | google-play-datasafety | Get data safety information |\n| categories  | ✅ | google-play-categories | Get list of categories |\n\n## Usage\n\nStart the MCP server:\n\n```bash\nnode src/server.js\n```\n\nThe server exposes tools that can be used through any MCP client. For example, using Claude for Desktop, you can:\n\n- Search for apps across both stores\n- Get detailed app information\n- Read reviews and ratings\n- Find similar apps\n- Check app privacy and permissions\n- And more\n\n## Usage Examples\n\n```javascript\n// Get top free iOS apps\n{\n  \"name\": \"app-store-list\",\n  \"params\": {\n    \"collection\": \"topfreeapplications\",\n    \"num\": 10\n  }\n}\n\n// Get top paid Android games\n{\n  \"name\": \"google-play-list\",\n  \"params\": {\n    \"collection\": \"TOP_PAID\",\n    \"category\": \"GAME\",\n    \"num\": 10\n  }\n}\n```\n\n## Test with MCP Inspector\n\n```\nnpm run test:inspector\n```\n\n\n\n## Test with mcp-cli\n\n```\nnpx @wong2/mcp-cli npx -y \"app-insight-mcp\"\n```\n\n## Usage with Claude Desktop\nAdd this to your `claude_desktop_config.json`:\n\n### Installing via Smithery\n\nTo install App Market Intelligence for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@JiantaoFu/appinsightmcp):\n\n```bash\nnpx -y @smithery/cli install @JiantaoFu/appinsightmcp --client claude\n```\n\n### Docker\n\n```json\n{\n  \"mcpServers\": {\n    \"app-insight-mcp\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"-i\",\n        \"--rm\",\n        \"app-insight-mcp\"\n      ]\n    }\n  }\n}\n```\n\n### NPX\n\n```json\n{\n  \"mcpServers\": {\n    \"app-insight-mcp\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"@jeromyfu/app-insight-mcp\"\n      ]\n    }\n  }\n}\n```\n\n## Build\n\nDocker build:\n\n```bash\ndocker build -t app-insight-mcp -f Dockerfile .\n```\n\n## Error Handling\n\nAll tools include error handling and will return appropriate error messages if:\n- Required parameters are missing\n- API calls fail\n- Rate limits are hit\n- Invalid IDs or parameters are provided\n\n## Contributing\n\nFeel free to contribute by:\n1. Implementing missing features\n2. Improving error handling\n3. Adding more API capabilities\n4. Enhancing documentation\n\n## License\n\nMIT",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "appinsightmcp",
        "apps",
        "app",
        "jiantaofu appinsightmcp",
        "search jiantaofu",
        "appinsightmcp analyzes"
      ],
      "category": "web-search"
    },
    "JigsawStack--jigsawstack-mcp-server": {
      "owner": "JigsawStack",
      "name": "jigsawstack-mcp-server",
      "url": "https://github.com/JigsawStack/jigsawstack-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/JigsawStack.webp",
      "description": "Scrape any website to obtain consistent, structured data quickly without the need for writing CSS selector code. Supports API integration for data extraction through a straightforward interface.",
      "stars": 23,
      "forks": 5,
      "license": "No License",
      "language": "TypeScript",
      "updated_at": "2025-08-22T15:15:38Z",
      "readme_content": "# JigsawStack MCP Server\n\n## Introduction\nJigsawStack MCP (Model Context Protocol) Server is a versatile platform designed to facilitate the integration and management of various tools. Each directory within the server represents a distinct tool that can be utilized for different purposes by an LLM. The server is built using Node.js and Express.js, and each tool is encapsulated within its own directory, making it easy to add, remove, or update tools without affecting the overall system.\n\nStart by obtaining your JIGSAWSTACK_API_KEY from the our website. You will need this key to access the JigsawStack services. You can get your API key by signing up for a free account at [JigsawStack](https://jigsawstack.com/dashboard).\n\nYou can also install our MCPs via [Smithery AI](https://smithery.ai/?q=jigsawstack)\n\n## Installation\n\n### Prerequisites\n- Ensure you have `git` installed on your system.\n- Ensure you have `node.js` and `npm` installed.\n- Alternatively, you can use `yarn` instead of `npm`. as a package manager.\n\n### Steps to Setup the repository:\n1. Clone the repository:\n    ```sh\n    git clone https://github.com/yourusername/jigsawstack-mcp-server.git\n    ```\n2. Navigate to the project directory:\n    ```sh\n    cd jigsawstack-mcp-server\n    ```\n3. Install the necessary dependencies:\n    ```sh\n    npm install or yarn install\n    ```\n\n## What is MCP?\nMCP stands for Model Context Protocol. It is a framework that allows users to integrate LLMs and manage various tools and components exposing external data in a modular fashion. Here each tool is encapsulated within its own directory, making it easy to add, remove, or update tools without affecting the overall system.\n\n## Using JigsawStack MCP Server\nThere are four tools available in the MCP Server. Each tool is contained within its own directory and has its own set of instructions for use.\n\n### Running a tool\nTo run a tool,\n1. cd into the tool directory and follow the instructions.\n2. Export the JIGSAWSTACK_API_KEY environment variable with your JIGSAWSTACK API key.\n    ```sh\n    export JIGSAWSTACK_API_KEY=your_api_key\n    ```\n3. Start the server:\n    ```sh\n    npm start\n    ```\n4. Access the server through your web browser at `http://localhost:3000`.\n\n### Directory Structure\n- `/ai-web-scraper`: Let AI scrape the internet for you!\n- `/ai-web-search`: Search powered by AI capable of handling complex queries.\n- `/image-generation`: Generate images using prompts, to receive a base64 string of the image.\n\n## Contact\nFor any questions or issues, please contact us at hello@jigsawstack.com.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "jigsawstack",
        "scrape",
        "web",
        "scrape website",
        "search jigsawstack",
        "server scrape"
      ],
      "category": "web-search"
    },
    "JoeBuildsStuff--mcp-jina-ai": {
      "owner": "JoeBuildsStuff",
      "name": "mcp-jina-ai",
      "url": "https://github.com/JoeBuildsStuff/mcp-jina-ai",
      "imageUrl": "/freedevtools/mcp/pfp/JoeBuildsStuff.webp",
      "description": "Access Jina AI's web services for web page reading, web search, and fact checking. Extract and format content from web pages for use with LLMs.",
      "stars": 30,
      "forks": 8,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-10-03T04:12:28Z",
      "readme_content": "# Jina AI MCP Server\n[![smithery badge](https://smithery.ai/badge/jina-ai-mcp-server)](https://smithery.ai/server/jina-ai-mcp)\n[![smithery badge](https://smithery.ai/badge/jina-ai-mcp-server)](https://smithery.ai/server/jina-ai-mcp-server)\n\nAn MCP server that provides access to Jina AI's powerful web services through Claude. This server implements three main tools:\n\n- Web page reading and content extraction\n- Web search\n- Fact checking/grounding\n\n<a href=\"https://glama.ai/mcp/servers/c1l6ib2j49\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/c1l6ib2j49/badge\" alt=\"mcp-jina-ai MCP server\" /></a>\n\n## Features\n\n### Tools\n\n#### `read_webpage`\n- Extract content from web pages in a format optimized for LLMs\n- Supports multiple output formats (Default, Markdown, HTML, Text, Screenshot, Pageshot)\n- Options for including links and images\n- Ability to generate alt text for images\n- Cache control options\n\n#### `search_web`\n- Search the web using Jina AI's search API\n- Configurable number of results (default: 5)\n- Support for image retention and alt text generation\n- Multiple return formats (markdown, text, html)\n- Returns structured results with titles, descriptions, and content\n\n#### `fact_check`\n- Fact-check statements using Jina AI's grounding engine\n- Provides factuality scores and supporting evidence \n- Optional deep-dive mode for more thorough analysis\n- Returns references with key quotes and supportive/contradictory classification\n\n## Setup\n\n### Prerequisites\n\nYou'll need a Jina AI API key to use this server. Get one for free at https://jina.ai/\n\n### Installation\n\nThere are two ways to use this server:\n\n#### Installing via Smithery\n\nTo install Jina AI for Claude Desktop automatically via [Smithery](https://smithery.ai/server/jina-ai-mcp-server):\n\n```bash\nnpx -y @smithery/cli install jina-ai-mcp-server --client claude\n```\n\n#### Option 1: NPX (Recommended)\nAdd this configuration to your Claude Desktop config file:\n\n```json\n{\n  \"mcpServers\": {\n    \"jina-ai-mcp-server\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"jina-ai-mcp-server\"\n      ],\n      \"env\": {\n        \"JINA_API_KEY\": \"<YOUR_KEY>\"\n      }\n    }\n  }\n}\n```\n\n#### Option 2: Local Installation\n1. Clone the repository\n2. Install dependencies:\n```bash\nnpm install\n```\n\n3. Build the server:\n```bash\nnpm run build\n```\n\n4. Add this configuration to your Claude Desktop config:\n```json\n{\n  \"mcpServers\": {\n    \"jina-ai-mcp-server\": {\n      \"command\": \"node\",\n      \"args\": [\n        \"/path/to/jina-ai-mcp-server/dist/index.js\"\n      ],\n      \"env\": {\n        \"JINA_API_KEY\": \"<YOUR_KEY>\"\n      }\n    }\n  }\n}\n```\n\n### Config File Location\n\nOn MacOS:\n```bash\n~/Library/Application Support/Claude/claude_desktop_config.json\n```\n\nOn Windows:\n```bash\n%APPDATA%/Claude/claude_desktop_config.json\n```\n\n### Debugging\n\nSince MCP servers communicate over stdio, debugging can be challenging. We recommend using the [MCP Inspector](https://github.com/modelcontextprotocol/inspector):\n\n```bash\nnpm run inspector\n```\n\nThe Inspector will provide a URL to access debugging tools in your browser.\n\n## API Response Types\n\nAll tools return structured JSON responses that include:\n\n- Status codes and metadata\n- Formatted content based on the requested output type\n- Usage information (token counts)\n- When applicable: images, links, and additional metadata\n\nFor detailed schema information, see `schemas.ts`.\n\n\n## Running evals\n\nThe evals package loads an mcp client that then runs the index.ts file, so there is no need to rebuild between tests. You can load environment variables by prefixing the npx command. Full documentation can be found [here](https://www.mcpevals.io/docs).\n\n```bash\nOPENAI_API_KEY=your-key  npx mcp-eval evals.ts index.ts\n```",
      "npm_url": "https://www.npmjs.com/package/mcp-jina-ai",
      "npm_downloads": 480,
      "keywords": [
        "web",
        "search",
        "jina",
        "web search",
        "ai web",
        "reading web"
      ],
      "category": "web-search"
    },
    "JonaFly--RedNote-MCP": {
      "owner": "JonaFly",
      "name": "RedNote-MCP",
      "url": "https://github.com/JonaFly/RedNote-MCP",
      "imageUrl": "/freedevtools/mcp/pfp/JonaFly.webp",
      "description": "Access and interact with Xiaohongshu (RedNote) content by authenticating, searching notes via keywords, and retrieving note content through URLs. Simplifies the integration of RedNote data into AI workflows and tools.",
      "stars": 5,
      "forks": 1,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-09-11T14:31:39Z",
      "readme_content": "# RedNote MCP\n\n[![English](https://img.shields.io/badge/English-Click-yellow)](docs/README.en.md)\n[![简体中文](https://img.shields.io/badge/简体中文-点击查看-orange)](README.md)\n[![npm](https://img.shields.io/npm/v/rednote-mcp)](https://www.npmjs.com/package/rednote-mcp)\n[![smithery badge](https://smithery.ai/badge/@JonaFly/rednote-mcp)](https://smithery.ai/server/@JonaFly/rednote-mcp)\n\n小红书内容访问的MCP服务\n\nhttps://github.com/user-attachments/assets/06b2c67f-d9ed-4a30-8f1d-9743f3edaa3a\n\n## 快速开始\n\n开始前确保安装了 [playwright](https://github.com/microsoft/playwright) 环境：\n\n```bash\nnpx playwright install\n```\n\n### Installing via Smithery\n\nTo install rednote-mcp for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@JonaFly/rednote-mcp):\n\n```bash\nnpx -y @smithery/cli install @JonaFly/rednote-mcp --client claude\n```\n\n### NPM 全局安装\n\n```bash\n# 全局安装\nnpm install -g rednote-mcp\n\n# 初始化登录，会自动记录cookie到 ~/.mcp/rednote/cookies.json\nrednote-mcp init\n```\n\n### 从源码安装\n\n```bash\n# 克隆项目\ngit clone https://github.com/ifuryst/rednote-mcp.git\ncd rednote-mcp\n\n# 安装依赖\nnpm install\n\n# 全局安装（可选，方便命令行调用）\nnpm install -g .\n\n# 或者直接运行，如初始化登录\nnpm run dev -- init\n```\n\n## 功能特性\n\n- 认证管理（支持 Cookie 持久化）\n- 关键词搜索笔记\n- 命令行初始化工具\n- 通过 URL 访问笔记内容\n- [ ] 通过 URL 访问评论内容\n\n## 使用说明\n\n### 1. 初始化登录\n\n首次使用需要先进行登录初始化：\n\n```bash\nrednote-mcp init\n# 或者直接从源码run\nnpm run dev -- init\n# 或者mcp-client里选择login\n```\n\n执行此命令后：\n\n1. 会自动打开浏览器窗口\n2. 跳转到小红书登录页面\n3. 请手动完成登录操作\n4. 登录成功后会自动保存 Cookie 到 `~/.mcp/rednote/cookies.json` 文件\n\n### 2. 在 Cursor 中配置 MCP Server\n\n在 Cursor 的 settings.json 中添加以下配置：\n\n```json\n{\n  \"mcpServers\": {\n    \"RedNote MCP\": {\n      \"command\": \"rednote-mcp\",\n      \"args\": [\n        \"--stdio\"\n      ]\n    }\n  }\n}\n```\n\n或者使用 npx 方式：\n\n```json\n{\n  \"mcpServers\": {\n    \"RedNote MCP\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"rednote-mcp\",\n        \"--stdio\"\n      ]\n    }\n  }\n}\n```\n\n配置说明：\n\n- `command`: 可以是全局安装后的 `rednote-mcp` 命令，或使用 `npx` 直接运行\n- `args`: 必须包含 `--stdio` 参数以支持 Cursor 的通信方式\n\n## 开发指南\n\n### 环境要求\n\n- Node.js >= 16\n- npm >= 7\n\n### 开发流程\n\n```bash\n# 安装依赖\nnpm install\n\n# 构建项目\nnpm run build\n\n# 开发模式运行\nnpm run dev\n\n# 运行测试\nnpm test\n```\n\n### 使用 MCP Inspector 进行调试\n\nMCP Inspector 是一个用于调试 MCP 服务器的工具，可以帮助开发者检查和验证 MCP 服务器的行为。使用以下命令启动：\n\n```bash\nnpx @modelcontextprotocol/inspector npx rednote-mcp --stdio\n```\n\n这个命令会：\n\n1. 启动 MCP Inspector 工具\n2. 通过 Inspector 运行 rednote-mcp 服务\n3. 提供一个交互式界面来检查请求和响应\n4. 帮助调试和验证 MCP 协议的实现\n\n## 注意事项\n\n1. 首次使用必须执行 `init` 命令进行登录\n2. Cookie 文件包含敏感信息，避免泄露\n3. 建议定期更新 Cookie，避免失效\n4. 确保已正确安装 Node.js 环境\n\n## 贡献指南\n\n1. Fork 本仓库\n2. 创建你的特性分支 (`git checkout -b feature/AmazingFeature`)\n3. 提交你的改动 (`git commit -m 'Add some AmazingFeature'`)\n4. 推送到分支 (`git push origin feature/AmazingFeature`)\n5. 开启一个 Pull Request\n\n## 许可证\n\nMIT License - 详见 [LICENSE](LICENSE) 文件 \n",
      "npm_url": "https://www.npmjs.com/package/rednote-mcp",
      "npm_downloads": 3377,
      "keywords": [
        "rednote",
        "notes",
        "xiaohongshu",
        "rednote mcp",
        "xiaohongshu rednote",
        "rednote content"
      ],
      "category": "web-search"
    },
    "JustasMonkev--mcp-accessibility-scanner": {
      "owner": "JustasMonkev",
      "name": "mcp-accessibility-scanner",
      "url": "https://github.com/JustasMonkev/mcp-accessibility-scanner",
      "imageUrl": "/freedevtools/mcp/pfp/JustasMonkev.webp",
      "description": "Automated web accessibility scanning using Playwright and Axe-core, enabling WCAG compliance checks and annotated screenshot capture. Generates detailed accessibility reports and interacts with web pages through browser automation.",
      "stars": 17,
      "forks": 5,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-10-01T15:40:11Z",
      "readme_content": "\n# MCP Accessibility Scanner 🔍\n\n[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/justasmonkev-mcp-accessibility-scanner-badge.png)](https://mseep.ai/app/justasmonkev-mcp-accessibility-scanner)\n\nA Model Context Protocol (MCP) server that provides automated web accessibility scanning using Playwright and Axe-core. This server enables LLMs to perform WCAG compliance checks, capture annotated screenshots, and generate detailed accessibility reports.\nA powerful Model Context Protocol (MCP) server that provides automated web accessibility scanning and browser automation using Playwright and Axe-core. This server enables LLMs to perform WCAG compliance checks, interact with web pages, manage persistent browser sessions, and generate detailed accessibility reports with visual annotations.\n\n## Features\n\n### Accessibility Scanning\n✅ Full WCAG 2.0/2.1/2.2 compliance checking (A, AA, AAA levels)  \n🖼️ Automatic screenshot capture with violation highlighting  \n📄 Detailed JSON reports with remediation guidance  \n🎯 Support for specific violation categories (color contrast, ARIA, forms, keyboard navigation, etc.)  \n\n### Browser Automation\n🖱️ Click, hover, and drag elements using accessibility snapshots  \n⌨️ Type text and handle keyboard inputs  \n🔍 Capture page snapshots to discover all interactive elements  \n📸 Take screenshots and save PDFs  \n🎯 Support for both element-based and coordinate-based interactions  \n\n### Advanced Features\n📑 Tab management for multi-page workflows  \n🌐 Monitor console messages and network requests  \n⏱️ Wait for dynamic content to load  \n📁 Handle file uploads and browser dialogs  \n🔄 Navigate through browser history\n\n## Installation\n\nYou can install the package using any of these methods:\n\nUsing npm:\n```bash\nnpm install -g mcp-accessibility-scanner\n```\n\n### Installation in VS Code\n\nInstall the Accessibility Scanner in VS Code using the VS Code CLI:\n\nFor VS Code:\n```bash\ncode --add-mcp '{\"name\":\"accessibility-scanner\",\"command\":\"npx\",\"args\":[\"mcp-accessibility-scanner\"]}'\n```\n\nFor VS Code Insiders:\n```bash\ncode-insiders --add-mcp '{\"name\":\"accessibility-scanner\",\"command\":\"npx\",\"args\":[\"mcp-accessibility-scanner\"]}'\n```\n\n## Configuration\n\nHere's the Claude Desktop configuration:\n\n```json\n{\n  \"mcpServers\": {\n    \"accessibility-scanner\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"mcp-accessibility-scanner\"]\n    }\n  }\n}\n```\n\n### Advanced Configuration\n\nYou can pass a configuration file to customize Playwright behavior:\n\n```json\n{\n  \"mcpServers\": {\n    \"accessibility-scanner\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"mcp-accessibility-scanner\", \"--config\", \"/path/to/config.json\"]\n    }\n  }\n}\n```\n\n#### Configuration Options\n\nCreate a `config.json` file with the following options:\n\n```json\n{\n  \"browser\": {\n    \"browserName\": \"chromium\",\n    \"launchOptions\": {\n      \"headless\": true,\n      \"channel\": \"chrome\"\n    }\n  },\n  \"timeouts\": {\n    \"navigationTimeout\": 60000,\n    \"defaultTimeout\": 5000\n  },\n  \"network\": {\n    \"allowedOrigins\": [\"example.com\", \"trusted-site.com\"],\n    \"blockedOrigins\": [\"ads.example.com\"]\n  }\n}\n```\n\n**Available Options:**\n\n- `browser.browserName`: Browser to use (`chromium`, `firefox`, `webkit`)\n- `browser.launchOptions.headless`: Run browser in headless mode (default: `true` on Linux without display, `false` otherwise)\n- `browser.launchOptions.channel`: Browser channel (`chrome`, `chrome-beta`, `msedge`, etc.)\n- `timeouts.navigationTimeout`: Maximum time for page navigation in milliseconds (default: `60000`)\n- `timeouts.defaultTimeout`: Default timeout for Playwright operations in milliseconds (default: `5000`)\n- `network.allowedOrigins`: List of origins to allow (blocks all others if specified)\n- `network.blockedOrigins`: List of origins to block\n\n## Available Tools\n\nThe MCP server provides comprehensive browser automation and accessibility scanning tools:\n\n### Core Accessibility Tool\n\n#### `scan_page`\nPerforms a comprehensive accessibility scan on the current page using Axe-core.\n\n**Parameters:**\n- `violationsTag`: Array of WCAG/violation tags to check\n\n**Supported Violation Tags:**\n- WCAG standards: `wcag2a`, `wcag2aa`, `wcag2aaa`, `wcag21a`, `wcag21aa`, `wcag21aaa`, `wcag22a`, `wcag22aa`, `wcag22aaa`\n- Section 508: `section508`\n- Categories: `cat.aria`, `cat.color`, `cat.forms`, `cat.keyboard`, `cat.language`, `cat.name-role-value`, `cat.parsing`, `cat.semantics`, `cat.sensory-and-visual-cues`, `cat.structure`, `cat.tables`, `cat.text-alternatives`, `cat.time-and-media`\n\n### Navigation Tools\n\n#### `browser_navigate`\nNavigate to a URL.\n- Parameters: `url` (string)\n\n#### `browser_navigate_back`\nGo back to the previous page.\n\n#### `browser_navigate_forward`\nGo forward to the next page.\n\n### Page Interaction Tools\n\n#### `browser_snapshot`\nCapture accessibility snapshot of the current page (better than screenshot for analysis).\n\n#### `browser_click`\nPerform click on a web page element.\n- Parameters: `element` (description), `ref` (element reference), `doubleClick` (optional)\n\n#### `browser_type`\nType text into editable element.\n- Parameters: `element`, `ref`, `text`, `submit` (optional), `slowly` (optional)\n\n#### `browser_hover`\nHover over element on page.\n- Parameters: `element`, `ref`\n\n#### `browser_drag`\nPerform drag and drop between two elements.\n- Parameters: `startElement`, `startRef`, `endElement`, `endRef`\n\n#### `browser_select_option`\nSelect an option in a dropdown.\n- Parameters: `element`, `ref`, `values` (array)\n\n#### `browser_press_key`\nPress a key on the keyboard.\n- Parameters: `key` (e.g., 'ArrowLeft' or 'a')\n\n### Screenshot & Visual Tools\n\n#### `browser_take_screenshot`\nTake a screenshot of the current page.\n- Parameters: `raw` (optional), `filename` (optional), `element` (optional), `ref` (optional)\n\n#### `browser_pdf_save`\nSave page as PDF.\n- Parameters: `filename` (optional, defaults to `page-{timestamp}.pdf`)\n\n### Browser Management\n\n#### `browser_close`\nClose the page.\n\n#### `browser_resize`\nResize the browser window.\n- Parameters: `width`, `height`\n\n### Tab Management\n\n#### `browser_tab_list`\nList all open browser tabs.\n\n#### `browser_tab_new`\nOpen a new tab.\n- Parameters: `url` (optional)\n\n#### `browser_tab_select`\nSelect a tab by index.\n- Parameters: `index`\n\n#### `browser_tab_close`\nClose a tab.\n- Parameters: `index` (optional, closes current tab if not provided)\n\n### Information & Monitoring Tools\n\n#### `browser_console_messages`\nReturns all console messages from the page.\n\n#### `browser_network_requests`\nReturns all network requests since loading the page.\n\n### Utility Tools\n\n#### `browser_wait_for`\nWait for text to appear/disappear or time to pass.\n- Parameters: `time` (optional), `text` (optional), `textGone` (optional)\n\n#### `browser_handle_dialog`\nHandle browser dialogs (alerts, confirms, prompts).\n- Parameters: `accept` (boolean), `promptText` (optional)\n\n#### `browser_file_upload`\nUpload files to the page.\n- Parameters: `paths` (array of absolute file paths)\n\n### Vision Mode Tools (Coordinate-based Interaction)\n\n#### `browser_screen_capture`\nTake a screenshot for coordinate-based interaction.\n\n#### `browser_screen_move_mouse`\nMove mouse to specific coordinates.\n- Parameters: `element`, `x`, `y`\n\n#### `browser_screen_click`\nClick at specific coordinates.\n- Parameters: `element`, `x`, `y`\n\n#### `browser_screen_drag`\nDrag from one coordinate to another.\n- Parameters: `element`, `startX`, `startY`, `endX`, `endY`\n\n#### `browser_screen_type`\nType text (coordinate-independent).\n- Parameters: `text`, `submit` (optional)\n\n## Usage Examples\n\n### Basic Accessibility Scan\n```\n1. Navigate to example.com using browser_navigate\n2. Run scan_page with violationsTag: [\"wcag21aa\"]\n```\n\n### Color Contrast Check\n```\n1. Use browser_navigate to go to example.com\n2. Run scan_page with violationsTag: [\"cat.color\"]\n```\n\n### Multi-step Workflow\n```\n1. Navigate to example.com with browser_navigate\n2. Take a browser_snapshot to see available elements\n3. Click the \"Sign In\" button using browser_click\n4. Type \"user@example.com\" using browser_type\n5. Run scan_page on the login page\n6. Take a browser_take_screenshot to capture the final state\n```\n\n### Page Analysis\n```\n1. Navigate to example.com\n2. Use browser_snapshot to capture all interactive elements\n3. Review console messages with browser_console_messages\n4. Check network activity with browser_network_requests\n```\n\n### Tab Management\n```\n1. Open a new tab with browser_tab_new\n2. Navigate to different pages in each tab\n3. Switch between tabs using browser_tab_select\n4. List all tabs with browser_tab_list\n```\n\n### Waiting for Dynamic Content\n```\n1. Navigate to a page\n2. Use browser_wait_for to wait for specific text to appear\n3. Interact with the dynamically loaded content\n```\n\n**Note:** Most interaction tools require element references from browser_snapshot. Always capture a snapshot before attempting to interact with page elements.\n\n## Development\n\nClone and set up the project:\n```bash\ngit clone https://github.com/JustasMonkev/mcp-accessibility-scanner.git\ncd mcp-accessibility-scanner\nnpm install\n```\n\n## License\n\nMIT\n\n",
      "npm_url": "https://www.npmjs.com/package/mcp-accessibility-scanner",
      "npm_downloads": 1851,
      "keywords": [
        "accessibility",
        "scanning",
        "scanner",
        "web accessibility",
        "accessibility scanning",
        "accessibility scanner"
      ],
      "category": "web-search"
    },
    "KBB99--mcp-registry-server": {
      "owner": "KBB99",
      "name": "mcp-registry-server",
      "url": "https://github.com/KBB99/mcp-registry-server",
      "imageUrl": "/freedevtools/mcp/pfp/KBB99.webp",
      "description": "Retrieve MCP Servers through semantic search capabilities, allowing users to find relevant servers efficiently based on queries.",
      "stars": 5,
      "forks": 2,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-07-25T06:11:20Z",
      "readme_content": "# MCP Registry Server\n\n[![smithery badge](https://smithery.ai/badge/@KBB99/mcp-registry-server)](https://smithery.ai/server/@KBB99/mcp-registry-server)\n\n## Features\n\n- **MCP Retriever**: Retrieve MCP Servers using semantic search\n\n<a href=\"https://glama.ai/mcp/servers/8pg7mzcpt8\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/8pg7mzcpt8/badge\" alt=\"mcp-registry-server MCP server\" /></a>\n\n## Tools\n\n- **retrieve_mcps**\n  - Perform retrieval operations against the MCP registry.\n  - Inputs:\n    - `query` (string): The search query for retrieval.\n\n## Configuration\n\n### Installation Guide\n\n### Installing via Smithery\n\nTo install MCP Registry Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@KBB99/mcp-registry-server):\n\n```bash\nnpx -y @smithery/cli install @KBB99/mcp-registry-server --client claude\n```\n\n### Usage with Claude Desktop\n\nFirst build the server:\n\n```bash\ngit clone https://github.com/KBB99/mcp-registry-server.git\ncd mcp-registry-server\nnpm install\nnpm run build\n```\n\nYou can confirm the server is working by running:\n\n```bash\nnode ./dist/index.js\n```\n\nAdd this to your `claude_desktop_config.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"mcp-registry-server\": {\n      \"command\": \"node\",\n      \"args\": [\n        \"./path/to/build/mcp-registry-server/dist/index.js\"\n      ]\n    }\n  }\n}\n```\n\n## License\n\nThis MCP server is licensed under the MIT License. This means you are free to use, modify, and distribute the software, subject to the terms and conditions of the MIT License. For more details, please see the LICENSE file in the project repository.\n\nThis README assumes that your server package is named `mcp-server-retriever`. Adjust the package name and installation details if they differ in your setup. Also, ensure that your server script is correctly built and that all dependencies are properly managed in your `package.json`.\n\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "registry",
        "mcp",
        "kbb99",
        "mcp registry",
        "mcp servers",
        "search kbb99"
      ],
      "category": "web-search"
    },
    "KaanCL--Spotify-MCP-Server": {
      "owner": "KaanCL",
      "name": "Spotify-MCP-Server",
      "url": "https://github.com/KaanCL/Spotify-MCP-Server",
      "imageUrl": "/freedevtools/mcp/pfp/KaanCL.webp",
      "description": "Enables AI assistants and clients to control Spotify playback using the Model-Context-Protocol. Offers functionalities such as searching for songs, playing music, pausing playback, and skipping tracks on Spotify devices.",
      "stars": 1,
      "forks": 1,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-05-05T05:31:15Z",
      "readme_content": "# 🎵 Spotify MCP Server\r\n\r\nWelcome to the **Spotify MCP Server**!  \r\nThis project enables AI assistants and other clients to control Spotify playback via a standardized protocol using the Model-Context-Protocol (MCP) architecture.\r\n\r\n---\r\n\r\n## 🚦 What is MCP?\r\n\r\n**Model-Context-Protocol (MCP)** is a framework that allows AI models and assistants to interact with external tools and services through a standardized interface.  \r\nThis project exposes Spotify functionality as MCP tools, making it accessible to any MCP-compatible client—including AI assistants, bots, and more.\r\n\r\n---\r\n\r\n## ✨ Features\r\n\r\n- 🔍 **Search**: Search for songs on Spotify\r\n- ▶️ **Play**: Play songs on your active Spotify device\r\n- ⏸️ **Pause**: Pause currently playing music\r\n- ⏭️ **Next Track**: Skip to the next track\r\n- ⏮️ **Previous Track**: Go back to the previous track\r\n- 🔊 **Volume Control**: Adjust the playback volume\r\n- 👤 **User Info**: Get current user information\r\n- 📋 **Playlists**: View your Spotify playlists\r\n- 🎵 **Now Playing**: Get information about the currently playing track\r\n\r\n---\r\n\r\n## 🛠️ Prerequisites\r\n\r\n- Python 3.11 or higher\r\n- Spotify Premium account\r\n- Spotify Developer credentials\r\n\r\n---\r\n\r\n## ⚡ Setup\r\n\r\n1. **Clone this repository:**\r\n   ```bash\r\n   git clone https://github.com/KaanCL/Spotify-MCP-Server.git\r\n   cd Spotify-MCP-Server\r\n   ```\r\n\r\n2. **Set up a virtual environment:**\r\n   ```bash\r\n   uv venv\r\n   ```\r\n\r\n3. **Install dependencies:**\r\n   ```bash\r\n   uv sync\r\n   ```\r\n   _If you need to install Spotipy separately:_\r\n   ```bash\r\n   pip install spotipy\r\n   ```\r\n   > **Note:** FastMCP and other dependencies are managed via `uv sync`.\r\n\r\n---\r\n\r\n## 🔑 Configuration\r\n\r\n1. Create a Spotify application at [Spotify Developer Dashboard](https://developer.spotify.com/dashboard)\r\n2. Get your Client ID and Client Secret from your Spotify application\r\n3. Create a `.env` file in the project root with your credentials:\r\n   ```\r\n   CLIENT_ID=your-spotify-client-id\r\n   CLIENT_SECRET=your-spotify-client-secret\r\n   REDIRECT_URI=http://localhost:8888/callback\r\n   ```\r\n\r\n---\r\n\r\n## 🚀 Initialization & First Run\r\n\r\nWhen you run the MCP server for the first time:\r\n\r\n1. A browser window will open asking you to log in to your Spotify account\r\n2. You'll need to authorize the application to access your Spotify account\r\n3. After authorization, you'll be redirected to the callback URL\r\n4. The MCP server will initialize and start listening for connections on port 8080\r\n\r\n---\r\n\r\n## 🏃 Usage\r\n\r\nRun the MCP server:\r\n```bash\r\nuv run mcp install main.py\r\n```\r\n\r\nThe server will initialize and register the following MCP tools:\r\n\r\n| Tool Name           | Description                        | Parameters                |\r\n|---------------------|------------------------------------|---------------------------|\r\n| `search`            | Find songs on Spotify              | `query: str`              |\r\n| `start_playback`    | Play a specific song               | `track_name: str`         |\r\n| `pause_playback`    | Pause the currently playing song   | None                      |\r\n| `resume_playback`   | Resume paused playback             | None                      |\r\n| `next_track`        | Skip to the next track             | None                      |\r\n| `previous_track`    | Go back to the previous track      | None                      |\r\n| `get_user_playlists`| View your Spotify playlists        | None                      |\r\n| `set_player_volume` | Adjust the volume                  | `volume: int` (0-100)     |\r\n| `current_playback`  | Get info about what's playing      | None                      |\r\n| `get_current_user`  | Get user profile information       | None                      |\r\n\r\n---\r\n\r\n## 🧩 Tool Documentation\r\n\r\nEach MCP tool is a Python function decorated with `@mcp.tool()` and is exposed as an API endpoint.  \r\nAll tools return Python dictionaries (or lists of dicts) for easy JSON serialization and integration.\r\n\r\n- **search(query: str) → dict**  \r\n  Search for tracks on Spotify by query string. Returns up to 5 matching tracks, each with name, artist, album, URI, and Spotify URL.\r\n\r\n- **start_playback(track_name: str) → dict**  \r\n  Start playback for a given track name. Searches for the track and starts playback on the user's active device.\r\n\r\n- **pause_playback() → dict**  \r\n  Pause the current playback on the active device.\r\n\r\n- **resume_playback() → dict**  \r\n  Resume playback if paused.\r\n\r\n- **next_track() → dict**  \r\n  Skip to the next track in the current playlist or queue.\r\n\r\n- **previous_track() → dict**  \r\n  Return to the previous track.\r\n\r\n- **get_user_playlists() → dict**  \r\n  Retrieve the user's playlists, including name, URL, ID, and track count.\r\n\r\n- **set_player_volume(volume: int) → dict**  \r\n  Set the Spotify player's volume (0-100).\r\n\r\n- **current_playback() → dict**  \r\n  Get info about the currently playing track, including playback state, track name, artist, album, progress, and duration.\r\n\r\n- **get_current_user() → dict**  \r\n  Get the current Spotify user's profile information (display name, email, user ID).\r\n\r\n---\r\n\r\n## 🤖 MCP Client Example\r\n\r\nAny MCP-compatible client can interact with this server, including AI assistants and programmatic clients:\r\n\r\n```python\r\nfrom mcp.client import MCPClient\r\n\r\n# Connect to the MCP server\r\nclient = MCPClient(\"http://localhost:8080\")\r\n\r\n# Search for a song\r\nresults = client.call(\"search\", {\"query\": \"Money Trees\"})\r\nprint(results)\r\n\r\n# Play a song\r\nclient.call(\"start_playback\", {\"track_name\": \"Money Trees\"})\r\n\r\n# Pause playback\r\nclient.call(\"pause_playback\")\r\n\r\n# Get current playback info\r\nplayback_info = client.call(\"current_playback\")\r\nprint(playback_info)\r\n```\r\n\r\n---\r\n\r\n## ⚠️ Error Handling\r\n\r\nAll API responses are returned as JSON objects.  \r\nIf an error occurs, the response will include an `\"error\"` key with a descriptive message.  \r\n**Examples:**\r\n\r\n```json\r\n{ \"error\": \"No tracks found: No tracks found for your query.\" }\r\n```\r\nor\r\n```json\r\n{ \"error\": \"No active device found: Please open Spotify on a device and try again.\" }\r\n```\r\n\r\n> **Tip:** Always check for the `\"error\"` key in responses before using the data.\r\n\r\n### Error Handling Internals\r\n\r\n- Errors are standardized using a utility function (`format_error`) and an Enum (`SpotifyError`) for clarity and maintainability.\r\n- Common error scenarios include: no active device, no tracks found, nothing playing, invalid volume, and authentication errors.\r\n- All errors are returned in a consistent format for easy client-side handling.\r\n\r\n---\r\n\r\n## 🏗️ Architecture Diagram\r\n\r\n```mermaid\r\ngraph TD\r\n    A[MCP Client] -- HTTP --> B[FastMCP Server]\r\n    B -- Python Call --> C[spotify.py]\r\n    C -- REST API --> D[Spotify Web API]\r\n```\r\n\r\n---\r\n\r\n## 🧠 How It Works: MCP Architecture\r\n\r\n1. **MCP Server**: Registers tools and handles requests.\r\n2. **FastMCP Implementation**: Uses FastMCP for a lightweight, high-performance server.\r\n3. **Spotify Integration**: Connects to the Spotify API using Spotipy.\r\n4. **Tool Registration**: Each Spotify function is registered as an MCP tool with type hints and documentation.\r\n\r\n> The MCP server exposes these tools through a standardized protocol, allowing AI models and other clients to discover and call the tools without needing to understand the underlying Spotify API implementation details.\r\n\r\n---\r\n\r\n## 🧩 Code Quality & Maintainability\r\n\r\n- **Enums**: Used for Spotify scopes and error types for clarity and type safety.\r\n- **Centralized Error Handling**: All errors are formatted using a utility function for consistency.\r\n- **DRY Principle**: Common checks (like active device) are factored into utility functions.\r\n- **Type Hints**: All functions use Python type hints for better code analysis and editor support.\r\n- **Comprehensive Docstrings**: Every function and class is documented for clarity.\r\n- **Extensible**: Add new MCP tools by defining a function in `spotify.py` and registering it with `@mcp.tool()`.\r\n\r\n---\r\n\r\n## 🧩 Extending the Server\r\n\r\nTo add new Spotify features:\r\n1. Implement a new function in `spotify.py` with proper error handling and docstrings.\r\n2. Register the function as an MCP tool in `main.py` using the `@mcp.tool()` decorator.\r\n3. Document the new tool in this README.\r\n\r\n> **Developer Note:**  \r\n> All MCP tools are designed to return Python dictionaries for easy JSON serialization and integration with web APIs or front-end clients.  \r\n> To extend functionality, simply add new `@mcp.tool()` functions following the same pattern as above.  \r\n> For debugging or logging, you can print or log responses within each tool.  \r\n> Type hints and detailed docstrings are provided for all tools to enhance code clarity, maintainability, and discoverability (especially in IDEs).  \r\n> Ensure that the `spotify` module implements the required API interactions.  \r\n> For authentication and authorization, make sure the Spotify API credentials are properly configured in your environment.\r\n\r\n---\r\n\r\n## 🔒 Security Notes\r\n\r\n- Your Spotify credentials are stored locally in the `.env` file\r\n- The `.env` file is excluded from git via `.gitignore`\r\n- The MCP server only accepts connections from localhost by default\r\n- **Never commit your credentials to version control!**\r\n\r\n---\r\n\r\n## 🧩 Troubleshooting\r\n\r\n- **No active device found:** Make sure Spotify is open and active on one of your devices.\r\n- **Authentication errors:** Double-check your `.env` credentials and Spotify Developer Dashboard settings.\r\n- **API rate limits:** Spotify API has rate limits; avoid excessive requests.\r\n- **Unexpected errors:** Check the `\"error\"` key in the response for details.\r\n\r\n---\r\n\r\n## 👨‍💻 Developer & Contribution Notes\r\n\r\n- All functions and tools return dicts or lists for easy JSON serialization.\r\n- Error handling is consistent; always check for `\"error\"` in responses.\r\n- Extend the MCP tools in `main.py` to add new Spotify features.\r\n- For debugging, add logging or print statements as needed.\r\n- Pull requests and suggestions are welcome! Please open an issue first to discuss what you would like to change.\r\n\r\n---\r\n\r\n## 📚 References\r\n\r\n- [Spotify Web API Docs](https://developer.spotify.com/documentation/web-api/)\r\n- [Spotipy Library](https://spotipy.readthedocs.io/)\r\n- [MCP Protocol](https://github.com/microsoft/mcp)\r\n\r\n---\r\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "spotify",
        "playback",
        "music",
        "kaancl spotify",
        "spotify mcp",
        "control spotify"
      ],
      "category": "web-search"
    },
    "Karry-cpu--go-proxy-bingai": {
      "owner": "Karry-cpu",
      "name": "go-proxy-bingai",
      "url": "https://github.com/Karry-cpu/go-proxy-bingai",
      "imageUrl": "/freedevtools/mcp/pfp/Karry-cpu.webp",
      "description": "Access Microsoft Bing AI with a consistent user interface for seamless chat interactions and advanced features such as image generation. The server supports local deployment and customization while facilitating use without the need for a Microsoft account login.",
      "stars": 0,
      "forks": 0,
      "license": "MIT License",
      "language": "",
      "updated_at": "2023-06-30T06:28:29Z",
      "readme_content": "# go-proxy-bing\n\n基于微软 New Bing 用 Vue3 和 Go 简单定制的微软 New Bing 演示站点，拥有一致的 UI 体验，支持 ChatGPT 提示词，国内可用，基本兼容微软 Bing AI 所有功能，无需登录即可畅聊。\n\n⭐ Bing 官方聊天服务器（相对较快和稳定，推荐）不可用时，可参考以下方案\n\n  > 1. 可用 ModHeader 添加 X-Forwarded-For 请求头，对应 URL 是 wss://sydney.bing.com/sydney/ChatHub，具体可参考 [issues #71](https://github.com/adams549659584/go-proxy-bingai/issues/71) 及 https://zhuanlan.zhihu.com/p/606655303\n\n  > 2. 本地部署再部署一份作为聊天中转服务，或下载 Release 直接运行，自定义聊天服务器中填入 http://localhost:8080，并选择。\n\n⭐ 聊天服务器 (暂时默认 Cloudflare Workers，请求数每天限额 100,000，撑不了多久 ，推荐自行部署，参考下面 [部署聊天服务器](#部署聊天服务器) ) 可在右上角 设置 => 服务选择 中切换\n\n⭐ 国内可用 （部署服务器需要直连 www.bing.com 不重定向 CN ，可配置 socks 连接）\n\n⭐ 支持现有开源 ChatGPT 提示词库\n\n⭐ 需要画图等高级功能时(需选更有创造力模式或右上角 设置 => 图像创建 )，可登录微软账号设置用户 Cookie 进行体验\n\n⭐ 遇到一切问题，先点左下角  试试，不行使用刷新大法（Shift + F5 或 Ctrl + Shift + R 或 右上角设置中的一键重置），最终大招就 清理浏览器缓存 及 Cookie ，比如（24 小时限制、未登录提示等等）\n\n- [go-proxy-bing](#go-proxy-bing)\n  - [网页展示](#网页展示)\n  - [侧边栏](#侧边栏)\n  - [演示站点](#演示站点)\n  - [设置用户](#设置用户)\n  - [环境变量](#环境变量)\n  - [部署](#部署)\n    - [Docker](#Docker)\n    - [Release](#Release)\n    - [Railway](#Railway)\n    - [Vercel](#Vercel)\n    - [Render](#Render)\n  - [部署聊天服务器](#部署聊天服务器)\n  - [TODO](#TODO)\n\n## 网页展示\n\n- 电脑端未登录状态\n\n\n\n- 电脑端登录\n\n\n\n\n\n\n- 电脑端画图\n\n> ⭐ 需登录，并选择 更有创造力 对话模式\n\n\n\n- 手机端未登录状态\n\n\n\n## 侧边栏\n\n- 在 Edge 浏览器可把聊天和撰写分别添加侧边栏\n\n\n\n\n\n\n\n## 演示站点\n\n### 甲骨文小鸡仔，轻虐\n\n- https://bing.vcanbb.top\n\n### Railway 搭建\n\n- https://bing-railway.vcanbb.top\n\n- https://go-proxy-bingai-production.up.railway.app\n\n### Vercel 搭建\n\n- https://bing-vercel.vcanbb.top\n\n- https://go-proxy-bingai-adams549659584.vercel.app\n\n### Render 搭建\n\n- https://bing-render.vcanbb.top\n\n- https://go-proxy-bingai.onrender.com\n\n## 设置用户\n\n- 访问 https://www.bing.com/ 或 https://cn.bing.com/ ，登录\n\n- F12 或 Ctrl + Shift + I 打开控制台\n\n- 拿到 Cookie 中 _U 的值 后，在网站设置 => 设置用户 中填入即可。\n\n\n\n## 环境变量\n\n```bash\n# 运行端口 默认 8080 可选\nPORT=8080\n# Socks 环境变量 示例 可选\nGo_Proxy_BingAI_SOCKS_URL=192.168.0.88:1070\n# Socks 账号、密码 可选\nGo_Proxy_BingAI_SOCKS_USER=xxx\nGo_Proxy_BingAI_SOCKS_PWD=xxx\n# 默认用户 Cookie 设置，可选，不推荐使用，固定前缀 Go_Proxy_BingAI_USER_TOKEN 可设置多个，未登录用户将随机使用，多人共用将很快触发图形验证，并很快达到该账号的24小时限制\nGo_Proxy_BingAI_USER_TOKEN_1=xxx\nGo_Proxy_BingAI_USER_TOKEN_2=xxx\nGo_Proxy_BingAI_USER_TOKEN_3=xxx ...\n# 简单授权认证密码，可选\nGo_Proxy_BingAI_AUTH_KEY=xxx\n```\n\n## 部署\n\n> ⭐ 需 https 域名 (自行配置 nginx 等) (前后端都有限制 只有在HTTPS的情况下，浏览器 Accept-Encoding 才会包含 br , localhost 除外)\n\n> 支持 Linux (amd64 / arm64)、Windows (amd64 / arm64)\n\n> 国内机器部署可配置 socks 环境变量\n\n### Docker\n\n> 参考 [Dockerfile](./docker/Dockerfile) 、[docker-compose.yml](./docker/docker-compose.yml)\n\n- docker 示例\n\n```bash\n# 运行容器 监听8080 端口\ndocker run -d -p 8080:8080 --name go-proxy-bingai --restart=unless-stopped adams549659584/go-proxy-bingai\n\n# 配置 socks 环境变量\ndocker run -e Go_Proxy_BingAI_SOCKS_URL=192.168.0.88:1070 -e Go_Proxy_BingAI_SOCKS_USER=xxx -e Go_Proxy_BingAI_SOCKS_PWD=xxx -d -p 8080:8080 --name go-proxy-bingai --restart=unless-stopped adams549659584/go-proxy-bingai\n```\n\n- docker compose 示例\n\n```yaml\nversion: '3'\n\nservices:\n  go-proxy-bingai:\n    # 镜像名称\n    image: adams549659584/go-proxy-bingai\n    # 容器名称\n    container_name: go-proxy-bingai  \n    # 自启动\n    restart: unless-stopped\n    ports:\n      - 8080:8080\n    # environment:\n    #   - Go_Proxy_BingAI_SOCKS_URL=192.168.0.88:1070\n    #   - Go_Proxy_BingAI_SOCKS_USER=xxx\n    #   - Go_Proxy_BingAI_SOCKS_PWD=xxx\n    #   - Go_Proxy_BingAI_USER_TOKEN_1=xxx\n    #   - Go_Proxy_BingAI_USER_TOKEN_2=xxx    \n```\n\n### Release\n\n在 [GitHub Releases](https://github.com/adams549659584/go-proxy-bingai/releases) 下载适用于对应平台的压缩包，解压后可得到可执行文件 go-proxy-bingai，直接运行即可。\n\n### Railway\n\n> 主要配置 Dockerfile 路径 及 端口就可以\n\n```bash\nPORT=8080\nRAILWAY_DOCKERFILE_PATH=docker/Dockerfile\n```\n\n一键部署，点这里 => [![Deploy on Railway](https://railway.app/button.svg)](https://railway.app/template/uIckWS?referralCode=BBs747)\n\n\n\n自行使用 Railway 部署配置如下\n\n\n\n\n\n### Vercel\n\n> ⭐ Vercel 部署不支持 Websocket ，需选择 官方聊天服务器 或 Cloudflare\n\n一键部署，点这里 => [![Deploy with Vercel](https://vercel.com/button)](https://vercel.com/new/clone?repository-url=https://github.com/adams549659584/go-proxy-bingai&project-name=go-proxy-bingai&repository-name=go-proxy-bingai-vercel)\n\n\n\n\n\n### Render\n\n一键部署，点这里 => [![Deploy to Render](https://render.com/images/deploy-to-render-button.svg)](https://render.com/deploy?repo=https://github.com/adams549659584/go-proxy-bingai)\n\n\n\n\n\n## 部署聊天服务器\n\n> 核心代码 [worker.js](./cloudflare/worker.js)\n\n> 具体部署 Cloudflare Workers 教程自行查询，大概如下\n\n- [注册 Cloudflare 账号](https://dash.cloudflare.com/sign-up)\n\n- 创建 Worker 服务，复制 [worker.js](./cloudflare/worker.js) 全部代码，粘贴至创建的服务中，保存并部署。\n\n- 触发器 中自定义访问域名。\n\n## TODO\n\n- [x] 撰写\n- [x] Vue3 重构\n- [x] 提示词\n- [x] 历史聊天\n- [x] 导出消息到本地（Markdown、图片、PDF）\n- [x] 简单访问权限控制",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "bingai",
        "bing",
        "web",
        "bingai access",
        "bing ai",
        "microsoft bing"
      ],
      "category": "web-search"
    },
    "Klavis-AI--klavis": {
      "owner": "Klavis-AI",
      "name": "klavis",
      "url": "https://github.com/Klavis-AI/klavis",
      "imageUrl": "/freedevtools/mcp/pfp/Klavis-AI.webp",
      "description": "Generates visually appealing web reports based on simple search queries, integrating live web search results and storing reports in a database for easy access. Utilizes AI to synthesize information into interactive HTML formats.",
      "stars": 4534,
      "forks": 431,
      "license": "Apache License 2.0",
      "language": "Python",
      "updated_at": "2025-10-03T04:42:04Z",
      "readme_content": "<div align=\"center\">\n  <picture>\n    <img alt=\"klavis_ai\" src=\"https://raw.githubusercontent.com/klavis-ai/klavis/main/static/klavis-ai.png\" width=\"100\">\n  </picture>\n</div>\n\n<h1 align=\"center\">Klavis AI</h1>\n<p align=\"center\"><strong>📦 MCP integration layers that let AI agents use tools reliably at any scale</strong></p>\n\n<div align=\"center\">\n\n[![Documentation](https://img.shields.io/badge/Documentation-📖-green)](https://docs.klavis.ai)\n[![Website](https://img.shields.io/badge/Website-🌐-purple)](https://www.klavis.ai)\n[![Discord](https://img.shields.io/badge/Discord-Join-7289DA?logo=discord&logoColor=white)](https://discord.gg/p7TuTEcssn)\n[![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](LICENSE)\n\n<a href=\"https://www.producthunt.com/products/strata-2?embed=true&utm_source=badge-top-post-badge&utm_medium=badge&utm_source=badge-strata&#0045;2\" target=\"_blank\"><img src=\"https://api.producthunt.com/widgets/embed-image/v1/top-post-badge.svg?post_id=1016948&theme=light&period=daily&t=1758639605639\" alt=\"Strata - One&#0032;MCP&#0032;server&#0032;for&#0032;AI&#0032;agents&#0032;to&#0032;handle&#0032;thousands&#0032;of&#0032;tools | Product Hunt\" style=\"width: 250px; height: 54px;\" width=\"250\" height=\"54\" /></a>\n\n</div>\n\n## 🎯 Choose Your Solution\n\n<div align=\"center\">\n  <table>\n    <tr>\n      <td align=\"center\" width=\"50%\" valign=\"top\" style=\"vertical-align: top; height: 250px;\">\n        <div style=\"height: 100%; display: flex; flex-direction: column; justify-content: space-between;\">\n          <div>\n            <h2>📦 Strata</h2>\n            <p><strong>Unified MCP Router</strong></p>\n            <p>One MCP server for AI agents to use tools reliably at any scale</p>\n          </div>\n          <div>\n            <a href=\"open-strata/README.md\">\n              <img alt=\"svg_xml_base64_PHN2ZyB3aWR0aD0iMjQiIGhlaWdodD0iMjQiIHZpZXdCb3g9IjAgMCAyNCAyNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPHJlY3QgeD0iMiIgeT0iMiIgd2lkdGg9IjIwIiBoZWlnaHQ9IjIwIiByeD0iNCIgcnk9IjQiIHN0cm9rZT0id2hpdGUiIHN0cm9rZS13aWR0aD0iMiIgc3Ryb2tlLWxpbmVjYXA9InJvdW5kIiBzdHJva2UtbGluZWpvaW49InJvdW5kIi8_CjxyZWN0IHg9IjYiIHk9IjYiIHdpZHRoPSI0IiBoZWlnaHQ9IjQiIHJ4PSIxIiByeT0iMSIgZmlsbD0id2hpdGUiLz4KPHJlY3QgeD0iMTQiIHk9IjYiIHdpZHRoPSI0IiBoZWlnaHQ9IjQiIHJ4PSIxIiByeT0iMSIgZmlsbD0id2hpdGUiLz4KPHJlY3QgeD0iNiIgeT0iMTQiIHdpZHRoPSI0IiBoZWlnaHQ9IjQiIHJ4PSIxIiByeT0iMSIgZmlsbD0id2hpdGUiLz4KPHJlY3QgeD0iMTQiIHk9IjE0IiB3aWR0aD0iNCIgaGVpZ2h0PSI0IiByeD0iMSIgcnk9IjEiIGZpbGw9IndoaXRlIi8_Cjwvc3ZnPg\" src=\"https://img.shields.io/badge/Explore-Strata-blue?style=for-the-badge&logo=data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjQiIGhlaWdodD0iMjQiIHZpZXdCb3g9IjAgMCAyNCAyNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPHJlY3QgeD0iMiIgeT0iMiIgd2lkdGg9IjIwIiBoZWlnaHQ9IjIwIiByeD0iNCIgcnk9IjQiIHN0cm9rZT0id2hpdGUiIHN0cm9rZS13aWR0aD0iMiIgc3Ryb2tlLWxpbmVjYXA9InJvdW5kIiBzdHJva2UtbGluZWpvaW49InJvdW5kIi8+CjxyZWN0IHg9IjYiIHk9IjYiIHdpZHRoPSI0IiBoZWlnaHQ9IjQiIHJ4PSIxIiByeT0iMSIgZmlsbD0id2hpdGUiLz4KPHJlY3QgeD0iMTQiIHk9IjYiIHdpZHRoPSI0IiBoZWlnaHQ9IjQiIHJ4PSIxIiByeT0iMSIgZmlsbD0id2hpdGUiLz4KPHJlY3QgeD0iNiIgeT0iMTQiIHdpZHRoPSI0IiBoZWlnaHQ9IjQiIHJ4PSIxIiByeT0iMSIgZmlsbD0id2hpdGUiLz4KPHJlY3QgeD0iMTQiIHk9IjE0IiB3aWR0aD0iNCIgaGVpZ2h0PSI0IiByeD0iMSIgcnk9IjEiIGZpbGw9IndoaXRlIi8+Cjwvc3ZnPg==\" height=\"40\">\n            </a>\n          </div>\n        </div>\n      </td>\n      <td align=\"center\" width=\"50%\" valign=\"top\" style=\"vertical-align: top; height: 250px;\">\n        <div style=\"height: 100%; display: flex; flex-direction: column; justify-content: space-between;\">\n          <div>\n            <h2>🛠️ MCP Integrations</h2>\n            <p><strong>50+ Production MCP Servers</strong></p>\n            <p>Self-hosted or managed MCP servers with enterprise OAuth support for all major services</p>\n          </div>\n          <div>\n            <a href=\"mcp_servers/README.md\">\n              <img alt=\"svg_xml_base64_PHN2ZyB3aWR0aD0iMjQiIGhlaWdodD0iMjQiIHZpZXdCb3g9IjAgMCAyNCAyNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPHBhdGggZD0iTTIwLjUgN0gzLjVDMi42NzE1NyA3IDIgNy42NzE1NyAyIDguNVYxNS41QzIgMTYuMzI4NCAyLjY3MTU3IDE3IDMuNSAxN0gyMC41QzIxLjMyODQgMTcgMjIgMTYuMzI4NCAyMiAxNS41VjguNUMyMiA3LjY3MTU3IDIxLjMyODQgNyAyMC41IDdaIiBzdHJva2U9IndoaXRlIiBzdHJva2Utd2lkdGg9IjIiIHN0cm9rZS1saW5lY2FwPSJyb3VuZCIgc3Ryb2tlLWxpbmVqb2luPSJyb3VuZCIvPgo8cGF0aCBkPSJNNiAxMkgxOCIgc3Ryb2tlPSJ3aGl0ZSIgc3Ryb2tlLXdpZHRoPSIyIiBzdHJva2UtbGluZWNhcD0icm91bmQiLz4KPGNpcmNsZSBjeD0iNSIgY3k9IjEyIiByPSIxIiBmaWxsPSJ3aGl0ZSIvPgo8Y2lyY2xlIGN4PSIxOSIgY3k9IjEyIiByPSIxIiBmaWxsPSJ3aGl0ZSIvPgo8L3N2Zz4\" src=\"https://img.shields.io/badge/Explore-MCP%20Servers-purple?style=for-the-badge&logo=data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjQiIGhlaWdodD0iMjQiIHZpZXdCb3g9IjAgMCAyNCAyNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPHBhdGggZD0iTTIwLjUgN0gzLjVDMi42NzE1NyA3IDIgNy42NzE1NyAyIDguNVYxNS41QzIgMTYuMzI4NCAyLjY3MTU3IDE3IDMuNSAxN0gyMC41QzIxLjMyODQgMTcgMjIgMTYuMzI4NCAyMiAxNS41VjguNUMyMiA3LjY3MTU3IDIxLjMyODQgNyAyMC41IDdaIiBzdHJva2U9IndoaXRlIiBzdHJva2Utd2lkdGg9IjIiIHN0cm9rZS1saW5lY2FwPSJyb3VuZCIgc3Ryb2tlLWxpbmVqb2luPSJyb3VuZCIvPgo8cGF0aCBkPSJNNiAxMkgxOCIgc3Ryb2tlPSJ3aGl0ZSIgc3Ryb2tlLXdpZHRoPSIyIiBzdHJva2UtbGluZWNhcD0icm91bmQiLz4KPGNpcmNsZSBjeD0iNSIgY3k9IjEyIiByPSIxIiBmaWxsPSJ3aGl0ZSIvPgo8Y2lyY2xlIGN4PSIxOSIgY3k9IjEyIiByPSIxIiBmaWxsPSJ3aGl0ZSIvPgo8L3N2Zz4=\" height=\"40\">\n            </a>\n          </div>\n        </div>\n      </td>\n    </tr>\n  </table>\n</div>\n\n## Strata\n\nStrata is one MCP server that guides your AI agents use tools reliably progressively at any scale.\n\n### Why Strata?\n\n🎯 **Scalable Tool Integration** → Beyond 40-50 tool limits  \n🚀 **Progressive Discovery** → Guides agents from intent to action, step-by-step.\n\n[📖 **Learn More** →](https://docs.klavis.ai/documentation/concepts/strata)\n\n## MCP Integrations\n\n**50+ production MCP servers. OAuth included. Deploy anywhere.**\n\nConnect your AI to GitHub, Gmail, Slack, Salesforce, and more - all with enterprise OAuth and Docker support.\n\n🔐 **Real OAuth** → Not just API keys  \n🐳 **Docker ready** → One-line deploy  \n\n[🌐 **Browse All Servers** →](https://docs.klavis.ai/documentation/mcp-server/overview)\n\n## 🚀 Quick Start\n\n### Option 1: Open Source\n\nSelf-host everything on your own infrastructure:\n\n```bash\n# Run any MCP Integration\ndocker pull ghcr.io/klavis-ai/github-mcp-server:latest\ndocker run -p 5000:5000 ghcr.io/klavis-ai/github-mcp-server:latest\n\n# Install Open Source Strata locally\npipx install strata-mcp\nstrata add --type stdio playwright npx @playwright/mcp@latest\n```\n\n### Option 2: Use Hosted Service by WebUI\n\nGet instant access without any setup:\n\n1. **Sign Up**: [Create account →](https://www.klavis.ai/auth/sign-up)\n2. **Get Started**: [Follow quickstart guide →](https://docs.klavis.ai/documentation/quickstart)\n3. **Use Strata or individual MCP servers** in Claude Code, Cursor, VSCode, etc.\n\nReady in under 2 minutes! 🚀\n\n### Option 3: SDK\n\nBuild custom applications with our SDKs:\n\n```python\n# Python SDK\nfrom klavis import Klavis\nfrom klavis.types import McpServerName\n\nklavis = Klavis(api_key=\"your-key\")\n\n# Create Strata instance\nstrata = klavis.mcp_server.create_strata_server(\n    user_id=\"user123\",\n    servers=[McpServerName.GMAIL, McpServerName.YOUTUBE],\n)\n\n# Or use individual MCP servers\ngmail = klavis.mcp_server.create_server_instance(\n    server_name=McpServerName.GMAIL,\n    user_id=\"user123\",\n)\n```\n\n```typescript\n// TypeScript SDK\nimport { KlavisClient, McpServerName } from 'klavis';\n\nconst klavis = new KlavisClient({ apiKey: 'your-api-key' });\n\n// Create Strata instance\nconst strata = await klavis.mcpServer.createStrataServer({\n    userId: \"user123\",\n    servers: [McpServerName.GMAIL, McpServerName.YOUTUBE]\n});\n\n// Or use individual MCP servers\nconst gmail = await klavis.mcpServer.createServerInstance({\n    serverName: McpServerName.GMAIL,\n    userId: \"user123\"\n});\n```\n\n### Option 4: Direct API\n\nUse REST API for any programming language:\n\n```bash\n# Create Strata server\ncurl -X POST \"https://api.klavis.ai/v1/mcp-server/strata\" \\\n  -H \"Authorization: Bearer your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"user_id\": \"user123\",\n    \"servers\": [\"GMAIL\", \"YOUTUBE\"]\n  }'\n\n# Create individual MCP server\ncurl -X POST \"https://api.klavis.ai/v1/mcp-server/instance\" \\\n  -H \"Authorization: Bearer your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"server_name\": \"GMAIL\",\n    \"user_id\": \"user123\"\n  }'\n```\n\n[📖 **Complete Documentation** →](https://docs.klavis.ai/documentation/quickstart)\n\n\n## 📚 Resources\n\n- 📖 [Documentation](https://docs.klavis.ai)\n- 💬 [Discord Community](https://discord.gg/p7TuTEcssn)\n- 🐛 [Report Issues](https://github.com/klavis-ai/klavis/issues)\n- 🌐 [Klavis AI Website](https://www.klavis.ai)\n\n## 📜 License\n\n- **Root Repository**: Apache 2.0 license - see [LICENSE](LICENSE)\n\n---\n\n<div align=\"center\">\n  <p><strong>Klavis AI (YC X25) 🚀 Empowering AI with Seamless Integration</strong></p>\n</div>",
      "npm_url": "https://www.npmjs.com/package/klavis",
      "npm_downloads": 16908,
      "keywords": [
        "klavis",
        "html",
        "search",
        "search klavis",
        "web search",
        "web reports"
      ],
      "category": "web-search"
    },
    "Kotelberg--playwright-mcp-server": {
      "owner": "Kotelberg",
      "name": "playwright-mcp-server",
      "url": "https://github.com/Kotelberg/playwright-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/Kotelberg.webp",
      "description": "Retrieve web page content and interact with it by simulating user actions, navigating pages, and extracting data using a standardized protocol.",
      "stars": 0,
      "forks": 0,
      "license": "No License",
      "language": "",
      "updated_at": "2025-03-15T10:13:00Z",
      "readme_content": "# Playwright MCP Server\n\n[![smithery badge](https://smithery.ai/badge/@showfive/playwright-mcp-server)](https://smithery.ai/server/@showfive/playwright-mcp-server)\n\nEnglish | [日本語](README.ja.md)\n\nThis project is a server that provides Playwright web page content retrieval functionality using the Model Context Protocol (MCP).\n\n## Features\n\n- Page navigation\n- Full page content retrieval\n- Visible content retrieval\n- Interactive elements detection\n- Mouse operation simulation\n- Echo functionality for testing\n\n## Installation\n\n### Installing via Smithery\n\nTo install Playwright MCP Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@showfive/playwright-mcp-server):\n\n```bash\nnpx -y @smithery/cli install @showfive/playwright-mcp-server --client claude\n```\n\n### Manual Installation\n```bash\nnpm install\n```\n\n## Usage\n\n### Starting the Server\n\n```bash\nnpm run build\nnpm start\n```\n\n### MCP Tools\n\nThe following tools are available:\n\n1. `navigate`\n   - Navigate to a specified URL\n   - Arguments: `{ url: string }`\n   - Returns: Navigation result\n\n2. `get_all_content`\n   - Retrieve content from the entire page\n   - Arguments: None\n   - Returns: All text content from the page\n\n3. `get_visible_content`\n   - Retrieve currently visible content\n   - Arguments: `{ minVisiblePercentage?: number }`\n   - Returns: Visible text content\n\n4. `get_interactive_elements`\n   - Get position information of interactive elements (buttons, links, etc.) on the page\n   - Arguments: None\n   - Returns: Coordinates and boundary information of interactive elements\n\n5. `move_mouse`\n   - Move mouse cursor to specified coordinates\n   - Arguments: `{ x: number, y: number }`\n   - Returns: Operation result\n\n6. `mouse_click`\n   - Execute mouse click at specified coordinates\n   - Arguments: `{ x: number, y: number, button?: \"left\" | \"right\" | \"middle\", clickCount?: number }`\n   - Returns: Click operation result\n\n7. `mouse_wheel`\n   - Execute mouse wheel scrolling\n   - Arguments: `{ deltaY: number, deltaX?: number }`\n   - Returns: Scroll operation result\n\n8. `drag_and_drop`\n   - Execute drag and drop operation\n   - Arguments: `{ sourceX: number, sourceY: number, targetX: number, targetY: number }`\n   - Returns: Drag and drop operation result\n\n9. `echo`\n   - Echo tool for testing\n   - Arguments: `{ message: string }`\n   - Returns: Sent message\n\n## Development\n\n### Running Tests\n\n```bash\n# Run all tests\nnpm test\n\n# Run tests in watch mode\nnpm run test:watch\n\n# Generate coverage report\nnpm run test:coverage\n```\n\n### Test Structure\n\n- `tools/*.test.ts`: Function tests for each tool\n- `mcp-server.test.ts`: MCP server function tests\n\n## Implementation Features\n\n1. Content Retrieval\n   - Full page content retrieval\n   - Visible content only retrieval\n   - Proper HTML parsing\n\n2. Interaction\n   - Detection and position information retrieval of interactive elements\n   - Mouse operation simulation (movement, clicks, scrolling)\n   - Drag and drop support\n\n3. Error Handling\n   - Proper navigation error handling\n   - Timeout processing\n   - Invalid URL detection\n\n4. Configuration Flexibility\n   - Headless/head mode selection\n   - Custom user agent\n   - Viewport size settings\n\n## Important Notes\n\n- Ensure necessary environment variables are set before using the MCP server\n- Follow the terms of service of target websites when retrieving web page content\n- Maintain appropriate intervals when sending multiple requests\n- When performing mouse operations, maintain appropriate intervals as they simulate actual user interactions\n\n## License\n\nISC\n",
      "npm_url": "https://www.npmjs.com/package/playwright-mcp-server",
      "npm_downloads": 5469,
      "keywords": [
        "web",
        "pages",
        "mcp",
        "mcp server",
        "web search",
        "retrieve web"
      ],
      "category": "web-search"
    },
    "Krieg2065--firecrawl-mcp-server": {
      "owner": "Krieg2065",
      "name": "firecrawl-mcp-server",
      "url": "https://github.com/Krieg2065/firecrawl-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/Krieg2065.webp",
      "description": "Enables advanced web scraping and content extraction, supporting JavaScript rendering and batch processing for deep research. It includes features for automatic retries and efficient data integration while monitoring credit usage and handling rate limits.",
      "stars": 2,
      "forks": 1,
      "license": "MIT License",
      "language": "",
      "updated_at": "2025-08-15T01:07:39Z",
      "readme_content": "# Firecrawl MCP Server\n\nA Model Context Protocol (MCP) server implementation that integrates with [Firecrawl](https://github.com/mendableai/firecrawl) for web scraping capabilities.\n\n> Big thanks to [@vrknetha](https://github.com/vrknetha), [@cawstudios](https://caw.tech) for the initial implementation!\n>\n> You can also play around with [our MCP Server on MCP.so's playground](https://mcp.so/playground?server=firecrawl-mcp-server). Thanks to MCP.so for hosting and [@gstarwd](https://github.com/gstarwd) for integrating our server.\n\n## Features\n\n- Scrape, crawl, search, extract, deep research and batch scrape support\n- Web scraping with JS rendering\n- URL discovery and crawling\n- Web search with content extraction\n- Automatic retries with exponential backoff\n  - Efficient batch processing with built-in rate limiting\n- Credit usage monitoring for cloud API\n- Comprehensive logging system\n- Support for cloud and self-hosted Firecrawl instances\n- Mobile/Desktop viewport support\n- Smart content filtering with tag inclusion/exclusion\n\n## Installation\n\n### Running with npx\n\n```bash\nenv FIRECRAWL_API_KEY=fc-YOUR_API_KEY npx -y firecrawl-mcp\n```\n\n### Manual Installation\n\n```bash\nnpm install -g firecrawl-mcp\n```\n\n### Running on Cursor\n\nConfiguring Cursor 🖥️\nNote: Requires Cursor version 0.45.6+\nFor the most up-to-date configuration instructions, please refer to the official Cursor documentation on configuring MCP servers:\n[Cursor MCP Server Configuration Guide](https://docs.cursor.com/context/model-context-protocol#configuring-mcp-servers)\n\nTo configure Firecrawl MCP in Cursor **v0.45.6**\n\n1. Open Cursor Settings\n2. Go to Features > MCP Servers\n3. Click \"+ Add New MCP Server\"\n4. Enter the following:\n   - Name: \"firecrawl-mcp\" (or your preferred name)\n   - Type: \"command\"\n   - Command: `env FIRECRAWL_API_KEY=your-api-key npx -y firecrawl-mcp`\n\nTo configure Firecrawl MCP in Cursor **v0.48.6**\n\n1. Open Cursor Settings\n2. Go to Features > MCP Servers\n3. Click \"+ Add new global MCP server\"\n4. Enter the following code:\n   ```json\n   {\n     \"mcpServers\": {\n       \"firecrawl-mcp\": {\n         \"command\": \"npx\",\n         \"args\": [\"-y\", \"firecrawl-mcp\"],\n         \"env\": {\n           \"FIRECRAWL_API_KEY\": \"YOUR-API-KEY\"\n         }\n       }\n     }\n   }\n   ```\n\n> If you are using Windows and are running into issues, try `cmd /c \"set FIRECRAWL_API_KEY=your-api-key && npx -y firecrawl-mcp\"`\n\nReplace `your-api-key` with your Firecrawl API key. If you don't have one yet, you can create an account and get it from https://www.firecrawl.dev/app/api-keys\n\nAfter adding, refresh the MCP server list to see the new tools. The Composer Agent will automatically use Firecrawl MCP when appropriate, but you can explicitly request it by describing your web scraping needs. Access the Composer via Command+L (Mac), select \"Agent\" next to the submit button, and enter your query.\n\n### Running on Windsurf\n\nAdd this to your `./codeium/windsurf/model_config.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"mcp-server-firecrawl\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"firecrawl-mcp\"],\n      \"env\": {\n        \"FIRECRAWL_API_KEY\": \"YOUR_API_KEY\"\n      }\n    }\n  }\n}\n```\n\n### Installing via Smithery (Legacy)\n\nTo install Firecrawl for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@mendableai/mcp-server-firecrawl):\n\n```bash\nnpx -y @smithery/cli install @mendableai/mcp-server-firecrawl --client claude\n```\n\n## Configuration\n\n### Environment Variables\n\n#### Required for Cloud API\n\n- `FIRECRAWL_API_KEY`: Your Firecrawl API key\n  - Required when using cloud API (default)\n  - Optional when using self-hosted instance with `FIRECRAWL_API_URL`\n- `FIRECRAWL_API_URL` (Optional): Custom API endpoint for self-hosted instances\n  - Example: `https://firecrawl.your-domain.com`\n  - If not provided, the cloud API will be used (requires API key)\n\n#### Optional Configuration\n\n##### Retry Configuration\n\n- `FIRECRAWL_RETRY_MAX_ATTEMPTS`: Maximum number of retry attempts (default: 3)\n- `FIRECRAWL_RETRY_INITIAL_DELAY`: Initial delay in milliseconds before first retry (default: 1000)\n- `FIRECRAWL_RETRY_MAX_DELAY`: Maximum delay in milliseconds between retries (default: 10000)\n- `FIRECRAWL_RETRY_BACKOFF_FACTOR`: Exponential backoff multiplier (default: 2)\n\n##### Credit Usage Monitoring\n\n- `FIRECRAWL_CREDIT_WARNING_THRESHOLD`: Credit usage warning threshold (default: 1000)\n- `FIRECRAWL_CREDIT_CRITICAL_THRESHOLD`: Credit usage critical threshold (default: 100)\n\n### Configuration Examples\n\nFor cloud API usage with custom retry and credit monitoring:\n\n```bash\n# Required for cloud API\nexport FIRECRAWL_API_KEY=your-api-key\n\n# Optional retry configuration\nexport FIRECRAWL_RETRY_MAX_ATTEMPTS=5        # Increase max retry attempts\nexport FIRECRAWL_RETRY_INITIAL_DELAY=2000    # Start with 2s delay\nexport FIRECRAWL_RETRY_MAX_DELAY=30000       # Maximum 30s delay\nexport FIRECRAWL_RETRY_BACKOFF_FACTOR=3      # More aggressive backoff\n\n# Optional credit monitoring\nexport FIRECRAWL_CREDIT_WARNING_THRESHOLD=2000    # Warning at 2000 credits\nexport FIRECRAWL_CREDIT_CRITICAL_THRESHOLD=500    # Critical at 500 credits\n```\n\nFor self-hosted instance:\n\n```bash\n# Required for self-hosted\nexport FIRECRAWL_API_URL=https://firecrawl.your-domain.com\n\n# Optional authentication for self-hosted\nexport FIRECRAWL_API_KEY=your-api-key  # If your instance requires auth\n\n# Custom retry configuration\nexport FIRECRAWL_RETRY_MAX_ATTEMPTS=10\nexport FIRECRAWL_RETRY_INITIAL_DELAY=500     # Start with faster retries\n```\n\n### Usage with Claude Desktop\n\nAdd this to your `claude_desktop_config.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"mcp-server-firecrawl\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"firecrawl-mcp\"],\n      \"env\": {\n        \"FIRECRAWL_API_KEY\": \"YOUR_API_KEY_HERE\",\n\n        \"FIRECRAWL_RETRY_MAX_ATTEMPTS\": \"5\",\n        \"FIRECRAWL_RETRY_INITIAL_DELAY\": \"2000\",\n        \"FIRECRAWL_RETRY_MAX_DELAY\": \"30000\",\n        \"FIRECRAWL_RETRY_BACKOFF_FACTOR\": \"3\",\n\n        \"FIRECRAWL_CREDIT_WARNING_THRESHOLD\": \"2000\",\n        \"FIRECRAWL_CREDIT_CRITICAL_THRESHOLD\": \"500\"\n      }\n    }\n  }\n}\n```\n\n### System Configuration\n\nThe server includes several configurable parameters that can be set via environment variables. Here are the default values if not configured:\n\n```typescript\nconst CONFIG = {\n  retry: {\n    maxAttempts: 3, // Number of retry attempts for rate-limited requests\n    initialDelay: 1000, // Initial delay before first retry (in milliseconds)\n    maxDelay: 10000, // Maximum delay between retries (in milliseconds)\n    backoffFactor: 2, // Multiplier for exponential backoff\n  },\n  credit: {\n    warningThreshold: 1000, // Warn when credit usage reaches this level\n    criticalThreshold: 100, // Critical alert when credit usage reaches this level\n  },\n};\n```\n\nThese configurations control:\n\n1. **Retry Behavior**\n\n   - Automatically retries failed requests due to rate limits\n   - Uses exponential backoff to avoid overwhelming the API\n   - Example: With default settings, retries will be attempted at:\n     - 1st retry: 1 second delay\n     - 2nd retry: 2 seconds delay\n     - 3rd retry: 4 seconds delay (capped at maxDelay)\n\n2. **Credit Usage Monitoring**\n   - Tracks API credit consumption for cloud API usage\n   - Provides warnings at specified thresholds\n   - Helps prevent unexpected service interruption\n   - Example: With default settings:\n     - Warning at 1000 credits remaining\n     - Critical alert at 100 credits remaining\n\n### Rate Limiting and Batch Processing\n\nThe server utilizes Firecrawl's built-in rate limiting and batch processing capabilities:\n\n- Automatic rate limit handling with exponential backoff\n- Efficient parallel processing for batch operations\n- Smart request queuing and throttling\n- Automatic retries for transient errors\n\n## Available Tools\n\n### 1. Scrape Tool (`firecrawl_scrape`)\n\nScrape content from a single URL with advanced options.\n\n```json\n{\n  \"name\": \"firecrawl_scrape\",\n  \"arguments\": {\n    \"url\": \"https://example.com\",\n    \"formats\": [\"markdown\"],\n    \"onlyMainContent\": true,\n    \"waitFor\": 1000,\n    \"timeout\": 30000,\n    \"mobile\": false,\n    \"includeTags\": [\"article\", \"main\"],\n    \"excludeTags\": [\"nav\", \"footer\"],\n    \"skipTlsVerification\": false\n  }\n}\n```\n\n### 2. Batch Scrape Tool (`firecrawl_batch_scrape`)\n\nScrape multiple URLs efficiently with built-in rate limiting and parallel processing.\n\n```json\n{\n  \"name\": \"firecrawl_batch_scrape\",\n  \"arguments\": {\n    \"urls\": [\"https://example1.com\", \"https://example2.com\"],\n    \"options\": {\n      \"formats\": [\"markdown\"],\n      \"onlyMainContent\": true\n    }\n  }\n}\n```\n\nResponse includes operation ID for status checking:\n\n```json\n{\n  \"content\": [\n    {\n      \"type\": \"text\",\n      \"text\": \"Batch operation queued with ID: batch_1. Use firecrawl_check_batch_status to check progress.\"\n    }\n  ],\n  \"isError\": false\n}\n```\n\n### 3. Check Batch Status (`firecrawl_check_batch_status`)\n\nCheck the status of a batch operation.\n\n```json\n{\n  \"name\": \"firecrawl_check_batch_status\",\n  \"arguments\": {\n    \"id\": \"batch_1\"\n  }\n}\n```\n\n### 4. Search Tool (`firecrawl_search`)\n\nSearch the web and optionally extract content from search results.\n\n```json\n{\n  \"name\": \"firecrawl_search\",\n  \"arguments\": {\n    \"query\": \"your search query\",\n    \"limit\": 5,\n    \"lang\": \"en\",\n    \"country\": \"us\",\n    \"scrapeOptions\": {\n      \"formats\": [\"markdown\"],\n      \"onlyMainContent\": true\n    }\n  }\n}\n```\n\n### 5. Crawl Tool (`firecrawl_crawl`)\n\nStart an asynchronous crawl with advanced options.\n\n```json\n{\n  \"name\": \"firecrawl_crawl\",\n  \"arguments\": {\n    \"url\": \"https://example.com\",\n    \"maxDepth\": 2,\n    \"limit\": 100,\n    \"allowExternalLinks\": false,\n    \"deduplicateSimilarURLs\": true\n  }\n}\n```\n\n### 6. Extract Tool (`firecrawl_extract`)\n\nExtract structured information from web pages using LLM capabilities. Supports both cloud AI and self-hosted LLM extraction.\n\n```json\n{\n  \"name\": \"firecrawl_extract\",\n  \"arguments\": {\n    \"urls\": [\"https://example.com/page1\", \"https://example.com/page2\"],\n    \"prompt\": \"Extract product information including name, price, and description\",\n    \"systemPrompt\": \"You are a helpful assistant that extracts product information\",\n    \"schema\": {\n      \"type\": \"object\",\n      \"properties\": {\n        \"name\": { \"type\": \"string\" },\n        \"price\": { \"type\": \"number\" },\n        \"description\": { \"type\": \"string\" }\n      },\n      \"required\": [\"name\", \"price\"]\n    },\n    \"allowExternalLinks\": false,\n    \"enableWebSearch\": false,\n    \"includeSubdomains\": false\n  }\n}\n```\n\nExample response:\n\n```json\n{\n  \"content\": [\n    {\n      \"type\": \"text\",\n      \"text\": {\n        \"name\": \"Example Product\",\n        \"price\": 99.99,\n        \"description\": \"This is an example product description\"\n      }\n    }\n  ],\n  \"isError\": false\n}\n```\n\n#### Extract Tool Options:\n\n- `urls`: Array of URLs to extract information from\n- `prompt`: Custom prompt for the LLM extraction\n- `systemPrompt`: System prompt to guide the LLM\n- `schema`: JSON schema for structured data extraction\n- `allowExternalLinks`: Allow extraction from external links\n- `enableWebSearch`: Enable web search for additional context\n- `includeSubdomains`: Include subdomains in extraction\n\nWhen using a self-hosted instance, the extraction will use your configured LLM. For cloud API, it uses Firecrawl's managed LLM service.\n\n### 7. Deep Research Tool (firecrawl_deep_research)\n\nConduct deep web research on a query using intelligent crawling, search, and LLM analysis.\n\n```json\n{\n  \"name\": \"firecrawl_deep_research\",\n  \"arguments\": {\n    \"query\": \"how does carbon capture technology work?\",\n    \"maxDepth\": 3,\n    \"timeLimit\": 120,\n    \"maxUrls\": 50\n  }\n}\n```\n\nArguments:\n\n- query (string, required): The research question or topic to explore.\n- maxDepth (number, optional): Maximum recursive depth for crawling/search (default: 3).\n- timeLimit (number, optional): Time limit in seconds for the research session (default: 120).\n- maxUrls (number, optional): Maximum number of URLs to analyze (default: 50).\n\nReturns:\n\n- Final analysis generated by an LLM based on research. (data.finalAnalysis)\n- May also include structured activities and sources used in the research process.\n\n### 8. Generate LLMs.txt Tool (firecrawl_generate_llmstxt)\n\nGenerate a standardized llms.txt (and optionally llms-full.txt) file for a given domain. This file defines how large language models should interact with the site.\n\n```json\n{\n  \"name\": \"firecrawl_generate_llmstxt\",\n  \"arguments\": {\n    \"url\": \"https://example.com\",\n    \"maxUrls\": 20,\n    \"showFullText\": true\n  }\n}\n```\n\nArguments:\n\n- url (string, required): The base URL of the website to analyze.\n- maxUrls (number, optional): Max number of URLs to include (default: 10).\n- showFullText (boolean, optional): Whether to include llms-full.txt contents in the response.\n\nReturns:\n\n- Generated llms.txt file contents and optionally the llms-full.txt (data.llmstxt and/or data.llmsfulltxt)\n\n## Logging System\n\nThe server includes comprehensive logging:\n\n- Operation status and progress\n- Performance metrics\n- Credit usage monitoring\n- Rate limit tracking\n- Error conditions\n\nExample log messages:\n\n```\n[INFO] Firecrawl MCP Server initialized successfully\n[INFO] Starting scrape for URL: https://example.com\n[INFO] Batch operation queued with ID: batch_1\n[WARNING] Credit usage has reached warning threshold\n[ERROR] Rate limit exceeded, retrying in 2s...\n```\n\n## Error Handling\n\nThe server provides robust error handling:\n\n- Automatic retries for transient errors\n- Rate limit handling with backoff\n- Detailed error messages\n- Credit usage warnings\n- Network resilience\n\nExample error response:\n\n```json\n{\n  \"content\": [\n    {\n      \"type\": \"text\",\n      \"text\": \"Error: Rate limit exceeded. Retrying in 2 seconds...\"\n    }\n  ],\n  \"isError\": true\n}\n```\n\n## Development\n\n```bash\n# Install dependencies\nnpm install\n\n# Build\nnpm run build\n\n# Run tests\nnpm test\n```\n\n### Contributing\n\n1. Fork the repository\n2. Create your feature branch\n3. Run tests: `npm test`\n4. Submit a pull request\n\n## License\n\nMIT License - see LICENSE file for details\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "scraping",
        "firecrawl",
        "search",
        "web scraping",
        "scraping content",
        "firecrawl mcp"
      ],
      "category": "web-search"
    },
    "Kryzo--mcp-bibliotheque_nationale_de_France": {
      "owner": "Kryzo",
      "name": "mcp-bibliotheque_nationale_de_France",
      "url": "https://github.com/Kryzo/mcp-bibliotheque_nationale_de_France",
      "imageUrl": "/freedevtools/mcp/pfp/Kryzo.webp",
      "description": "Access the Gallica digital library to search for documents, images, maps, and other resources, and generate structured research reports that include organized bibliographies and relevant visual content.",
      "stars": 5,
      "forks": 3,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-10-01T16:19:01Z",
      "readme_content": "# BnF API Server\n\nUn serveur MCP (Model-Client-Protocol) pour accéder à l'API Gallica de la Bibliothèque nationale de France (BnF) et générer des rapports de recherche séquentiels.\n\n## Fonctionnalités\n\n- **Recherche dans Gallica** : Recherche de documents, images, cartes et autres ressources dans la bibliothèque numérique Gallica\n- **Génération de rapports séquentiels** : Création automatique de rapports de recherche structurés sur n'importe quel sujet\n- **Intégration de graphiques** : Inclusion d'images et de cartes pertinentes dans les rapports générés\n- **Citations formatées** : Génération automatique de bibliographies avec citations correctement formatées\n\n## Installation\n\n### Prérequis\n\n- Python 3.8 ou supérieur\n- Pip (gestionnaire de paquets Python)\n\n### Étapes d'installation\n\n1. **Cloner le dépôt**:\n   ```bash\n   git clone https://github.com/votre-nom/mcp-bnf.git\n   cd mcp-bnf\n   ```\n\n2. **Installer les dépendances**:\n   ```bash\n   pip install -r requirements.txt\n   ```\n\n## Configuration avec Claude Desktop\n\n1. **Installer Claude Desktop** si ce n'est pas déjà fait.\n\n2. **Ouvrir la configuration de Claude Desktop**:\n   - Accéder aux paramètres de Claude Desktop\n   - Ouvrir le fichier de configuration (généralement situé à `%APPDATA%\\Claude\\claude_desktop_config.json`)\n\n```json\n{\n \"bnf\": {\n  \"command\": \"py\",\n  \"args\": [\n    \"c:\\\\chemin\\\\vers\\\\mcp-bnf\\\\bnf_server.py\"\n  ],\n  \"cwd\": \"c:\\\\chemin\\\\vers\\\\mcp-bnf\"\n},\n```\n\nRemplacez `chemin\\\\vers\\\\mcp-bnf` par le chemin réel vers votre répertoire d'installation.\n\n3. **Enregistrer le fichier de configuration** et redémarrer Claude Desktop\n\n## Outils MCP disponibles\n\nUne fois configuré, les outils suivants seront disponibles dans Claude Desktop:\n\n### Recherche dans Gallica\n\nPermet de rechercher des documents dans la bibliothèque numérique Gallica de la BnF en utilisant différents critères (titre, auteur, sujet, date, type de document).\n\n### Génération de rapports séquentiels\n\nCrée des rapports de recherche complets sur n'importe quel sujet en utilisant les sources de Gallica. Les rapports incluent:\n- Une bibliographie formatée\n- Une introduction\n- Un contexte historique\n- Une analyse\n- Une conclusion\n- Des images et cartes pertinentes (optionnel)\n\n## Structure du projet\n\n```\nmcp-bnf/\n│\n├── bnf_server.py              # Serveur MCP principal\n├── requirements.txt           # Dépendances du projet\n│\n└── bnf_api/                   # Package API BnF\n    ├── __init__.py            # Exports du package\n    ├── api.py                 # Client API Gallica BnF\n    ├── search.py              # Fonctions de recherche\n    ├── config.py              # Constantes et configuration\n    └── sequential_reporting.py # Outil de génération de rapports séquentiels\n```\n\n## Utilisation\n\nUne fois configuré avec Claude Desktop, vous pouvez demander à Claude d'utiliser les outils BnF pour:\n\n1. **Rechercher des documents**:\n   - \"Recherche des livres sur Victor Hugo dans Gallica\"\n   - \"Trouve des cartes de Paris du 19ème siècle\"\n\n2. **Générer des rapports**:\n   - \"Crée un rapport sur l'impressionnisme en France\"\n   - \"Génère un rapport sur l'histoire du Liban sous mandat français avec des images\"\n\n## Développement\n\nPour contribuer au projet:\n\n1. Forker le dépôt\n2. Créer une branche pour votre fonctionnalité (`git checkout -b feature/nouvelle-fonctionnalite`)\n3. Committer vos changements (`git commit -am 'Ajouter une nouvelle fonctionnalité'`)\n4. Pousser vers la branche (`git push origin feature/nouvelle-fonctionnalite`)\n5. Créer une Pull Request\n\n## Licence\n\nCe projet est open source.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "bibliotheque_nationale_de_france",
        "bibliographies",
        "search",
        "mcp bibliotheque_nationale_de_france",
        "bibliotheque_nationale_de_france access",
        "search kryzo"
      ],
      "category": "web-search"
    },
    "KunihiroS--google-patents-mcp": {
      "owner": "KunihiroS",
      "name": "google-patents-mcp",
      "url": "https://github.com/KunihiroS/google-patents-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/KunihiroS.webp",
      "description": "Search Google Patents information using the SerpApi, providing access to comprehensive patent data for streamlined research. Facilitates quick querying and retrieval of patent-related information.",
      "stars": 11,
      "forks": 6,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-09-22T02:34:00Z",
      "readme_content": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/kunihiros-google-patents-mcp-badge.png)](https://mseep.ai/app/kunihiros-google-patents-mcp)\n\n# Google Patents MCP Server (`google-patents-mcp`)\n\n[![smithery badge](https://smithery.ai/badge/@KunihiroS/google-patents-mcp)](https://smithery.ai/server/@KunihiroS/google-patents-mcp)\n[![npm version](https://badge.fury.io/js/%40kunihiros%2Fgoogle-patents-mcp.svg)](https://badge.fury.io/js/%40kunihiros%2Fgoogle-patents-mcp)\n\nThis project provides a Model Context Protocol (MCP) server that allows searching Google Patents information via the [SerpApi Google Patents API](https://serpapi.com/google-patents-api).\n\n### Installing via Smithery\n\nTo install Google Patents MCP Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@KunihiroS/google-patents-mcp):\n\n```bash\nnpx -y @smithery/cli install @KunihiroS/google-patents-mcp --client claude\n```\n\n## Changelog\n\n### v0.2.0 (2025-04-17)\n\n*   **Fix:** Implemented empty handlers for `resources/list` and `prompts/list` MCP methods.\n*   **Fix:** Declared `prompts` capability in server initialization.\n*   **Chore:** Updated dependencies.\n\nThese changes aim to improve compatibility with MCP clients like Claude Desktop which may require these standard endpoints, though direct testing with Claude Desktop has not yet been performed.\n\n## Features\n\n*   Provides an MCP tool `search_patents` to search Google Patents.\n*   Uses SerpApi as the backend.\n*   Can be run directly using `npx` without local installation.\n\n## Prerequisites\n\n*   **Node.js:** Version 18 or higher is recommended.\n*   **npm:** Required to run the `npx` command.\n*   **SerpApi API Key:** You need a valid API key from [SerpApi](https://serpapi.com/) to use the Google Patents API.\n\n## Quick Start (Using npx)\n\nThe easiest way to run this server is using `npx`. This command downloads (if necessary) and runs the server directly.\n\n```bash\nnpx @kunihiros/google-patents-mcp\n```\n\n**Note:** Replace `@kunihiros/google-patents-mcp` with the actual published package name if it differs.\n\nThe server will start and listen for MCP requests on standard input/output.\n\n## Configuration\n\nThe server requires your SerpApi API key. You can provide it in one of the following ways:\n\n1.  **Environment Variable (Recommended for MCP Hosts):**\n    Set the `SERPAPI_API_KEY` environment variable when running the server. MCP Host configurations often allow setting environment variables for servers.\n\n    Example MCP Host configuration snippet (`config.json` or similar):\n    ```json\n    {\n      \"mcpServers\": {\n        \"google-patents-mcp\": {\n          \"command\": \"npx\",\n          \"args\": [\n            \"-y\", // Skips confirmation if the package isn't installed locally\n            \"@kunihiros/google-patents-mcp\" // Use the correct package name\n          ],\n          \"env\": {\n            \"SERPAPI_API_KEY\": \"YOUR_ACTUAL_SERPAPI_KEY\"\n            // Optional: Set log level\n            // \"LOG_LEVEL\": \"debug\"\n          }\n        }\n      }\n    }\n    ```\n\n2.  **.env File:**\n    Create a `.env` file in the directory where you run the `npx` command (for local testing or if not using an MCP Host), or in your home directory (`~/.google-patents-mcp.env`), with the following content:\n\n    ```dotenv\n    SERPAPI_API_KEY=YOUR_ACTUAL_SERPAPI_KEY\n    # Optional: Set log level (e.g., debug, info, warn, error)\n    # LOG_LEVEL=debug\n    ```\n    **Note:** While using a `.env` file is convenient for local testing, for production or integration with MCP Hosts, setting the environment variable directly via the host configuration is the recommended and more secure approach. The primary intended use case is execution via `npx`, where environment variables are typically managed by the calling process or MCP Host.\n\nThe server searches for `.env` files in the following order:\n    *   `./.env` (relative to where `npx` is run)\n    *   `~/.google-patents-mcp.env` (in your home directory)\n\n## Provided MCP Tool\n\n### `search_patents`\n\nSearches Google Patents via SerpApi.\n\n**Input Schema:**\n\n```json\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"q\": {\n      \"type\": \"string\",\n      \"description\": \"Search query (required). Although optional in SerpApi docs, a non-empty query is practically needed. Use semicolon (;) to separate multiple terms. Advanced syntax like '(Coffee) OR (Tea);(A47J)' is supported. See 'About Google Patents' for details.\"\n    },\n    \"page\": {\n      \"type\": \"integer\",\n      \"description\": \"Page number for pagination (default: 1).\",\n      \"default\": 1\n    },\n    \"num\": {\n      \"type\": \"integer\",\n      \"description\": \"Number of results per page (default: 10). **IMPORTANT: Must be 10 or greater (up to 100).**\",\n      \"default\": 10,\n      \"minimum\": 10,\n      \"maximum\": 100\n    },\n    \"sort\": {\n      \"type\": \"string\",\n      \"enum\": [\"relevance\", \"new\", \"old\"],\n      \"description\": \"Sorting method. 'relevance' (default), 'new' (newest by filing/publication date), 'old' (oldest by filing/publication date).\",\n      \"default\": \"relevance\"\n    },\n    \"before\": {\n      \"type\": \"string\",\n      \"description\": \"Maximum date filter (e.g., 'publication:20231231', 'filing:20220101'). Format: type:YYYYMMDD where type is 'priority', 'filing', or 'publication'.\"\n    },\n    \"after\": {\n      \"type\": \"string\",\n      \"description\": \"Minimum date filter (e.g., 'publication:20230101', 'filing:20220601'). Format: type:YYYYMMDD where type is 'priority', 'filing', or 'publication'.\"\n    },\n    \"inventor\": {\n      \"type\": \"string\",\n      \"description\": \"Filter by inventor names. Separate multiple names with a comma (,).\"\n    },\n    \"assignee\": {\n      \"type\": \"string\",\n      \"description\": \"Filter by assignee names. Separate multiple names with a comma (,).\"\n    },\n    \"country\": {\n      \"type\": \"string\",\n      \"description\": \"Filter by country codes (e.g., 'US', 'WO,JP'). Separate multiple codes with a comma (,).\"\n    },\n    \"language\": {\n      \"type\": \"string\",\n      \"description\": \"Filter by language (e.g., 'ENGLISH', 'JAPANESE,GERMAN'). Separate multiple languages with a comma (,). Supported: ENGLISH, GERMAN, CHINESE, FRENCH, SPANISH, ARABIC, JAPANESE, KOREAN, PORTUGUESE, RUSSIAN, ITALIAN, DUTCH, SWEDISH, FINNISH, NORWEGIAN, DANISH.\"\n    },\n    \"status\": {\n      \"type\": \"string\",\n      \"enum\": [\"GRANT\", \"APPLICATION\"],\n      \"description\": \"Filter by patent status: 'GRANT' or 'APPLICATION'.\"\n    },\n    \"type\": {\n      \"type\": \"string\",\n      \"enum\": [\"PATENT\", \"DESIGN\"],\n      \"description\": \"Filter by patent type: 'PATENT' or 'DESIGN'.\"\n    },\n    \"scholar\": {\n      \"type\": \"boolean\",\n      \"description\": \"Include Google Scholar results (default: false).\",\n      \"default\": false\n    }\n  },\n  \"required\": [\"q\"]\n}\n```\n\n**Output:**\n\nReturns a JSON object containing the search results from SerpApi. The structure follows the SerpApi response format.\n\n**Example Usage (MCP Request):**\n\n```json\n{\n  \"mcp_version\": \"1.0\",\n  \"type\": \"CallToolRequest\",\n  \"id\": \"req-123\",\n  \"server_name\": \"google-patents-mcp\",\n  \"params\": {\n    \"name\": \"search_patents\",\n    \"arguments\": {\n      \"q\": \"organic light emitting diode\",\n      \"num\": 10,\n      \"language\": \"ENGLISH\",\n      \"status\": \"GRANT\",\n      \"after\": \"publication:20230101\"\n    }\n  }\n}\n```\n\n## Development\n\n1.  **Clone the repository (if needed for development):**\n    ```bash\n    # git clone <repository-url>\n    # cd google-patents-mcp\n    ```\n2.  **Install dependencies:**\n    ```bash\n    npm install\n    ```\n3.  **Create `.env` file:**\n    Copy `.env.example` to `.env` and add your `SERPAPI_API_KEY`.\n4.  **Build:**\n    ```bash\n    npm run build\n    ```\n5.  **Run locally:**\n    ```bash\n    npm start\n    ```\n    Or for development with auto-rebuild:\n    ```bash\n    npm run dev\n    ```\n\n## Logging\n\n*   Logs are output to standard error.\n*   Log level can be controlled via the `LOG_LEVEL` environment variable (`error`, `warn`, `info`, `http`, `verbose`, `debug`, `silly`). Defaults to `info`.\n*   A log file is attempted to be created in the project root (`google-patents-server.log`), user's home directory (`~/.google-patents-server.log`), or `/tmp/google-patents-server.log`.\n\n## License\n\nMIT License (See LICENSE file)\n",
      "npm_url": "https://www.npmjs.com/package/@kunihiros/google-patents-mcp",
      "npm_downloads": 0,
      "keywords": [
        "patents",
        "patent",
        "serpapi",
        "patents information",
        "google patents",
        "patent data"
      ],
      "category": "web-search"
    },
    "L3-N0X--Minecraft-Wiki-MCP": {
      "owner": "L3-N0X",
      "name": "Minecraft-Wiki-MCP",
      "url": "https://github.com/L3-N0X/Minecraft-Wiki-MCP",
      "imageUrl": "/freedevtools/mcp/pfp/L3-N0X.webp",
      "description": "Browse and search the official Minecraft Wiki for detailed information on structures, entities, items, and blocks. Access multi-language support and navigate through wiki categories and specific sections for comprehensive content.",
      "stars": 6,
      "forks": 4,
      "license": "No License",
      "language": "TypeScript",
      "updated_at": "2025-09-20T04:16:05Z",
      "readme_content": "# Minecraft Wiki MCP\n[![smithery badge](https://smithery.ai/badge/@L3-N0X/Minecraft-Wiki-MCP)](https://smithery.ai/server/@L3-N0X/Minecraft-Wiki-MCP)\n[![Verified on MseeP](https://mseep.ai/badge.svg)](https://mseep.ai/app/f80cbb34-35d6-4652-a302-2413ffe60cb4)\n\nA MCP Server for browsing the official Minecraft Wiki!\n\n> [!WARNING]\n> This MCP is still in development and while working most of the time, there might still be smaller issues and bugs left!\n\n<a href=\"https://glama.ai/mcp/servers/@L3-N0X/Minecraft-Wiki-MCP\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@L3-N0X/Minecraft-Wiki-MCP/badge\" alt=\"Minecraft Wiki MCP server\" />\n</a>\n\n## Features\n\n- **Wiki Search**: Find information about Minecraft structures, entities, items, and blocks\n- **Page Navigation**: Get summaries and detailed content from wiki pages\n- **Section Access**: Target specific sections within wiki pages\n- **Category Browsing**: Explore wiki categories and their member pages\n- **Multi-Language Support**: Connect to different language versions of the Minecraft Wiki\n\n## Installation\n\nCurrently, only local installation is supported, other might follow!\n\n### Installing via Smithery\n\nTo install Minecraft Wiki Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@L3-N0X/Minecraft-Wiki-MCP):\n\n```bash\nnpx -y @smithery/cli install @L3-N0X/Minecraft-Wiki-MCP --client claude\n```\n\n### Manual Installation\n\n```bash\n# Clone the repository\ngit clone https://github.com/L3-N0X/Minecraft-Wiki-MCP.git\ncd Minecraft-Wiki-MCP\n\n# Install dependencies\nnpm install\n\n# Build the project\nnpm run build\n```\n\nThen, you can use the server with this configuration in your `claude_desktop_config.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"minecraft-wiki\": {\n      \"command\": \"node\",\n      \"args\": [\n        \"/path/to/your/dist/server.js\", \n        \"--api-url\",\n        \"https://minecraft.wiki/api.php\"\n      ]\n    }\n  }\n}\n```\n\n## Configuration\n\nMake sure to update the path to the server.js file!\nBy default, this server connects to <https://minecraft.wiki/api.php> (English version). You can use a different wiki API URL by using the `api-url` option to access different language versions:\n\n```json\n{\n  \"mcpServers\": {\n    \"minecraft-wiki\": {\n      \"command\": \"node\",\n       \"args\": [\n        \"/path/to/your/dist/server.js\", \n        \"--api-url\",\n        \"https://de.minecraft.wiki/api.php\" // German version\n      ]\n    }\n  }\n}\n```\n\n## Available Tools\n\nThis server provides the following tools for interacting with the Minecraft Wiki:\n\n### Search and Navigation\n\n- `MinecraftWiki_searchWiki`: Search for structures, entities, items, or blocks\n- `MinecraftWiki_getPageSummary`: Get page summary and list of available sections\n- `MinecraftWiki_resolveRedirect`: Resolve redirect pages to their targets\n\n### Page Content\n\n- `MinecraftWiki_getPageContent`: Get full page content\n- `MinecraftWiki_getPageSection`: Get specific section content\n- `MinecraftWiki_getSectionsInPage`: Get overview of all sections in a page\n\n### Categories\n\n- `MinecraftWiki_listAllCategories`: List all available categories\n- `MinecraftWiki_listCategoryMembers`: List pages within a category\n- `MinecraftWiki_getCategoriesForPage`: Get categories for a specific page\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "minecraft",
        "l3",
        "wiki",
        "minecraft wiki",
        "n0x minecraft",
        "official minecraft"
      ],
      "category": "web-search"
    },
    "Laksh-star--mcp-server-tmdb": {
      "owner": "Laksh-star",
      "name": "mcp-server-tmdb",
      "url": "https://github.com/Laksh-star/mcp-server-tmdb",
      "imageUrl": "/freedevtools/mcp/pfp/Laksh-star.webp",
      "description": "Integrates with The Movie Database (TMDB) API to provide access to movie information, search capabilities, and personalized recommendations.",
      "stars": 51,
      "forks": 16,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-09-30T05:19:05Z",
      "readme_content": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/laksh-star-mcp-server-tmdb-badge.png)](https://mseep.ai/app/laksh-star-mcp-server-tmdb)\n\n# TMDB MCP Server\n\n[![smithery badge](https://smithery.ai/badge/@Laksh-star/mcp-server-tmdb)](https://smithery.ai/server/@Laksh-star/mcp-server-tmdb)\nThis MCP server integrates with The Movie Database (TMDB) API to provide movie information, search capabilities, and recommendations.\n\n<a href=\"https://glama.ai/mcp/servers/g3nl1a0n25\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/g3nl1a0n25/badge\" alt=\"mcp-server-tmdb MCP server\" /></a>\n\n[![Ask DeepWiki](https://deepwiki.com/badge.svg)](https://deepwiki.com/Laksh-star/mcp-server-tmdb)\n## Prerequisites\n\nBefore installing and running the TMDB MCP server, ensure you have the following prerequisites installed and configured:\n\n### Required software\n\n- **Node.js**\n  - Version 18.0.0 or higher\n  - Download from [Node.js official website](https://nodejs.org/)\n  - Verify installation: `node --version`\n\n- **npm (Node Package Manager)**\n  - Version 8.0.0 or higher (comes with Node.js)\n  - Verify installation: `npm --version`\n\n- **TypeScript**\n  - Will be installed as a project dependency\n  - Can be installed globally: `npm install -g typescript`\n  - Verify installation: `tsc --version`\n\n### Required accounts & API keys\n\n- **TMDB account**\n  - Free account at [TMDB](https://www.themoviedb.org/)\n  - API key from TMDB dashboard\n  - API access must be approved by TMDB\n\n- **Claude desktop application**\n  - Latest version installed\n  - Access to modify configuration files\n\n### System requirements\n\n- **Operating systems**\n  - macOS (10.15 or later)\n  - Linux (modern distributions)\n\n- **Hardware requirements**\n- Minimum 4GB RAM\n  - 1GB free disk space\n  - Stable internet connection\n\n### Development environment\n\nFor the best development experience, we recommend:\n- A code editor with TypeScript support (e.g., VS Code)\n- Terminal access\n- Git (for version control)\n\n## Features\n\n### Tools\n\n- **search_movies**\n  - Search for movies by title or keywords\n  - Input: `query` (string): Search query\n  - Returns: List of movies with titles, release years, IDs, ratings, and overviews\n  - Example: Search for movies about space exploration\n\n- **get_recommendations**\n  - Get movie recommendations based on a movie ID\n  - Input: `movieId` (string): TMDB movie ID\n  - Returns: Top 5 recommended movies with details\n  - Example: Get recommendations based on movie ID 550 (Fight Club)\n\n- **get_trending**\n  - Get trending movies for a specified time window\n  - Input: `timeWindow` (string): Either \"day\" or \"week\"\n  - Returns: Top 10 trending movies with details\n  - Example: Get today's trending movies\n\n### Resources\n\nThe server provides access to TMDB movie information:\n\n- **Movies** (`tmdb:///movie/<movie_id>`)\n  - Comprehensive movie details including:\n    - Title and release date\n    - Rating and overview\n    - Genres\n    - Poster URL\n    - Cast information (top 5 actors)\n    - Director\n    - Selected reviews\n  - All data is returned in JSON format\n\n## Getting started\n\n1. Get a TMDB API key:\n   - Sign up at [TMDB](https://www.themoviedb.org/)\n   - Go to your account settings\n   - Navigate to the API section\n   - Request an API key for developer use\n\n2. Clone and set up the project:\n   ```bash\n   git clone [repository-url]\n   cd mcp-server-tmdb\n   npm install\n   ```\n\n3. Build the server:\n   ```bash\n   npm run build\n   ```\n\n4. Set up your environment variable:\n   ```bash\n   export TMDB_API_KEY=your_api_key_here\n   ```\n\n### Usage with Claude Desktop\n\nTo integrate this server with Claude Desktop, add the following to your app's server configuration file (located at `~/Library/Application Support/Claude/config.json`):\n\n```json\n{\n  \"mcpServers\": {\n    \"tmdb\": {\n      \"command\": \"/full/path/to/dist/index.js\",\n      \"env\": {\n        \"TMDB_API_KEY\": \"your_api_key_here\"\n      }\n    }\n  }\n}\n```\n\nReplace `/full/path/to` with the actual path to your project directory.\n\n### Installing via Smithery\n\nTo install TMDB Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@Laksh-star/mcp-server-tmdb):\n\n```bash\nnpx -y @smithery/cli install @Laksh-star/mcp-server-tmdb --client claude\n```\n\n## Example usage\n\nOnce the server is running with Claude Desktop, you can use commands like:\n\n1. Search for movies:\n   ```\n   \"Search for movies about artificial intelligence\"\n   ```\n\n2. Get trending movies:\n   ```\n   \"What are the trending movies today?\"\n   \"Show me this week's trending movies\"\n   ```\n\n3. Get movie recommendations:\n   ```\n   \"Get movie recommendations based on movie ID 550\"\n   ```\n\n4. Get movie details:\n   ```\n   \"Tell me about the movie with ID 550\"\n   ```\n\n## Error handling\n\nThe server includes comprehensive error handling for:\n- Invalid API keys\n- Network errors\n- Invalid movie IDs\n- Malformed requests\n\nError messages will be returned in a user-friendly format through Claude Desktop.\n\n## Development\n\nTo watch for changes during development:\n```bash\nnpm run watch\n```\n\n## License\n\nThis MCP server is licensed under the MIT License. See the LICENSE file for details.\n\n## Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n",
      "npm_url": "https://www.npmjs.com/package/mcp-server-tmdb",
      "npm_downloads": 78,
      "keywords": [
        "tmdb",
        "search",
        "database",
        "tmdb api",
        "server tmdb",
        "database tmdb"
      ],
      "category": "web-search"
    },
    "LinkupPlatform--python-mcp-server": {
      "owner": "LinkupPlatform",
      "name": "python-mcp-server",
      "url": "https://github.com/LinkupPlatform/python-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/LinkupPlatform.webp",
      "description": "Fetch real-time web search results using the LinkupClient to provide updated information for AI models, enhancing their responses with current events and trusted sources.",
      "stars": 44,
      "forks": 16,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-29T16:02:32Z",
      "readme_content": "# 🌟 Linkup Python MCP Server\n\nA Model Context Protocol (MCP) server that provides web search capabilities through Linkup's advanced search API. This server enables AI assistants and development tools to perform intelligent web searches with natural language queries.\n\n<a href=\"https://glama.ai/mcp/servers/69qbbv8hl9\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/69qbbv8hl9/badge\" alt=\"mcp-search-linkup MCP server\" /></a>\n\n## ✨ Why Linkup?\n\n- 🔍 **Advanced Web Search**: Leverage Linkup's AI-powered search engine for high-quality, relevant results\n- 💬 **Natural Language Queries**: Ask questions in plain English or your preferred language - no need for keyword optimization\n- 🚀 **Real-time Information**: Access up-to-date web content and current information\n- 📚 **Comprehensive Results**: Get detailed search results with source citations\n- 🔧 **Easy Integration**: Works with any MCP-compatible client\n\n## 🚀 Installation\n\nThe Linkup MCP server can be used with any MCP-compatible client. \n\nFor an integration with Claude Desktop or with Cursor, please follow instruction [here](https://docs.linkup.so/pages/integrations/mcp/mcp).\n\nFor other MCP-compatible clients, use these connection details:\n\n- **Command**: `python -m linkup_mcp_server`\n- **Required Environment Variables**: `LINKUP_API_KEY`\n\nConsult your MCP client's documentation for specific configuration instructions.\n\n## 💬 Example Queries\n\nThe Linkup MCP server excels at answering complex questions and finding specific information:\n\n- \"What are the latest developments in quantum computing?\"\n- \"How does the EU AI Act affect startups?\"\n- \"Find recent research on sustainable aviation fuel\"\n- \"What are the current best practices for MCP server development?\"\n\n## 🤝 Contributing\n\nPull requests are welcome! Feel free to open an issue first to discuss what you’d like to see improved.\n\n## 📚 Resources\n\n- [Linkup Documentation](https://docs.linkup.so)\n- [MCP Protocol Specification](https://modelcontextprotocol.io)\n- [Linkup API Reference](https://docs.linkup.so/api-reference)\n\n## 📣 Community & Support\n\n* Email: [support@linkup.so](mailto:support@linkup.so)\n* Discord: [Join our community](https://discord.com/invite/9q9mCYJa86)\n* X / Twitter: [@Linkup_platform](https://x.com/Linkup_platform)\n\n## 📄 License\n\nThis project is licensed under the MIT License - Innovate freely! 🚀\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "linkupclient",
        "search",
        "web",
        "web search",
        "search linkupplatform",
        "search results"
      ],
      "category": "web-search"
    },
    "Lomaloque--AUDIO": {
      "owner": "Lomaloque",
      "name": "AUDIO",
      "url": "https://github.com/Lomaloque/AUDIO",
      "imageUrl": "/freedevtools/mcp/pfp/Lomaloque.webp",
      "description": "Integrate advanced audio processing features into web applications using JavaScript and WebAssembly. Provides tools for low-latency audio effects, mixing, and analysis without requiring direct management of WebAssembly or C++.",
      "stars": 0,
      "forks": 0,
      "license": "No License",
      "language": "",
      "updated_at": "2025-03-24T19:55:34Z",
      "readme_content": "<p align=\"center\"><img alt=\"logo\" width=\"450\" src=\"https://superpowered.com/images/logo.png\"></p>\n\nSuperpowered Inc develops the Superpowered Web Audio JavaScript and WebAssembly SDK (\"JS/WASM SDK\") for modern web browsers, websites, progressive web apps and more.\n\nDevelopers can use Superpowered interactive audio features in JavaScript without the need of building, initializing or even touching WebAssembly or C++.\n\nFor the most up-to-date information, see: https://docs.superpowered.com/?lang=js\n\n\n# JavaScript + WebAssembly\n\nThe JS/WASM SDK is contained in this repository. For C++ SDKs for native apps, we offer Superpowered C++ Audio SDK, C++ Networking SDK, and C++ Crypto SDK featuring low-power and real-time latency. They can be found here: https://github.com/superpoweredSDK/Low-Latency-Android-iOS-Linux-Windows-tvOS-macOS-Interactive-Audio-Platform/\n\n# Installation and Usage\n\n```\nnpm install @superpoweredsdk/web\n```\n\nSee usage guide over at https://docs.superpowered.com/getting-started/how-to-integrate?lang=js\n\n# Supported Functionality\n\n- Effects: echo, delay, bitcrusher, flanger, gate, roll, reverb, whoosh, compressor, clipper, limiter, 3 band EQ\n- Filters: resonant low-pass, resonant high-pass, low-shelf, high-shelf, bandpass, notch, parametric\n- Music Analysis: bpm detection, key detection, beatgrid detection, audio waveform, filter bank analysis\n- Object-based 3D Audio Spatializer\n- Mixing: stereo mixer, mono mixer, crossfading, mixing, volume, peak\n- Format conversion (32 bit, 24 bit, 16 bit)\n- Audio Resampler\n- Time domain to frequency domain, frequency domain to time domain\n- Time Stretching, Pitch Shifting\n- FFT: complex, real, real-polar\n- Web Audio I/O, support for Workers, Worklets and Audio Worklet\n\n\n# Demos\n\nReal-time (NOT RENDERED), low-latency time-stretching in the browser:\\\nhttps://superpowered.com/js-wasm-sdk/example_timestretching/\n\nReal-time low-latency reverb and filter in the browser:\\\nhttps://superpowered.com/js-wasm-sdk/example_effects/\n\nReal-time low-latency guitar distortion in the browser:\\\nhttps://superpowered.com/js-wasm-sdk/example_guitardistortion/\n\n\n# Supported Web Browsers\n\nThe Superpowered Web Audio JavaScript and WebAssembly SDK supports the following web browsers: official public stable versions of all major web browsers, including desktop and mobile variants (iOS, Android), such as Chrome, Safari, Firefox and Opera. The only exception is Microsoft Edge, that requires developer build version 74 minimum.\n\n\n# Support\n\nSuperpowered offers multiple support options.\n\nDeveloper Documentation (C++ and JavaScript): https://docs.superpowered.com\n\nEmail: support@superpowered.zendesk.com\n\nKnowledge base: https://superpowered.zendesk.com/hc/en-us\n\nStackOverflow: https://stackoverflow.com/search?tab=newest&q=superpowered\n\nYouTube: https://www.youtube.com/playlist?list=PLtRKsB6a4xFMXJrZ9wjscOow3nASBoEbU\n\nPaid support options: https://superpowered.com/support\n\n\n# Licensing\n\nJS/WASM SDK is licensed separately on a case-by-case basis. Parties interested in using JS/WASM SDK must contact licensing@superpowered.com. Free license may be available at our sole discretion. Parties are encouraged to experiment and create private applications with the JS/WASM SDK, but may not launch publicly and/or without a license, which we shall grant at our sole discretion. Please note that any unauthorized use of JS/WASM SDK may result in interruption of service without notice.\n\nFor details, please see: https://superpowered.com/licensing\n\nFor licensing inquiries, please email licensing@superpowered.com.\n\n\n# Custom Application Development Services\n\nSuperpowered offers custom development services focusing on low-latency, interactive audio applications for mobile, web, desktop and embedded.\n\nFor development inquiries, please email hello@superpowered.com.\n\n\n# Contact\n\nIf you want to be informed about new code releases, bug fixes, general news and information about Superpowered, please email hello@superpowered.com.\n\nFor licensing inquiries, please email licensing@superpowered.com.\n\n\n# Notes\n\nSuperpowered FFT benefits from ideas in Construction of a High-Performance FFT by Eric Postpischil (http://edp.org/resume.htm).\n\nThe Superpowered MP3 and AAC decoder benefits from optimizations by Ken Cooke.\n",
      "npm_url": "https://www.npmjs.com/package/audio",
      "npm_downloads": 4651,
      "keywords": [
        "audio",
        "webassembly",
        "lomaloque",
        "lomaloque audio",
        "audio integrate",
        "audio processing"
      ],
      "category": "web-search"
    },
    "Lorhlona--geminiserchMCP": {
      "owner": "Lorhlona",
      "name": "geminiserchMCP",
      "url": "https://github.com/Lorhlona/geminiserchMCP",
      "imageUrl": "/freedevtools/mcp/pfp/Lorhlona.webp",
      "description": "Generates responses based on the latest information using the Gemini API and Google Search, requiring integration with AI assistants like Cline for functionality.",
      "stars": 23,
      "forks": 3,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-09-03T22:52:12Z",
      "readme_content": "# Gemini Search MCP Server\n\n[![MCP Server](https://glama.io/mcp-servers/badge/Lorhlona/geminiserchMCP)](https://glama.io/mcp-servers/Lorhlona/geminiserchMCP)\n\nAn MCP server that generates responses based on the latest information using the Gemini API and Google Search.\n\n> **Note**: This MCP server does not work standalone. It needs to be used in combination with AI assistants like [Cline](https://github.com/ClineLabs/cline). The Gemini search functionality becomes available when you load this project into an AI assistant.\n\n---\n\nGemini APIとGoogle検索を使用して、最新の情報に基づいた回答を生成するMCPサーバーです。\n\n> **注意**: このMCPサーバーは単体では動作しません。[Cline](https://github.com/ClineLabs/cline)などのAIアシスタントと組み合わせて使用する必要があります。AIアシスタントにこのプロジェクトを読み込ませることで、Gemini検索機能が利用可能になります。\n\n## Features\n\n### Tools\n- `search` - Generate answers using Gemini 2.0 and Google Search\n  - Takes a query as input and returns Gemini's response along with relevant search results\n\n## 機能\n\n### Tools\n- `search` - Gemini 2.0とGoogle検索を使用して質問に回答\n  - クエリを入力として受け取り、Geminiの回答と関連する検索結果を返します\n\n## Setup\n\n1. Install dependencies:\n```bash\nnpm install\n```\n\n2. Build:\n```bash\nnpm run build\n```\n\n3. Set environment variables:\nCreate a `.env` file in the project root with the following content:\n```\nGEMINI_API_KEY=your_api_key_here\n```\nNote: You can get your Gemini API key from [Google AI Studio](https://makersuite.google.com/app/apikey).\n\n## Development\n\nFor automatic builds during development:\n```bash\nnpm run watch\n```\n\n## Installation\n\nTo use with Claude Desktop, add the following configuration:\n\nWindows: `%APPDATA%/Claude/claude_desktop_config.json`\n```json\n{\n  \"mcpServers\": {\n    \"gemini\": {\n      \"command\": \"node\",\n      \"args\": [\"path/to/gemini-search-server/build/index.js\"],\n      \"env\": {\n        \"GEMINI_API_KEY\": \"your_api_key_here\"\n      }\n    }\n  }\n}\n```\n\n### Debugging\n\nSince MCP servers communicate via stdio, we recommend using [MCP Inspector](https://github.com/modelcontextprotocol/inspector) for debugging:\n\n```bash\nnpm run inspector\n```\n\nThe Inspector provides a URL to access debugging tools in your browser.\n\n## License\n\nThe code in this project is released under the [MIT License](LICENSE).\nHowever, please note that this project uses the Google Gemini API, which is subject to [Google's Terms of Service](https://ai.google.dev/terms). When using this MCP server, you must comply with both the MIT License for our code and Google's terms for the Gemini API.\n\n---\n\n## セットアップ\n\n1. 依存関係のインストール:\n```bash\nnpm install\n```\n\n2. ビルド:\n```bash\nnpm run build\n```\n\n3. 環境変数の設定:\n`.env`ファイルをプロジェクトのルートに作成し、以下の内容を設定してください：\n```\nGEMINI_API_KEY=your_api_key_here\n```\n※ Gemini APIキーは[Google AI Studio](https://makersuite.google.com/app/apikey)から取得できます。\n\n## 開発\n\n開発時の自動ビルド:\n```bash\nnpm run watch\n```\n\n## インストール\n\nClaude Desktopで使用するには、以下の設定を追加してください：\n\nWindows: `%APPDATA%/Claude/claude_desktop_config.json`\n```json\n{\n  \"mcpServers\": {\n    \"gemini\": {\n      \"command\": \"node\",\n      \"args\": [\"path/to/gemini-search-server/build/index.js\"],\n      \"env\": {\n        \"GEMINI_API_KEY\": \"your_api_key_here\"\n      }\n    }\n  }\n}\n```\n\n### デバッグ\n\nMCPサーバーはstdioを介して通信するため、デバッグには[MCP Inspector](https://github.com/modelcontextprotocol/inspector)の使用を推奨します：\n\n```bash\nnpm run inspector\n```\n\nInspectorはブラウザでデバッグツールにアクセスするためのURLを提供します。\n\n## ライセンス\n\nこのプロジェクトのコードは[MIT License](LICENSE)の下で公開されています。\nただし、このプロジェクトはGoogle Gemini APIを使用しているため、[Googleの利用規約](https://ai.google.dev/terms)も適用されます。このMCPサーバーを使用する際は、コードのMITライセンスとGemini APIの利用規約の両方に従う必要があります。\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "geminiserchmcp",
        "search",
        "api",
        "gemini api",
        "lorhlona geminiserchmcp",
        "web search"
      ],
      "category": "web-search"
    },
    "MaitreyaM--WEB-SCRAPING-MCP": {
      "owner": "MaitreyaM",
      "name": "WEB-SCRAPING-MCP",
      "url": "https://github.com/MaitreyaM/WEB-SCRAPING-MCP",
      "imageUrl": "/freedevtools/mcp/pfp/MaitreyaM.webp",
      "description": "Enables AI agents to scrape web content, search for specific text snippets, and perform structured information extraction from websites using natural language instructions. Utilizes LLMs for intelligent content retrieval and provides an API for interaction.",
      "stars": 21,
      "forks": 7,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-10-02T19:15:34Z",
      "readme_content": "# Crawl4AI Web Scraper MCP Server\n\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT) <!-- Optional: Add license -->\n\nThis project provides an MCP (Model Context Protocol) server that uses the **[crawl4ai](https://github.com/unclecode/crawl4ai)** library to perform web scraping and intelligent content extraction tasks. It allows AI agents (like Claude, or agents built with LangChain/LangGraph) to interact with web pages, retrieve content, search for specific text, and perform LLM-based extraction based on natural language instructions.\n\nThis server uses:\n\n*   **[FastMCP](https://github.com/model-context-protocol/mcp-py/blob/main/docs/fastmcp.md):** For creating the MCP server endpoint.\n*   **[crawl4ai](https://github.com/unclecode/crawl4ai):** For the core web crawling and extraction logic.\n*   **[dotenv](https://github.com/theskumar/python-dotenv):** For managing API keys via a `.env` file.\n*   **(Optional) Docker:** For containerized deployment, bundling Python and dependencies.\n\n## Features\n\n*   Exposes MCP tools for web interaction:\n    *   `scrape_url`: Get the full content of a webpage in Markdown format.\n    *   `extract_text_by_query`: Find specific text snippets on a page based on a query.\n    *   `smart_extract`: Use an LLM (currently Google Gemini) to extract structured information based on instructions.\n*   Configurable via environment variables (API keys).\n*   Includes Docker configuration (`Dockerfile`) for easy, self-contained deployment.\n*   Communicates over Server-Sent Events (SSE) on port 8002 by default.\n\n## Exposed MCP Tools\n\n### `scrape_url`\n\nScrape a webpage and return its content in Markdown format.\n\n**Arguments:**\n\n*   `url` (str, **required**): The URL of the webpage to scrape.\n\n**Returns:**\n\n*   (str): The webpage content in Markdown format, or an error message.\n\n### `extract_text_by_query`\n\nExtract relevant text snippets from a webpage that contain a specific search query. Returns up to the first 5 matches found.\n\n**Arguments:**\n\n*   `url` (str, **required**): The URL of the webpage to search within.\n*   `query` (str, **required**): The text query to search for (case-insensitive).\n*   `context_size` (int, *optional*): The number of characters to include before and after the matched query text in each snippet. Defaults to `300`.\n\n**Returns:**\n\n*   (str): A formatted string containing the found text snippets or a message indicating no matches were found, or an error message.\n\n### `smart_extract`\n\nIntelligently extract specific information from a webpage using the configured LLM (currently requires Google Gemini API key) based on a natural language instruction.\n\n**Arguments:**\n\n*   `url` (str, **required**): The URL of the webpage to analyze and extract from.\n*   `instruction` (str, **required**): Natural language instruction specifying what information to extract (e.g., \"List all the speakers mentioned on this page\", \"Extract the main contact email address\", \"Summarize the key findings\").\n\n**Returns:**\n\n*   (str): The extracted information (often formatted as JSON or structured text based on the instruction) or a message indicating no relevant information was found, or an error message (including if the required API key is missing).\n\n## Setup and Running\n\nYou can run this server either locally or using the provided Docker configuration.\n\n### Option 1: Running with Docker (Recommended for Deployment)\n\nThis method bundles Python and all necessary libraries. You only need Docker installed on the host machine.\n\n1.  **Install Docker:** Download and install [Docker Desktop](https://www.docker.com/products/docker-desktop/) for your OS. Start Docker Desktop.\n2.  **Clone Repository:**\n    ```bash\n    git clone https://github.com/your-username/your-repo-name.git # Replace with your repo URL\n    cd your-repo-name\n    ```\n3.  **Create `.env` File:** Create a file named `.env` in the project root directory and add your API keys:\n    ```.env\n    # Required for the smart_extract tool\n    GOOGLE_API_KEY=your_google_ai_api_key_here\n\n    # Optional, checked by server but not currently used by tools\n    # OPENAI_API_KEY=your_openai_key_here\n    # MISTRAL_API_KEY=your_mistral_key_here\n    ```\n4.  **Build the Docker Image:**\n    ```bash\n    docker build -t crawl4ai-mcp-server .\n    ```\n5.  **Run the Container:** This starts the server, making port 8002 available on your host machine. It uses `--env-file` to securely pass the API keys from your local `.env` file into the container's environment.\n    ```bash\n    docker run -it --rm -p 8002:8002 --env-file .env crawl4ai-mcp-server\n    ```\n    *   `-it`: Runs interactively.\n    *   `--rm`: Removes container on exit.\n    *   `-p 8002:8002`: Maps host port 8002 to container port 8002.\n    *   `--env-file .env`: Loads environment variables from your local `.env` file into the container. **Crucial for API keys.**\n    *   `crawl4ai-mcp-server`: The name of the image you built.\n6.  **Server is Running:** Logs will appear, indicating the server is listening on SSE (`http://0.0.0.0:8002`).\n7.  **Connecting Client:** Configure your MCP client (e.g., LangChain agent) to connect to `http://127.0.0.1:8002/sse` with `transport: \"sse\"`.\n\n### Option 2: Running Locally\n\nThis requires Python and manual installation of dependencies on your host machine.\n\n1.  **Install Python:** Ensure Python >= 3.9 (check `crawl4ai` requirements if needed, 3.10+ recommended).\n2.  **Clone Repository:**\n    ```bash\n    git clone https://github.com/your-username/your-repo-name.git # Replace with your repo URL\n    cd your-repo-name\n    ```\n3.  **Create Virtual Environment (Recommended):**\n    ```bash\n    python -m venv venv\n    source venv/bin/activate # Linux/macOS\n    # venv\\Scripts\\activate # Windows\n    ```\n    *(Or use Conda: `conda create --name crawl4ai-env python=3.11 -y && conda activate crawl4ai-env`)*\n4.  **Install Dependencies:**\n    ```bash\n    pip install -r requirements.txt\n    ```\n5.  **Create `.env` File:** Create a file named `.env` in the project root directory and add your API keys (same content as in Docker setup step 3).\n6.  **Run the Server:**\n    ```bash\n    python your_server_script_name.py # e.g., python webcrawl_mcp_server.py\n    ```\n7.  **Server is Running:** It will listen on `http://127.0.0.1:8002/sse`.\n8.  **Connecting Client:** Configure your MCP client to connect to `http://127.0.0.1:8002/sse`.\n\n## Environment Variables\n\nThe server uses the following environment variables, typically loaded from an `.env` file:\n\n*   `GOOGLE_API_KEY`: **Required** for the `smart_extract` tool to function (uses Google Gemini). Get one from [Google AI Studio](https://aistudio.google.com/app/apikey).\n*   `OPENAI_API_KEY`: Checked for existence but **not currently used** by any tool in this version.\n*   `MISTRAL_API_KEY`: Checked for existence but **not currently used** by any tool in this version.\n\n## Example Agent Interaction\n\n```\n# Example using the agent CLI from the previous setup\n\nYou: scrape_url https://example.com\nAgent: Thinking...\n[Agent calls scrape_url tool]\nAgent: [Markdown content of example.com]\n------------------------------\nYou: extract text from https://en.wikipedia.org/wiki/Web_scraping using the query \"ethical considerations\"\nAgent: Thinking...\n[Agent calls extract_text_by_query tool]\nAgent: Found X matches for 'ethical considerations' on the page. Here are the relevant sections:\nMatch 1:\n... text snippet ...\n---\nMatch 2:\n... text snippet ...\n------------------------------\nYou: Use smart_extract on https://blog.google/technology/ai/google-gemini-ai/ to get the main points about Gemini models\nAgent: Thinking...\n[Agent calls smart_extract tool with Google API Key]\nAgent: Successfully extracted information based on your instruction:\n{\n  \"main_points\": [\n    \"Gemini is Google's most capable AI model family (Ultra, Pro, Nano).\",\n    \"Designed to be multimodal, understanding text, code, audio, image, video.\",\n    \"Outperforms previous models on various benchmarks.\",\n    \"Being integrated into Google products like Bard and Pixel.\"\n  ]\n}\n\n```\n\n## Files\n\n*   `your_server_script_name.py`: The main Python script for the MCP server (e.g., `webcrawl_mcp_server.py`).\n*   `Dockerfile`: Instructions for building the Docker container image.\n*   `requirements.txt`: Python dependencies.\n*   `.env.example`: (Recommended) An example environment file showing needed keys. **Do not commit your actual `.env` file.**\n*   `.gitignore`: Specifies intentionally untracked files for Git (should include `.env`).\n*   `README.md`: This file.\n\n## Contributing\n\n(Add contribution guidelines if desired)\n\n## License\n\n(Specify your license, e.g., MIT License)",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "scraping",
        "retrieval",
        "web",
        "web scraping",
        "extraction websites",
        "scraping mcp"
      ],
      "category": "web-search"
    },
    "Meeting-BaaS--meeting-mcp": {
      "owner": "Meeting-BaaS",
      "name": "meeting-mcp",
      "url": "https://github.com/Meeting-BaaS/meeting-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/Meeting-BaaS.webp",
      "description": "Manage meeting data including transcripts, recordings, and calendar events while providing search functionality for easy organization and retrieval.",
      "stars": 20,
      "forks": 7,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-09-12T20:09:05Z",
      "readme_content": "# Meeting BaaS MCP Server\n[](https://meetingBaaS.com)\n\n<p align=\"center\"><a href=\"https://discord.com/invite/dsvFgDTr6c\"><img height=\"60px\" src=\"https://user-images.githubusercontent.com/31022056/158916278-4504b838-7ecb-4ab9-a900-7dc002aade78.png\" alt=\"Join our Discord!\"></a></p>\n\nA Model Context Protocol (MCP) server that provides tools for managing meeting data, including transcripts, recordings, calendar events, and search functionality.\n\n## QUICK START: Claude Desktop Integration\n\nTo use Meeting BaaS with Claude Desktop:\n\n1. Edit the Claude Desktop configuration file:\n   ```bash\n   vim ~/Library/Application\\ Support/Claude/claude_desktop_config.json\n   ```\n\n2. Add the Meeting BaaS configuration:\n   ```json\n   \"meetingbaas\": {\n     \"command\": \"/bin/bash\",\n     \"args\": [\n       \"-c\",\n       \"cd /path/to/meeting-mcp && (npm run build 1>&2) && export MCP_FROM_CLAUDE=true && node dist/index.js\"\n     ],\n     \"headers\": {\n       \"x-api-key\": \"YOUR_API_KEY\"\n     }\n   }\n   ```\n\n3. For calendar integration, you can add the `calendarOAuth` section to your `botConfig`:\n   ```json\n   \"botConfig\": {\n     \"calendarOAuth\": {\n       \"platform\": \"Google\",  // or \"Microsoft\"\n       \"clientId\": \"YOUR_OAUTH_CLIENT_ID\",\n       \"clientSecret\": \"YOUR_OAUTH_CLIENT_SECRET\", \n       \"refreshToken\": \"YOUR_REFRESH_TOKEN\",\n       \"rawCalendarId\": \"primary@gmail.com\"  // Optional\n     }\n   }\n   ```\n\n4. Save the file and restart Claude Desktop.\n\n> **Note:** Calendar integration is optional. Meeting BaaS can be used without connecting a calendar by simply omitting the `calendarOAuth` section.\n\n## Overview\n\nThis project implements a Model Context Protocol (MCP) server that allows AI assistants like Claude and Cursor to access and manipulate meeting data. It exposes a set of tools and resources that can be used to:\n\n- **Invite Meeting Bots**: Create and invite bots to your video conferences that automatically record and transcribe meetings\n\n  ```\n  \"Create a new meeting bot for my Zoom call tomorrow\"\n  ```\n\n- **Query Meeting Data**: Search through meeting transcripts and find specific information without watching entire recordings\n\n  ```\n  \"Search my recent meetings for discussions about the quarterly budget\"\n  \"Find all mentions of Project Apollo in yesterday's team meeting\"\n  \"Show me parts of the meeting where Jane was speaking\"\n  ```\n\n- **Manage Calendar Events**: View and organize calendar entries and upcoming meetings\n\n- **Access Recording Information**: Get metadata about meeting recordings and their status\n\n## Prerequisites\n\n- Node.js (v16 or later)\n- npm\n- **MeetingBaaS Account**: You need access to a MeetingBaaS account using your corporate email address\n  - All logs, bots, and shared links are available to colleagues with the same corporate domain (not personal emails like gmail.com)\n  - This enables seamless collaboration where all team members can access meeting recordings and transcripts created by anyone in your organization\n\n## Installation\n\n1. Clone the repository:\n\n   ```bash\n   git clone <repository-url>\n   cd mcp-baas\n   ```\n\n2. Install dependencies:\n\n   ```bash\n   npm install\n   ```\n\n3. Build the project:\n   ```bash\n   npm run build\n   ```\n\n## Usage\n\nStart the server:\n\n```bash\nnpm run start\n```\n\nBy default, the server runs on port 7017 and exposes the MCP endpoint at `http://localhost:7017/mcp`.\n\n## Available Tools\n\nThe server exposes several tools through the MCP protocol:\n\n### Calendar Tools\n\n- `oauthGuidance`: Get detailed step-by-step instructions on setting up OAuth for Google or Microsoft calendars\n  - No parameters required\n  - Returns comprehensive instructions for obtaining OAuth credentials and setting up calendar integration\n\n- `listRawCalendars`: Lists available calendars from Google or Microsoft before integration\n  - Parameters: `platform` (\"Google\" or \"Microsoft\"), `clientId`, `clientSecret`, `refreshToken`\n  - Returns a list of available calendars with their IDs and primary status\n\n- `setupCalendarOAuth`: Integrates a calendar using OAuth credentials\n  - Parameters: `platform` (\"Google\" or \"Microsoft\"), `clientId`, `clientSecret`, `refreshToken`, `rawCalendarId` (optional)\n  - Returns confirmation of successful integration with calendar details\n\n- `listCalendars`: Lists all integrated calendars\n  - No parameters required\n  - Returns a list of all calendars with their names, email addresses, and UUIDs\n\n- `getCalendar`: Gets detailed information about a specific calendar integration\n  - Parameters: `calendarId` (UUID of the calendar)\n  - Returns comprehensive calendar details\n\n- `deleteCalendar`: Permanently removes a calendar integration\n  - Parameters: `calendarId` (UUID of the calendar)\n  - Returns confirmation of successful deletion\n\n- `resyncAllCalendars`: Forces a refresh of all connected calendars\n  - No parameters required\n  - Returns the status of the sync operation\n\n- `listUpcomingMeetings`: Lists upcoming meetings from a calendar\n  - Parameters: `calendarId`, `status` (optional: \"upcoming\", \"past\", \"all\"), `limit` (optional)\n  - Returns a list of meetings with their names, times, and recording status\n\n- `listEvents`: Lists calendar events with comprehensive filtering options\n  - Parameters: `calendarId`, plus optional filters like `startDateGte`, `startDateLte`, `attendeeEmail`, etc.\n  - Returns detailed event listings with rich information\n\n- `listEventsWithCredentials`: Lists calendar events with credentials provided directly in the query\n  - Parameters: `calendarId`, `apiKey`, plus same optional filters as `listEvents`\n  - Returns the same detailed information as `listEvents` but with direct authentication\n\n- `getEvent`: Gets detailed information about a specific calendar event\n  - Parameters: `eventId` (UUID of the event)\n  - Returns comprehensive event details including attendees and recording status\n\n- `scheduleRecording`: Schedules a bot to record an upcoming meeting\n  - Parameters: `eventId`, `botName`, plus optional settings like `botImage`, `recordingMode`, etc.\n  - Returns confirmation of successful scheduling\n\n- `scheduleRecordingWithCredentials`: Schedules recording with credentials provided directly in the query\n  - Parameters: `eventId`, `apiKey`, `botName`, plus same optional settings as `scheduleRecording`\n  - Returns confirmation of successful scheduling\n\n- `cancelRecording`: Cancels a previously scheduled recording\n  - Parameters: `eventId`, `allOccurrences` (optional, for recurring events)\n  - Returns confirmation of successful cancellation\n\n- `cancelRecordingWithCredentials`: Cancels recording with credentials provided directly in the query\n  - Parameters: `eventId`, `apiKey`, `allOccurrences` (optional)\n  - Returns confirmation of successful cancellation\n\n- `checkCalendarIntegration`: Checks and diagnoses calendar integration status\n  - No parameters required\n  - Returns a comprehensive status report and troubleshooting tips\n\n### Meeting Tools\n\n- `createBot`: Creates a meeting bot that can join video conferences to record and transcribe meetings\n  - Parameters: \n    - `meeting_url` (URL of the meeting to join)\n    - `name` (optional bot name)\n    - `botImage` (optional URL to an image for the bot's avatar) \n    - `entryMessage` (optional message the bot will send when joining)\n    - `deduplicationKey` (optional key to override the 5-minute restriction on joining the same meeting)\n    - `nooneJoinedTimeout` (optional timeout in seconds for bot to leave if no one joins)\n    - `waitingRoomTimeout` (optional timeout in seconds for bot to leave if stuck in waiting room)\n    - `speechToTextProvider` (optional provider for transcription: \"Gladia\", \"Runpod\", or \"Default\")\n    - `speechToTextApiKey` (optional API key for the speech-to-text provider)\n    - `streamingInputUrl` (optional WebSocket URL to stream audio input)\n    - `streamingOutputUrl` (optional WebSocket URL to stream audio output)\n    - `streamingAudioFrequency` (optional frequency for streaming: \"16khz\" or \"24khz\")\n    - `extra` (optional object with additional metadata about the meeting, such as meeting type, custom summary prompt, search keywords)\n  - Returns: Bot details including ID and join status\n- `getBots`: Lists all bots and their associated meetings\n- `getBotsByMeeting`: Gets bots for a specific meeting URL\n- `getRecording`: Retrieves recording information for a specific bot/meeting\n- `getRecordingStatus`: Checks the status of a recording in progress\n- `getMeetingData`: Gets transcript and recording data for a specific meeting\n  - Parameters: `meetingId` (ID of the meeting to get data for)\n  - Returns: Information about the meeting recording including duration and transcript segment count\n- `getMeetingDataWithCredentials`: Gets transcript and recording data using direct API credentials\n  - Parameters: `meetingId` (ID of the meeting), `apiKey` (API key for authentication)\n  - Returns: Same information as `getMeetingData` but with direct authentication\n\n### Transcript Tools\n\n- `getMeetingTranscript`: Gets a meeting transcript with speaker names and content grouped by speaker\n  - Parameters: `botId` (the bot that recorded the meeting)\n  - Returns: Complete transcript with speaker information, formatted as paragraphs grouped by speaker\n  - Example output:\n    ```\n    Meeting: \"Weekly Team Meeting\"\n    Duration: 45m 30s\n    Transcript:\n\n    John Smith: Hello everyone, thanks for joining today's call. We have a lot to cover regarding the Q3 roadmap and our current progress on the platform redesign.\n\n    Sarah Johnson: Thanks John. I've prepared some slides about the user testing results we got back yesterday. The feedback was generally positive but there are a few areas we need to address.\n    ```\n\n- `findKeyMoments`: Automatically identifies and shares links to important moments in a meeting\n  - Parameters: `botId`, optional `meetingTitle`, optional list of `topics` to look for, and optional `maxMoments`\n  - Returns: Markdown-formatted list of key moments with links, automatically detected based on transcript\n  - Uses AI-powered analysis to find significant moments without requiring manual timestamp selection\n\n### QR Code Tools\n\n- `generateQRCode`: Creates an AI-generated QR code image that can be used as a bot avatar\n  - Parameters:\n    - `type`: Type of QR code (url, email, phone, sms, text)\n    - `to`: Destination for the QR code (URL, email, phone number, or text)\n    - `prompt`: AI prompt to customize the QR code (max 1000 characters). You can include your API key directly in the prompt text by typing \"API key: qrc_your_key\" or similar phrases.\n    - `style`: Style of the QR code (style_default, style_dots, style_rounded, style_crystal)\n    - `useAsBotImage`: Whether to use the generated QR code as the bot avatar (default: true)\n    - `template`: Template ID for the QR code (optional)\n    - `apiKey`: Your QR Code AI API key (optional, will use default if not provided)\n  - Returns: URL to the generated QR code image that can be used directly with the joinMeeting tool\n  - Example usage:\n    ```\n    \"Generate a QR code with my email lazare@spoke.app that looks like a Tiger in crystal style\"\n    ```\n  - Example with API key in the prompt:\n    ```\n    \"Generate a QR code for my website https://example.com that looks like a mountain landscape. Use API key: qrc_my-personal-api-key-123456\"\n    ```\n  - Example with formal parameter:\n    ```\n    \"Generate a QR code with the following parameters:\n    - Type: email\n    - To: john.doe@example.com\n    - Prompt: Create a QR code that looks like a mountain landscape\n    - Style: style_rounded\n    - API Key: qrc_my-personal-api-key-123456\"\n    ```\n\n### Link Sharing Tools\n\n- `shareableMeetingLink`: Generates a nicely formatted, shareable link to a meeting recording\n  - Parameters: `botId`, plus optional `timestamp`, `title`, `speakerName`, and `description`\n  - Returns: Markdown-formatted link with metadata that can be shared directly in chat\n  - Example: \n    ```\n    📽️ **Meeting Recording: Weekly Team Sync**\n    ⏱️ Timestamp: 00:12:35\n    🎤 Speaker: Sarah Johnson\n    📝 Discussing the new product roadmap\n\n    🔗 [View Recording](https://meetingbaas.com/viewer/abc123?t=755)\n    ```\n\n- `shareMeetingSegments`: Creates a list of links to multiple important moments in a meeting\n  - Parameters: `botId` and an array of `segments` with timestamps, speakers, and descriptions\n  - Returns: Markdown-formatted list of segments with direct links to each moment\n  - Useful for creating a table of contents for a long meeting\n\n## Example Workflows\n\n### Recording a Meeting\n\n1. Create a bot for your upcoming meeting:\n\n   ```\n   \"Create a bot for my Zoom meeting at https://zoom.us/j/123456789\"\n   ```\n\n2. The bot joins the meeting automatically and begins recording.\n\n3. Check recording status:\n   ```\n   \"What's the status of my meeting recording for the Zoom call I started earlier?\"\n   ```\n\n### Calendar Integration and Automatic Recording\n\n1. Get guidance on obtaining OAuth credentials:\n\n   ```\n   \"I want to integrate my Google Calendar. How do I get OAuth credentials?\"\n   ```\n\n2. List your available calendars before integration:\n\n   ```\n   \"List my available Google calendars. Here are my OAuth credentials:\n   - Client ID: my-client-id-123456789.apps.googleusercontent.com\n   - Client Secret: my-client-secret-ABCDEF123456\n   - Refresh Token: my-refresh-token-ABCDEF123456789\"\n   ```\n\n3. Set up calendar integration with a specific calendar:\n\n   ```\n   \"Integrate my Google Calendar using these credentials:\n   - Platform: Google\n   - Client ID: my-client-id-123456789.apps.googleusercontent.com\n   - Client Secret: my-client-secret-ABCDEF123456\n   - Refresh Token: my-refresh-token-ABCDEF123456789\n   - Raw Calendar ID: primary@gmail.com\"\n   ```\n\n4. View your upcoming meetings:\n\n   ```\n   \"Show me my upcoming meetings from calendar 1a2b3c4d-5e6f-7a8b-9c0d-1e2f3a4b5c6d\"\n   ```\n\n5. Schedule recording for an upcoming meeting:\n\n   ```\n   \"Schedule a recording for my team meeting with event ID 7a8b9c0d-1e2f-3a4b-5c6d-7e8f9a0b1c2d.\n   Configure the bot with:\n   - Name: Team Meeting Bot\n   - Recording Mode: gallery_view\n   - Entry Message: Hello everyone, I'm here to record the meeting\"\n   ```\n\n6. Check all recordings scheduled in your calendar:\n\n   ```\n   \"Show me all meetings in my calendar that have recordings scheduled\"\n   ```\n\n7. Cancel a previously scheduled recording:\n\n   ```\n   \"Cancel the recording for event 7a8b9c0d-1e2f-3a4b-5c6d-7e8f9a0b1c2d\"\n   ```\n\n8. Refresh calendar data if meetings are missing:\n\n   ```\n   \"Force a resync of all my connected calendars\"\n   ```\n\n### Analyzing Meeting Content\n\n1. Get the full transcript of a meeting:\n\n   ```\n   \"Get the transcript from my team meeting with bot ID abc-123\"\n   ```\n\n2. Find key moments in a meeting:\n\n   ```\n   \"Identify key moments from yesterday's product planning meeting with bot ID xyz-456\"\n   ```\n\n3. Share a specific moment from a meeting:\n\n   ```\n   \"Create a shareable link to the part of meeting abc-123 at timestamp 12:45 where John was talking about the budget\"\n   ```\n\n### Using Direct Credential Tools\n\nYou can provide API credentials directly in your queries:\n\n1. List events with direct credentials:\n\n   ```\n   \"List events from calendar 5c99f8a4-f498-40d0-88f0-29f698c53c51 using API key tesban where attendee is philipe@spoke.app\"\n   ```\n\n2. Schedule a recording with direct credentials:\n\n   ```\n   \"Schedule a recording for event 78d06b42-794f-4efe-8195-62db1f0052d5 using API key tesban with bot name 'Weekly Meeting Bot'\"\n   ```\n\n3. Cancel a recording with direct credentials:\n\n   ```\n   \"Cancel the recording for event 97cd62f0-ea9b-42b3-add5-7a607ce6d80f using API key tesban\"\n   ```\n\n4. Get meeting data with direct credentials:\n\n   ```\n   \"Get meeting data for meeting 47de9462-bea7-406c-b79a-fd6b82c3de76 using API key tesban\"\n   ```\n\n### Using AI-Generated QR Codes as Bot Avatars\n\n1. Generate a QR code with your contact information and a custom design:\n\n   ```\n   \"Generate a QR code with the following parameters:\n   - Type: email\n   - To: john.doe@company.com\n   - Prompt: Create a professional-looking QR code with abstract blue patterns that resemble a corporate logo\n   - Style: style_crystal\"\n   ```\n\n2. Use the generated QR code as a bot avatar in a meeting:\n\n   ```\n   \"Join my Zoom meeting at https://zoom.us/j/123456789 with the following parameters:\n   - Bot name: QR Code Assistant\n   - Bot image: [URL from the generated QR code]\n   - Entry message: Hello everyone, I'm here to record the meeting. You can scan my avatar to get my contact information.\"\n   ```\n\n3. Generate a QR code with a meeting link for easy sharing:\n\n   ```\n   \"Generate a QR code with the following parameters:\n   - Type: url\n   - To: https://zoom.us/j/123456789\n   - Prompt: Create a colorful QR code with a calendar icon in the center\n   - Style: style_rounded\"\n   ```\n\n### Accessing Meeting Recordings\n\nMeeting recordings can be accessed directly through the Meeting BaaS viewer using the bot ID:\n\n```\nhttps://meetingbaas.com/viewer/{BOT_ID}\n```\n\nFor example:\n```\nhttps://meetingbaas.com/viewer/67738f48-2360-4f9e-a999-275a74208ff5\n```\n\nThis viewer provides:\n- The meeting video recording\n- Synchronized transcript with speaker identification\n- Navigation by speaker or topic\n- Direct link sharing with teammates\n\nWhen using the `createBot`, `getBots`, or search tools, you'll receive bot IDs that can be used to construct these viewer URLs for easy access to recordings.\n\n> **Important**: All meeting recordings and links are automatically shared with colleagues who have the same corporate email domain (e.g., @yourcompany.com). This allows your entire team to access recordings without requiring individual permissions, creating a collaborative environment where meeting knowledge is accessible to everyone in your organization.\n\n## Configuration\n\nThe server can be configured through environment variables or by editing the `src/config.ts` file.\n\nKey configuration options:\n\n- `PORT`: The port the server listens on (default: 7017)\n- `API_BASE_URL`: The base URL for the Meeting BaaS API\n- `DEFAULT_API_KEY`: Default API key for testing\n\n## Integration with Cursor\n\nTo integrate with Cursor:\n\n1. Open Cursor\n2. Go to Settings\n3. Navigate to \"Model Context Protocol\"\n4. Add a new server with:\n   - Name: \"Meeting BaaS MCP\"\n   - Type: \"sse\"\n   - Server URL: \"http://localhost:7017/mcp\"\n   - Optionally add headers if authentication is required\n\n## Development\n\n### Build\n\n```bash\nnpm run build\n```\n\n### Test with MCP Inspector\n\n```bash\nnpm run inspect\n```\n\n### Development mode (with auto-reload)\n\n```bash\nnpm run dev\n```\n\n### Log Management\n\nThe server includes optimized logging with:\n\n```bash\nnpm run cleanup\n```\n\nThis command:\n- Cleans up unnecessary log files and cached data\n- Filters out repetitive ping messages from logs\n- Reduces disk usage while preserving important log information\n- Maintains a smaller log footprint for long-running servers\n\n## Project Structure\n\n- `src/index.ts`: Main entry point\n- `src/tools/`: Tool implementations\n- `src/resources/`: Resource definitions\n- `src/api/`: API client for the Meeting BaaS backend\n- `src/types/`: TypeScript type definitions\n- `src/config.ts`: Server configuration\n- `src/utils/`: Utility functions\n  - `logging.ts`: Log filtering and management\n  - `tinyDb.ts`: Persistent bot tracking database\n\n## Authentication\n\nThe server expects an API key in the `x-api-key` header for authentication. You can configure the default API key in the configuration.\n\nDirect authentication is also supported in many tools (named with \"WithCredentials\") where you can provide the API key directly as a parameter rather than in headers.\n\n## License\n\n[MIT](LICENSE)\n\n## QR Code API Key Configuration\n\nThe QR code generator tool requires an API key from QR Code AI API. There are several ways to provide this:\n\n1. **Directly in the prompt**: Include your API key directly in the prompt text when using the `generateQRCode` tool, e.g., \"Generate a QR code for my website https://example.com with API key: qrc_your_key\"\n\n2. **As a parameter**: Provide your API key as the `apiKey` parameter when using the `generateQRCode` tool\n\n3. **Environment variable**: Set the `QRCODE_API_KEY` environment variable\n\n4. **Claude Desktop config**: Add the API key to your Claude Desktop configuration file located at:\n   - Mac/Linux: `~/Library/Application Support/Claude/claude_desktop_config.json`\n   - Windows: `%APPDATA%\\Claude\\claude_desktop_config.json`\n\n   Example configuration:\n   ```json\n   {\n     \"headers\": {\n       \"x-api-key\": \"qrc_your_key_here\" \n     }\n   }\n   ```\n\nThe tool will check for the API key in the order listed above. If no API key is provided, the default API key will be used if available.\n\nYou can obtain an API key by signing up at [QR Code AI API](https://qrcode-ai.com).",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "meeting",
        "search",
        "mcp",
        "search meeting",
        "baas meeting",
        "manage meeting"
      ],
      "category": "web-search"
    },
    "MrunmayS--exa-mcp-server": {
      "owner": "MrunmayS",
      "name": "exa-mcp-server",
      "url": "https://github.com/MrunmayS/exa-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/MrunmayS.webp",
      "description": "Connects AI assistants to the Exa AI Search API for real-time web searches, facilitating access to updated web information in a structured manner.",
      "stars": 0,
      "forks": 1,
      "license": "No License",
      "language": "",
      "updated_at": "2025-01-30T12:31:07Z",
      "readme_content": "# Exa MCP Server 🔍\n[![npm version](https://badge.fury.io/js/exa-mcp-server.svg)](https://www.npmjs.com/package/exa-mcp-server)\n[![smithery badge](https://smithery.ai/badge/exa)](https://smithery.ai/server/exa)\n\nA Model Context Protocol (MCP) server lets AI assistants like Claude use the Exa AI Search API for web searches. This setup allows AI models to get real-time web information in a safe and controlled way.\n\nDemo video https://www.loom.com/share/ac676f29664e4c6cb33a2f0a63772038?sid=0e72619f-5bfc-415d-a705-63d326373f60\n\n## What is MCP? 🤔\n\nThe Model Context Protocol (MCP) is a system that lets AI apps, like Claude Desktop, connect to external tools and data sources. It gives a clear and safe way for AI assistants to work with local services and APIs while keeping the user in control.\n\n## What does this server do? 🚀\n\nThe Exa MCP server:\n- Enables AI assistants to perform web searches using Exa's powerful search API\n- Provides structured search results including titles, URLs, and content snippets\n- Handles rate limiting and error cases gracefully\n\n\n## Prerequisites 📋\n\nBefore you begin, ensure you have:\n\n- [Node.js](https://nodejs.org/) (v18 or higher)\n- [Claude Desktop](https://claude.ai/download) installed\n- An [Exa API key](https://dashboard.exa.ai/api-keys)\n- Git installed\n\nYou can verify your Node.js installation by running:\n```bash\nnode --version  # Should show v18.0.0 or higher\n````\n\n## Installation 🛠️\n\n### NPM Installation\n\n```bash\nnpm install -g exa-mcp-server\n```\n\n### Using Smithery\n\nTo install the Exa MCP server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/exa):\n\n```bash\nnpx -y @smithery/cli install exa --client claude\n```\n\n### Manual Installation\n\n1.  Clone the repository:\n    \n\n```\ngit clone https://github.com/exa-labs/exa-mcp-server.git\ncd exa-mcp-server\n```\n\n2.  Install dependencies:\n    \n\n```\nnpm install --save axios dotenv\n```\n\n3.  Build the project:\n    \n\n```\nnpm run build\n```\n\n4.  Create a global link (this makes the server executable from anywhere):\n    \n\n```\nnpm link\n```\n\n## Configuration ⚙️\n\n### 1. Configure Claude Desktop to recognize the Exa MCP server\n\nYou can find claude_desktop_config.json inside the settings of Claude Desktop app:\n\nOpen the Claude Desktop app and enable Developer Mode from the top-left menu bar. \n\nOnce enabled, open Settings (also from the top-left menu bar) and navigate to the Developer Option, where you'll find the Edit Config button. Clicking it will open the claude_desktop_config.json file, allowing you to make the necessary edits. \n\nOR (if you want to open claude_desktop_config.json from terminal)\n\n#### For macOS:\n\n1.  Open your Claude Desktop configuration:\n    \n\n```\ncode ~/Library/Application\\ Support/Claude/claude_desktop_config.json\n```\n\n#### For Windows:\n\n1.  Open your Claude Desktop configuration:\n    \n\n```\ncode %APPDATA%\\Claude\\claude_desktop_config.json\n```\n\n\n### 2.  Add the Exa server configuration:\n    \n\n```\n{\n  \"mcpServers\": {\n    \"exa\": {\n      \"command\": \"npx\",\n      \"args\": [\"/path/to/exa-mcp-server/build/index.js\"],\n      \"env\": {\n        \"EXA_API_KEY\": \"your-api-key-here\"\n      }\n    }\n  }\n}\n```\n\nReplace `your-api-key-here` with your actual Exa API key from [dashboard.exa.ai/api-keys](https://dashboard.exa.ai/api-keys).\n\n### 3. Restart Claude Desktop\n\nFor the changes to take effect:\n\n1.  Completely quit Claude Desktop (not just close the window)\n    \n2.  Start Claude Desktop again\n    \n3.  Look for the 🔌 icon to verify the Exa server is connected\n    \n\n## Usage 🎯\n\nOnce configured, you can ask Claude to perform web searches. Here are some example prompts:\n\n```\nCan you search for recent developments in quantum computing?\n```\n\n```\nSearch for and summarize the latest news about artificial intelligence startups in new york.\n```\n\n```\nFind and analyze recent research papers about climate change solutions.\n```\n\nThe server will:\n\n1.  Process the search request\n    \n2.  Query the Exa API\n    \n3.  Return formatted results to Claude\n    \n4.  Cache the search for future reference\n    \n\n## Features ✨\n\n*   **Web Search Tool**: Enables Claude to search the web using natural language queries\n    \n*   **Error Handling**: Gracefully handles API errors and rate limits\n    \n*   **Type Safety**: Full TypeScript implementation with proper type checking\n    \n\n## Troubleshooting 🔧\n\n### Common Issues\n\n1.  **Server Not Found**\n    \n    *   Verify the npm link is correctly set up\n        \n    *   Check Claude Desktop configuration syntax\n        \n    *   Ensure Node.js is properly installed\n        \n2.  **API Key Issues**\n    \n    *   Confirm your Exa API key is valid\n        \n    *   Check the API key is correctly set in the Claude Desktop config\n        \n    *   Verify no spaces or quotes around the API key\n        \n3.  **Connection Issues**\n    \n    *   Restart Claude Desktop completely\n        \n    *   Check Claude Desktop logs:\n        \n        ```\n        # macOS\n        tail -n 20 -f ~/Library/Logs/Claude/mcp*.log\n        ```\n        \n\n### Getting Help\n\nIf you encounter issues review the [MCP Documentation](https://modelcontextprotocol.io)\n    \n    \n\n\n## Acknowledgments 🙏\n\n*   [Exa AI](https://exa.ai) for their powerful search API\n    \n*   [Model Context Protocol](https://modelcontextprotocol.io) for the MCP specification\n    \n*   [Anthropic](https://anthropic.com) for Claude Desktop\n    \n",
      "npm_url": "https://www.npmjs.com/package/exa-mcp-server",
      "npm_downloads": 97542,
      "keywords": [
        "searches",
        "ai",
        "search",
        "ai search",
        "ai assistants",
        "exa ai"
      ],
      "category": "web-search"
    },
    "Msparihar--mcp-server-firecrawl": {
      "owner": "Msparihar",
      "name": "mcp-server-firecrawl",
      "url": "https://github.com/Msparihar/mcp-server-firecrawl",
      "imageUrl": "/freedevtools/mcp/pfp/Msparihar.webp",
      "description": "Provides capabilities for web scraping, intelligent content searching, and site crawling using the Firecrawl API, facilitating customizable data extraction and structured output.",
      "stars": 2,
      "forks": 3,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-02-20T06:10:43Z",
      "readme_content": "# Firecrawl MCP Server\n\nA Model Context Protocol (MCP) server for web scraping, content searching, site crawling, and data extraction using the Firecrawl API.\n\n## Features\n\n- **Web Scraping**: Extract content from any webpage with customizable options\n  - Mobile device emulation\n  - Ad and popup blocking\n  - Content filtering\n  - Structured data extraction\n  - Multiple output formats\n\n- **Content Search**: Intelligent search capabilities\n  - Multi-language support\n  - Location-based results\n  - Customizable result limits\n  - Structured output formats\n\n- **Site Crawling**: Advanced web crawling functionality\n  - Depth control\n  - Path filtering\n  - Rate limiting\n  - Progress tracking\n  - Sitemap integration\n\n- **Site Mapping**: Generate site structure maps\n  - Subdomain support\n  - Search filtering\n  - Link analysis\n  - Visual hierarchy\n\n- **Data Extraction**: Extract structured data from multiple URLs\n  - Schema validation\n  - Batch processing\n  - Web search enrichment\n  - Custom extraction prompts\n\n## Installation\n\n```bash\n# Global installation\nnpm install -g @modelcontextprotocol/mcp-server-firecrawl\n\n# Local project installation\nnpm install @modelcontextprotocol/mcp-server-firecrawl\n```\n\n## Quick Start\n\n1. Get your Firecrawl API key from the [developer portal](https://firecrawl.dev/dashboard)\n\n2. Set your API key:\n\n   **Unix/Linux/macOS (bash/zsh):**\n\n   ```bash\n   export FIRECRAWL_API_KEY=your-api-key\n   ```\n\n   **Windows (Command Prompt):**\n\n   ```cmd\n   set FIRECRAWL_API_KEY=your-api-key\n   ```\n\n   **Windows (PowerShell):**\n\n   ```powershell\n   $env:FIRECRAWL_API_KEY = \"your-api-key\"\n   ```\n\n   **Alternative: Using .env file (recommended for development):**\n\n   ```bash\n   # Install dotenv\n   npm install dotenv\n\n   # Create .env file\n   echo \"FIRECRAWL_API_KEY=your-api-key\" > .env\n   ```\n\n   Then in your code:\n\n   ```javascript\n   import dotenv from 'dotenv';\n   dotenv.config();\n   ```\n\n3. Run the server:\n\n   ```bash\n   mcp-server-firecrawl\n   ```\n\n## Integration\n\n### Claude Desktop App\n\nAdd to your MCP settings:\n\n```json\n{\n  \"firecrawl\": {\n    \"command\": \"mcp-server-firecrawl\",\n    \"env\": {\n      \"FIRECRAWL_API_KEY\": \"your-api-key\"\n    }\n  }\n}\n```\n\n### Claude VSCode Extension\n\nAdd to your MCP configuration:\n\n```json\n{\n  \"mcpServers\": {\n    \"firecrawl\": {\n      \"command\": \"mcp-server-firecrawl\",\n      \"env\": {\n        \"FIRECRAWL_API_KEY\": \"your-api-key\"\n      }\n    }\n  }\n}\n```\n\n## Usage Examples\n\n### Web Scraping\n\n```typescript\n// Basic scraping\n{\n  name: \"scrape_url\",\n  arguments: {\n    url: \"https://example.com\",\n    formats: [\"markdown\"],\n    onlyMainContent: true\n  }\n}\n\n// Advanced extraction\n{\n  name: \"scrape_url\",\n  arguments: {\n    url: \"https://example.com/blog\",\n    jsonOptions: {\n      prompt: \"Extract article content\",\n      schema: {\n        title: \"string\",\n        content: \"string\"\n      }\n    },\n    mobile: true,\n    blockAds: true\n  }\n}\n```\n\n### Site Crawling\n\n```typescript\n// Basic crawling\n{\n  name: \"crawl\",\n  arguments: {\n    url: \"https://example.com\",\n    maxDepth: 2,\n    limit: 100\n  }\n}\n\n// Advanced crawling\n{\n  name: \"crawl\",\n  arguments: {\n    url: \"https://example.com\",\n    maxDepth: 3,\n    includePaths: [\"/blog\", \"/products\"],\n    excludePaths: [\"/admin\"],\n    ignoreQueryParameters: true\n  }\n}\n```\n\n### Site Mapping\n\n```typescript\n// Generate site map\n{\n  name: \"map\",\n  arguments: {\n    url: \"https://example.com\",\n    includeSubdomains: true,\n    limit: 1000\n  }\n}\n```\n\n### Data Extraction\n\n```typescript\n// Extract structured data\n{\n  name: \"extract\",\n  arguments: {\n    urls: [\"https://example.com/product1\", \"https://example.com/product2\"],\n    prompt: \"Extract product details\",\n    schema: {\n      name: \"string\",\n      price: \"number\",\n      description: \"string\"\n    }\n  }\n}\n```\n\n## Configuration\n\nSee [configuration guide](https://github.com/Msparihar/mcp-server-firecrawl/blob/main/docs/configuration.md) for detailed setup options.\n\n## API Documentation\n\nSee [API documentation](https://github.com/Msparihar/mcp-server-firecrawl/blob/main/docs/api.md) for detailed endpoint specifications.\n\n## Development\n\n```bash\n# Install dependencies\nnpm install\n\n# Build\nnpm run build\n\n# Run tests\nnpm test\n\n# Start in development mode\nnpm run dev\n```\n\n## Examples\n\nCheck the [examples](https://github.com/Msparihar/mcp-server-firecrawl/tree/main/examples) directory for more usage examples:\n\n- Basic scraping: [scrape.ts](https://github.com/Msparihar/mcp-server-firecrawl/blob/main/examples/scrape.ts)\n- Crawling and mapping: [crawl-and-map.ts](https://github.com/Msparihar/mcp-server-firecrawl/blob/main/examples/crawl-and-map.ts)\n\n## Error Handling\n\nThe server implements robust error handling:\n\n- Rate limiting with exponential backoff\n- Automatic retries\n- Detailed error messages\n- Debug logging\n\n## Security\n\n- API key protection\n- Request validation\n- Domain allowlisting\n- Rate limiting\n- Safe error messages\n\n## Contributing\n\nSee [CONTRIBUTING.md](https://github.com/Msparihar/mcp-server-firecrawl/blob/main/CONTRIBUTING.md) for contribution guidelines.\n\n## License\n\nMIT License - see [LICENSE](https://github.com/Msparihar/mcp-server-firecrawl/blob/main/LICENSE) for details.\n",
      "npm_url": "https://www.npmjs.com/package/mcp-server-firecrawl",
      "npm_downloads": 9784,
      "keywords": [
        "firecrawl",
        "scraping",
        "crawling",
        "firecrawl provides",
        "server firecrawl",
        "firecrawl api"
      ],
      "category": "web-search"
    },
    "Muzzera--web-ui": {
      "owner": "Muzzera",
      "name": "web-ui",
      "url": "https://github.com/Muzzera/web-ui",
      "imageUrl": "/freedevtools/mcp/pfp/Muzzera.webp",
      "description": "Interact seamlessly with AI agents via a user-friendly interface, enabling the use of various Large Language Models while maintaining persistent browser sessions for productive tasks. The server supports high-definition screen recording and custom browser functionality without requiring re-authentication.",
      "stars": 1,
      "forks": 0,
      "license": "MIT License",
      "language": "",
      "updated_at": "2025-03-29T00:52:09Z",
      "readme_content": "<br/>\n\n[![GitHub stars](https://img.shields.io/github/stars/browser-use/web-ui?style=social)](https://github.com/browser-use/web-ui/stargazers)\n[![Discord](https://img.shields.io/discord/1303749220842340412?color=7289DA&label=Discord&logo=discord&logoColor=white)](https://link.browser-use.com/discord)\n[![Documentation](https://img.shields.io/badge/Documentation-📕-blue)](https://docs.browser-use.com)\n[![WarmShao](https://img.shields.io/twitter/follow/warmshao?style=social)](https://x.com/warmshao)\n\nThis project builds upon the foundation of the [browser-use](https://github.com/browser-use/browser-use), which is designed to make websites accessible for AI agents.\n\nWe would like to officially thank [WarmShao](https://github.com/warmshao) for his contribution to this project.\n\n**WebUI:** is built on Gradio and supports a most of `browser-use` functionalities. This UI is designed to be user-friendly and enables easy interaction with the browser agent.\n\n**Expanded LLM Support:** We've integrated support for various Large Language Models (LLMs), including: Gemini, OpenAI, Azure OpenAI, Anthropic, DeepSeek, Ollama etc. And we plan to add support for even more models in the future.\n\n**Custom Browser Support:** You can use your own browser with our tool, eliminating the need to re-login to sites or deal with other authentication challenges. This feature also supports high-definition screen recording.\n\n**Persistent Browser Sessions:** You can choose to keep the browser window open between AI tasks, allowing you to see the complete history and state of AI interactions.\n\n<video src=\"https://github.com/user-attachments/assets/56bc7080-f2e3-4367-af22-6bf2245ff6cb\" controls=\"controls\">Your browser does not support playing this video!</video>\n\n## Installation Options\n\n### Option 1: Local Installation\n\nRead the [quickstart guide](https://docs.browser-use.com/quickstart#prepare-the-environment) or follow the steps below to get started.\n\n> Python 3.11 or higher is required.\n\nFirst, we recommend using [uv](https://docs.astral.sh/uv/) to setup the Python environment.\n\n```bash\nuv venv --python 3.11\n```\n\nand activate it with:\n\n```bash\nsource .venv/bin/activate\n```\n\nInstall the dependencies:\n\n```bash\nuv pip install -r requirements.txt\n```\n\nThen install playwright:\n\n```bash\nplaywright install\n```\n\n### Option 2: Docker Installation\n\n1. **Prerequisites:**\n   - Docker and Docker Compose installed on your system\n   - Git to clone the repository\n\n2. **Setup:**\n   ```bash\n   # Clone the repository\n   git clone https://github.com/browser-use/web-ui.git\n   cd web-ui\n\n   # Copy and configure environment variables\n   cp .env.example .env\n   # Edit .env with your preferred text editor and add your API keys\n   ```\n\n3. **Run with Docker:**\n   ```bash\n   # Build and start the container with default settings (browser closes after AI tasks)\n   docker compose up --build\n\n   # Or run with persistent browser (browser stays open between AI tasks)\n   CHROME_PERSISTENT_SESSION=true docker compose up --build\n   ```\n\n4. **Access the Application:**\n   - WebUI: `http://localhost:7788`\n   - VNC Viewer (to see browser interactions): `http://localhost:6080/vnc.html`\n   \n   Default VNC password is \"vncpassword\". You can change it by setting the `VNC_PASSWORD` environment variable in your `.env` file.\n\n\n## Usage\n\n### Local Setup\n1.  Copy `.env.example` to `.env` and set your environment variables, including API keys for the LLM. `cp .env.example .env`\n2.  **Run the WebUI:**\n    ```bash\n    python webui.py --ip 127.0.0.1 --port 7788\n    ```\n4. WebUI options:\n   - `--ip`: The IP address to bind the WebUI to. Default is `127.0.0.1`.\n   - `--port`: The port to bind the WebUI to. Default is `7788`.\n   - `--theme`: The theme for the user interface. Default is `Ocean`.\n     - **Default**: The standard theme with a balanced design.\n     - **Soft**: A gentle, muted color scheme for a relaxed viewing experience.\n     - **Monochrome**: A grayscale theme with minimal color for simplicity and focus.\n     - **Glass**: A sleek, semi-transparent design for a modern appearance.\n     - **Origin**: A classic, retro-inspired theme for a nostalgic feel.\n     - **Citrus**: A vibrant, citrus-inspired palette with bright and fresh colors.\n     - **Ocean** (default): A blue, ocean-inspired theme providing a calming effect.\n   - `--dark-mode`: Enables dark mode for the user interface.\n3.  **Access the WebUI:** Open your web browser and navigate to `http://127.0.0.1:7788`.\n4.  **Using Your Own Browser(Optional):**\n    - Set `CHROME_PATH` to the executable path of your browser and `CHROME_USER_DATA` to the user data directory of your browser. Leave `CHROME_USER_DATA` empty if you want to use local user data.\n      - Windows\n        ```env\n         CHROME_PATH=\"C:\\Program Files\\Google\\Chrome\\Application\\chrome.exe\"\n         CHROME_USER_DATA=\"C:\\Users\\YourUsername\\AppData\\Local\\Google\\Chrome\\User Data\"\n        ```\n        > Note: Replace `YourUsername` with your actual Windows username for Windows systems.\n      - Mac\n        ```env\n         CHROME_PATH=\"/Applications/Google Chrome.app/Contents/MacOS/Google Chrome\"\n         CHROME_USER_DATA=\"/Users/YourUsername/Library/Application Support/Google/Chrome\"\n        ```\n    - Close all Chrome windows\n    - Open the WebUI in a non-Chrome browser, such as Firefox or Edge. This is important because the persistent browser context will use the Chrome data when running the agent.\n    - Check the \"Use Own Browser\" option within the Browser Settings.\n5. **Keep Browser Open(Optional):**\n    - Set `CHROME_PERSISTENT_SESSION=true` in the `.env` file.\n\n### Docker Setup\n1. **Environment Variables:**\n   - All configuration is done through the `.env` file\n   - Available environment variables:\n     ```\n     # LLM API Keys\n     OPENAI_API_KEY=your_key_here\n     ANTHROPIC_API_KEY=your_key_here\n     GOOGLE_API_KEY=your_key_here\n\n     # Browser Settings\n     CHROME_PERSISTENT_SESSION=true   # Set to true to keep browser open between AI tasks\n     RESOLUTION=1920x1080x24         # Custom resolution format: WIDTHxHEIGHTxDEPTH\n     RESOLUTION_WIDTH=1920           # Custom width in pixels\n     RESOLUTION_HEIGHT=1080          # Custom height in pixels\n\n     # VNC Settings\n     VNC_PASSWORD=your_vnc_password  # Optional, defaults to \"vncpassword\"\n     ```\n\n2. **Browser Persistence Modes:**\n   - **Default Mode (CHROME_PERSISTENT_SESSION=false):**\n     - Browser opens and closes with each AI task\n     - Clean state for each interaction\n     - Lower resource usage\n\n   - **Persistent Mode (CHROME_PERSISTENT_SESSION=true):**\n     - Browser stays open between AI tasks\n     - Maintains history and state\n     - Allows viewing previous AI interactions\n     - Set in `.env` file or via environment variable when starting container\n\n3. **Viewing Browser Interactions:**\n   - Access the noVNC viewer at `http://localhost:6080/vnc.html`\n   - Enter the VNC password (default: \"vncpassword\" or what you set in VNC_PASSWORD)\n   - You can now see all browser interactions in real-time\n\n4. **Container Management:**\n   ```bash\n   # Start with persistent browser\n   CHROME_PERSISTENT_SESSION=true docker compose up -d\n\n   # Start with default mode (browser closes after tasks)\n   docker compose up -d\n\n   # View logs\n   docker compose logs -f\n\n   # Stop the container\n   docker compose down\n   ```\n\n## Changelog\n- [x] **2025/01/26:** Thanks to @vvincent1234. Now browser-use-webui can combine with DeepSeek-r1 to engage in deep thinking!\n- [x] **2025/01/10:** Thanks to @casistack. Now we have Docker Setup option and also Support keep browser open between tasks.[Video tutorial demo](https://github.com/browser-use/web-ui/issues/1#issuecomment-2582511750).\n- [x] **2025/01/06:** Thanks to @richard-devbot. A New and Well-Designed WebUI is released. [Video tutorial demo](https://github.com/warmshao/browser-use-webui/issues/1#issuecomment-2573393113).",
      "npm_url": "https://www.npmjs.com/package/web-ui",
      "npm_downloads": 6569,
      "keywords": [
        "browser",
        "ui",
        "web",
        "muzzera web",
        "search muzzera",
        "web ui"
      ],
      "category": "web-search"
    },
    "NaveenBandarage--poke-mcp": {
      "owner": "NaveenBandarage",
      "name": "poke-mcp",
      "url": "https://github.com/NaveenBandarage/poke-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/NaveenBandarage.webp",
      "description": "Poke-MCP provides a standardized interface for accessing Pokémon information through the PokeAPI. It enables querying specific Pokémon details, discovering random Pokémon, and retrieving Pokémon based on their types or regions.",
      "stars": 11,
      "forks": 6,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-09-25T15:53:51Z",
      "readme_content": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/naveenbandarage-poke-mcp-badge.png)](https://mseep.ai/app/naveenbandarage-poke-mcp)\n\n[![smithery badge](https://smithery.ai/badge/@NaveenBandarage/poke-mcp)](https://smithery.ai/server/@NaveenBandarage/poke-mcp)\n\n<a href=\"https://glama.ai/mcp/servers/@NaveenBandarage/poke-mcp\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@NaveenBandarage/poke-mcp/badge\" alt=\"Poke-MCP MCP server\" />\n</a>\n\n## Overview\n\nPoke-MCP is a Model Context Protocol (MCP) server that provides Pokémon information through a standardized interface. It connects to the [PokeAPI](https://pokeapi.co/) to fetch Pokémon data and exposes it through MCP tools that can be used by any MCP-compatible client, such as Claude Desktop App, Continue, Cline, and others.\n\n**This server now supports HTTP transport using Server-Sent Events (SSE) for real-time communication, making it accessible over HTTP instead of just stdio.**\n\n## Features\n\n- Get information about specific Pokémon by name\n- Discover random Pokémon\n- Find random Pokémon from specific regions (Kanto, Johto, Hoenn, etc.)\n- Get random Pokémon of specific types (Fire, Water, Electric, etc.)\n- Natural language query interface for Pokémon information\n\n## How It Works\n\nPoke-MCP is built using the [Model Context Protocol](https://modelcontextprotocol.io/), which enables AI applications to access external tools and data sources in a standardized way. The server:\n\n1. Connects to the PokeAPI to fetch Pokémon data\n2. Exposes several tools through the MCP interface\n3. Processes requests from MCP clients\n4. Returns formatted Pokémon information\n\n### MCP Tools\n\nThe server provides the following tools:\n\n- get-pokemon: Get detailed information about a specific Pokémon by name\n- random-pokemon: Get information about a random Pokémon\n- random-pokemon-from-region: Get a random Pokémon from a specific region\n- random-pokemon-by-type: Get a random Pokémon of a specific type\n- pokemon-query: Answer natural language queries about Pokémon\n\n### Architecture\n\nThe server is built using:\n\n- TypeScript\n- MCP TypeScript SDK (@modelcontextprotocol/sdk)\n- Zod for input validation\n- HTTP transport with Server-Sent Events (SSE) for MCP communication\n- Node.js built-in HTTP server\n\n## Installation\n\n### Installing via Smithery\n\nTo install Pokémcp for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@NaveenBandarage/poke-mcp):\n\n```bash\nnpx -y @smithery/cli install @NaveenBandarage/poke-mcp --client claude\n```\n\n### Manual Installation\n\n```bash\n# Clone the repository\ngit clone https://github.com/yourusername/poke-mcp.git\ncd poke-mcp\n\n# Install dependencies\nnpm install\n\n# Build the project\nnpm run build\n```\n\n## Usage\n\n### Running the HTTP Server\n\nTo start the server in HTTP mode:\n\n```bash\n# Start the server (defaults to port 3000)\nnpm start\n\n# Or specify a custom port\nPORT=8080 npm start\n```\n\nThe server will be available at:\n\n- **Info endpoint**: `http://127.0.0.1:3000/` - Server information and status\n- **SSE endpoint**: `http://127.0.0.1:3000/sse` - Server-Sent Events connection for MCP clients\n- **Message endpoint**: `http://127.0.0.1:3000/message` - POST endpoint for sending MCP messages\n\n### With Claude Desktop App (HTTP Transport)\n\n1. Download and install [Claude Desktop App](https://claude.ai/download)\n2. Open Claude Desktop settings\n3. Go to Developer settings and edit the config file\n4. Add the following configuration for HTTP transport:\n\n```json\n{\n  \"mcpServers\": {\n    \"pokedex\": {\n      \"transport\": {\n        \"type\": \"sse\",\n        \"url\": \"http://127.0.0.1:3000/sse\"\n      }\n    }\n  }\n}\n```\n\n5. Start the Poke-MCP server: `npm start`\n6. Restart Claude Desktop\n7. You should now see the Pokémon tools available in Claude\n\n### Legacy Usage (stdio)\n\nFor backward compatibility, you can still run the server with stdio transport by reverting to the stdio implementation.\n\n### Example Queries\n\nOnce connected to an MCP client, you can ask questions like:\n\n- \"Tell me about Pikachu\"\n- \"Give me a random Pokémon\"\n- \"Show me a random Pokémon from Kanto\"\n- \"What's a random Water Pokémon?\"\n\n### Project Structure\n\n- src/index.ts: Main server implementation\n- src/types.ts: TypeScript type definitions for Pokémon data\n- package.json: Project dependencies and scripts\n- tsconfig.json: TypeScript configuration\n\n### Adding New Features\n\nTo add new tools or enhance existing ones:\n\n1. Define new helper functions to fetch and format data\n2. Register new tools using the server.tool() method\n3. Implement the tool logic to handle requests and return responses\n\n## License\n\nISC\n\n## Acknowledgments\n\n- [PokeAPI](https://pokeapi.co/) for providing the Pokémon data\n- [Model Context Protocol](https://modelcontextprotocol.io/) for the standardized interface\n\n---\n\nThis project demonstrates how to build custom MCP servers that can extend AI assistants with domain-specific knowledge and capabilities.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "pokeapi",
        "poke",
        "pokémon",
        "information pokeapi",
        "mcp poke",
        "pokémon information"
      ],
      "category": "web-search"
    },
    "NightTrek--Serper-search-mcp": {
      "owner": "NightTrek",
      "name": "Serper-search-mcp",
      "url": "https://github.com/NightTrek/Serper-search-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/NightTrek.webp",
      "description": "Integrates Google search capabilities through the Serper API, offering rich search data and an AI-powered Deep Research tool for thorough topic exploration and information synthesis.",
      "stars": 41,
      "forks": 15,
      "license": "No License",
      "language": "TypeScript",
      "updated_at": "2025-09-16T15:27:59Z",
      "readme_content": "# Serper Search MCP Server\n\nA Model Context Protocol server that provides Google search capabilities through the Serper API, along with an AI-powered Deep Research tool. This server enables easy integration of search and research functionality into your MCP-enabled applications.\n\n## ✨ Features\n\n- 🌐 Powerful Google search integration through Serper API\n- 🔄 Rich search response data including:\n  - Knowledge Graph information\n  - Organic search results\n  - \"People Also Ask\" questions\n  - Related searches\n- 🧠 AI-powered Deep Research tool:\n  - Performs multi-step, iterative research\n  - Generates sub-queries to explore topics thoroughly\n  - Synthesizes information from multiple sources\n  - Provides citations for all information\n  - Adjustable research depth levels\n  - Built-in quality metrics for continuous improvement\n- 🛠 Configurable search parameters:\n  - Country targeting\n  - Language selection\n  - Result count customization\n  - Autocorrect options\n- 🔒 Secure API key handling\n- ⚡️ Rate limiting and caching support\n- 📝 TypeScript support with full type definitions\n- 📊 Integrated performance metrics for result optimization\n\n## 🚀 Installation\n\n1. Clone the repository:\n```bash\ngit clone https://github.com/yourusername/serper-search-server.git\ncd serper-search-server\n```\n\n2. Install dependencies:\n```bash\npnpm install\n```\n\n3. Build the server:\n```bash\npnpm run build\n```\n\n## ⚙️ Configuration\n\n1. Get your Serper API key from [Serper.dev](https://serper.de\n\n2. Create a `.env` file in the root directory:\n```env\n# Required\nSERPER_API_KEY=your_api_key_here\n\n# Optional - Advanced Quality Metrics Configuration (pre-configured by default)\nUSAGE_METRICS_KEY=your-custom-metrics-key     # Optional\nUSAGE_PROJECT_ID=your-custom-project-id       # Optional\nMETRICS_ENDPOINT=https://your-custom-host.com # Optional\nDISABLE_METRICS=false                         # Not recommended\n```\n\nSee [TELEMETRY.md](TELEMETRY.md) for detailed information about:\n- Quality metrics collection\n- Performance monitoring\n- Usage analytics\n- Dashboard setup\n- Continuous improvement\n\n## 🔌 Integration\n\n### Claude Desktop\n\nAdd the server config to your Claude Desktop configuration:\n\n**MacOS**: `~/Library/Application Support/Claude/claude_desktop_config.json`\n**Windows**: `%APPDATA%/Claude/claude_desktop_config.json`\n\n```json\n{\n  \"mcpServers\": {\n    \"serper-search-server\": {\n      \"command\": \"/path/to/serper-search-server/build/index.js\",\n      \"env\": {\n        \"SERPER_API_KEY\": \"your_api_key_here\"\n      }\n    }\n  }\n}\n```\n\n## 🛠 Usage\n\n### Search Tool\n\nThe server provides a powerful search tool with the following parameters:\n\n```typescript\n{\n  \"query\": string,          // Search query\n  \"numResults\"?: number,    // Number of results (default: 10, max: 100)\n  \"gl\"?: string,           // Country code (e.g., \"us\", \"uk\")\n  \"hl\"?: string,           // Language code (e.g., \"en\", \"es\")\n  \"autocorrect\"?: boolean, // Enable autocorrect (default: true)\n  \"type\"?: \"search\"        // Search type (more types coming soon)\n}\n```\n\n### Deep Research Tool\n\nFor more comprehensive research needs, the server provides a deep research tool that performs multi-step research with the following parameters:\n\n```typescript\n{\n  \"query\": string,          // Research query or question\n  \"depth\"?: \"basic\" | \"standard\" | \"deep\",  // Research depth (default: \"standard\")\n  \"maxSources\"?: number     // Maximum sources to include (default: 10)\n}\n```\n\nThe deep research tool:\n- Breaks down complex queries into focused sub-queries\n- Executes multiple searches to gather comprehensive information\n- Uses AI to synthesize information from multiple sources\n- Formats results with proper citations and references\n- Adapts its research strategy based on intermediate results\n- Collects anonymous quality metrics to improve search results\n\nDepth Levels:\n- basic: Quick overview (3-5 sources, ~5 min)\n  Good for: Simple facts, quick definitions, straightforward questions\n- standard: Comprehensive analysis (5-10 sources, ~10 min)\n  Good for: Most research needs, balanced depth and speed\n- deep: Exhaustive research (10+ sources, ~15-20 min)\n  Good for: Complex topics, academic research, thorough analysis\n\n### Search Tool Example Response\n\nThe search results include rich data:\n\n```json\n{\n  \"searchParameters\": {\n    \"q\": \"apple inc\",\n    \"gl\": \"us\",\n    \"hl\": \"en\",\n    \"autocorrect\": true,\n    \"type\": \"search\"\n  },\n  \"knowledgeGraph\": {\n    \"title\": \"Apple\",\n    \"type\": \"Technology company\",\n    \"website\": \"http://www.apple.com/\",\n    \"description\": \"Apple Inc. is an American multinational technology company...\",\n    \"attributes\": {\n      \"Headquarters\": \"Cupertino, CA\",\n      \"CEO\": \"Tim Cook (Aug 24, 2011–)\",\n      \"Founded\": \"April 1, 1976, Los Altos, CA\"\n    }\n  },\n  \"organic\": [\n    {\n      \"title\": \"Apple\",\n      \"link\": \"https://www.apple.com/\",\n      \"snippet\": \"Discover the innovative world of Apple...\",\n      \"position\": 1\n    }\n  ],\n  \"peopleAlsoAsk\": [\n    {\n      \"question\": \"What does Apple Inc mean?\",\n      \"snippet\": \"Apple Inc., formerly Apple Computer, Inc....\",\n      \"link\": \"https://www.britannica.com/topic/Apple-Inc\"\n    }\n  ],\n  \"relatedSearches\": [\n    {\n      \"query\": \"Who invented the iPhone\"\n    }\n  ]\n}\n```\n\n## 🔍 Response Types\n\n### Knowledge Graph\nContains entity information when available:\n- Title and type\n- Website URL\n- Description\n- Key attributes\n\n### Organic Results\nList of search results including:\n- Title and URL\n- Snippet (description)\n- Position in results\n- Sitelinks when available\n\n### People Also Ask\nCommon questions related to the search:\n- Question text\n- Answer snippet\n- Source link\n\n### Related Searches\nList of related search queries users often make.\n\n## 📊 Quality Metrics\n\nThe Deep Research tool includes integrated quality metrics:\n\n- Research process metrics\n- Performance monitoring\n- Issue tracking\n- Usage patterns\n- Result quality indicators\n\nSee [TELEMETRY.md](TELEMETRY.md) for detailed information about the metrics collected to improve search quality.\n\n## 🤝 Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n\n## 📝 License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n\n## 🙏 Acknowledgments\n\n- [Serper API](https://serper.dev) for providing the Google search capabilities\n- [Model Context Protocol](https://github.com/modelcontextprotocol/mcp) for the MCP framework\n- [PostHog](https://posthog.com) for analytics capabilities\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "search",
        "google",
        "serper",
        "serper search",
        "web search",
        "search capabilities"
      ],
      "category": "web-search"
    },
    "Omniyin--halo": {
      "owner": "Omniyin",
      "name": "halo",
      "url": "https://github.com/Omniyin/halo",
      "imageUrl": "/freedevtools/mcp/pfp/Omniyin.webp",
      "description": "Halo is a modern blogging platform designed for personal use, enabling users to create, manage, and publish blog content with ease. It offers a user-friendly interface and features tailored to enhance the blogging experience.",
      "stars": 0,
      "forks": 0,
      "license": "GNU General Public License v3.0",
      "language": "",
      "updated_at": "2021-10-14T15:25:21Z",
      "readme_content": "<p align=\"center\">\n    <a href=\"https://halo.run\" target=\"_blank\" rel=\"noopener noreferrer\">\n        <img width=\"100\" src=\"https://halo.run/logo\" alt=\"Halo logo\">\n    </a>\n</p>\n\n> Halo 是一款现代化的个人独立博客系统，给习惯写博客的同学多一个选择。\n\n<p align=\"center\">\n<a href=\"https://github.com/halo-dev/halo/releases\"><img alt=\"GitHub release\" src=\"https://img.shields.io/github/release/halo-dev/halo.svg?style=flat-square\"/></a>\n<a href=\"https://github.com/halo-dev/halo/releases\"><img alt=\"GitHub All Releases\" src=\"https://img.shields.io/github/downloads/halo-dev/halo/total.svg?style=flat-square\"></a>\n<a href=\"https://hub.docker.com/r/halohub/halo\"><img alt=\"Docker pulls\" src=\"https://img.shields.io/docker/pulls/halohub/halo?style=flat-square\"></a>\n<a href=\"https://github.com/halo-dev/halo/commits\"><img alt=\"GitHub last commit\" src=\"https://img.shields.io/github/last-commit/halo-dev/halo.svg?style=flat-square\"></a>\n<a href=\"https://github.com/halo-dev/halo/actions\"><img alt=\"GitHub Workflow Status\" src=\"https://img.shields.io/github/workflow/status/halo-dev/halo/Halo%20CI?style=flat-square\"/></a>\n</p>\n\n------------------------------\n\n## 简介\n\n**Halo** `[ˈheɪloʊ]`，一个优秀的开源博客发布应用，值得一试。\n\n[官网](https://halo.run) | [文档](https://docs.halo.run) | [社区](https://bbs.halo.run) | [Gitee](https://gitee.com/halo-dev) | [Telegram 频道](https://t.me/halo_dev)\n\n## 快速开始\n\n### Fat Jar\n\n下载最新的 Halo 运行包：\n\n```bash\ncurl -L https://github.com/halo-dev/halo/releases/download/v1.4.12/halo-1.4.12.jar --output halo.jar\n```\n\n其他地址：https://docs.halo.run/install/downloads\n\n```bash\njava -jar halo.jar\n```\n\n### Docker\n\n```bash\ndocker run -it -d --name halo -p 8090:8090 -v ~/.halo:/root/.halo --restart=always halohub/halo\n```\n\n详细部署文档请查阅：<https://docs.halo.run/install/index>\n\n## 生态\n\n| 项目                                                                         | 状态                                                                                                                                                              | 描述                                     |\n| ---------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------- |\n| [halo-admin](https://github.com/halo-dev/halo-admin)                         | <a href=\"https://www.npmjs.com/package/halo-admin\"><img alt=\"npm release\" src=\"https://img.shields.io/npm/v/halo-admin?style=flat-square\"/></a>                   | Web 管理端 UI，已内置在主应用            |\n| [halo-comment](https://github.com/halo-dev/halo-comment)                     | <a href=\"https://www.npmjs.com/package/halo-comment\"><img alt=\"npm release\" src=\"https://img.shields.io/npm/v/halo-comment?style=flat-square\"/></a>               | 独立评论组件，可以非常方便的集成到主题中 |\n| [halo-comment-normal](https://github.com/halo-dev/halo-comment-normal)       | <a href=\"https://www.npmjs.com/package/halo-comment-normal\"><img alt=\"npm release\" src=\"https://img.shields.io/npm/v/halo-comment-normal?style=flat-square\"/></a> | 另外一款评论组件                         |\n| [halo-mobile-app](https://github.com/halo-dev/halo-mobile-app)                             | 已停止维护                                                                                                                                                        | 移动端管理 APP                           |\n| [tencent-cloudbase-halo](https://github.com/halo-dev/tencent-cloudbase-halo) | 无                                                                                                                                                                | 腾讯云 CloudBase 一键部署配置            |\n| [halo-theme-*](https://github.com/topics/halo-theme)                         | 无                                                                                                                                                                | GitHub 上开源的 Halo 主题集合            | \n\n## 许可证\n\n[![license](https://img.shields.io/github/license/halo-dev/halo.svg?style=flat-square)](https://github.com/halo-dev/halo/blob/master/LICENSE)\n\nHalo 使用 GPL-v3.0 协议开源，请遵守开源协议。\n\n## 贡献\n\n参考 [CONTRIBUTING](./CONTRIBUTING.md)。\n\n<a href=\"https://github.com/halo-dev/halo/graphs/contributors\"><img alt=\"contributors_svg_width_890_button_false\" src=\"https://opencollective.com/halo/contributors.svg?width=890&button=false\" /></a>\n\n## 赞助我们\n\n> 如果 Halo 对您有帮助，不妨赞助我们\n\n<https://docs.halo.run/zh/contribution/sponsor>\n\n",
      "npm_url": "https://www.npmjs.com/package/halo",
      "npm_downloads": 275,
      "keywords": [
        "halo",
        "omniyin",
        "blogging",
        "omniyin halo",
        "halo halo",
        "halo modern"
      ],
      "category": "web-search"
    },
    "Ozamatash--deep-research-mcp": {
      "owner": "Ozamatash",
      "name": "deep-research-mcp",
      "url": "https://github.com/Ozamatash/deep-research-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/Ozamatash.webp",
      "description": "Performs deep research on various topics by integrating search engines, web scraping, and language models to create comprehensive reports and follow-up questions.",
      "stars": 275,
      "forks": 28,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-10-03T04:12:50Z",
      "readme_content": "# Open Deep Research MCP Server\n\nAn AI-powered research assistant that performs deep, iterative research on any topic. It combines search engines, web scraping, and AI to explore topics in depth and generate comprehensive reports. Available as a Model Context Protocol (MCP) tool or standalone CLI. Look at exampleout.md to see what a report might look like.\n\n## Quick Start\n\n1. Clone and install:\n```bash\ngit clone https://github.com/Ozamatash/deep-research\ncd deep-research\nnpm install\n```\n\n2. Set up environment in `.env.local`:\n```bash\n# Copy the example environment file\ncp .env.example .env.local\n```\n\n3. Build:\n```bash\n# Build the server\nnpm run build\n```\n\n4. Run the cli version:\n```bash\nnpm run start\n```\n5. Test MCP Server with Claude Desktop:  \nFollow the guide thats at the bottom of server quickstart to add the server to Claude Desktop:  \nhttps://modelcontextprotocol.io/quickstart/server\n\nFor remote servers: Streamable HTTP\n```bash\nnpm run start:http\n```\nServer runs on `http://localhost:3000/mcp` without session management.\n\n## Features\n\n- Performs deep, iterative research by generating targeted search queries\n- Controls research scope with depth (how deep) and breadth (how wide) parameters\n- Evaluates source reliability with detailed scoring (0-1) and reasoning\n- Prioritizes high-reliability sources (≥0.7) and verifies less reliable information\n- Generates follow-up questions to better understand research needs\n- Produces detailed markdown reports with findings, sources, and reliability assessments\n- Available as a Model Context Protocol (MCP) tool for AI agents\n- For now MCP version doesn't ask follow up questions\n- Natural-language source preferences (avoid listicles, forums, affiliate reviews, specific domains)\n\n### Model Selection (OpenAI, Anthropic, Google, xAI)\n\nPick a provider and model per run.\n\n- CLI: you will be prompted for provider and model. Example: `openai` + `gpt-5`.\n- MCP/HTTP: pass `model`, e.g. `openai:gpt-5`,\n\nSet the corresponding API key in `.env.local`:\n\n```\nOPENAI_API_KEY=...\nANTHROPIC_API_KEY=...\nGOOGLE_API_KEY=...\nXAI_API_KEY=...\n```\n\n## How It Works\n\n```mermaid\nflowchart TB\n    subgraph Input\n        Q[User Query]\n        B[Breadth Parameter]\n        D[Depth Parameter]\n        FQ[Feedback Questions]\n    end\n\n    subgraph Research[Deep Research]\n        direction TB\n        SQ[Generate SERP Queries]\n        SR[Search]\n        RE[Source Reliability Evaluation]\n        PR[Process Results]\n    end\n\n    subgraph Results[Research Output]\n        direction TB\n        L((Learnings with\n        Reliability Scores))\n        SM((Source Metadata))\n        ND((Next Directions:\n        Prior Goals,\n        New Questions))\n    end\n\n    %% Main Flow\n    Q & FQ --> CQ[Combined Query]\n    CQ & B & D --> SQ\n    SQ --> SR\n    SR --> RE\n    RE --> PR\n\n    %% Results Flow\n    PR --> L\n    PR --> SM\n    PR --> ND\n\n    %% Depth Decision and Recursion\n    L & ND --> DP{depth > 0?}\n    DP -->|Yes| SQ\n    \n    %% Final Output\n    DP -->|No| MR[Markdown Report]\n\n    %% Styling\n    classDef input fill:#7bed9f,stroke:#2ed573,color:black\n    classDef process fill:#70a1ff,stroke:#1e90ff,color:black\n    classDef output fill:#ff4757,stroke:#ff6b81,color:black\n    classDef results fill:#a8e6cf,stroke:#3b7a57,color:black,width:150px,height:150px\n\n    class Q,B,D,FQ input\n    class SQ,SR,RE,PR process\n    class MR output\n    class L,SM,ND results\n```\n## Advanced Setup\n\n### Using Local Firecrawl (Free Option)\n\nInstead of using the Firecrawl API, you can run a local instance. You can use the official repo or my fork which uses searXNG as the search backend to avoid using a searchapi key:\n\n1. Set up local Firecrawl:\n```bash\ngit clone https://github.com/Ozamatash/localfirecrawl\ncd localfirecrawl\n# Follow setup in localfirecrawl README\n```\n\n2. Update `.env.local`:\n```bash\nFIRECRAWL_BASE_URL=\"http://localhost:3002\"\n```\n\n### Optional: Observability\n\nAdd observability to track research flows, queries, and results using Langfuse:\n\n```bash\n# Add to .env.local\nLANGFUSE_PUBLIC_KEY=\"your_langfuse_public_key\"\nLANGFUSE_SECRET_KEY=\"your_langfuse_secret_key\"\n```\n\nThe app works normally without observability if no Langfuse keys are provided.\n\n## License\n\nMIT License\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "search",
        "research",
        "web",
        "search engines",
        "web search",
        "deep research"
      ],
      "category": "web-search"
    },
    "PV-Bhat--GemForge-MCP": {
      "owner": "PV-Bhat",
      "name": "GemForge-MCP",
      "url": "https://github.com/PV-Bhat/GemForge-MCP",
      "imageUrl": "/freedevtools/mcp/pfp/PV-Bhat.webp",
      "description": "Provides tools for interacting with Google's Gemini AI models, enabling intelligent model selection and advanced file handling. Facilitates AI tasks such as search, reasoning, code analysis, and file operations through a standardized MCP server interface.",
      "stars": 3,
      "forks": 3,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-08-09T19:47:04Z",
      "readme_content": "# GemForge (Gemini Tools)\n<img src=\"https://github.com/user-attachments/assets/8cee4293-b0e0-461f-a9d9-f750397aa2b5\" alt=\"GemForgeLogo\" width=\"100\" height=\"100\">\n\n<img src=\"https://glama.ai/mcp/servers/@PV-Bhat/GemForge-MCP/badge\" alt=\"Glama Badge\" width=\"210\" height=\"110\">\n\n## Overview\n\n[![Smithery Badge](https://smithery.ai/badge/@PV-Bhat/gemforge-gemini-tools-mcp)](https://smithery.ai/server/@PV-Bhat/gemforge-gemini-tools-mcp)\n[![MCP.so](https://img.shields.io/badge/MCP-Directory-blue)](https://mcp.so/server/gemforge-gemini-tools-mcp/PV-Bhat)\n<a href=\"https://glama.ai/mcp/servers/@PV-Bhat/GemForge-MCP\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@PV-Bhat/GemForge-MCP/badge\" alt=\"GemForge-Gemini-Tools-MCP MCP server\" />\n</a>\n\n\n**GemForge-Gemini-Tools-MCP**: Enterprise-grade Gemini integration for your favorite MCP agents. Supercharge Claude, Roo Code, and Windsurf with codebase analysis, live search, text/PDF/image processing, and more.\n\n## Quick Navigation\n\n- [Features](#why-gemforge)\n- [Quick Start](#quick-start)\n- [Configuration](#configuration)\n- [Tools](#key-tools)\n- [Heavy-Duty Reliability](#heavy-duty-reliability)\n- [Deployment](#deployment)\n- [Examples](#examples)\n- [Community](#community--support)\n- [Documentation](#documentation)\n\n## Why GemForge?\n\nGemForge is the essential bridge between Google's Gemini AI and the MCP ecosystem:\n![gemfog](https://github.com/user-attachments/assets/18cee069-d176-40c8-8ff9-3d643d918bc4)\n\n- **Real-Time Web Access**: Fetch breaking news, market trends, and current data with `gemini_search`\n- **Advanced Reasoning**: Process complex logic problems with step-by-step thinking via `gemini_reason`\n- **Code Mastery**: Analyze full repositories, generate solutions, and debug code with `gemini_code`\n- **Multi-File Processing**: Handle 60+ file formats including PDFs, images, and more with `gemini_fileops`\n\n- **Intelligent Model Selection**: Automatically routes to optimal Gemini model for each task\n  \n- **Enterprise-Ready**: Robust error handling, rate limit management, and API fallback mechanisms\n\n## Quick Start\n\n### One-Line Install\n\n```bash\nnpx @gemforge/mcp-server@latest init\n```\n\n### Manual Setup\n\n1. Create configuration file (`claude_desktop_config.json`):\n\n```json\n{\n  \"mcpServers\": {\n    \"GemForge\": {\n      \"command\": \"node\",\n      \"args\": [\"./dist/index.js\"],\n      \"env\": {\n        \"GEMINI_API_KEY\": \"your_api_key_here\"\n      }\n    }\n  }\n}\n```\n\n2. Install and run:\n\n```bash\nnpm install gemforge-mcp\nnpm start\n```\n\n[Watch 30-second setup demo →](https://www.youtube.com/your-demo-link)\n\n## Heavy-Duty Reliability\n\nGemForge is built for production environments:\n\n- **Support for 60+ File Types**: Process everything from code to documents to images\n- **Automatic Model Fallbacks**: Continues functioning even during rate limits or service disruptions\n- **Enterprise-Grade Error Logging**: Detailed diagnostics for troubleshooting\n- **API Resilience**: Exponential backoff, retry logic, and seamless model switching\n- **Full Repository Support**: Analyze entire codebases with configurable inclusion/exclusion patterns\n- **XML Content Processing**: Specialized handling for structured data\n\n## Key Tools\n\n| Tool | Description | Key Capability |\n|------|-------------|----------------|\n| `gemini_search` | Web-connected information retrieval | Real-time data access |\n| `gemini_reason` | Complex problem solving with step-by-step logic | Transparent reasoning process |\n| `gemini_code` | Deep code understanding and generation | Full repository analysis |\n| `gemini_fileops` | Multi-file processing across 60+ formats | Document comparison and transformation |\n\n<details>\n<summary><strong>Example: Real-Time Search</strong></summary>\n\n```json\n{\n  \"toolName\": \"gemini_search\",\n  \"toolParams\": {\n    \"query\": \"Latest advancements in quantum computing\",\n    \"enable_thinking\": true\n  }\n}\n```\n</details>\n\n<details>\n<summary><strong>Example: Code Analysis</strong></summary>\n\n```json\n{\n  \"toolName\": \"gemini_code\",\n  \"toolParams\": {\n    \"question\": \"Identify improvements and new features\",\n    \"directory_path\": \"path/to/project\",\n    \"repomix_options\": \"--include \\\"**/*.js\\\" --no-gitignore\"\n  }\n}\n```\n</details>\n\n<details>\n<summary><strong>Example: Multi-File Comparison</strong></summary>\n\n```json\n{\n  \"toolName\": \"gemini_fileops\",\n  \"toolParams\": {\n    \"file_path\": [\"contract_v1.pdf\", \"contract_v2.pdf\"],\n    \"operation\": \"analyze\",\n    \"instruction\": \"Compare these contract versions and extract all significant changes.\"\n  }\n}\n```\n</details>\n\n## Configuration\n\nGemForge offers flexible configuration options:\n\n<details>\n<summary><strong>Environment Variables</strong></summary>\n\n```\nGEMINI_API_KEY=your_api_key_here       # Required: Gemini API key\nGEMINI_PAID_TIER=true                  # Optional: Set to true if using paid tier (better rate limits)\nDEFAULT_MODEL_ID=gemini-2.5-pro        # Optional: Override default model selection\nLOG_LEVEL=info                         # Optional: Set logging verbosity (debug, info, warn, error)\n```\n</details>\n\n<details>\n<summary><strong>Claude Desktop Integration</strong></summary>\n\n```json\n{\n  \"mcpServers\": {\n    \"GemForge\": {\n      \"command\": \"node\",\n      \"args\": [\"./dist/index.js\"],\n      \"env\": {\n        \"GEMINI_API_KEY\": \"your_api_key_here\"\n      }\n    }\n  }\n}\n```\n</details>\n\n<details>\n<summary><strong>Advanced Model Selection</strong></summary>\n\nGemForge intelligently selects the best model for each task:\n- `gemini_search`: Uses `gemini-2.5-flash` for speed and search integration\n- `gemini_reason`: Uses `gemini-2.5-pro` for deep reasoning capabilities\n- `gemini_code`: Uses `gemini-2.5-pro` for complex code understanding\n- `gemini_fileops`: Selects between `gemini-2.0-flash-lite` or `gemini-1.5-pro` based on file size\n\nOverride with `model_id` parameter in any tool call or set `DEFAULT_MODEL_ID` environment variable.\n</details>\n\n## Deployment\n\n### Smithery.ai\nOne-click deployment via [Smithery.ai](https://smithery.ai/server/@PV-Bhat/gemforge-gemini-tools-mcp)\n\n### Docker\n```bash\ndocker run -e GEMINI_API_KEY=your_api_key ghcr.io/pv-bhat/gemforge:latest\n```\n\n### Self-Hosted\nUse our [MCP.so Directory listing](https://mcp.so/server/gemforge-gemini-tools-mcp/PV-Bhat) for integration instructions.\n\n## What Sets GemForge Apart?\n\n- **Cross-Ecosystem Power**: Bridge Google's AI with Claude and other MCP agents\n- **Multi-File Analysis**: Compare documents, images, or code versions\n- **Smart Routing**: Automatic model selection based on task requirements\n- **Production-Ready**: Built for enterprise environments\n\n\n\n## Community & Support\n\n- **Join Us**: [MCP Discord](https://discord.me/mcp) | [GemForge Discord](https://discord.gg/your-invite-link)\n- **Contribute**: [GitHub Discussions](https://github.com/your-username/GemForge/discussions)\n- **Feedback**: Open an issue or share thoughts on Discord\n\n## Documentation\n\nVisit our [Documentation Site](https://your-username.github.io/GemForge) for:\n- Advanced usage tutorials\n- API reference\n- Troubleshooting tips\n\n## License\n\nLicensed under the MIT License. See [LICENSE](LICENSE) for details.\n\n## Acknowledgments\n- Google Gemini API for providing the underlying AI capabilities\n- Model Context Protocol (MCP) for standardizing AI tool interfaces",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "mcp",
        "search",
        "gemforge",
        "gemforge mcp",
        "mcp provides",
        "gemini ai"
      ],
      "category": "web-search"
    },
    "PhamQuangVinh22022648--brave-search": {
      "owner": "PhamQuangVinh22022648",
      "name": "brave-search",
      "url": "https://github.com/PhamQuangVinh22022648/brave-search",
      "imageUrl": "/freedevtools/mcp/pfp/PhamQuangVinh22022648.webp",
      "description": "Integrates the Brave Search API to provide web and local search capabilities, allowing general web queries, news and article searches, as well as searches for local businesses and services. Supports pagination, filtering, and smart fallbacks to web results when local queries yield no results.",
      "stars": 0,
      "forks": 1,
      "license": "No License",
      "language": "",
      "updated_at": "2025-05-12T05:33:10Z",
      "readme_content": "# Brave Search MCP Server\n\nAn MCP server implementation that integrates the Brave Search API, providing both web and local search capabilities.\n\n## Features\n\n- **Web Search**: General queries, news, articles, with pagination and freshness controls\n- **Local Search**: Find businesses, restaurants, and services with detailed information\n- **Flexible Filtering**: Control result types, safety levels, and content freshness\n- **Smart Fallbacks**: Local search automatically falls back to web when no results are found\n\n## Tools\n\n- **brave_web_search**\n\n  - Execute web searches with pagination and filtering\n  - Inputs:\n    - `query` (string): Search terms\n    - `count` (number, optional): Results per page (max 20)\n    - `offset` (number, optional): Pagination offset (max 9)\n\n- **brave_local_search**\n  - Search for local businesses and services\n  - Inputs:\n    - `query` (string): Local search terms\n    - `count` (number, optional): Number of results (max 20)\n  - Automatically falls back to web search if no local results found\n\n## Configuration\n\n### Getting an API Key\n\n1. Sign up for a [Brave Search API account](https://brave.com/search/api/)\n2. Choose a plan (Free tier available with 2,000 queries/month)\n3. Generate your API key [from the developer dashboard](https://api-dashboard.search.brave.com/app/keys)\n\n### Usage with Claude Desktop\n\nAdd this to your `claude_desktop_config.json`:\n\n### Docker\n\n```json\n{\n  \"mcpServers\": {\n    \"brave-search\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"-i\",\n        \"--rm\",\n        \"-e\",\n        \"BRAVE_API_KEY\",\n        \"mcp/brave-search\"\n      ],\n      \"env\": {\n        \"BRAVE_API_KEY\": \"YOUR_API_KEY_HERE\"\n      }\n    }\n  }\n}\n```\n\n### NPX\n\n```json\n{\n  \"mcpServers\": {\n    \"brave-search\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"@modelcontextprotocol/server-brave-search\"\n      ],\n      \"env\": {\n        \"BRAVE_API_KEY\": \"YOUR_API_KEY_HERE\"\n      }\n    }\n  }\n}\n```\n\n### Usage with VS Code\n\nFor quick installation, use the one-click installation buttons below...\n\n[![Install with NPX in VS Code](https://img.shields.io/badge/VS_Code-NPM-0098FF?style=flat-square&logo=visualstudiocode&logoColor=white)](https://insiders.vscode.dev/redirect/mcp/install?name=brave&inputs=%5B%7B%22type%22%3A%22promptString%22%2C%22id%22%3A%22apiKey%22%7D%5D&config=%7B%22command%22%3A%22npx%22%2C%22args%22%3A%5B%22-y%22%2C%22%40modelcontextprotocol%2Fserver-brave-search%22%5D%2C%22env%22%3A%7B%22BRAVE_API_KEY%22%3A%22%24%7Binput%3Abrave_api_key%7D%22%7D%7D) [![Install with NPX in VS Code Insiders](https://img.shields.io/badge/VS_Code_Insiders-NPM-24bfa5?style=flat-square&logo=visualstudiocode&logoColor=white)](https://insiders.vscode.dev/redirect/mcp/install?name=brave&inputs=%5B%7B%22type%22%3A%22promptString%22%2C%22id%22%3A%22apiKey%22%7D%5D&config=%7B%22command%22%3A%22npx%22%2C%22args%22%3A%5B%22-y%22%2C%22%40modelcontextprotocol%2Fserver-brave-search%22%5D%2C%22env%22%3A%7B%22BRAVE_API_KEY%22%3A%22%24%7Binput%3Abrave_api_key%7D%22%7D%7D&quality=insiders)\n\n[![Install with Docker in VS Code](https://img.shields.io/badge/VS_Code-Docker-0098FF?style=flat-square&logo=visualstudiocode&logoColor=white)](https://insiders.vscode.dev/redirect/mcp/install?name=brave&inputs=%5B%7B%22type%22%3A%22promptString%22%2C%22id%22%3A%22apiKey%22%7D%5D&config=%7B%22command%22%3A%22docker%22%2C%22args%22%3A%5B%22run%22%2C%22-i%22%2C%22--rm%22%2C%22-e%22%2C%22BRAVE_API_KEY%22%2C%22mcp%2Fbrave-search%22%5D%2C%22env%22%3A%7B%22BRAVE_API_KEY%22%3A%22%24%7Binput%3Abrave_api_key%7D%22%7D%7D) [![Install with Docker in VS Code Insiders](https://img.shields.io/badge/VS_Code_Insiders-Docker-24bfa5?style=flat-square&logo=visualstudiocode&logoColor=white)](https://insiders.vscode.dev/redirect/mcp/install?name=brave&inputs=%5B%7B%22type%22%3A%22promptString%22%2C%22id%22%3A%22apiKey%22%7D%5D&config=%7B%22command%22%3A%22docker%22%2C%22args%22%3A%5B%22run%22%2C%22-i%22%2C%22--rm%22%2C%22-e%22%2C%22BRAVE_API_KEY%22%2C%22mcp%2Fbrave-search%22%5D%2C%22env%22%3A%7B%22BRAVE_API_KEY%22%3A%22%24%7Binput%3Abrave_api_key%7D%22%7D%7D&quality=insiders)\n\nFor manual installation, add the following JSON block to your User Settings (JSON) file in VS Code. You can do this by pressing `Ctrl + Shift + P` and typing `Preferences: Open User Settings (JSON)`.\n\nOptionally, you can add it to a file called `.vscode/mcp.json` in your workspace. This will allow you to share the configuration with others.\n\n> Note that the `mcp` key is not needed in the `.vscode/mcp.json` file.\n\n#### Docker\n\n```json\n{\n  \"mcp\": {\n    \"inputs\": [\n      {\n        \"type\": \"promptString\",\n        \"id\": \"brave_api_key\",\n        \"description\": \"Brave Search API Key\",\n        \"password\": true\n      }\n    ],\n    \"servers\": {\n      \"brave-search\": {\n        \"command\": \"docker\",\n        \"args\": [\n          \"run\",\n          \"-i\",\n          \"--rm\",\n          \"-e\",\n          \"BRAVE_API_KEY\",\n          \"mcp/brave-search\"\n        ],\n        \"env\": {\n          \"BRAVE_API_KEY\": \"${input:brave_api_key}\"\n        }\n      }\n    }\n  }\n}\n```\n\n#### NPX\n\n```json\n{\n  \"mcp\": {\n    \"inputs\": [\n      {\n        \"type\": \"promptString\",\n        \"id\": \"brave_api_key\",\n        \"description\": \"Brave Search API Key\",\n        \"password\": true\n      }\n    ],\n    \"servers\": {\n      \"brave-search\": {\n        \"command\": \"npx\",\n        \"args\": [\"-y\", \"@modelcontextprotocol/server-brave-search\"],\n        \"env\": {\n          \"BRAVE_API_KEY\": \"${input:brave_api_key}\"\n        }\n      }\n    }\n  }\n}\n```\n\n## Build\n\nDocker build:\n\n```bash\ndocker build -t mcp/brave-search:latest -f src/brave-search/Dockerfile .\n```\n\n## License\n\nThis MCP server is licensed under the MIT License. This means you are free to use, modify, and distribute the software, subject to the terms and conditions of the MIT License. For more details, please see the LICENSE file in the project repository.",
      "npm_url": "https://www.npmjs.com/package/brave-search",
      "npm_downloads": 50744,
      "keywords": [
        "searches",
        "search",
        "brave",
        "brave search",
        "search api",
        "web search"
      ],
      "category": "web-search"
    },
    "PhamQuangVinh22022648--playwright-mcp": {
      "owner": "PhamQuangVinh22022648",
      "name": "playwright-mcp",
      "url": "https://github.com/PhamQuangVinh22022648/playwright-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/PhamQuangVinh22022648.webp",
      "description": "Enables interaction with web pages through structured accessibility snapshots for reliable browser automation without the need for vision models. Supports tasks such as web navigation, form-filling, and data extraction using deterministic methods.",
      "stars": 0,
      "forks": 0,
      "license": "Apache License 2.0",
      "language": "",
      "updated_at": "2025-05-06T21:45:34Z",
      "readme_content": "## Playwright MCP\n\nA Model Context Protocol (MCP) server that provides browser automation capabilities using [Playwright](https://playwright.dev). This server enables LLMs to interact with web pages through structured accessibility snapshots, bypassing the need for screenshots or visually-tuned models.\n\n### Key Features\n\n- **Fast and lightweight**: Uses Playwright's accessibility tree, not pixel-based input.\n- **LLM-friendly**: No vision models needed, operates purely on structured data.\n- **Deterministic tool application**: Avoids ambiguity common with screenshot-based approaches.\n\n### Use Cases\n\n- Web navigation and form-filling\n- Data extraction from structured content\n- Automated testing driven by LLMs\n- General-purpose browser interaction for agents\n\n<!--\n// Generate using:\nnode utils/generate-links.js\n-->\n\n[<img src=\"https://img.shields.io/badge/VS_Code-VS_Code?style=flat-square&label=Install%20Server&color=0098FF\" alt=\"Install in VS Code\">](https://insiders.vscode.dev/redirect?url=vscode%3Amcp%2Finstall%3F%257B%2522name%2522%253A%2522playwright%2522%252C%2522command%2522%253A%2522npx%2522%252C%2522args%2522%253A%255B%2522%2540playwright%252Fmcp%2540latest%2522%255D%257D) [<img alt=\"Install in VS Code Insiders\" src=\"https://img.shields.io/badge/VS_Code_Insiders-VS_Code_Insiders?style=flat-square&label=Install%20Server&color=24bfa5\">](https://insiders.vscode.dev/redirect?url=vscode-insiders%3Amcp%2Finstall%3F%257B%2522name%2522%253A%2522playwright%2522%252C%2522command%2522%253A%2522npx%2522%252C%2522args%2522%253A%255B%2522%2540playwright%252Fmcp%2540latest%2522%255D%257D)\n\n### Example config\n\n```js\n{\n  \"mcpServers\": {\n    \"playwright\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"@playwright/mcp@latest\"\n      ]\n    }\n  }\n}\n```\n\n### Table of Contents\n\n- [Installation in VS Code](#installation-in-vs-code)\n- [Command line](#command-line)\n- [User profile](#user-profile)\n- [Configuration file](#configuration-file)\n- [Running on Linux](#running-on-linux)\n- [Docker](#docker)\n- [Programmatic usage](#programmatic-usage)\n- [Tool modes](#tool-modes)\n\n### Installation in VS Code\n\nYou can install the Playwright MCP server using the VS Code CLI:\n\n```bash\n# For VS Code\ncode --add-mcp '{\"name\":\"playwright\",\"command\":\"npx\",\"args\":[\"@playwright/mcp@latest\"]}'\n```\n\nAfter installation, the Playwright MCP server will be available for use with your GitHub Copilot agent in VS Code.\n\n### Command line\n\nThe Playwright MCP server supports the following command-line options:\n\n- `--browser <browser>`: Browser or chrome channel to use. Possible values:\n  - `chrome`, `firefox`, `webkit`, `msedge`\n  - Chrome channels: `chrome-beta`, `chrome-canary`, `chrome-dev`\n  - Edge channels: `msedge-beta`, `msedge-canary`, `msedge-dev`\n  - Default: `chrome`\n- `--caps <caps>`: Comma-separated list of capabilities to enable, possible values: tabs, pdf, history, wait, files, install. Default is all.\n- `--cdp-endpoint <endpoint>`: CDP endpoint to connect to\n- `--executable-path <path>`: Path to the browser executable\n- `--headless`: Run browser in headless mode (headed by default)\n- `--device`: Emulate mobile device\n- `--user-data-dir <path>`: Path to the user data directory\n- `--port <port>`: Port to listen on for SSE transport\n- `--host <host>`: Host to bind server to. Default is localhost. Use 0.0.0.0 to bind to all interfaces.\n- `--allowed-origins <origins>`: Semicolon-separated list of origins to allow the browser to request. Default is to allow all. Origins matching both `--allowed-origins` and `--blocked-origins` will be blocked.\n- `--blocked-origins <origins>`: Semicolon-separated list of origins to block the browser to request. Origins matching both `--allowed-origins` and `--blocked-origins` will be blocked.\n- `--vision`: Run server that uses screenshots (Aria snapshots are used by default)\n- `--output-dir`: Directory for output files\n- `--config <path>`: Path to the configuration file\n\n### User profile\n\nPlaywright MCP will launch the browser with the new profile, located at\n\n```\n- `%USERPROFILE%\\AppData\\Local\\ms-playwright\\mcp-{channel}-profile` on Windows\n- `~/Library/Caches/ms-playwright/mcp-{channel}-profile` on macOS\n- `~/.cache/ms-playwright/mcp-{channel}-profile` on Linux\n```\n\nAll the logged in information will be stored in that profile, you can delete it between sessions if you'd like to clear the offline state.\n\n### Configuration file\n\nThe Playwright MCP server can be configured using a JSON configuration file. Here's the complete configuration format:\n\n```typescript\n{\n  // Browser configuration\n  browser?: {\n    // Browser type to use (chromium, firefox, or webkit)\n    browserName?: 'chromium' | 'firefox' | 'webkit';\n\n    // Path to user data directory for browser profile persistence\n    userDataDir?: string;\n\n    // Browser launch options (see Playwright docs)\n    // @see https://playwright.dev/docs/api/class-browsertype#browser-type-launch\n    launchOptions?: {\n      channel?: string;        // Browser channel (e.g. 'chrome')\n      headless?: boolean;      // Run in headless mode\n      executablePath?: string; // Path to browser executable\n      // ... other Playwright launch options\n    };\n\n    // Browser context options\n    // @see https://playwright.dev/docs/api/class-browser#browser-new-context\n    contextOptions?: {\n      viewport?: { width: number, height: number };\n      // ... other Playwright context options\n    };\n\n    // CDP endpoint for connecting to existing browser\n    cdpEndpoint?: string;\n\n    // Remote Playwright server endpoint\n    remoteEndpoint?: string;\n  },\n\n  // Server configuration\n  server?: {\n    port?: number;  // Port to listen on\n    host?: string;  // Host to bind to (default: localhost)\n  },\n\n  // List of enabled capabilities\n  capabilities?: Array<\n    'core' |    // Core browser automation\n    'tabs' |    // Tab management\n    'pdf' |     // PDF generation\n    'history' | // Browser history\n    'wait' |    // Wait utilities\n    'files' |   // File handling\n    'install' | // Browser installation\n    'testing'   // Testing\n  >;\n\n  // Enable vision mode (screenshots instead of accessibility snapshots)\n  vision?: boolean;\n\n  // Directory for output files\n  outputDir?: string;\n\n  // Network configuration\n  network?: {\n    // List of origins to allow the browser to request. Default is to allow all. Origins matching both `allowedOrigins` and `blockedOrigins` will be blocked.\n    allowedOrigins?: string[];\n\n    // List of origins to block the browser to request. Origins matching both `allowedOrigins` and `blockedOrigins` will be blocked.\n    blockedOrigins?: string[];\n  };\n \n  /**\n   * Do not send image responses to the client.\n   */\n  noImageResponses?: boolean;\n}\n```\n\nYou can specify the configuration file using the `--config` command line option:\n\n```bash\nnpx @playwright/mcp@latest --config path/to/config.json\n```\n\n### Running on Linux\n\nWhen running headed browser on system w/o display or from worker processes of the IDEs,\nrun the MCP server from environment with the DISPLAY and pass the `--port` flag to enable SSE transport.\n\n```bash\nnpx @playwright/mcp@latest --port 8931\n```\n\nAnd then in MCP client config, set the `url` to the SSE endpoint:\n\n```js\n{\n  \"mcpServers\": {\n    \"playwright\": {\n      \"url\": \"http://localhost:8931/sse\"\n    }\n  }\n}\n```\n\n### Docker\n\n**NOTE:** The Docker implementation only supports headless chromium at the moment.\n\n```js\n{\n  \"mcpServers\": {\n    \"playwright\": {\n      \"command\": \"docker\",\n      \"args\": [\"run\", \"-i\", \"--rm\", \"--init\", \"mcp/playwright\"]\n    }\n  }\n}\n```\n\nYou can build the Docker image yourself.\n\n```\ndocker build -t mcp/playwright .\n```\n\n### Programmatic usage\n\n```js\nimport http from 'http';\n\nimport { createServer } from '@playwright/mcp';\nimport { SSEServerTransport } from '@modelcontextprotocol/sdk/server/sse.js';\n\nhttp.createServer(async (req, res) => {\n  // ...\n\n  // Creates a headless Playwright MCP server with SSE transport\n  const connection = await createConnection({ headless: true });\n  const transport = new SSEServerTransport('/messages', res);\n  await connection.connect(transport);\n\n  // ...\n});\n```\n\n### Tool modes\n\nThe tools are available in two modes:\n\n1. **Snapshot Mode** (default): Uses accessibility snapshots for better performance and reliability\n2. **Vision Mode**: Uses screenshots for visual-based interactions\n\nTo use Vision Mode, add the `--vision` flag when starting the server:\n\n```js\n{\n  \"mcpServers\": {\n    \"playwright\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"@playwright/mcp@latest\",\n        \"--vision\"\n      ]\n    }\n  }\n}\n```\n\nVision Mode works best with the computer use models that are able to interact with elements using\nX Y coordinate space, based on the provided screenshot.\n\n\n<!--- Generated by update-readme.js -->\n\n### Snapshot-based Interactions\n\n<!-- NOTE: This has been generated via update-readme.js -->\n\n- **browser_snapshot**\n  - Title: Page snapshot\n  - Description: Capture accessibility snapshot of the current page, this is better than screenshot\n  - Parameters: None\n  - Read-only: **true**\n\n<!-- NOTE: This has been generated via update-readme.js -->\n\n- **browser_click**\n  - Title: Click\n  - Description: Perform click on a web page\n  - Parameters:\n    - `element` (string): Human-readable element description used to obtain permission to interact with the element\n    - `ref` (string): Exact target element reference from the page snapshot\n  - Read-only: **false**\n\n<!-- NOTE: This has been generated via update-readme.js -->\n\n- **browser_drag**\n  - Title: Drag mouse\n  - Description: Perform drag and drop between two elements\n  - Parameters:\n    - `startElement` (string): Human-readable source element description used to obtain the permission to interact with the element\n    - `startRef` (string): Exact source element reference from the page snapshot\n    - `endElement` (string): Human-readable target element description used to obtain the permission to interact with the element\n    - `endRef` (string): Exact target element reference from the page snapshot\n  - Read-only: **false**\n\n<!-- NOTE: This has been generated via update-readme.js -->\n\n- **browser_hover**\n  - Title: Hover mouse\n  - Description: Hover over element on page\n  - Parameters:\n    - `element` (string): Human-readable element description used to obtain permission to interact with the element\n    - `ref` (string): Exact target element reference from the page snapshot\n  - Read-only: **true**\n\n<!-- NOTE: This has been generated via update-readme.js -->\n\n- **browser_type**\n  - Title: Type text\n  - Description: Type text into editable element\n  - Parameters:\n    - `element` (string): Human-readable element description used to obtain permission to interact with the element\n    - `ref` (string): Exact target element reference from the page snapshot\n    - `text` (string): Text to type into the element\n    - `submit` (boolean, optional): Whether to submit entered text (press Enter after)\n    - `slowly` (boolean, optional): Whether to type one character at a time. Useful for triggering key handlers in the page. By default entire text is filled in at once.\n  - Read-only: **false**\n\n<!-- NOTE: This has been generated via update-readme.js -->\n\n- **browser_select_option**\n  - Title: Select option\n  - Description: Select an option in a dropdown\n  - Parameters:\n    - `element` (string): Human-readable element description used to obtain permission to interact with the element\n    - `ref` (string): Exact target element reference from the page snapshot\n    - `values` (array): Array of values to select in the dropdown. This can be a single value or multiple values.\n  - Read-only: **false**\n\n<!-- NOTE: This has been generated via update-readme.js -->\n\n- **browser_take_screenshot**\n  - Title: Take a screenshot\n  - Description: Take a screenshot of the current page. You can't perform actions based on the screenshot, use browser_snapshot for actions.\n  - Parameters:\n    - `raw` (boolean, optional): Whether to return without compression (in PNG format). Default is false, which returns a JPEG image.\n    - `element` (string, optional): Human-readable element description used to obtain permission to screenshot the element. If not provided, the screenshot will be taken of viewport. If element is provided, ref must be provided too.\n    - `ref` (string, optional): Exact target element reference from the page snapshot. If not provided, the screenshot will be taken of viewport. If ref is provided, element must be provided too.\n  - Read-only: **true**\n\n### Vision-based Interactions\n\n<!-- NOTE: This has been generated via update-readme.js -->\n\n- **browser_screen_capture**\n  - Title: Take a screenshot\n  - Description: Take a screenshot of the current page\n  - Parameters: None\n  - Read-only: **true**\n\n<!-- NOTE: This has been generated via update-readme.js -->\n\n- **browser_screen_move_mouse**\n  - Title: Move mouse\n  - Description: Move mouse to a given position\n  - Parameters:\n    - `element` (string): Human-readable element description used to obtain permission to interact with the element\n    - `x` (number): X coordinate\n    - `y` (number): Y coordinate\n  - Read-only: **true**\n\n<!-- NOTE: This has been generated via update-readme.js -->\n\n- **browser_screen_click**\n  - Title: Click\n  - Description: Click left mouse button\n  - Parameters:\n    - `element` (string): Human-readable element description used to obtain permission to interact with the element\n    - `x` (number): X coordinate\n    - `y` (number): Y coordinate\n  - Read-only: **false**\n\n<!-- NOTE: This has been generated via update-readme.js -->\n\n- **browser_screen_drag**\n  - Title: Drag mouse\n  - Description: Drag left mouse button\n  - Parameters:\n    - `element` (string): Human-readable element description used to obtain permission to interact with the element\n    - `startX` (number): Start X coordinate\n    - `startY` (number): Start Y coordinate\n    - `endX` (number): End X coordinate\n    - `endY` (number): End Y coordinate\n  - Read-only: **false**\n\n<!-- NOTE: This has been generated via update-readme.js -->\n\n- **browser_screen_type**\n  - Title: Type text\n  - Description: Type text\n  - Parameters:\n    - `text` (string): Text to type into the element\n    - `submit` (boolean, optional): Whether to submit entered text (press Enter after)\n  - Read-only: **false**\n\n### Tab Management\n\n<!-- NOTE: This has been generated via update-readme.js -->\n\n- **browser_tab_list**\n  - Title: List tabs\n  - Description: List browser tabs\n  - Parameters: None\n  - Read-only: **true**\n\n<!-- NOTE: This has been generated via update-readme.js -->\n\n- **browser_tab_new**\n  - Title: Open a new tab\n  - Description: Open a new tab\n  - Parameters:\n    - `url` (string, optional): The URL to navigate to in the new tab. If not provided, the new tab will be blank.\n  - Read-only: **true**\n\n<!-- NOTE: This has been generated via update-readme.js -->\n\n- **browser_tab_select**\n  - Title: Select a tab\n  - Description: Select a tab by index\n  - Parameters:\n    - `index` (number): The index of the tab to select\n  - Read-only: **true**\n\n<!-- NOTE: This has been generated via update-readme.js -->\n\n- **browser_tab_close**\n  - Title: Close a tab\n  - Description: Close a tab\n  - Parameters:\n    - `index` (number, optional): The index of the tab to close. Closes current tab if not provided.\n  - Read-only: **false**\n\n### Navigation\n\n<!-- NOTE: This has been generated via update-readme.js -->\n\n- **browser_navigate**\n  - Title: Navigate to a URL\n  - Description: Navigate to a URL\n  - Parameters:\n    - `url` (string): The URL to navigate to\n  - Read-only: **false**\n\n<!-- NOTE: This has been generated via update-readme.js -->\n\n- **browser_navigate_back**\n  - Title: Go back\n  - Description: Go back to the previous page\n  - Parameters: None\n  - Read-only: **true**\n\n<!-- NOTE: This has been generated via update-readme.js -->\n\n- **browser_navigate_forward**\n  - Title: Go forward\n  - Description: Go forward to the next page\n  - Parameters: None\n  - Read-only: **true**\n\n### Keyboard\n\n<!-- NOTE: This has been generated via update-readme.js -->\n\n- **browser_press_key**\n  - Title: Press a key\n  - Description: Press a key on the keyboard\n  - Parameters:\n    - `key` (string): Name of the key to press or a character to generate, such as `ArrowLeft` or `a`\n  - Read-only: **false**\n\n### Console\n\n<!-- NOTE: This has been generated via update-readme.js -->\n\n- **browser_console_messages**\n  - Title: Get console messages\n  - Description: Returns all console messages\n  - Parameters: None\n  - Read-only: **true**\n\n### Files and Media\n\n<!-- NOTE: This has been generated via update-readme.js -->\n\n- **browser_file_upload**\n  - Title: Upload files\n  - Description: Upload one or multiple files\n  - Parameters:\n    - `paths` (array): The absolute paths to the files to upload. Can be a single file or multiple files.\n  - Read-only: **false**\n\n<!-- NOTE: This has been generated via update-readme.js -->\n\n- **browser_pdf_save**\n  - Title: Save as PDF\n  - Description: Save page as PDF\n  - Parameters: None\n  - Read-only: **true**\n\n### Utilities\n\n<!-- NOTE: This has been generated via update-readme.js -->\n\n- **browser_close**\n  - Title: Close browser\n  - Description: Close the page\n  - Parameters: None\n  - Read-only: **true**\n\n<!-- NOTE: This has been generated via update-readme.js -->\n\n- **browser_wait**\n  - Title: Wait\n  - Description: Wait for a specified time in seconds\n  - Parameters:\n    - `time` (number): The time to wait in seconds\n  - Read-only: **true**\n\n<!-- NOTE: This has been generated via update-readme.js -->\n\n- **browser_resize**\n  - Title: Resize browser window\n  - Description: Resize the browser window\n  - Parameters:\n    - `width` (number): Width of the browser window\n    - `height` (number): Height of the browser window\n  - Read-only: **true**\n\n<!-- NOTE: This has been generated via update-readme.js -->\n\n- **browser_install**\n  - Title: Install the browser specified in the config\n  - Description: Install the browser specified in the config. Call this if you get an error about the browser not being installed.\n  - Parameters: None\n  - Read-only: **false**\n\n<!-- NOTE: This has been generated via update-readme.js -->\n\n- **browser_handle_dialog**\n  - Title: Handle a dialog\n  - Description: Handle a dialog\n  - Parameters:\n    - `accept` (boolean): Whether to accept the dialog.\n    - `promptText` (string, optional): The text of the prompt in case of a prompt dialog.\n  - Read-only: **false**\n\n<!-- NOTE: This has been generated via update-readme.js -->\n\n- **browser_network_requests**\n  - Title: List network requests\n  - Description: Returns all network requests since loading the page\n  - Parameters: None\n  - Read-only: **true**\n\n### Testing\n\n<!-- NOTE: This has been generated via update-readme.js -->\n\n- **browser_generate_playwright_test**\n  - Title: Generate a Playwright test\n  - Description: Generate a Playwright test for given scenario\n  - Parameters:\n    - `name` (string): The name of the test\n    - `description` (string): The description of the test\n    - `steps` (array): The steps of the test\n  - Read-only: **true**\n\n<!--- End of generated section -->\n",
      "npm_url": "https://www.npmjs.com/package/playwright-mcp",
      "npm_downloads": 88803,
      "keywords": [
        "accessibility",
        "playwright",
        "browser",
        "playwright mcp",
        "web navigation",
        "browser automation"
      ],
      "category": "web-search"
    },
    "PhialsBasement--Pagespeed-MCP-Server": {
      "owner": "PhialsBasement",
      "name": "Pagespeed-MCP-Server",
      "url": "https://github.com/PhialsBasement/Pagespeed-MCP-Server",
      "imageUrl": "/freedevtools/mcp/pfp/PhialsBasement.webp",
      "description": "Connects AI models to Google's PageSpeed Insights API for detailed website performance analysis and metrics interpretation.",
      "stars": 7,
      "forks": 8,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-09-13T23:16:43Z",
      "readme_content": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/phialsbasement-pagespeed-mcp-server-badge.png)](https://mseep.ai/app/phialsbasement-pagespeed-mcp-server)\n\n# PageSpeed MCP Server\n[![smithery badge](https://smithery.ai/badge/mcp-pagespeed-server)](https://smithery.ai/server/mcp-pagespeed-server)\n\nA Model Context Protocol (MCP) server that extends AI assistant capabilities with PageSpeed Insights functionality. This server acts as a bridge between AI models and Google's PageSpeed Insights API, enabling detailed performance analysis of websites.\n\n## Overview\n\nThe PageSpeed MCP server is designed to enhance AI assistants' capabilities by allowing them to perform comprehensive web performance analysis. When integrated, AI models can request and interpret detailed performance metrics, Core Web Vitals, and other critical web performance data for any given URL.\n\n## Installation\n\n### Installing via Smithery\n\nTo install PageSpeed Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/mcp-pagespeed-server):\n\n```bash\nnpx -y @smithery/cli install mcp-pagespeed-server --client claude\n```\n\n### Manual Installation\n```bash\nnpm install pagespeed-mcp-server\n```\n\n## Configuration\n\nAdd the PageSpeed MCP to your AI assistant's(claude in this case) configuration file:\n\n```json\n{\n    \"pagespeed\": {\n        \"command\": \"node\",\n        \"args\": [\"path/to/mcp-pagespeed-server/dist/index.js\"],\n        \"env\": {\n           \"GOOGLE_API_KEY\": \"<YOUR-API-KEY>\"\n        }\n    }\n}\n```\n\n## Detailed Capabilities\n\n### Performance Metrics Analysis\n- First Contentful Paint (FCP)\n- Largest Contentful Paint (LCP)\n- Time to Interactive (TTI)\n- Total Blocking Time (TBT)\n- Cumulative Layout Shift (CLS)\n- Speed Index\n- Time to First Byte (TTFB)\n\n### Best Practices Assessment\n- HTTPS usage\n- JavaScript error monitoring\n- Browser console warnings\n- Deprecated API usage\n- Image aspect ratio analysis\n- Link security checks\n\n### SEO Analysis\n- Meta description validation\n- Robots.txt validation\n- Structured data validation\n- Crawlable links verification\n- Meta tags assessment\n- Mobile friendliness\n\n### Accessibility Audits\n- ARIA attribute validation\n- Color contrast checking\n- Heading hierarchy analysis\n- Alt text verification\n- Focus management assessment\n- Keyboard navigation testing\n\n### Resource Optimization\n- Image optimization suggestions\n- JavaScript bundling analysis\n- CSS optimization recommendations\n- Cache policy validation\n- Resource minification checks\n- Render-blocking resource identification\n\n## API Response Structure\n\nThe MCP server provides detailed JSON responses including:\n\n```javascript\n{\n    \"lighthouseResult\": {\n        \"categories\": {\n            \"performance\": { /* Performance metrics */ },\n            \"accessibility\": { /* Accessibility results */ },\n            \"best-practices\": { /* Best practices audit */ },\n            \"seo\": { /* SEO findings */ }\n        },\n        \"audits\": {\n            // Detailed audit results for each category\n        },\n        \"timing\": {\n            // Performance timing data\n        },\n        \"stackPacks\": {\n            // Technology-specific advice\n        }\n    }\n}\n```\n\n## Advanced Usage\n\n### Custom Configuration\nYou can customize the PageSpeed analysis by providing additional parameters:\n\n```json\n{\n    \"strategy\": \"mobile\", // or \"desktop\"\n    \"category\": [\"performance\", \"accessibility\", \"best-practices\", \"seo\"],\n    \"locale\": \"en\",\n    \"threshold\": {\n        \"performance\": 90,\n        \"accessibility\": 100,\n        \"best-practices\": 90,\n        \"seo\": 90\n    }\n}\n```\n\n### Error Handling\nThe MCP server includes robust error handling for:\n- Invalid URLs\n- Network timeouts\n- API rate limiting\n- Invalid parameters\n- Server-side errors\n\n## Requirements\n\n\n### Network Requirements\n- Stable internet connection\n- Access to Google's PageSpeed Insights API\n\n### Platform Support\n- Windows (x64, x86)\n- Linux (x64)\n- macOS (x64, arm64)\n\n## Integration Examples\n\n### Basic Integration\n```javascript\nconst PageSpeedMCP = require('pagespeed-mcp-server');\nconst mcp = new PageSpeedMCP();\n\nawait mcp.analyze('https://example.com');\n```\n\n### With Custom Options\n```javascript\nconst results = await mcp.analyze('https://example.com', {\n    strategy: 'mobile',\n    categories: ['performance', 'accessibility'],\n    locale: 'en-US'\n});\n```\n\n## Troubleshooting\n\n### Common Issues\n1. Connection Timeouts\n   - Check internet connectivity\n\n2. API Rate Limiting\n   - Use API key for higher limits\n\n3. Memory Issues\n   - Adjust Node.js memory limits\n\n## Development\n\n### Building from Source\n```bash\ngit clone https://github.com/phialsbasement/mcp-pagespeed-server\ncd mcp-pagespeed-server\nnpm install\nnpm run build\n```\n\n### Running Tests\n```bash\nnpm run test\n```\n\n### Contributing\n1. Fork the repository\n2. Create a feature branch\n3. Commit your changes\n4. Push to the branch\n5. Create a Pull Request\n\n## Support\n\n### Getting Help\n- GitHub Issues: Report bugs and feature requests\n\n## License\n\nMIT License - See LICENSE file for details\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "pagespeed",
        "performance",
        "google",
        "pagespeed insights",
        "google pagespeed",
        "phialsbasement pagespeed"
      ],
      "category": "web-search"
    },
    "PhialsBasement--mcp-webresearch-stealthified": {
      "owner": "PhialsBasement",
      "name": "mcp-webresearch-stealthified",
      "url": "https://github.com/PhialsBasement/mcp-webresearch-stealthified",
      "imageUrl": "/freedevtools/mcp/pfp/PhialsBasement.webp",
      "description": "Connects AI models to the web for real-time information retrieval, webpage content extraction, and research session tracking, along with the ability to capture screenshots.",
      "stars": 7,
      "forks": 1,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-09-24T02:14:39Z",
      "readme_content": "# MCP Web Research Server\n\nA Model Context Protocol (MCP) server for web research. \n\nBring real-time info into Claude and easily research any topic.\n\n## Features\n\n- Google search integration --- THIS FORK FIXES THIS --- NOW NO LONGER GETTING CAPTCHA BLOCKED\n- Webpage content extraction\n- Research session tracking (list of visited pages, search queries, etc.)\n- Screenshot capture\n\n## Prerequisites\n\n- [Node.js](https://nodejs.org/) >= 18 (includes `npm` and `npx`)\n- [Claude Desktop app](https://claude.ai/download)\n\n## Installation\n\nFirst, ensure you've downloaded and installed the [Claude Desktop app](https://claude.ai/download) and you have npm installed.\n\nNext, add this entry to your `claude_desktop_config.json` (on Mac, found at `~/Library/Application\\ Support/Claude/claude_desktop_config.json`):\n\n```json\n{\n  \"mcpServers\": {\n    \"webresearch\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@mzxrai/mcp-webresearch@latest\"]\n    }\n  }\n}\n```\n\nThis config allows Claude Desktop to automatically start the web research MCP server when needed.\n\n## Usage\n\nSimply start a chat with Claude and send a prompt that would benefit from web research. If you'd like a prebuilt prompt customized for deeper web research, you can use the `agentic-research` prompt that we provide through this package. Access that prompt in Claude Desktop by clicking the Paperclip icon in the chat input and then selecting `Choose an integration` → `webresearch` → `agentic-research`.\n\n<img src=\"https://i.ibb.co/N6Y3C0q/Screenshot-2024-12-05-at-11-01-27-PM.png\" alt=\"Example screenshot of web research\" width=\"400\"/>\n\n### Tools\n\n1. `search_google`\n   - Performs Google searches and extracts results\n   - Arguments: `{ query: string }`\n\n2. `visit_page`\n   - Visits a webpage and extracts its content\n   - Arguments: `{ url: string, takeScreenshot?: boolean }`\n\n3. `take_screenshot`\n   - Takes a screenshot of the current page\n   - No arguments required\n\n### Prompts\n\n#### `agentic-research`\nA guided research prompt that helps Claude conduct thorough web research. The prompt instructs Claude to:\n- Start with broad searches to understand the topic landscape\n- Prioritize high-quality, authoritative sources\n- Iteratively refine the research direction based on findings\n- Keep you informed and let you guide the research interactively\n- Always cite sources with URLs\n\n### Resources\n\nWe expose two things as MCP resources: (1) captured webpage screenshots, and (2) the research session.\n\n#### Screenshots\n\nWhen you take a screenshot, it's saved as an MCP resource. You can access captured screenshots in Claude Desktop via the Paperclip icon.\n\n#### Research Session\n\nThe server maintains a research session that includes:\n- Search queries\n- Visited pages\n- Extracted content\n- Screenshots\n- Timestamps\n\n### Suggestions\n\nFor the best results, if you choose not to use the `agentic-research` prompt when doing your research, it may be helpful to suggest high-quality sources for Claude to use when researching general topics. For example, you could prompt `news today from reuters or AP` instead of `news today`.\n\n## Problems\n\nThis is very much pre-alpha code. And it is also AIGC, so expect bugs.\n\nIf you run into issues, it may be helpful to check Claude Desktop's MCP logs:\n\n```bash\ntail -n 20 -f ~/Library/Logs/Claude/mcp*.log\n```\n\n## Development\n\n```bash\n# Install dependencies\npnpm install\n\n# Build the project\npnpm build\n\n# Watch for changes\npnpm watch\n\n# Run in development mode\npnpm dev\n```\n\n## Requirements\n\n- Node.js >= 18\n- Playwright (automatically installed as a dependency)\n\n## Verified Platforms\n\n- [x] macOS\n- [x] Linux\n- [x] Windows\n\n## License\n\nMIT\n\n## Author\n\n[mzxrai](https://github.com/mzxrai) \n",
      "npm_url": "https://www.npmjs.com/package/mcp-webresearch-stealthified",
      "npm_downloads": 328,
      "keywords": [
        "webresearch",
        "webpage",
        "web",
        "webresearch stealthified",
        "web search",
        "mcp webresearch"
      ],
      "category": "web-search"
    },
    "Prajwal-ak-0--youtube-mcp": {
      "owner": "Prajwal-ak-0",
      "name": "youtube-mcp",
      "url": "https://github.com/Prajwal-ak-0/youtube-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/Prajwal-ak-0.webp",
      "description": "Extract transcripts from YouTube videos, summarize content using Gemini AI, and facilitate natural language queries about video content. Provides tools for YouTube video search and comment analysis.",
      "stars": 12,
      "forks": 1,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-08-07T05:33:13Z",
      "readme_content": "# YouTube MCP\n[![smithery badge](https://smithery.ai/badge/@Prajwal-ak-0/youtube-mcp)](https://smithery.ai/server/@Prajwal-ak-0/youtube-mcp)\n\nA Model Context Protocol (MCP) server for YouTube video analysis, providing tools to get transcripts, summarize content, and query videos using Gemini AI.\n\n## Features\n\n- 📝 **Transcript Extraction**: Get detailed transcripts from YouTube videos\n- 📊 **Video Summarization**: Generate concise summaries using Gemini AI\n- ❓ **Natural Language Queries**: Ask questions about video content\n- 🔍 **YouTube Search**: Find videos matching specific queries\n- 💬 **Comment Analysis**: Retrieve and analyze video comments\n\n## Requirements\n\n- Python 3.9+\n- Google Gemini API key\n- YouTube Data API key\n\n## Running Locally\n\n### Installing via Smithery\n\nTo install youtube-mcp for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@Prajwal-ak-0/youtube-mcp):\n\n```bash\nnpx -y @smithery/cli install @Prajwal-ak-0/youtube-mcp --client claude\n```\n\n### Option 1: Install directly from smithery\n\n[![smithery badge](https://smithery.ai/badge/@Prajwal-ak-0/youtube-mcp)](https://smithery.ai/server/@Prajwal-ak-0/youtube-mcp)\n\n### Option 2: Local setup\n\n1. Clone the repository:\n   ```bash\n   git clone https://github.com/Prajwal-ak-0/youtube-mcp\n   cd youtube-mcp\n   ```\n\n2. Create a virtual environment and install dependencies:\n   ```bash\n   python -m venv .venv\n   source .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\n   pip install -e .\n   ```\n\n3. Create a `.env` file with your API keys:\n   ```\n   GEMINI_API_KEY=your_gemini_api_key\n   YOUTUBE_API_KEY=your_youtube_api_key\n   ```\n   \n4. Run MCP Server\n   ```bash\n   mcp dev main.py\n   ```\n   Navigate to [Stdio](http://localhost:5173)\n\n   OR\n\n6. Go cursor or windsurf configure with this json content:\n   ```json\n   {\n     \"youtube\": {\n       \"command\": \"uv\",\n       \"args\": [\n         \"--directory\",\n         \"/absolute/path/to/youtube-mcp\",\n         \"run\",\n         \"main.py\",\n         \"--transport\",\n         \"stdio\",\n         \"--debug\"\n       ]\n     }\n   }\n   ```\n\n## Available Tools\n\n- `youtube/get-transcript`: Get video transcript\n- `youtube/summarize`: Generate a video summary\n- `youtube/query`: Answer questions about a video\n- `youtube/search`: Search for YouTube videos\n- `youtube/get-comments`: Retrieve video comments\n- `youtube/get-likes`: Get video like count\n\n## Contributing\n\nContributions welcome! Please feel free to submit a Pull Request.\n",
      "npm_url": "https://www.npmjs.com/package/youtube-mcp",
      "npm_downloads": 590,
      "keywords": [
        "youtube",
        "prajwal",
        "search",
        "transcripts youtube",
        "youtube mcp",
        "ak youtube"
      ],
      "category": "web-search"
    },
    "PykeW--playwright-mcp-server": {
      "owner": "PykeW",
      "name": "playwright-mcp-server",
      "url": "https://github.com/PykeW/playwright-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/PykeW.webp",
      "description": "Retrieve and interact with web page content, enabling navigation, content extraction, and user action simulation on webpages using a standardized protocol.",
      "stars": 0,
      "forks": 0,
      "license": "No License",
      "language": "",
      "updated_at": "2025-04-10T06:33:48Z",
      "readme_content": "# Playwright MCP Server\n\n[![smithery badge](https://smithery.ai/badge/@showfive/playwright-mcp-server)](https://smithery.ai/server/@showfive/playwright-mcp-server)\n\nEnglish | [日本語](README.ja.md)\n\nThis project is a server that provides Playwright web page content retrieval functionality using the Model Context Protocol (MCP).\n\n## Features\n\n- Page navigation\n- Full page content retrieval\n- Visible content retrieval\n- Interactive elements detection\n- Mouse operation simulation\n- Echo functionality for testing\n\n## Installation\n\n### Installing via Smithery\n\nTo install Playwright MCP Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@showfive/playwright-mcp-server):\n\n```bash\nnpx -y @smithery/cli install @showfive/playwright-mcp-server --client claude\n```\n\n### Manual Installation\n```bash\nnpm install\n```\n\n## Usage\n\n### Starting the Server\n\n```bash\nnpm run build\nnpm start\n```\n\n### MCP Tools\n\nThe following tools are available:\n\n1. `navigate`\n   - Navigate to a specified URL\n   - Arguments: `{ url: string }`\n   - Returns: Navigation result\n\n2. `get_all_content`\n   - Retrieve content from the entire page\n   - Arguments: None\n   - Returns: All text content from the page\n\n3. `get_visible_content`\n   - Retrieve currently visible content\n   - Arguments: `{ minVisiblePercentage?: number }`\n   - Returns: Visible text content\n\n4. `get_interactive_elements`\n   - Get position information of interactive elements (buttons, links, etc.) on the page\n   - Arguments: None\n   - Returns: Coordinates and boundary information of interactive elements\n\n5. `move_mouse`\n   - Move mouse cursor to specified coordinates\n   - Arguments: `{ x: number, y: number }`\n   - Returns: Operation result\n\n6. `mouse_click`\n   - Execute mouse click at specified coordinates\n   - Arguments: `{ x: number, y: number, button?: \"left\" | \"right\" | \"middle\", clickCount?: number }`\n   - Returns: Click operation result\n\n7. `mouse_wheel`\n   - Execute mouse wheel scrolling\n   - Arguments: `{ deltaY: number, deltaX?: number }`\n   - Returns: Scroll operation result\n\n8. `drag_and_drop`\n   - Execute drag and drop operation\n   - Arguments: `{ sourceX: number, sourceY: number, targetX: number, targetY: number }`\n   - Returns: Drag and drop operation result\n\n9. `echo`\n   - Echo tool for testing\n   - Arguments: `{ message: string }`\n   - Returns: Sent message\n\n## Development\n\n### Running Tests\n\n```bash\n# Run all tests\nnpm test\n\n# Run tests in watch mode\nnpm run test:watch\n\n# Generate coverage report\nnpm run test:coverage\n```\n\n### Test Structure\n\n- `tools/*.test.ts`: Function tests for each tool\n- `mcp-server.test.ts`: MCP server function tests\n\n## Implementation Features\n\n1. Content Retrieval\n   - Full page content retrieval\n   - Visible content only retrieval\n   - Proper HTML parsing\n\n2. Interaction\n   - Detection and position information retrieval of interactive elements\n   - Mouse operation simulation (movement, clicks, scrolling)\n   - Drag and drop support\n\n3. Error Handling\n   - Proper navigation error handling\n   - Timeout processing\n   - Invalid URL detection\n\n4. Configuration Flexibility\n   - Headless/head mode selection\n   - Custom user agent\n   - Viewport size settings\n\n## Important Notes\n\n- Ensure necessary environment variables are set before using the MCP server\n- Follow the terms of service of target websites when retrieving web page content\n- Maintain appropriate intervals when sending multiple requests\n- When performing mouse operations, maintain appropriate intervals as they simulate actual user interactions\n\n## License\n\nISC\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "webpages",
        "web",
        "page",
        "simulation webpages",
        "webpages using",
        "interact web"
      ],
      "category": "web-search"
    },
    "RSS3-Network--mcp-server-rss3": {
      "owner": "RSS3-Network",
      "name": "mcp-server-rss3",
      "url": "https://github.com/RSS3-Network/mcp-server-rss3",
      "imageUrl": "/freedevtools/mcp/pfp/RSS3-Network.webp",
      "description": "Integrates with the RSS3 API to query data from decentralized chains, social media platforms, and the RSS3 network, providing real-time information on staking, nodes, and AI insights.",
      "stars": 3,
      "forks": 4,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-09-01T13:44:19Z",
      "readme_content": "# MCP Server for RSS3\n\nAn MCP server implementation that integrates the RSS3 API. Query the Open Web like a charm.\n\n## Features\n\nAnything in <https://docs.rss3.io/guide/developer/api>.\n\nFor example,\n\n- query data on decentralized chains and platforms;\n- query data on social media platforms;\n- query data on RSS3 network (about staking, nodes, etc.);\n- query ai intels.\n- ...\n\n### Examples\n\n> What did vitalik do recently?\n\n\n\n> Tell me about recent AI intels?\n\n\n\n> show me the rss3 chip with id 2048\n\n\n\n> what is the best rss3 node to stake?\n\n\n\n## Usage\n\n### Usage with Claude Desktop\n\nAdd this to your `claude_desktop_config.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"rss3\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"mcp-server-rss3\"\n      ]\n    }\n  }\n}\n```\n\n### Usage with Cursor\n\n1. Open Settings -> Cursor Settings\n2. Click on \"MCP\"\n3. Add new MCP Server with this:\n\n```json\n{\n  \"mcpServers\": {\n    \"rss3\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"mcp-server-rss3\"\n      ]\n    }\n  }\n}\n```\n\n### Usage with ChatWise\n\n1. Open Settings -> Tools\n2. Add new tool with this command:\n\n```\nnpx mcp-server-rss3\n```",
      "npm_url": "https://www.npmjs.com/package/mcp-server-rss3",
      "npm_downloads": 193,
      "keywords": [
        "rss3",
        "nodes",
        "api",
        "search rss3",
        "server rss3",
        "rss3 api"
      ],
      "category": "web-search"
    },
    "Raghu6798--Browser_scrape_mcp": {
      "owner": "Raghu6798",
      "name": "Browser_scrape_mcp",
      "url": "https://github.com/Raghu6798/Browser_scrape_mcp",
      "imageUrl": "/freedevtools/mcp/pfp/Raghu6798.webp",
      "description": "Automates web browsing and scraping tasks, leveraging AI to intelligently search and extract content from various online sources such as Google, GitHub, and Stack Overflow. Offers capabilities like saving screenshots and processing text for efficient data collection.",
      "stars": 1,
      "forks": 0,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-06-22T14:53:40Z",
      "readme_content": "# 🤖 Browser Automation Agent\r\n\r\nA powerful browser automation tool built with MCP (Model Controlled Program) that combines web scraping capabilities with LLM-powered intelligence. This agent can search Google, navigate to webpages, and intelligently scrape content from various websites including GitHub, Stack Overflow, and documentation sites.\r\n\r\n## 🚀 Features\r\n\r\n- **🔍 Google Search Integration**: Finds and retrieves top search results for any query\r\n- **🕸️ Intelligent Web Scraping**: Tailored scraping strategies for different website types:\r\n  - 📂 GitHub repositories\r\n  - 💬 Stack Overflow questions and answers\r\n  - 📚 Documentation pages\r\n  - 🌐 Generic websites\r\n- **🧠 AI-Powered Processing**: Uses Mistral AI for understanding and processing scraped content\r\n- **🥷 Stealth Mode**: Implements browser fingerprint protection to avoid detection\r\n- **💾 Content Saving**: Automatically saves both screenshots and text content from scraped pages\r\n\r\n## 🏗️ Architecture\r\n\r\nThis project uses a client-server architecture powered by MCP:\r\n\r\n- **🖥️ Server**: Handles browser automation and web scraping tasks\r\n- **👤 Client**: Provides the AI interface using Mistral AI and LangGraph\r\n- **📡 Communication**: Uses stdio for client-server communication\r\n\r\n## ⚙️ Requirements\r\n\r\n- 🐍 Python 3.8+\r\n- 🎭 Playwright\r\n- 🧩 MCP (Model Controlled Program)\r\n- 🔑 Mistral AI API key\r\n\r\n## 📥 Installation\r\n\r\n1. Clone the repository:\r\n\r\n```bash\r\ngit clone https://github.com/yourusername/browser-automation-agent.git\r\ncd browser-automation-agent\r\n```\r\n\r\n2. Install dependencies:\r\n\r\n```bash\r\npip install -r requirements.txt\r\n```\r\n\r\n3. Install Playwright browsers:\r\n\r\n```bash\r\nplaywright install\r\n```\r\n\r\n4. Create a `.env` file in the project root and add your Mistral AI API key:\r\n\r\n```\r\nMISTRAL_API_KEY=your_api_key_here\r\n```\r\n\r\n## 📋 Usage\r\n\r\n### Running the Server\r\n\r\n```bash\r\npython main.py\r\n```\r\n\r\n### Running the Client\r\n\r\n```bash\r\npython client.py\r\n```\r\n\r\n### Sample Interaction\r\n\r\nOnce both the server and client are running:\r\n\r\n1. Enter your query when prompted\r\n2. The agent will:\r\n   - 🔍 Search Google for relevant results\r\n   - 🧭 Navigate to the top result\r\n   - 📊 Scrape content based on the website type\r\n   - 📸 Save screenshots and content to files\r\n   - 📤 Return processed information\r\n\r\n## 🛠️ Tool Functions\r\n\r\n### `get_top_google_url`\r\n🔍 Searches Google and returns the top result URL for a given query.\r\n\r\n### `browse_and_scrape`\r\n🌐 Navigates to a URL and scrapes content based on the website type.\r\n\r\n### `scrape_github`\r\n📂 Specializes in extracting README content and code blocks from GitHub repositories.\r\n\r\n### `scrape_stackoverflow`\r\n💬 Extracts questions, answers, comments, and code blocks from Stack Overflow pages.\r\n\r\n### `scrape_documentation`\r\n📚 Optimized for extracting documentation content and code examples.\r\n\r\n### `scrape_generic`\r\n🌐 Extracts paragraph text and code blocks from generic websites.\r\n\r\n## 📁 File Structure\r\n\r\n```\r\nbrowser-automation-agent/\r\n├── main.py            # MCP server implementation\r\n├── client.py          # Mistral AI client implementation\r\n├── requirements.txt   # Project dependencies\r\n├── .env               # Environment variables (API keys)\r\n└── README.md          # Project documentation\r\n```\r\n\r\n## 📤 Output Files\r\n\r\nThe agent generates two types of output files with timestamps:\r\n\r\n- 📸 `final_page_YYYYMMDD_HHMMSS.png`: Screenshot of the final page state\r\n- 📄 `scraped_content_YYYYMMDD_HHMMSS.txt`: Extracted text content from the page\r\n\r\n## ⚙️ Customization\r\n\r\nYou can modify the following parameters in the code:\r\n\r\n- 🖥️ Browser window size: Adjust `width` and `height` in `browse_and_scrape`\r\n- 👻 Headless mode: Set `headless=True` for invisible browser operation\r\n- 🔢 Number of Google results: Change `num_results` in `get_top_google_url`\r\n\r\n## ❓ Troubleshooting\r\n\r\n- **🔌 Connection Issues**: Ensure both server and client are running in separate terminals\r\n- **🎭 Playwright Errors**: Make sure browsers are installed with `playwright install`\r\n- **🔑 API Key Errors**: Verify your Mistral API key is correctly set in the `.env` file\r\n- **🛣️ Path Errors**: Update the path to `main.py` in `client.py` if needed\r\n\r\n## 📜 License\r\n\r\n[MIT License](LICENSE)\r\n\r\n## 🤝 Contributing\r\n\r\nContributions are welcome! Please feel free to submit a Pull Request.\r\n\r\n---\r\n\r\nBuilt with 🧩 MCP, 🎭 Playwright, and 🧠 Mistral AI\r\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "scraping",
        "browser_scrape_mcp",
        "google",
        "browsing scraping",
        "raghu6798 browser_scrape_mcp",
        "browser_scrape_mcp automates"
      ],
      "category": "web-search"
    },
    "RamXX--mcp-tavily": {
      "owner": "RamXX",
      "name": "mcp-tavily",
      "url": "https://github.com/RamXX/mcp-tavily",
      "imageUrl": "/freedevtools/mcp/pfp/RamXX.webp",
      "description": "Provides AI-powered web search capabilities through Tavily's search API, enabling sophisticated searches, direct answers to questions, and retrieval of recent news articles with relevant content extraction.",
      "stars": 70,
      "forks": 16,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-10-03T04:12:22Z",
      "readme_content": "# Tavily MCP Server\n\nA Model Context Protocol server that provides AI-powered web search capabilities using Tavily's search API. This server enables LLMs to perform sophisticated web searches, get direct answers to questions, and search recent news articles with AI-extracted relevant content.\n\n## Features\n\n### Available Tools\n\n- `tavily_web_search` - Performs comprehensive web searches with AI-powered content extraction.\n\n  - `query` (string, required): Search query\n  - `max_results` (integer, optional): Maximum number of results to return (default: 5, max: 20)\n  - `search_depth` (string, optional): Either \"basic\" or \"advanced\" search depth (default: \"basic\")\n  - `include_domains` (list or string, optional): List of domains to specifically include in results\n  - `exclude_domains` (list or string, optional): List of domains to exclude from results\n\n- `tavily_answer_search` - Performs web searches and generates direct answers with supporting evidence.\n\n  - `query` (string, required): Search query\n  - `max_results` (integer, optional): Maximum number of results to return (default: 5, max: 20)\n  - `search_depth` (string, optional): Either \"basic\" or \"advanced\" search depth (default: \"advanced\")\n  - `include_domains` (list or string, optional): List of domains to specifically include in results\n  - `exclude_domains` (list or string, optional): List of domains to exclude from results\n\n- `tavily_news_search` - Searches recent news articles with publication dates.\n  - `query` (string, required): Search query\n  - `max_results` (integer, optional): Maximum number of results to return (default: 5, max: 20)\n  - `days` (integer, optional): Number of days back to search (default: 3)\n  - `include_domains` (list or string, optional): List of domains to specifically include in results\n  - `exclude_domains` (list or string, optional): List of domains to exclude from results\n\n### Prompts\n\nThe server also provides prompt templates for each search type:\n\n- **tavily_web_search** - Search the web using Tavily's AI-powered search engine\n- **tavily_answer_search** - Search the web and get an AI-generated answer with supporting evidence\n- **tavily_news_search** - Search recent news articles with Tavily's news search\n\n## Prerequisites\n\n- Python 3.11 or later\n- A Tavily API key (obtain from [Tavily's website](https://tavily.com))\n- `uv` Python package manager (recommended)\n\n## Installation\n\n### Option 1: Using pip or uv\n\n```bash\n# With pip\npip install mcp-tavily\n\n# Or with uv (recommended)\nuv add mcp-tavily\n```\n\nYou should see output similar to:\n\n```\nResolved packages: mcp-tavily, mcp, pydantic, python-dotenv, tavily-python [...]\nSuccessfully installed mcp-tavily-0.1.4 mcp-1.0.0 [...]\n```\n\n### Option 2: From source\n\n```bash\n# Clone the repository\ngit clone https://github.com/RamXX/mcp-tavily.git\ncd mcp-tavily\n\n# Create a virtual environment (optional but recommended)\npython -m venv .venv\nsource .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\n\n# Install dependencies and build\nuv sync  # Or: pip install -r requirements.txt\nuv build  # Or: pip install -e .\n\n# To install with test dependencies:\nuv sync --dev  # Or: pip install -r requirements-dev.txt\n```\n\nDuring installation, you should see the package being built and installed with its dependencies.\n\n### Usage with VS Code\n\nFor quick installation, use one of the one-click install buttons below:\n\n[![Install with UV in VS Code](https://img.shields.io/badge/VS_Code-UV-0098FF?style=flat-square&logo=visualstudiocode&logoColor=white)](https://insiders.vscode.dev/redirect/mcp/install?name=tavily&inputs=%5B%7B%22type%22%3A%22promptString%22%2C%22id%22%3A%22apiKey%22%2C%22description%22%3A%22Tavily%20API%20Key%22%2C%22password%22%3Atrue%7D%5D&config=%7B%22command%22%3A%22uvx%22%2C%22args%22%3A%5B%22mcp-tavily%22%5D%2C%22env%22%3A%7B%22TAVILY_API_KEY%22%3A%22%24%7Binput%3AapiKey%7D%22%7D%7D) [![Install with UV in VS Code Insiders](https://img.shields.io/badge/VS_Code_Insiders-UV-24bfa5?style=flat-square&logo=visualstudiocode&logoColor=white)](https://insiders.vscode.dev/redirect/mcp/install?name=tavily&inputs=%5B%7B%22type%22%3A%22promptString%22%2C%22id%22%3A%22apiKey%22%2C%22description%22%3A%22Tavily%20API%20Key%22%2C%22password%22%3Atrue%7D%5D&config=%7B%22command%22%3A%22uvx%22%2C%22args%22%3A%5B%22mcp-tavily%22%5D%2C%22env%22%3A%7B%22TAVILY_API_KEY%22%3A%22%24%7Binput%3AapiKey%7D%22%7D%7D&quality=insiders)\n\nFor manual installation, add the following JSON block to your User Settings (JSON) file in VS Code. You can do this by pressing `Ctrl + Shift + P` and typing `Preferences: Open User Settings (JSON)`.\n\nOptionally, you can add it to a file called `.vscode/mcp.json` in your workspace. This will allow you to share the configuration with others.\n\n> Note that the `mcp` key is not needed in the `.vscode/mcp.json` file.\n\n```json\n{\n  \"mcp\": {\n    \"inputs\": [\n      {\n        \"type\": \"promptString\",\n        \"id\": \"apiKey\",\n        \"description\": \"Tavily API Key\",\n        \"password\": true\n      }\n    ],\n    \"servers\": {\n      \"tavily\": {\n        \"command\": \"uvx\",\n        \"args\": [\"mcp-tavily\"],\n        \"env\": {\n          \"TAVILY_API_KEY\": \"${input:apiKey}\"\n        }\n      }\n    }\n  }\n}\n```\n\n## Configuration\n\n### API Key Setup\n\nThe server requires a Tavily API key, which can be provided in three ways:\n\n1. Through a `.env` file in your project directory:\n\n   ```\n   TAVILY_API_KEY=your_api_key_here\n   ```\n\n2. As an environment variable:\n\n   ```bash\n   export TAVILY_API_KEY=your_api_key_here\n   ```\n\n3. As a command-line argument:\n   ```bash\n   python -m mcp_server_tavily --api-key=your_api_key_here\n   ```\n\n### Configure for Claude.app\n\nAdd to your Claude settings:\n\n```json\n\"mcpServers\": {\n  \"tavily\": {\n    \"command\": \"python\",\n    \"args\": [\"-m\", \"mcp_server_tavily\"]\n  },\n  \"env\": {\n    \"TAVILY_API_KEY\": \"your_api_key_here\"\n  }\n}\n```\n\nIf you encounter issues, you may need to specify the full path to your Python interpreter. Run `which python` to find the exact path.\n\n## Usage Examples\n\nFor a regular web search:\n\n```\nTell me about Anthropic's newly released MCP protocol\n```\n\nTo generate a report with domain filtering:\n\n```\nTell me about redwood trees. Please use MLA format in markdown syntax and include the URLs in the citations. Exclude Wikipedia sources.\n```\n\nTo use answer search mode for direct answers:\n\n```\nI want a concrete answer backed by current web sources: What is the average lifespan of redwood trees?\n```\n\nFor news search:\n\n```\nGive me the top 10 AI-related news in the last 5 days\n```\n\n## Testing\n\nThe project includes a comprehensive test suite with automated dependency compatibility testing.\n\n### Running Tests\n\n1. Install test dependencies:\n\n   ```bash\n   source .venv/bin/activate  # If using a virtual environment\n   uv sync --dev  # Or: pip install -r requirements-dev.txt\n   ```\n\n2. Run the standard test suite:\n   ```bash\n   ./tests/run_tests.sh\n   # Or using Make\n   make test\n   ```\n\n### Dependency Compatibility Testing\n\nTo ensure the project works with the latest dependency versions, use these commands:\n\n```bash\n# Test with latest dependencies using Make\nmake test-deps\n\n# Full compatibility test with verbose output\nmake test-compatibility\n\n# Or use the standalone script\n./scripts/test-compatibility.sh\n```\n\nThese commands will:\n- Update all dependencies to their latest versions\n- Run the full test suite with coverage\n- Report any compatibility issues\n- Show version changes for transparency\n\n### Automated Testing\n\nThe project includes automated dependency compatibility testing through GitHub Actions:\n\n- **Weekly Testing**: Runs every Monday at 8 AM UTC\n- **Multi-Python Support**: Tests against Python 3.11, 3.12, and 3.13\n- **Issue Creation**: Automatically creates GitHub issues when tests fail\n- **Manual Trigger**: Can be triggered manually from the GitHub Actions tab\n\n### Understanding Test Results\n\n**When tests pass**: Your project is compatible with the latest dependency versions. You can safely update your requirements files.\n\n**When tests fail**: Review the test output to identify breaking changes, update your code to handle API changes, update tests if needed, or consider pinning problematic dependency versions.\n\n### Test Output Example\n\nYou should see output similar to:\n\n```\n======================================================= test session starts ========================================================\nplatform darwin -- Python 3.13.3, pytest-8.3.5, pluggy-1.5.0\nrootdir: /Users/ramirosalas/workspace/mcp-tavily\nconfigfile: pyproject.toml\nplugins: cov-6.0.0, asyncio-0.25.3, anyio-4.8.0, mock-3.14.0\nasyncio: mode=Mode.STRICT, asyncio_default_fixture_loop_scope=function\ncollected 50 items                                                                                                                 \n\ntests/test_docker.py ..                                                                                                      [  4%]\ntests/test_integration.py .....                                                                                              [ 14%]\ntests/test_models.py .................                                                                                       [ 48%]\ntests/test_server_api.py .....................                                                                               [ 90%]\ntests/test_utils.py .....                                                                                                    [100%]\n\n---------- coverage: platform darwin, python 3.13.3-final-0 ----------\nName                                Stmts   Miss  Cover\n-------------------------------------------------------\nsrc/mcp_server_tavily/__init__.py      16      2    88%\nsrc/mcp_server_tavily/__main__.py       2      2     0%\nsrc/mcp_server_tavily/server.py       149     16    89%\n-------------------------------------------------------\nTOTAL                                 167     20    88%\n```\n\nThe test suite includes tests for data models, utility functions, integration testing, error handling, and parameter validation. It focuses on verifying that all API capabilities work correctly, including handling of domain filters and various input formats.\n\n## Release Management\n\nThe project includes tools for building and releasing with the latest dependency versions:\n\n### Building with Latest Dependencies\n\n```bash\n# Build package with latest dependency versions\nmake build-latest\n\n# Complete release workflow: test, build, and check with latest deps\nmake release-all\n\n# Prepare a release with version management\n./scripts/prepare-release.sh [new_version]\n```\n\n### Release Workflow\n\n**Recommended approach for releases with latest dependencies:**\n\n1. **Complete release preparation**: `make release-all`\n2. **Upload without downgrades**: `make upload-latest`\n\n**Alternative step-by-step approach:**\n\n1. **Test with latest dependencies**: `make test-compatibility`\n2. **Build for release**: `make release-build`\n3. **Upload without rebuilding**: `make upload-latest`\n\n**One-command release and publish:**\n```bash\nmake release-publish\n```\n\n**Important**: Use `make upload-latest` instead of `make upload` to prevent dependency downgrades during the upload process. The `upload-latest` command uses existing distribution files without reinstalling dependencies.\n\nThe release commands ensure your package is built and tested with the most recent compatible dependency versions, preventing the downgrades that can occur with traditional build chains.\n\n## Docker\n\nBuild the Docker image:\n\n```bash\nmake docker-build\n```\n\nAlternatively, build directly with Docker:\n\n```bash\ndocker build -t mcp_tavily .\n```\n\nRun a detached Docker container (default name `mcp_tavily_container`, port 8000 → 8000):\n\n```bash\nmake docker-run\n```\n\nOr manually:\n\n```bash\ndocker run -d --name mcp_tavily_container \\\n  -e TAVILY_API_KEY=your_api_key_here \\\n  -p 8000:8000 mcp_tavily\n```\n\nStop and remove the container:\n\n```bash\nmake docker-stop\n```\n\nFollow container logs:\n\n```bash\nmake docker-logs\n```\n\nYou can override defaults by setting environment variables:\n  - DOCKER_IMAGE: image name (default `mcp_tavily`)\n  - DOCKER_CONTAINER: container name (default `mcp_tavily_container`)\n  - HOST_PORT: host port to bind (default `8000`)\n  - CONTAINER_PORT: container port (default `8000`)\n\n## Debugging\n\nYou can use the MCP inspector to debug the server:\n\n```bash\n# Using npx\nnpx @modelcontextprotocol/inspector python -m mcp_server_tavily\n\n# For development\ncd path/to/mcp-tavily\nnpx @modelcontextprotocol/inspector python -m mcp_server_tavily\n```\n\n## Contributing\n\nWe welcome contributions to improve mcp-tavily! Here's how you can help:\n\n1. Fork the repository\n2. Create a feature branch (`git checkout -b feature/amazing-feature`)\n3. Make your changes\n4. Run tests to ensure they pass\n5. Commit your changes (`git commit -m 'Add amazing feature'`)\n6. Push to the branch (`git push origin feature/amazing-feature`)\n7. Open a Pull Request\n\nFor examples of other MCP servers and implementation patterns, see:\nhttps://github.com/modelcontextprotocol/servers\n\n## License\n\nmcp-tavily is licensed under the MIT License. See the [LICENSE](LICENSE) file for details.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "searches",
        "search",
        "retrieval",
        "tavily search",
        "search api",
        "web search"
      ],
      "category": "web-search"
    },
    "Richard-Weiss--mcp-google-cse": {
      "owner": "Richard-Weiss",
      "name": "mcp-google-cse",
      "url": "https://github.com/Richard-Weiss/mcp-google-cse",
      "imageUrl": "/freedevtools/mcp/pfp/Richard-Weiss.webp",
      "description": "Provides search capabilities using a custom Google search engine, returning only search results without content extraction. Designed for integration with other tools to enable content retrieval and more extensive searching.",
      "stars": 28,
      "forks": 5,
      "license": "The Unlicense",
      "language": "Python",
      "updated_at": "2025-09-08T02:35:59Z",
      "readme_content": "# Google Custom Search Engine MCP Server\n\nA Model Context Protocol server that provides search capabilities using a CSE (custom search engine). This server enables LLMs to provide a regular google search term and returns the found search results.\n\nThe tool only returns the results itself and not the content, the tool should be combined with other servers like [mcp-server-fetch](https://github.com/modelcontextprotocol/servers/tree/main/src/fetch) to extract the content from the search results.\nYou may also combine it with other tools to enable some kind of \"deep search\" or tool chaining in general.\n\n**The free quota is 100 searches (1 tool call == 1 search) per day, if you don't want to set up billing and this is insufficient for your use case, you should consider using another server.**\n\n<a href=\"https://glama.ai/mcp/servers/mieczol4lv\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/mieczol4lv/badge\" alt=\"Google Custom Search Engine Server MCP server\" /></a>\n[![smithery badge](https://smithery.ai/badge/@Richard-Weiss/mcp-google-cse)](https://smithery.ai/server/@Richard-Weiss/mcp-google-cse)\n\n## Available Tools\n\n- `google_search` - Searches the custom search engine using the search term and returns a list of results containing the title, link and snippet of each result.\n    - `search_term` (string, required): The search term to search for, equaling the [query parameter](https://bit.ly/AllTheOperators) `q` in the usual Google search.\n\n## Environment variables\n\n- `API_KEY` (required): The API key for the custom search engine.\n- `ENGINE_ID` (required): The engine ID for the custom search engine.\n- `SERVICE_NAME` (required/optional): The name of the service, leave empty if you haven't changed the name (customsearch).\n- `COUNTRY_REGION` (optional): Restricts search results to documents originating in a particular country. See [Country Parameter Values](https://developers.google.com/custom-search/docs/json_api_reference#countryCollections) for valid values.\n- `GEOLOCATION` (optional, default \"us\"): The geolocation of the end-user performing the search. See [Geolocation Parameter Values](https://developers.google.com/custom-search/docs/json_api_reference#countryCodes) for valid values.\n- `RESULT_LANGUAGE` (optional, default \"lang_en\"): The language of the search results. See [CSE Query parameters, lr](https://developers.google.com/custom-search/v1/reference/rest/v1/cse/list?apix=true#query-parameters) for valid values.\n- `RESULT_NUM` (optional, default 10): The number of search results to return. Range from 1-10.\n\n## CSE Setup\nCreating a custom search engine is comparatively easy, completely free and can be done in under 5 minutes.\n\n1. Go to https://console.cloud.google.com/ and create a new project. Call it \"Claude CSE\" for example.\n2. Select the project and search for \"Custom Search API\" in the search bar.\n3. Click on the search result and click on \"Enable\".\n4. Click on the Credentials tab and create a new API key.\n5. Go to https://programmablesearchengine.google.com to create a new custom search engine.\n6. Create a new search engine and give it any name, the name doesn't correlate to SERVICE_NAME.\n7. Select \"Search the entire web\" if you want a normal Google Search experience.\n8. Click on \"Create\" and copy the engine id from the js code, or hit customize and get it from the overview.\n9. You can optionally customize the search engine to your liking.\n\nWith the default quota, you will get 100 searches per day for free. A tool call only costs 1 search, even if you get 10 results for example.\n\n\n## Installation\n\n### Using uv (recommended)\n\nWhen using [`uv`](https://docs.astral.sh/uv/) no specific installation is needed. We will\nuse [`uvx`](https://docs.astral.sh/uv/guides/tools/) to directly run *mcp-google-cse*.\n\n### Using PIP\n\nAlternatively you can install `mcp-google-cse` via pip:\n\n```\npip install mcp-google-cse\n```\n\nAfter installation, you can run it as a script using:\n\n```\npython -m mcp-google-cse\n```\n\n### Installing via Smithery\n\nTo install Google Custom Search Engine for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@Richard-Weiss/mcp-google-cse):\n\n```bash\nnpx -y @smithery/cli install @Richard-Weiss/mcp-google-cse --client claude\n```\n\n## Configuration\n\n### Configure for Claude app\n\nAdd to your `claude_desktop_config.json`:\n\n\n#### Using uvx (use this if you don't know which one to choose)\n```\n\"mcp-google-cse\": {\n    \"command\": \"uvx\",\n    \"args\": [\"mcp-google-cse\"],\n    \"env\": {\n        \"API_KEY\": \"\",\n        \"ENGINE_ID\": \"\"\n    }\n}\n```\n\n\n#### Using pip installation\n\n```\n\"mcp-google-cse\": {\n    \"command\": \"python\",\n    \"args\": [\"-m\", \"mcp-google-cse\"],\n    \"env\": {\n        \"API_KEY\": \"\",\n        \"ENGINE_ID\": \"\"\n    }\n}\n```\n\n#### Running locally\n\n```\n    \"mcp-google-cse\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"--directory\",\n        \"{{Path to the cloned repo\",\n        \"run\",\n        \"mcp-google-cse\"\n      ],\n      \"env\": {\n        \"API_KEY\": \"\",\n        \"ENGINE_ID\": \"\"\n      }\n    }\n```\n\n### Example result\ngoogle_search(\"What is MCP after:2024-11-01\")\nResult:\n```json\n[\n    {\n        \"title\": \"Can someone explain MCP to me? How are you using it? And what ...\",\n        \"link\": \"https://www.reddit.com/r/ClaudeAI/comments/1h55zxd/can_someone_explain_mcp_to_me_how_are_you_using/\",\n        \"snippet\": \"Dec 2, 2024 ... Comments Section ... MCP essentially allows you to give Claude access to various external systems. This can be files on your computer, an API, a browser, a ...\"\n    },\n    {\n        \"title\": \"Introducing the Model Context Protocol \\\\ Anthropic\",\n        \"link\": \"https://www.anthropic.com/news/model-context-protocol\",\n        \"snippet\": \"Nov 25, 2024 ... The Model Context Protocol (MCP) is an open standard for connecting AI assistants to the systems where data lives, including content repositories, ...\"\n    },\n    {\n        \"title\": \"3.5 Sonnet + MCP + Aider = Complete Game Changer : r ...\",\n        \"link\": \"https://www.reddit.com/r/ChatGPTCoding/comments/1hwn6qd/35_sonnet_mcp_aider_complete_game_changer/\",\n        \"snippet\": \"Jan 8, 2025 ... Really cool stuff. For those out of the loop here are some MCP servers. You can give your Claude chat (in the desktop version, or in a tool like Cline) ...\"\n    },\n    {\n        \"title\": \"Announcing Spring AI MCP: A Java SDK for the Model Context ...\",\n        \"link\": \"https://spring.io/blog/2024/12/11/spring-ai-mcp-announcement\",\n        \"snippet\": \"Dec 11, 2024 ... This SDK will enable Java developers to easily connect with an expanding array of AI models and tools while maintaining consistent, reliable integration ...\"\n    },\n    {\n        \"title\": \"Implementing a MCP server in Quarkus - Quarkus\",\n        \"link\": \"https://quarkus.io/blog/mcp-server/\",\n        \"snippet\": \"6 days ago ... The Model Context Protocol (MCP) is an emerging standard that enables AI models to safely interact with external tools and resources. In this tutorial, I'll ...\"\n    },\n    {\n        \"title\": \"mark3labs/mcp-go: A Go implementation of the Model ... - GitHub\",\n        \"link\": \"https://github.com/mark3labs/mcp-go\",\n        \"snippet\": \"Dec 18, 2024 ... A Go implementation of the Model Context Protocol (MCP), enabling seamless integration between LLM applications and external data sources and tools.\"\n    },\n    {\n        \"title\": \"MCP enables Claude to Build, Run and Test Web Apps by Looking ...\",\n        \"link\": \"https://wonderwhy-er.medium.com/mcp-enable-claude-to-build-run-and-test-web-apps-using-screenshots-3ae06aea6c4a\",\n        \"snippet\": \"Dec 18, 2024 ... How to Replicate My Experiment on Your Machine. If you're ready to dive into setting up MCP for Claude, follow these steps: ... 2. Download the Project: ... 3.\"\n    },\n    {\n        \"title\": \"MCP definition and meaning | Collins English Dictionary\",\n        \"link\": \"https://www.collinsdictionary.com/dictionary/english/mcp\",\n        \"snippet\": \"2 days ago ... 2 meanings: male chauvinist pig → informal, derogatory a man who exhibits male chauvinism Abbreviation: MCP.... Click for more definitions.\"\n    },\n    {\n        \"title\": \"What is Anthropic's New MCP Standard and How Can It Improve ...\",\n        \"link\": \"https://dappier.medium.com/what-is-anthropics-new-mcp-standard-and-how-can-it-improve-your-ai-agent-be6f6c72eb6a\",\n        \"snippet\": \"Nov 26, 2024 ... Anthropic has released a new protocol, MCP, for connecting AI agents to data sets. This blog explores when and why developers might use MCP to improve their ...\"\n    },\n    {\n        \"title\": \"Mostafa Gharib on LinkedIn: What is MCP and how it works\",\n        \"link\": \"https://www.linkedin.com/posts/mostafa-gharib_what-is-mcp-and-how-it-works-activity-7274301560594026497-p_yq\",\n        \"snippet\": \"Dec 15, 2024 ... ... MCP Host can use. (Bonus: SDKs in Python and TypeScript make it easy to build these servers!) 2️⃣ MCP Clients These interact with MCP Servers via the protocol.\"\n    }\n]\n```\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "search",
        "cse",
        "searching",
        "google cse",
        "web search",
        "search engine"
      ],
      "category": "web-search"
    },
    "Rickyyy1116--mcp-youtube-sheets": {
      "owner": "Rickyyy1116",
      "name": "mcp-youtube-sheets",
      "url": "https://github.com/Rickyyy1116/mcp-youtube-sheets",
      "imageUrl": "/freedevtools/mcp/pfp/Rickyyy1116.webp",
      "description": "Search YouTube videos and save results to Google Sheets with structured data including video title, URL, channel name, and publish date.",
      "stars": 11,
      "forks": 6,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-08-07T05:32:35Z",
      "readme_content": "# YouTube to Google Sheets MCP Server\n\nThis MCP server provides functionality to search YouTube videos and automatically save the results to Google Sheets. It's designed to work with Claude and other AI assistants that support the Model Context Protocol.\n\n[English](README.md) | [日本語](README.ja.md)\n\n## Features\n\n- Search YouTube videos using the YouTube Data API v3\n- Save search results to Google Sheets automatically\n- Configurable search parameters (query, max results)\n- Results include video title, URL, channel name, and publish date\n\n## Installation\n\n```bash\nnpm install @rikukawa/youtube-sheets-server\n```\n\n## Prerequisites\n\n1. YouTube Data API v3 Setup:\n   - Go to [Google Cloud Console](https://console.cloud.google.com/)\n   - Create a new project\n   - Enable YouTube Data API v3\n   - Create an API key\n\n2. Google Sheets API Setup:\n   - In the same project, enable Google Sheets API\n   - Create a service account\n   - Download the service account key (JSON format)\n   - Share your target Google Sheet with the service account email\n\n## Configuration\n\nAdd the server to your MCP settings file:\n\n```json\n{\n  \"mcpServers\": {\n    \"youtube-sheets\": {\n      \"command\": \"node\",\n      \"args\": [\"path/to/youtube-sheets-server/build/index.js\"],\n      \"env\": {\n        \"YOUTUBE_API_KEY\": \"your-youtube-api-key\",\n        \"SPREADSHEET_ID\": \"your-spreadsheet-id\"\n      },\n      \"disabled\": false,\n      \"alwaysAllow\": []\n    }\n  }\n}\n```\n\n## Usage\n\n“Ask the AI assistant to ‘search for YouTube videos with “ChatGPT usage” and retrieve 10 videos’ and try using it in that way.”\n\n## Output Format\n\nThe tool will save the following information to your Google Sheet:\n- Video Title\n- Video URL\n- Channel Name\n- Publish Date\n\n## License\n\nMIT\n\n## Author\n\nRiku Kawashima\n\n## Repository\n\n[GitHub Repository](https://github.com/Rickyyy1116/mcp-youtube-sheets)\n\n## NPM Package\n\n[@rikukawa/youtube-sheets-server](https://www.npmjs.com/package/@rikukawa/youtube-sheets-server)\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "rickyyy1116",
        "youtube",
        "videos",
        "mcp youtube",
        "search rickyyy1116",
        "rickyyy1116 mcp"
      ],
      "category": "web-search"
    },
    "RmMargt--searchAPI-mcp": {
      "owner": "RmMargt",
      "name": "searchAPI-mcp",
      "url": "https://github.com/RmMargt/searchAPI-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/RmMargt.webp",
      "description": "Provides standardized access to various search services such as Google Maps, Google Flights, and Google Hotels, enabling efficient retrieval of information through a unified interface. It encapsulates search operations, functioning as a bridge between AI assistants and these services.",
      "stars": 59,
      "forks": 4,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-10-02T21:44:42Z",
      "readme_content": "# SearchAPI MCP Server\n\n一个基于 Model Context Protocol (MCP) 的搜索 API 服务器，提供了对 Google Maps、Google Flights、Google Hotels 等服务的标准化访问接口。该服务器使 AI 助手能够通过统一的接口访问各种搜索服务。\n\nA Model Context Protocol (MCP) based search API server that provides standardized access to Google Maps, Google Flights, Google Hotels and other services. This server enables AI assistants to access various search services through a unified interface.\n\n## 概述 | Overview\n\nSearchAPI-MCP-Server 实现了 Model Context Protocol，将各种搜索操作封装为工具和资源。它作为 AI 助手和搜索服务之间的桥梁，支持地图搜索、航班查询、酒店预订等多种功能。\n\nSearchAPI-MCP-Server implements the Model Context Protocol, encapsulating various search operations as tools and resources. It serves as a bridge between AI assistants and search services, supporting map search, flight queries, hotel bookings, and more.\n\n## 功能特性 | Features\n\n### Google 搜索 | Google Search\n* 网页搜索结果 | Web search results\n* 知识图谱集成 | Knowledge graph integration\n* 相关问题推荐 | Related questions\n* 搜索建议 | Search suggestions\n* 多语言支持 | Multi-language support\n* 地区特定结果 | Region-specific results\n* 时间范围过滤 | Time range filtering\n* 安全搜索选项 | Safe search options\n\n### Google Video 搜索 | Google Video Search\n* 视频内容搜索 | Video content search\n* 视频列表获取 | Video list retrieval\n* 视频轮播支持 | Video carousel support\n* 短视频内容 | Short video content\n* 按时长筛选 | Duration filtering\n* 按来源过滤 | Source filtering\n* 按上传时间排序 | Upload time sorting\n* 高清预览支持 | HD preview support\n\n### Google Maps 搜索 | Google Maps Search\n* 搜索地点和服务 | Search places and services\n* 获取地点详细信息 | Get place details\n* 查看用户评论 | View user reviews\n* 获取位置坐标 | Get location coordinates\n\n### Google Flights 航班搜索 | Google Flights Search\n* 单程/往返航班搜索 | One-way/round-trip flight search\n* 多城市行程规划 | Multi-city itinerary planning\n* 航班价格日历 | Flight price calendar\n* 航班筛选和排序 | Flight filtering and sorting\n* 行李额度查询 | Baggage allowance query\n* 航空公司选择 | Airline selection\n\n### Google Hotels 酒店搜索 | Google Hotels Search\n* 酒店位置搜索 | Hotel location search\n* 价格和可用性查询 | Price and availability query\n* 设施和服务筛选 | Facilities and services filtering\n* 用户评分和评论 | User ratings and reviews\n* 特殊优惠查询 | Special offers query\n\n* 房型选择 | Room type selection\n\n### SearchAPI 新增功能 | Additional SearchAPI Features\n* SearchAPI Dashboard 与账号信息 | Dashboard and account management\n* 搜索历史记录查看 | View search history\n* 更多搜索引擎支持，如 Google Ads Transparency、Google Shopping、Google Images、Google News、Bing、Baidu、Naver、Yahoo、Amazon、Shein、eBay、Google Play Store、Apple App Store、DuckDuckGo、YouTube\n* 专用接口：Google Travel Explore、Google Hotels Autocomplete、Google Flights Location Search、Google Maps Photos、Google Maps Reviews、Google Maps Place\n\n## 安装说明 | Installation\n\n### 环境要求 | Requirements\n* Python 3.7 或更高版本 | Python 3.7 or higher\n* pip 包管理器 | pip package manager\n\n### 基础安装 | Basic Installation\n\n```bash\n# 克隆仓库 | Clone repository\ngit clone https://github.com/RmMargt/searchAPI-mcp.git\ncd searchAPI-mcp\n\n# 创建并激活虚拟环境 | Create and activate virtual environment\npython -m venv venv\nsource venv/bin/activate  # Linux/Mac\n# 或 | or\n.\\venv\\Scripts\\activate  # Windows\n\n# 安装依赖 | Install dependencies\npip install -r requirements.txt\n```\n\n## MCP 配置 | MCP Configuration\n\n### Claude for Desktop 配置示例 | Claude for Desktop Configuration Example\n\n在 Claude for Desktop 的配置文件中添加以下内容：\nAdd the following to your Claude for Desktop configuration file:\n\n```json\n{\n  \"mcpServers\": {\n    \"searchapi\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"run\",\n        \"--with\",\n        \"mcp[cli]\",\n        \"/path/to/searchAPI-mcp/mcp_server.py\"\n      ],\n      \"env\": {\n        \"SEARCHAPI_API_KEY\": \"your_api_key_here\"\n      }\n    }\n  }\n}\n```\n\n配置文件位置 | Configuration file location:\n* macOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\n* Windows: `%APPDATA%\\Claude\\claude_desktop_config.json`\n\n## 许可证 | License\n\n本项目采用 MIT 许可证 - 详见 LICENSE 文件\nThis project is licensed under the MIT License - see the LICENSE file for details\n\n## 致谢 | Acknowledgments\n\n* Model Context Protocol - 协议规范 | Protocol specification\n* FastMCP - Python MCP 实现 | Python MCP implementation\n* SearchAPI.io - 搜索服务提供商 | Search service provider\n\n---\n\n_注意：本服务器会与外部 API 进行交互。在使用 MCP 客户端确认操作之前，请始终验证请求的操作是否合适。_\n_Note: This server interacts with external APIs. Always verify that requested operations are appropriate before confirming them in MCP clients._ ",
      "npm_url": "https://www.npmjs.com/package/searchapi-mcp",
      "npm_downloads": 81,
      "keywords": [
        "searchapi",
        "search",
        "retrieval",
        "searchapi mcp",
        "rmmargt searchapi",
        "search services"
      ],
      "category": "web-search"
    },
    "RmMargt--searchapi-mcp-agent": {
      "owner": "RmMargt",
      "name": "searchapi-mcp-agent",
      "url": "https://github.com/RmMargt/searchapi-mcp-agent",
      "imageUrl": "/freedevtools/mcp/pfp/RmMargt.webp",
      "description": "Integrates multiple search APIs for maps, flights, hotels, and videos through a standardized interface, enabling complex search operations via natural language queries. Offers multilingual support and customizable filtering options for enhanced search results.",
      "stars": 11,
      "forks": 2,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-10-03T04:13:09Z",
      "readme_content": "# SearchAPI MCP Agent with A2A 支持 | SearchAPI MCP Agent with A2A Support\n\n一个基于 Agent-to-Agent (A2A) 协议的 SearchAPI 代理，通过 Model Context Protocol (MCP) 系统集成了多种搜索 API 工具。\n\nAn Agent-to-Agent (A2A) protocol based SearchAPI agent that integrates various search API tools through the Model Context Protocol (MCP) system.\n\n## 更新说明 | Update Notes\n\n**2024 更新**: \n- 修复了导入路径问题：从 samples.python.agents 导入修改为直接从当前目录导入\n- 修复了 a2a_common 导入问题：修改为从 common 模块导入\n- 移除了 a2a_common 依赖安装需求\n\n**2024 Updates**:\n- Fixed import path issues: Changed from samples.python.agents imports to direct imports from the current directory\n- Fixed a2a_common import issues: Changed to import from the common module\n- Removed a2a_common dependency installation requirements\n\n## 概述 | Overview\n\nSearchAPI-MCP-Agent 实现了 A2A 协议和 Model Context Protocol，将各种搜索操作封装为工具和资源。它作为 AI 助手和搜索服务之间的桥梁，支持地图搜索、航班查询、酒店预订等多种功能。\n\nSearchAPI-MCP-Agent implements the A2A protocol and Model Context Protocol, encapsulating various search operations as tools and resources. It serves as a bridge between AI assistants and search services, supporting map search, flight queries, hotel bookings, and more.\n\n## SearchAPI Agent 核心特性 | Core Features\n\n- **多MCP配置支持** - 作为MCP客户端，可以同时连接和配置多个MCP服务器，扩展可用的工具集\n  **Multiple MCP Configuration** - As an MCP client, can connect to and configure multiple MCP servers simultaneously, expanding the available toolset\n- **动态工具发现** - 自动发现和加载MCP服务器提供的工具列表，无需手动配置\n  **Dynamic Tool Discovery** - Automatically discovers and loads tool lists provided by MCP servers without manual configuration\n- **智能LLM路由** - 使用Gemini模型自动将自然语言查询路由到合适的工具并提取参数，确保调用成功\n  **Intelligent LLM Routing** - Uses Gemini model to automatically route natural language queries to appropriate tools and extract parameters, ensuring successful invocation\n- **实时状态反馈** - 通过A2A协议向Host Agent提供实时的工具执行状态更新和流式响应\n  **Real-time Status Feedback** - Provides real-time tool execution status updates and streaming responses to the Host Agent via A2A protocol\n- **错误处理和恢复** - 自动处理API调用错误，提供友好的错误信息和回退机制\n  **Error Handling and Recovery** - Automatically handles API call errors, providing friendly error messages and fallback mechanisms\n\n### Google 搜索 | Google Search\n* 网页搜索结果 | Web search results\n* 知识图谱集成 | Knowledge graph integration\n* 相关问题推荐 | Related questions\n* 搜索建议 | Search suggestions\n* 多语言支持 | Multi-language support\n* 地区特定结果 | Region-specific results\n* 时间范围过滤 | Time range filtering\n* 安全搜索选项 | Safe search options\n\n### Google Video 搜索 | Google Video Search\n* 视频内容搜索 | Video content search\n* 视频列表获取 | Video list retrieval\n* 视频轮播支持 | Video carousel support\n* 短视频内容 | Short video content\n* 按时长筛选 | Duration filtering\n* 按来源过滤 | Source filtering\n* 按上传时间排序 | Upload time sorting\n* 高清预览支持 | HD preview support\n\n### Google Maps 搜索 | Google Maps Search\n* 搜索地点和服务 | Search places and services\n* 获取地点详细信息 | Get place details\n* 查看用户评论 | View user reviews\n* 获取位置坐标 | Get location coordinates\n\n### Google Flights 航班搜索 | Google Flights Search\n* 单程/往返航班搜索 | One-way/round-trip flight search\n* 多城市行程规划 | Multi-city itinerary planning\n* 航班价格日历 | Flight price calendar\n* 航班筛选和排序 | Flight filtering and sorting\n* 行李额度查询 | Baggage allowance query\n* 航空公司选择 | Airline selection\n\n### Google Hotels 酒店搜索 | Google Hotels Search\n* 酒店位置搜索 | Hotel location search\n* 价格和可用性查询 | Price and availability query\n* 设施和服务筛选 | Facilities and services filtering\n* 用户评分和评论 | User ratings and reviews\n* 特殊优惠查询 | Special offers query\n* 房型选择 | Room type selection\n\n## 安装说明 | Installation\n\n### 环境要求 | Requirements\n* Python 3.9 或更高版本 | Python 3.9 or higher\n* pip 包管理器 | pip package manager\n* UV 包管理器（推荐）| UV package manager (recommended)\n\n### 基础安装 | Basic Installation\n\n```bash\n# 克隆仓库 | Clone repository\ngit clone https://github.com/RmMargt/searchapi-mcp-agent.git\ncd searchapi-mcp-agent\n\n# 创建并激活虚拟环境 | Create and activate virtual environment\npython -m venv venv\nsource venv/bin/activate  # Linux/Mac\n# 或 | or\n.\\venv\\Scripts\\activate  # Windows\n\n# 安装依赖 | Install dependencies\npip install -r requirements.txt\n```\n\n### 配置环境变量 | Configure Environment Variables\n\n创建 `.env` 文件并设置以下环境变量：\nCreate a `.env` file and set the following environment variables:\n\n```\nSEARCHAPI_API_KEY=your_searchapi_key_here\nGOOGLE_API_KEY=your_google_api_key_here\n```\n\n## 使用方法 | Usage\n\n### 启动 Google A2A 项目的 Host Agent 和 SearchAPI Agent\n\n按照以下步骤启动完整的 A2A 环境，包括 Host Agent 和 SearchAPI Agent:\n\n#### 1. 启动 SearchAPI Agent\n```bash\n# 在searchapi-mcp-agent目录下\npython -m searchapi_mcp_agent --host localhost --port 10001\n```\n\n#### 2. 启动 Host Agent (基于 Google A2A 项目)\n```bash\n# 切换到 Google A2A 样例目录\ncd path/to/A2A/samples/python\n\n# 运行 Host Agent (选择一种)\nuv run hosts/cli        # 命令行界面\n# 或\nuv run hosts/multiagent # 多代理环境\n```\n\n#### 3. 在本地浏览器中访问 Demo UI\n如果你运行的是多代理环境，可以在浏览器中访问以下地址:\n```\nhttp://localhost:12000\n```\n\n在 UI 中，点击机器人图标添加 SearchAPI Agent，使用以下地址:\n```\nhttp://localhost:10001/agent-card\n```\n\n### 直接发送请求 | Send Requests\n可以通过以下方式发送请求：\nYou can send requests in the following ways:\n\n1. **自然语言查询 | Natural Language Query**：\n   ```json\n   {\n     \"query\": \"查找从纽约到洛杉矶的航班\"\n   }\n   ```\n   Agent会使用LLM自动将查询路由到合适的工具。\n   The agent will use LLM to automatically route the query to the appropriate tool.\n\n2. **直接指定工具 | Direct Tool Specification**：\n   ```json\n   {\n     \"tool_name\": \"search_google_flights\",\n     \"parameters\": {\n       \"departure_id\": \"NYC\",\n       \"arrival_id\": \"LAX\",\n       \"outbound_date\": \"2024-12-01\"\n     }\n   }\n   ```\n\n## A2A 集成 | A2A Integration\n\n本项目已完全实现 A2A 协议，可以作为 AI 助手的服务端点。API 符合 A2A 规范，支持任务创建、状态查询和流式响应。\n\nThis project fully implements the A2A protocol and can serve as a service endpoint for AI assistants. The API complies with the A2A specification, supporting task creation, status queries, and streaming responses.\n\n### A2A 协议特性实现 | A2A Protocol Implementation\n\n- **动态工具路由** - 通过自然语言处理自动识别用户意图并选择合适的搜索工具\n  **Dynamic Tool Routing** - Automatically identifies user intent through natural language processing and selects the appropriate search tool\n- **流式响应** - 支持大型搜索结果的分块流式传输，提供实时反馈\n  **Streaming Responses** - Supports chunked streaming of large search results, providing real-time feedback\n- **任务状态更新** - 实时报告搜索任务的进度和状态变化\n  **Task Status Updates** - Reports progress and status changes of search tasks in real-time\n- **错误处理** - 优雅处理搜索API错误，提供有用的错误消息\n  **Error Handling** - Gracefully handles search API errors, providing useful error messages\n\n## MCP 配置 | MCP Configuration\n\n### Claude for Desktop 配置示例 | Claude for Desktop Configuration Example\n\n在 Claude for Desktop 的配置文件中添加以下内容：\nAdd the following to your Claude for Desktop configuration file:\n\n```json\n{\n  \"mcpServers\": {\n    \"searchapi\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"run\",\n        \"--with\",\n        \"mcp[cli]\",\n        \"/path/to/searchapi-mcp-agent/mcp_server.py\"\n      ],\n      \"env\": {\n        \"SEARCHAPI_API_KEY\": \"your_api_key_here\",\n        \"GOOGLE_API_KEY\": \"your_google_api_key_here\"\n      }\n    }\n  }\n}\n```\n\n配置文件位置 | Configuration file location:\n* macOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\n* Windows: `%APPDATA%\\Claude\\claude_desktop_config.json`\n\n## 许可证 | License\n\n本项目采用 MIT 许可证 - 详见 LICENSE 文件\nThis project is licensed under the MIT License - see the LICENSE file for details\n\n## 致谢 | Acknowledgments\n\n* Model Context Protocol - 协议规范 | Protocol specification\n* A2A Protocol - Agent-to-Agent 协议规范 | Agent-to-Agent protocol specification \n* FastMCP - Python MCP 实现 | Python MCP implementation\n* SearchAPI.io - 搜索服务提供商 | Search service provider\n* Google A2A - Agent-to-Agent 协议参考实现 | A2A protocol reference implementation\n\n---\n\n_注意：本服务器会与外部 API 进行交互。在使用 MCP 客户端确认操作之前，请始终验证请求的操作是否合适。_\n_Note: This server interacts with external APIs. Always verify that requested operations are appropriate before confirming them in MCP clients._ \n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "searchapi",
        "search",
        "filtering",
        "searchapi mcp",
        "rmmargt searchapi",
        "search apis"
      ],
      "category": "web-search"
    },
    "RossH121--perplexity-mcp": {
      "owner": "RossH121",
      "name": "perplexity-mcp",
      "url": "https://github.com/RossH121/perplexity-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/RossH121.webp",
      "description": "Utilizes web search capabilities to efficiently retrieve information while automatically selecting the most suitable AI model based on the user’s query intent. Offers intelligent options for domain and recency filtering to enhance search results.",
      "stars": 3,
      "forks": 3,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-09-07T19:26:52Z",
      "readme_content": "# Perplexity MCP Server\n\nAn MCP server that provides web search capabilities using Perplexity's API with automatic model selection based on query intent.\n\n<a href=\"https://glama.ai/mcp/servers/6qmvjay9z5\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/6qmvjay9z5/badge\" alt=\"Perplexity Server MCP server\" />\n</a>\n\n## Prerequisites\n\n- Node.js (v14 or higher)\n- A Perplexity API key (get one at <https://www.perplexity.ai/settings/api>)\n- Claude Desktop App\n\n## Installation\n\n### Installing via Git\n\n1. Clone this repository:\n\n    ```bash\n    git clone https://github.com/RossH121/perplexity-mcp.git\n    cd perplexity-mcp\n    ```\n\n2. Install dependencies:\n\n    ```bash\n    npm install\n    ```\n\n3. Build the server:\n\n    ```bash\n    npm run build\n    ```\n\n## Configuration\n\n1. Get your Perplexity API key from <https://www.perplexity.ai/settings/api>\n\n2. Add the server to Claude's config file at `~/Library/Application Support/Claude/claude_desktop_config.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"perplexity-server\": {\n      \"command\": \"node\",\n      \"args\": [\n        \"/absolute/path/to/perplexity-mcp/build/index.js\"\n      ],\n      \"env\": {\n        \"PERPLEXITY_API_KEY\": \"your-api-key-here\",\n        \"PERPLEXITY_MODEL\": \"sonar\"\n      }\n    }\n  }\n}\n```\n\nReplace `/absolute/path/to` with the actual path to where you cloned the repository.\n\n### Available Models\n\nThe server now supports automatic model selection based on query intent, but you can also specify a default model using the `PERPLEXITY_MODEL` environment variable. Available options:\n\n- `sonar-deep-research` - Specialized for extensive research and expert-level analysis across domains\n- `sonar-reasoning-pro` - Optimized for advanced logical reasoning and complex problem-solving\n- `sonar-reasoning` - Designed for reasoning tasks with balanced performance\n- `sonar-pro` - General-purpose model with excellent search capabilities and citation density\n- `sonar` - Fast and efficient for straightforward queries\n\nThe default model (specified in the environment variable) will be used as the baseline for automatic model selection.\n\nFor up-to-date model pricing and availability, visit: <https://docs.perplexity.ai/guides/pricing>\n\n## Usage\n\nAfter configuring the server and restarting Claude, you can simply ask Claude to search for information. For example:\n\n- \"What's the latest news about SpaceX?\"\n- \"Search for the best restaurants in Chicago\"\n- \"Find information about the history of jazz music\"\n- \"I need a deep research analysis of recent AI developments\" (uses sonar-deep-research)\n- \"Help me reason through this complex problem\" (uses sonar-reasoning-pro)\n\nClaude will automatically use the Perplexity search tool to find and return relevant information. The server will automatically select the most appropriate model based on your query's intent.\n\nIf for whatever reason it decides not to use the search tool, you can force the issue by prepending your prompt with \"Search the web\".\n\n### Intelligent Model Selection\n\nThe server automatically selects the most appropriate Perplexity model based on your query:\n\n- Use research-oriented terms like \"deep research,\" \"comprehensive,\" or \"in-depth\" to trigger sonar-deep-research\n- Use reasoning terms like \"solve,\" \"figure out,\" or \"complex problem\" to trigger sonar-reasoning-pro\n- Use simple terms like \"quick,\" \"brief,\" or \"basic\" to trigger the lightweight sonar model\n- General search terms default to sonar-pro for balanced performance\n\nEach search response includes information about which model was used and why.\n\n### Domain Filtering\n\nThis server supports domain filtering to customize your search experience. You can allow or block specific domains using these commands:\n\n- **Add an allowed domain**: \"Use the domain_filter tool to allow wikipedia.org\"\n- **Add a blocked domain**: \"Use the domain_filter tool to block pinterest.com\"\n- **View current filters**: \"Use the list_filters tool\" (shows domain and recency filters)\n- **Clear all filters**: \"Use the clear_filters tool\" (clears both domain and recency filters)\n\n**Note**: Perplexity API supports up to 3 domains total with priority given to allowed domains. Domain filtering requires a Perplexity API tier that supports this feature.\n\nExample usage flow:\n1. \"Use the domain_filter tool to allow wikipedia.org\"\n2. \"Use the domain_filter tool to allow arxiv.org\"\n3. \"Use the list_filters tool\" (to verify your settings)\n4. \"Search for quantum computing advances\" (results will prioritize wikipedia.org and arxiv.org)\n\n### Recency Filtering\n\nYou can limit search results to a specific time window using the recency filter:\n\n- **Set recency filter**: \"Use the recency_filter tool with filter=hour\" (options: hour, day, week, month)\n- **Disable recency filter**: \"Use the recency_filter tool with filter=none\"\n\nThis is particularly useful for time-sensitive queries like current events or breaking news.\n\n### Model Selection Control\n\nWhile the automatic model selection works well for most cases, you can manually control which model is used:\n\n- **View model information**: \"Use the model_info tool\"\n- **Set a specific model**: \"Use the model_info tool with model=sonar-deep-research\"\n- **Return to automatic selection**: Set the model back to the default model\n\nExample usage:\n1. \"Use the model_info tool\" (to see available models and current status)\n2. \"Use the model_info tool with model=sonar-reasoning-pro\" (to force using reasoning model)\n3. \"Search for a mathematical proof of the Pythagorean theorem\" (will use sonar-reasoning-pro)\n4. \"Use the model_info tool with model=sonar-pro\" (to return to automatic selection)\n\n## Development\n\nTo modify the server:\n\n1. Edit `src/index.ts`\n2. Rebuild with `npm run build`\n3. Restart Claude to load the changes\n\n## License\n\nMIT",
      "npm_url": "https://www.npmjs.com/package/perplexity-mcp",
      "npm_downloads": 8172,
      "keywords": [
        "search",
        "filtering",
        "mcp",
        "web search",
        "search capabilities",
        "search results"
      ],
      "category": "web-search"
    },
    "Rudra-ravi--wikipedia-mcp": {
      "owner": "Rudra-ravi",
      "name": "wikipedia-mcp",
      "url": "https://github.com/Rudra-ravi/wikipedia-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/Rudra-ravi.webp",
      "description": "Provides real-time access to Wikipedia content, enabling retrieval of articles, summaries, sections, and links in multiple languages for AI applications. Facilitates grounded responses by integrating reliable Wikipedia data into Large Language Models.",
      "stars": 114,
      "forks": 26,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-10-02T07:52:30Z",
      "readme_content": "# Wikipedia MCP Server\n\n[![smithery badge](https://smithery.ai/badge/@Rudra-ravi/wikipedia-mcp)](https://smithery.ai/server/@Rudra-ravi/wikipedia-mcp)\n\nA Model Context Protocol (MCP) server that retrieves information from Wikipedia to provide context to Large Language Models (LLMs). This tool helps AI assistants access factual information from Wikipedia to ground their responses in reliable sources.\n\n<a href=\"https://glama.ai/mcp/servers/@Rudra-ravi/wikipedia-mcp\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@Rudra-ravi/wikipedia-mcp/badge\" alt=\"Wikipedia Server MCP server\" />\n</a>\n\n![image](https://github.com/user-attachments/assets/e41382f7-111a-4105-97f3-7851c906843e)\n\n## Overview\n\nThe Wikipedia MCP server provides real-time access to Wikipedia information through a standardized Model Context Protocol interface. This allows LLMs to retrieve accurate and up-to-date information directly from Wikipedia to enhance their responses.\n\n## Verified By\n\n[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/rudra-ravi-wikipedia-mcp-badge.png)](https://mseep.ai/app/rudra-ravi-wikipedia-mcp)\n\n## Features\n\n- **Search Wikipedia**: Find articles matching specific queries with enhanced diagnostics\n- **Retrieve Article Content**: Get full article text with all information\n- **Article Summaries**: Get concise summaries of articles\n- **Section Extraction**: Retrieve specific sections from articles\n- **Link Discovery**: Find links within articles to related topics\n- **Related Topics**: Discover topics related to a specific article\n- **Multi-language Support**: Access Wikipedia in different languages by specifying the `--language` or `-l` argument when running the server (e.g., `wikipedia-mcp --language ta` for Tamil).\n- **Country/Locale Support**: Use intuitive country codes like `--country US`, `--country China`, or `--country TW` instead of language codes. Automatically maps to appropriate Wikipedia language variants.\n- **Language Variant Support**: Support for language variants such as Chinese traditional/simplified (e.g., `zh-hans` for Simplified Chinese, `zh-tw` for Traditional Chinese), Serbian scripts (`sr-latn`, `sr-cyrl`), and other regional variants.\n- **Optional caching**: Cache API responses for improved performance using --enable-cache\n- **Google ADK Compatibility**: Fully compatible with Google ADK agents and other AI frameworks that use strict function calling schemas\n\n## Installation\n\n### Using pipx (Recommended for Claude Desktop)\n\nThe best way to install for Claude Desktop usage is with pipx, which installs the command globally:\n\n```bash\n# Install pipx if you don't have it\npip install pipx\npipx ensurepath\n\n# Install the Wikipedia MCP server\npipx install wikipedia-mcp\n```\n\nThis ensures the `wikipedia-mcp` command is available in Claude Desktop's PATH.\n\n### Installing via Smithery\n\nTo install wikipedia-mcp for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@Rudra-ravi/wikipedia-mcp):\n\n```bash\nnpx -y @smithery/cli install @Rudra-ravi/wikipedia-mcp --client claude\n```\n\n### From PyPI (Alternative)\n\nYou can also install directly from PyPI:\n\n```bash\npip install wikipedia-mcp\n```\n\n**Note**: If you use this method and encounter connection issues with Claude Desktop, you may need to use the full path to the command in your configuration. See the [Configuration](#configuration-for-claude-desktop) section for details.\n\n### Using a virtual environment\n\n```bash\n# Create a virtual environment\npython3 -m venv venv\n\n# Activate the virtual environment\nsource venv/bin/activate\n\n# Install the package\npip install git+https://github.com/rudra-ravi/wikipedia-mcp.git\n```\n\n### From source\n\n```bash\n# Clone the repository\ngit clone https://github.com/rudra-ravi/wikipedia-mcp.git\ncd wikipedia-mcp\n\n# Create a virtual environment\npython3 -m venv wikipedia-mcp-env\nsource wikipedia-mcp-env/bin/activate\n\n# Install in development mode\npip install -e .\n```\n\n## Usage\n\n### Running the server\n\n```bash\n# If installed with pipx\nwikipedia-mcp\n\n# If installed in a virtual environment\nsource venv/bin/activate\nwikipedia-mcp\n\n# Specify transport protocol (default: stdio)\nwikipedia-mcp --transport stdio  # For Claude Desktop\nwikipedia-mcp --transport sse    # For HTTP streaming\n\n# Specify language (default: en for English)\nwikipedia-mcp --language ja  # Example for Japanese\nwikipedia-mcp --language zh-hans  # Example for Simplified Chinese\nwikipedia-mcp --language zh-tw    # Example for Traditional Chinese (Taiwan)\nwikipedia-mcp --language sr-latn  # Example for Serbian Latin script\n\n# Specify country/locale (alternative to language codes)\nwikipedia-mcp --country US        # English (United States)\nwikipedia-mcp --country China     # Chinese Simplified\nwikipedia-mcp --country Taiwan    # Chinese Traditional (Taiwan)  \nwikipedia-mcp --country Japan     # Japanese\nwikipedia-mcp --country Germany   # German\nwikipedia-mcp --country france    # French (case insensitive)\n\n# List all supported countries\nwikipedia-mcp --list-countries\n\n# Optional: Specify host/port for SSE (use 0.0.0.0 for containers)\nwikipedia-mcp --transport sse --host 0.0.0.0 --port 8080\n\n# Optional: Enable caching\nwikipedia-mcp --enable-cache\n\n# Optional: Use Personal Access Token to avoid rate limiting (403 errors)\nwikipedia-mcp --access-token your_wikipedia_token_here\n\n# Or set via environment variable\nexport WIKIPEDIA_ACCESS_TOKEN=your_wikipedia_token_here\nwikipedia-mcp\n\n# Security note for SSE: The transport does not define built-in endpoint authentication.\n# To restrict access, run the server behind an authenticating reverse proxy (e.g., Nginx/Traefik),\n# or expose it only on a private network/VPN and use firewall rules.\n\n# Combine options\nwikipedia-mcp --country Taiwan --enable-cache --access-token your_token --transport sse --port 8080\n\n### Docker/Kubernetes\n\nWhen running inside containers, bind the SSE server to all interfaces and map\nthe container port to the host or service:\n\n```bash\n# Build and run with Docker\ndocker build -t wikipedia-mcp .\ndocker run --rm -p 8080:8080 wikipedia-mcp --transport sse --host 0.0.0.0 --port 8080\n```\n\nKubernetes example (minimal):\n\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: wikipedia-mcp\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: wikipedia-mcp\n  template:\n    metadata:\n      labels:\n        app: wikipedia-mcp\n    spec:\n      containers:\n        - name: server\n          image: your-repo/wikipedia-mcp:latest\n          args: [\"--transport\", \"sse\", \"--host\", \"0.0.0.0\", \"--port\", \"8080\"]\n          ports:\n            - containerPort: 8080\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: wikipedia-mcp\nspec:\n  selector:\n    app: wikipedia-mcp\n  ports:\n    - name: http\n      port: 8080\n      targetPort: 8080\n```\n```\n\n### Configuration for Claude Desktop\n\nAdd the following to your Claude Desktop configuration file:\n\n**Option 1: Using command name (requires `wikipedia-mcp` to be in PATH)**\n```json\n{\n  \"mcpServers\": {\n    \"wikipedia\": {\n      \"command\": \"wikipedia-mcp\"\n    }\n  }\n}\n```\n\n**Option 2: Using full path (recommended if you get connection errors)**\n```json\n{\n  \"mcpServers\": {\n    \"wikipedia\": {\n      \"command\": \"/full/path/to/wikipedia-mcp\"\n    }\n  }\n}\n```\n\n**Option 3: With country/language specification**\n```json\n{\n  \"mcpServers\": {\n    \"wikipedia-us\": {\n      \"command\": \"wikipedia-mcp\",\n      \"args\": [\"--country\", \"US\"]\n    },\n    \"wikipedia-taiwan\": {\n      \"command\": \"wikipedia-mcp\", \n      \"args\": [\"--country\", \"TW\"]\n    },\n    \"wikipedia-japan\": {\n      \"command\": \"wikipedia-mcp\",\n      \"args\": [\"--country\", \"Japan\"]\n    }\n  }\n}\n```\n\nTo find the full path, run: `which wikipedia-mcp`\n\n**Configuration file locations:**\n- macOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\n- Windows: `%APPDATA%/Claude/claude_desktop_config.json`\n- Linux: `~/.config/Claude/claude_desktop_config.json`\n\n> **Note**: If you encounter connection errors, see the [Troubleshooting](#common-issues) section for solutions.\n\n## Available MCP Tools\n\nThe Wikipedia MCP server provides the following tools for LLMs to interact with Wikipedia:\n\n### `search_wikipedia`\n\nSearch Wikipedia for articles matching a query.\n\n**Parameters:**\n- `query` (string): The search term\n- `limit` (integer, optional): Maximum number of results to return (default: 10)\n\n**Returns:**\n- A list of search results with titles, snippets, and metadata\n\n### `get_article`\n\nGet the full content of a Wikipedia article.\n\n**Parameters:**\n- `title` (string): The title of the Wikipedia article\n\n**Returns:**\n- Article content including text, summary, sections, links, and categories\n\n### `get_summary`\n\nGet a concise summary of a Wikipedia article.\n\n**Parameters:**\n- `title` (string): The title of the Wikipedia article\n\n**Returns:**\n- A text summary of the article\n\n### `get_sections`\n\nGet the sections of a Wikipedia article.\n\n**Parameters:**\n- `title` (string): The title of the Wikipedia article\n\n**Returns:**\n- A structured list of article sections with their content\n\n### `get_links`\n\nGet the links contained within a Wikipedia article.\n\n**Parameters:**\n- `title` (string): The title of the Wikipedia article\n\n**Returns:**\n- A list of links to other Wikipedia articles\n\n### `get_coordinates`\n\nGet the coordinates of a Wikipedia article.\n\n**Parameters:**\n- `title` (string): The title of the Wikipedia article\n\n**Returns:**\n- A dictionary containing coordinate information including:\n  - `title`: The article title\n  - `pageid`: The page ID\n  - `coordinates`: List of coordinate objects with latitude, longitude, and metadata\n  - `exists`: Whether the article exists\n  - `error`: Any error message if retrieval failed\n\n### `get_related_topics`\n\nGet topics related to a Wikipedia article based on links and categories.\n\n**Parameters:**\n- `title` (string): The title of the Wikipedia article\n- `limit` (integer, optional): Maximum number of related topics (default: 10)\n\n**Returns:**\n- A list of related topics with relevance information\n\n### `summarize_article_for_query`\n\nGet a summary of a Wikipedia article tailored to a specific query.\n\n**Parameters:**\n- `title` (string): The title of the Wikipedia article\n- `query` (string): The query to focus the summary on\n- `max_length` (integer, optional): Maximum length of the summary (default: 250)\n\n**Returns:**\n- A dictionary containing the title, query, and the focused summary\n\n### `summarize_article_section`\n\nGet a summary of a specific section of a Wikipedia article.\n\n**Parameters:**\n- `title` (string): The title of the Wikipedia article\n- `section_title` (string): The title of the section to summarize\n- `max_length` (integer, optional): Maximum length of the summary (default: 150)\n\n**Returns:**\n- A dictionary containing the title, section title, and the section summary\n\n### `extract_key_facts`\n\nExtract key facts from a Wikipedia article, optionally focused on a specific topic within the article.\n\n**Parameters:**\n- `title` (string): The title of the Wikipedia article\n- `topic_within_article` (string, optional): A specific topic within the article to focus fact extraction\n- `count` (integer, optional): Number of key facts to extract (default: 5)\n\n**Returns:**\n- A dictionary containing the title, topic, and a list of extracted facts\n\n## Country/Locale Support\n\nThe Wikipedia MCP server supports intuitive country and region codes as an alternative to language codes. This makes it easier to access region-specific Wikipedia content without needing to know language codes.\n\n### Supported Countries and Regions\n\nUse `--list-countries` to see all supported countries:\n\n```bash\nwikipedia-mcp --list-countries\n```\n\nThis will display countries organized by language, for example:\n\n```\nSupported Country/Locale Codes:\n========================================\n    en: US, USA, United States, UK, GB, Canada, Australia, ...\n    zh-hans: CN, China\n    zh-tw: TW, Taiwan  \n    ja: JP, Japan\n    de: DE, Germany\n    fr: FR, France\n    es: ES, Spain, MX, Mexico, AR, Argentina, ...\n    pt: PT, Portugal, BR, Brazil\n    ru: RU, Russia\n    ar: SA, Saudi Arabia, AE, UAE, EG, Egypt, ...\n```\n\n### Usage Examples\n\n```bash\n# Major countries by code\nwikipedia-mcp --country US       # United States (English)\nwikipedia-mcp --country CN       # China (Simplified Chinese)\nwikipedia-mcp --country TW       # Taiwan (Traditional Chinese)\nwikipedia-mcp --country JP       # Japan (Japanese)\nwikipedia-mcp --country DE       # Germany (German)\nwikipedia-mcp --country FR       # France (French)\nwikipedia-mcp --country BR       # Brazil (Portuguese)\nwikipedia-mcp --country RU       # Russia (Russian)\n\n# Countries by full name (case insensitive)\nwikipedia-mcp --country \"United States\"\nwikipedia-mcp --country China\nwikipedia-mcp --country Taiwan  \nwikipedia-mcp --country Japan\nwikipedia-mcp --country Germany\nwikipedia-mcp --country france    # Case insensitive\n\n# Regional variants\nwikipedia-mcp --country HK       # Hong Kong (Traditional Chinese)\nwikipedia-mcp --country SG       # Singapore (Simplified Chinese)\nwikipedia-mcp --country \"Saudi Arabia\"  # Arabic\nwikipedia-mcp --country Mexico   # Spanish\n```\n\n### Country-to-Language Mapping\n\nThe server automatically maps country codes to appropriate Wikipedia language editions:\n\n- **English-speaking**: US, UK, Canada, Australia, New Zealand, Ireland, South Africa → `en`\n- **Chinese regions**: \n  - CN, China → `zh-hans` (Simplified Chinese)\n  - TW, Taiwan → `zh-tw` (Traditional Chinese - Taiwan)\n  - HK, Hong Kong → `zh-hk` (Traditional Chinese - Hong Kong)\n  - SG, Singapore → `zh-sg` (Simplified Chinese - Singapore)\n- **Major languages**: JP→`ja`, DE→`de`, FR→`fr`, ES→`es`, IT→`it`, RU→`ru`, etc.\n- **Regional variants**: Supports 140+ countries and regions\n\n### Error Handling\n\nIf you specify an unsupported country, you'll get a helpful error message:\n\n```bash\n$ wikipedia-mcp --country INVALID\nError: Unsupported country/locale: 'INVALID'. \nSupported country codes include: US, USA, UK, GB, CA, AU, NZ, IE, ZA, CN. \nUse --language parameter for direct language codes instead.\n\nUse --list-countries to see supported country codes.\n```\n\n## Language Variants\n\nThe Wikipedia MCP server supports language variants for languages that have multiple writing systems or regional variations. This feature is particularly useful for Chinese, Serbian, Kurdish, and other languages with multiple scripts or regional differences.\n\n### Supported Language Variants\n\n#### Chinese Language Variants\n- `zh-hans` - Simplified Chinese\n- `zh-hant` - Traditional Chinese  \n- `zh-tw` - Traditional Chinese (Taiwan)\n- `zh-hk` - Traditional Chinese (Hong Kong)\n- `zh-mo` - Traditional Chinese (Macau)\n- `zh-cn` - Simplified Chinese (China)\n- `zh-sg` - Simplified Chinese (Singapore)\n- `zh-my` - Simplified Chinese (Malaysia)\n\n#### Serbian Language Variants\n- `sr-latn` - Serbian Latin script\n- `sr-cyrl` - Serbian Cyrillic script\n\n#### Kurdish Language Variants\n- `ku-latn` - Kurdish Latin script\n- `ku-arab` - Kurdish Arabic script\n\n#### Norwegian Language Variants\n- `no` - Norwegian (automatically mapped to Bokmål)\n\n### Usage Examples\n\n```bash\n# Access Simplified Chinese Wikipedia\nwikipedia-mcp --language zh-hans\n\n# Access Traditional Chinese Wikipedia (Taiwan)\nwikipedia-mcp --language zh-tw\n\n# Access Serbian Wikipedia in Latin script\nwikipedia-mcp --language sr-latn\n\n# Access Serbian Wikipedia in Cyrillic script\nwikipedia-mcp --language sr-cyrl\n```\n\n### How Language Variants Work\n\nWhen you specify a language variant like `zh-hans`, the server:\n1. Maps the variant to the base Wikipedia language (e.g., `zh` for Chinese variants)\n2. Uses the base language for API connections to the Wikipedia servers\n3. Includes the variant parameter in API requests to get content in the specific variant\n4. Returns content formatted according to the specified variant's conventions\n\nThis approach ensures optimal compatibility with Wikipedia's API while providing access to variant-specific content and formatting.\n\n## Example Prompts\n\nOnce the server is running and configured with Claude Desktop, you can use prompts like:\n\n### General Wikipedia queries:\n- \"Tell me about quantum computing using the Wikipedia information.\"\n- \"Summarize the history of artificial intelligence based on Wikipedia.\"\n- \"What does Wikipedia say about climate change?\"\n- \"Find Wikipedia articles related to machine learning.\"\n- \"Get me the introduction section of the article on neural networks from Wikipedia.\"\n- \"What are the coordinates of the Eiffel Tower?\"\n- \"Find the latitude and longitude of Mount Everest from Wikipedia.\"\n- \"Get coordinate information for famous landmarks in Paris.\"\n\n### Using country-specific Wikipedia:\n- \"Search Wikipedia China for information about the Great Wall.\" (uses Chinese Wikipedia)\n- \"Tell me about Tokyo from Japanese Wikipedia sources.\"\n- \"What does German Wikipedia say about the Berlin Wall?\"\n- \"Find information about the Eiffel Tower from French Wikipedia.\"\n- \"Get Taiwan Wikipedia's article about Taiwanese cuisine.\"\n\n### Language variant examples:\n- \"Search Traditional Chinese Wikipedia for information about Taiwan.\"\n- \"Find Simplified Chinese articles about modern China.\"\n- \"Get information from Serbian Latin Wikipedia about Belgrade.\"\n\n## MCP Resources\n\nThe server also provides MCP resources (similar to HTTP endpoints but for MCP):\n\n- `search/{query}`: Search Wikipedia for articles matching the query\n- `article/{title}`: Get the full content of a Wikipedia article\n- `summary/{title}`: Get a summary of a Wikipedia article\n- `sections/{title}`: Get the sections of a Wikipedia article\n- `links/{title}`: Get the links in a Wikipedia article\n- `coordinates/{title}`: Get the coordinates of a Wikipedia article\n- `summary/{title}/query/{query}/length/{max_length}`: Get a query-focused summary of an article\n- `summary/{title}/section/{section_title}/length/{max_length}`: Get a summary of a specific article section\n- `facts/{title}/topic/{topic_within_article}/count/{count}`: Extract key facts from an article\n\n## Development\n\n### Local Development Setup\n\n```bash\n# Clone the repository\ngit clone https://github.com/rudra-ravi/wikipedia-mcp.git\ncd wikipedia-mcp\n\n# Create a virtual environment\npython3 -m venv venv\nsource venv/bin/activate\n\n# Install the package in development mode\npip install -e .\n\n# Install development and test dependencies\npip install -r requirements-dev.txt\n\n# Run the server\nwikipedia-mcp\n```\n\n### Project Structure\n\n- `wikipedia_mcp/`: Main package\n  - `__main__.py`: Entry point for the package\n  - `server.py`: MCP server implementation\n  - `wikipedia_client.py`: Wikipedia API client\n  - `api/`: API implementation\n  - `core/`: Core functionality\n  - `utils/`: Utility functions\n- `tests/`: Test suite\n  - `test_basic.py`: Basic package tests\n  - `test_cli.py`: Command-line interface tests\n  - `test_server_tools.py`: Comprehensive server and tool tests\n\n## Testing\n\nThe project includes a comprehensive test suite to ensure reliability and functionality.\n\n### Test Structure\n\nThe test suite is organized in the `tests/` directory with the following test files:\n\n- **`test_basic.py`**: Basic package functionality tests\n- **`test_cli.py`**: Command-line interface and transport tests\n- **`test_server_tools.py`**: Comprehensive tests for all MCP tools and Wikipedia client functionality\n\n### Running Tests\n\n#### Run All Tests\n```bash\n# Install test dependencies\npip install -r requirements-dev.txt\n\n# Run all tests\npython -m pytest tests/ -v\n\n# Run tests with coverage\npython -m pytest tests/ --cov=wikipedia_mcp --cov-report=html\n```\n\n#### Run Specific Test Categories\n```bash\n# Run only unit tests (excludes integration tests)\npython -m pytest tests/ -v -m \"not integration\"\n\n# Run only integration tests (requires internet connection)\npython -m pytest tests/ -v -m \"integration\"\n\n# Run specific test file\npython -m pytest tests/test_server_tools.py -v\n```\n\n### Test Categories\n\n#### Unit Tests\n- **WikipediaClient Tests**: Mock-based tests for all client methods\n  - Search functionality\n  - Article retrieval\n  - Summary extraction\n  - Section parsing\n  - Link extraction\n  - Related topics discovery\n- **Server Tests**: MCP server creation and tool registration\n- **CLI Tests**: Command-line interface functionality\n\n#### Integration Tests\n- **Real API Tests**: Tests that make actual calls to Wikipedia API\n- **End-to-End Tests**: Complete workflow testing\n\n### Test Configuration\n\nThe project uses `pytest.ini` for test configuration:\n\n```ini\n[pytest]\nmarkers =\n    integration: marks tests as integration tests (may require network access)\n    slow: marks tests as slow running\n\ntestpaths = tests\naddopts = -v --tb=short\n```\n\n### Continuous Integration\n\nAll tests are designed to:\n- Run reliably in CI/CD environments\n- Handle network failures gracefully\n- Provide clear error messages\n- Cover edge cases and error conditions\n\n### Adding New Tests\n\nWhen contributing new features:\n\n1. Add unit tests for new functionality\n2. Include both success and failure scenarios\n3. Mock external dependencies (Wikipedia API)\n4. Add integration tests for end-to-end validation\n5. Follow existing test patterns and naming conventions\n\n## Troubleshooting\n\n### Common Issues\n\n#### Claude Desktop Connection Issues\n\n**Problem**: Claude Desktop shows errors like `spawn wikipedia-mcp ENOENT` or cannot find the command.\n\n**Cause**: This occurs when the `wikipedia-mcp` command is installed in a user-specific location (like `~/.local/bin/`) that's not in Claude Desktop's PATH.\n\n**Solutions**:\n\n1. **Use full path to the command** (Recommended):\n   ```json\n   {\n     \"mcpServers\": {\n       \"wikipedia\": {\n         \"command\": \"/home/username/.local/bin/wikipedia-mcp\"\n       }\n     }\n   }\n   ```\n   \n   To find your exact path, run: `which wikipedia-mcp`\n\n2. **Install with pipx for global access**:\n   ```bash\n   pipx install wikipedia-mcp\n   ```\n   Then use the standard configuration:\n   ```json\n   {\n     \"mcpServers\": {\n       \"wikipedia\": {\n         \"command\": \"wikipedia-mcp\"\n       }\n     }\n   }\n   ```\n\n3. **Create a symlink to a global location**:\n   ```bash\n   sudo ln -s ~/.local/bin/wikipedia-mcp /usr/local/bin/wikipedia-mcp\n   ```\n\n#### Other Issues\n\n- **Article Not Found**: Check the exact spelling of article titles\n- **Rate Limiting**: Wikipedia API has rate limits; consider adding delays between requests\n- **Large Articles**: Some Wikipedia articles are very large and may exceed token limits\n\n## Troubleshooting Search Issues\n\nIf you're experiencing empty search results, use the new diagnostic tools:\n\n### 1. Test Connectivity\n\nUse the `test_wikipedia_connectivity` tool to check if you can reach Wikipedia's API:\n\n```json\n{\n  \"tool\": \"test_wikipedia_connectivity\"\n}\n```\n\nThis returns diagnostics including:\n- Connection status (`success` or `failed`)\n- Response time in milliseconds\n- Site/host information when successful\n- Error details when connectivity fails\n\n### 2. Enhanced Search Error Information\n\nThe `search_wikipedia` tool now returns detailed metadata:\n\n```json\n{\n  \"tool\": \"search_wikipedia\",\n  \"arguments\": {\n    \"query\": \"Ada Lovelace\",\n    \"limit\": 10\n  }\n}\n```\n\nExample response:\n\n```json\n{\n  \"query\": \"Ada Lovelace\",\n  \"results\": [...],\n  \"count\": 5,\n  \"status\": \"success\",\n  \"language\": \"en\"\n}\n```\n\nWhen no results are found, you receive:\n\n```json\n{\n  \"query\": \"nonexistent\",\n  \"results\": [],\n  \"status\": \"no_results\",\n  \"count\": 0,\n  \"language\": \"en\",\n  \"message\": \"No search results found. This could indicate connectivity issues, API errors, or simply no matching articles.\"\n}\n```\n\n### 3. Common Search Issues and Solutions\n\n- **Empty results**: Run the connectivity test, verify query spelling, try broader terms.\n- **Connection errors**: Check firewall or proxy settings, ensure `*.wikipedia.org` is reachable.\n- **API limits**: Requests with `limit > 500` are automatically capped; negative values reset to the default (10).\n\n### 4. Debugging with Verbose Logging\n\nLaunch the server with debug logging for deeper insight:\n\n```bash\nwikipedia-mcp --log-level DEBUG\n```\n\nThis emits the request parameters, response status codes, and any warnings returned by the API.\n\n## Understanding the Model Context Protocol (MCP)\n\nThe Model Context Protocol (MCP) is not a traditional HTTP API but a specialized protocol for communication between LLMs and external tools. Key characteristics:\n\n- Uses stdio (standard input/output) or SSE (Server-Sent Events) for communication\n- Designed specifically for AI model interaction\n- Provides standardized formats for tools, resources, and prompts\n- Integrates directly with Claude and other MCP-compatible AI systems\n\nClaude Desktop acts as the MCP client, while this server provides the tools and resources that Claude can use to access Wikipedia information.\n\n## Contributing\n\nContributions are welcome! Please see [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.\n\n## License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n\n## Connect with the Author\n\n- 🌐 Portfolio: [ravikumar-dev.me](https://ravikumar-dev.me)\n- 📝 Blog: [Medium](https://medium.com/@Ravikumar-e)\n- 💼 LinkedIn: [in/ravi-kumar-e](https://linkedin.com/in/ravi-kumar-e)\n- 🐦 Twitter: [@Ravikumar_d3v](https://twitter.com/Ravikumar_d3v) ",
      "npm_url": "https://www.npmjs.com/package/wikipedia-mcp",
      "npm_downloads": 4363,
      "keywords": [
        "wikipedia",
        "retrieval",
        "search",
        "retrieval articles",
        "wikipedia data",
        "wikipedia content"
      ],
      "category": "web-search"
    },
    "RusianHu--weibo_hotsearch_mcp": {
      "owner": "RusianHu",
      "name": "weibo_hotsearch_mcp",
      "url": "https://github.com/RusianHu/weibo_hotsearch_mcp",
      "imageUrl": "/freedevtools/mcp/pfp/RusianHu.webp",
      "description": "Provides real-time access to the top 10 trending topics on Weibo without authentication. Integrates trending data into AI workflows for analysis of popular social media topics.",
      "stars": 1,
      "forks": 1,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-02T23:02:01Z",
      "readme_content": "# 微博热搜 MCP 服务\n\n这是一个基于 [fastmcp](https://github.com/jlowin/fastmcp) 创建的微博热搜 MCP 服务，可以在 Claude 等支持 MCP 协议的 AI 助手中使用。\n\n## 功能\n\n- 获取微博热搜榜前10条内容\n- 无需提供 Cookie，使用微博移动版 API\n\n## 安装方法\n\n### 方法一：从 GitHub 安装\n\n```bash\n# 直接从 GitHub 安装\npip install git+https://github.com/RusianHu/weibo_hotsearch_mcp.git\n\n# 如果需要使用代理\npip install git+https://github.com/RusianHu/weibo_hotsearch_mcp.git --proxy socks5://127.0.0.1:10808\n```\n\n### 方法二：从源码安装\n\n```bash\n# 克隆仓库\ngit clone https://github.com/RusianHu/weibo_hotsearch_mcp.git\ncd weibo_hotsearch_mcp\n\n# 安装依赖\npip install -e .\n```\n\n### 方法三：从本地构建安装\n\n```bash\n# 构建包\npython build_package.py\n\n# 从本地安装\npip install dist/weibo_hotsearch_mcp-1.0.0-py3-none-any.whl\n```\n\n## 安装到 Claude Desktop\n\n推荐使用这种方式安装，它会创建一个隔离的环境，更加可靠：\n\n```bash\n# 安装到 Claude Desktop\nfastmcp install weibo_hotsearch_mcp.py\n```\n\n安装成功后，您可以直接在 Claude Desktop 中使用这个服务。\n\n## 在 Roo Code 插件中配置使用\n\n1. 安装 [Roo Code](https://marketplace.visualstudio.com/items?itemName=RooVeterinaryInc.roo-cline) VS Code 插件\n\n2. 打开 VS Code 设置，找到 `mcp_settings.json` 文件\n   - 路径通常为：`C:\\Users\\<用户名>\\AppData\\Roaming\\Code\\User\\globalStorage\\rooveterinaryinc.roo-cline\\settings\\mcp_settings.json`\n\n3. 在 `mcpServers` 对象中添加以下配置：\n\n```json\n\"weibo-hotsearch\": {\n  \"command\": \"python\",\n  \"args\": [\n    \"-m\", \"weibo_hotsearch_mcp\"\n  ],\n  \"alwaysAllow\": [\n    \"get_hot_search\"\n  ],\n  \"disabled\": false\n}\n```\n\n4. 保存文件后，重启 VS Code\n\n5. 现在你可以在 Claude 中使用微博热搜 MCP 服务了\n\n## 使用示例\n\n在 Claude 中，你可以这样使用微博热搜 MCP 服务：\n\n```\n请获取当前微博热搜榜的内容，并分析一下热门话题的类别分布。\n```\n\n## 开发\n\n如果你想修改或扩展这个 MCP 服务，可以按照以下步骤进行：\n\n1. 克隆仓库并安装依赖\n2. 修改 `weibo_hotsearch_mcp.py` 文件\n3. 使用 `fastmcp dev weibo_hotsearch_mcp.py` 命令测试你的修改\n4. 使用 `fastmcp install weibo_hotsearch_mcp.py` 命令安装到 Claude Desktop\n\n## 许可证\n\n[MIT](LICENSE)\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "weibo_hotsearch_mcp",
        "weibo",
        "trending",
        "topics weibo",
        "trending topics",
        "weibo_hotsearch_mcp provides"
      ],
      "category": "web-search"
    },
    "Sacode--searxng-simple-mcp": {
      "owner": "Sacode",
      "name": "searxng-simple-mcp",
      "url": "https://github.com/Sacode/searxng-simple-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/Sacode.webp",
      "description": "Provides web search capabilities using SearxNG, enabling AI models to efficiently search the web while maintaining user privacy. Integrates seamlessly with LLMs to deliver accurate search results without tracking.",
      "stars": 6,
      "forks": 3,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-25T19:39:17Z",
      "readme_content": "# SearxNG MCP Server\n\nA Model Context Protocol (MCP) server that provides web search capabilities using SearxNG, allowing AI assistants like Claude to search the web.\n\n> *Created by AI with human supervision - because sometimes even artificial intelligence needs someone to tell it when to take a coffee break! 🤖☕*\n\n## Overview\n\nThis project implements an MCP server that connects to SearxNG, a privacy-respecting metasearch engine. The server provides a simple and efficient way for Large Language Models to search the web without tracking users.\n\nThe server is specifically designed for LLMs and includes only essential features to minimize context window usage. This streamlined approach ensures efficient communication between LLMs and the search engine, preserving valuable context space for more important information.\n\n### Features\n\n- Privacy-focused web search through SearxNG\n- Simple API for LLM integration\n- Compatible with Claude Desktop and other MCP-compliant clients\n- Configurable search parameters\n- Clean, formatted search results optimized for LLMs\n\n## Integration with MCP-Compatible Applications\n\n### Integration Examples\n\n#### Using pipx run (Recommended, no installation required)\n\nCreate a `.clauderc` file in your home directory:\n\n```json\n{\n  \"mcpServers\": {\n    \"searxng\": {\n      \"command\": \"pipx\",\n      \"args\": [\n        \"run\", \"searxng-simple-mcp@latest\"\n      ],\n      \"env\": {\n        \"SEARXNG_MCP_SEARXNG_URL\": \"https://your-instance.example.com\"\n      }\n    }\n  }\n}\n```\n\n#### Using uvx run (No installation required)\n\n```json\n{\n  \"mcpServers\": {\n    \"searxng\": {\n      \"command\": \"uvx\",\n      \"args\": [\n        \"run\", \"searxng-simple-mcp@latest\"\n      ],\n      \"env\": {\n        \"SEARXNG_MCP_SEARXNG_URL\": \"https://your-instance.example.com\"\n      }\n    }\n  }\n}\n```\n\n#### Using Python with pip (requires installation)\n\n```json\n{\n  \"mcpServers\": {\n    \"searxng\": {\n      \"command\": \"python\",\n      \"args\": [\"-m\", \"searxng_simple_mcp.server\"],\n      \"env\": {\n        \"SEARXNG_MCP_SEARXNG_URL\": \"https://your-instance.example.com\"\n      }\n    }\n  }\n}\n```\n\n#### Using with Docker (No installation required)\n\n```json\n{\n  \"mcpServers\": {\n    \"searxng\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\", \"--rm\", \"-i\", \"--network=host\",\n        \"-e\", \"SEARXNG_MCP_SEARXNG_URL=http://localhost:8080\",\n        \"ghcr.io/sacode/searxng-simple-mcp:latest\"\n      ]\n    }\n  }\n}\n```\n\n**Note:** When using Docker with MCP servers:\n\n1. Environment variables must be passed directly using the `-e` flag in the `args` array, as the `env` object is not properly passed to the Docker container.\n2. If you need to access a SearxNG instance running on localhost (e.g., <http://localhost:8080>), you must use the `--network=host` flag to allow the container to access the host's network. Otherwise, \"localhost\" inside the container will refer to the container itself, not your host machine.\n3. When using `--network=host`, port mappings (`-p`) are not needed and will be ignored, as the container shares the host's network stack directly.\n\n## Configuration\n\nConfigure the server using environment variables:\n\n| Environment Variable | Description | Default Value |\n|----------------------|-------------|---------------|\n| SEARXNG_MCP_SEARXNG_URL | URL of the SearxNG instance to use | <https://paulgo.io/> |\n| SEARXNG_MCP_TIMEOUT | HTTP request timeout in seconds | 10 |\n| SEARXNG_MCP_DEFAULT_RESULT_COUNT | Default number of results to return | 10 |\n| SEARXNG_MCP_DEFAULT_LANGUAGE | Language code for results (e.g., 'en', 'ru', 'all') | all |\n| SEARXNG_MCP_DEFAULT_FORMAT | Default format for results ('text', 'json') | text |\n| SEARXNG_MCP_LOG_LEVEL | Logging level (e.g., 'DEBUG', 'INFO', 'WARNING', 'ERROR', 'CRITICAL') | ERROR |\n| TRANSPORT_PROTOCOL | Transport protocol ('stdio' or 'sse') | stdio |\n\n**Note:** Setting log levels higher than ERROR (such as DEBUG or INFO) may break integration with some applications due to excessive output in the communication channel.\n\nYou can find a list of public SearxNG instances at [https://searx.space](https://searx.space) if you don't want to host your own.\n\n## Installation & Usage\n\n### Prerequisites\n\n- Python 3.10 or higher\n- A SearxNG instance (public or self-hosted)\n\n### Option 1: Run Without Installation (Recommended)\n\nThe easiest way to use this server is with pipx or uvx, which allows you to run the package without installing it permanently:\n\n```bash\n# Using pipx\npip install pipx  # Install pipx if you don't have it\npipx run searxng-simple-mcp\n\n# OR using uvx\npip install uvx  # Install uvx if you don't have it\nuvx run searxng-simple-mcp\n```\n\nYou can pass configuration options directly:\n\n```bash\n# Using pipx with custom SearxNG instance\npipx run searxng-simple-mcp --searxng-url https://your-instance.example.com\n```\n\n### Option 2: Install from PyPI or Source\n\nFor more permanent installation:\n\n```bash\n# From PyPI using pip\npip install searxng-simple-mcp\n\n# OR using uv (faster installation)\npip install uv\nuv pip install searxng-simple-mcp\n\n# OR from source\ngit clone https://github.com/Sacode/searxng-simple-mcp.git\ncd searxng-simple-mcp\npip install uv\nuv pip install -e .\n```\n\nAfter installation, you can run the server with:\n\n```bash\n# Run directly after installation\npython -m searxng_simple_mcp.server\n\n# OR with configuration options\npython -m searxng_simple_mcp.server --searxng-url https://your-instance.example.com\n```\n\n### Option 3: Docker\n\nIf you prefer using Docker:\n\n```bash\n# Pull the Docker image\ndocker pull ghcr.io/sacode/searxng-simple-mcp:latest\n\n# Run the container with default settings (stdio transport)\ndocker run --rm -i ghcr.io/sacode/searxng-simple-mcp:latest\n\n# Run with environment file for configuration\ndocker run --rm -i --env-file .env ghcr.io/sacode/searxng-simple-mcp:latest\n\n# Run with SSE transport (starts HTTP server on port 8000)\ndocker run -p 8000:8000 -e TRANSPORT_PROTOCOL=sse ghcr.io/sacode/searxng-simple-mcp:latest\n\n# Building locally\ndocker build -t searxng-simple-mcp:local .\ndocker run --rm -i searxng-simple-mcp:local\n\n# Using Docker Compose\ndocker-compose up -d\n```\n\nFor complete Docker usage information, see the [Docker Configuration](#docker-configuration) section below.\n\n## Transport Protocols\n\nThe MCP server supports two transport protocols:\n\n- **STDIO** (default): For CLI applications and direct integration\n  - Used by default in all examples\n  - Suitable for integration with Claude Desktop and other MCP-compliant clients\n  - No HTTP server is started\n\n- **SSE** (Server-Sent Events): For web-based clients and HTTP-based integrations\n  - Starts an HTTP server that clients can connect to\n  - Useful for web applications and services that need real-time updates\n  - Requires port mapping when using Docker\n\n### Using SSE Transport\n\nTo use the SSE transport protocol:\n\n1. **With direct execution**:\n\n   ```bash\n   # Set the transport protocol to SSE\n   TRANSPORT_PROTOCOL=sse python -m searxng_simple_mcp.server\n   \n   # Or with FastMCP\n   fastmcp run src/searxng_simple_mcp/server.py --transport sse\n   ```\n\n2. **With Docker**:\n\n   ```bash\n   # Run with SSE transport protocol\n   docker run -p 8000:8000 -e TRANSPORT_PROTOCOL=sse -e SEARXNG_MCP_SEARXNG_URL=https://your-instance.example.com ghcr.io/sacode/searxng-simple-mcp:latest\n   ```\n\n3. **With Docker Compose** (from the included `docker-compose.yml`):\n\n   ```yaml\n   environment:\n     - SEARXNG_MCP_SEARXNG_URL=https://searx.info\n     - SEARXNG_MCP_TIMEOUT=10\n     - SEARXNG_MCP_MAX_RESULTS=20\n     - SEARXNG_MCP_LANGUAGE=all\n     - TRANSPORT_PROTOCOL=sse # Transport protocol: stdio or sse\n   ```\n\nWhen using SSE, the server will be accessible via HTTP at `http://localhost:8000` by default.\n\nTo connect to the SSE server from an MCP client, use a configuration like:\n\n```json\n{\n  \"mcpServers\": {\n    \"searxng\": {\n      \"url\": \"http://localhost:8000\",\n      \"transport\": \"sse\"\n    }\n  }\n}\n```\n\n**Note:** Not all applications support the SSE transport protocol. Make sure your MCP client is compatible with SSE before using this transport method.\n\n## Development\n\nFor development and testing:\n\n```bash\n# Install dependencies\nuv pip install -e .\n\n# Run linter and formatter\nruff check .\nruff check --fix .\nruff format .\n\n# Run the server directly\npython -m src.searxng_simple_mcp.server\n\n# OR using FastMCP\nfastmcp run src/searxng_simple_mcp/server.py  # Use stdio transport (default)\nfastmcp run src/searxng_simple_mcp/server.py --transport sse  # Use sse transport\n\n# Run in development mode (launches MCP Inspector)\nfastmcp dev src/searxng_simple_mcp/server.py\n```\n\n## Publishing to PyPI\n\nFor maintainers who need to publish new versions of the package to PyPI:\n\n```bash\n# Install development dependencies\nnpm run install:deps\n\n# Clean, build, and check the package\nnpm run build:package\nnpm run check:package\n\n# Publish to PyPI (requires PyPI credentials)\nnpm run publish:pypi\n\n# Alternatively, use the all-in-one commands to update version and publish\nnpm run publish:patch  # Increments patch version (1.0.1 -> 1.0.2)\nnpm run publish:minor  # Increments minor version (1.0.1 -> 1.1.0)\nnpm run publish:major  # Increments major version (1.0.1 -> 2.0.0)\n```\n\nThese commands will:\n\n1. Update the version in both package.json and pyproject.toml\n2. Clean the dist directory to remove old builds\n3. Build the package (creating wheel and source distribution)\n4. Check the package for errors\n5. Upload the package to PyPI\n\nYou'll need to have a PyPI account and be authenticated with twine. You can set up authentication by:\n\n- Creating a `.pypirc` file in your home directory\n- Using environment variables (`TWINE_USERNAME` and `TWINE_PASSWORD`)\n- Using PyPI API tokens (recommended)\n\n## Docker Configuration\n\nWhen using Docker with MCP servers, keep these points in mind:\n\n1. **Integration with MCP clients**: Use the configuration shown in the [Using with Docker](#using-with-docker-no-installation-required) section for integrating with Claude Desktop or other MCP-compliant clients.\n\n2. **Transport protocols**:\n   - By default, the Docker container uses the stdio transport protocol\n   - For SSE transport, see the [Using SSE Transport](#using-sse-transport) section\n\n3. **Configuration options**:\n   - Use an environment file (.env) to configure the server: `docker run --env-file .env ...`\n   - Pass individual environment variables with the `-e` flag: `docker run -e SEARXNG_MCP_SEARXNG_URL=https://example.com ...`\n   - See the [Configuration](#configuration) section for available environment variables\n\n4. **Networking**:\n   - Use `--network=host` when you need to access services on your host machine\n   - Use `-p 8000:8000` when exposing the SSE server to your network\n\n## Package Structure\n\n```\nsearxng-simple-mcp/\n├── src/\n│   ├── run_server.py         # Entry point script\n│   └── searxng_simple_mcp/   # Main package\n├── docker-compose.yml        # Docker Compose configuration\n├── Dockerfile                # Docker configuration\n└── pyproject.toml            # Python project configuration\n```\n\n## Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n\n## License\n\nThis project is licensed under the MIT License - see the LICENSE file for details.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "searxng",
        "search",
        "web",
        "search web",
        "searxng simple",
        "web search"
      ],
      "category": "web-search"
    },
    "Saik0s--mcp-browser-use": {
      "owner": "Saik0s",
      "name": "mcp-browser-use",
      "url": "https://github.com/Saik0s/mcp-browser-use",
      "imageUrl": "/freedevtools/mcp/pfp/Saik0s.webp",
      "description": "Automates interactions with web browsers for tasks in testing and development, enabling natural language control and web research capabilities.",
      "stars": 819,
      "forks": 103,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-10-03T03:55:03Z",
      "readme_content": "<br/>\n\n# browser-use MCP server & CLI\n[![Documentation](https://img.shields.io/badge/Documentation-📕-blue)](https://docs.browser-use.com)\n[![License](https://img.shields.io/badge/License-MIT-green)](LICENSE)\n\n> **Project Note**: This MCP server implementation builds upon the [browser-use/web-ui](https://github.com/browser-use/web-ui) foundation. Core browser automation logic and configuration patterns are adapted from the original project.\n\nAI-driven browser automation server implementing the Model Context Protocol (MCP) for natural language browser control and web research. Also provides CLI access to its core functionalities.\n\n<a href=\"https://glama.ai/mcp/servers/@Saik0s/mcp-browser-use\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@Saik0s/mcp-browser-use/badge\" alt=\"Browser-Use MCP server\" /></a>\n\n## Features\n\n-   🧠 **MCP Integration** - Full protocol implementation for AI agent communication.\n-   🌐 **Browser Automation** - Page navigation, form filling, element interaction via natural language (`run_browser_agent` tool).\n-   👁️ **Visual Understanding** - Optional screenshot analysis for vision-capable LLMs.\n-   🔄 **State Persistence** - Option to manage a server browser session across multiple MCP calls or connect to user's browser.\n-   🔌 **Multi-LLM Support** - Integrates with OpenAI, Anthropic, Azure, DeepSeek, Google, Mistral, Ollama, OpenRouter, Alibaba, Moonshot, Unbound AI.\n-   🔍 **Deep Research Tool** - Dedicated tool for multi-step web research and report generation (`run_deep_research` tool).\n-   ⚙️ **Environment Variable Configuration** - Fully configurable via environment variables using a structured Pydantic model.\n-   🔗 **CDP Connection** - Ability to connect to and control a user-launched Chrome/Chromium instance via Chrome DevTools Protocol.\n-   ⌨️ **CLI Interface** - Access core agent functionalities (`run_browser_agent`, `run_deep_research`) directly from the command line for testing and scripting.\n\n## Quick Start\n\n### The Essentials\n\n1. Install UV - the rocket-powered Python installer:\n`curl -LsSf https://astral.sh/uv/install.sh | sh`\n\n2. Get Playwright browsers (required for automation):\n`uvx --from mcp-server-browser-use@latest python -m playwright install`\n\n### Integration Patterns\n\nFor MCP clients like Claude Desktop, add a server configuration that's as simple as:\n\n```json\n// Example 1: One-Line Latest Version (Always Fresh)\n\"mcpServers\": {\n    \"browser-use\": {\n      \"command\": \"uvx\",\n      \"args\": [\"mcp-server-browser-use@latest\"],\n      \"env\": {\n        \"MCP_LLM_GOOGLE_API_KEY\": \"YOUR_KEY_HERE_IF_USING_GOOGLE\",\n        \"MCP_LLM_PROVIDER\": \"google\",\n        \"MCP_LLM_MODEL_NAME\": \"gemini-2.5-flash-preview-04-17\",\n        \"MCP_BROWSER_HEADLESS\": \"true\",\n      }\n    }\n}\n```\n\n```json\n// Example 2: Advanced Configuration with CDP\n\"mcpServers\": {\n    \"browser-use\": {\n      \"command\": \"uvx\",\n      \"args\": [\"mcp-server-browser-use@latest\"],\n      \"env\": {\n        \"MCP_LLM_OPENROUTER_API_KEY\": \"YOUR_KEY_HERE_IF_USING_OPENROUTER\",\n        \"MCP_LLM_PROVIDER\": \"openrouter\",\n        \"MCP_LLM_MODEL_NAME\": \"anthropic/claude-3.5-haiku\",\n        \"MCP_LLM_TEMPERATURE\": \"0.4\",\n\n        \"MCP_BROWSER_HEADLESS\": \"false\",\n        \"MCP_BROWSER_WINDOW_WIDTH\": \"1440\",\n        \"MCP_BROWSER_WINDOW_HEIGHT\": \"1080\",\n        \"MCP_AGENT_TOOL_USE_VISION\": \"true\",\n\n        \"MCP_RESEARCH_TOOL_SAVE_DIR\": \"/path/to/your/research\",\n        \"MCP_RESEARCH_TOOL_MAX_PARALLEL_BROWSERS\": \"5\",\n\n        \"MCP_PATHS_DOWNLOADS\": \"/path/to/your/downloads\",\n\n        \"MCP_BROWSER_USE_OWN_BROWSER\": \"true\",\n        \"MCP_BROWSER_CDP_URL\": \"http://localhost:9222\",\n\n        \"MCP_AGENT_TOOL_HISTORY_PATH\": \"/path/to/your/history\",\n\n        \"MCP_SERVER_LOGGING_LEVEL\": \"DEBUG\",\n        \"MCP_SERVER_LOG_FILE\": \"/path/to/your/log/mcp_server_browser_use.log\",\n      }\n    }\n}\n```\n\n```json\n// Example 3: Advanced Configuration with User Data and custom chrome path\n\"mcpServers\": {\n    \"browser-use\": {\n      \"command\": \"uvx\",\n      \"args\": [\"mcp-server-browser-use@latest\"],\n      \"env\": {\n        \"MCP_LLM_OPENAI_API_KEY\": \"YOUR_KEY_HERE_IF_USING_OPENAI\",\n        \"MCP_LLM_PROVIDER\": \"openai\",\n        \"MCP_LLM_MODEL_NAME\": \"gpt-4.1-mini\",\n        \"MCP_LLM_TEMPERATURE\": \"0.2\",\n\n        \"MCP_BROWSER_HEADLESS\": \"false\",\n\n        \"MCP_BROWSER_BINARY_PATH\": \"/path/to/your/chrome/binary\",\n        \"MCP_BROWSER_USER_DATA_DIR\": \"/path/to/your/user/data\",\n        \"MCP_BROWSER_DISABLE_SECURITY\": \"true\",\n        \"MCP_BROWSER_KEEP_OPEN\": \"true\",\n        \"MCP_BROWSER_TRACE_PATH\": \"/path/to/your/trace\",\n\n        \"MCP_AGENT_TOOL_HISTORY_PATH\": \"/path/to/your/history\",\n\n        \"MCP_SERVER_LOGGING_LEVEL\": \"DEBUG\",\n        \"MCP_SERVER_LOG_FILE\": \"/path/to/your/log/mcp_server_browser_use.log\",\n      }\n    }\n}\n```\n\n```json\n// Example 4: Local Development Flow\n\"mcpServers\": {\n    \"browser-use\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"--directory\",\n        \"/your/dev/path\",\n        \"run\",\n        \"mcp-server-browser-use\"\n      ],\n      \"env\": {\n        \"MCP_LLM_OPENROUTER_API_KEY\": \"YOUR_KEY_HERE_IF_USING_OPENROUTER\",\n        \"MCP_LLM_PROVIDER\": \"openrouter\",\n        \"MCP_LLM_MODEL_NAME\": \"openai/gpt-4o-mini\",\n        \"MCP_BROWSER_HEADLESS\": \"true\",\n      }\n    }\n}\n```\n\n**Key Insight:** The best configurations emerge from starting simple (Example 1). The .env.example file contains all possible dials.\n\n## MCP Tools\n\nThis server exposes the following tools via the Model Context Protocol:\n\n### Synchronous Tools (Wait for Completion)\n\n1.  **`run_browser_agent`**\n    *   **Description:** Executes a browser automation task based on natural language instructions and waits for it to complete. Uses settings from `MCP_AGENT_TOOL_*`, `MCP_LLM_*`, and `MCP_BROWSER_*` environment variables.\n    *   **Arguments:**\n        *   `task` (string, required): The primary task or objective.\n    *   **Returns:** (string) The final result extracted by the agent or an error message. Agent history (JSON, optional GIF) saved if `MCP_AGENT_TOOL_HISTORY_PATH` is set.\n\n2.  **`run_deep_research`**\n    *   **Description:** Performs in-depth web research on a topic, generates a report, and waits for completion. Uses settings from `MCP_RESEARCH_TOOL_*`, `MCP_LLM_*`, and `MCP_BROWSER_*` environment variables. If `MCP_RESEARCH_TOOL_SAVE_DIR` is set, outputs are saved to a subdirectory within it; otherwise, operates in memory-only mode.\n    *   **Arguments:**\n        *   `research_task` (string, required): The topic or question for the research.\n        *   `max_parallel_browsers` (integer, optional): Overrides `MCP_RESEARCH_TOOL_MAX_PARALLEL_BROWSERS` from environment.\n    *   **Returns:** (string) The generated research report in Markdown format, including the file path (if saved), or an error message.\n\n## CLI Usage\n\nThis package also provides a command-line interface `mcp-browser-cli` for direct testing and scripting.\n\n**Global Options:**\n*   `--env-file PATH, -e PATH`: Path to a `.env` file to load configurations from.\n*   `--log-level LEVEL, -l LEVEL`: Override the logging level (e.g., `DEBUG`, `INFO`).\n\n**Commands:**\n\n1.  **`mcp-browser-cli run-browser-agent [OPTIONS] TASK`**\n    *   **Description:** Runs a browser agent task.\n    *   **Arguments:**\n        *   `TASK` (string, required): The primary task for the agent.\n    *   **Example:**\n        ```bash\n        mcp-browser-cli run-browser-agent \"Go to example.com and find the title.\" -e .env\n        ```\n\n2.  **`mcp-browser-cli run-deep-research [OPTIONS] RESEARCH_TASK`**\n    *   **Description:** Performs deep web research.\n    *   **Arguments:**\n        *   `RESEARCH_TASK` (string, required): The topic or question for research.\n    *   **Options:**\n        *   `--max-parallel-browsers INTEGER, -p INTEGER`: Override `MCP_RESEARCH_TOOL_MAX_PARALLEL_BROWSERS`.\n    *   **Example:**\n        ```bash\n        mcp-browser-cli run-deep-research \"What are the latest advancements in AI-driven browser automation?\" --max-parallel-browsers 5 -e .env\n        ```\n\nAll other configurations (LLM keys, paths, browser settings) are picked up from environment variables (or the specified `.env` file) as detailed in the Configuration section.\n\n## Configuration (Environment Variables)\n\nConfigure the server and CLI using environment variables. You can set these in your system or place them in a `.env` file in the project root (use `--env-file` for CLI). Variables are structured with prefixes.\n\n| Variable Group (Prefix)             | Example Variable                               | Description                                                                                                | Default Value                     |\n| :---------------------------------- | :--------------------------------------------- | :--------------------------------------------------------------------------------------------------------- | :-------------------------------- |\n| **Main LLM (MCP_LLM_)**             |                                                | Settings for the primary LLM used by agents.                                                               |                                   |\n|                                     | `MCP_LLM_PROVIDER`                             | LLM provider. Options: `openai`, `azure_openai`, `anthropic`, `google`, `mistral`, `ollama`, etc.         | `openai`                          |\n|                                     | `MCP_LLM_MODEL_NAME`                           | Specific model name for the provider.                                                                      | `gpt-4.1`                         |\n|                                     | `MCP_LLM_TEMPERATURE`                          | LLM temperature (0.0-2.0).                                                                                 | `0.0`                             |\n|                                     | `MCP_LLM_BASE_URL`                             | Optional: Generic override for LLM provider's base URL.                                                    | Provider-specific                 |\n|                                     | `MCP_LLM_API_KEY`                              | Optional: Generic LLM API key (takes precedence).                                                          | -                                 |\n|                                     | `MCP_LLM_OPENAI_API_KEY`                       | API Key for OpenAI (if provider is `openai`).                                                              | -                                 |\n|                                     | `MCP_LLM_ANTHROPIC_API_KEY`                    | API Key for Anthropic.                                                                                     | -                                 |\n|                                     | `MCP_LLM_GOOGLE_API_KEY`                       | API Key for Google AI (Gemini).                                                                            | -                                 |\n|                                     | `MCP_LLM_AZURE_OPENAI_API_KEY`                 | API Key for Azure OpenAI.                                                                                  | -                                 |\n|                                     | `MCP_LLM_AZURE_OPENAI_ENDPOINT`                | **Required if using Azure.** Your Azure resource endpoint.                                                 | -                                 |\n|                                     | `MCP_LLM_OLLAMA_ENDPOINT`                      | Ollama API endpoint URL.                                                                                   | `http://localhost:11434`          |\n|                                     | `MCP_LLM_OLLAMA_NUM_CTX`                       | Context window size for Ollama models.                                                                     | `32000`                           |\n| **Planner LLM (MCP_LLM_PLANNER_)**  |                                                | Optional: Settings for a separate LLM for agent planning. Defaults to Main LLM if not set.                |                                   |\n|                                     | `MCP_LLM_PLANNER_PROVIDER`                     | Planner LLM provider.                                                                                      | Main LLM Provider                 |\n|                                     | `MCP_LLM_PLANNER_MODEL_NAME`                   | Planner LLM model name.                                                                                    | Main LLM Model                    |\n| **Browser (MCP_BROWSER_)**          |                                                | General browser settings.                                                                                  |                                   |\n|                                     | `MCP_BROWSER_HEADLESS`                         | Run browser without UI (general setting).                                                                  | `false`                           |\n|                                     | `MCP_BROWSER_DISABLE_SECURITY`                 | Disable browser security features (general setting, use cautiously).                                       | `false`                           |\n|                                     | `MCP_BROWSER_BINARY_PATH`                      | Path to Chrome/Chromium executable.                                                                        | -                                 |\n|                                     | `MCP_BROWSER_USER_DATA_DIR`                    | Path to Chrome user data directory.                                                                        | -                                 |\n|                                     | `MCP_BROWSER_WINDOW_WIDTH`                     | Browser window width (pixels).                                                                             | `1280`                            |\n|                                     | `MCP_BROWSER_WINDOW_HEIGHT`                    | Browser window height (pixels).                                                                            | `1080`                            |\n|                                     | `MCP_BROWSER_USE_OWN_BROWSER`                  | Connect to user's browser via CDP URL.                                                                     | `false`                           |\n|                                     | `MCP_BROWSER_CDP_URL`                          | CDP URL (e.g., `http://localhost:9222`). Required if `MCP_BROWSER_USE_OWN_BROWSER=true`.                  | -                                 |\n|                                     | `MCP_BROWSER_KEEP_OPEN`                        | Keep server-managed browser open between MCP calls (if `MCP_BROWSER_USE_OWN_BROWSER=false`).               | `false`                           |\n|                                     | `MCP_BROWSER_TRACE_PATH`                       | Optional: Directory to save Playwright trace files. If not set, tracing to file is disabled.               | ` ` (empty, tracing disabled)     |\n| **Agent Tool (MCP_AGENT_TOOL_)**    |                                                | Settings for the `run_browser_agent` tool.                                                                 |                                   |\n|                                     | `MCP_AGENT_TOOL_MAX_STEPS`                     | Max steps per agent run.                                                                                   | `100`                             |\n|                                     | `MCP_AGENT_TOOL_MAX_ACTIONS_PER_STEP`          | Max actions per agent step.                                                                                | `5`                               |\n|                                     | `MCP_AGENT_TOOL_TOOL_CALLING_METHOD`           | Method for tool invocation ('auto', 'json_schema', 'function_calling').                                    | `auto`                            |\n|                                     | `MCP_AGENT_TOOL_MAX_INPUT_TOKENS`              | Max input tokens for LLM context.                                                                          | `128000`                          |\n|                                     | `MCP_AGENT_TOOL_USE_VISION`                    | Enable vision capabilities (screenshot analysis).                                                          | `true`                            |\n|                                     | `MCP_AGENT_TOOL_HEADLESS`                      | Override `MCP_BROWSER_HEADLESS` for this tool (true/false/empty).                                          | ` ` (uses general)                |\n|                                     | `MCP_AGENT_TOOL_DISABLE_SECURITY`              | Override `MCP_BROWSER_DISABLE_SECURITY` for this tool (true/false/empty).                                  | ` ` (uses general)                |\n|                                     | `MCP_AGENT_TOOL_ENABLE_RECORDING`              | Enable Playwright video recording.                                                                         | `false`                           |\n|                                     | `MCP_AGENT_TOOL_SAVE_RECORDING_PATH`           | Optional: Path to save recordings. If not set, recording to file is disabled even if `ENABLE_RECORDING=true`. | ` ` (empty, recording disabled)   |\n|                                     | `MCP_AGENT_TOOL_HISTORY_PATH`                  | Optional: Directory to save agent history JSON files. If not set, history saving is disabled.              | ` ` (empty, history saving disabled) |\n| **Research Tool (MCP_RESEARCH_TOOL_)** |                                             | Settings for the `run_deep_research` tool.                                                                 |                                   |\n|                                     | `MCP_RESEARCH_TOOL_MAX_PARALLEL_BROWSERS`      | Max parallel browser instances for deep research.                                                          | `3`                               |\n|                                     | `MCP_RESEARCH_TOOL_SAVE_DIR`                   | Optional: Base directory to save research artifacts. Task ID will be appended. If not set, operates in memory-only mode. | `None`                           |\n| **Paths (MCP_PATHS_)**              |                                                | General path settings.                                                                                     |                                   |\n|                                     | `MCP_PATHS_DOWNLOADS`                          | Optional: Directory for downloaded files. If not set, persistent downloads to a specific path are disabled.  | ` ` (empty, downloads disabled)  |\n| **Server (MCP_SERVER_)**            |                                                | Server-specific settings.                                                                                  |                                   |\n|                                     | `MCP_SERVER_LOG_FILE`                          | Path for the server log file. Empty for stdout.                                                            | ` ` (empty, logs to stdout)       |\n|                                     | `MCP_SERVER_LOGGING_LEVEL`                     | Logging level (`DEBUG`, `INFO`, `WARNING`, `ERROR`, `CRITICAL`).                                           | `ERROR`                           |\n|                                     | `MCP_SERVER_ANONYMIZED_TELEMETRY`              | Enable/disable anonymized telemetry (`true`/`false`).                                                      | `true`                            |\n|                                     | `MCP_SERVER_MCP_CONFIG`                        | Optional: JSON string for MCP client config used by the internal controller.                               | `null`                            |\n\n**Supported LLM Providers (`MCP_LLM_PROVIDER`):**\n`openai`, `azure_openai`, `anthropic`, `google`, `mistral`, `ollama`, `deepseek`, `openrouter`, `alibaba`, `moonshot`, `unbound`\n\n*(Refer to `.env.example` for a comprehensive list of all supported environment variables and their specific provider keys/endpoints.)*\n\n## Connecting to Your Own Browser (CDP)\n\nInstead of having the server launch and manage its own browser instance, you can connect it to a Chrome/Chromium browser that you launch and manage yourself.\n\n**Steps:**\n\n1.  **Launch Chrome/Chromium with Remote Debugging Enabled:**\n    (Commands for macOS, Linux, Windows as previously listed, e.g., `google-chrome --remote-debugging-port=9222`)\n\n2.  **Configure Environment Variables:**\n    Set the following environment variables:\n    ```dotenv\n    MCP_BROWSER_USE_OWN_BROWSER=true\n    MCP_BROWSER_CDP_URL=http://localhost:9222 # Use the same port\n    # Optional: MCP_BROWSER_USER_DATA_DIR=/path/to/your/profile\n    ```\n\n3.  **Run the MCP Server or CLI:**\n    Start the server (`uv run mcp-server-browser-use`) or CLI (`mcp-browser-cli ...`) as usual.\n\n**Important Considerations:**\n*   The browser launched with `--remote-debugging-port` must remain open.\n*   Settings like `MCP_BROWSER_HEADLESS` and `MCP_BROWSER_KEEP_OPEN` are ignored when `MCP_BROWSER_USE_OWN_BROWSER=true`.\n\n## Development\n\n```bash\n# Install dev dependencies and sync project deps\nuv sync --dev\n\n# Install playwright browsers\nuv run playwright install\n\n# Run MCP server with debugger (Example connecting to own browser via CDP)\n# 1. Launch Chrome: google-chrome --remote-debugging-port=9222 --user-data-dir=\"optional/path/to/user/profile\"\n# 2. Run inspector command with environment variables:\nnpx @modelcontextprotocol/inspector@latest \\\n  -e MCP_LLM_GOOGLE_API_KEY=$GOOGLE_API_KEY \\\n  -e MCP_LLM_PROVIDER=google \\\n  -e MCP_LLM_MODEL_NAME=gemini-2.5-flash-preview-04-17 \\\n  -e MCP_BROWSER_USE_OWN_BROWSER=true \\\n  -e MCP_BROWSER_CDP_URL=http://localhost:9222 \\\n  -e MCP_RESEARCH_TOOL_SAVE_DIR=./tmp/dev_research_output \\\n  uv --directory . run mcp-server-browser-use\n\n# Note: Change timeout in inspector's config panel if needed (default is 10 seconds)\n\n# Run CLI example\n# Create a .env file with your settings (including MCP_RESEARCH_TOOL_SAVE_DIR) or use environment variables\nuv run mcp-browser-cli -e .env run-browser-agent \"What is the title of example.com?\"\nuv run mcp-browser-cli -e .env run-deep-research \"What is the best material for a pan for everyday use on amateur kitchen and dishwasher?\"\n```\n\n## Troubleshooting\n\n-   **Configuration Error on Startup**: If the application fails to start with an error about a missing setting, ensure all **mandatory** environment variables (like `MCP_RESEARCH_TOOL_SAVE_DIR`) are set correctly in your environment or `.env` file.\n-   **Browser Conflicts**: If *not* using CDP (`MCP_BROWSER_USE_OWN_BROWSER=false`), ensure no conflicting Chrome instances are running with the same user data directory if `MCP_BROWSER_USER_DATA_DIR` is specified.\n-   **CDP Connection Issues**: If using `MCP_BROWSER_USE_OWN_BROWSER=true`:\n    *   Verify Chrome was launched with `--remote-debugging-port`.\n    *   Ensure the port in `MCP_BROWSER_CDP_URL` matches.\n    *   Check firewalls and ensure the browser is running.\n-   **API Errors**: Double-check API keys (`MCP_LLM_<PROVIDER>_API_KEY` or `MCP_LLM_API_KEY`) and endpoints (e.g., `MCP_LLM_AZURE_OPENAI_ENDPOINT` for Azure).\n-   **Vision Issues**: Ensure `MCP_AGENT_TOOL_USE_VISION=true` and your LLM supports vision.\n-   **Dependency Problems**: Run `uv sync` and `uv run playwright install`.\n-   **File/Path Issues**:\n    *   If optional features like history saving, tracing, or downloads are not working, ensure the corresponding path variables (`MCP_AGENT_TOOL_HISTORY_PATH`, `MCP_BROWSER_TRACE_PATH`, `MCP_PATHS_DOWNLOADS`) are set and the application has write permissions to those locations.\n    *   For deep research, ensure `MCP_RESEARCH_TOOL_SAVE_DIR` is set to a valid, writable directory.\n-   **Logging**: Check the log file (`MCP_SERVER_LOG_FILE`, if set) or console output. Increase `MCP_SERVER_LOGGING_LEVEL` to `DEBUG` for more details. For CLI, use `--log-level DEBUG`.\n\n## License\n\nMIT - See [LICENSE](LICENSE) for details.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "browsers",
        "browser",
        "web",
        "browsers tasks",
        "web browsers",
        "control web"
      ],
      "category": "web-search"
    },
    "Saunved--mcp-server-clash-of-clans": {
      "owner": "Saunved",
      "name": "mcp-server-clash-of-clans",
      "url": "https://github.com/Saunved/mcp-server-clash-of-clans",
      "imageUrl": "/freedevtools/mcp/pfp/Saunved.webp",
      "description": "Obtain and analyze player and clan statistics from Clash of Clans to gain insights into war performance and strategies for gameplay improvement.",
      "stars": 0,
      "forks": 1,
      "license": "No License",
      "language": "JavaScript",
      "updated_at": "2025-04-25T22:20:18Z",
      "readme_content": "[![MseeP Badge](https://mseep.net/pr/saunved-mcp-server-clash-of-clans-badge.jpg)](https://mseep.ai/app/saunved-mcp-server-clash-of-clans)\n\nClash of Clans MCP server!\n\nFor usage with Claude Desktop, you can update the `claude_desktop_config.json` file to register this server.\n\n```json\n{\n    \"mcpServers\": {\n        \"Clash of Clans\": {\n            \"command\": \"npx\",\n            \"args\": [\n                \"-y\",\n                \"mcp-server-clash-of-clans\"\n            ],\n            \"env\": {\n                \"CLASH_API_KEY\": \"<your-api-key>\"\n            }\n        }\n    }\n}\n```\n\n# Available tools\n\n## get-player\nObtains information for a given player tag and summarizes it.\n\n## get-clan\nObtains information for a given clan and summarizes it.\n\n## clan-war-league-info\nObtains information about the most recent CWL rounds.\n\n## clan-war-league-war\nObtains information about a specific CWL war based on the round.\n\n## get-current-war\nGets the current war info for the clan (provided it is public).\n\n## get-war-log\nGets a clan's war log (provided it is public).\n\n## get-capital-raids\nGets information regarding the clan's capital raids.\n\n# Available prompts\n\n## analyze-current-war\nAnalyzes the current war. Provides an overview with the stats, top-performers, and potential strategy changes.\n\n## analyze-war-log\nAnalyzes a clan's war log and summarizes its overall performance.\n\n## analyze-cwl-war\nAnalyzes a given CWL war and summarizes the clan's overall performance in that war.\n\n## analyze-player\nAnalyzes a player's statistics and suggests scope for improvement.\n\n## analyze-clan\nAnalyzes a given clan and assesses it based on the members, their TH levels, war record, etc.\n\n## analyze-capital-raids\nAnalyzes a clan's last few capital raids (3 by default).",
      "npm_url": "https://www.npmjs.com/package/mcp-server-clash-of-clans",
      "npm_downloads": 374,
      "keywords": [
        "clans",
        "clan",
        "clash",
        "clash clans",
        "clan statistics",
        "player clan"
      ],
      "category": "web-search"
    },
    "ScrapeGraphAI--scrapegraph-mcp": {
      "owner": "ScrapeGraphAI",
      "name": "scrapegraph-mcp",
      "url": "https://github.com/ScrapeGraphAI/scrapegraph-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/ScrapeGraphAI.webp",
      "description": "A scraping API for structured data extraction from websites that includes features for specific webpage data extraction, web search integration, and content filtering. It also converts webpages into markdown format.",
      "stars": 39,
      "forks": 9,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-22T03:15:16Z",
      "readme_content": "# ScrapeGraph MCP Server\n\n\n<a href=\"https://glama.ai/mcp/servers/37us0q2tr6\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/37us0q2tr6/badge\" alt=\"ScrapeGraph Server MCP server\" style=\"display: inline-block;\"/>\n</a>\n<a href=\"https://mseep.ai/app/scrapegraphai-scrapegraph-mcp\">\n  <img src=\"https://mseep.net/pr/scrapegraphai-scrapegraph-mcp-badge.png\" alt=\"MseeP.ai Security Assessment Badge\" style=\"display: inline-block;\"/>\n</a>\n\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![Python 3.10](https://img.shields.io/badge/python-3.10-blue.svg)](https://www.python.org/downloads/release/python-3100/)\n[![smithery badge](https://smithery.ai/badge/@ScrapeGraphAI/scrapegraph-mcp)](https://smithery.ai/server/@ScrapeGraphAI/scrapegraph-mcp)\n\n\nA production-ready [Model Context Protocol](https://modelcontextprotocol.io/introduction) (MCP) server that provides seamless integration with the [ScapeGraph AI](https://scrapegraphai.com) API. This server enables language models to leverage advanced AI-powered web scraping capabilities with enterprise-grade reliability.\n\n\n## Available Tools\n\nThe server provides the following enterprise-ready tools:\n\n- `markdownify(website_url: str)`: Transform any webpage into clean, structured markdown format\n- `smartscraper(user_prompt: str, website_url: str)`: Leverage AI to extract structured data from any webpage\n- `searchscraper(user_prompt: str)`: Execute AI-powered web searches with structured, actionable results\n\n## Setup Instructions\n\nTo utilize this server, you'll need a ScapeGraph API key. Follow these steps to obtain one:\n\n1. Navigate to the [ScapeGraph Dashboard](https://dashboard.scrapegraphai.com)\n2. Create an account and generate your API key\n\n### Automated Installation via Smithery\n\nFor automated installation of the ScrapeGraph API Integration Server using [Smithery](https://smithery.ai/server/@ScrapeGraphAI/scrapegraph-mcp):\n\n```bash\nnpx -y @smithery/cli install @ScrapeGraphAI/scrapegraph-mcp --client claude\n```\n\n### Claude Desktop Configuration\n\nUpdate your Claude Desktop configuration file with the following settings (located on the top rigth of the Cursor page):\n\n(remember to add your API key inside the config)\n\n```json\n{\n    \"mcpServers\": {\n        \"@ScrapeGraphAI-scrapegraph-mcp\": {\n            \"command\": \"npx\",\n            \"args\": [\n                \"-y\",\n                \"@smithery/cli@latest\",\n                \"run\",\n                \"@ScrapeGraphAI/scrapegraph-mcp\",\n                \"--config\",\n                \"\\\"{\\\\\\\"scrapegraphApiKey\\\\\\\":\\\\\\\"YOUR-SGAI-API-KEY\\\\\\\"}\\\"\"\n            ]\n        }\n    }\n}\n```\n\nThe configuration file is located at:\n- Windows: `%APPDATA%/Claude/claude_desktop_config.json`\n- macOS: `~/Library/Application\\ Support/Claude/claude_desktop_config.json`\n\n### Cursor Integration\n\nAdd the ScrapeGraphAI MCP server on the settings:\n\n\n\n## Example Use Cases\n\nThe server enables sophisticated queries such as:\n\n- \"Analyze and extract the main features of the ScapeGraph API\"\n- \"Generate a structured markdown version of the ScapeGraph homepage\"\n- \"Extract and analyze pricing information from the ScapeGraph website\"\n- \"Research and summarize recent developments in AI-powered web scraping\"\n- \"Create a comprehensive summary of the Python documentation website\"\n\n## Error Handling\n\nThe server implements robust error handling with detailed, actionable error messages for:\n\n- API authentication issues\n- Malformed URL structures\n- Network connectivity failures\n- Rate limiting and quota management\n\n## Common Issues\n\n### Windows-Specific Connection\n\nWhen running on Windows systems, you may need to use the following command to connect to the MCP server:\n\n```bash\nC:\\Windows\\System32\\cmd.exe /c npx -y @smithery/cli@latest run @ScrapeGraphAI/scrapegraph-mcp --config \"{\\\"scrapegraphApiKey\\\":\\\"YOUR-SGAI-API-KEY\\\"}\"\n```\n\nThis ensures proper execution in the Windows environment.\n\n## License\n\nThis project is distributed under the MIT License. For detailed terms and conditions, please refer to the LICENSE file.\n\n## Acknowledgments\n\nSpecial thanks to [tomekkorbak](https://github.com/tomekkorbak) for his implementation of [oura-mcp-server](https://github.com/tomekkorbak/oura-mcp-server), which served as starting point for this repo.\n\nMade with ❤️ by [ScrapeGraphAI](https://scrapegraphai.com) Team",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "scrapegraph",
        "scraping",
        "scrapegraphai",
        "scrapegraph mcp",
        "mcp scraping",
        "extraction web"
      ],
      "category": "web-search"
    },
    "Scrapezy--mcp": {
      "owner": "Scrapezy",
      "name": "mcp",
      "url": "https://github.com/Scrapezy/mcp",
      "imageUrl": "/freedevtools/mcp/pfp/Scrapezy.webp",
      "description": "Extract structured data from websites using AI models by providing a URL and a clear prompt. It simplifies data extraction tasks and integrates with AI workflows for enhanced application capabilities.",
      "stars": 10,
      "forks": 6,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-09-26T18:17:20Z",
      "readme_content": "# @scrapezy/mcp MCP Server\n\n<a href=\"https://glama.ai/mcp/servers/rnktqsouvy\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/rnktqsouvy/badge\" alt=\"Scrapezy MCP server\" />\n</a>\n\n[![smithery badge](https://smithery.ai/badge/@Scrapezy/mcp)](https://smithery.ai/server/@Scrapezy/mcp)\n\nA Model Context Protocol server for [Scrapezy](https://scrapezy.com) that enables AI models to extract structured data from websites.\n\n## Features\n\n### Tools\n- `extract_structured_data` - Extract structured data from a website\n  - Takes URL and prompt as required parameters\n  - Returns structured data extracted from the website based on the prompt\n  - The prompt should clearly describe what data to extract from the website\n\n## Installation\n\n### Installing via Smithery\n\nTo install Scrapezy MCP Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@Scrapezy/mcp):\n\n```bash\nnpx -y @smithery/cli install @Scrapezy/mcp --client claude\n```\n\n### Manual Installation\n```bash\nnpm install -g @scrapezy/mcp\n```\n\n## Usage\n\n### API Key Setup\n\nThere are two ways to provide your Scrapezy API key:\n\n1. **Environment Variable:**\n   ```bash\n   export SCRAPEZY_API_KEY=your_api_key\n   npx @scrapezy/mcp\n   ```\n\n2. **Command-line Argument:**\n   ```bash\n   npx @scrapezy/mcp --api-key=your_api_key\n   ```\n\nTo use with Claude Desktop, add the server config:\n\nOn MacOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\nOn Windows: `%APPDATA%/Claude/claude_desktop_config.json`\n\n```json\n{\n  \"mcpServers\": {\n    \"scrapezy\": {\n      \"command\": \"npx @scrapezy/mcp --api-key=your_api_key\"\n    }\n  }\n}\n```\n\n### Example Usage in Claude\n\nYou can use this tool in Claude with prompts like:\n\n```\nPlease extract product information from this page: https://example.com/product\nExtract the product name, price, description, and available colors.\n```\n\nClaude will use the MCP server to extract the requested structured data from the website.\n\n### Debugging\n\nSince MCP servers communicate over stdio, debugging can be challenging. We recommend using the [MCP Inspector](https://github.com/modelcontextprotocol/inspector), which is available as a package script:\n\n```bash\nnpm run inspector\n```\n\nThe Inspector will provide a URL to access debugging tools in your browser.\n\n## License\n\nMIT",
      "npm_url": "https://www.npmjs.com/package/mcp",
      "npm_downloads": 17624,
      "keywords": [
        "scrapezy",
        "extract",
        "extraction",
        "search scrapezy",
        "data extraction",
        "scrapezy mcp"
      ],
      "category": "web-search"
    },
    "Scrapybara--scrapybara-mcp": {
      "owner": "Scrapybara",
      "name": "scrapybara-mcp",
      "url": "https://github.com/Scrapybara/scrapybara-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/Scrapybara.webp",
      "description": "Facilitates interaction with virtual Ubuntu desktops, enabling code execution, web browsing, and various actions through an intuitive interface. Supports real-time operations to enhance MCP client capabilities.",
      "stars": 17,
      "forks": 4,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-09-13T16:42:25Z",
      "readme_content": "<div id=\"toc\" align=\"center\">\n  <ul style=\"list-style: none\">\n    <summary>\n      <h1> Scrapybara MCP </h1>\n    </summary>\n  </ul>\n</div>\n\n<p align=\"center\">\n  <a href=\"https://github.com/scrapybara/scrapybara-playground/blob/main/license\"><img alt=\"MIT License\" src=\"https://img.shields.io/badge/license-MIT-blue\" /></a>\n  <a href=\"https://discord.gg/s4bPUVFXqA\"><img alt=\"Discord\" src=\"https://img.shields.io/badge/Discord-Join%20the%20community-6D1CCF.svg?logo=discord\" /></a>\n  <a href=\"https://x.com/scrapybara\"><img alt=\"X\" src=\"https://img.shields.io/badge/Twitter-Follow%20us-6D1CCF.svg?logo=X\" /></a>\n\nA Model Context Protocol server for [Scrapybara](https://scrapybara.com). This server enables MCP clients such as [Claude Desktop](https://claude.ai/download), [Cursor](https://www.cursor.com/), and [Windsurf](https://codeium.com/windsurf) to interact with virtual Ubuntu desktops and take actions such as browsing the web, running code, and more.\n\n## Prerequisites\n\n- Node.js 18+\n- pnpm\n- Scrapybara API key (get one at [scrapybara.com](https://scrapybara.com))\n\n## Installation\n\n1. Clone the repository:\n\n```bash\ngit clone https://github.com/scrapybara/scrapybara-mcp.git\ncd scrapybara-mcp\n```\n\n2. Install dependencies:\n\n```bash\npnpm install\n```\n\n3. Build the project:\n\n```bash\npnpm build\n```\n\n4. Add the following to your MCP client config:\n\n```json\n{\n  \"mcpServers\": {\n    \"scrapybara-mcp\": {\n      \"command\": \"node\",\n      \"args\": [\"path/to/scrapybara-mcp/dist/index.js\"],\n      \"env\": {\n        \"SCRAPYBARA_API_KEY\": \"<YOUR_SCRAPYBARA_API_KEY>\",\n        \"ACT_MODEL\": \"<YOUR_ACT_MODEL>\", // \"anthropic\" or \"openai\"\n        \"AUTH_STATE_ID\": \"<YOUR_AUTH_STATE_ID>\" // Optional, for authenticating the browser\n      }\n    }\n  }\n}\n```\n\n5. Restart your MCP client and you're good to go!\n\n## Tools\n\n- **start_instance** - Start a Scrapybara Ubuntu instance. Use it as a desktop sandbox to access the web or run code. Always present the stream URL to the user afterwards so they can watch the instance in real time.\n- **get_instances** - Get all running Scrapybara instances.\n- **stop_instance** - Stop a running Scrapybara instance.\n- **bash** - Run a bash command in a Scrapybara instance.\n- **act** - Take action on a Scrapybara instance through an agent. The agent can control the instance with mouse/keyboard and bash commands.\n\n## Contributing\n\nScrapybara MCP is a community-driven project. Whether you're submitting an idea, fixing a typo, adding a new tool, or improving an existing one, your contributions are greatly appreciated!\n\nBefore contributing, read through the existing issues and pull requests to see if someone else is already working on something similar. That way you can avoid duplicating efforts.\n\nIf there are more tools or features you'd like to see, feel free to suggest them on the [issues page](https://github.com/scrapybara/scrapybara-mcp/issues).",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "scrapybara",
        "web",
        "mcp",
        "search scrapybara",
        "scrapybara mcp",
        "scrapybara scrapybara"
      ],
      "category": "web-search"
    },
    "SecretiveShell--MCP-searxng": {
      "owner": "SecretiveShell",
      "name": "MCP-searxng",
      "url": "https://github.com/SecretiveShell/MCP-searxng",
      "imageUrl": "/freedevtools/mcp/pfp/SecretiveShell.webp",
      "description": "Connects agentic systems to perform web searches using SearXNG, facilitating access to various search results from the web.",
      "stars": 104,
      "forks": 17,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-30T10:57:58Z",
      "readme_content": "# MCP-searxng\n\nAn MCP server for connecting agentic systems to search systems via [searXNG](https://docs.searxng.org/).\n\n<p align=\"center\">\n  <a href=\"https://glama.ai/mcp/servers/sl2zl8vaz8\">\n    <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/sl2zl8vaz8/badge\" alt=\"MCP SearxNG Badge\"/>\n  </a>\n</p>\n\n## Tools\n\nSearch the web with SearXNG\n\n## Prompts\n\n```python\nsearch(query: str) -> f\"Searching for {query} using searXNG\"\n```\n\n## Usage\n\n### via uvx\n\n1) configure your client JSON like\n\n```json\n{\n  \"mcpServers\": {\n    \"searxng\": {\n      \"command\": \"uvx\", \n      \"args\": [\n        \"mcp-searxng\"\n      ]\n    }\n  }\n}\n```\n\n### via git clone\n\n1) Add the server to claude desktop (the entrypoint is main.py)\n\nClone the repo and add this JSON to claude desktop\n\nyou can run this server with `uvx mcp-searxng`, or use a local copy of the repo\n\n```json\n{\n  \"mcpServers\": {\n    \"searxng\": {\n      \"command\": \"uv\", \n      \"args\": [\n        \"--project\",\n        \"/absoloute/path/to/MCP-searxng/\",\n        \"run\",\n        \"/absoloute/path/to/MCP-searxng/mcp-searxng/main.py\"\n      ]\n    }\n  }\n}\n```\n\nyou will need to change the paths to match your environment\n\n### Custom SearXNG URL\n\n2) set the environment variable `SEARXNG_URL` to the URL of the searxng server (default is `http://localhost:8080`)\n\n3) run your MCP client and you should be able to search the web with searxng\n\nNote: if you are using claude desktop make sure to kill the process (task manager or equivalent) before running the server again\n",
      "npm_url": "https://www.npmjs.com/package/mcp-searxng",
      "npm_downloads": 27918,
      "keywords": [
        "searxng",
        "searches",
        "search",
        "searxng facilitating",
        "using searxng",
        "mcp searxng"
      ],
      "category": "web-search"
    },
    "Selenium39--mcp-server-weibo": {
      "owner": "Selenium39",
      "name": "mcp-server-weibo",
      "url": "https://github.com/Selenium39/mcp-server-weibo",
      "imageUrl": "/freedevtools/mcp/pfp/Selenium39.webp",
      "description": "Retrieves detailed Weibo user information, feeds, and conducts user searches. Facilitates the analysis and scraping of Weibo data for integration into various applications.",
      "stars": 30,
      "forks": 6,
      "license": "No License",
      "language": "TypeScript",
      "updated_at": "2025-09-09T07:08:53Z",
      "readme_content": "# Weibo MCP Server (TypeScript 版本)\n\n## 安装\n\n从源码安装：\n\n```json\n{\n    \"mcpServers\": {\n        \"weibo\": {\n            \"command\": \"npx\",\n            \"args\": [\n                \"--from\",\n                \"git+https://github.com/Selenium39/mcp-server-weibo.git\",\n                \"mcp-server-weibo\"\n            ]\n        }\n    }\n}\n```\n\n从包管理器安装：\n\n```json\n{\n    \"mcpServers\": {\n        \"weibo\": {\n            \"command\": \"npx\",\n            \"args\": [\"mcp-server-weibo\"]\n        }\n    }\n}\n```\n\n## 组件\n\n### Tools\n\n- `search_users(keyword, limit)`：根据关键词搜索微博用户\n- `get_profile(uid)`：获取用户详细资料信息\n- `get_feeds(uid, limit)`：获取用户微博动态\n- `get_hot_search(limit)`：获取微博热搜榜\n- `search_content(keyword, limit, page?)`：根据关键词搜索微博内容\n\n### Resources\n\n无\n\n### Prompts\n\n无\n\n## 系统要求\n\n- Node.js >= 18.0.0\n\n## 许可证\n\nMIT License\n\n## 免责声明\n\n本项目与微博无关，仅用于学习和研究目的。\n\n## MCP Server推荐\n\n[mcp-server-tempmail](https://chat-tempmail.com/zh/mcp-server)\n",
      "npm_url": "https://www.npmjs.com/package/mcp-server-weibo",
      "npm_downloads": 3944,
      "keywords": [
        "weibo",
        "scraping",
        "selenium39",
        "weibo retrieves",
        "weibo data",
        "scraping weibo"
      ],
      "category": "web-search"
    },
    "Shoofio--brave-search-mcp-sse": {
      "owner": "Shoofio",
      "name": "brave-search-mcp-sse",
      "url": "https://github.com/Shoofio/brave-search-mcp-sse",
      "imageUrl": "/freedevtools/mcp/pfp/Shoofio.webp",
      "description": "Integrate web and local search capabilities to access comprehensive search results with flexible filtering and smart fallbacks. Utilizes Server-Sent Events (SSE) to connect AI models with the Brave Search API.",
      "stars": 13,
      "forks": 10,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-09-06T13:57:11Z",
      "readme_content": "# Brave Search MCP/SSE Server\n\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![Docker Hub](https://img.shields.io/docker/v/shoofio/brave-search-mcp-sse/latest?sort=semver&label=Docker%20Hub)](https://hub.docker.com/r/shoofio/brave-search-mcp-sse)\n[![Helm Chart](https://img.shields.io/badge/Helm%20Chart-1.0.10-blue?link=https://shoofio.github.io/brave-search-mcp-sse/)](https://shoofio.github.io/brave-search-mcp-sse/)\n<!-- Add other badges here if applicable (e.g., build status, Docker pulls) -->\n\nAn implementation of the Model Context Protocol (MCP) using Server-Sent Events (SSE) that integrates the [Brave Search API](https://brave.com/search/api/), providing AI models and other clients with web and local search capabilities through a streaming interface.\n\n## Overview\n\nThis server acts as a tool provider for Large Language Models that understand the Model Context Protocol. It exposes Brave's powerful web and local search functionalities via an SSE connection, allowing for real-time streaming of search results and status updates.\n\n**Key Design Goals:**\n\n*   **Centralized Access:** Designed with centrality in mind, allowing organizations or individuals to manage a single Brave Search API key and provide controlled access to multiple internal clients or applications.\n*   **Observability:** Features robust logging to track requests, API interactions, errors, and rate limits, providing visibility into usage and aiding debugging.\n*   **Flexible Deployment:** Can be deployed privately within a network or optionally exposed publicly via methods like Kubernetes Ingress or direct Docker port mapping.\n\n## Features\n\n*   **Web Search**: Access Brave's independent web search index for general queries, news, articles, etc. Supports pagination and filtering controls.\n*   **Local Search**: Find businesses, restaurants, and services with detailed information like address, phone number, and ratings.\n*   **Smart Fallbacks**: Local search automatically falls back to a filtered web search if no specific local results are found for the query.\n*   **Server-Sent Events (SSE)**: Efficient, real-time streaming of search results and tool execution status.\n*   **Model Context Protocol (MCP)**: Adheres to the MCP standard for seamless integration with compatible clients.\n*   **Docker Support**: Includes a `Dockerfile` for easy containerization and deployment.\n*   **Helm Chart**: Provides a Helm chart for straightforward deployment to Kubernetes clusters.\n\n## Prerequisites\n\nDepending on your chosen deployment method, you will need some of the following:\n\n*   **Brave Search API Key**: Required for all deployment methods. See \"Getting Started\" below.\n*   **Docker**: Required if deploying using Docker.\n*   **kubectl & Helm**: Required if deploying to Kubernetes using Helm.\n*   **Node.js & npm**: Required *only* for local development (Node.js v22.x or later recommended).\n*   **Git**: Required for cloning the repository for local development or building custom Docker images.\n\n## Getting Started\n\n### 1. Obtain a Brave Search API Key\n\n1.  Sign up for a [Brave Search API account](https://brave.com/search/api/).\n2.  Choose a plan (a free tier is available).\n3.  Generate your API key from the [developer dashboard](https://api.search.brave.com/app/keys).\n\n### 2. Configuration\n\nThe server requires the Brave Search API key to be set via the `BRAVE_API_KEY` environment variable.\n\nOther potential environment variables (check `src/config/config.ts` for details):\n*   `PORT`: The port the server listens on (defaults to `8080`).\n*   `LOG_LEVEL`: Logging verbosity (e.g., `info`, `debug`).\n\nSet these variables in your environment or using a `.env` file in the project root for local development.\n\n## Installation & Usage\n\nChoose the deployment method that best suits your needs:\n\n### Option 1: Docker (Recommended for Deployment)\n\n**Prerequisites:** Docker installed.\n\n1.  **Obtain a Brave Search API Key:** Follow the steps in the \"Getting Started\" section.\n2.  **Pull the Docker image:**\n    Pull the latest image from Docker Hub:\n    ```bash\n    docker pull shoofio/brave-search-mcp-sse:latest\n    ```\n    Or pull a specific version tag (e.g., `1.0.10`):\n    ```bash\n    docker pull shoofio/brave-search-mcp-sse:1.0.10\n    ```\n    *(Alternatively, you can build the image locally if needed. Clone the repository and run `docker build -t brave-search-mcp-sse:custom .`)*\n3.  **Run the Docker container:**\n    Use the tag you pulled (e.g., `latest` or `1.0.10`):\n    ```bash\n    docker run -d --rm \\\n      -p 8080:8080 \\\n      -e BRAVE_API_KEY=\"YOUR_API_KEY_HERE\" \\\n      -e PORT=\"8080\" # Optional: Define the port if needed\n      # -e LOG_LEVEL=\"info\" # Optional: Set log level\n      --name brave-search-server \\\n      shoofio/brave-search-mcp-sse:latest # Or your specific tag\n    ```\n    This runs the server in detached mode, mapping port 8080 on your host to the container.\n\n### Option 2: Helm (Kubernetes Deployment)\n\n**Prerequisites:** `kubectl` connected to your cluster, Helm installed.\n\n1.  **Obtain a Brave Search API Key:** Follow the steps in the \"Getting Started\" section.\n2.  **Add the Helm repository:**\n    ```bash\n    helm repo add brave-search-mcp-sse https://shoofio.github.io/brave-search-mcp-sse/\n    helm repo update\n    ```\n3.  **Prepare API Key Secret (Recommended):**\n    Create a Kubernetes secret in the target namespace:\n    ```bash\n    kubectl create secret generic brave-search-secret \\\n      --from-literal=api-key='YOUR_API_KEY_HERE' \\\n      -n <your-namespace>\n    ```\n4.  **Install the Helm chart:**\n    The chart version corresponds to the application version (latest is `1.0.10`). Install using the secret:\n    ```bash\n    helm install brave-search brave-search-mcp-sse/brave-search-mcp-sse \\\n      -n <your-namespace> \\\n      --set braveSearch.existingSecret=brave-search-secret\n      # Optionally specify a version: --version 1.0.10\n    ```\n    Or provide the key directly (less secure):\n    ```bash\n    helm install brave-search brave-search-mcp-sse/brave-search-mcp-sse \\\n      -n <your-namespace> \\\n      --set braveSearch.apiKey=\"YOUR_API_KEY_HERE\"\n    ```\n5.  **Chart Configuration:**\n    You can customize the deployment by overriding default values. Create a YAML file (e.g., `dev-values.yaml`, `prod-values.yaml`) with your desired settings and use the `-f` flag during installation: `helm install ... -f dev-values.yaml`.\n\n    Refer to the chart's default [`values.yaml`](./helm/brave-search-mcp-sse/values.yaml) file to see all available configuration options and their default settings.\n\n### Option 3: Local Development\n\n**Prerequisites:** Node.js and npm (v22.x or later recommended), Git.\n\n1.  **Obtain a Brave Search API Key:** Follow the steps in the \"Getting Started\" section.\n2.  **Clone the repository:**\n    ```bash\n    git clone <repository_url> # Replace with the actual URL\n    cd brave-search-mcp-sse\n    ```\n3.  **Install dependencies:**\n    ```bash\n    npm install\n    ```\n4.  **Set Environment Variables:**\n    Create a `.env` file in the root directory:\n    ```env\n    BRAVE_API_KEY=YOUR_API_KEY_HERE\n    PORT=8080\n    # LOG_LEVEL=debug\n    ```\n5.  **Build the TypeScript code:**\n    ```bash\n    npm run build\n    ```\n6.  **Run the server:**\n    ```bash\n    npm start\n    # Or for development with auto-reloading (if nodemon/ts-node-dev is configured)\n    # npm run dev\n    ```\n    The server will start listening on the configured port (default `8080`).\n\n## API / Protocol Interaction\n\nClients connect to this server via HTTP GET request to establish an SSE connection. The specific endpoint depends on your deployment (e.g., `http://localhost:8080/`, `http://<k8s-service-ip>:8080/`, or through an Ingress).\n\nOnce connected, the server and client communicate using MCP messages over the SSE stream.\n\n### Available Tools\n\nThe server exposes the following tools to connected clients:\n\n1.  **`brave_web_search`**\n    *   **Description**: Performs a general web search using the Brave Search API.\n    *   **Inputs**:\n        *   `query` (string, required): The search query.\n        *   `count` (number, optional): Number of results to return (1-20, default 10).\n        *   `offset` (number, optional): Pagination offset (0-9, default 0).\n        *   *(Other Brave API parameters like `search_lang`, `country`, `freshness`, `result_filter`, `safesearch` might be supported - check `src/services/braveSearchApi.ts`)*\n    *   **Output**: Streams MCP messages containing search results (title, URL, snippet, etc.).\n\n2.  **`brave_local_search`**\n    *   **Description**: Performs a search for local businesses and places using the Brave Search API. Falls back to web search if no local results are found.\n    *   **Inputs**:\n        *   `query` (string, required): The local search query (e.g., \"pizza near me\", \"cafes in downtown\").\n        *   `count` (number, optional): Maximum number of results (1-20, default 5).\n    *   **Output**: Streams MCP messages containing local business details (name, address, phone, rating, etc.).\n\n*(Example using `curl` - Note: Actual MCP interaction requires a client library)*\n```bash\n# Example: Connect to SSE endpoint (won't show MCP messages directly)\ncurl -N http://localhost:8080/ # Or your deployed endpoint\n```\n\n### Client Configuration Example (Cursor)\n\nTo use this server with an MCP client like Cursor, you need to configure the client to connect to the server's SSE endpoint.\n\nAdd the following configuration to your Cursor settings (`mcp.json` or similar configuration file), replacing the URL with the actual address and port where your `brave-search-mcp-sse` server is accessible:\n\n```json\n{\n  \"mcpServers\": {\n    \"brave-search\": {\n      \"transport\": \"sse\",\n      \"url\": \"http://localhost:8080/sse\"\n    }\n  }\n}\n```\n\n**Explanation:**\n\n*   `transport`: Must be set to `\"sse\"` for this server.\n*   `url`: This is the crucial part.\n    *   If running locally via Docker (as shown in the example), `http://localhost:8080/sse` is likely correct.\n    *   If running in Kubernetes, replace `localhost:8080` with the appropriate Kubernetes Service address/port or the Ingress hostname/path configured to reach the server's port 8080.\n    *   Ensure the URL path ends with `/sse`.\n\n*(Similar configuration steps might apply to other MCP clients that support the SSE transport, like recent versions of Claude Desktop, but refer to their specific documentation.)*\n\n## Project Structure\n\n```\n.\n├── Dockerfile             # Container build definition\n├── helm/                  # Helm chart for Kubernetes deployment\n│   └── brave-search-mcp-sse/\n├── node_modules/        # Project dependencies (ignored by git)\n├── src/                   # Source code (TypeScript)\n│   ├── config/            # Configuration loading\n│   ├── services/          # Brave API interaction logic\n│   ├── tools/             # Tool definitions for MCP\n│   ├── transport/         # SSE/MCP communication handling\n│   ├── types/             # TypeScript type definitions\n│   ├── utils/             # Utility functions\n│   └── index.ts           # Main application entry point\n├── dist/                  # Compiled JavaScript output (ignored by git)\n├── package.json           # Project metadata and dependencies\n├── tsconfig.json          # TypeScript compiler options\n├── .env.example           # Example environment file\n├── .gitignore\n└── README.md              # This file\n```\n\n## Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request with your changes. Ensure your code adheres to the existing style and includes tests where applicable. I will review PRs as time permits.\n\n## License\n\nThis MCP server is licensed under the[MIT License](./LICENSE). This means you are free to use, modify, and distribute the software, subject to the terms and conditions of the MIT License. For more details, please see the LICENSE file in the project repository.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "sse",
        "search",
        "web",
        "brave search",
        "search api",
        "web search"
      ],
      "category": "web-search"
    },
    "Siddhant-K-code--memory-journal-mcp-server": {
      "owner": "Siddhant-K-code",
      "name": "memory-journal-mcp-server",
      "url": "https://github.com/Siddhant-K-code/memory-journal-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/Siddhant-K-code.webp",
      "description": "Search and analyze photos in a library using various intuitive tools, including location-based searches to easily find images taken in specific places.",
      "stars": 21,
      "forks": 4,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-07-28T18:59:40Z",
      "readme_content": "# 📸 Smart Photo Journal MCP Server\n\n**Smart Photo Journal** is an MCP server designed to help you search and analyze your photo library with powerful, intuitive tools. Whether you're reminiscing about family moments or looking for a specific photo with friends, this server has got you covered! 🎉\n\n> **Inspired by:** [burningion/video-editing-mcp](https://github.com/burningion/video-editing-mcp)\n> A huge shoutout to [@burningion](https://x.com/burningion) for the innovative idea of using MCP for creative media management!\n\n<a href=\"https://glama.ai/mcp/servers/51jiworg5k\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/51jiworg5k/badge\" alt=\"Smart Photo Journal Server MCP server\" /></a>\n\n## 🎯 Features\n\n- **Location Search:** Find photos from specific places with ease. 🌍\n- **Label Search:** Search photos by keywords or labels like \"Birthday,\" \"Beach,\" or \"Vacation.\" 🎉\n- **People Search:** Quickly locate photos featuring specific people. 👥\n- **Photo Analysis:** Discover fun insights like the most popular times and days for your photo shoots. 🕰️\n- **Fuzzy Matching:** Not sure of the exact name? Don't worry! The server supports fuzzy matching for flexibility. 🔍\n\n## 🚀 Getting started\n\n### Prerequisites\n\n1. Ensure you have macOS with a Photos library.\n2. Install [uv](https://docs.astral.sh/uv/) to manage dependencies and run the server.\n\n### Installation\n\n1. Clone the repository:\n\n   ```bash\n   git clone https://github.com/Siddhant-K-code/memory-journal-mcp-server.git\n   cd memory-journal-mcp-server\n   ```\n\n2. Install dependencies using `uv`:\n\n   ```bash\n   uv sync\n   ```\n\n3. Configure the MCP server. Update your `claude_desktop_config.json` with the following configuration:\n\n   ```json\n   {\n     \"mcpServers\": {\n       \"smart-photo-journal\": {\n         \"command\": \"/Users/<YOUR_DEVICE_USERNAME>/.local/bin/uv\",\n         \"args\": [\n           \"--directory\",\n           \"/Users/<PATH_TO_CLONED_DIR>/memory-journal-mcp-server\",\n           \"run\",\n           \"server.py\"\n         ]\n       }\n     }\n   }\n   ```\n\n4. Start the server with following command or just open Claude Desktop:\n   ```bash\n   uv run server.py\n   ```\n\n> **Note:** Replace `<YOUR_DEVICE_USERNAME>` and `<PATH_TO_CLONED_DIR>` with your actual device username and the path to the cloned directory.\n> You will get a popup to authorize the server to access your photos. It will be in local only, and no data will be shared with anyone except Claude services.\n\n### MCP Server Initialization\n\nWhen the server starts, you'll see:\n\n```\nStarting Smart Photo Journal MCP server.\n```\n\nIt's now ready to process your photo queries! 🎉\n\n---\n\n## 🛠️ Usage\n\n### Available Tools\n\n1. **Location Search**\n\n   - Description: Find photos taken in a specific location.\n   - Input Example:\n     ```json\n     {\n       \"location\": \"Udaipur\"\n     }\n     ```\n   - Expected Output:\n     ```\n     Found 5 photos from Udaipur:\n     📷 IMG_1234.jpg\n     ...\n     ```\n\n2. **Label Search**\n\n   - Description: Search for photos by labels or keywords.\n   - Input Example:\n     ```json\n     {\n       \"label\": \"Birthday\"\n     }\n     ```\n   - Expected Output:\n     ```\n     Photos labeled as 'Birthday' (3 found):\n     📷 IMG_5678.jpg\n     ...\n     ```\n\n3. **People Search**\n\n   - Description: Find photos containing specific people.\n   - Input Example:\n     ```json\n     {\n       \"person\": \"Maa\"\n     }\n     ```\n   - Expected Output:\n     ```\n     Photos with Maa (10 found):\n     📷 IMG_9101.jpg\n     ...\n     ```\n\n4. **Photo Analysis**\n   - Description: Analyze patterns in your photo library, such as the most common times or days for photo shoots.\n   - Input Example:\n     ```json\n     {}\n     ```\n   - Expected Output:\n     ```\n     📸 Photo Taking Patterns:\n     Total Photos: 200\n     ...\n     ```\n\n---\n\n## 📚 Example Use-Cases\n\n### 1. **Family & Friends Album Organizer**\n\nWant to gather all your family moments in one place? Use the `people-search` tool with names like \"Papa\" or \"Mom\" or \"Any Friend\" to find photos with specific people.\n\n### 2. **Vacation Highlights**\n\nSearch for photos from your vacation destination using the `location-search` tool.\n\n### 3. **Throwback Fun**\n\nCurious about your past birthday photos? Use `label-search` with \"Birthday\" and relive the fun!\n\n### 4. **Understand Your Photography Habits**\n\nUse the `photo-analysis` tool to understand when and where you take most of your photos. Plan your next shoot accordingly!\n\n---\n\n## ⚡ Tips for Best Results\n\n- Ensure your Photos library is loaded in macOS.\n- Be as specific as possible with search queries for more accurate results.\n- Use fuzzy matching for flexibility when you're unsure of the exact name.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "searches",
        "photos",
        "search",
        "photos library",
        "analyze photos",
        "search siddhant"
      ],
      "category": "web-search"
    },
    "Sivan22--mcp-otzaria-server": {
      "owner": "Sivan22",
      "name": "mcp-otzaria-server",
      "url": "https://github.com/Sivan22/mcp-otzaria-server",
      "imageUrl": "/freedevtools/mcp/pfp/Sivan22.webp",
      "description": "Provides powerful search capabilities for Jewish texts and literature through a standardized interface, enabling complex queries and relevance-based scoring for rich search results.",
      "stars": 19,
      "forks": 7,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-24T21:15:26Z",
      "readme_content": "# Jewish Library MCP Server\n\nAn MCP (Model Context Protocol) server that provides powerful search capabilities for Jewish texts and literature. This server enables Large Language Models to search and reference Jewish texts through a standardized interface.\n\n## Features\n\n- Full-text search across Jewish texts and literature\n- Advanced query syntax support:\n  - Field-specific search (text:term, reference:term, topics:term)\n  - Boolean operators (AND, OR)\n  - Required/excluded terms (+term, -term)\n  - Phrase search ('exact phrase')\n  - Wildcards (?, *)\n- Relevance-based scoring\n- Rich search results including references, topics, and highlighted excerpts\n\n## Installation\n\nRequires Python 3.10 or higher.\n\n\n### Clone the repository\n```bash\ngit clone https://github.com/sivan22/mcp-otzaria-server.git\ncd mcp-otzaria-server\n```\n### Get the index\ndownload and extract the index from [here](https://drive.google.com/file/d/1lpbBCPimwcNfC0VZOlQueA4SHNGIp5_t/view?usp=drive_link)\n\n### Install dependencies\n```\npip install .\n```\n## Running the Server\n\nThe server can be run directly:\n\n```bash\nuv --directory path/to/directory run jewish_library\n```\n\nOr through an MCP client that supports the Model Context Protocol.\nfor claude desktop app and cline you should use the following config:\n```\n{\n  \"mcpServers\": {        \n      \"jewish_library\": {\n          \"command\": \"uv\",\n          \"args\": [\n              \"--directory\",\n              \"your/path/to/directory\",\n              \"run\",\n              \"jewish_library\"\n          ],\n          \"env\": {\n            \"PYTHONIOENCODING\": \"utf-8\" \n          }\n      }\n  }\n}\n```\n\n## Available tools\n\nThe server provides a single tool through the MCP interface:\n\n### full_text_search\n\nPerforms a full-text search across the Jewish library with advanced query capabilities.\n\nExample query formats:\n```\n# Basic search\n\"maimonides on prayer\"\n\n# Field-specific search\ntext:\"love your neighbor\" AND topics:mitzvot\n\n# Required terms\n+shabbat +candles\n\n# Phrase search with topic filter\n\"four species\" AND topics:sukkot\n\n# Wildcard search\npray* AND reference:psalms\n```\n\nSearch results include:\n- Reference information\n- Relevant topics\n- Highlighted excerpts showing query matches\n- Relevance score\n\n## Development\n\nThis project uses:\n- [MCP SDK](https://github.com/modelcontextprotocol/sdk) for server implementation\n- [Tantivy](https://github.com/quickwit-oss/tantivy) for full-text search capabilities\n\n\n\n\n\n## Requirements\n\n- Python >= 3.10\n- MCP SDK >= 1.1.1\n- Tantivy search engine\n\n## License\n\nMIT License\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "search",
        "texts",
        "queries",
        "search results",
        "web search",
        "rich search"
      ],
      "category": "web-search"
    },
    "SleepyRabbit--playwright-mcp": {
      "owner": "SleepyRabbit",
      "name": "playwright-mcp",
      "url": "https://github.com/SleepyRabbit/playwright-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/SleepyRabbit.webp",
      "description": "Enables interaction with web pages through structured accessibility snapshots for browser automation without relying on vision models. Facilitates web navigation, form-filling, data extraction, and automated testing using structured data.",
      "stars": 0,
      "forks": 0,
      "license": "Apache License 2.0",
      "language": "",
      "updated_at": "2025-04-21T08:54:00Z",
      "readme_content": "## Playwright MCP\n\nA Model Context Protocol (MCP) server that provides browser automation capabilities using [Playwright](https://playwright.dev). This server enables LLMs to interact with web pages through structured accessibility snapshots, bypassing the need for screenshots or visually-tuned models.\n\n### Key Features\n\n- **Fast and lightweight**: Uses Playwright's accessibility tree, not pixel-based input.\n- **LLM-friendly**: No vision models needed, operates purely on structured data.\n- **Deterministic tool application**: Avoids ambiguity common with screenshot-based approaches.\n\n### Use Cases\n\n- Web navigation and form-filling\n- Data extraction from structured content\n- Automated testing driven by LLMs\n- General-purpose browser interaction for agents\n\n### Example config\n\n```js\n{\n  \"mcpServers\": {\n    \"playwright\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"@playwright/mcp@latest\"\n      ]\n    }\n  }\n}\n```\n\n\n#### Installation in VS Code\n\nInstall the Playwright MCP server in VS Code using one of these buttons:\n\n<!--\n// Generate using?:\nconst config = JSON.stringify({ name: 'playwright', command: 'npx', args: [\"-y\", \"@playwright/mcp@latest\"] });\nconst urlForWebsites = `vscode:mcp/install?${encodeURIComponent(config)}`;\n// Github markdown does not allow linking to `vscode:` directly, so you can use our redirect:\nconst urlForGithub = `https://insiders.vscode.dev/redirect?url=${encodeURIComponent(urlForWebsites)}`;\n-->\n\n[<img src=\"https://img.shields.io/badge/VS_Code-VS_Code?style=flat-square&label=Install%20Server&color=0098FF\" alt=\"Install in VS Code\">](https://insiders.vscode.dev/redirect?url=vscode%3Amcp%2Finstall%3F%257B%2522name%2522%253A%2522playwright%2522%252C%2522command%2522%253A%2522npx%2522%252C%2522args%2522%253A%255B%2522-y%2522%252C%2522%2540playwright%252Fmcp%2540latest%2522%255D%257D)  [<img alt=\"Install in VS Code Insiders\" src=\"https://img.shields.io/badge/VS_Code_Insiders-VS_Code_Insiders?style=flat-square&label=Install%20Server&color=24bfa5\">](https://insiders.vscode.dev/redirect?url=vscode-insiders%3Amcp%2Finstall%3F%257B%2522name%2522%253A%2522playwright%2522%252C%2522command%2522%253A%2522npx%2522%252C%2522args%2522%253A%255B%2522-y%2522%252C%2522%2540playwright%252Fmcp%2540latest%2522%255D%257D)\n\nAlternatively, you can install the Playwright MCP server using the VS Code CLI:\n\n```bash\n# For VS Code\ncode --add-mcp '{\"name\":\"playwright\",\"command\":\"npx\",\"args\":[\"@playwright/mcp@latest\"]}'\n```\n\n```bash\n# For VS Code Insiders\ncode-insiders --add-mcp '{\"name\":\"playwright\",\"command\":\"npx\",\"args\":[\"@playwright/mcp@latest\"]}'\n```\n\nAfter installation, the Playwright MCP server will be available for use with your GitHub Copilot agent in VS Code.\n\n### CLI Options\n\nThe Playwright MCP server supports the following command-line options:\n\n- `--browser <browser>`: Browser or chrome channel to use. Possible values:\n  - `chrome`, `firefox`, `webkit`, `msedge`\n  - Chrome channels: `chrome-beta`, `chrome-canary`, `chrome-dev`\n  - Edge channels: `msedge-beta`, `msedge-canary`, `msedge-dev`\n  - Default: `chrome`\n- `--caps <caps>`: Comma-separated list of capabilities to enable, possible values: tabs, pdf, history, wait, files, install. Default is all.\n- `--cdp-endpoint <endpoint>`: CDP endpoint to connect to\n- `--executable-path <path>`: Path to the browser executable\n- `--headless`: Run browser in headless mode (headed by default)\n- `--port <port>`: Port to listen on for SSE transport\n- `--user-data-dir <path>`: Path to the user data directory\n- `--vision`: Run server that uses screenshots (Aria snapshots are used by default)\n\n### User data directory\n\nPlaywright MCP will launch the browser with the new profile, located at\n\n```\n- `%USERPROFILE%\\AppData\\Local\\ms-playwright\\mcp-chrome-profile` on Windows\n- `~/Library/Caches/ms-playwright/mcp-chrome-profile` on macOS\n- `~/.cache/ms-playwright/mcp-chrome-profile` on Linux\n```\n\nAll the logged in information will be stored in that profile, you can delete it between sessions if you'd like to clear the offline state.\n\n\n### Running headless browser (Browser without GUI).\n\nThis mode is useful for background or batch operations.\n\n```js\n{\n  \"mcpServers\": {\n    \"playwright\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"@playwright/mcp@latest\",\n        \"--headless\"\n      ]\n    }\n  }\n}\n```\n\n### Running headed browser on Linux w/o DISPLAY\n\nWhen running headed browser on system w/o display or from worker processes of the IDEs,\nrun the MCP server from environment with the DISPLAY and pass the `--port` flag to enable SSE transport.\n\n```bash\nnpx @playwright/mcp@latest --port 8931\n```\n\nAnd then in MCP client config, set the `url` to the SSE endpoint:\n\n```js\n{\n  \"mcpServers\": {\n    \"playwright\": {\n      \"url\": \"http://localhost:8931/sse\"\n    }\n  }\n}\n```\n\n### Tool Modes\n\nThe tools are available in two modes:\n\n1. **Snapshot Mode** (default): Uses accessibility snapshots for better performance and reliability\n2. **Vision Mode**: Uses screenshots for visual-based interactions\n\nTo use Vision Mode, add the `--vision` flag when starting the server:\n\n```js\n{\n  \"mcpServers\": {\n    \"playwright\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"@playwright/mcp@latest\",\n        \"--vision\"\n      ]\n    }\n  }\n}\n```\n\nVision Mode works best with the computer use models that are able to interact with elements using\nX Y coordinate space, based on the provided screenshot.\n\n### Programmatic usage with custom transports\n\n```js\nimport { createServer } from '@playwright/mcp';\n\n// ...\n\nconst server = createServer({\n  launchOptions: { headless: true }\n});\ntransport = new SSEServerTransport(\"/messages\", res);\nserver.connect(transport);\n```\n\n### Snapshot-based Interactions\n\n- **browser_click**\n  - Description: Perform click on a web page\n  - Parameters:\n    - `element` (string): Human-readable element description used to obtain permission to interact with the element\n    - `ref` (string): Exact target element reference from the page snapshot\n\n- **browser_hover**\n  - Description: Hover over element on page\n  - Parameters:\n    - `element` (string): Human-readable element description used to obtain permission to interact with the element\n    - `ref` (string): Exact target element reference from the page snapshot\n\n- **browser_drag**\n  - Description: Perform drag and drop between two elements\n  - Parameters:\n    - `startElement` (string): Human-readable source element description used to obtain permission to interact with the element\n    - `startRef` (string): Exact source element reference from the page snapshot\n    - `endElement` (string): Human-readable target element description used to obtain permission to interact with the element\n    - `endRef` (string): Exact target element reference from the page snapshot\n\n- **browser_type**\n  - Description: Type text into editable element\n  - Parameters:\n    - `element` (string): Human-readable element description used to obtain permission to interact with the element\n    - `ref` (string): Exact target element reference from the page snapshot\n    - `text` (string): Text to type into the element\n    - `submit` (boolean, optional): Whether to submit entered text (press Enter after)\n    - `slowly` (boolean, optional): Whether to type one character at a time. Useful for triggering key handlers in the page. By default entire text is filled in at once.\n\n- **browser_select_option**\n  - Description: Select an option in a dropdown\n  - Parameters:\n    - `element` (string): Human-readable element description used to obtain permission to interact with the element\n    - `ref` (string): Exact target element reference from the page snapshot\n    - `values` (array): Array of values to select in the dropdown. This can be a single value or multiple values.\n\n- **browser_snapshot**\n  - Description: Capture accessibility snapshot of the current page, this is better than screenshot\n  - Parameters: None\n\n- **browser_take_screenshot**\n  - Description: Take a screenshot of the current page. You can't perform actions based on the screenshot, use browser_snapshot for actions.\n  - Parameters:\n    - `raw` (boolean, optional): Whether to return without compression (in PNG format). Default is false, which returns a JPEG image.\n\n### Vision-based Interactions\n\n- **browser_screen_move_mouse**\n  - Description: Move mouse to a given position\n  - Parameters:\n    - `element` (string): Human-readable element description used to obtain permission to interact with the element\n    - `x` (number): X coordinate\n    - `y` (number): Y coordinate\n\n- **browser_screen_capture**\n  - Description: Take a screenshot of the current page\n  - Parameters: None\n\n- **browser_screen_click**\n  - Description: Click left mouse button\n  - Parameters:\n    - `element` (string): Human-readable element description used to obtain permission to interact with the element\n    - `x` (number): X coordinate\n    - `y` (number): Y coordinate\n\n- **browser_screen_drag**\n  - Description: Drag left mouse button\n  - Parameters:\n    - `element` (string): Human-readable element description used to obtain permission to interact with the element\n    - `startX` (number): Start X coordinate\n    - `startY` (number): Start Y coordinate\n    - `endX` (number): End X coordinate\n    - `endY` (number): End Y coordinate\n\n- **browser_screen_type**\n  - Description: Type text\n  - Parameters:\n    - `text` (string): Text to type\n    - `submit` (boolean, optional): Whether to submit entered text (press Enter after)\n\n- **browser_press_key**\n  - Description: Press a key on the keyboard\n  - Parameters:\n    - `key` (string): Name of the key to press or a character to generate, such as `ArrowLeft` or `a`\n\n### Tab Management\n\n- **browser_tab_list**\n  - Description: List browser tabs\n  - Parameters: None\n\n- **browser_tab_new**\n  - Description: Open a new tab\n  - Parameters:\n    - `url` (string, optional): The URL to navigate to in the new tab. If not provided, the new tab will be blank.\n\n- **browser_tab_select**\n  - Description: Select a tab by index\n  - Parameters:\n    - `index` (number): The index of the tab to select\n\n- **browser_tab_close**\n  - Description: Close a tab\n  - Parameters:\n    - `index` (number, optional): The index of the tab to close. Closes current tab if not provided.\n\n### Navigation\n\n- **browser_navigate**\n  - Description: Navigate to a URL\n  - Parameters:\n    - `url` (string): The URL to navigate to\n\n- **browser_navigate_back**\n  - Description: Go back to the previous page\n  - Parameters: None\n\n- **browser_navigate_forward**\n  - Description: Go forward to the next page\n  - Parameters: None\n\n### Keyboard\n\n- **browser_press_key**\n  - Description: Press a key on the keyboard\n  - Parameters:\n    - `key` (string): Name of the key to press or a character to generate, such as `ArrowLeft` or `a`\n\n### Console\n\n- **browser_console_messages**\n  - Description: Returns all console messages\n  - Parameters: None\n\n### Files and Media\n\n- **browser_file_upload**\n  - Description: Choose one or multiple files to upload\n  - Parameters:\n    - `paths` (array): The absolute paths to the files to upload. Can be a single file or multiple files.\n\n- **browser_pdf_save**\n  - Description: Save page as PDF\n  - Parameters: None\n\n### Utilities\n\n- **browser_wait**\n  - Description: Wait for a specified time in seconds\n  - Parameters:\n    - `time` (number): The time to wait in seconds (capped at 10 seconds)\n\n- **browser_resize**\n  - Description: Resize the browser window\n  - Parameters:\n    - `width` (number): The desired width of the browser window\n    - `height` (number): The desired height of the browser window\n\n- **browser_handle_dialog**\n  - Description: Handle browser dialogs (alert, confirm, prompt)\n  - Parameters:\n    - `accept` (boolean): Whether to accept or dismiss the dialog\n    - `promptText` (string, optional): Text to enter in case of prompt dialogs\n\n- **browser_close**\n  - Description: Close the page\n  - Parameters: None\n\n- **browser_install**\n  - Description: Install the browser specified in the config. Call this if you get an error about the browser not being installed.\n  - Parameters: None\n",
      "npm_url": "https://www.npmjs.com/package/playwright-mcp",
      "npm_downloads": 88803,
      "keywords": [
        "accessibility",
        "browser",
        "web",
        "browser automation",
        "accessibility snapshots",
        "web navigation"
      ],
      "category": "web-search"
    },
    "SomeiLam--news-mcp": {
      "owner": "SomeiLam",
      "name": "news-mcp",
      "url": "https://github.com/SomeiLam/news-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/SomeiLam.webp",
      "description": "Aggregates and searches news articles, formatting them into Markdown digests using real-time news data. Supports fetching top headlines by various filters and performing full-text searches with optional criteria.",
      "stars": 0,
      "forks": 2,
      "license": "No License",
      "language": "TypeScript",
      "updated_at": "2025-05-03T04:52:13Z",
      "readme_content": "# NewsDigest MCP\n\nA **Model Context Protocol (MCP)** server written in TypeScript that aggregates, searches, and formats news articles using the NewsAPI.org REST endpoints.\n\n## Features\n\n- **Fetch Top Headlines** by country (ISO‑2 code), language (ISO‑639‑1 code), and category (e.g., business, sports).\n- **Search Everything** via full‑text queries (keywords) with optional filters: language, date range, sorting, page size.\n- **Format to Markdown**: Convert raw JSON responses into a clean, reader‑friendly Markdown digest using an LLM (Gemini and Claude).\n\n## Requirements\n\n- **Node.js 18+** (for built‑in `fetch` support)\n- **npm** or **yarn**\n- **NewsAPI.org API key** (free tier available)\n- **LLM API key** (Gemini and Claude)\n\n## Installation\n\n```\n# Clone the repo\ngit clone https://github.com/yourusername/newsdigest-mcp.git\ncd newsdigest-mcp\n\n# Install dependencies\nnpm install\n# or\n# yarn install\n```\n\n## Configuration\n\nCreate a `.env` file in the project root:\n```\n# .env\nNEWSAPI_KEY=<your_newsapi_key>\nGEMINI_API_KEY=<your_gemini_key>   # for Google Gemini\nANTHROPIC_API_KEY=<your_claude_key>  # for Anthropic Claude\n```\n\n## Usage\nStart the MCP server:\n```\nclaude\n```\n\nHere are some example user queries and response:\n\nQ: News about Bitcoin\n\nA: Latest Bitcoin News (May 1, 2025)\n\n1. El Salvador Committed to Buy Bitcoin Despite IMF Requirements\n  - El Salvador's economy minister reaffirmed the country's commitment to accumulate more\nBitcoin despite IMF requirements to reduce exposure\n  - The minister called it an \"important project\" started by President Nayib Bukele\n2. North Carolina Passes Bill to Integrate Bitcoin Into $127 Billion Pension Fund\n  - North Carolina House approved legislation allowing state pension fund to invest in\ncryptocurrencies\n  - The bill modernizes investment strategy for the state's $127 billion pension fund\n3. US Could Buy $100 Billion in Bitcoin This Year, According to Coinbase Executive\n  - Sebastian Bea, president of Coinbase Asset Management, suggests a technical change in\nhow Washington accounts for gold could enable a strategic Bitcoin purchase\n  - The proposed purchase could approach $100 billion, years ahead of market expectations\n4. North Carolina House Passes State Crypto Investment Bill\n  - The Digital Assets Investment Act (House Bill 92) passed on its third reading\n  - Bill allows the state treasurer to invest public funds in approved cryptocurrencies\n  - Now moving to the Senate for consideration\n5. Crypto Industry Event in Dubai as \"Trump Euphoria\" Shows Signs of Receding\n  - Crypto enthusiasts gathered in Dubai for TOKEN2049 conference\n  - Industry showing mixed sentiments about Donald Trump's impact on cryptocurrencies\n  - Despite strong start to the year with significant venture capital investments\n6. Malaysian Police Dismantle Bitcoin Mining Operation Stealing Electricity\n  - Police in Terengganu, Malaysia dismantled a syndicate stealing electricity for illegal\nbitcoin mining\n  - Operations were discovered in Hulu Terengganu and Marang regions",
      "npm_url": "https://www.npmjs.com/package/news-mcp-server",
      "npm_downloads": 4630,
      "keywords": [
        "searches",
        "headlines",
        "news",
        "searches news",
        "news data",
        "fetching headlines"
      ],
      "category": "web-search"
    },
    "SpaceFrontiers--mcp": {
      "owner": "SpaceFrontiers",
      "name": "mcp",
      "url": "https://github.com/SpaceFrontiers/mcp",
      "imageUrl": "/freedevtools/mcp/pfp/SpaceFrontiers.webp",
      "description": "Interact with Space Frontiers data sources through a simple API for keyword and semantic searches across various databases. Enables seamless access to rich space-related information to enhance language model capabilities.",
      "stars": 6,
      "forks": 3,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-09-27T21:39:03Z",
      "readme_content": "# Space Frontiers MCP Server\n\n## General Overview\n\nThis project implements a Model Context Protocol (MCP) server that acts as an interface to the Space Frontiers API. It allows language models to interact with Space Frontiers data sources through MCP tools. The server is built using the FastMCP library.\n\nAt a high level, the server provides LLM-accessible search and discovery across Space Frontiers sources such as scholarly literature, Telegram, and Reddit, including both query-based search and recent-item retrieval. Tools are self-describing via MCP schemas and annotations; your MCP client can list and introspect them at runtime.\n\n**Hosted option:** Space Frontiers provides a publicly hosted MCP server at `https://mcp.spacefrontiers.org`. Obtain an API key from `https://spacefrontiers.org/developers/keys` and include it via the `Authorization: Bearer <your_api_key>` header.\n\n## Environment Variables\n\nThe server utilizes the following environment variables:\n\n*   `SPACE_FRONTIERS_API_ENDPOINT`: The base URL for the Space Frontiers API.\n    *   **Default:** `https://api.spacefrontiers.org`\n*   `SPACE_FRONTIERS_API_KEY`: An optional API key for authenticating requests to the Space Frontiers API.\n    *   **Note:** Authentication can also be provided via request headers:\n        *   `Authorization: Bearer <your_api_key>`\n        *   `X-Api-Key: <your_api_key>`\n        *   Alternatively, a user ID can be provided via the `X-User-Id` header. If none of these are provided, the server will attempt to use the `SPACE_FRONTIERS_API_KEY` environment variable if set.\n        *   **Note on `X-User-Id`:** This header is intended for Space Frontiers internal usage only and cannot be exploited for external authentication.\n\n## Running the Server\n\n```bash\nuv run fastmcp run mcp_server.py\n```\n\nEnsure `SPACE_FRONTIERS_API_KEY` is set in the environment if your client does not pass authentication headers.\n\n### Example Claude Desktop App Configuration (`claude_desktop_config.json`)\n\n```json\n{\n  \"mcpServers\": {\n    \"Space Frontiers MCP server\": {\n      \"command\": \"/path/to/your/uv\",\n      \"args\": [\n        \"run\",\n        \"fastmcp\",\n        \"run\",\n        \"--with\",\n        \"izihawa-loglib\",\n        \"--with\",\n        \"mcp[cli]\",\n        \"--with\",\n        \"spacefrontiers-clients\",\n        \"/path/to/your/spacefrontiers-mcp/mcp_server.py\"\n      ],\n      \"env\": {\n        \"SPACE_FRONTIERS_API_KEY\": \"YOUR_API_KEY_HERE\"\n      }\n    }\n  }\n}\n```\n\nNote: replace placeholder paths and the API key with your actual values.",
      "npm_url": "https://www.npmjs.com/package/mcp",
      "npm_downloads": 17624,
      "keywords": [
        "spacefrontiers",
        "space",
        "searches",
        "search spacefrontiers",
        "semantic searches",
        "space frontiers"
      ],
      "category": "web-search"
    },
    "TencentEdgeOne--edgeone-pages-mcp": {
      "owner": "TencentEdgeOne",
      "name": "edgeone-pages-mcp",
      "url": "https://github.com/TencentEdgeOne/edgeone-pages-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/TencentEdgeOne.webp",
      "description": "Deploy HTML content and static files to EdgeOne Pages, generating publicly accessible URLs for content delivery. Supports serverless edge functions for improved web performance.",
      "stars": 302,
      "forks": 36,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-10-03T03:55:11Z",
      "readme_content": "# EdgeOne Pages MCP\n\nAn MCP service for deploying HTML content, folders, or full-stack projects to EdgeOne Pages and obtaining publicly accessible URLs.\n\n<a href=\"https://glama.ai/mcp/servers/@TencentEdgeOne/edgeone-pages-mcp\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@TencentEdgeOne/edgeone-pages-mcp/badge\" alt=\"EdgeOne Pages MCP server\" />\n</a>\n\n## Demo\n\n### Deploy HTML\n\n![U_GpJ_1746519327306](https://cdnstatic.tencentcs.com/edgeone/pages/assets/U_GpJ-1746519327306.gif)\n\n### Deploy Folder\n\n![kR_Kk_1746519251292](https://cdnstatic.tencentcs.com/edgeone/pages/assets/kR_Kk-1746519251292.gif)\n\n## Requirements\n\n- Node.js 18 or higher\n\n## MCP Configuration\n\n### stdio MCP Server\n\nFull-featured MCP service that supports the `deploy_folder` tool for deploying full-stack projects.\n\n```jsonc\n// Tencent Cloud International (Default)\n{\n  \"mcpServers\": {\n    \"edgeone-pages-mcp-server\": {\n      \"timeout\": 600,\n      \"command\": \"npx\",\n      \"args\": [\"edgeone-pages-mcp-fullstack\"]\n    }\n  }\n}\n\n// Tencent Cloud China\n{\n  \"mcpServers\": {\n    \"edgeone-pages-mcp-server\": {\n      \"timeout\": 600,\n      \"command\": \"npx\",\n      \"args\": [\"edgeone-pages-mcp-fullstack\", \"--region\", \"china\"]\n    }\n  }\n}\n```\n\nThe following MCP Server will be deprecated soon:\n\nSupports both `deploy_html` and `deploy_folder_or_zip` tools.\n\n```jsonc\n{\n  \"mcpServers\": {\n    \"edgeone-pages-mcp-server\": {\n      \"command\": \"npx\",\n      \"args\": [\"edgeone-pages-mcp\"],\n      \"env\": {\n        // Optional. \n        // If you need to deploy folders or zip files to \n        // EdgeOne Pages projects, provide your EdgeOne Pages API token.\n        // How to obtain your API token: \n        // https://edgeone.ai/document/177158578324279296\n        \"EDGEONE_PAGES_API_TOKEN\": \"\",\n        // Optional. Leave empty to create a new EdgeOne Pages project.\n        // Provide a project name to update an existing project.\n        \"EDGEONE_PAGES_PROJECT_NAME\": \"\"\n      }\n    }\n  }\n}\n```\n\n### Streaming HTTP MCP Server\n\nFor MCP clients that support HTTP streaming, only supports the `deploy_html` tool.\n\n```json\n{\n  \"mcpServers\": {\n    \"edgeone-pages-mcp-server\": {\n      \"url\": \"https://mcp-on-edge.edgeone.site/mcp-server\"\n    }\n  }\n}\n```\n\n## Tool Details\n\n### deploy_html Tool\n\n#### Architecture Design\n\n\n\nThe architecture diagram shows the complete workflow of the `deploy_html` tool:\n\n1. Large Language Model generates HTML content\n2. Content is sent to the EdgeOne Pages MCP Server\n3. MCP Server deploys the content to EdgeOne Pages Edge Functions\n4. Content is stored in EdgeOne KV Store for fast edge access\n5. MCP Server returns a publicly accessible URL\n6. Users can access the deployed content via browser with fast edge delivery\n\n#### Implementation Details\n\nThis tool integrates with EdgeOne Pages Functions to deploy static HTML content:\n\n1. **EdgeOne Pages Functions** - A serverless computing platform that supports executing JavaScript/TypeScript code at the edge\n\n2. **Core Implementation Features**:\n\n   - Uses EdgeOne Pages KV storage to save and serve HTML content\n   - Automatically generates publicly accessible URLs for each deployment\n   - Provides comprehensive API error handling and feedback\n\n3. **How It Works**:\n   - MCP server receives HTML content through the `deploy_html` tool\n   - Connects to EdgeOne Pages API to obtain the base URL\n   - Deploys HTML content using the EdgeOne Pages KV API\n   - Returns an immediately accessible public URL\n\nFor more information, refer to the [EdgeOne Pages Functions documentation](https://edgeone.ai/document/162227908259442688) and [EdgeOne Pages KV Storage Guide](https://edgeone.ai/document/162227803822321664).\n\nThe source code is open source and can be self-deployed with custom domain binding: https://github.com/TencentEdgeOne/self-hosted-pages-mcp\n\n### deploy_folder Tool\n\nThis tool supports deploying complete projects to EdgeOne Pages:\n\n- Supports full deployment of static website projects\n- Supports deployment of full-stack applications\n- Option to update existing projects or create new ones\n\n## License\n\nMIT",
      "npm_url": "https://www.npmjs.com/package/edgeone-pages-mcp",
      "npm_downloads": 36380,
      "keywords": [
        "edgeone",
        "edge",
        "tencentedgeone",
        "edgeone pages",
        "serverless edge",
        "files edgeone"
      ],
      "category": "web-search"
    },
    "The-AI-Workshops--searxng-mcp-server": {
      "owner": "The-AI-Workshops",
      "name": "searxng-mcp-server",
      "url": "https://github.com/The-AI-Workshops/searxng-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/The-AI-Workshops.webp",
      "description": "Integrates with a SearXNG instance to enable AI agents to perform privacy-respecting web searches. Provides a standardized MCP server interface for seamless interaction within AI workflows.",
      "stars": 11,
      "forks": 5,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-09-25T13:19:59Z",
      "readme_content": "# SearXNG MCP Server\n\nAn MCP sse implementation of the Model Context Protocol (MCP) server integrated with [SearXNG](https://github.com/searxng/searxng) for providing AI agents with powerful, privacy-respecting search capabilities.\n\n---\n\n## Overview\n\nThis project demonstrates how to build an MCP server that enables AI agents to perform web searches using a SearXNG instance. It serves as a practical template for creating your own MCP servers, using SearXNG as a backend.\n\nThe implementation follows the best practices laid out by Anthropic for building MCP servers, allowing seamless integration with any MCP-compatible client.\n\n---\n\n## Prerequisites\n\n- Python 3.9+\n- Access to a running SearXNG instance (local or remote)\n- Docker (optional, for containerized deployment)\n- [uv](https://github.com/astral-sh/uv) (optional, for fast Python dependency management)\n- [Smithery](https://github.com/The-AI-Workshops/smithery) (optional, for MCP server management)\n\n### SearXNG Server (Required)\n\nYou must have a SearXNG server running and accessible. The recommended way is via Docker:\n\n```bash\ndocker run -d --name=searxng -p 32768:8080 -v \"/root/searxng:/etc/searxng\" \\\n  -e \"BASE_URL=http://0.0.0.0:32768/\" \\\n  -e \"INSTANCE_NAME=home\" \\\n  --restart always searxng/searxng\n```\n\n- This will run SearXNG on port 32768 and persist configuration in `/root/searxng`.\n- The MCP server expects SearXNG to be available at `http://172.17.0.1:32768` by default (see `.env`).\n\n---\n\n## Installation\n\n### Using uv\n\nInstall uv if you don't have it:\n\n```bash\npip install uv\n```\n\nClone this repository:\n\n```bash\ngit clone https://github.com/The-AI-Workshops/searxng-mcp-server.git\ncd searxng-mcp-server/dev/searXNG-mcp\n```\n\nInstall dependencies:\n\n```bash\nuv pip install -r requirements.txt\n```\n\nCreate a `.env` file based on the provided example:\n\n```bash\nnano .env\n# Edit .env as needed\n```\n\nConfigure your environment variables in the `.env` file (see Configuration section).\n\n---\n\n### Using Docker (Recommended)\n\nBuild the Docker image:\n\n```bash\ndocker build -t mcp/searxng-mcp .\n```\n\nCreate a `.env` file and configure your environment variables.\n\n---\nRun the Docker image:\n\n```bash\ndocker run -d --env-file ./.env -p 32769:32769 mcp/searxng-mcp\n```\n\n---\n\n### Using Smithery\n\n[Smithery](https://github.com/The-AI-Workshops/smithery) is a command-line tool for managing AI agent tools and MCP servers.\n\nInstall Smithery if you don't have it (see Smithery documentation for various installation methods, e.g., using pipx):\n```bash\npipx install smithery\n```\n\nInstall the SearXNG MCP server using Smithery:\n```bash\nsmithery install @The-AI-Workshops/searxng-mcp-server\n```\nThis will install the server and its dependencies into a dedicated environment managed by Smithery.\n\nAfter installation, Smithery will provide you with the path to the installed server. You will need to navigate to this directory to configure it. For example, if Smithery installs tools into `~/.smithery/tools/`, the path might be `~/.smithery/tools/The-AI-Workshops/searxng-mcp-server`.\n\nCreate a `.env` file in the server's directory by copying the example:\n```bash\n# Example:\n# cd ~/.smithery/tools/The-AI-Workshops/searxng-mcp-server\ncp .env.example .env\nnano .env\n# Edit .env as needed\n```\nConfigure your environment variables in the `.env` file (see Configuration section).\n\n---\n\n## Configuration\n\nThe following environment variables can be configured in your `.env` file:\n\n| Variable           | Description                                 | Example                                 |\n|--------------------|---------------------------------------------|-----------------------------------------|\n| SEARXNG_BASE_URL   | Base URL of your SearXNG instance           | http://172.17.0.1:32768                 |\n| HOST               | Host to bind to when using SSE transport    | 0.0.0.0                                 |\n| PORT               | Port to listen on when using SSE transport  | 32769                                   |\n| TRANSPORT          | Transport protocol (sse or stdio)           | sse                                     |\n\n---\n\n## Running the Server\n\n### Using uv\n\n**SSE Transport**\n\nSet `TRANSPORT=sse` in `.env` then:\n\n```bash\nuv run dev/searXNG-mcp/server.py\n```\n\n**Stdio Transport**\n\nWith stdio, the MCP client itself can spin up the MCP server, so nothing to run at this point.\n\n---\n\n### Using Docker\n\n**SSE Transport**\n\n```bash\ndocker build -t mcp/searxng-mcp .\ndocker run --rm -it -p 32769:32769 --env-file dev/searXNG-mcp/.env -v $(pwd)/dev/searXNG-mcp:/app mcp/searxng-mcp\n```\n\n- The `-v $(pwd)/dev/searXNG-mcp:/app` mount allows you to live-edit the code and .env file on your host and have changes reflected in the running container.\n- The server will be available at `http://localhost:32769/sse`.\n\n**Stdio Transport**\n\nWith stdio, the MCP client itself can spin up the MCP server container, so nothing to run at this point.\n\n---\n\n### Running with Smithery\n\n**SSE Transport**\n\nSet `TRANSPORT=sse` in `.env` in the Smithery-installed server directory.\nThen, you can typically run the server using the Python interpreter from the virtual environment Smithery created for the tool:\n```bash\n# Navigate to the server directory, e.g.,\n# cd ~/.smithery/tools/The-AI-Workshops/searxng-mcp-server\n~/.smithery/venvs/The-AI-Workshops_searxng-mcp-server/bin/python server.py\n```\nAlternatively, if Smithery provides a direct run command for installed tools (check Smithery documentation):\n```bash\nsmithery run @The-AI-Workshops/searxng-mcp-server\n```\nThe server will be available based on your HOST and PORT settings in `.env` (e.g., `http://localhost:32769/sse`).\n\n**Stdio Transport**\n\nWith stdio, the MCP client itself will spin up the server. The client configuration will need to point to the `server.py` script within the Smithery-managed directory, potentially using `smithery exec` or the direct path to the Python interpreter in the tool's virtual environment. See the \"Integration with MCP Clients\" section for examples.\n\n---\n\n## Integration with MCP Clients\n\n### SSE Configuration\n\nOnce you have the server running with SSE transport, you can connect to it using this configuration:\n\n```json\n{\n  \"mcpServers\": {\n    \"searxng\": {\n      \"transport\": \"sse\",\n      \"url\": \"http://localhost:32769/sse\"\n    }\n  }\n}\n```\n\n**Note for Windsurf users:** Use `serverUrl` instead of `url` in your configuration:\n\n```json\n{\n  \"mcpServers\": {\n    \"searxng\": {\n      \"transport\": \"sse\",\n      \"serverUrl\": \"http://localhost:32769/sse\"\n    }\n  }\n}\n```\n\n**Note for n8n users:** Use `host.docker.internal` instead of `localhost` since n8n has to reach outside of its own container to the host machine:\n\nSo the full URL in the MCP node would be: `http://host.docker.internal:32769/sse`\n\nMake sure to update the port if you are using a value other than the default 32769.\n\n---\n\n### Python with Stdio Configuration\n\nAdd this server to your MCP configuration for Claude Desktop, Windsurf, or any other MCP client:\n\n```json\n{\n  \"mcpServers\": {\n    \"searxng\": {\n      \"command\": \"python\",\n      \"args\": [\"dev/searXNG-mcp/server.py\"],\n      \"env\": {\n        \"TRANSPORT\": \"stdio\",\n        \"SEARXNG_BASE_URL\": \"http://localhost:32768\",\n        \"HOST\": \"0.0.0.0\",\n        \"PORT\": \"32769\"\n      }\n    }\n  }\n}\n```\n\n---\n\n### Docker with Stdio Configuration\n\n```json\n{\n  \"mcpServers\": {\n    \"searxng\": {\n      \"command\": \"docker\",\n      \"args\": [\"run\", \"--rm\", \"-i\",\n               \"-e\", \"TRANSPORT\",\n               \"-e\", \"SEARXNG_BASE_URL\",\n               \"-e\", \"HOST\",\n               \"-e\", \"PORT\",\n               \"mcp/searxng-mcp\"],\n      \"env\": {\n        \"TRANSPORT\": \"stdio\",\n        \"SEARXNG_BASE_URL\": \"http://localhost:32768\",\n        \"HOST\": \"0.0.0.0\",\n        \"PORT\": \"32769\"\n      }\n    }\n  }\n}\n```\n\n---\n\n### Smithery with Stdio Configuration\n\nIf you installed the server using Smithery, you can configure your MCP client to run it via stdio. Smithery provides an `exec` command to run executables from within the tool's environment.\n\n```json\n{\n  \"mcpServers\": {\n    \"searxng\": {\n      \"command\": \"smithery\",\n      \"args\": [\"exec\", \"@The-AI-Workshops/searxng-mcp-server\", \"--\", \"python\", \"server.py\"],\n      // \"cwd\" (current working directory) might be automatically handled by Smithery.\n      // If server.py is in a subdirectory, adjust the python script path e.g., \"python\", \"path/to/server.py\"\n      \"env\": {\n        \"TRANSPORT\": \"stdio\",\n        \"SEARXNG_BASE_URL\": \"http://localhost:32768\", // Adjust as needed\n        \"HOST\": \"0.0.0.0\", // Typically not used by stdio server itself but good to set\n        \"PORT\": \"32769\"  // Typically not used by stdio server itself\n      }\n    }\n  }\n}\n```\nAlternatively, you can find the path to the Python interpreter in the virtual environment created by Smithery (e.g., `~/.smithery/venvs/The-AI-Workshops_searxng-mcp-server/bin/python`) and the path to `server.py` (e.g., `~/.smithery/tools/The-AI-Workshops/searxng-mcp-server/server.py`) and use those directly:\n```json\n{\n  \"mcpServers\": {\n    \"searxng\": {\n      \"command\": \"~/.smithery/venvs/The-AI-Workshops_searxng-mcp-server/bin/python\",\n      \"args\": [\"~/.smithery/tools/The-AI-Workshops/searxng-mcp-server/server.py\"],\n      // \"cwd\" should be the directory containing server.py if not using absolute paths for args,\n      // or if server.py relies on relative paths for other files (like .env).\n      // Example: \"cwd\": \"~/.smithery/tools/The-AI-Workshops/searxng-mcp-server\",\n      \"env\": {\n        \"TRANSPORT\": \"stdio\",\n        \"SEARXNG_BASE_URL\": \"http://localhost:32768\"\n        // Other necessary env vars from .env can be duplicated here\n      }\n    }\n  }\n}\n```\nEnsure the paths are correct for your Smithery installation and that the `.env` file is discoverable by `server.py` (usually by setting `cwd` to the server's root directory or ensuring `server.py` loads it from an absolute path if Smithery sets one).\n\n---\n\n## Building Your Own Server\n\nThis template provides a foundation for building more complex MCP servers. To build your own:\n\n- Add your own tools by creating methods with the `@mcp.tool()` decorator\n- Create your own lifespan function to add your own dependencies (clients, database connections, etc.)\n- Add prompts and resources as well with `@mcp.resource()` and `@mcp.prompt()`\n\n---\n\n## SearXNG Search Tool Parameters\n\nThe `search` tool supports the following parameters (all optional except `q`):\n\n- `q` (required): The search query string.\n- `categories`: Comma-separated list of active search categories.\n- `engines`: Comma-separated list of active search engines.\n- `language`: Code of the language.\n- `page`: Search page number (default: 1).\n- `time_range`: [day, month, year]\n- `format`: [json, csv, rss] (default: json)\n- `results_on_new_tab`: [0, 1]\n- `image_proxy`: [true, false]\n- `autocomplete`: [google, dbpedia, duckduckgo, mwmbl, startpage, wikipedia, stract, swisscows, qwant]\n- `safesearch`: [0, 1, 2]\n- `theme`: [simple]\n- `enabled_plugins`: List of enabled plugins.\n- `disabled_plugins`: List of disabled plugins.\n- `enabled_engines`: List of enabled engines.\n- `disabled_engines`: List of disabled engines.\n\nSee the [SearXNG documentation](https://docs.searxng.org/) for more details.\n\n---\n\n## License\n\nMIT License\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "searxng",
        "searches",
        "ai",
        "ai workflows",
        "search ai",
        "searxng mcp"
      ],
      "category": "web-search"
    },
    "Tomatio13--mcp-server-tavily": {
      "owner": "Tomatio13",
      "name": "mcp-server-tavily",
      "url": "https://github.com/Tomatio13/mcp-server-tavily",
      "imageUrl": "/freedevtools/mcp/pfp/Tomatio13.webp",
      "description": "Perform web searches and retrieve information using the Tavily API, returning results in text format along with AI responses, URIs, and titles.",
      "stars": 43,
      "forks": 11,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-14T15:59:56Z",
      "readme_content": "# tavily-search MCP server\n\nA MCP server project\n\n[![Trust Score](https://archestra.ai/mcp-catalog/api/badge/quality/Tomatio13/mcp-server-tavily)](https://archestra.ai/mcp-catalog/tomatio13__mcp-server-tavily)\n<a href=\"https://glama.ai/mcp/servers/s0hka6zney\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/s0hka6zney/badge\" alt=\"tavily-search MCP server\" /></a>\n\n## Components\n\nThis server uses the Tavily API to perform searches based on specified queries.\n- Search results are returned in text format.\n- Search results include AI responses, URIs, and titles of the search results.\n\n### Tools\n\nThis server implements the following tools:\n- search: Performs searches based on specified queries\n  - Required argument: \"query\"\n  - Optional argument: \"search_depth\" (basic or advanced)\n\n### Installing via Smithery\n\nTo install Tavily Search for Claude Desktop automatically via [Smithery](https://smithery.ai/server/tavily-search):\n\n```bash\nnpx -y @smithery/cli install tavily-search --client claude\n```\n\n### Install\n\n1. Download the repository.\n```bash\ngit clone https://github.com/Tomatio13/mcp-server-tavily.git\n``` \n2. Open the Claude Desktop configuration file.\n```\nOn MacOS: `~/Library/Application\\ Support/Claude/claude_desktop_config.json`\nOn Windows: `C:\\Users\\[username]\\AppData\\Roaming\\Claude\\claude_desktop_config.json`\n```\n\n3. Edit the configuration file as follows:\n  ```yaml\n  \"mcpServers\": {\n    \"tavily-search\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"--directory\",\n        \"C:\\\\your_path\\\\mcp-server-tavily\",\n        \"run\",\n        \"tavily-search\"\n      ],\n      \"env\": {\n        \"TAVILY_API_KEY\": \"YOUR_TAVILY_API_KEY\",\n        \"PYTHONIOENCODING\": \"utf-8\"\n      }\n    }\n  }\n  ```\n\n4. Restart Claude Desktop.\n\n## Usage\n\nIn Claude Desktop, when you ask \"Please search for something\", you will receive search results.\n\nSearch example:\n```\nPlease search in detail for today's events in Kamakura\n```\nResponse example:\n```\nAccording to the search results, the following events start today, December 1st:\n\"Kamakura Promotion Photo Contest 2025\"\nPeriod: December 1, 2024 - January 31, 2025\nA photo contest for those who love Kamakura\nApplications start accepting from today\nAlso, as a related upcoming event:\nOn December 7th, an exhibition by 12 Kamakura artists will be held at the Seibu Press Inn Kamakura Ofuna Station East Exit Lounge.\n```\n\n## Log Storage Location\n\nLogs are stored in the following location:\n\nFor Windows:\n```\nC:\\Users\\[username]\\AppData\\Roaming\\Claude\\logs\\mcp-server-tavily-search\n```\n## Execution with Cursor\n\n1. Create a shell script (e.g., `script.sh`) as shown below:\n\n```bash\n#!/bin/bash\nTARGET_DIR=/path/to/mcp-server-tavily\ncd \"${TARGET_DIR}\"\nexport TAVILY_API_KEY=\"your-api-key\"\nexport PYTHONIOENCODING=utf-8\nuv --directory $PWD run tavily-search\n```\n\n2. Configure Cursor's MCP Server settings as follows:\n\n```\nName: tavily-search\nType: command\nCommand: /path/to/your/script.sh\n```\n\n3. Save the settings.\n\n4. Once the settings are saved, you can ask Cursor's Composer-Agent to \"search for something,\" and it will return the search results.\n\n## Running in Local Environment Using Docker Compose\n\n### Purpose\nFor operating systems other than Windows/MacOS where Claude Desktop cannot be used,\nthis section explains how to set up and run an MCP server and client in a local environment\nusing Docker compose.\n\n### Steps\n1. Install Docker.\n2. Download the repository.\n```bash\ngit clone https://github.com/Tomatio13/mcp-server-tavily.git\n``` \n3. Run Docker compose.\n```bash\ndocker compose up -d\n``` \n4. Execute the client.\n```bash\ndocker exec mcp_server uv --directory /usr/src/app/mcp-server-tavily/src run client.py\n```\n5. Execution Results\n6. After searching for available tools as shown below, a query will be issued to Tavily and a response will be returned:\n```bash\n2024-12-01 11:21:56,930 - tavily-search-server - INFO - Starting Tavily search server\n2024-12-01 11:21:56,932 - tavily-search-server - INFO - Server initialized, starting main loop\n2024-12-01 11:21:56,936 - mcp.server - INFO - Processing request of type ListToolsRequest\n2024-12-01 11:21:56,936 - tavily-search-server - INFO - Listing available tools\n利用可能なツール: nextCursor=None tools=[Tool(name='search', description='Search the web using Tavily API', inputSchema={'type': 'object', 'properties': {'query': {'type': 'string', 'description': 'Search query'}, 'search_depth': {'type': 'string', 'description': 'Search depth (basic or advanced)', 'enum': ['basic', 'advanced']}}, 'required': ['query']})]\n2024-12-01 11:21:56,937 - mcp.server - INFO - Processing request of type CallToolRequest\n2024-12-01 11:21:56,937 - tavily-search-server - INFO - TOOL_CALL_DEBUG: Tool called - name: search, arguments: {'query': '今日の東京タワーのイベントを教えて下さい'}\n2024-12-01 11:21:56,937 - tavily-search-server - INFO - Executing search with query: '今日の東京タワーのイベントを教えて下さい'\n2024-12-01 11:22:00,243 - httpx - INFO - HTTP Request: POST https://api.tavily.com/search \"HTTP/1.1 200 OK\"\n2024-12-01 11:22:00,243 - tavily-search-server - INFO - Search successful - Answer generated\n2024-12-01 11:22:00,243 - tavily-search-server - INFO - Search successful - Results available\nツール実行結果: content=[TextContent(type='text', text='AI Answer:\\n今日の東京タワーのイベントは以下の通りです：\\n1. Candlelight: エド・シーランとコールドプレイのヒットメドレー - 12月01日\\n2. チームラボプラネッツ TOKYO - 12月01日から1月21日\\n\\n他にもイベントがある可能性がありますので、公式ウェブサイト等で最新情報をご確認ください。\\n\\n\\n\\nSearch Results:\\n\\n1. 東京タワー (東京): 現在のイベントとチケット | Fever\\nURL: https://feverup.com/ja/tokyo/venue/tokyo-tower\\nSummary: Summary not found\\n\\n\\n2. 東京タワー(東京都)の施設で開催するイベント一覧｜ウォーカープラス\\nURL: https://www.walkerplus.com/spot/ar0313s03867/e_list.html\\nSummary: Summary not found\\n\\n\\n3. 東京タワー - Tokyo Tower\\nURL: https://www.tokyotower.co.jp/event/\\nSummary: Summary not found\\n')] isError=False\n``` \n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "tomatio13",
        "searches",
        "api",
        "search tomatio13",
        "tavily api",
        "tomatio13 mcp"
      ],
      "category": "web-search"
    },
    "Victorzwx--zh_mcp_server": {
      "owner": "Victorzwx",
      "name": "zh_mcp_server",
      "url": "https://github.com/Victorzwx/zh_mcp_server",
      "imageUrl": "/freedevtools/mcp/pfp/Victorzwx.webp",
      "description": "Automates article generation and posting on Zhihu using large language models. Integrates with MCP clients to facilitate content creation workflows through automated browser interactions and cookie management.",
      "stars": 17,
      "forks": 1,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-09-19T11:47:03Z",
      "readme_content": "# zh_mcp_server介绍\n一种用于知乎发文章的模型上下文协议（MCP）服务器，使用者可以通过该服务与大模型自动生成文章并在知乎发文章。\n\n# 使用方法\n\n## 1. 克隆代码\n\n```\ngit https://github.com/Victorzwx/zh_mcp_server.git\n```\n\n## 2. 环境配置前提\n\n建议在Windows环境下运行\n  \npython版本要求 >= 3.10\n\n- **方式1：**\n\n配置环境要求满足 requirements.txt 文件的要求\n  - selenium>=4.0.0\n  - requests>=2.25.1\n  - mcp>=0.1.0\n  - webdriver-manager>=3.8.0\n可以通过如下代码安装：\n```\npip install -r requirements.txt\n```\n然后安装ChromeDriver，本项目依赖于谷歌浏览器，134.0.6998.166是版本号，需要手动查询使用者电脑上的谷歌浏览器版本\n```\nnpx @puppeteer/browsers install chromedriver@134.0.6998.166\n```\n- **方式2：**\n  \n运行setup_environment.py，如果该方式失败则有可能是ChromeDriver版本不正确，建议以方式1重新安装\n```\npython setup_environment.py\n```\n\n## 3.保存个人cookie\n\n在该代码文件夹下运行保存cookie的代码：\n\n```\npython -m zh_mcp_server.__login__\n```\n- 注意运行后会自动打开谷歌浏览器\n- 在谷歌浏览器输入使用者的手机账号，然后点击获取验证码\n- 然后，将得到的**验证码输入到Terminal**，即运行python -m zh_mcp_server.__login__的终端，这很重要！\n\n## 4. 在MCP客户端（如Cherry Studio）配置MCP服务\n通过python的方式运行\n```\n\"zh_mcp_server\": {\n      \"command\": \"python\",\n      \"args\": [\n        \"-m\",\n        \"zh_mcp_server\"\n      ]\n }\n```\n然后就可以使用了\n\n如果是通过代码使用该MCP服务，如基于Spring AI的JAVA代码，还需要加上编码方式，以避免生成乱码：\n```\n\"zh_mcp_server\": {\n      \"command\": \"D:\\\\aconda\\\\python.exe\",\n      \"args\": [\n        \"-m\",\n        \"zh_mcp_server\",\n        \"--encoding=utf-8\"\n      ],\n      \"env\": {\n        \"PYTHONIOENCODING\": \"utf-8\"\n      }\n    }\n```\n\n# 调试\n如果需要调试大模型调用该MCP服务时的具体过程或者可视化浏览器的操作，需要关闭无头浏览器模式，如下：\n```\nposter = ZhuHuPoster(path, headless=True)##如果要调试，请设置为False\n```\n代码位于server.py中\n# CSDN\n[本人CSDN账号](https://blog.csdn.net/qq_61302385?type=blog)\n\n# 微信\n![image](https://github.com/user-attachments/assets/f7a51982-917f-48b1-9d1f-9f90dc02143f)\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "zhihu",
        "zh_mcp_server",
        "search",
        "automates article",
        "article generation",
        "zh_mcp_server automates"
      ],
      "category": "web-search"
    },
    "VinhPhamAI--brave-search": {
      "owner": "VinhPhamAI",
      "name": "brave-search",
      "url": "https://github.com/VinhPhamAI/brave-search",
      "imageUrl": "/freedevtools/mcp/pfp/null.webp",
      "description": "Integrates the Brave Search API to provide web and local search capabilities, including general queries, news and articles searches, and detailed local business and service lookups with pagination and filtering options.",
      "stars": 0,
      "forks": 0,
      "license": "Unknown",
      "language": "Unknown",
      "updated_at": "",
      "readme_content": "",
      "npm_url": "https://www.npmjs.com/package/brave-search",
      "npm_downloads": 50744,
      "keywords": [
        "searches",
        "search",
        "api",
        "brave search",
        "search api",
        "search vinhphamai"
      ],
      "category": "web-search"
    },
    "VivekKumarNeu--MCP-Lucene-Server": {
      "owner": "VivekKumarNeu",
      "name": "MCP-Lucene-Server",
      "url": "https://github.com/VivekKumarNeu/MCP-Lucene-Server",
      "imageUrl": "/freedevtools/mcp/pfp/VivekKumarNeu.webp",
      "description": "Efficiently manage and retrieve documents using Apache Lucene with a RESTful API for complex querying and document management tasks. Supports adding, updating, deleting, and querying documents while utilizing Lucene's powerful indexing features.",
      "stars": 0,
      "forks": 1,
      "license": "Apache License 2.0",
      "language": "Java",
      "updated_at": "2025-09-28T01:48:15Z",
      "readme_content": "![License](https://img.shields.io/github/license/VivekKumarNeu/MCP-Lucene-Server)\n\n\n# MCP Lucene Server\n\n## Description\n\nThe MCP Lucene Server is a Java-based implementation of the Model Context Protocol (MCP) designed to provide efficient search and retrieval capabilities using Apache Lucene. This server allows you to manage and query documents, leveraging Lucene's powerful indexing and search features. It is built using Spring Boot for easy setup and deployment.\n\n\n![lucene_mcp1](https://github.com/user-attachments/assets/5dc28224-2dda-4b42-ac90-83343c9c386d)\n\n![lucene_mcp2](https://github.com/user-attachments/assets/b5ffd0cf-87ad-4129-af34-98163690f2ba)\n\n## Features\n\n* **MCP Compliance:** Implements the core Model Context Protocol.\n\n* **Lucene-Powered:** Utilizes Apache Lucene for full-text search and indexing.\n\n* **RESTful API:** Provides a RESTful API for interacting with the server.\n\n* **Document Management:**\n\n    * **Upsert:** Add or update documents in the Lucene index.\n\n    * **Delete:** Delete documents from the Lucene index.\n\n    * **List:** Retrieve a list of documents from the index.\n\n* **Querying:**\n\n    * Supports complex queries using the Lucene query syntax.\n\n    * Filtering: Filter queries based on document metadata.\n\n* **Status:** Check the server status.\n\n* **Spring Boot:** Built with Spring Boot for easy setup and deployment.\n* **Dockerization:** Includes instructions for containerizing the application using Docker.\n\n## Table of Contents\n\n* [Description](#description)\n\n* [Features](#features)\n\n* [Getting Started](#getting-started)\n\n    * [Prerequisites](#prerequisites)\n\n    * [Installation](#installation)\n\n    * [Running the Server](#running-the-server)\n\n* [Usage](#usage)\n\n    * [API Endpoints](#api-endpoints)\n\n    * [Examples](#examples)\n\n* [Configuration](#configuration)\n\n* [License](#license)\n\n## Getting Started\n\n### Prerequisites\n\n* **Java:** Java 11 or higher.\n\n* **Maven:** Maven 3.6.0 or higher.\n* **Docker:** [Install Docker](https://docs.docker.com/get-docker/) if you plan to use the Docker image.\n\n### Installation\n\n1.  **Clone the repository:**\n\n    ```\n    git clone [https://github.com/your-username/mcp-lucene-server.git](https://github.com/your-username/mcp-lucene-server.git)\n    cd mcp-lucene-server\n    ```\n\n    (Replace `your-username` with your GitHub username)\n\n2.  **Build the project using Maven:**\n\n    ```\n    mvn clean install\n    ```\n\n### Running the Server\n\n#### Without Docker\n\n1.  **Run the Spring Boot application:**\n    ```bash\n    java -jar target/mcp-lucene-server-0.0.1-SNAPSHOT.jar\n    ```\n    (The exact name of the `.jar` file might vary slightly depending on your project version.)\n\n2.  The server will start on port `8080` by default.\n\n#### With Docker\n\n1.  **Ensure you have Docker installed:** Follow the instructions on the official Docker website: [https://docs.docker.com/get-docker/](https://docs.docker.com/get-docker/)\n2.  **Build the Docker image:**\n    Navigate to the root directory of your project in your terminal and run:\n    ```bash\n    docker build -t mcp-lucene-server .\n    ```\n\n5.  **Run the Docker container:**\n    ```bash\n    docker run -p 8080:8080 mcp-lucene-server\n    ```\n    This will map port `8080` on your host machine to port `8080` inside the container.\n\n## MCP Shim for Claude Desktop\n\nThis project includes an optional MCP shim (`mcp-shim/`) that exposes the server's REST endpoints as MCP tools over STDIO so you can use them directly from Claude Desktop.\n\n### Prerequisites\n- Java 17+\n- Node.js 18+\n- Maven 3.6+\n\n### 1) Run the Spring Boot server\n```bash\nmvn spring-boot:run\n```\nThe API will be available at `http://localhost:8080/mcp/v1`.\n\n### 2) Run the MCP shim\n```bash\ncd mcp-shim\nnpm install\n# JSON + text output (default)\nLUCENE_BASE_URL=http://localhost:8080/mcp/v1 npm start\n# If your client cannot render JSON tool outputs, force text-only\nMCP_FORCE_TEXT=1 LUCENE_BASE_URL=http://localhost:8080/mcp/v1 npm start\n```\n\n### 3) Configure Claude Desktop\nUpdate `~/.claude/mcp/config.json`:\n```json\n{\n  \"mcpServers\": {\n    \"lucene\": {\n      \"command\": \"/opt/homebrew/bin/node\",\n      \"args\": [\".../MCP-Lucene-Server/mcp-shim/server.js\"],\n      \"env\": {\n        \"LUCENE_BASE_URL\": \"http://localhost:8080/mcp/v1\",\n        \"MCP_FORCE_TEXT\": \"1\"\n      }\n    }\n  }\n}\n```\nAlternatively, use the wrapper script to capture shim logs to `/tmp/mcp-lucene-shim.stderr.log`:\n```bash\ncat > .../MCP-Lucene-Server/mcp-shim/run-shim.sh <<'SH'\n#!/usr/bin/env bash\nset -euo pipefail\nexport LUCENE_BASE_URL=\"${LUCENE_BASE_URL:-http://localhost:8080/mcp/v1}\"\nexec node .../MCP-Lucene-Server/mcp-shim/server.js \\\n  2> /tmp/mcp-lucene-shim.stderr.log\nSH\nchmod +x .../MCP-Lucene-Server/mcp-shim/run-shim.sh\n```\nThen set in `~/.claude/mcp/config.json`:\n```json\n{\n  \"mcpServers\": {\n    \"lucene\": {\n      \"command\": \".../MCP-Lucene-Server/mcp-shim/run-shim.sh\",\n      \"env\": {\n        \"LUCENE_BASE_URL\": \"http://localhost:8080/mcp/v1\",\n        \"MCP_FORCE_TEXT\": \"1\"\n      }\n    }\n  }\n}\n```\n\n### 4) Available tools\n- `lucene_status`: Get server/index status\n- `lucene_upsert`: Upsert documents\n- `lucene_query`: Query documents (with optional metadata filters)\n- `lucene_delete`: Delete by IDs\n- `lucene_list`: List documents with pagination\n\n### 5) Example prompts for Claude Desktop\n- Run `lucene_status`\n- Run `lucene_list` with: `{ \"limit\": 10, \"offset\": 0 }`\n- Run `lucene_upsert` with: `{\"documents\":[{\"id\":\"doc-1\",\"text\":\"hello world\",\"metadata\":{\"lang\":\"en\"}}]}`\n- Run `lucene_query` with: `{\"queries\":[{\"query\":\"hello\",\"top_k\":5}]}`\n- Run `lucene_delete` with: `{ \"ids\": [\"doc-1\"] }`\n\n### 6) Troubleshooting\n- Verify the API returns JSON:\n```bash\ncurl -i http://localhost:8080/mcp/v1/status\n```\n- If Claude shows \"unsupported format\", start the shim with text-only output:\n```bash\nMCP_FORCE_TEXT=1 LUCENE_BASE_URL=http://localhost:8080/mcp/v1 npm start\n```\n- View shim logs (when using wrapper):\n```bash\ntail -n +1 /tmp/mcp-lucene-shim.stderr.log\n```\n- Ensure the paths in your `config.json` are absolute and correct, then restart Claude Desktop.\n\n\n### API Endpoints (for Curl)\n\nThe server provides the following API endpoints:\n\n* `GET /mcp/v1/status`\n\n    * Returns the status of the server.\n\n* `POST /mcp/v1/upsert`\n\n    * Upserts (inserts or updates) one or more documents.\n\n    * Request body:\n\n        ```json\n        {\n          \"documents\": [\n            {\n              \"id\": \"doc1\",\n              \"text\": \"This is the text of document 1.\",\n              \"metadata\": {\n                \"category\": \"example\",\n                \"language\": \"english\"\n              }\n            },\n            {\n              \"id\": \"doc2\",\n              \"text\": \"This is document 2's text.\",\n              \"metadata\": {\n                \"category\": \"sample\",\n                \"language\": \"spanish\"\n              }\n            }\n          ]\n        }\n        ```\n\n* `POST /mcp/v1/query`\n\n    * Queries the Lucene index.\n\n    * Request body:\n\n        ```json\n        {\n          \"queries\": [\n            {\n              \"query\": \"document\",\n              \"top_k\": 10,\n              \"filter\": {\n                \"language\": \"english\"\n              }\n            },\n             {\n              \"query\": \"text search\",\n              \"filter\": {\n                 \"category\": \"example\"\n               }\n             }\n          ]\n        }\n        ```\n\n    * `query`: The Lucene query string.\n\n    * `top_k`: (Optional) The maximum number of results to return (default: 10).\n\n    * `filter`: (Optional) A map of metadata fields and values to filter by.\n\n* `POST /mcp/v1/delete`\n\n    * Deletes documents from the Lucene index.\n\n    * Request body:\n\n        ```json\n        {\n            \"ids\": [\"doc1\", \"doc2\"]\n        }\n        ```\n\n* `GET /mcp/v1/list`\n\n    * Lists documents from the Lucene index.\n\n    * Request body:\n\n        ```json\n        {\n            \"ids\": [\"doc1\", \"doc2\"]\n        }\n        ```\n\n### Examples\n\n**Get server status:**\n\n```bash\ncurl http://localhost:8080/mcp/v1/status\n```\n\n**Upsert documents:**\n\n```bash\ncurl -X POST \n\nhttp://localhost:8080/mcp/v1/upsert \n\n-H 'Content-Type: application/json' \n\n-d '{\n\"documents\": [\n{\n\"id\": \"doc1\",\n\"text\": \"This is the text of document 1.\",\n\"metadata\": {\n\"category\": \"example\",\n\"language\": \"english\"\n}\n},\n{\n\"id\": \"doc2\",\n\"text\": \"This is document 2''s text.\",\n\"metadata\": {\n\"category\": \"sample\",\n\"language\": \"spanish\"\n}\n}\n]\n}'\n```\n\n**Query documents:**\n\n```bash\ncurl -X POST \n\nhttp://localhost:8080/mcp/v1/query \n\n-H 'Content-Type: application/json' \n\n-d '{\n\"queries\": [\n{\n\"query\": \"document text\",\n\"top_k\": 5,\n\"filter\": {\n\"language\": \"english\"\n}\n}\n]\n}'\n```\n\n**Delete documents:**\n\n```bash\ncurl -X POST \n\nhttp://localhost:8080/mcp/v1/delete \n\n-H 'Content-Type: application/json' \n\n-d '{\n\"ids\": [\"doc1\"]\n}'\n```\n\n**List documents:**\n\n```bash\ncurl -X POST \n\nhttp://localhost:8080/mcp/v1/list \n\n-H 'Content-Type: application/json' \n\n-d '{\n\"ids\": [\"doc1\", \"doc2\"]\n}'\n```\n\n## Configuration\n\nThe server can be configured using Spring Boot's application properties. Here are some of the key properties:\n\n* `server.port`: The port the server listens on (default: 8080).\n\n* `lucene.index.path`: The path to the Lucene index directory. This is where the indexed data is stored. If not set, a default location is used. It is highly recommended to configure this to a persistent storage location.\n\nYou can set these properties in an `application.properties` or `application.yml` file in your `src/main/resources` directory, or by using environment variables.\n\n**Example `application.properties`:**\n\n\nserver.port=8080\nlucene.index.path=/path/to/lucene/index\n\n## License\n\nThis project is licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0).\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "lucene",
        "indexing",
        "search",
        "lucene restful",
        "apache lucene",
        "lucene server"
      ],
      "category": "web-search"
    },
    "WebChatAppAi--webchat": {
      "owner": "WebChatAppAi",
      "name": "webchat",
      "url": "https://github.com/WebChatAppAi/webchat",
      "imageUrl": "/freedevtools/mcp/pfp/WebChatAppAi.webp",
      "description": "Build interactive chat experiences using a responsive web chat application powered by Next.js, enabling real-time updates as modifications are made. Customize and enhance chat functionalities easily with integrated tools and data sources.",
      "stars": 0,
      "forks": 1,
      "license": "No License",
      "language": "TypeScript",
      "updated_at": "2025-05-31T01:55:59Z",
      "readme_content": "# WebChat Application\n\n[![Next.js](https://img.shields.io/badge/Next.js-14-black)](https://nextjs.org/)\n[![TypeScript](https://img.shields.io/badge/TypeScript-5-blue)](https://www.typescriptlang.org/)\n[![Prisma](https://img.shields.io/badge/Prisma-Latest-green)](https://www.prisma.io/)\n[![TailwindCSS](https://img.shields.io/badge/TailwindCSS-Latest-cyan)](https://tailwindcss.com/)\n[![License](https://img.shields.io/badge/license-MIT-blue.svg)](LICENSE)\n\n## 📌 Overview\n\nA modern real-time web chat application built with Next.js, Prisma, and NextAuth.js. This application provides secure authentication and real-time messaging capabilities in a sleek, responsive interface.\n\n## ✨ Features\n\n- Real-time messaging\n- Email-based authentication with NextAuth.js\n- SQLite database with Prisma ORM\n- Responsive UI with Tailwind CSS\n\n## 🚀 Getting Started\n\n### Prerequisites\n\n- Node.js (v14 or later recommended)\n- npm or yarn\n- [Any other prerequisites]\n\n### Installation\n\n1. Clone the repository\n   ```bash\n   git clone https://github.com/WebChatAppAi/webchat.git\n   cd webchat\n   ```\n\n2. Install dependencies\n   ```bash\n   npm install\n   # or\n   yarn install\n   ```\n\n3. Create a `.env` file in the root directory with the following variables:\n\n   ```\n   # Prisma\n   DATABASE_URL=\"file:./dev.db\"\n\n   # NextAuth.js\n   # Generate a random string for NEXTAUTH_SECRET (e.g., using `openssl rand -base64 32` or an online generator)\n   NEXTAUTH_SECRET=\"your_nextauth_secret\"\n   NEXTAUTH_URL=\"http://localhost:3000\" # Replace with your deployment URL in production\n\n   # Email Provider (for NextAuth.js)\n   # For development, NextAuth will log email links to the console if you don't provide real email credentials\n   EMAIL_SERVER_HOST=\"smtp.your-email-provider.com\"\n   EMAIL_SERVER_PORT=\"465\"\n   EMAIL_SERVER_USER=\"your-email@example.com\"\n   EMAIL_SERVER_PASSWORD=\"your-email-password\"\n   EMAIL_FROM=\"your-email@example.com\"\n   ```\n\n4. Start the development server\n   ```bash\n   npm run dev\n   # or\n   yarn dev\n   ```\n\n## 🛠️ Technologies Used\n\n- **Frontend & Backend**: Next.js 14 (App Router)\n- **Language**: TypeScript\n- **Database**: SQLite with Prisma ORM\n- **Authentication**: NextAuth.js\n- **Styling**: TailwindCSS\n- **UI Components**: Shadcn UI\n\n## 📋 Project Structure\n\n```\nwebchat/\n├── prisma/            # Prisma schema and migrations\n├── public/            # Static assets\n├── src/               # Next.js application code\n│   ├── app/           # App router pages and components\n│   ├── components/    # Reusable UI components\n│   └── lib/           # Utility functions and configurations\n├── .env               # Environment variables (not committed)\n├── .gitignore         # Git ignore rules\n├── next.config.ts     # Next.js configuration\n└── README.md          # This file\n```\n\n## 🤝 Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n\n## 📄 License\n\nThis project is licensed under the MIT License - see the LICENSE file for details.\n",
      "npm_url": "https://www.npmjs.com/package/webchat",
      "npm_downloads": 2620,
      "keywords": [
        "webchatappai",
        "webchat",
        "chat",
        "webchatappai webchat",
        "webchat build",
        "search webchatappai"
      ],
      "category": "web-search"
    },
    "XPE-7--MCP-Server": {
      "owner": "XPE-7",
      "name": "MCP-Server",
      "url": "https://github.com/XPE-7/MCP-Server",
      "imageUrl": "/freedevtools/mcp/pfp/XPE-7.webp",
      "description": "Facilitates intelligent search and retrieval of AI/ML library documentation using Claude's reasoning capabilities, supporting multiple sources and an extendable architecture for adding new documentation easily.",
      "stars": 0,
      "forks": 0,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-03-22T10:56:15Z",
      "readme_content": "# 🤖 Claude AI Documentation Assistant 📚\n\n<div align=\"center\">\n\n\n\n*A powerful MCP server that supercharges Claude with documentation search capabilities*\n\n[![Python 3.8+](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/downloads/)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg)](http://makeapullrequest.com)\n\n</div>\n\n## ✨ Features\n\n- 🔍 **Smart Documentation Search** - Search across multiple AI/ML library documentation\n- 🧠 **Claude Integration** - Seamless connection with Claude's advanced reasoning capabilities\n- 🌐 **Intelligent Web Search** - Leverages Serper API for targeted documentation lookup\n- 💨 **Fast Response Times** - Optimized for quick retrieval and processing\n- 🧩 **Extendable Architecture** - Easily add more documentation sources\n\n## 📋 Prerequisites\n\n- 🐍 Python 3.8 or higher\n- 🔑 Claude Pro subscription\n- 🔐 Serper API key ([Get one here](https://serper.dev))\n- 💻 Claude Desktop application\n\n## 🚀 Quick Start\n\n### 1️⃣ Installation\n\n```bash\n# Clone the repository\ngit clone https://github.com/your-username/claude-docs-assistant.git\ncd claude-docs-assistant\n\n# Create a virtual environment (recommended)\npython -m venv venv\nsource venv/bin/activate  # On Windows: venv\\Scripts\\activate\n\n# Install dependencies\npip install -r requirements.txt\n```\n\n### 2️⃣ Configuration\n\nCreate a `.env` file in the project root with your API keys:\n\n```\nSERPER_API_KEY=your_serper_api_key_here\n```\n\n### 3️⃣ Start the MCP Server\n\n```bash\npython main.py\n```\n\nYou should see output indicating the server is running and waiting for Claude to connect.\n\n### 4️⃣ Connect Claude Desktop App\n\n1. 📱 Open the Claude Desktop App\n2. ⚙️ Click on your profile icon and select \"Settings\"\n3. 🧰 Navigate to the \"Tools\" section\n4. ➕ Click \"Add Tool\"\n5. 🔗 Select \"Connect to a local tool\"\n6. 🖥️ Follow the prompts to connect to your running MCP server\n7. ✅ Confirm the connection is successful\n\n## 🎮 Using Your Claude Documentation Assistant\n\nOnce connected, you can start asking Claude questions that will trigger the documentation search. For example:\n\n```\nCould you explain how to use FAISS with LangChain? Please search the langchain documentation to help me.\n```\n\nClaude will automatically use your MCP server to:\n1. 🔍 Search for relevant documentation\n2. 📥 Retrieve the content\n3. 🧠 Process and explain the information\n\n## 🔧 Under the Hood\n\n### 📄 Code Structure\n\n```\nclaude-docs-assistant/\n├── main.py           # MCP server implementation\n├── requirements.txt  # Project dependencies\n├── .env              # Environment variables (API keys)\n└── README.md         # This documentation\n```\n\n### 🔌 Supported Libraries\n\nThe assistant currently supports searching documentation for:\n\n- 🦜 **LangChain**: `python.langchain.com/docs`\n- 🦙 **LlamaIndex**: `docs.llamaindex.ai/en/stable`\n- 🧠 **OpenAI**: `platform.openai.com/docs`\n\n### 🧩 How It Works\n\n1. 📡 The MCP server exposes a `get_docs` tool to Claude\n2. 🔍 When invoked, the tool searches for documentation using Serper API\n3. 📚 Results are scraped for their content\n4. 🔄 Content is returned to Claude for analysis and explanation\n\n## 🛠️ Advanced Configuration\n\n### Adding New Documentation Sources\n\nExtend the `docs_urls` dictionary in `main.py`:\n\n```python\ndocs_urls = {\n    \"langchain\": \"python.langchain.com/docs\",\n    \"llama-index\": \"docs.llamaindex.ai/en/stable\",\n    \"openai\": \"platform.openai.com/docs\",\n    \"huggingface\": \"huggingface.co/docs\",  # Add new documentation sources\n    \"tensorflow\": \"www.tensorflow.org/api_docs\",\n}\n```\n\n### Customizing Search Behavior\n\nModify the `search_web` function to adjust the number of results:\n\n```python\npayload = json.dumps({\"q\": query, \"num\": 5})  # Increase from default 2\n```\n\n## 🔍 Troubleshooting\n\n### Common Issues\n\n- **🚫 \"Connection refused\" error**: Ensure the MCP server is running before connecting Claude\n- **⏱️ Timeout errors**: Check your internet connection or increase the timeout value\n- **🔒 API key issues**: Verify your Serper API key is correct in the `.env` file\n\n### Debugging Tips\n\nAdd more detailed logging by modifying the main.py file:\n\n```python\nimport logging\nlogging.basicConfig(level=logging.DEBUG)\n```\n\n## 📈 Performance Optimization\n\n- ⚡ For faster response times, consider caching frequently accessed documentation\n- 🧠 Limit the amount of text returned to Claude to avoid token limitations\n- 🌐 Use more specific queries to get more relevant documentation\n\n## 🤝 Contributing\n\nContributions are welcome! Here's how you can help:\n\n1. 🍴 Fork the repository\n2. 🌿 Create a feature branch (`git checkout -b feature/amazing-feature`)\n3. 💾 Commit your changes (`git commit -m 'Add some amazing feature'`)\n4. 📤 Push to the branch (`git push origin feature/amazing-feature`)\n5. 🔍 Open a Pull Request\n\n## 📜 License\n\nThis project is licensed under the MIT License - see the LICENSE file for details.\n\n## 🙏 Acknowledgements\n\n- [Anthropic](https://www.anthropic.com/) for creating Claude\n- [Serper.dev](https://serper.dev) for their search API\n- All the open-source libraries that make this project possible\n\n---\n\n<div align=\"center\">\n  Made with ❤️ for Claude enthusiasts\n</div>",
      "npm_url": "https://www.npmjs.com/package/mcp-server",
      "npm_downloads": 29732,
      "keywords": [
        "search",
        "documentation",
        "xpe",
        "search xpe",
        "intelligent search",
        "retrieval ai"
      ],
      "category": "web-search"
    },
    "YeonwooSung--metasearch-mcp": {
      "owner": "YeonwooSung",
      "name": "metasearch-mcp",
      "url": "https://github.com/YeonwooSung/metasearch-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/YeonwooSung.webp",
      "description": "Perform searches based on specified queries and receive AI responses along with URIs and titles of the search results. It enhances search capabilities within applications like Claude Desktop or Cursor, streamlining information retrieval.",
      "stars": 3,
      "forks": 2,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-05-01T03:32:43Z",
      "readme_content": "# metasearch MCP server\n\nA MCP server for metasearch\n\n<a href=\"https://glama.ai/mcp/servers/xxb4uogn02\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/xxb4uogn02/badge\" alt=\"tavily-search MCP server\" /></a>\n\n## Components\n\nThis server uses the Tavily API to perform searches based on specified queries.\n- Search results are returned in text format.\n- Search results include AI responses, URIs, and titles of the search results.\n\n### Tools\n\nThis server implements the following tools:\n- search: Performs searches based on specified queries\n  - Required argument: \"query\"\n  - Optional argument: \"search_depth\" (basic or advanced)\n\n### Install\n\n1. Download the repository.\n```bash\ngit clone https://github.com/YeonwooSung/metasearch-mcp.git\n``` \n\n2. Open the Claude Desktop configuration file.\n```\nOn MacOS: `~/Library/Application\\ Support/Claude/claude_desktop_config.json`\nOn Windows: `C:\\Users\\[username]\\AppData\\Roaming\\Claude\\claude_desktop_config.json`\n```\n\n3. Edit the configuration file as follows:\n  ```yaml\n  \"mcpServers\": {\n    \"tavily-search\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"--directory\",\n        \"C:\\\\your_path\\\\mcp-server-tavily\",\n        \"run\",\n        \"tavily-search\"\n      ],\n      \"env\": {\n        \"TAVILY_API_KEY\": \"YOUR_TAVILY_API_KEY\",\n        \"PYTHONIOENCODING\": \"utf-8\"\n      }\n    }\n  }\n  ```\n\n4. Restart Claude Desktop.\n\n## Usage\n\nIn Claude Desktop, when you ask \"Please search for something\", you will receive search results.\n\nSearch example:\n```\nPlease search in detail for today's events in Kamakura\n```\nResponse example:\n```\nAccording to the search results, the following events start today, December 1st:\n\"Kamakura Promotion Photo Contest 2025\"\nPeriod: December 1, 2024 - January 31, 2025\nA photo contest for those who love Kamakura\nApplications start accepting from today\nAlso, as a related upcoming event:\nOn December 7th, an exhibition by 12 Kamakura artists will be held at the Seibu Press Inn Kamakura Ofuna Station East Exit Lounge.\n```\n\n## Log Storage Location\n\nLogs are stored in the following location:\n\nFor Windows:\n```\nC:\\Users\\[username]\\AppData\\Roaming\\Claude\\logs\\mcp-server-tavily-search\n```\n## Execution with Cursor\n\n1. Create a shell script (e.g., `script.sh`) as shown below:\n\n```bash\n#!/bin/bash\nTARGET_DIR=/path/to/mcp-server-tavily\ncd \"${TARGET_DIR}\"\nexport TAVILY_API_KEY=\"your-api-key\"\nexport PYTHONIOENCODING=utf-8\nuv --directory $PWD run tavily-search\n```\n\n2. Configure Cursor's MCP Server settings as follows:\n\n```\nName: tavily-search\nType: command\nCommand: /path/to/your/script.sh\n```\n\n3. Save the settings.\n\n4. Once the settings are saved, you can ask Cursor's Composer-Agent to \"search for something,\" and it will return the search results.\n\n## Running in Local Environment Using Docker Compose\n\n### Purpose\nFor operating systems other than Windows/MacOS where Claude Desktop cannot be used,\nthis section explains how to set up and run an MCP server and client in a local environment\nusing Docker compose.\n\n### Steps\n1. Install Docker.\n2. Download the repository.\n```bash\ngit clone https://github.com/YeonwooSung/metasearch-mcp.git\n``` \n3. Run Docker compose.\n```bash\ndocker compose up -d\n``` \n4. Execute the client.\n```bash\ndocker exec mcp_server uv --directory /usr/src/app/mcp-server-tavily/src run client.py\n```\n5. Execution Results\n6. After searching for available tools as shown below, a query will be issued to Tavily and a response will be returned\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "metasearch",
        "searches",
        "search",
        "metasearch mcp",
        "yeonwoosung metasearch",
        "web search"
      ],
      "category": "web-search"
    },
    "Zo-Valentine--mcp-twikit": {
      "owner": "Zo-Valentine",
      "name": "mcp-twikit",
      "url": "https://github.com/Zo-Valentine/mcp-twikit",
      "imageUrl": "/freedevtools/mcp/pfp/Zo-Valentine.webp",
      "description": "Interact with Twitter to analyze sentiments and retrieve tweets from timelines, providing insights into public opinion and updates on trending topics.",
      "stars": 0,
      "forks": 0,
      "license": "MIT License",
      "language": "",
      "updated_at": "2025-03-08T00:09:33Z",
      "readme_content": "# MCP-Twikit\n\n[![smithery badge](https://smithery.ai/badge/mcp-twikit)](https://smithery.ai/server/mcp-twikit)\nA Model Context Protocol (MCP) server for interacting with Twitter.\n\n<a href=\"https://glama.ai/mcp/servers/49i9dd08w8\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/49i9dd08w8/badge\" alt=\"mcp-twikit MCP server\" /></a>\n\n## Installation\n\n### Installing via Smithery\n\nTo install Twikit Twitter Search for Claude Desktop automatically via [Smithery](https://smithery.ai/server/mcp-twikit):\n\n```bash\nnpx -y @smithery/cli install mcp-twikit --client claude\n```\n\n### Manual Installation\n```json\n{\n    \"mcpServer\": {\n        \"command\": \"uvx\",\n        \"args\": [\"--from\", \"git+https://github.com/adhikasp/mcp-twikit\", \"mcp-twikit\"],\n        \"env\": {\n            \"TWITTER_USERNAME\": \"@example\",\n            \"TWITTER_EMAIL\": \"me@example.com\",\n            \"TWITTER_PASSWORD\": \"secret\",\n        }\n    }\n}\n```\n\n## Example usage\n\nUsing [mcp-client-cli](https://github.com/adhikasp/mcp-client-cli)\n\n\n### Comparing sentiments across different twitter accounts\n\n```\n$ llm compare 20 latest tweet directed @IndiHomeCare, @di_cbn, @BiznetHome, @ID_MyRepublic. What are people sentiment to the product? Do 1 search for each account\n\nI'll help you analyze the latest tweets for these internet service providers in Indonesia to understand the public sentiment. I'll use the search_twitter function for each account.\n\nTool Calls:\n  search_twitter\n  Args:\n    query: to:IndiHomeCare\n    count: 20\n    sort_by: Latest\n\nTool Calls:\n  search_twitter\n  Args:\n    query: to:di_cbn\n    count: 20\n    sort_by: Latest\n\nTool Calls:\n  search_twitter\n  Args:\n    query: to:BiznetHome\n    count: 20\n    sort_by: Latest\n\nTool Calls:\n  search_twitter\n  Args:\n    query: to:ID_MyRepublic\n    count: 20\n    sort_by: Latest\n\nSentiment Analysis Summary:\n\n1. IndiHomeCare:\n- Predominantly Negative Sentiment\n- Key Issues:\n  - Unstable internet connection\n  - Slow response times\n  - Intermittent service disruptions\n  - Long waiting times for customer support\n- Specific Complaints:\n  - Network instability\n  - Lagging internet\n  - Unresolved technical issues\n  - Lack of timely communication\n\n2. CBN (di_cbn):\n- Mostly Negative Sentiment\n- Key Issues:\n  - Prolonged internet outages (some reporting 1-2 days of no service)\n  - Slow customer support response\n  - Lack of clear communication about service disruptions\n- Specific Complaints:\n  - LOS (Line of Sight) issues\n  - No timely resolution of technical problems\n  - Delayed email and support responses\n\n3. BiznetHome:\n- Strongly Negative Sentiment\n- Key Issues:\n  - Frequent network interruptions\n  - Slow internet speeds\n  - Poor customer support responsiveness\n- Specific Complaints:\n  - Packet loss\n  - Inconsistent connection\n  - Long download times\n  - Unresolved technical issues\n\n4. MyRepublic (ID_MyRepublic):\n- Extremely Negative Sentiment\n- Key Issues:\n  - Severe and prolonged service disruptions\n  - Complete internet outages\n  - Inability to access online services and games\n  - Extremely poor customer support\n- Specific Complaints:\n  - Multi-day service interruptions\n  - Inability to access websites and online games\n  - Missed technician appointments\n  - Lack of compensation or clear communication\n\nOverall Sentiment Ranking (from Bad to Worst):\n1. IndiHomeCare (Bad)\n2. CBN (Worse)\n3. BiznetHome (Very Bad)\n4. MyRepublic (Extremely Bad)\n\nCommon Themes Across Providers:\n- Poor network stability\n- Slow customer support response\n- Lack of transparent communication\n- Prolonged service interruptions\n- Unresolved technical issues\n\nRecommendation: These Indonesian internet service providers are experiencing significant customer satisfaction issues. Customers are frustrated with unreliable services, poor technical support, and lack of timely problem resolution.\n```\n\n### Getting tweets from your home timeline\n\n```\n$ llm what is happening on my twitter timeline?\n\nI'll help you check your Twitter timeline. I'll retrieve the latest tweets from your timeline:\n\nTool Calls:\n  get_timeline\n  Args:\n\nI'll summarize the key highlights from your Twitter timeline:\n\n1. Professional/Tech Highlights:\n- @ibamarief is preparing to onboard their first intern, noting it's a significant responsibility\n- @papanberjalan shared a critique about web developers not understanding computer performance\n- @pathofexile is running a Twitch Drops promotion for their game\n\n2. Notable Tweets:\n- @elonmusk posted a cryptic \"Hmm\"\n- @realDonaldTrump discussed eliminating Daylight Saving Time\n- @blinkist shared an article about billionaires reading up to 50 books per year\n\n3. Miscellaneous Interesting Tweets:\n- @nealtaparia shared a thread about 11 companies built around a single product\n- @Rixhabh__ posted about creative and iconic ads\n- Several tweets in Indonesian covering various topics from personal stories to social issues\n\nWould you like me to elaborate on any of these tweets or provide more context about any specific post?\n```\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "twitter",
        "tweets",
        "twikit",
        "tweets timelines",
        "twitter analyze",
        "retrieve tweets"
      ],
      "category": "web-search"
    },
    "ZubeidHendricks--youtube-mcp-server": {
      "owner": "ZubeidHendricks",
      "name": "youtube-mcp-server",
      "url": "https://github.com/ZubeidHendricks/youtube-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/ZubeidHendricks.webp",
      "description": "Interacts with YouTube content by retrieving video details, managing channels, and accessing transcripts through a standardized interface. Allows for video statistics, channel management, and playlist details.",
      "stars": 354,
      "forks": 62,
      "license": "No License",
      "language": "TypeScript",
      "updated_at": "2025-10-03T09:49:36Z",
      "readme_content": "# YouTube MCP Server\n[![smithery badge](https://smithery.ai/badge/@ZubeidHendricks/youtube)](https://smithery.ai/server/@ZubeidHendricks/youtube)\n\nA Model Context Protocol (MCP) server implementation for YouTube, enabling AI language models to interact with YouTube content through a standardized interface.\n\n## Features\n\n### Video Information\n* Get video details (title, description, duration, etc.)\n* List channel videos\n* Get video statistics (views, likes, comments)\n* Search videos across YouTube\n\n### Transcript Management\n* Retrieve video transcripts\n* Support for multiple languages\n* Get timestamped captions\n* Search within transcripts\n\n### Channel Management\n* Get channel details\n* List channel playlists\n* Get channel statistics\n* Search within channel content\n\n### Playlist Management\n* List playlist items\n* Get playlist details\n* Search within playlists\n* Get playlist video transcripts\n\n## Installation\n\n### Quick Setup for Claude Desktop\n\n1. Install the package:\n```bash\nnpm install -g zubeid-youtube-mcp-server\n```\n\n2. Add to your Claude Desktop configuration (`~/Library/Application Support/Claude/claude_desktop_config.json` on macOS or `%APPDATA%\\Claude\\claude_desktop_config.json` on Windows):\n\n```json\n{\n  \"mcpServers\": {\n    \"zubeid-youtube-mcp-server\": {\n      \"command\": \"zubeid-youtube-mcp-server\",\n      \"env\": {\n        \"YOUTUBE_API_KEY\": \"your_youtube_api_key_here\"\n      }\n    }\n  }\n}\n```\n\n### Alternative: Using NPX (No Installation Required)\n\nAdd this to your Claude Desktop configuration:\n\n```json\n{\n  \"mcpServers\": {\n    \"youtube\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"zubeid-youtube-mcp-server\"],\n      \"env\": {\n        \"YOUTUBE_API_KEY\": \"your_youtube_api_key_here\"\n      }\n    }\n  }\n}\n```\n\n### Installing via Smithery\n\nTo install YouTube MCP Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@ZubeidHendricks/youtube):\n\n```bash\nnpx -y @smithery/cli install @ZubeidHendricks/youtube --client claude\n```\n\n## Configuration\nSet the following environment variables:\n* `YOUTUBE_API_KEY`: Your YouTube Data API key (required)\n* `YOUTUBE_TRANSCRIPT_LANG`: Default language for transcripts (optional, defaults to 'en')\n### Using with VS Code\n\nFor one-click installation, click one of the install buttons below:\n\n[![Install with NPX in VS Code](https://img.shields.io/badge/VS_Code-NPM-0098FF?style=flat-square&logo=visualstudiocode&logoColor=white)](https://insiders.vscode.dev/redirect/mcp/install?name=youtube&config=%7B%22command%22%3A%22npx%22%2C%22args%22%3A%5B%22-y%22%2C%22zubeid-youtube-mcp-server%22%5D%2C%22env%22%3A%7B%22YOUTUBE_API_KEY%22%3A%22%24%7Binput%3AapiKey%7D%22%7D%7D&inputs=%5B%7B%22type%22%3A%22promptString%22%2C%22id%22%3A%22apiKey%22%2C%22description%22%3A%22YouTube+API+Key%22%2C%22password%22%3Atrue%7D%5D) [![Install with NPX in VS Code Insiders](https://img.shields.io/badge/VS_Code_Insiders-NPM-24bfa5?style=flat-square&logo=visualstudiocode&logoColor=white)](https://insiders.vscode.dev/redirect/mcp/install?name=youtube&config=%7B%22command%22%3A%22npx%22%2C%22args%22%3A%5B%22-y%22%2C%22zubeid-youtube-mcp-server%22%5D%2C%22env%22%3A%7B%22YOUTUBE_API_KEY%22%3A%22%24%7Binput%3AapiKey%7D%22%7D%7D&inputs=%5B%7B%22type%22%3A%22promptString%22%2C%22id%22%3A%22apiKey%22%2C%22description%22%3A%22YouTube+API+Key%22%2C%22password%22%3Atrue%7D%5D&quality=insiders)\n\n### Manual Installation\n\nIf you prefer manual installation, first check the install buttons at the top of this section. Otherwise, follow these steps:\n\nAdd the following JSON block to your User Settings (JSON) file in VS Code. You can do this by pressing `Ctrl + Shift + P` and typing `Preferences: Open User Settings (JSON)`.\n\n```json\n{\n  \"mcp\": {\n    \"inputs\": [\n      {\n        \"type\": \"promptString\",\n        \"id\": \"apiKey\",\n        \"description\": \"YouTube API Key\",\n        \"password\": true\n      }\n    ],\n    \"servers\": {\n      \"youtube\": {\n        \"command\": \"npx\",\n        \"args\": [\"-y\", \"zubeid-youtube-mcp-server\"],\n        \"env\": {\n          \"YOUTUBE_API_KEY\": \"${input:apiKey}\"\n        }\n      }\n    }\n  }\n}\n```\n\nOptionally, you can add it to a file called `.vscode/mcp.json` in your workspace:\n\n```json\n{\n  \"inputs\": [\n    {\n      \"type\": \"promptString\",\n      \"id\": \"apiKey\",\n      \"description\": \"YouTube API Key\",\n      \"password\": true\n    }\n  ],\n  \"servers\": {\n    \"youtube\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"zubeid-youtube-mcp-server\"],\n      \"env\": {\n        \"YOUTUBE_API_KEY\": \"${input:apiKey}\"\n      }\n    }\n  }\n}\n```\n## YouTube API Setup\n1. Go to Google Cloud Console\n2. Create a new project or select an existing one\n3. Enable the YouTube Data API v3\n4. Create API credentials (API key)\n5. Copy the API key for configuration\n\n## Examples\n\n### Managing Videos\n\n```javascript\n// Get video details\nconst video = await youtube.videos.getVideo({\n  videoId: \"video-id\"\n});\n\n// Get video transcript\nconst transcript = await youtube.transcripts.getTranscript({\n  videoId: \"video-id\",\n  language: \"en\"\n});\n\n// Search videos\nconst searchResults = await youtube.videos.searchVideos({\n  query: \"search term\",\n  maxResults: 10\n});\n```\n\n### Managing Channels\n\n```javascript\n// Get channel details\nconst channel = await youtube.channels.getChannel({\n  channelId: \"channel-id\"\n});\n\n// List channel videos\nconst videos = await youtube.channels.listVideos({\n  channelId: \"channel-id\",\n  maxResults: 50\n});\n```\n\n### Managing Playlists\n\n```javascript\n// Get playlist items\nconst playlistItems = await youtube.playlists.getPlaylistItems({\n  playlistId: \"playlist-id\",\n  maxResults: 50\n});\n\n// Get playlist details\nconst playlist = await youtube.playlists.getPlaylist({\n  playlistId: \"playlist-id\"\n});\n```\n\n## Development\n\n```bash\n# Install dependencies\nnpm install\n\n# Run tests\nnpm test\n\n# Build\nnpm run build\n\n# Lint\nnpm run lint\n```\n\n## Contributing\nSee CONTRIBUTING.md for information about contributing to this repository.\n\n## License\nThis project is licensed under the MIT License - see the LICENSE file for details.\n",
      "npm_url": "https://www.npmjs.com/package/youtube-mcp-server",
      "npm_downloads": 258,
      "keywords": [
        "youtube",
        "web",
        "search",
        "youtube mcp",
        "zubeidhendricks youtube",
        "interacts youtube"
      ],
      "category": "web-search"
    },
    "Zzzccs123--mcp-baike-render": {
      "owner": "Zzzccs123",
      "name": "mcp-baike-render",
      "url": "https://github.com/Zzzccs123/mcp-baike-render",
      "imageUrl": "/freedevtools/mcp/pfp/Zzzccs123.webp",
      "description": "Access and render content from Baidu Baike, providing detailed discussion data for specific entries and generating readable analyses that highlight key topics and trends. Integrate rich insights from Baidu Baike seamlessly into applications.",
      "stars": 0,
      "forks": 2,
      "license": "No License",
      "language": "TypeScript",
      "updated_at": "2025-03-20T01:37:44Z",
      "readme_content": "# MCP-Baike-Render\n\nMCP-Baike-Render 是一个用于访问和渲染百度百科内容的 MCP Server。它提供了两个主要功能：\n\n1. 通过 `request_baike` 工具获取百度百科讨论数据\n2. 通过 `render_baike` 提示模板将结构化数据渲染为易读的分析内容\n\n## 安装\n\n```bash\n# 安装依赖\nnpm install\n\n# 构建项目\nnpm run build\n```\n\n## 环境变量配置\n\n复制 `.env.example` 文件并重命名为 `.env`，然后根据需要修改其中的配置。\n\n```bash\n# 百度百科API配置\nBAIKE_API_BASE_URL=https://baike.baidu.com/api   # 百度百科API基础URL\nBAIKE_DISCUSSION_API=/discussion/gettashuos      # 百度百科讨论API路径\nDEFAULT_LEMMA_ID=65258669                        # 默认词条ID（当无法解析提供的URL时使用）\nBAIKE_COOKIE=''                                  # 百度百科Cookie，用于API认证和访问限制绕过\n\n# 应用信息\nMCP_SERVER_NAME=Baike-Render\nMCP_SERVER_VERSION=1.0.0\nMCP_SERVER_DESCRIPTION=百度百科内容访问和渲染服务\n```\n\n### Cookie配置说明\n\n`BAIKE_COOKIE` 环境变量用于存储百度百科的Cookie信息，这对于获取某些受限内容或防止请求被限制非常重要。要获取Cookie：\n\n1. 使用浏览器登录百度百科\n2. 使用浏览器开发者工具(F12)，在网络请求中查看任意百科API请求的Cookie头\n3. 复制完整的Cookie值，放入`.env`文件的`BAIKE_COOKIE`字段中\n4. Cookie示例格式：`PSTM=1709884903; BIDUPSID=AC7D9378915FB2E7A091826DB946A7BA; ...`\n\n注意：Cookie包含敏感信息，请勿将其提交到版本控制系统或分享给他人。\n\n## 使用方法\n\n### 启动服务器\n\n```bash\nnpm start\n```\n\n### 开发模式\n\n```bash\nnpm run dev\n```\n\n## 功能\n\n### request_baike 工具\n\n该工具允许获取特定百度百科词条的讨论数据。\n\n参数：\n- `url`: 百度百科URL或词条ID，例如 \"https://baike.baidu.com/item/DeepSeek\" 或 \"65258669\"\n\n### render_baike 提示模板\n\n该提示模板基于获取的百科讨论数据，生成一份全面的分析，包括：\n\n1. 讨论的主要话题\n2. 主要观点及其倾向\n3. 热门讨论\n4. 反映的社会热点或技术趋势\n5. 词条主题的重要性和影响力\n\n## 示例\n\n```javascript\n// 获取百科讨论数据\nconst result = await mcp.tools.request_baike({ url: \"https://baike.baidu.com/item/DeepSeek\" });\n\n// 生成百科内容分析\nconst prompt = await mcp.prompts.render_baike({ url: \"https://baike.baidu.com/item/DeepSeek\" });\n``` ",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "baidu",
        "baike",
        "content",
        "baike render",
        "content baidu",
        "insights baidu"
      ],
      "category": "web-search"
    },
    "anewkim--docusaurus": {
      "owner": "anewkim",
      "name": "docusaurus",
      "url": "https://github.com/anewkim/docusaurus",
      "imageUrl": "/freedevtools/mcp/pfp/anewkim.webp",
      "description": "Generate and deploy modern static websites efficiently, with features for live reloading during development and seamless deployment to GitHub Pages or other static hosting services. Provides an extensible framework for website management and content generation.",
      "stars": 0,
      "forks": 0,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2023-02-23T08:59:06Z",
      "readme_content": "# Website\n\nThis website is built using [Docusaurus 2](https://docusaurus.io/), a modern static website generator.\n\n### Installation\n\n```\n$ yarn\n```\n\n### Local Development\n\n```\n$ yarn start\n```\n\nThis command starts a local development server and opens up a browser window. Most changes are reflected live without having to restart the server.\n\n### Build\n\n```\n$ yarn build\n```\n\nThis command generates static content into the `build` directory and can be served using any static contents hosting service.\n\n### Deployment\n\nUsing SSH:\n\n```\n$ USE_SSH=true yarn deploy\n```\n\nNot using SSH:\n\n```\n$ GIT_USER=<Your GitHub username> yarn deploy\n```\n\nIf you are using GitHub pages for hosting, this command is a convenient way to build the website and push to the `gh-pages` branch.\n",
      "npm_url": "https://www.npmjs.com/package/docusaurus",
      "npm_downloads": 514357,
      "keywords": [
        "anewkim",
        "docusaurus",
        "web",
        "anewkim docusaurus",
        "search anewkim",
        "docusaurus generate"
      ],
      "category": "web-search"
    },
    "aws-powertools--powertools-mcp": {
      "owner": "aws-powertools",
      "name": "powertools-mcp",
      "url": "https://github.com/aws-powertools/powertools-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/aws-powertools.webp",
      "description": "Provides efficient search capabilities for AWS Lambda Powertools documentation across various runtimes, enabling rapid querying and retrieval of relevant documentation content. Supports version-specific searches to ensure results are current with the latest documentation available.",
      "stars": 35,
      "forks": 9,
      "license": "Other",
      "language": "TypeScript",
      "updated_at": "2025-09-28T16:24:30Z",
      "readme_content": "# Powertools for AWS MCP\n\n![NodeSupport](https://img.shields.io/static/v1?label=node&message=%2022&color=green?style=flat-square&logo=node)\n![GitHub Release](https://img.shields.io/github/v/release/aws-powertools/powertools-mcp?include_prereleases)\n[![OpenSSF Scorecard](https://api.securityscorecards.dev/projects/github.com/aws-powertools/powertools-mcp/badge)](https://api.securityscorecards.dev/projects/github.com/aws-powertools/powertools-mcp)\n[![Status](https://img.shields.io/badge/Status-Experimental-orange.svg)](https://shields.io/)\n[![Stability](https://img.shields.io/badge/Stability-Evolving-yellow.svg)](https://shields.io/)\n[![Discord](https://img.shields.io/badge/Discord-Join_Community-7289da.svg)](https://discord.gg/B8zZKbbyET)\n\nThe Powertools for AWS [Model Context Protocol (MCP)](https://modelcontextprotocol.io/introduction) is an MCP implementation that provides search functionality for the Powertools for AWS Lambda documentation across multiple runtimes. It allows your LLM agents to search for documentation and examples related to the toolkit, helping you to quickly find the information you need to use Powertools for AWS Lambda effectively.\n\n> [!WARNING]\n> **This project is experimental and under active development.** APIs and features may change frequently without notice.\n\n## 💡 Get Involved\n\nWe're actively seeking community feedback and feature suggestions [join our Discord](https://discord.gg/B8zZKbbyET) or [open an issue](https://github.com/aws-powertools/powertools-mcp/issues/new/choose) to share your thoughts.\n\n## Use Cases\n\n- Bring documentation and examples directly into your LLM agents' context.\n- Search for specific topics or keywords within the Powertools for AWS documentation.\n- Help your agents understand how to use the Powertools for AWS Lambda toolkit effectively.\n\n## Getting Started\n\n|                                                                                                           Cursor                                                                                                           |                                                                                                                                                             VS Code                                                                                                                                                             |\n| :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------: | :-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------: |\n| [![Install MCP Server](https://cursor.com/deeplink/mcp-install-light.svg)](https://cursor.com/install-mcp?name=powertools&config=JTdCJTIyY29tbWFuZCUyMiUzQSUyMm5weCUyMC15JTIwcG93ZXJ0b29scy1mb3ItYXdzLW1jcCUyMiU3RA%3D%3D) | [![Install on VS Code](https://img.shields.io/badge/Install_on-VS_Code-FF9900?style=flat-square&logo=visualstudiocode&logoColor=white)](https://insiders.vscode.dev/redirect/mcp/install?name=Powertools%20for%20AWS%20MCP&config=%7B%22command%22%3A%22npx%22%2C%22args%22%3A%5B%22-y%22%2C%22powertools-for-aws-mcp%22%5D%7D) |\n\nMost clients that support MCP can use this server out of the box using a configuration similar to the following:\n\n> [!NOTE]\n> If you are using an older version of the MCP, make sure to update your configuration to use the new package name `powertools-for-aws-mcp`.\n\n```json\n{\n  \"mcpServers\": {\n    \"powertools\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"powertools-for-aws-mcp\"]\n    }\n  }\n}\n```\n\nThis setup uses the Node.js package manager to run the MCP server locally and communicate with it using the STDIO interface.\n\n### Client-Specific Setup Instructions\n\nFor detailed setup instructions for specific clients, see the configurations below:\n\n### Getting Started with Amazon Q Developer CLI\n\n<details>\n<summary>Use in Amazon Q Developer CLI</summary>\n\nSee [Amazon Q Developer CLI documentation](https://docs.aws.amazon.com/amazonq/latest/qdeveloper-ug/command-line-mcp-config-CLI.html) for details.\n\n**Add MCP Server using CLI commands:**\n\n```bash\nqchat mcp add --name powertools --command \"npx -y powertools-for-aws-mcp\"\n```\n\n**Manual Configuration:**\nIf you select global scope, the MCP server configuration is stored in `~/.aws/amazonq/mcp.json` and available across all your projects. If you select local scope, the configuration is stored in `.amazonq/mcp.json` within your current project.\n\n#### `~/.aws/amazonq/mcp.json`\n\n```json\n{\n  \"mcpServers\": {\n    \"powertools\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"powertools-for-aws-mcp\"]\n    }\n  }\n}\n```\n\n</details>\n\n### Getting Started with Kiro\n\n<details>\n<summary>Use in Kiro</summary>\n\nSee [Kiro Model Context Protocol Documentation](https://kiro.dev/docs/mcp/configuration/) for details.\n\n1. Navigate to `Kiro` > `MCP Servers`\n2. Add a new MCP server by selecting the `+ Add` button.\n3. Paste the configuration given below:\n\n#### `kiro_mcp_settings.json`\n\nFor macOS/Linux:\n\n```json\n{\n  \"mcpServers\": {\n    \"powertools\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"powertools-for-aws-mcp\"]\n    }\n  }\n}\n```\n\nFor Windows:\n\n```json\n{\n  \"mcpServers\": {\n    \"powertools\": {\n      \"disabled\": false,\n      \"timeout\": 60,\n      \"type\": \"stdio\",\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"powertools-for-aws-mcp\"]\n    }\n  }\n}\n```\n\n</details>\n\n### Getting Started with Cursor\n\n<details>\n<summary>Getting Started with Cursor</summary>\n\n1. You can place MCP configuration in two locations, depending on your use case:\n\nA. **Project Configuration** - For tools specific to a project, create a `.cursor/mcp.json` file in your project directory. - This allows you to define MCP servers that are only available within that specific project.\n\nB. **Global Configuration** - For tools that you want to use across all projects, create a `~/.cursor/mcp.json` file in your home directory. - This makes MCP servers available in all your Cursor workspaces.\n\n#### `.cursor/mcp.json`\n\n```json\n{\n  \"mcpServers\": {\n    \"powertools\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"powertools-for-aws-mcp\"]\n    }\n  }\n}\n```\n\n2. **Using MCP in Chat:** The Composer Agent will automatically use any MCP tools that are listed under Available Tools on the MCP settings page if it determines them to be relevant. To prompt tool usage intentionally, please prompt Cursor to use the desired MCP Server you wish to use. For example, `Using the Powertools MCP Server, do...`\n\n3. **Tool Approval:** By default, when the Agent wants to use an MCP tool, it will display a message asking for your approval. You can use the arrow next to the tool name to expand the message and see what arguments the Agent is calling the tool with.\n\n</details>\n\n### Getting Started with Windsurf\n\n<details>\n<summary>Getting Started with Windsurf</summary>\n\n1. **Access MCP Settings**\n\n   - Navigate to Windsurf - Settings > Advanced Settings or use the Command Palette > Open Windsurf Settings Page\n   - Look for the \"Model Context Protocol (MCP) Servers\" section\n\n2. **Add MCP Servers**\n\n   - Select \"Add Server\" to add a new MCP server\n   - You can choose from available templates like GitHub, Puppeteer, PostgreSQL, etc.\n   - Alternatively, select \"Add custom server\" to configure your own server\n\n3. **Manual Configuration**\n   - You can also manually edit the MCP configuration file located at `~/.codeium/windsurf/mcp_config.json`\n\n#### `~/.codeium/windsurf/mcp_config.json`\n\n```json\n{\n  \"mcpServers\": {\n    \"powertools\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"powertools-for-aws-mcp\"]\n    }\n  }\n}\n```\n\n</details>\n\n### Getting Started with VS Code\n\n<details>\n<summary>Install in VS Code</summary>\n\nConfigure MCP servers in VS Code settings or in `.vscode/mcp.json` (see [VS Code MCP docs](https://code.visualstudio.com/docs/copilot/chat/mcp-servers) for more info.):\n\n#### `.vscode/mcp.json`\n\n```json\n{\n  \"mcpServers\": {\n    \"powertools\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"powertools-for-aws-mcp\"]\n    }\n  }\n}\n```\n\n</details>\n\n### Getting Started with Claude Code\n\n<details>\n<summary>Use in Claude Code</summary>\n\n**Add MCP Server using CLI commands:**\n\n```bash\nclaude mcp add powertools\n```\n\n**Manual Configuration (Recommended):**\nYou can directly edit the configuration file located at `~/.claude.json`. This approach is more flexible and allows you to see all configurations at once.\n\n#### `~/.claude.json`\n\n```json\n{\n  \"mcpServers\": {\n    \"powertools\": {\n      \"type\": \"stdio\",\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"powertools-for-aws-mcp\"]\n    }\n  }\n}\n```\n\n**Restart Claude Code:**\nAfter editing the config file, restart Claude Code for the changes to take effect.\n\n</details>\n\n## Development\n\nAfter cloning the repository, you can set up your development environment by running:\n\n```bash\nnpm ci\nnpm run setup:hooks\n```\n\nAfter that you can run tests using `npm t` or `npm run test:unit:coverage` for coverage reports.\n\nYou can also run the server locally using: `npm run dev`, this will start an inspector server that lets you interact with the MCP server using a browser UI.\n\nIf you want, you can also configure the server to run with Amazon Q, Claude Desktop, or other LLM clients that support the Model Context Protocol (MCP) by using `node` as command and passing the `--experimental-transform-types` flag and the path to the `src/index.ts` file of this project.\n\nFor example, with Claude Code, you can add the server by running:\n\n```bash\nclaude mcp add pt-dev node -- --experimental-transform-types /path/to/project/powertools-mcp/src/index.ts\n```\n\n## Credits\n\n[Michael Walmsley](https://www.linkedin.com/in/walmsles/) at [ServerlessDNA.com](https://serverlessdna.com) for creating the initial implementation of this MCP server and donating it to the Powertools for AWS team at Amazon Web Services.\n\n## License\n\nThis library is licensed under the MIT License. See the [LICENSE](https://github.com/aws-powertools/powertools-mcp/blob/main/LICENSE) file.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "powertools",
        "searches",
        "search",
        "aws powertools",
        "search aws",
        "powertools documentation"
      ],
      "category": "web-search"
    },
    "beordle--time-mcp-server": {
      "owner": "beordle",
      "name": "time-mcp-server",
      "url": "https://github.com/beordle/time-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/beordle.webp",
      "description": "Provides current time information and supports time conversions between different timezones, including handling Daylight Saving Time and calculating time differences.",
      "stars": 0,
      "forks": 0,
      "license": "No License",
      "language": "",
      "updated_at": "2025-03-31T03:45:55Z",
      "readme_content": "# Time MCP Server\n\n> A Model Context Protocol server that provides time-related capabilities. This server enables LLMs to access current time information and convert times between different timezones. This is a Swift implementation of a time MCP server using the MCP Swift SDK.\n\n![Swift Platform](https://img.shields.io/badge/platform-macOS-lightgrey)\n![License](https://img.shields.io/badge/license-MIT-blue)\n\n## ✨ Features\n\n* **Current Time Queries**: Get the current time in any timezone\n* **Time Zone Conversions**: Convert time between different timezones\n* **Daylight Saving Time Information**: Check if a timezone is currently in DST\n* **Time Difference Calculation**: Get the time difference between timezones when converting\n\n## Available Tools\n\n* `get_current_time` - Get the current time in a specific timezone\n  * `timezone` (string, required): IANA timezone name (e.g., 'America/New_York', 'Europe/London'). If empty or not provided, the system timezone will be used.\n\n* `convert_time` - Convert time between timezones\n  * `source_timezone` (string, required): Source IANA timezone name. If empty or not provided, the system timezone will be used.\n  * `time` (string, required): Time to convert in 24-hour format (HH:MM)\n  * `target_timezone` (string, required): Target IANA timezone name. If empty or not provided, the system timezone will be used.\n\n## Installation\n\n### Option 1: One-Line Installation (curl)\n\nThe easiest way to install is with the one-line installer, which automatically downloads the latest version and installs it to `~/.local/bin` in your home directory:\n\n```bash\ncurl -fsSL https://raw.githubusercontent.com/okooo5km/time-mcp-server/main/install.sh | bash\n```\n\nThe installer will:\n\n* Create `~/.local/bin` if it doesn't exist\n* Add this directory to your PATH (in .zshrc or .bashrc)\n* Download and install the latest version\n* Make the binary executable\n\n### Option 2: Build from Source\n\n1. Clone the repository:\n\n   ```bash\n   git clone https://github.com/okooo5km/time-mcp-server.git\n   cd time-mcp-server\n   ```\n\n2. Build the project:\n\n   ```bash\n   swift build -c release\n   ```\n\n3. Install the binary:\n\n   ```bash\n   # Install to user directory (recommended, no sudo required)\n   mkdir -p ~/.local/bin\n   cp $(swift build -c release --show-bin-path)/time-mcp-server ~/.local/bin/\n   ```\n\n   Make sure `~/.local/bin` is in your PATH by adding to your shell configuration file:\n\n   ```bash\n   echo 'export PATH=\"$HOME/.local/bin:$PATH\"' >> ~/.zshrc  # or ~/.bashrc\n   source ~/.zshrc  # or source ~/.bashrc\n   ```\n\n## Command Line Arguments\n\nThe server supports the following command line arguments:\n\n* `-h, --help`: Display help information about the server, its usage, and available options\n* `-v, --version`: Display the version number of the time-mcp-server\n\nExample usage:\n\n```bash\n# Display help information\ntime-mcp-server --help\n\n# Display version information\ntime-mcp-server --version\n```\n\n### Configure for Claude.app\n\nAdd to your Claude settings:\n\n```json\n\"mcpServers\": {\n  \"RealTime\": {\n    \"command\": \"time-mcp-server\"\n  }\n}\n```\n\n### Configure for Cursor\n\nAdd the following configuration to your Cursor editor's Settings - mcp.json:\n\n```json\n{\n  \"mcpServers\": {\n    \"RealTime\": {\n      \"command\": \"time-mcp-server\"\n    }\n  }\n}\n```\n\n\n\n### Configure for ChatWise\n\nAdd the memory MCP server to your Chatwise Settings - Tools.\n\n\n\n### Example System Prompt\n\nYou can use the following system prompt to help Claude utilize the time-mcp-server effectively:\n\n```\nYou have access to time-related tools through MCP. Use these to help users:\n\n- Get the current time in any timezone\n- Convert times between timezones\n- Calculate time differences between locations\n- Plan meetings across different timezones\n\nUse the following tools appropriately:\n- `get_current_time` for checking the current time in a specific timezone\n- `convert_time` when the user needs to convert between timezones\n\nAlways use proper IANA timezone names (e.g., 'America/New_York', 'Europe/London', 'Asia/Tokyo') \nrather than abbreviations or common names.\n```\n\n## Development Requirements\n\n* Swift 6.0 or later\n* macOS 14.0 or later\n* MCP Swift SDK 0.2.0 or later\n\n## Usage Examples\n\n### Getting Current Time\n\n```json\n{\n  \"timezone\": \"America/New_York\"\n}\n```\n\nResponse:\n\n```json\n{\n  \"timezone\": \"America/New_York\",\n  \"datetime\": \"2024-11-05T14:30:45-05:00\",\n  \"is_dst\": false\n}\n```\n\n### Converting Time\n\n```json\n{\n  \"source_timezone\": \"America/Los_Angeles\",\n  \"time\": \"15:30\",\n  \"target_timezone\": \"Asia/Tokyo\"\n}\n```\n\nResponse:\n\n```json\n{\n  \"source\": {\n    \"timezone\": \"America/Los_Angeles\",\n    \"datetime\": \"2024-11-05T15:30:00-08:00\",\n    \"is_dst\": false\n  },\n  \"target\": {\n    \"timezone\": \"Asia/Tokyo\",\n    \"datetime\": \"2024-11-06T08:30:00+09:00\",\n    \"is_dst\": false\n  },\n  \"time_difference\": \"+17h\"\n}\n```\n\n## Use Cases\n\n* **International Meeting Planning**: Schedule meetings across different timezones\n* **Travel Planning**: Check local times at destination\n* **Remote Work Coordination**: Coordinate work hours with international teams\n* **Event Scheduling**: Set up global events with correct local times\n* **Time-Sensitive Operations**: Ensure operations happen at the correct local time\n\n## Version History\n\nSee GitHub Releases for version history and changelog.\n\n## ☕️ Support the Project\n\nIf you find time-mcp-server helpful, please consider supporting its development:\n\n* ⭐️ Star the project on GitHub\n* 🐛 Report bugs or suggest features\n* 💝 Support via:\n\n<p align=\"center\">\n  <a href=\"https://buymeacoffee.com/okooo5km\">\n    <img alt=\"text_Buy_me_a_coffee_emoji_slug_okooo5km_button_colour_FFDD00_font_colour_000000_font_family_Cookie_outline_colour_000000_coffee_colour_ffffff\" src=\"https://img.buymeacoffee.com/button-api/?text=Buy me a coffee&emoji=&slug=okooo5km&button_colour=FFDD00&font_colour=000000&font_family=Cookie&outline_colour=000000&coffee_colour=ffffff\" style=\"border-radius: 8px;\" />\n  </a>\n</p>\n\n## License\n\ntime-mcp-server is licensed under the MIT License. This means you are free to use, modify, and distribute the software, subject to the terms and conditions of the MIT License.\n\n## About\n\nA Swift implementation of a time server for Model Context Protocol (MCP), enabling AI assistants to access current time information and convert between timezones. This project is built using the MCP Swift SDK.",
      "npm_url": "https://www.npmjs.com/package/time-mcp-server",
      "npm_downloads": 0,
      "keywords": [
        "timezones",
        "mcp",
        "daylight",
        "timezones including",
        "time mcp",
        "different timezones"
      ],
      "category": "web-search"
    },
    "bertGPT-feng--deepseek-free-api": {
      "owner": "bertGPT-feng",
      "name": "deepseek-free-api",
      "url": "https://github.com/bertGPT-feng/deepseek-free-api",
      "imageUrl": "/freedevtools/mcp/pfp/bertGPT-feng.webp",
      "description": "Provides access to conversational AI capabilities, enabling multi-turn dialogues and internet searches with zero configuration deployment. Fully compatible with ChatGPT interfaces for enhanced interaction.",
      "stars": 0,
      "forks": 0,
      "license": "GNU General Public License v3.0",
      "language": "",
      "updated_at": "2025-02-28T12:23:21Z",
      "readme_content": "# DeepSeek V3 Free 服务\n\n<span>[ 中文 | <a href=\"README_EN.md\">English</a> ]</span>\n\n[![deepseek_free_api](https://img.shields.io/github/license/llm-red-team/deepseek-free-api.svg)](LICENSE)\n![deepseek_free_api](https://img.shields.io/github/stars/llm-red-team/deepseek-free-api.svg)\n![deepseek_free_api](https://img.shields.io/github/forks/llm-red-team/deepseek-free-api.svg)\n![deepseek_free_api](https://img.shields.io/docker/pulls/vinlic/deepseek-free-api.svg)\n\n# 风险警告\n\n## **近期，我们发现部分自媒体引导用户将本仓库源码或镜像部署至非个人使用渠道，并公开提供服务。此行为可能违反了DeepSeek的[《用户协议》](https://chat.deepseek.com/downloads/DeepSeek%20Terms%20of%20Use.html)。我们特此提醒，请相关自媒体和个人立即停止此类不当行为。若持续违规，DeepSeek官方将保留依法追究其法律责任的权利。**\n\n支持高速流式输出、支持多轮对话、支持联网搜索、支持R1深度思考和静默深度思考，零配置部署，多路token支持。\n\n与ChatGPT接口完全兼容。\n\n还有以下十个free-api欢迎关注：\n\nMoonshot AI（Kimi.ai）接口转API [kimi-free-api](https://github.com/LLM-Red-Team/kimi-free-api)\n\n智谱AI (智谱清言) 接口转API [glm-free-api](https://github.com/LLM-Red-Team/glm-free-api)\n\n阶跃星辰 (跃问StepChat) 接口转API [step-free-api](https://github.com/LLM-Red-Team/step-free-api)\n\n阿里通义 (Qwen) 接口转API [qwen-free-api](https://github.com/LLM-Red-Team/qwen-free-api)\n\n秘塔AI (Metaso) 接口转API [metaso-free-api](https://github.com/LLM-Red-Team/metaso-free-api)\n\n字节跳动（豆包）接口转API [doubao-free-api](https://github.com/LLM-Red-Team/doubao-free-api)\n\n字节跳动（即梦AI）接口转API [jimeng-free-api](https://github.com/LLM-Red-Team/jimeng-free-api)\n\n讯飞星火（Spark）接口转API [spark-free-api](https://github.com/LLM-Red-Team/spark-free-api)\n\nMiniMax（海螺AI）接口转API [hailuo-free-api](https://github.com/LLM-Red-Team/hailuo-free-api)\n\n聆心智能 (Emohaa) 接口转API [emohaa-free-api](https://github.com/LLM-Red-Team/emohaa-free-api)\n\n## 目录\n\n* [免责声明](#免责声明)\n* [效果示例](#效果示例)\n* [接入准备](#接入准备)\n  * [多账号接入](#多账号接入)\n* [Docker部署](#Docker部署)\n  * [Docker-compose部署](#Docker-compose部署)\n* [Render部署](#Render部署)\n* [Vercel部署](#Vercel部署)\n* [原生部署](#原生部署)\n* [推荐使用客户端](#推荐使用客户端)\n* [接口列表](#接口列表)\n  * [对话补全](#对话补全)\n  * [userToken存活检测](#userToken存活检测)\n* [注意事项](#注意事项)\n  * [Nginx反代优化](#Nginx反代优化)\n  * [Token统计](#Token统计)\n* [Star History](#star-history)\n  \n## 免责声明\n\n**逆向API是不稳定的，建议前往DeepSeek官方 https://platform.deepseek.com/ 付费使用API，避免封禁的风险。**\n\n**本组织和个人不接受任何资金捐助和交易，此项目是纯粹研究交流学习性质！**\n\n**仅限自用，禁止对外提供服务或商用，避免对官方造成服务压力，否则风险自担！**\n\n**仅限自用，禁止对外提供服务或商用，避免对官方造成服务压力，否则风险自担！**\n\n**仅限自用，禁止对外提供服务或商用，避免对官方造成服务压力，否则风险自担！**\n\n## 效果示例\n\n### 验明正身Demo\n\n\n\n### 多轮对话Demo\n\n\n\n### 联网搜索Demo\n\n\n\n## 接入准备\n\n请确保您在中国境内或者拥有中国境内的个人计算设备，否则部署后可能因无法访问DeepSeek而无法使用。\n\n从 [DeepSeek](https://chat.deepseek.com/) 获取userToken value\n\n进入DeepSeek随便发起一个对话，然后F12打开开发者工具，从Application > LocalStorage中找到`userToken`中的value值，这将作为Authorization的Bearer Token值：`Authorization: Bearer TOKEN`\n\n\n\n### 多账号接入\n\n目前同个账号同时只能有*一路*输出，你可以通过提供多个账号的userToken value并使用`,`拼接提供：\n\n`Authorization: Bearer TOKEN1,TOKEN2,TOKEN3`\n\n每次请求服务会从中挑选一个。\n\n### 环境变量（可选）\n\n| 环境变量 | 是否必填 | 说明                               |\n|------|------|----------------------------------|\n|  DEEP_SEEK_CHAT_AUTHORIZATION   | 否    | 当配置了token 则使用token，未配置则需要在请求头中传递Authorization |\n\n## Docker部署\n\n拉取镜像并启动服务。\n\n```shell\ndocker run -it -d --init --name deepseek-free-api -p 8000:8000 -e TZ=Asia/Shanghai  vinlic/deepseek-free-api:latest\n# 或将token配置在环境变量\ndocker run -it -d --init --name deepseek-free-api -p 8000:8000 -e TZ=Asia/Shanghai -e DEEP_SEEK_CHAT_AUTHORIZATION=xxx  vinlic/deepseek-free-api:latest\n```\n\n查看服务实时日志\n\n```shell\ndocker logs -f deepseek-free-api\n```\n\n重启服务\n\n```shell\ndocker restart deepseek-free-api\n```\n\n停止服务\n\n```shell\ndocker stop deepseek-free-api\n```\n\n### Docker-compose部署\n\n```yaml\nversion: '3'\n\nservices:\n  deepseek-free-api:\n    container_name: deepseek-free-api\n    image: vinlic/deepseek-free-api:latest\n    restart: always\n    ports:\n      - \"8000:8000\"\n    environment:\n      - TZ=Asia/Shanghai\n```\n\n### Render部署\n\n**注意：部分部署区域可能无法连接deepseek，如容器日志出现请求超时或无法连接，请切换其他区域部署！**\n**注意：免费账户的容器实例将在一段时间不活动时自动停止运行，这会导致下次请求时遇到50秒或更长的延迟，建议查看[Render容器保活](https://github.com/LLM-Red-Team/free-api-hub/#Render%E5%AE%B9%E5%99%A8%E4%BF%9D%E6%B4%BB)**\n\n1. fork本项目到你的github账号下。\n\n2. 访问 [Render](https://dashboard.render.com/) 并登录你的github账号。\n\n3. 构建你的 Web Service（New+ -> Build and deploy from a Git repository -> Connect你fork的项目 -> 选择部署区域 -> 选择实例类型为Free -> Create Web Service）。\n\n4. 等待构建完成后，复制分配的域名并拼接URL访问即可。\n\n### Vercel部署\n\n**注意：Vercel免费账户的请求响应超时时间为10秒，但接口响应通常较久，可能会遇到Vercel返回的504超时错误！**\n\n请先确保安装了Node.js环境。\n\n```shell\nnpm i -g vercel --registry http://registry.npmmirror.com\nvercel login\ngit clone https://github.com/LLM-Red-Team/deepseek-free-api\ncd deepseek-free-api\nvercel --prod\n```\n\n## 原生部署\n\n请先安装好Node.js环境并且配置好环境变量，确认node命令可用。\n\n安装依赖\n\n```shell\nnpm i\n```\n\n安装PM2进行进程守护\n\n```shell\nnpm i -g pm2\n```\n\n编译构建，看到dist目录就是构建完成\n\n```shell\nnpm run build\n```\n\n启动服务\n\n```shell\npm2 start dist/index.js --name \"deepseek-free-api\"\n```\n\n查看服务实时日志\n\n```shell\npm2 logs deepseek-free-api\n```\n\n重启服务\n\n```shell\npm2 reload deepseek-free-api\n```\n\n停止服务\n\n```shell\npm2 stop deepseek-free-api\n```\n\n## 推荐使用客户端\n\n使用以下二次开发客户端接入free-api系列项目更快更简单，支持文档/图像上传！\n\n由 [Clivia](https://github.com/Yanyutin753/lobe-chat) 二次开发的LobeChat [https://github.com/Yanyutin753/lobe-chat](https://github.com/Yanyutin753/lobe-chat)\n\n由 [时光@](https://github.com/SuYxh) 二次开发的ChatGPT Web [https://github.com/SuYxh/chatgpt-web-sea](https://github.com/SuYxh/chatgpt-web-sea)\n\n## 接口列表\n\n目前支持与openai兼容的 `/v1/chat/completions` 接口，可自行使用与openai或其他兼容的客户端接入接口，或者使用 [dify](https://dify.ai/) 等线上服务接入使用。\n\n### 对话补全\n\n对话补全接口，与openai的 [chat-completions-api](https://platform.openai.com/docs/guides/text-generation/chat-completions-api) 兼容。\n\n**POST /v1/chat/completions**\n\nheader 需要设置 Authorization 头部：\n\n```\nAuthorization: Bearer [userToken value]\n```\n\n请求数据：\n```json\n{\n    // model名称\n    // 默认：deepseek\n    // 深度思考：deepseek-think 或 deepseek-r1\n    // 联网搜索：deepseek-search\n    // 深度思考+联网搜索：deepseek-r1-search 或 deepseek-think-search\n    // 静默模式（不输出思考过程或联网搜索结果）：deepseek-think-silent 或 deepseek-r1-silent 或 deepseek-search-silent\n    // 深度思考但思考过程使用<details>可折叠标签包裹（需要页面支持显示）：deepseek-think-fold 或 deepseek-r1-fold\n    \"model\": \"deepseek\",\n    // 默认多轮对话基于消息合并实现，某些场景可能导致能力下降且受单轮最大token数限制\n    // 如果您想获得原生的多轮对话体验，可以传入上一轮消息获得的id，来接续上下文\n    // \"conversation_id\": \"50207e56-747e-4800-9068-c6fd618374ee@2\",\n    \"messages\": [\n        {\n            \"role\": \"user\",\n            \"content\": \"你是谁？\"\n        }\n    ],\n    // 如果使用流式响应请设置为true，默认false\n    \"stream\": false\n}\n```\n\n响应数据：\n```json\n{\n    \"id\": \"50207e56-747e-4800-9068-c6fd618374ee@2\",\n    \"model\": \"deepseek\",\n    \"object\": \"chat.completion\",\n    \"choices\": [\n        {\n            \"index\": 0,\n            \"message\": {\n                \"role\": \"assistant\",\n                \"content\": \" 我是DeepSeek Chat，一个由深度求索公司开发的智能助手，旨在通过自然语言处理和机器学习技术来提供信息查询、对话交流和解答问题等服务。\"\n            },\n            \"finish_reason\": \"stop\"\n        }\n    ],\n    \"usage\": {\n        \"prompt_tokens\": 1,\n        \"completion_tokens\": 1,\n        \"total_tokens\": 2\n    },\n    \"created\": 1715061432\n}\n```\n\n### userToken存活检测\n\n检测userToken是否存活，如果存活live为true，否则为false，请不要频繁（小于10分钟）调用此接口。\n\n**POST /token/check**\n\n请求数据：\n```json\n{\n    \"token\": \"eyJhbGciOiJIUzUxMiIsInR5cCI6IkpXVCJ9...\"\n}\n```\n\n响应数据：\n```json\n{\n    \"live\": true\n}\n```\n\n## 注意事项\n\n### Nginx反代优化\n\n如果您正在使用Nginx反向代理deepseek-free-api，请添加以下配置项优化流的输出效果，优化体验感。\n\n```nginx\n# 关闭代理缓冲。当设置为off时，Nginx会立即将客户端请求发送到后端服务器，并立即将从后端服务器接收到的响应发送回客户端。\nproxy_buffering off;\n# 启用分块传输编码。分块传输编码允许服务器为动态生成的内容分块发送数据，而不需要预先知道内容的大小。\nchunked_transfer_encoding on;\n# 开启TCP_NOPUSH，这告诉Nginx在数据包发送到客户端之前，尽可能地发送数据。这通常在sendfile使用时配合使用，可以提高网络效率。\ntcp_nopush on;\n# 开启TCP_NODELAY，这告诉Nginx不延迟发送数据，立即发送小数据包。在某些情况下，这可以减少网络的延迟。\ntcp_nodelay on;\n# 设置保持连接的超时时间，这里设置为120秒。如果在这段时间内，客户端和服务器之间没有进一步的通信，连接将被关闭。\nkeepalive_timeout 120;\n```\n\n### Token统计\n\n由于推理侧不在deepseek-free-api，因此token不可统计，将以固定数字返回。\n\n## Star History\n\n[![Star History Chart](https://api.star-history.com/svg?repos=LLM-Red-Team/deepseek-free-api&type=Date)](https://star-history.com/#LLM-Red-Team/deepseek-free-api&Date)",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "bertgpt",
        "dialogues",
        "chatgpt",
        "search bertgpt",
        "conversational ai",
        "dialogues internet"
      ],
      "category": "web-search"
    },
    "bharathvaj-ganesan--whois-mcp": {
      "owner": "bharathvaj-ganesan",
      "name": "whois-mcp",
      "url": "https://github.com/bharathvaj-ganesan/whois-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/bharathvaj-ganesan.webp",
      "description": "Perform WHOIS lookups to retrieve domain ownership details, registration dates, and availability directly from AI agents without leaving the workspace.",
      "stars": 43,
      "forks": 16,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-09-26T14:47:41Z",
      "readme_content": "# Whois MCP\n\n[Model Context Protocol](https://modelcontextprotocol.io) server for whois lookups.\n\n<a href=\"https://glama.ai/mcp/servers/cwu9e3fcwg\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/cwu9e3fcwg/badge\" alt=\"Whois MCP server\" />\n</a>\n\n**Cursor IDE Demo**\n\nhttps://github.com/user-attachments/assets/57a82adc-3f30-453f-aabd-7138c2e6a21d\n\n**Claude Desktop Demo**\n\nhttps://github.com/user-attachments/assets/d30a1f45-fdaf-4280-80f2-d5d4fc9743b1\n\n## Overview\n\nThis MCP server allows AI agents like Claude Desktop, Cursor, Windsurf,.. etc to perform WHOIS lookups and retrieve domain details. \n\n**Purpose**\nYou can directly ask the AI to check if a domain is available, who owns it, when it was registered, and other important details. No need to go to browser and search.\n\n**What is a WHOIS Lookup?**\nA WHOIS lookup is the process of querying a WHOIS database to retrieve registration details about a domain name, IP address, or autonomous system. It helps users find out who owns a domain, when it was registered, when it expires, and other important details.\n\n**What Information Can a WHOIS Lookup Provide?**\n\nWhen you perform a WHOIS lookup, you can retrieve details such as:\n\n- Domain Name – The specific domain queried\n- Registrar Name – The company managing the domain registration (e.g., GoDaddy, Namecheap)\n- Registrant Details – The name, organization, and contact details of the domain owner (unless protected by WHOIS privacy)\n- Registration & Expiry Date – When the domain was registered and when it will expire\n- Name Servers – The DNS servers the domain is using\n- Domain Status – Active, expired, locked, or pending deletion\n- Contact Information – Administrative, technical, and billing contacts (if not hidden)\n\n## Available Tools\n\n| Tool                  | Description                                |\n| --------------------- | ------------------------------------------ |\n| `whois_domain`        | Looksup whois information about the domain |\n| `whois_tld`           | Looksup whois information about the Top Level Domain (TLD)    |\n| `whois_ip`            | Looksup whois information about the IP     |\n| `whois_as`            | Looksup whois information about the Autonomous System Number (ASN)     |\n\n## Using with Cursor\n\n**Installation - Globally**\n\nRun the MCP server using npx:\n\n```bash\nnpx -y @bharathvaj/whois-mcp@latest\n```\n\nIn your Cursor IDE\n\n1. Go to `Cursor Settings` > `MCP`\n2. Click `+ Add New MCP Server`\n3. Fill in the form:\n   - Name: `Whois Lookup` (or any name you prefer)\n   - Type: `command`\n   - Command: `npx -y @bharathvaj/whois-mcp@latest`\n\n\n**Installation - Project-specific**\n\nAdd an `.cursor/mcp.json` file to your project:\n\n```json\n{\n  \"mcpServers\": {\n    \"whois\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"@bharathvaj/whois-mcp@latest\"\n      ]\n    }\n  }\n}\n```\n\n**Usage**\n\nOnce configured, the whois tools will be automatically available to the Cursor AI Agent. You can:\n\n1. The tool will be listed under `Available Tools` in MCP settings\n2. Agent will automatically use it when relevant\n3. You can explicitly ask Agent to send notifications\n\n## Using with Roo Code\nAccess the MCP settings by clicking “Edit MCP Settings” in Roo Code settings or using the “Roo Code: Open MCP Config” command in VS Code's command palette.\n\n```json\n{\n  \"mcpServers\": {\n    \"whois\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"@bharathvaj/whois-mcp@latest\"\n      ]\n    }\n  }\n}\n```\n3. The whois capabilities will be available to Roo Code's AI agents\n\n## Development\n\n```bash\n# Install dependencies\npnpm install\n\n# Build\npnpm build\n\n```\n\n## Debugging the Server\n\nTo debug your server, you can use the [MCP Inspector](https://github.com/modelcontextprotocol/inspector).\n\nFirst build the server\n\n```\npnpm build\n```\n\nRun the following command in your terminal:\n\n```\n# Start MCP Inspector and server with all tools\nnpx @modelcontextprotocol/inspector node dist/index.js\n```\n\n## License\n\n[MIT](LICENSE)",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "whois",
        "domain",
        "lookups",
        "whois lookups",
        "ganesan whois",
        "whois mcp"
      ],
      "category": "web-search"
    },
    "bilhasry-deriv--mcp-web-a11y": {
      "owner": "bilhasry-deriv",
      "name": "mcp-web-a11y",
      "url": "https://github.com/bilhasry-deriv/mcp-web-a11y",
      "imageUrl": "/freedevtools/mcp/pfp/bilhasry-deriv.webp",
      "description": "Analyze web accessibility of any URL and simulate color blindness using color matrices. The server provides detailed reporting based on the accessibility analysis results.",
      "stars": 4,
      "forks": 3,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-09-08T14:11:33Z",
      "readme_content": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/bilhasry-deriv-mcp-web-a11y-badge.png)](https://mseep.ai/app/bilhasry-deriv-mcp-web-a11y)\n\n# Web Accessibility MCP Server\n\n[![smithery badge](https://smithery.ai/badge/@bilhasry-deriv/mcp-web-a11y)](https://smithery.ai/server/@bilhasry-deriv/mcp-web-a11y)\n\nAn MCP (Model Context Protocol) server that provides web accessibility analysis capabilities using axe-core and Puppeteer.\n\n<a href=\"https://glama.ai/mcp/servers/mya2mkxy9a\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/mya2mkxy9a/badge\" alt=\"Web Accessibility Server MCP server\" /></a>\n\n## Features\n\n- Analyze web accessibility of any URL using axe-core\n- Simulate color blindness (protanopia, deuteranopia, tritanopia) using color matrices\n- Detailed reporting of accessibility violations\n- Support for custom user agents and selectors\n- Debug logging for troubleshooting\n- Comprehensive accessibility checks based on WCAG guidelines\n\n## Prerequisites\n\n- Node.js (v14 or higher)\n- npm\n\n## Installation\n\n### Installing via Smithery\n\nTo install Web Accessibility MCP Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@bilhasry-deriv/mcp-web-a11y):\n\n```bash\nnpx -y @smithery/cli install @bilhasry-deriv/mcp-web-a11y --client claude\n```\n\n### Manual Installation\n1. Clone the repository:\n```bash\ngit clone [repository-url]\ncd mcp-web-a11y\n```\n\n2. Install dependencies:\n```bash\nnpm install\n```\n\n3. Build the server:\n```bash\nnpm run build\n```\n\n## Configuration\n\nAdd the server to your MCP settings file (typically located at `~/Library/Application Support/Code/User/globalStorage/saoudrizwan.claude-dev/settings/cline_mcp_settings.json`):\n\n```json\n{\n  \"mcpServers\": {\n    \"web-a11y\": {\n      \"command\": \"node\",\n      \"args\": [\"/path/to/mcp-web-a11y/build/index.js\"],\n      \"disabled\": false,\n      \"autoApprove\": [],\n      \"env\": {\n        \"MCP_OUTPUT_DIR\": \"/path/to/output/directory\"\n      }\n    }\n  }\n}\n```\n\n### Environment Variables\n\n- `MCP_OUTPUT_DIR`: Directory where screenshot outputs will be saved\n  - Required for the `simulate_colorblind` tool\n  - If not specified, defaults to './output' relative to the current working directory\n  - Must be an absolute path when configured in MCP settings\n\n## Usage\n\nThe server provides two tools: `check_accessibility` for analyzing web accessibility and `simulate_colorblind` for simulating color blindness.\n\n### Tool: check_accessibility\n\nChecks the accessibility of a given URL using axe-core.\n\n#### Parameters\n\n- `url` (required): The URL to analyze\n- `waitForSelector` (optional): CSS selector to wait for before analysis\n- `userAgent` (optional): Custom user agent string for the request\n\n#### Example Usage\n\n```typescript\n<use_mcp_tool>\n<server_name>mcp-web-a11y</server_name>\n<tool_name>check_accessibility</tool_name>\n<arguments>\n{\n  \"url\": \"https://example.com\",\n  \"waitForSelector\": \".main-content\",\n  \"userAgent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n}\n</arguments>\n</use_mcp_tool>\n```\n\n### Tool: simulate_colorblind\n\nSimulates how a webpage appears to users with different types of color blindness using color matrix transformations.\n\n#### Color Blindness Types\n\nThe tool supports three types of color blindness simulation:\n\n1. **Protanopia** (red-blind) - Uses matrix:\n   ```\n   0.567, 0.433, 0\n   0.558, 0.442, 0\n   0, 0.242, 0.758\n   ```\n\n2. **Deuteranopia** (green-blind) - Uses matrix:\n   ```\n   0.625, 0.375, 0\n   0.7, 0.3, 0\n   0, 0.3, 0.7\n   ```\n\n3. **Tritanopia** (blue-blind) - Uses matrix:\n   ```\n   0.95, 0.05, 0\n   0, 0.433, 0.567\n   0, 0.475, 0.525\n   ```\n\n#### Parameters\n\n- `url` (required): The URL to capture\n- `type` (required): Type of color blindness to simulate ('protanopia', 'deuteranopia', or 'tritanopia')\n- `outputPath` (optional): Custom path for the screenshot output\n- `userAgent` (optional): Custom user agent string for the request\n\n#### Example Usage\n\n```typescript\n<use_mcp_tool>\n<server_name>mcp-web-a11y</server_name>\n<tool_name>simulate_colorblind</tool_name>\n<arguments>\n{\n  \"url\": \"https://example.com\",\n  \"type\": \"deuteranopia\",\n  \"outputPath\": \"colorblind_simulation.png\"\n}\n</arguments>\n</use_mcp_tool>\n```\n\n### Response Format\n\n#### check_accessibility Response\n\n```json\n{\n  \"url\": \"analyzed-url\",\n  \"timestamp\": \"ISO-timestamp\",\n  \"violations\": [\n    {\n      \"impact\": \"serious|critical|moderate|minor\",\n      \"description\": \"Description of the violation\",\n      \"help\": \"Help text explaining the issue\",\n      \"helpUrl\": \"URL to detailed documentation\",\n      \"nodes\": [\n        {\n          \"html\": \"HTML of the affected element\",\n          \"failureSummary\": \"Summary of what needs to be fixed\"\n        }\n      ]\n    }\n  ],\n  \"passes\": 42,\n  \"inapplicable\": 45,\n  \"incomplete\": 3\n}\n```\n\n#### simulate_colorblind Response\n\n```json\n{\n  \"url\": \"analyzed-url\",\n  \"type\": \"colorblind-type\",\n  \"outputPath\": \"path/to/screenshot.png\",\n  \"timestamp\": \"ISO-timestamp\",\n  \"message\": \"Screenshot saved with [type] simulation\"\n}\n```\n\n### Error Handling\n\nThe server includes comprehensive error handling for common scenarios:\n\n- Network errors\n- Invalid URLs\n- Timeout issues\n- DNS resolution problems\n\nError responses will include detailed messages to help diagnose the issue.\n\n## Development\n\n### Project Structure\n\n```\nmcp-web-a11y/\n├── src/\n│   └── index.ts    # Main server implementation\n├── build/          # Compiled JavaScript\n├── output/         # Generated screenshots\n├── package.json    # Project dependencies and scripts\n└── tsconfig.json   # TypeScript configuration\n```\n\n### Building\n\n```bash\nnpm run build\n```\n\nThis will:\n1. Compile TypeScript to JavaScript\n2. Make the output file executable\n3. Place the compiled files in the `build` directory\n\n### Debugging\n\nThe server includes detailed debug logging that can be observed in the console output. This includes:\n- Network requests and responses\n- Page loading status\n- Selector waiting status\n- Any console messages from the analyzed page\n- Color simulation progress\n\n## Common Issues and Solutions\n\n1. **Timeout Errors**\n   - Increase the timeout value in the code\n   - Check network connectivity\n   - Verify the URL is accessible\n\n2. **DNS Resolution Errors**\n   - Verify the URL is correct\n   - Check network connectivity\n   - Try using the www subdomain\n\n3. **Selector Not Found**\n   - Verify the selector exists on the page\n   - Wait for dynamic content to load\n   - Check the page source for the correct selector\n\n4. **Color Simulation Issues**\n   - Ensure the page's colors are specified in a supported format (RGB, RGBA, or HEX)\n   - Check if the page uses dynamic color changes (may require additional wait time)\n   - Verify the screenshot output directory exists and is writable\n\n## Contributing\n\n1. Fork the repository\n2. Create a feature branch\n3. Commit your changes\n4. Push to the branch\n5. Create a Pull Request\n\n## License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "accessibility",
        "bilhasry",
        "blindness",
        "web accessibility",
        "accessibility url",
        "color blindness"
      ],
      "category": "web-search"
    },
    "billster45--mcp-chatgpt-responses": {
      "owner": "billster45",
      "name": "mcp-chatgpt-responses",
      "url": "https://github.com/billster45/mcp-chatgpt-responses",
      "imageUrl": "/freedevtools/mcp/pfp/billster45.webp",
      "description": "Access OpenAI's ChatGPT API for dynamic conversations with Claude Desktop, featuring customizable parameters and automatic conversation state management. Engage in real-time discussions between AI models with web search capabilities.",
      "stars": 13,
      "forks": 7,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-08-02T03:17:10Z",
      "readme_content": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/billster45-mcp-chatgpt-responses-badge.png)](https://mseep.ai/app/billster45-mcp-chatgpt-responses)\n\n# MCP ChatGPT Server\n[![smithery badge](https://smithery.ai/badge/@billster45/mcp-chatgpt-responses)](https://smithery.ai/server/@billster45/mcp-chatgpt-responses)\n\nThis MCP server allows you to access OpenAI's ChatGPT API directly from Claude Desktop.\n\n📝 **Read about why I built this project**: [I Built an AI That Talks to Other AIs: Demystifying the MCP Hype](https://medium.com/@billcockerill/i-built-an-ai-that-talks-to-other-ais-demystifying-the-mcp-hype-88dc03520552)\n\n## Features\n\n- Call the ChatGPT API with customisable parameters\n- Aks Claude and ChatGPT to talk to each other in a long running discussion!\n- Configure model versions, temperature, and other parameters\n- Use web search to get up-to-date information from the internet\n- Uses OpenAI's Responses API for automatic conversation state management\n- Use your own OpenAI API key\n\n## Setup Instructions\n\n### Installing via Smithery\n\nTo install ChatGPT Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@billster45/mcp-chatgpt-responses):\n\n```bash\nnpx -y @smithery/cli install @billster45/mcp-chatgpt-responses --client claude\n```\n\n### Prerequisites\n\n- Python 3.10 or higher\n- [Claude Desktop](https://claude.ai/download) application\n- [OpenAI API key](https://platform.openai.com/settings/organization/api-keys)\n- [uv](https://github.com/astral-sh/uv) for Python package management\n\n### Installation\n\n1. Clone this repository:\n   ```bash\n   git clone https://github.com/billster45/mcp-chatgpt-responses.git\n   cd mcp-chatgpt-responses\n   ```\n\n2. Set up a virtual environment and install dependencies using uv:\n   ```bash\n   uv venv\n   ```\n\n   ```bash\n   .venv\\\\Scripts\\\\activate\n   ```\n   \n   ```bash\n   uv pip install -r requirements.txt\n   ```\n\n### Using with Claude Desktop\n\n1. Configure Claude Desktop to use this MCP server by following the instructions at:\n   [MCP Quickstart Guide](https://modelcontextprotocol.io/quickstart/user#2-add-the-filesystem-mcp-server)\n\n2. Add the following configuration to your Claude Desktop config file (adjust paths as needed):\n   ```json\n   {\n     \"mcpServers\": {\n       \"chatgpt\": {\n         \"command\": \"uv\",\n         \"args\": [\n           \"--directory\",\n           \"\\\\path\\\\to\\\\mcp-chatgpt-responses\",\n           \"run\",\n           \"chatgpt_server.py\"\n         ],\n         \"env\": {\n           \"OPENAI_API_KEY\": \"your-api-key-here\",\n           \"DEFAULT_MODEL\": \"gpt-4o\",\n           \"DEFAULT_TEMPERATURE\": \"0.7\",\n           \"MAX_TOKENS\": \"1000\"\n         }\n       }\n     }\n   }\n   ```\n\n3. Restart Claude Desktop.\n\n4. You can now use the ChatGPT API through Claude by asking questions that mention ChatGPT or that Claude might not be able to answer.\n\n## Available Tools\n\nThe MCP server provides the following tools:\n\n1. `ask_chatgpt(prompt, model, temperature, max_output_tokens, response_id)` - Send a prompt to ChatGPT and get a response\n\n2. `ask_chatgpt_with_web_search(prompt, model, temperature, max_output_tokens, response_id)` - Send a prompt to ChatGPT with web search enabled to get up-to-date information\n\n## Example Usage\n\n### Basic ChatGPT usage:\n\nTell Claude to ask ChatGPT a question!\n```\nUse the ask_chatgpt tool to answer: What is the best way to learn Python?\n```\n\nTell Claude to have a conversation with ChatGPT:\n```\nUse the ask_chatgpt tool to have a two way conversation between you and ChatGPT about the topic that is most important to you.\n```\nNote how in a turn taking conversation the response id allows ChatGPT to store the history of the conversation so its a genuine conversation and not just as series of API calls. This is called [conversation state](https://platform.openai.com/docs/guides/conversation-state?api-mode=responses#openai-apis-for-conversation-state).\n\n### With web search:\n\nFor questions that may benefit from up-to-date information:\n```\nUse the ask_chatgpt_with_web_search tool to answer: What are the latest developments in quantum computing?\n```\n\nNow try web search in agentic way to plan your perfect day out based on the weather!\n```\nUse the ask_chatgpt_with_web_search tool to find the weather tomorrow in New York, then based on that weather and what it returns, keep using the tool to build up a great day out for someone who loves food and parks\n```\n\n## How It Works\n\nThis tool utilizes OpenAI's Responses API, which automatically maintains conversation state on OpenAI's servers. This approach:\n\n1. Simplifies code by letting OpenAI handle the conversation history\n2. Provides more reliable context tracking\n3. Improves the user experience by maintaining context across messages\n4. Allows access to the latest information from the web with the web search tool\n\n## License\n\nMIT License\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "chatgpt",
        "conversations",
        "conversation",
        "openai chatgpt",
        "chatgpt api",
        "automatic conversation"
      ],
      "category": "web-search"
    },
    "blake365--macrostrat-mcp": {
      "owner": "blake365",
      "name": "macrostrat-mcp",
      "url": "https://github.com/blake365/macrostrat-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/blake365.webp",
      "description": "Query and analyze comprehensive geologic data using the Macrostrat API, providing access to geologic units, minerals, timescales, and more. Create visualizations based on geological data to enhance understanding.",
      "stars": 3,
      "forks": 5,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-08-26T01:43:52Z",
      "readme_content": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/blake365-macrostrat-mcp-badge.png)](https://mseep.ai/app/blake365-macrostrat-mcp)\n\n# macrostrat-mcp\nMCP server for Macrostrat API\n\n[![smithery badge](https://smithery.ai/badge/@blake365/macrostrat-mcp)](https://smithery.ai/server/@blake365/macrostrat-mcp)\n\n<a href=\"https://glama.ai/mcp/servers/v67anfiq0s\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/v67anfiq0s/badge\" alt=\"Macrostrat Server MCP server\" />\n</a>\n\nAn MCP server implementation for providing access to the [Macrostrat API](https://macrostrat.org/api) within [Claude Desktop](https://claude.ai/download).\n\n## Overview\n\nThis Model Context Protocol (MCP) server enables AI assistants and applications to:\n\n- Query comprehensive geologic data and information provided by the Macrostrat API\n- Access geologic units, columns, minerals, timescales, and more\n- Get geologic map tiles that the LLM can interpret (questionable utility, but might be useful for some applications)\n- Analyze geologic data to improve geological understanding\n- Create graphics and perform data analysis from the data\n\nUsers can run queries using natural language.\n\n## Sample Output\n\n**Q. What is the geologic history of Nashville, TN?**\n\nA. \nLet me check the geological data for Nashville's location.\n\nView Result from find-units from macrostrat (local)\n```json\n{\n  \"lat\": 36.1627,\n  \"lng\": -86.7816,\n  \"responseType\": \"long\"\n}\n```\n\n### Geological History of Nashville, Tennessee\n1. Mesoproterozoic Basement (1.6-1.1 billion years ago)\n- Igneous basement rocks (granite and rhyolite)\n- Part of the ancient North American craton\n\n2. Early-Middle Cambrian (518-504 million years ago)\n- Rome Formation: Mixed carbonate-siliciclastic sediments\n- Shallow marine environment\n- Part of the developing continental shelf\n\n3. Middle-Late Cambrian (504-498 million years ago)\n- Conasauga Formation: Limestone and mixed sediments\n- Continued marine deposition\n- Alternating limestone and shale indicating fluctuating sea levels\n\n4. Late Cambrian-Early Ordovician (498-470 million years ago)\n- Knox Dolomite: Massive dolomite sequence\n- Extensive shallow marine platform\n- Up to 1,200 meters thick\n- Represents long period of stable marine conditions\n\n5. Middle-Late Ordovician (470-443 million years ago)\nSeveral important formations:\n- Stones River Group (including Murfreesboro, Pierce, Lebanon Limestones)\n- Nashville Group (including Hermitage, Bigby-Cannon, Catheys Formations)\n- Primarily limestone deposits\n- Rich in fossils\n- Shallow marine environment\n- Multiple cycles of sea level change\n\n6. Early-Middle Silurian (443-423 million years ago)\n- Brassfield through Decatur Formations\n- Continued marine limestone deposition\n- Various marine environments from shallow to deeper water\n\n7. Middle Devonian-Early Mississippian (390-357 million years ago)\n- Pegram Formation through Chattanooga Shale\n- Shift to more clastic (shale) deposition\n- Chattanooga Shale represents deeper, oxygen-poor conditions\n\n8. Early-Late Mississippian (357-331 million years ago)\nSeveral limestone formations:\n- Fort Payne Formation\n- Warsaw Limestone\n- St. Louis Limestone\n- Monteagle Limestone\n- Hartselle Formation\n- Return to carbonate platform conditions\n- Diverse marine environments\n\n9. Cenozoic to Present\n- Long period of erosion and exposure\n- Development of the current landscape\n- Formation of the Nashville Dome structure\n- River and stream deposits\n\n### Key Features\n- The area was predominantly a marine environment for most of its geological history\n- Dominated by limestone formations indicating warm, shallow seas\n- Multiple cycles of sea level change\n- Rich fossil record, especially in Ordovician rocks\n- Part of the Nashville Dome structural feature\n- Long periods of erosion have removed younger rocks\n- Current topography shaped by differential erosion of various limestone units\n\n### Present Day Bedrock\nThe bedrock of Nashville today consists primarily of Ordovician limestone formations exposed at the surface, particularly the Stones River and Nashville Group limestones. These rocks are well exposed in many areas and have influenced the development of local karst topography, including caves and sinkholes that are common in the region.\n\n## Installation\n\n### Installing via Smithery\n\nTo install Macrostrat API Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@blake365/macrostrat-mcp):\n\n```bash\nnpx -y @smithery/cli install @blake365/macrostrat-mcp --client claude\n```\n\n### Extra Steps\n\nIf you want to make changes to the server you can do so by editing the `src/index.ts` file.\n- Run `npm install` to install dependencies\n- Run `npm run build` to build the server\n- Quit and restart Claude Desktop after making changes\n\n\n## Connecting with Claude Desktop\n\n1. Open your Claude Desktop configuration at:\n   - macOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\n   - Windows: `%APPDATA%\\Claude\\claude_desktop_config.json`\n\n2. Add the server configuration:\n```json\n{\n    \"mcpServers\": {\n        \"macrostrat\": {\n            \"command\": \"node\",\n            \"args\": [\n                \"/Full/Route/to/Folder/macrostrat/build/index.js\"\n            ]\n        }\n    }\n}\n```\n\n3. Close/Quit then restart Claude Desktop\n\nOnce you restart you should see an icon for search and tools. Click this button to see the MCP tools available.\n\n## Troubleshooting\n\nIf you get errors when running the server you may need to provide the full path to the `node` command. For example, on macOS: `/usr/local/bin/node`",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "geological",
        "macrostrat",
        "geologic",
        "geological data",
        "geologic data",
        "access geologic"
      ],
      "category": "web-search"
    },
    "blake365--usgs-quakes-mcp": {
      "owner": "blake365",
      "name": "usgs-quakes-mcp",
      "url": "https://github.com/blake365/usgs-quakes-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/blake365.webp",
      "description": "Access real-time earthquake data from the USGS API, enabling users to query earthquake information based on various parameters and retrieve details about specific events.",
      "stars": 1,
      "forks": 5,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-08-10T23:31:07Z",
      "readme_content": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/blake365-usgs-quakes-mcp-badge.png)](https://mseep.ai/app/blake365-usgs-quakes-mcp)\n\n# usgs-quakes-mcp\n[![smithery badge](https://smithery.ai/badge/@blake365/usgs-quakes-mcp)](https://smithery.ai/server/@blake365/usgs-quakes-mcp)\nClaude MCP server for USGS Quakes API\n\nAn MCP server implementation for providing access to the [USGS Quakes API](https://earthquake.usgs.gov/fdsnws/event/1/) within [Claude Desktop](https://claude.ai/download).\n\n## Overview\n\nThis Model Context Protocol (MCP) server enables AI assistants and applications to:\n\n- Get earthquake data through a wide variety of search parameters\n- Get details about a specific earthquake\n\nUsers can run queries using natural language.\n\n## Sample Output\n\n**Q: find earthquakes over 2 magnitude in florida over the last 20 years**\n\n**A:** I'll help you search for earthquakes in Florida with a magnitude greater than 2.0 over the past 20 years.\n\nView Result from find-earthquakes from usgs-quakes (local)\n\n```json\n{\n  \"endTime\": \"2024-12-28T23:59:59\",\n  \"startTime\": \"2004-12-28T00:00:00\",\n  \"maxLatitude\": 31.000888,\n  \"minLatitude\": 24.396308,\n  \"maxLongitude\": -80.031362,\n  \"minLongitude\": -87.634896,\n  \"minmagnitude\": 2\n}\n```\n\nBased on the USGS earthquake data, there have been three notable earthquakes in or near Florida over the past 20 years:\n\n1. A magnitude 5.9 earthquake in the Gulf of Mexico on September 10, 2006\nLocated offshore at coordinates 26.319°N, 86.606°W\nDepth: 14 km\nThis was the largest earthquake in the region during this period\nIt was widely felt, with 5,763 people reporting feeling the quake\n\n2. A magnitude 2.8 earthquake near Jay, Florida on March 6, 2019\nLocated at coordinates 30.9718°N, 87.1719°W\nDepth: 5 km\n49 people reported feeling this earthquake\n\n\n3. A magnitude 2.7 earthquake near Century, Florida on March 24, 2019\nLocated at coordinates 30.9622°N, 87.2384°W\nDepth: 5 km\n4 people reported feeling this earthquake\n\nIt's worth noting that Florida experiences relatively few earthquakes compared to more seismically active regions of the United States. The two 2019 earthquakes occurred in the Florida panhandle, while the larger 2006 event was offshore in the Gulf of Mexico.\n\n**Q: get the details of the Jay Florida earthquake**\n\n**A:** I'll get the detailed information about the magnitude 2.8 earthquake that occurred near Jay, Florida.\n\nView Result from find-earthquake-details from usgs-quakes (local)\n\n```json\n{\n  \"eventid\": \"us1000jbw4\"\n}\n```\n\nHere are the key details about the Jay, Florida earthquake:\nDate and Time: March 7, 2019, at 04:45:52 UTC\nLocation: 2 km (1.2 miles) Northwest of Jay, Florida\nCoordinates: 30.9718°N, 87.1719°W\nDepth: 5 kilometers (3.1 miles)\nMagnitude: 2.8 mb_lg (body wave magnitude)\n\nAdditional details:\n- 49 people reported feeling the earthquake\n- The Community Internet Intensity Map (CIIM) showed a maximum intensity of 2.7\n- There was no tsunami generated\n- The earthquake was reviewed and confirmed by USGS scientists\n- The location had a horizontal error margin of about 6.2 km\n- The magnitude calculation used data from 11 seismic stations\n- The earthquake was also detected by the New Mexico seismic network (NM), which calculated a slightly different magnitude of 2.63\n\nTechnical measurements:\n\n- Azimuthal gap: 243 degrees (indicates moderate coverage by seismic stations)\n- RMS error: 0.59 (indicates reasonable quality of the location estimate)\n- Minimum distance to nearest station: 0.222 degrees\n- Standard error for magnitude: 0.154\n\nThis was a relatively minor earthquake, but notable for Florida which rarely experiences seismic activity. The fact that it was felt by 49 people suggests it was noticeable to residents in the immediate area, though no significant damage would be expected from an earthquake of this magnitude.\n\n## Installation\n\n### Installing via Smithery\n\nTo install USGS Earthquake Data for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@blake365/usgs-quakes-mcp):\n\n```bash\nnpx -y @smithery/cli install @blake365/usgs-quakes-mcp --client claude\n```\n\n### Manual Installation\nInstalling this might be tricky if you're not familiar with Node.js and the MCP protocol.\n\n1. Install [Node.js](https://nodejs.org/en/download)\n2. Clone this repository to a folder on your local machine\n3. The server is already built so you can skip the build step.\n\n### Extra Steps\n\nIf you want to make changes to the server you can do so by editing the `src/index.ts` file.\n- Run `npm install` to install dependencies\n- Run `npm run build` to build the server\n- Quit and restart Claude Desktop after making changes\n\n\n## Connecting with Claude Desktop\n\n1. Open your Claude Desktop configuration at:\n   - macOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\n   - Windows: `%APPDATA%\\Claude\\claude_desktop_config.json`\n\n2. Add the server configuration:\n```json \n{\n    \"mcpServers\": {\n        \"usgs-quakes\": {\n            \"command\": \"node\",\n            \"args\": [\n                \"/Full/Route/to/Folder/usgs-quakes/build/index.js\"\n            ]\n        }\n    }\n}\n```\n\n3. Close/Quit then restart Claude Desktop\n\nOnce you restart you should see a small hammer icon in the lower right corner of the textbox. If you hover over the icon you'll see the number of MCP tools available.\n\n## Troubleshooting\n\nIf you get errors when running the server you may need to provide the full path to the `node` command. For example, on macOS: `/usr/local/bin/node`\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "earthquake",
        "quakes",
        "usgs",
        "earthquake data",
        "earthquake information",
        "query earthquake"
      ],
      "category": "web-search"
    },
    "blazickjp--arxiv-mcp-server": {
      "owner": "blazickjp",
      "name": "arxiv-mcp-server",
      "url": "https://github.com/blazickjp/arxiv-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/blazickjp.webp",
      "description": "Provides access to arXiv research papers via a straightforward MCP interface, enabling AI models to search and retrieve academic articles.",
      "stars": 1742,
      "forks": 120,
      "license": "Apache License 2.0",
      "language": "Python",
      "updated_at": "2025-10-02T21:34:55Z",
      "readme_content": "[![Twitter Follow](https://img.shields.io/twitter/follow/JoeBlazick?style=social)](https://twitter.com/JoeBlazick)\n[![smithery badge](https://smithery.ai/badge/arxiv-mcp-server)](https://smithery.ai/server/arxiv-mcp-server)\n[![Python Version](https://img.shields.io/badge/python-3.11+-blue.svg)](https://www.python.org/downloads/)\n[![Tests](https://github.com/blazickjp/arxiv-mcp-server/actions/workflows/tests.yml/badge.svg)](https://github.com/blazickjp/arxiv-mcp-server/actions/workflows/tests.yml)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![PyPI Downloads](https://img.shields.io/pypi/dm/arxiv-mcp-server.svg)](https://pypi.org/project/arxiv-mcp-server/)\n[![PyPI Version](https://img.shields.io/pypi/v/arxiv-mcp-server.svg)](https://pypi.org/project/arxiv-mcp-server/)\n\n# ArXiv MCP Server\n\n> 🔍 Enable AI assistants to search and access arXiv papers through a simple MCP interface.\n\nThe ArXiv MCP Server provides a bridge between AI assistants and arXiv's research repository through the Model Context Protocol (MCP). It allows AI models to search for papers and access their content in a programmatic way.\n\n<div align=\"center\">\n  \n🤝 **[Contribute](https://github.com/blazickjp/arxiv-mcp-server/blob/main/CONTRIBUTING.md)** • \n📝 **[Report Bug](https://github.com/blazickjp/arxiv-mcp-server/issues)**\n\n<a href=\"https://www.pulsemcp.com/servers/blazickjp-arxiv-mcp-server\"><img src=\"https://www.pulsemcp.com/badge/top-pick/blazickjp-arxiv-mcp-server\" width=\"400\" alt=\"Pulse MCP Badge\"></a>\n</div>\n\n## ✨ Core Features\n\n- 🔎 **Paper Search**: Query arXiv papers with filters for date ranges and categories\n- 📄 **Paper Access**: Download and read paper content\n- 📋 **Paper Listing**: View all downloaded papers\n- 🗃️ **Local Storage**: Papers are saved locally for faster access\n- 📝 **Prompts**: A Set of Research Prompts\n\n## 🚀 Quick Start\n\n### Installing via Smithery\n\nTo install ArXiv Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/arxiv-mcp-server):\n\n```bash\nnpx -y @smithery/cli install arxiv-mcp-server --client claude\n```\n\n### Installing Manually\nInstall using uv:\n\n```bash\nuv tool install arxiv-mcp-server\n```\n\nFor development:\n\n```bash\n# Clone and set up development environment\ngit clone https://github.com/blazickjp/arxiv-mcp-server.git\ncd arxiv-mcp-server\n\n# Create and activate virtual environment\nuv venv\nsource .venv/bin/activate\n\n# Install with test dependencies\nuv pip install -e \".[test]\"\n```\n\n### 🔌 MCP Integration\n\nAdd this configuration to your MCP client config file:\n\n```json\n{\n    \"mcpServers\": {\n        \"arxiv-mcp-server\": {\n            \"command\": \"uv\",\n            \"args\": [\n                \"tool\",\n                \"run\",\n                \"arxiv-mcp-server\",\n                \"--storage-path\", \"/path/to/paper/storage\"\n            ]\n        }\n    }\n}\n```\n\nFor Development:\n\n```json\n{\n    \"mcpServers\": {\n        \"arxiv-mcp-server\": {\n            \"command\": \"uv\",\n            \"args\": [\n                \"--directory\",\n                \"path/to/cloned/arxiv-mcp-server\",\n                \"run\",\n                \"arxiv-mcp-server\",\n                \"--storage-path\", \"/path/to/paper/storage\"\n            ]\n        }\n    }\n}\n```\n\n## 💡 Available Tools\n\nThe server provides four main tools:\n\n### 1. Paper Search\nSearch for papers with optional filters:\n\n```python\nresult = await call_tool(\"search_papers\", {\n    \"query\": \"transformer architecture\",\n    \"max_results\": 10,\n    \"date_from\": \"2023-01-01\",\n    \"categories\": [\"cs.AI\", \"cs.LG\"]\n})\n```\n\n### 2. Paper Download\nDownload a paper by its arXiv ID:\n\n```python\nresult = await call_tool(\"download_paper\", {\n    \"paper_id\": \"2401.12345\"\n})\n```\n\n### 3. List Papers\nView all downloaded papers:\n\n```python\nresult = await call_tool(\"list_papers\", {})\n```\n\n### 4. Read Paper\nAccess the content of a downloaded paper:\n\n```python\nresult = await call_tool(\"read_paper\", {\n    \"paper_id\": \"2401.12345\"\n})\n```\n\n## 📝 Research Prompts\n\nThe server offers specialized prompts to help analyze academic papers:\n\n### Paper Analysis Prompt\nA comprehensive workflow for analyzing academic papers that only requires a paper ID:\n\n```python\nresult = await call_prompt(\"deep-paper-analysis\", {\n    \"paper_id\": \"2401.12345\"\n})\n```\n\nThis prompt includes:\n- Detailed instructions for using available tools (list_papers, download_paper, read_paper, search_papers)\n- A systematic workflow for paper analysis\n- Comprehensive analysis structure covering:\n  - Executive summary\n  - Research context\n  - Methodology analysis\n  - Results evaluation\n  - Practical and theoretical implications\n  - Future research directions\n  - Broader impacts\n\n## ⚙️ Configuration\n\nConfigure through environment variables:\n\n| Variable | Purpose | Default |\n|----------|---------|---------|\n| `ARXIV_STORAGE_PATH` | Paper storage location | ~/.arxiv-mcp-server/papers |\n\n## 🧪 Testing\n\nRun the test suite:\n\n```bash\npython -m pytest\n```\n\n## 📄 License\n\nReleased under the MIT License. See the LICENSE file for details.\n\n---\n\n<div align=\"center\">\n\nMade with ❤️ by the Pearl Labs Team\n\n<a href=\"https://glama.ai/mcp/servers/04dtxi5i5n\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/04dtxi5i5n/badge\" alt=\"ArXiv Server MCP server\" /></a>\n</div>\n",
      "npm_url": "https://www.npmjs.com/package/arxiv-mcp-server",
      "npm_downloads": 800,
      "keywords": [
        "search",
        "articles",
        "research",
        "academic articles",
        "arxiv mcp",
        "research papers"
      ],
      "category": "web-search"
    },
    "blazickjp--web-browser-mcp-server": {
      "owner": "blazickjp",
      "name": "web-browser-mcp-server",
      "url": "https://github.com/blazickjp/web-browser-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/blazickjp.webp",
      "description": "Provides advanced web browsing capabilities for AI applications, enabling interaction with web content and retrieval of information from online sources.",
      "stars": 42,
      "forks": 11,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-18T11:51:57Z",
      "readme_content": "[![Twitter Follow](https://img.shields.io/twitter/follow/JoeBlazick?style=social)](https://twitter.com/JoeBlazick)\n[![smithery badge](https://smithery.ai/badge/web-browser-mcp-server)](https://smithery.ai/server/web-browser-mcp-server)\n[![Python Version](https://img.shields.io/badge/python-3.11+-blue.svg)](https://www.python.org/downloads/)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![PyPI Downloads](https://img.shields.io/pypi/dm/web-browser-mcp-server.svg)](https://pypi.org/project/web-browser-mcp-server/)\n[![PyPI Version](https://img.shields.io/pypi/v/web-browser-mcp-server.svg)](https://pypi.org/project/web-browser-mcp-server/)\n\n<a href=\"https://glama.ai/mcp/servers/3hphahzvql\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/3hphahzvql/badge\" alt=\"web-browser-mcp-server MCP server\" /></a>\n\n## ✨ Features\n\n> 🌐 Enable AI assistants to browse and extract content from the web through a simple MCP interface.\n\nThe Web Browser MCP Server provides AI models with the ability to browse websites, extract content, and understand web pages through the Message Control Protocol (MCP). It enables smart content extraction with CSS selectors and robust error handling.\n\n<div align=\"center\">\n  \n🤝 **[Contribute](https://github.com/blazickjp/web-browser-mcp-server/blob/main/CONTRIBUTING.md)** • \n📝 **[Report Bug](https://github.com/blazickjp/web-browser-mcp-server/issues)**\n\n</div>\n\n## ✨ Core Features\n\n- 🎯 **Smart Content Extraction**: Target exactly what you need with CSS selectors\n- ⚡ **Lightning Fast**: Built with async processing for optimal performance\n- 📊 **Rich Metadata**: Capture titles, links, and structured content\n- 🛡️ **Robust & Reliable**: Built-in error handling and timeout management\n- 🌍 **Cross-Platform**: Works everywhere Python runs\n\n## 🚀 Quick Start\n\n### Installing via Smithery\n\nTo install Web Browser Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/web-browser-mcp-server):\n\n```bash\nnpx -y @smithery/cli install web-browser-mcp-server --client claude\n```\n\n### Installing Manually\nInstall using uv:\n\n```bash\nuv tool install web-browser-mcp-server\n```\n\nFor development:\n\n```bash\n# Clone and set up development environment\ngit clone https://github.com/blazickjp/web-browser-mcp-server.git\ncd web-browser-mcp-server\n\n# Create and activate virtual environment\nuv venv\nsource .venv/bin/activate\n\n# Install with test dependencies\nuv pip install -e \".[test]\"\n```\n\n### 🔌 MCP Integration\n\nAdd this configuration to your MCP client config file:\n\n```json\n{\n    \"mcpServers\": {\n        \"web-browser-mcp-server\": {\n            \"command\": \"uv\",\n            \"args\": [\n                \"tool\",\n                \"run\",\n                \"web-browser-mcp-server\"\n            ],\n            \"env\": {\n                \"REQUEST_TIMEOUT\": \"30\"\n            }\n        }\n    }\n}\n```\n\nFor Development:\n\n```json\n{\n    \"mcpServers\": {\n        \"web-browser-mcp-server\": {\n            \"command\": \"uv\",\n            \"args\": [\n                \"--directory\",\n                \"path/to/cloned/web-browser-mcp-server\",\n                \"run\",\n                \"web-browser-mcp-server\"\n            ],\n            \"env\": {\n                \"REQUEST_TIMEOUT\": \"30\"\n            }\n        }\n    }\n}\n```\n\n## 💡 Available Tools\n\nThe server provides a powerful web browsing tool:\n\n### browse_webpage\nBrowse and extract content from web pages with optional CSS selectors:\n\n```python\n# Basic webpage fetch\nresult = await call_tool(\"browse_webpage\", {\n    \"url\": \"https://example.com\"\n})\n\n# Target specific content with CSS selectors\nresult = await call_tool(\"browse_webpage\", {\n    \"url\": \"https://example.com\",\n    \"selectors\": {\n        \"headlines\": \"h1, h2\",\n        \"main_content\": \"article.content\",\n        \"navigation\": \"nav a\"\n    }\n})\n```\n\n## ⚙️ Configuration\n\nConfigure through environment variables:\n\n| Variable | Purpose | Default |\n|----------|---------|---------|\n| `REQUEST_TIMEOUT` | Webpage request timeout in seconds | 30 |\n\n## 🧪 Testing\n\nRun the test suite:\n\n```bash\npython -m pytest\n```\n\n## 📄 License\n\nReleased under the MIT License. See the LICENSE file for details.\n\n---\n\n<div align=\"center\">\n\nMade with ❤️ by the Pear Labs Team\n\n<a href=\"https://glama.ai/mcp/servers/04dtxi5i5n\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/04dtxi5i5n/badge\" alt=\"Web Browser MCP Server\" /></a>\n</div>",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "web",
        "browsing",
        "search",
        "web search",
        "blazickjp web",
        "web browsing"
      ],
      "category": "web-search"
    },
    "bmorphism--marginalia-mcp-server": {
      "owner": "bmorphism",
      "name": "marginalia-mcp-server",
      "url": "https://github.com/bmorphism/marginalia-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/bmorphism.webp",
      "description": "Access the Marginalia Search API to perform searches for non-commercial content on the internet, allowing for configurable search parameters and result counts. It includes rate limiting protection and uses the MCP SDK for integration.",
      "stars": 4,
      "forks": 7,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-09-25T16:17:54Z",
      "readme_content": "# Marginalia MCP Server\n\nAn MCP (Model Context Protocol) server that provides access to [Marginalia Search](https://search.marginalia.nu/), a search engine focused on finding non-commercial content and hidden gems of the internet.\n\n## Features\n\n- Search the web using Marginalia Search API\n- Configurable search parameters including index and result count\n- Rate limiting protection with helpful error messages\n- Built using the MCP SDK for seamless integration\n\n## Installation\n\n```bash\nnpm install\nnpm run build\n```\n\n## Configuration\n\nThe server can be configured using environment variables:\n\n- `MARGINALIA_API_KEY`: Your Marginalia Search API key (optional, defaults to public access)\n\nTo request a dedicated API key, contact: kontakt@marginalia.nu\n\n## Usage\n\nThe server provides the following MCP tool:\n\n### search\n\nSearch the web using Marginalia Search with the following parameters:\n\n- `query` (required): Search query string\n- `index` (optional): Search index number (corresponds to dropdown in main GUI)\n- `count` (optional): Number of results to return (1-100, default: 10)\n\nExample usage through MCP:\n\n```typescript\nconst result = await mcp.useTool(\"marginalia\", \"search\", {\n  query: \"interesting non-commercial websites\",\n  count: 5\n});\n```\n\n## Response Format\n\nThe search results are returned in the following format:\n\n```json\n{\n  \"query\": \"your search query\",\n  \"license\": \"license information\",\n  \"results\": [\n    {\n      \"url\": \"result url\",\n      \"title\": \"page title\",\n      \"description\": \"page description\"\n    }\n    // ... more results\n  ]\n}\n```\n\n## Error Handling\n\nThe server includes robust error handling for:\n- Rate limiting (503 responses)\n- Invalid requests\n- Network errors\n- API-specific errors\n\n## Adding Text-to-Speech Support\n\nTo enable text-to-speech capabilities using the say MCP server, add the following configuration:\n\n### For Cline (VSCode Extension)\n\nAdd to `~/Library/Application Support/Code/User/globalStorage/saoudrizwan.claude-dev/settings/cline_mcp_settings.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"say\": {\n      \"command\": \"node\",\n      \"args\": [\"/Users/barton/worlds/servers/src/say-mcp-server/build/index.js\"]\n    }\n  }\n}\n```\n\n### For Claude Desktop\n\nAdd to `~/Library/Application Support/Claude/claude_desktop_config.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"say\": {\n      \"command\": \"node\",\n      \"args\": [\"/Users/barton/worlds/servers/src/say-mcp-server/build/index.js\"]\n    }\n  }\n}\n```\n\nThe say MCP server provides text-to-speech capabilities with multiple voices and languages. Available tools:\n\n- `speak`: Read text aloud using specified voice and rate\n- `list_voices`: List all available text-to-speech voices\n\n## License\n\nMIT License - See LICENSE file for details\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "searches",
        "search",
        "marginalia",
        "marginalia search",
        "marginalia mcp",
        "web search"
      ],
      "category": "web-search"
    },
    "bneil--mcp-go-colly": {
      "owner": "bneil",
      "name": "mcp-go-colly",
      "url": "https://github.com/bneil/mcp-go-colly",
      "imageUrl": "/freedevtools/mcp/pfp/bneil.webp",
      "description": "Provides a framework for concurrent web crawling tailored for large language model applications, allowing for the extraction and structuring of web content with customizable settings for depth and domain restrictions. Integrates seamlessly with the Model Context Protocol to facilitate web data integration into LLM workflows with effective error handling and batch processing capabilities.",
      "stars": 0,
      "forks": 0,
      "license": "MIT License",
      "language": "Go",
      "updated_at": "2025-06-06T03:22:05Z",
      "readme_content": "# MCP Go Colly Crawler\n[![smithery badge](https://smithery.ai/badge/@bneil/mcp-go-colly)](https://smithery.ai/server/@bneil/mcp-go-colly)\n\n## Overview\nMCP Go Colly is a sophisticated web crawling framework that integrates the Model Context Protocol (MCP) with the powerful Colly web scraping library. This project aims to provide a flexible and extensible solution for extracting web content for large language model (LLM) applications.\n\n## Features\n- Concurrent web crawling with configurable depth and domain restrictions\n- MCP server integration for tool-based crawling\n- Graceful shutdown handling\n- Robust error handling and result formatting\n- Support for both single URL and batch URL crawling\n\n## Building from Source\n\n### Prerequisites\n- Go 1.21 or later\n- Make (for using Makefile commands)\n\n### Installation\n\n1. Clone the repository:\n```bash\ngit clone https://github.com/yourusername/mcp-go-colly.git\ncd mcp-go-colly\n```\n\n2. Install dependencies:\n```bash\nmake deps\n```\n\n### Building\n\nThe project includes a Makefile with several useful commands:\n\n```bash\n# Build the binary (outputs to bin/mcp-go-colly)\nmake build\n\n# Build for all platforms (Linux, Windows, macOS)\nmake build-all\n\n# Run tests\nmake test\n\n# Clean build artifacts\nmake clean\n\n# Format code\nmake fmt\n\n# Run linter\nmake lint\n```\n\nAll binaries will be generated in the `bin/` directory.\n\nThen you need to add the following configuration to the `claude_desktop_config.json` file:\n\n```json\n{\n  \"mcpServers\": {\n    \"web-scraper\": {\n      \"command\": \"<add path here>/mcp-go-colly/bin/mcp-go-colly\"\n    }\n  }\n}\n```\n\n## Usage\n\n### As an MCP Tool\n\nThe crawler is implemented as an MCP tool that can be called with the following parameters:\n\n```json\n{\n    \"urls\": [\"https://example.com\"],  // Single URL or array of URLs\n    \"max_depth\": 2                    // Optional: Maximum crawl depth (default: 2)\n}\n```\n\n### Example MCP Tool Call\n\n```go\nresult, err := crawlerTool.Call(ctx, mcp.CallToolRequest{\n    Params: struct{ Arguments map[string]interface{} }{\n        Arguments: map[string]interface{}{\n            \"urls\": []string{\"https://example.com\"},\n            \"max_depth\": 2,\n        },\n    },\n})\n```\n\n## Configuration Options\n- `max_depth`: Set maximum crawl depth (default: 2)\n- `urls`: Single URL string or array of URLs to crawl\n- Domain restrictions are automatically applied based on the provided URLs\n\n## Contributing\n1. Fork the repository\n2. Create your feature branch\n3. Commit your changes\n4. Push to the branch\n5. Create a Pull Request\n\n## License\nMIT\n\n## Acknowledgments\n- Colly Web Scraping Framework\n- Mark3 Labs MCP Project\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "crawling",
        "search",
        "web",
        "web crawling",
        "web search",
        "concurrent web"
      ],
      "category": "web-search"
    },
    "bradleygolden--hexdocs-mcp": {
      "owner": "bradleygolden",
      "name": "hexdocs-mcp",
      "url": "https://github.com/bradleygolden/hexdocs-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/bradleygolden.webp",
      "description": "HexDocs MCP provides semantic search capabilities for Hex package documentation, enabling integration with AI applications to access and find relevant documentation efficiently. It includes an Elixir binary for processing documentation and a TypeScript server that implements the Model Context Protocol to facilitate searches.",
      "stars": 58,
      "forks": 3,
      "license": "MIT License",
      "language": "Elixir",
      "updated_at": "2025-09-26T07:52:49Z",
      "readme_content": "# HexDocs MCP\n\nHexDocs MCP is a project that provides semantic search capabilities for Hex package documentation, designed specifically for AI applications. It consists of two main components:\n\n1. An Elixir binary that downloads, processes, and generates embeddings from Hex package documentation\n2. A TypeScript server implementing the Model Context Protocol (MCP) that calls the Elixir binary to fetch and search documentation\n\n> [!CAUTION]\n> **This documentation reflects the current development state on the main branch.**\n> For documentation on the latest stable release, please see the [latest release page](https://github.com/bradleygolden/hexdocs-mcp/releases/latest) and the [latest release branch](https://github.com/bradleygolden/hexdocs-mcp/tree/v0.5.0).\n\n## Installation\n\n### MCP Client Configuration\n\nThe TypeScript MCP server implements the [Model Context Protocol (MCP)](https://modelcontextprotocol.io) and is designed to be used by MCP-compatible clients such as Cursor, Claude Desktop App, Continue, and others. The server provides tools for semantic search of Hex documentation. For a complete list of MCP-compatible clients, see the [MCP Clients documentation](https://modelcontextprotocol.io/clients).\n\nAdd this to your client's MCP json config:\n\n```json\n{\n  \"mcpServers\": {\n    \"hexdocs-mcp\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"hexdocs-mcp@0.5.0\"\n      ]\n    }\n  }\n}\n```\n\nThis command will automatically download the elixir binaries to both fetch_docs and search documentation. While the server handles downloading the binaries, you still need Elixir and Mix installed on your system for the HexDocs fetching functionality to work properly.\n\n#### Smithery\n\nAlternatively, you can use [Smithery](https://smithery.ai/server/@bradleygolden/hexdocs-mcp) to automatically add the MCP server to your client config.\n\nFor example, for Cursor, you can use the following command:\n\n```bash\nnpx -y @smithery/cli@latest install @bradleygolden/hexdocs-mcp --client cursor\n```\n\n### Elixir Package\n\nAlternatively, you can add the hexdocs_mcp package to your project if you don't want to use the MCP server.\n\n```elixir\n{:hexdocs_mcp, \"~> 0.5.0\", only: :dev, runtime: false}\n```\n\nAnd if you use floki or any other dependencies that are marked as only available in\nanother environment, update them to be available in the `:dev` environment as well.\n\nFor example floki is commonly used in `:test`:\n\n```elixir\n{:floki, \">= 0.30.0\", only: :test}\n```\n\nBut you can update it to be available in the :dev environment:\n\n```elixir\n{:floki, \">= 0.30.0\", only: [:dev, :test]}\n```\n\n### Requirements\n\n- [Ollama](https://ollama.ai) - Required for generating embeddings\n  - Run `ollama pull mxbai-embed-large` to download the recommended embedding model\n  - Ensure Ollama is running before using the embedding features\n- Elixir 1.16+ and Erlang/OTP 26+ \n  - Installed automatically in CI environments\n  - Required locally for development\n- Mix - The Elixir build tool (comes with Elixir installation)\n- Node.js 22 or later (for the MCP server)\n\n### Breaking Change: Model Migration (v0.6.0+)\n\n**⚠️ IMPORTANT**: Version 0.6.0 introduces a breaking change with the default embedding model.\n\n**What changed**: \n- Default model changed from `nomic-embed-text` (384 dimensions) to `mxbai-embed-large` (1024 dimensions)\n- Existing embeddings are incompatible and will be cleared during upgrade\n\n**To upgrade**:\n1. Pull the new model:\n   ```bash\n   ollama pull mxbai-embed-large\n   ```\n\n2. Your existing embeddings will be automatically cleared when you first run any command\n\n3. Regenerate embeddings for your packages:\n   ```bash\n   mix hex.docs.mcp fetch_docs phoenix\n   ```\n\n**Why this change**: `mxbai-embed-large` provides significantly better semantic search quality and consistent dimensions across all platforms (Windows/macOS/Linux).\n\n## Configuration\n\n### Environment Variables\n\nThe following environment variables can be used to configure the tool:\n\n| Variable | Description | Default |\n|----------|-------------|---------|\n| `HEXDOCS_MCP_PATH` | Path where data will be stored | `~/.hexdocs_mcp` |\n| `HEXDOCS_MCP_MIX_PROJECT_PATHS` | Comma-separated list of paths to mix.exs files | (none) |\n\n#### Examples:\n\n```bash\n# Set custom storage location\nexport HEXDOCS_MCP_PATH=/path/to/custom/directory\n\n# Configure common project paths to avoid specifying --project flag each time\nexport HEXDOCS_MCP_MIX_PROJECT_PATHS=\"/path/to/project1/mix.exs,/path/to/project2/mix.exs\"\n```\n\n### MCP Server Configuration\n\nYou can also configure environment variables in the MCP configuration for the server:\n\n```json\n{\n  \"mcpServers\": {\n    \"hexdocs-mcp\": {\n      \"command\": \"...\",\n      \"args\": [\n        \"...\"\n      ],\n      \"env\": {\n        \"HEXDOCS_MCP_PATH\": \"/path/to/custom/directory\",\n        \"HEXDOCS_MCP_MIX_PROJECT_PATHS\": \"/path/to/project1/mix.exs,/path/to/project2/mix.exs\"\n      }\n    }\n  }\n}\n```\n\n## Usage\n\n### AI Tooling\n\nThe MCP server can be used by any MCP-compatible AI tooling. The server will automatically fetch documentation when needed and store it in the configured data directory.\n\nNote that large packages make take time to download and process.\n\n### Elixir Package\n\nThe SQLite database for vector storage and retrieval is created automatically when needed.\n\nFetch documentation, process, and generate embeddings for a package:\n\n```bash\nmix hex.docs.mcp fetch_docs phoenix\n```\n\nFetch documentation for a specific version:\n\n```bash\nmix hex.docs.mcp fetch_docs phoenix 1.5.9\n```\n\nFetch documentation for a package using the version from your project:\n\n```bash\nmix hex.docs.mcp fetch_docs phoenix --project path/to/mix.exs\n```\n\nConfigure project paths to avoid specifying them every time:\n\n```bash\nexport HEXDOCS_MCP_MIX_PROJECT_PATHS=\"/path/to/project1/mix.exs,/path/to/project2/mix.exs\"\nmix hex.docs.mcp fetch_docs phoenix  # Will use the first path from HEXDOCS_MCP_MIX_PROJECT_PATHS\n```\n\nSearch in the existing embeddings:\n\n```bash\nmix hex.docs.mcp semantic_search phoenix --query \"channels\"\n```\n\nCheck if embeddings exist for a package:\n\n```bash\nmix hex.docs.mcp check_embeddings phoenix\nmix hex.docs.mcp check_embeddings phoenix 1.7.0\n```\n\n## Acknowledgements\n\n- [hex2text](https://github.com/mjrusso/hex2txt) - For the initial idea and as a reference\n\n## Development\n\nThis project uses [mise](https://mise.jdx.dev/) (formerly rtx) to manage development tools and tasks. Mise provides consistent tool versions and task automation across the project.\n\n### Setting Up Development Environment\n\n1. Install mise (if you don't have it already):\n   ```bash\n   # macOS with Homebrew\n   brew install mise\n   \n   # Using the installer script\n   curl https://mise.run | sh\n   ```\n\n2. Clone the repository and setup the development environment:\n   ```bash\n   git clone https://github.com/bradleygolden/hexdocs-mcp.git\n   cd hexdocs-mcp\n   mise install # Installs the right versions of Elixir and Node.js\n   ```\n\n3. Setup dependencies:\n   ```bash\n   mise build\n   ```\n\n### Development Tasks\n\nMise defines several useful development tasks:\n\n- `mise build` - Build both Elixir and TypeScript components\n- `mise test` - Run all tests\n- `mise mcp_inspect` - Start the MCP inspector for testing the server\n- `mise start_mcp_server` - Start the MCP server (primarily for debugging)\n\n### Without Mise\n\nIf you prefer not to use mise, you'll need:\n\n- Elixir 1.18.x\n- Node.js 22.x\n\nThen, you can run these commands directly:\n\n```bash\n# Instead of mise run setup_elixir\nmix setup\n\n# Instead of mise run setup_ts\nnpm install\n\n# Instead of mise run build\nmix compile --no-optional-deps --warnings-as-errors\nnpm run build\n\n# Instead of mise run test\nmix test\nmix format --check-formatted\nmix deps --check-unused\nmix deps.unlock --all\nmix deps.get\nmix test\n\n# Instead of mise run mcp_inspect\nMCP_INSPECTOR=true npx @modelcontextprotocol/inspector node dist/index.js\n```\n\n## AI Assistant Integration\n\nThis project includes custom instructions for AI assistants to help optimize your workflow when working with Hex documentation.\n\n### Example Custom Instructions\n\nYou can find sample custom instructions in the repository:\n- [Cursor rules](.cursor/rules/hexdocs-mcp.mdc) - Custom rules for Cursor editor\n- [GitHub Copilot](.github/copilot/instructions.md) - Custom instructions for GitHub Copilot\n\n### Suggested Content\n\n```\nWhen working with Elixir projects that use Hex packages:\n\n## HexDocs MCP Workflow\n\n1. Use `search` to find relevant documentation\n2. Use `fetch` to fetch documentation for a package\n```\n\n## Release Guidelines\n\nWhen preparing a new release, please follow these guidelines to ensure consistency:\n\n### Version Management\n\n1. **SemVer Compliance**: Follow [Semantic Versioning](https://semver.org/) strictly:\n   - MAJOR: incompatible API changes\n   - MINOR: backward-compatible functionality\n   - PATCH: backward-compatible bug fixes\n\n2. **Version Synchronization**:\n   - Hex package version (in `mix.exs`) and npm package version (in `package.json`) MUST be identical\n   - Update both files when changing the version\n\n### Code Style\n\n1. **Formatting and Comments**:\n   - Follow the Elixir formatter rules defined in .formatter.exs\n   - Do not add comments to code unless strictly necessary for context\n   - Self-documenting code with clear function names is preferred\n   - Use module and function documentation (@moduledoc and @doc) instead of inline comments\n\n### Changelog Management\n\n1. **Update CHANGELOG.md**:\n   - Document all changes under the appropriate heading (Added, Changed, Fixed, etc.)\n   - Include the new version number and date\n   - Keep an [Unreleased] section for tracking current changes\n   - Follow the [Keep a Changelog](https://keepachangelog.com/) format\n\n2. **Entry Format**:\n   - Use present tense, imperative style (e.g., \"Add feature\" not \"Added feature\")\n   - Include issue/PR numbers where applicable\n   - Group related changes\n\n### Release Process\n\n1. **Before Release**:\n   - Run `mix test` to ensure all tests pass\n   - Run `mix format` to ensure code is properly formatted\n   - Verify CHANGELOG.md is updated\n\n2. **Release Commits**:\n   - Create a version bump commit that updates:\n     - mix.exs\n     - package.json\n     - CHANGELOG.md (move [Unreleased] to new version)\n   - Tag the commit with the version number (v0.1.0 format)\n\n3. **After Release**:\n   - Add a new [Unreleased] section to CHANGELOG.md\n   - Update version links at the bottom of CHANGELOG.md\n\nThese guidelines apply to both human contributors and AI assistants working on this project.\n\n## Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request. For major changes, please open an issue first to discuss what you would like to change.\n\nThis project is licensed under MIT - see the [LICENSE](https://github.com/bradleygolden/hexdocs-mcp/blob/main/LICENSE) file for details.",
      "npm_url": "https://www.npmjs.com/package/hexdocs-mcp",
      "npm_downloads": 2723,
      "keywords": [
        "hexdocs",
        "searches",
        "documentation",
        "mcp hexdocs",
        "hexdocs mcp",
        "semantic search"
      ],
      "category": "web-search"
    },
    "briandconnelly--mcp-server-ipinfo": {
      "owner": "briandconnelly",
      "name": "mcp-server-ipinfo",
      "url": "https://github.com/briandconnelly/mcp-server-ipinfo",
      "imageUrl": "/freedevtools/mcp/pfp/briandconnelly.webp",
      "description": "Retrieve detailed information about an IP address to determine geographic location and network details using the IPInfo API.",
      "stars": 34,
      "forks": 12,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-20T20:12:21Z",
      "readme_content": "# IP Geolocation MCP Server\n\nThis is a simple [Model Context Protocol](https://modelcontextprotocol.io) server that uses the [ipinfo.io](https://ipinfo.io) API to get detailed information about an IP address.\nThis can be used to determine where the user is located (approximately) and what network they are used.\n\n<a href=\"https://glama.ai/mcp/servers/pll7u5ak1h\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/pll7u5ak1h/badge\" alt=\"IP Geolocation Server MCP server\" />\n</a>\n\n\n\n\n## Installation\n\nYou'll need to create a token to use the IPInfo API.\nIf you don't already have one, you can sign up for a free account at https://ipinfo.io/signup.\n\nWhile each client has its own way of specifying, you'll generally use the following values:\n\n| Field | Value |\n|-------|-------|\n| **Command** | `uvx` |\n| **Arguments** | `mcp-server-ipinfo` |\n| **Environment** | `IPINFO_API_TOKEN` = `<YOUR TOKEN>` |\n\n\n### Development Version\n\nIf you'd like to use the latest and greatest, the server can be pulled straight from GitHub.\nJust add an additional `--from` argument:\n\n\n| Field | Value |\n|-------|-------|\n| **Command** | `uvx` |\n| **Arguments** | `--from`, `git+https://github.com/briandconnelly/mcp-server-ipinfo`, `mcp-server-ipinfo` |\n| **Environment** | `IPINFO_API_TOKEN` = `<YOUR TOKEN>` |\n\n\n## Components\n\n### Tools\n\n- `get_ip_details`: This tool is used to get detailed information about an IP address.\n    - **Input:** `ip`: The IP address to get information about.\n    - **Output:** `IPDetails`: A Pydantic model containing detailed information about the IP, including location, organization, and country details.\n\n### Resources   \n\n_No custom resources are included_\n\n### Prompts\n\n_No custom prompts are included_\n\n\n## License\n\nMIT License - See [LICENSE](LICENSE) file for details.\n\n## Disclaimer\n\nThis project is not affiliated with [IPInfo](https://ipinfo.io).",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "ipinfo",
        "ip",
        "mcp",
        "ipinfo retrieve",
        "ipinfo api",
        "using ipinfo"
      ],
      "category": "web-search"
    },
    "brianellin--bsky-mcp-server": {
      "owner": "brianellin",
      "name": "bsky-mcp-server",
      "url": "https://github.com/brianellin/bsky-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/brianellin.webp",
      "description": "Connect to Bluesky and interact with the ATProtocol using natural language to fetch, analyze, and post content seamlessly. Provides access to various Bluesky API endpoints directly within LLM applications.",
      "stars": 32,
      "forks": 10,
      "license": "No License",
      "language": "TypeScript",
      "updated_at": "2025-09-29T05:08:25Z",
      "readme_content": "# Bluesky MCP Server\n\n[![smithery badge](https://smithery.ai/badge/@brianellin/bsky-mcp-server)](https://smithery.ai/server/@brianellin/bsky-mcp-server)\n\nA [Model Context Protocol](https://modelcontextprotocol.io/) server that connects to [Bluesky](https://bsky.app/) and provides tools to interact with the ATProtocol.\n\nYou can use this MCP server to bring context from various Bluesky / ATProtocol API endpoints directly into the context window of your LLM based application. For example, you can add this server to Claude Desktop and then use it as a natural language Bluesky client. \n\n## Features & Tools\n\n- Interact with common Bluesky features via natural language (e.g. \"Get recent posts from David Roberts\")\n- Fetch and analyze feeds (\"Find me a feed about Seattle and tell me what people are talking about\")\n- Fetch and analyze lists of followers (\"What types of accounts does Mark Cuban follow? Give me a detailed report\")\n- Use an LLM to write a post and then post it for you 😱 (\"Write a haiku about today's weather in my area and post it to bluesky\")\n- Search for feeds, posts, and people (\"Find posts about the #teslatakedown and give me a summary of recent events\")\n- Analyze who follows you? (\"Who follows me on Bluesky? Give me a report\")\n\nHere's the current list of tools provided:\n\n- **get-pinned-feeds**: returns the set of all \"pinned\" items from the authenticated user's preferences.\n- **get-timeline-posts**: returns posts from the authenticated user's home timeline\n- **get-feed-posts**: returns posts from the specified feed\n- **get-list-posts**: returns posts from the specified list\n- **get-user-posts**: returns the specified user's posts\n- **get-profile**: returns the profile details of the specified user\n- **get-follows**: returns the set of users an account follows\n- **get-followers**: returns the set of users who follow an account\n- **get-liked-posts**: returns recent posts liked by the authenticated user\n- **get-trends**: returns current trending topics on Bluesky with post counts\n- **get-post-thread**: returns a full conversation thread for a specific post, showing all replies and context\n- **convert-url-to-uri**: converts a Bluesky web URL to an AT URI format that can be used with other tools\n- **search-posts**: returns posts for a given query. can specify top or latest\n- **search-people**: returns people for a given search query\n- **search-feeds**: returns feeds for a given query\n- **like-post**: like a post with a specific URI\n- **create-post**: publish a post \n- **follow-user**: follow a specific user\n\nTips:\n- You can ask for post from search, timelines, lists, feeds, or profiles by time range. For example: Summarize posts from my timeline for the last three days\" or \"Find me the most interesting article people have been talking about this week\"\n- Get weird: \"What the funniest/most unhinged/weirdest/goofiest post you've seen on my timeline in the last 24 hours?\"\n- Learn about yourself: \"Analyze my liked posts and tell me what I'm into. Give me 3 interesting facts about what you've found and how it relates to my personality on bluesky\" or \"Who follows me on bluesky? Give me a comprehensive report.\"\n\n\n## Installation\n\n### Installing via Smithery\n\nTo install Bluesky MCP Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@brianellin/bsky-mcp-server):\n\n```bash\nnpx -y @smithery/cli install @brianellin/bsky-mcp-server --client claude\n```\n\n### Installing Manually\nFirst clone this repo, then install dependencies and build the server:\n\n```bash\n# Install dependencies\npnpm install\n\n# Build the project\npnpm run build\n```\n\n### Testing with MCP Inspector\n\nYou can test the bluesky tools directly without connecting to an LLM via the amazing [MCP Inspector](https://modelcontextprotocol.io/docs/tools/inspector). First make sure you have built the server and then run:\n\n```bash\nnpx @modelcontextprotocol/inspector node build/src/index.js\n```\n\nNavigate to the local URL provided in your terminal, and then set your BLUESKY_IDENTIFIER, BLUESKY_APP_PASSWORD, and BLUESKY_SERVICE_URL environment variables from the panel on the left. Try the get-timeline tool to see the most recent posts from your home timeline. \n\n## MCP Client Configuration \n\nFollow the steps to set up MCP with your client of choice. For example, to set up Claude for desktop to connect to Bluesky, add the following to bluesky section to your claude_desktop_config.json:\n\n```json\n{\n    \"mcpServers\": {\n      \"bluesky\": {\n        \"command\": \"node\",\n        \"args\": [\"/path/to/bsky-mcp-server/build/src/index.js\"],\n        \"env\": {\n            \"BLUESKY_IDENTIFIER\": \"your-bluesky-handle\",\n            \"BLUESKY_APP_PASSWORD\": \"your-app-password\",\n            \"BLUESKY_SERVICE_URL\": \"https://bsky.social\"\n          }\n      }\n    }\n  }\n```\n\nFor more details about running MCP servers in Claude for desktop, see https://modelcontextprotocol.io/quickstart/user\n\n\n## Creating App Passwords\n\nTo use this MCP server, you need to create an app password for your Bluesky account:\n\n1. Log in to Bluesky\n2. Go to Settings > App Passwords\n3. Create a new app password specifically for this integration\n4. Set the app password using the BLUESKY_APP_PASSWORD environment variable\n\n## Security Notes\n\n- This server stores your session information in memory only and does not share it with the MCP client.\n- The MCP client only has access to the tools, not to your authentication or app password\n\n## License\n\nMIT\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "bsky",
        "bluesky",
        "search",
        "bluesky api",
        "bsky mcp",
        "bluesky interact"
      ],
      "category": "web-search"
    },
    "brianshin22--youtube-translate-mcp": {
      "owner": "brianshin22",
      "name": "youtube-translate-mcp",
      "url": "https://github.com/brianshin22/youtube-translate-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/brianshin22.webp",
      "description": "Interfacing with the YouTube Translate API to obtain transcripts, translations, subtitles, and summaries of YouTube videos. Enables searching video transcripts for specific keywords and phrases, and directly processes video content to enhance understanding.",
      "stars": 3,
      "forks": 3,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-17T02:49:03Z",
      "readme_content": "# YouTube Translate MCP\n[![smithery badge](https://smithery.ai/badge/@brianshin22/youtube-translate-mcp)](https://smithery.ai/server/@brianshin22/youtube-translate-mcp)\n\nA [Model Context Protocol (MCP)](https://github.com/anthropics/anthropic-cookbook/tree/main/model_composition_protocol) server for accessing the YouTube Translate API, allowing you to obtain transcripts, translations, and summaries of YouTube videos.\n\n## Features\n\n- Get transcripts of YouTube videos\n- Translate transcripts to different languages\n- Generate subtitles in SRT or VTT format\n- Create summaries of video content\n- Search for specific content within videos\n\n## Installation\n\n### Installing via Smithery\n\nTo install youtube-translate-mcp for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@brianshin22/youtube-translate-mcp):\n\n```bash\nnpx -y @smithery/cli install @brianshin22/youtube-translate-mcp --client claude\n```\n\n### Installing Manually\n\nThis package requires Python 3.12 or higher:\n\n```bash\n# Using uv (recommended)\nuv pip install youtube-translate-mcp\n\n# Using pip\npip install youtube-translate-mcp\n```\n\nOr install from source:\n\n```bash\n# Clone the repository\ngit clone https://github.com/yourusername/youtube-translate-mcp.git\ncd youtube-translate-mcp\n\n# Using uv (recommended)\nuv pip install -e .\n\n# Using pip\npip install -e .\n```\n\n## Usage\n\nTo run the server:\n\n```bash\n# Using stdio transport (default)\nYOUTUBE_TRANSLATE_API_KEY=your_api_key youtube-translate-mcp\n\n# Using SSE transport\nYOUTUBE_TRANSLATE_API_KEY=your_api_key youtube-translate-mcp --transport sse --port 8000\n```\n\n## Docker\n\nYou can also run the server using Docker:\n\n```bash\n# Build the Docker image\ndocker build -t youtube-translate-mcp .\n\n# Run with stdio transport\ndocker run -e YOUTUBE_TRANSLATE_API_KEY=your_api_key youtube-translate-mcp\n\n# Run with SSE transport\ndocker run -p 8000:8000 -e YOUTUBE_TRANSLATE_API_KEY=your_api_key youtube-translate-mcp --transport sse\n```\n\n## Environment Variables\n\n- `YOUTUBE_TRANSLATE_API_KEY`: Required. Your API key for accessing the YouTube Translate API.\n\n## Deployment with Smithery\n\nThis package includes a `smithery.yaml` file for easy deployment with [Smithery](https://smithery.anthropic.com). \n\nTo deploy, set the `YOUTUBE_TRANSLATE_API_KEY` configuration parameter to your YouTube Translate API key.\n\n## Development\n\n### Prerequisites\n\n- Python 3.12+\n- Docker (optional)\n\n### Setup\n\n```bash\n# Create and activate a virtual environment using uv (recommended)\nuv venv\nsource .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\n\n# Install dependencies using uv\nuv pip install -e .\n\n# Alternatively, with standard tools\npython -m venv .venv\nsource .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\npip install -e .\n```\n\n### Testing with Claude Desktop\n\nTo test with Claude Desktop (macOS/Windows only), you'll need to add your server to the Claude Desktop configuration file located at `~/Library/Application Support/Claude/claude_desktop_config.json`.\n\n#### Method 1: Local Development\n\nUse this method if you want to test your local development version:\n\n```json\n{\n    \"mcpServers\": {\n        \"youtube-translate\": {\n            \"command\": \"uv\",\n            \"args\": [\n                \"--directory\",\n                \"/ABSOLUTE/PATH/TO/youtube-translate-mcp\",\n                \"run\",\n                \"-m\", \"youtube_translate_mcp\"\n            ],\n            \"env\": {\n              \"YOUTUBE_TRANSLATE_API_KEY\": \"YOUR_API_KEY\"\n            }\n        }\n    }\n}\n```\n\nMake sure to replace `/ABSOLUTE/PATH/TO/youtube-translate-mcp` with the actual path to your project directory.\n\n#### Method 2: Docker-based Testing\n\nIf you prefer to test using Docker (recommended for more reproducible testing):\n\n```json\n{\n  \"mcpServers\": {\n    \"youtube-translate\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"-i\",\n        \"--rm\",\n        \"-e\",\n        \"YOUTUBE_TRANSLATE_API_KEY\",\n        \"youtube-translate-mcp\"\n      ],\n      \"env\": {\n        \"YOUTUBE_TRANSLATE_API_KEY\": \"YOUR_API_KEY\"\n      }\n    }\n  }\n}\n```\n\nReplace `YOUR_API_KEY` with your actual YouTube Translate API key.\n\nFor more information on using MCP servers with Claude Desktop, see the [MCP documentation](https://modelcontextprotocol.io/quickstart/server).\n\n### Debugging\n - The normal MCP Inspector has a built in timeout for MCP tool calls, which is generally too short for these video processing calls (as of March 13, 2025). Better to use Claude Desktop and look at the MCP logs from Claude at ~/Library/Logs/Claude/mcp-server-{asfasf}.log.\n - Can do tail -f {log-file}.log to follow as you interact with Claude.\n\n## License\n\nMIT\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "translations",
        "youtube",
        "subtitles",
        "youtube translate",
        "video transcripts",
        "brianshin22 youtube"
      ],
      "category": "web-search"
    },
    "brightdata--brightdata-mcp": {
      "owner": "brightdata",
      "name": "brightdata-mcp",
      "url": "https://github.com/brightdata/brightdata-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/brightdata.webp",
      "description": "Access and extract structured data from public web sources without captcha challenges, enabling real-time market information retrieval and web searches. Supports ethical web scraping techniques for various applications like weather forecasting and decision-making.",
      "stars": 1387,
      "forks": 187,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-10-03T08:44:14Z",
      "readme_content": "<div align=\"center\">\n  <a href=\"https://brightdata.com/ai/mcp-server\">\n    <img src=\"https://github.com/user-attachments/assets/c21b3f7b-7ff1-40c3-b3d8-66706913d62f\" alt=\"Bright Data Logo\">\n  </a>\n\n  <h1>The Web MCP</h1>\n  \n  <p>\n    <strong>🌐 Give your AI real-time web superpowers</strong><br/>\n    <i>Seamlessly connect LLMs to the live web without getting blocked</i>\n  </p>\n\n  <p>\n    <a href=\"https://www.npmjs.com/package/@brightdata/mcp\">\n      <img src=\"https://img.shields.io/npm/v/@brightdata/mcp?style=for-the-badge&color=blue\" alt=\"npm version\"/>\n    </a>\n    <a href=\"https://www.npmjs.com/package/@brightdata/mcp\">\n      <img src=\"https://img.shields.io/npm/dw/@brightdata/mcp?style=for-the-badge&color=green\" alt=\"npm downloads\"/>\n    </a>\n    <a href=\"https://github.com/brightdata-com/brightdata-mcp/blob/main/LICENSE\">\n      <img src=\"https://img.shields.io/badge/license-MIT-purple?style=for-the-badge\" alt=\"License\"/>\n    </a>\n  </p>\n\n  <p>\n    <a href=\"#-quick-start\">Quick Start</a> •\n    <a href=\"#-features\">Features</a> •\n    <a href=\"#-pricing--modes\">Pricing</a> •\n    <a href=\"#-demos\">Demos</a> •\n    <a href=\"#-documentation\">Docs</a> •\n    <a href=\"#-support\">Support</a>\n  </p>\n\n  <div>\n    <h3>🎉 <strong>Free Tier Available!</strong> 🎉</h3>\n    <p><strong>5,000 requests/month FREE</strong> <br/>\n    <sub>Perfect for prototyping and everyday AI workflows</sub></p>\n  </div>\n</div>\n\n---\n\n## 🌟 Overview\n\n**The Web MCP** is your gateway to giving AI assistants true web capabilities. No more outdated responses, no more \"I can't access real-time information\" - just seamless, reliable web access that actually works.\n\nBuilt by [Bright Data](https://brightdata.com), the world's #1 web data platform, this MCP server ensures your AI never gets blocked, rate-limited, or served CAPTCHAs.\n\n<div align=\"center\">\n  <table>\n    <tr>\n      <td align=\"center\">✅ <strong>Works with Any LLM</strong><br/><sub>Claude, GPT, Gemini, Llama</sub></td>\n      <td align=\"center\">🛡️ <strong>Never Gets Blocked</strong><br/><sub>Enterprise-grade unblocking</sub></td>\n      <td align=\"center\">🚀 <strong>5,000 Free Requests</strong><br/><sub>Monthly</sub></td>\n      <td align=\"center\">⚡ <strong>Zero Config</strong><br/><sub>Works out of the box</sub></td>\n    </tr>\n  </table>\n</div>\n\n---\n\n## 🎯 Perfect For\n\n- 🔍 **Real-time Research** - Get current prices, news, and live data\n- 🛍️ **E-commerce Intelligence** - Monitor products, prices, and availability  \n- 📊 **Market Analysis** - Track competitors and industry trends\n- 🤖 **AI Agents** - Build agents that can actually browse the web\n- 📝 **Content Creation** - Access up-to-date information for writing\n- 🎓 **Academic Research** - Gather data from multiple sources efficiently\n\n---\n\n## ⚡ Quick Start\n\n\n<summary><b>📡 Use our hosted server - No installation needed!</b></summary>\n\nPerfect for users who want zero setup. Just add this URL to your MCP client:\n\n```\nhttps://mcp.brightdata.com/mcp?token=YOUR_API_TOKEN_HERE\n```\n\n**Setup in Claude Desktop:**\n1. Go to: Settings → Connectors → Add custom connector\n2. Name: `Bright Data Web`\n3. URL: `https://mcp.brightdata.com/mcp?token=YOUR_API_TOKEN`\n4. Click \"Add\" and you're done! ✨\n\n\n<summary><b>Run locally on your machine</b></summary>\n\n```json\n{\n  \"mcpServers\": {\n    \"Bright Data\": {\n      \"command\": \"npx\",\n      \"args\": [\"@brightdata/mcp\"],\n      \"env\": {\n        \"API_TOKEN\": \"<your-api-token-here>\"\n      }\n    }\n  }\n}\n```\n\n\n---\n\n## 🚀 Pricing & Modes\n\n<div align=\"center\">\n  <table>\n    <tr>\n      <th width=\"33%\">⚡ Rapid Mode (Free tier)</th>\n      <th width=\"33%\">💎 Pro Mode</th>\n    </tr>\n    <tr>\n      <td align=\"center\">\n        <h3>$0/month</h3>\n        <p><strong>5,000 requests</strong></p>\n        <hr/>\n        <p>✅ Web Search<br/>\n        ✅ Scraping with Web unlocker<br/>\n        ❌ Browser Automation<br/>\n        ❌ Web data tools</p>\n        <br/>\n        <code>Default Mode</code>\n      </td>\n      <td align=\"center\">\n        <h3>Pay-as-you-go</h3>\n        <p><strong>Every thing in rapid and 60+ Advanced Tools</strong></p>\n        <hr/>\n        <p>✅ Browser Control<br/>\n        ✅ Web Data APIs<br/>\n        <br/>\n        <br/>\n        <br/>\n        <code>PRO_MODE=true</code>\n      </td>\n    </tr>\n  </table>\n</div>\n\n> **💡 Note:** Pro mode is **not included** in the free tier and incurs additional charges based on usage.\n\n---\n\n## ✨ Features\n\n### 🔥 Core Capabilities\n\n<table>\n  <tr>\n    <td>🔍 <b>Smart Web Search</b><br/>Google-quality results optimized for AI</td>\n    <td>📄 <b>Clean Markdown</b><br/>AI-ready content extraction</td>\n  </tr>\n  <tr>\n    <td>🌍 <b>Global Access</b><br/>Bypass geo-restrictions automatically</td>\n    <td>🛡️ <b>Anti-Bot Protection</b><br/>Never get blocked or rate-limited</td>\n  </tr>\n  <tr>\n    <td>🤖 <b>Browser Automation</b><br/>Control real browsers remotely (Pro)</td>\n    <td>⚡ <b>Lightning Fast</b><br/>Optimized for minimal latency</td>\n  </tr>\n</table>\n\n### 🎯 Example Queries That Just Work\n\n```yaml\n✅ \"What's Tesla's current stock price?\"\n✅ \"Find the best-rated restaurants in Tokyo right now\"\n✅ \"Get today's weather forecast for New York\"\n✅ \"What movies are releasing this week?\"\n✅ \"What are the trending topics on Twitter today?\"\n```\n\n---\n\n## 🎬 Demos\n\n> **Note:** These videos show earlier versions. New demos coming soon! 🎥\n\n<details>\n<summary><b>View Demo Videos</b></summary>\n\n### Basic Web Search Demo\nhttps://github.com/user-attachments/assets/59f6ebba-801a-49ab-8278-1b2120912e33\n\n### Advanced Scraping Demo\nhttps://github.com/user-attachments/assets/61ab0bee-fdfa-4d50-b0de-5fab96b4b91d\n\n[📺 More tutorials on YouTube →](https://github.com/brightdata-com/brightdata-mcp/blob/main/examples/README.md)\n\n</details>\n\n---\n\n## 🔧 Available Tools\n\n### ⚡ Rapid Mode Tools (Default - Free)\n\n| Tool | Description | Use Case |\n|------|-------------|----------|\n| 🔍 `search_engine` | Web search with AI-optimized results | Research, fact-checking, current events |\n| 📄 `scrape_as_markdown` | Convert any webpage to clean markdown | Content extraction, documentation |\n\n### 💎 Pro Mode Tools (60+ Tools)\n\n<details>\n<summary><b>Click to see all Pro tools</b></summary>\n\n| Category | Tools | Description |\n|----------|-------|-------------|\n| **Browser Control** | `scraping_browser.*` | Full browser automation |\n| **Web Data APIs** | `web_data_*` | Structured data extraction |\n| **E-commerce** | Product scrapers | Amazon, eBay, Walmart data |\n| **Social Media** | Social scrapers | Twitter, LinkedIn, Instagram |\n| **Maps & Local** | Location tools | Google Maps, business data |\n\n[📚 View complete tool documentation →](https://github.com/brightdata-com/brightdata-mcp/blob/main/assets/Tools.md)\n\n</details>\n\n---\n\n## 🎮 Try It Now!\n\n### 🧪 Online Playground\nTry the Web MCP without any setup:\n\n<div align=\"center\">\n  <a href=\"https://brightdata.com/ai/playground-chat\">\n    <img src=\"https://img.shields.io/badge/Try_on-Playground-00C7B7?style=for-the-badge&logo=data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjQiIGhlaWdodD0iMjQiIHZpZXdCb3g9IjAgMCAyNCAyNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPHBhdGggZD0iTTEyIDJMMyA3VjE3TDEyIDIyTDIxIDE3VjdMMTIgMloiIHN0cm9rZT0id2hpdGUiIHN0cm9rZS13aWR0aD0iMiIvPgo8L3N2Zz4=\" alt=\"Playground\"/>\n  </a>\n</div>\n\n---\n\n## 🔧 Configuration\n\n### Basic Setup\n```json\n{\n  \"mcpServers\": {\n    \"Bright Data\": {\n      \"command\": \"npx\",\n      \"args\": [\"@brightdata/mcp\"],\n      \"env\": {\n        \"API_TOKEN\": \"your-token-here\"\n      }\n    }\n  }\n}\n```\n\n### Advanced Configuration\n```json\n{\n  \"mcpServers\": {\n    \"Bright Data\": {\n      \"command\": \"npx\",\n      \"args\": [\"@brightdata/mcp\"],\n      \"env\": {\n        \"API_TOKEN\": \"your-token-here\",\n        \"PRO_MODE\": \"true\",              // Enable all 60+ tools\n        \"RATE_LIMIT\": \"100/1h\",          // Custom rate limiting\n        \"WEB_UNLOCKER_ZONE\": \"custom\",   // Custom unlocker zone\n        \"BROWSER_ZONE\": \"custom_browser\" // Custom browser zone\n      }\n    }\n  }\n}\n```\n\n---\n\n## 📚 Documentation\n\n<div align=\"center\">\n  <table>\n    <tr>\n      <td align=\"center\">\n        <a href=\"https://docs.brightdata.com/mcp-server/overview\">\n          <img src=\"https://img.shields.io/badge/📖-API_Docs-blue?style=for-the-badge\" alt=\"API Docs\"/>\n        </a>\n      </td>\n      <td align=\"center\">\n        <a href=\"https://github.com/brightdata-com/brightdata-mcp/blob/main/examples\">\n          <img src=\"https://img.shields.io/badge/💡-Examples-green?style=for-the-badge\" alt=\"Examples\"/>\n        </a>\n      </td>\n      <td align=\"center\">\n        <a href=\"https://github.com/brightdata-com/brightdata-mcp/blob/main/CHANGELOG.md\">\n          <img src=\"https://img.shields.io/badge/📝-Changelog-orange?style=for-the-badge\" alt=\"Changelog\"/>\n        </a>\n      </td>\n      <td align=\"center\">\n        <a href=\"https://brightdata.com/blog/ai/web-scraping-with-mcp\">\n          <img src=\"https://img.shields.io/badge/📚-Tutorial-purple?style=for-the-badge\" alt=\"Tutorial\"/>\n        </a>\n      </td>\n    </tr>\n  </table>\n</div>\n\n---\n\n## 🚨 Common Issues & Solutions\n\n<details>\n<summary><b>🔧 Troubleshooting Guide</b></summary>\n\n### ❌ \"spawn npx ENOENT\" Error\n**Solution:** Install Node.js or use the full path to node:\n```json\n\"command\": \"/usr/local/bin/node\"  // macOS/Linux\n\"command\": \"C:\\\\Program Files\\\\nodejs\\\\node.exe\"  // Windows\n```\n\n### ⏱️ Timeouts on Complex Sites\n**Solution:** Increase timeout in your client settings to 180s\n\n### 🔑 Authentication Issues\n**Solution:** Ensure your API token is valid and has proper permissions\n\n### 📡 Remote Server Connection\n**Solution:** Check your internet connection and firewall settings\n\n[More troubleshooting →](https://github.com/brightdata-com/brightdata-mcp#troubleshooting)\n\n</details>\n\n---\n\n## 🤝 Contributing\n\nWe love contributions! Here's how you can help:\n\n- 🐛 [Report bugs](https://github.com/brightdata-com/brightdata-mcp/issues)\n- 💡 [Suggest features](https://github.com/brightdata-com/brightdata-mcp/issues)\n- 🔧 [Submit PRs](https://github.com/brightdata-com/brightdata-mcp/pulls)\n- ⭐ Star this repo!\n\nPlease follow [Bright Data's coding standards](https://brightdata.com/dna/js_code).\n\n---\n\n## 📞 Support\n\n<div align=\"center\">\n  <table>\n    <tr>\n      <td align=\"center\">\n        <a href=\"https://github.com/brightdata-com/brightdata-mcp/issues\">\n          <strong>🐛 GitHub Issues</strong><br/>\n          <sub>Report bugs & features</sub>\n        </a>\n      </td>\n      <td align=\"center\">\n        <a href=\"https://docs.brightdata.com/mcp-server/overview\">\n          <strong>📚 Documentation</strong><br/>\n          <sub>Complete guides</sub>\n        </a>\n      </td>\n      <td align=\"center\">\n        <a href=\"mailto:support@brightdata.com\">\n          <strong>✉️ Email</strong><br/>\n          <sub>support@brightdata.com</sub>\n        </a>\n      </td>\n    </tr>\n  </table>\n</div>\n\n---\n\n## 📜 License\n\nMIT © [Bright Data Ltd.](https://brightdata.com)\n\n---\n\n<div align=\"center\">\n  <p>\n    <strong>Built with ❤️ by</strong><br/>\n    <a href=\"https://brightdata.com\">\n      <img src=\"https://idsai.net.technion.ac.il/files/2022/01/Logo-600.png\" alt=\"Bright Data\" height=\"30\"/>\n    </a>\n  </p>\n  <p>\n    <sub>The world's #1 web data platform</sub>\n  </p>\n  \n  <br/>\n  \n  <p>\n    <a href=\"https://github.com/brightdata-com/brightdata-mcp\">⭐ Star us on GitHub</a> • \n    <a href=\"https://brightdata.com/blog\">Read our Blog</a>\n  </p>\n</div>\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "brightdata",
        "scraping",
        "web",
        "web scraping",
        "search brightdata",
        "ethical web"
      ],
      "category": "web-search"
    },
    "bsmi021--mcp-server-webscan": {
      "owner": "bsmi021",
      "name": "mcp-server-webscan",
      "url": "https://github.com/bsmi021/mcp-server-webscan",
      "imageUrl": "/freedevtools/mcp/pfp/bsmi021.webp",
      "description": "Scan and analyze web content, fetching pages in Markdown format, extracting links, and generating sitemaps for comprehensive site analysis.",
      "stars": 11,
      "forks": 9,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-09-03T22:33:16Z",
      "readme_content": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/bsmi021-mcp-server-webscan-badge.png)](https://mseep.ai/app/bsmi021-mcp-server-webscan)\n\n# MCP Webscan Server\n\n[![smithery badge](https://smithery.ai/badge/mcp-server-webscan)](https://smithery.ai/server/mcp-server-webscan)\n\nA Model Context Protocol (MCP) server for web content scanning and analysis. This server provides tools for fetching, analyzing, and extracting information from web pages.\n\n<a href=\"https://glama.ai/mcp/servers/u0tna3hemh\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/u0tna3hemh/badge\" alt=\"Webscan Server MCP server\" /></a>\n\n## Features\n\n- **Page Fetching**: Convert web pages to Markdown for easy analysis\n- **Link Extraction**: Extract and analyze links from web pages\n- **Site Crawling**: Recursively crawl websites to discover content\n- **Link Checking**: Identify broken links on web pages\n- **Pattern Matching**: Find URLs matching specific patterns\n- **Sitemap Generation**: Generate XML sitemaps for websites\n\n## Installation\n\n### Installing via Smithery\n\nTo install Webscan for Claude Desktop automatically via [Smithery](https://smithery.ai/server/mcp-server-webscan):\n\n```bash\nnpx -y @smithery/cli install mcp-server-webscan --client claude\n```\n\n### Manual Installation\n\n```bash\n# Clone the repository\ngit clone <repository-url>\ncd mcp-server-webscan\n\n# Install dependencies\nnpm install\n\n# Build the project\nnpm run build\n```\n\n## Usage\n\n### Starting the Server\n\n```bash\nnpm start\n```\n\nThe server runs on stdio transport, making it compatible with MCP clients like Claude Desktop.\n\n### Available Tools\n\n1. `fetch-page`\n   - Fetches a web page and converts it to Markdown.\n   - Parameters:\n     - `url` (required): URL of the page to fetch.\n     - `selector` (optional): CSS selector to target specific content.\n\n2. `extract-links`\n   - Extracts all links from a web page with their text.\n   - Parameters:\n     - `url` (required): URL of the page to analyze.\n     - `baseUrl` (optional): Base URL to filter links.\n     - `limit` (optional, default: 100): Maximum number of links to return.\n\n3. `crawl-site`\n   - Recursively crawls a website up to a specified depth.\n   - Parameters:\n     - `url` (required): Starting URL to crawl.\n     - `maxDepth` (optional, default: 2): Maximum crawl depth (0-5).\n\n4. `check-links`\n   - Checks for broken links on a page.\n   - Parameters:\n     - `url` (required): URL to check links for.\n\n5. `find-patterns`\n   - Finds URLs matching a specific pattern.\n   - Parameters:\n     - `url` (required): URL to search in.\n     - `pattern` (required): JavaScript-compatible regex pattern to match URLs against.\n\n6. `generate-site-map`\n   - Generates a simple XML sitemap by crawling.\n   - Parameters:\n     - `url` (required): Root URL for sitemap crawl.\n     - `maxDepth` (optional, default: 2): Maximum crawl depth for discovering URLs (0-5).\n     - `limit` (optional, default: 1000): Maximum number of URLs to include in the sitemap.\n\n## Example Usage with Claude Desktop\n\n1. Configure the server in your Claude Desktop settings:\n\n```json\n{\n  \"mcpServers\": {\n    \"webscan\": {\n      \"command\": \"node\",\n      \"args\": [\"path/to/mcp-server-webscan/build/index.js\"], // Corrected path\n      \"env\": {\n        \"NODE_ENV\": \"development\",\n        \"LOG_LEVEL\": \"info\" // Example: Set log level via env var\n      }\n    }\n  }\n}\n```\n\n2. Use the tools in your conversations:\n\n```\nCould you fetch the content from https://example.com and convert it to Markdown?\n```\n\n## Development\n\n### Prerequisites\n\n- Node.js >= 18\n- npm\n\n### Project Structure (Post-Refactor)\n\n```\nmcp-server-webscan/\n├── src/\n│   ├── config/\n│   │   └── ConfigurationManager.ts\n│   ├── services/\n│   │   ├── CheckLinksService.ts\n│   │   ├── CrawlSiteService.ts\n│   │   ├── ExtractLinksService.ts\n│   │   ├── FetchPageService.ts\n│   │   ├── FindPatternsService.ts\n│   │   ├── GenerateSitemapService.ts\n│   │   └── index.ts\n│   ├── tools/\n│   │   ├── checkLinksTool.ts\n│   │   ├── checkLinksToolParams.ts\n│   │   ├── crawlSiteTool.ts\n│   │   ├── crawlSiteToolParams.ts\n│   │   ├── extractLinksTool.ts\n│   │   ├── extractLinksToolParams.ts\n│   │   ├── fetchPageTool.ts\n│   │   ├── fetchPageToolParams.ts\n│   │   ├── findPatterns.ts\n│   │   ├── findPatternsToolParams.ts\n│   │   ├── generateSitemapTool.ts\n│   │   ├── generateSitemapToolParams.ts\n│   │   └── index.ts\n│   ├── types/\n│   │   ├── checkLinksTypes.ts\n│   │   ├── crawlSiteTypes.ts\n│   │   ├── extractLinksTypes.ts\n│   │   ├── fetchPageTypes.ts\n│   │   ├── findPatternsTypes.ts\n│   │   ├── generateSitemapTypes.ts\n│   │   └── index.ts\n│   ├── utils/\n│   │   ├── errors.ts\n│   │   ├── index.ts\n│   │   ├── logger.ts\n│   │   ├── markdownConverter.ts\n│   │   └── webUtils.ts\n│   ├── initialize.ts\n│   └── index.ts    # Main server entry point\n├── build/          # Compiled JavaScript (Corrected)\n├── node_modules/\n├── .clinerules\n├── .gitignore\n├── Dockerfile\n├── LICENSE\n├── mcp-consistant-servers-guide.md\n├── package.json\n├── package-lock.json\n├── README.md\n├── RFC-2025-001-Refactor.md\n├── smithery.yaml\n└── tsconfig.json\n```\n\n### Building\n\n```bash\nnpm run build\n```\n\n### Development Mode\n\n```bash\nnpm run dev\n```\n\n\n\n## Running evals\n\nThe evals package loads an mcp client that then runs the index.ts file, so there is no need to rebuild between tests. You can load environment variables by prefixing the npx command. Full documentation can be found [here](https://www.mcpevals.io/docs).\n\n```bash\nOPENAI_API_KEY=your-key  npx mcp-eval src/evals/evals.ts src/tools/extractLinksTool.ts\n```\n## Error Handling\n\nThe server implements comprehensive error handling:\n\n- Invalid parameters\n- Network errors\n- Content parsing errors\n- URL validation\n\nAll errors are properly formatted according to the MCP specification.\n\n## Contributing\n\n1. Fork the repository\n2. Create your feature branch (`git checkout -b feature/amazing-feature`)\n3. Commit your changes (`git commit -m 'Add some amazing feature'`)\n4. Push to the branch (`git push origin feature/amazing-feature`)\n5. Open a Pull Request\n\n## License\n\nMIT License - see the LICENSE file for details\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "webscan",
        "sitemaps",
        "web",
        "webscan scan",
        "server webscan",
        "site analysis"
      ],
      "category": "web-search"
    },
    "btwiuse--npm-search-mcp-server": {
      "owner": "btwiuse",
      "name": "npm-search-mcp-server",
      "url": "https://github.com/btwiuse/npm-search-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/btwiuse.webp",
      "description": "Search for npm packages using the `npm search` command to find relevant libraries and tools from the npm registry.",
      "stars": 14,
      "forks": 5,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-08-14T14:17:51Z",
      "readme_content": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/btwiuse-npm-search-mcp-server-badge.png)](https://mseep.ai/app/btwiuse-npm-search-mcp-server)\n\n# npm-search MCP Server\n[![smithery badge](https://smithery.ai/badge/npm-search-mcp-server)](https://smithery.ai/server/npm-search-mcp-server)\n\nA Model Context Protocol server that allows you to search for npm packages by calling the `npm search` command.\n\n<a href=\"https://glama.ai/mcp/servers/yeb3luefvf\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/yeb3luefvf/badge\" alt=\"npm-search-mcp-server MCP server\" /></a>\n\n### Available Tools\n\n- `search_npm_packages` - Search for npm packages.\n  - Required arguments:\n    - `query` (string): The search query.\n\n\n\n## Installation\n\n### Installing via Smithery\n\nTo install npm-search for Claude Desktop automatically via [Smithery](https://smithery.ai/server/npm-search-mcp-server):\n\n```bash\nnpx -y @smithery/cli install npm-search-mcp-server --client claude\n```\n\n### Using NPM (recommended)\n\nAlternatively you can install `npm-search-mcp-server` via npm:\n\n```bash\nnpm install -g npm-search-mcp-server\n```\n\nAfter installation, you can run it as a command using:\n\n```bash\nnpm-search-mcp-server\n```\n\n### Using uv\n\nWhen using [`uv`](https://docs.astral.sh/uv/) no specific installation is needed. We will\nuse [`uvx`](https://docs.astral.sh/uv/guides/tools/) to directly run *npm-search-mcp-server*.\n\n## Configuration\n\n### Configure for Claude.app\n\nAdd to your Claude settings:\n\n<details>\n<summary>Using npm installation</summary>\n\n```json\n\"mcpServers\": {\n  \"npm-search\": {\n    \"command\": \"npx\",\n    \"args\": [\"-y\", \"npm-search-mcp-server\"]\n  }\n}\n```\n</details>\n\n<details>\n<summary>Using uvx</summary>\n\n```json\n\"mcpServers\": {\n  \"npm-search\": {\n    \"command\": \"uvx\",\n    \"args\": [\"npm-search-mcp-server\"]\n  }\n}\n```\n</details>\n\n### Configure for Zed\n\nAdd to your Zed settings.json:\n\n<details>\n<summary>Using npm installation</summary>\n\n```json\n\"context_servers\": {\n  \"npm-search-mcp-server\": {\n    \"command\": \"npx\",\n    \"args\": [\"-y\", \"npm-search-mcp-server\"]\n  }\n},\n```\n</details>\n\n<details>\n<summary>Using uvx</summary>\n\n```json\n\"context_servers\": [\n  \"npm-search-mcp-server\": {\n    \"command\": \"uvx\",\n    \"args\": [\"npm-search-mcp-server\"]\n  }\n],\n```\n</details>\n\n## Example Interactions\n\n1. Search for npm packages:\n```json\n{\n  \"name\": \"search_npm_packages\",\n  \"arguments\": {\n    \"query\": \"express\"\n  }\n}\n```\nResponse:\n```json\n{\n  \"results\": [\n    {\n      \"name\": \"express\",\n      \"description\": \"Fast, unopinionated, minimalist web framework\",\n      \"version\": \"4.17.1\",\n      \"author\": \"TJ Holowaychuk\",\n      \"license\": \"MIT\"\n    },\n    ...\n  ]\n}\n```\n\n## Debugging\n\nYou can use the MCP inspector to debug the server. For uvx installations:\n\n```bash\nnpx @modelcontextprotocol/inspector npx -y npm-search-mcp-server\n```\n\nOr if you've installed the package in a specific directory or are developing on it:\n\n```bash\ncd path/to/servers/src/npm-search\nnpx @modelcontextprotocol/inspector uv run npm-search-mcp-server\n```\n\n## Examples of Questions for Claude\n\n1. \"Search for express package on npm\"\n2. \"Find packages related to react\"\n3. \"Show me npm packages for web development\"\n\n## Build\n\nDocker build:\n\n```bash\ncd src/npm-search\ndocker build -t mcp/npm-search .\n```\n\n## Contributing\n\nWe encourage contributions to help expand and improve npm-search-mcp-server. Whether you want to add new npm-related tools, enhance existing functionality, or improve documentation, your input is valuable.\n\nFor examples of other MCP servers and implementation patterns, see:\nhttps://github.com/modelcontextprotocol/servers\n\nPull requests are welcome! Feel free to contribute new ideas, bug fixes, or enhancements to make npm-search-mcp-server even more powerful and useful.\n\n## License\n\nnpm-search-mcp-server is licensed under the MIT License. This means you are free to use, modify, and distribute the software, subject to the terms and conditions of the MIT License. For more details, please see the LICENSE file in the project repository.",
      "npm_url": "https://www.npmjs.com/package/npm-search-mcp-server",
      "npm_downloads": 6100,
      "keywords": [
        "npm",
        "packages",
        "search",
        "npm packages",
        "search npm",
        "npm search"
      ],
      "category": "web-search"
    },
    "buhe--mcp_rss": {
      "owner": "buhe",
      "name": "mcp_rss",
      "url": "https://github.com/buhe/mcp_rss",
      "imageUrl": "/freedevtools/mcp/pfp/buhe.webp",
      "description": "Fetch and manage articles from various RSS feeds with features for automatic fetching, filtering, and favoriting articles. Supports importing RSS feed subscriptions via OPML files and exposes the content through an MCP API.",
      "stars": 19,
      "forks": 6,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-08-12T08:09:37Z",
      "readme_content": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/buhe-mcp-rss-badge.png)](https://mseep.ai/app/buhe-mcp-rss)\n\n# MCP RSS\n\nMCP RSS is a Model Context Protocol (MCP) server for interacting with RSS feeds.\n\n## Features\n\n- Parse OPML files to import RSS feed subscriptions\n- Automatically fetch and update articles from RSS feeds\n- Expose RSS content through MCP API\n- Mark articles as favorites\n- Filter articles by source and status\n\n## Installation\n\n### Prerequisites\n\n- Node.js (v14 or higher)\n- MySQL database\n\n### Setup MySQL\n\n```bash\ndocker run -itd --name mysql-test -p 3306:3306 -e MYSQL_ROOT_PASSWORD=123456 mysql\n```\n\n### Install MCP\n\nThis MCP uses stdio for communication. Fill in ``` npx mcp_rss ``` in the command, fill in the configuration that needs to be customized in the environment variable, and at least use your own opml file.\n\nUse in claude desktop\n```json\n{\n  \"command\": \"npx\",\n  \"args\": [\n    \"mcp_rss\"\n  ],\n  \"env\": {\n    \"OPML_FILE_PATH\": \"/PATH/YOUR_FEED_FILE.opml\"\n  }\n}\n\n```\n\n## Configuration\n\nSet the following environment variables to configure MCP RSS:\n\n### Configuration Options\n\n| Option | Description | Default Value |\n|--------|-------------|--------------|\n| DB_HOST | Database host | localhost |\n| DB_PORT | Database port | 3306 |\n| DB_USERNAME | Database username | root |\n| DB_PASSWORD | Database password | 123456 |\n| DB_DATABASE | Database name | mcp_rss |\n| OPML_FILE_PATH | Path to your OPML file with RSS feeds | \"./feeds.opml\" |\n| RSS_UPDATE_INTERVAL | Interval to fetch RSS updates (in minutes) | 1 |\n\n## MCP API Reference\n\nThe MCP RSS server exposes the following API methods:\n\n### get_content\n\nGet articles from subscribed RSS feeds.\n\n**Parameters:**\n\n| Parameter | Type | Description | Required |\n|-----------|------|-------------|---------|\n| status | string | Filter by article status (\"normal\" or \"favorite\") | No |\n| source | string | Filter by source (feed title) | No |\n| limit | number | Maximum number of articles to return | No (default: 10) |\n\n**Response:**\n\n```json\n{\n  \"articles\": [\n    {\n      \"id\": 1,\n      \"title\": \"Article Title\",\n      \"content\": \"Article content...\",\n      \"link\": \"https://example.com/article\",\n      \"pubDate\": \"2023-01-01T12:00:00Z\",\n      \"fetchDate\": \"2023-01-01T12:30:00Z\",\n      \"status\": \"normal\",\n      \"feedTitle\": \"Example Feed\",\n      \"feedCategory\": \"Technology\"\n    }\n  ],\n  \"success\": true\n}\n```\n\n### get_sources\n\nGet all available RSS feed sources.\n\n**Parameters:** None\n\n**Response:**\n\n```json\n{\n  \"sources\": [\n    {\n      \"id\": 1,\n      \"title\": \"Example Feed\",\n      \"category\": \"Technology\"\n    }\n  ],\n  \"success\": true\n}\n```\n\n### set_tag\n\nSet the status of an article (normal or favorite).\n\n**Parameters:**\n\n| Parameter | Type | Description | Required |\n|-----------|------|-------------|---------|\n| status | string | Article status (\"normal\" or \"favorite\") | Yes |\n| articleId | number | ID of the article to update | Yes |\n\n**Response:**\n\n```json\n{\n  \"success\": true,\n  \"message\": \"Article 1 status has been updated to favorite\"\n}\n```\n\n## License\n\nMIT\n",
      "npm_url": "https://www.npmjs.com/package/mcp_rss",
      "npm_downloads": 1106,
      "keywords": [
        "feeds",
        "mcp_rss",
        "rss",
        "rss feeds",
        "rss feed",
        "feed subscriptions"
      ],
      "category": "web-search"
    },
    "cabrit0--mcp_server_reuneMacacada": {
      "owner": "cabrit0",
      "name": "mcp_server_reuneMacacada",
      "url": "https://github.com/cabrit0/mcp_server_reuneMacacada",
      "imageUrl": "/freedevtools/mcp/pfp/cabrit0.webp",
      "description": "Generates structured learning paths by aggregating and organizing resources from the web based on specified topics, with support for web search and scraping. The server returns a standardized JSON structure for easy integration with client applications.",
      "stars": 1,
      "forks": 0,
      "license": "Other",
      "language": "Python",
      "updated_at": "2025-07-28T16:51:32Z",
      "readme_content": "# MCP Server\n\nA server that generates Master Content Plans (MCPs) based on topics. The server aggregates resources from the web and organizes them into structured learning paths.\n\n## Features\n\n- Generate learning paths for any topic (not just technology topics)\n- Find relevant resources using web search and scraping\n- Organize resources into a logical sequence with customizable number of nodes\n- Support for multiple languages with focus on Portuguese\n- Performance optimizations for Render's free tier\n- Caching system for faster responses\n- Return a standardized JSON structure for consumption by client applications\n- **NEW**: TF-IDF based resource relevance filtering to ensure resources match the requested topic\n- **NEW**: Strategic quiz distribution across learning trees for balanced learning experiences\n- **NEW**: YouTube integration to include relevant videos in learning paths\n- **NEW**: Category system to generate more specific content for different types of topics\n- **NEW**: Asynchronous task system with real-time progress feedback to improve user experience and avoid timeouts\n- **NEW**: Enhanced caching system for improved performance and faster response times\n- **NEW**: Optimized web scraping techniques for better resource utilization\n- **NEW**: Adaptive scraping system that automatically chooses the most efficient method for each website\n- **NEW**: Puppeteer instance pool for efficient browser reuse and reduced memory usage\n\n## Tech Stack\n\n- Python 3.9+\n- FastAPI\n- Pyppeteer for JavaScript-heavy web scraping\n- Pyppeteer-stealth for avoiding detection during web scraping\n- Puppeteer instance pool for efficient browser reuse\n- DuckDuckGo Search API\n- BeautifulSoup for HTML parsing\n- scikit-learn for TF-IDF based resource relevance filtering\n- yt-dlp for YouTube video search and metadata extraction\n- Redis (optional) for distributed caching\n- msgpack for efficient data serialization\n- cachetools for advanced in-memory caching\n\n## Installation\n\n1. Clone the repository:\n\n   ```\n   git clone https://github.com/yourusername/mcp_server.git\n   cd mcp_server\n   ```\n\n2. Create a virtual environment:\n\n   ```\n   python -m venv venv\n   source venv/bin/activate  # On Windows: venv\\Scripts\\activate\n   ```\n\n3. Install Python dependencies:\n\n   ```\n   pip install -r requirements.txt\n   ```\n\n4. Install Node.js dependencies (for the optimized scraping system):\n\n   ```\n   npm install\n   ```\n\n5. Install Chrome/Chromium for Pyppeteer (if not already installed)\n\n## Usage\n\n### Running Locally\n\n1. Start the server using the provided batch file (Windows):\n\n   ```\n   run_local.bat\n   ```\n\n   Or manually with uvicorn:\n\n   ```\n   uvicorn main:app --reload --host 0.0.0.0 --port 8000\n   ```\n\n2. Access the API at `http://localhost:8000`\n\n3. Generate an MCP by making a GET request to:\n\n   ```\n   GET /generate_mcp?topic=your_topic\n   ```\n\n4. Check the API documentation at `http://localhost:8000/docs`\n\n### Production URL\n\nThe production server is available at:\n\n```\nhttps://reunemacacada.onrender.com\n```\n\nAll endpoints documented in this README are available at both the local and production URLs.\n\n### Testing the Caching System\n\n1. Make a first request to generate an MCP (this will populate the cache):\n\n   ```\n   GET /generate_mcp?topic=python&num_nodes=15&language=pt\n   ```\n\n2. Make a second request with the same parameters (this should use the cache):\n\n   ```\n   GET /generate_mcp?topic=python&num_nodes=15&language=pt\n   ```\n\n   The second request should be significantly faster as the result will be retrieved from the cache.\n\n## Documentation\n\nDetailed documentation is available in the `docs` folder:\n\n- [API Reference](docs/api_reference.md) - Detailed API documentation\n- [Endpoints Reference](docs/endpoints_reference.md) - Complete reference of all endpoints\n- [Flutter Integration](docs/flutter_integration.md) - Guide for integrating with Flutter apps\n- [Async Tasks System](docs/async_tasks_system.md) - Documentation for the asynchronous task system\n- [Performance Improvements](docs/performance_improvements.md) - Overview of performance optimizations\n- [Caching System](docs/caching_system.md) - Documentation for the caching system\n- [Web Scraping Optimization](docs/web_scraping_optimization.md) - Details on web scraping optimizations\n\n## API Endpoints\n\n- `GET /health` - Health check endpoint\n- `GET /generate_mcp?topic={topic}&max_resources={max_resources}&num_nodes={num_nodes}&min_width={min_width}&max_width={max_width}&min_height={min_height}&max_height={max_height}&language={language}&category={category}` - Generate an MCP for the specified topic synchronously\n  - `topic` (required): The topic to generate an MCP for (minimum 3 characters)\n  - `max_resources` (optional): Maximum number of resources to include (default: 15, min: 5, max: 30)\n  - `num_nodes` (optional): Number of nodes to include in the learning path (default: 15, min: 10, max: 30)\n  - `min_width` (optional): Minimum width of the tree (nodes at first level) (default: 3, min: 2, max: 10)\n  - `max_width` (optional): Maximum width at any level of the tree (default: 5, min: 3, max: 15)\n  - `min_height` (optional): Minimum height of the tree (depth) (default: 3, min: 2, max: 8)\n  - `max_height` (optional): Maximum height of the tree (depth) (default: 7, min: 3, max: 12)\n  - `language` (optional): Language for resources (default: \"pt\")\n  - `category` (optional): Category for the topic (e.g., \"technology\", \"finance\", \"health\"). If not provided, it will be detected automatically.\n- `POST /generate_mcp_async?topic={topic}&max_resources={max_resources}&num_nodes={num_nodes}&min_width={min_width}&max_width={max_width}&min_height={min_height}&max_height={max_height}&language={language}&category={category}` - Start asynchronous generation of an MCP\n- `GET /status/{task_id}` - Check the status of an asynchronous task\n- `GET /tasks` - List all tasks\n- `POST /clear_cache?pattern={pattern}&clear_domain_cache={clear_domain_cache}` - Clear the cache based on a pattern\n  - `pattern` (optional): Pattern to match cache keys (default: \"\\*\" for all)\n  - `clear_domain_cache` (optional): Whether to also clear the domain method cache (default: false)\n- `GET /cache_stats` - Get statistics about the cache, including information about the domain method cache\n\n## Examples\n\n### Basic usage (Portuguese)\n\n```\nGET /generate_mcp?topic=python\n```\n\n### Custom number of nodes\n\n```\nGET /generate_mcp?topic=machine+learning&num_nodes=20\n```\n\n### English language\n\n```\nGET /generate_mcp?topic=javascript&language=en\n```\n\n### Specify category manually\n\n```\nGET /generate_mcp?topic=python&category=technology\n```\n\n### Full customization\n\n```\nGET /generate_mcp?topic=história+do+brasil&max_resources=20&num_nodes=25&min_width=4&max_width=8&min_height=4&max_height=8&language=pt\n```\n\n### Control tree structure\n\n```\nGET /generate_mcp?topic=machine+learning&min_width=2&max_width=4&min_height=5&max_height=10\n```\n\n### Asynchronous generation\n\n```\nPOST /generate_mcp_async?topic=inteligência+artificial&category=technology\n```\n\n### Check task status\n\n```\nGET /status/550e8400-e29b-41d4-a716-446655440000\n```\n\n### Clear cache\n\n```\nPOST /clear_cache\n```\n\n### Clear specific cache\n\n```\nPOST /clear_cache?pattern=mcp:*\n```\n\n## Performance Improvements\n\nThe MCP Server includes several performance optimizations:\n\n- **Caching System**: Results are cached to improve response times for repeated queries\n- **Asynchronous Task System**: Long-running operations are handled asynchronously\n- **Resource Filtering**: TF-IDF based filtering to select the most relevant resources\n- **Optimized Web Scraping**: Efficient web scraping techniques to reduce resource usage\n- **Adaptive Scraping System**: Automatically chooses the most efficient scraping method for each website\n- **Puppeteer Instance Pool**: Reuses browser instances to reduce memory usage and startup time\n- **Domain Method Cache**: Remembers which scraping method works best for each domain\n- **Resource Blocking**: Blocks unnecessary resources (images, stylesheets, fonts) during scraping\n\n### Performance Gains\n\n- **60-80% reduction in response time** for topics already in cache\n- **30-50% reduction in response time** for new topics\n- **40-60% reduction in memory usage** during web scraping\n- **3-5x increase in throughput** for simultaneous requests\n\n## Deployment\n\nThe server can be deployed to various platforms:\n\n### Using Docker\n\n```\ndocker build -t mcp-server .\ndocker run -p 8080:8080 mcp-server\n```\n\n### Deploying to Render, Fly.io, or other platforms\n\nFollow the platform-specific instructions for deploying a Docker container or a Python application.\n\n## License\n\n**Proprietary Software - All Rights Reserved**\n\nThis software is proprietary and confidential. Unauthorized copying, distribution, modification, public display, or public performance of this software is strictly prohibited. This software is intended for use under a paid subscription model only.\n\n© 2024 ReuneMacacada. All rights reserved.\n\nLast commit: v1.1.2 - Correção de problemas com DuckDuckGo rate limit e Puppeteer\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "mcp_server_reunemacacada",
        "search",
        "web",
        "web search",
        "learning paths",
        "resources web"
      ],
      "category": "web-search"
    },
    "cancelei--mcp_test_booking": {
      "owner": "cancelei",
      "name": "mcp_test_booking",
      "url": "https://github.com/cancelei/mcp_test_booking",
      "imageUrl": "/freedevtools/mcp/pfp/null.webp",
      "description": "Fetch and parse content from websites using advanced web scraping techniques, including the ability to extract structured data and bypass bot detection. Supports customizable content extraction through CSS selectors and direct HTTP requests.",
      "stars": 0,
      "forks": 0,
      "license": "Unknown",
      "language": "Unknown",
      "updated_at": "",
      "readme_content": "",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "scraping",
        "mcp_test_booking",
        "search",
        "web scraping",
        "scraping techniques",
        "content extraction"
      ],
      "category": "web-search"
    },
    "captainChaozi--search-intent-mcp": {
      "owner": "captainChaozi",
      "name": "search-intent-mcp",
      "url": "https://github.com/captainChaozi/search-intent-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/captainChaozi.webp",
      "description": "Analyzes user search keywords to reveal intent and provides insights for enhancing SEO strategies. Delivers possible categories, reasoning, and related references to improve search effectiveness.",
      "stars": 1,
      "forks": 2,
      "license": "No License",
      "language": "JavaScript",
      "updated_at": "2025-04-02T07:55:22Z",
      "readme_content": "# Search Intent MCP\n\n请通过 https://aisearchintent.com 获取 API key\n\n这是一个基于 MCP (Model Context Protocol) 的搜索意图分析服务。它可以帮助分析用户搜索关键词的意图，为 SEO 分析提供支持。\n\n## 功能特点\n\n- 分析搜索关键词的意图\n- 提供可能的分类\n- 提供推理过程\n- 提供相关参考链接\n- 提供搜索建议\n\n## 使用方法\n\n### claude 安装\n\n```json\n{\n  \"mcpServers\": {\n    \"search_intent\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@search-intent/mcp\"],\n      \"env\": {\n        \"SEARCH_INTENT_API_KEY\": \"xxx\"\n      }\n    }\n  }\n}\n```\n\n## 开发\n\n```bash\n# 克隆仓库\ngit clone\n\n# 安装依赖\npnpm install\n\n# 设置环境变量\nexport SEARCH_INTENT_API_KEY=your_api_key\n```\n\n## API 使用示例\n\n服务提供了一个名为 `search_intent_analysis` 的工具，可以这样使用：\n\n```json\n{\n  \"name\": \"search_intent_analysis\",\n  \"arguments\": {\n    \"query\": \"grok3\"\n  }\n}\n```\n\n返回结果示例：\n\n```json\n{\n  \"query\": \"grok3\",\n  \"intent\": \"Information Lookup about xAI's Grok 3...\",\n  \"possibleCategories\": [\n    \"AI Model\",\n    \"Technology\",\n    \"Chatbot\",\n    \"Product Information\"\n  ],\n  \"reasoning\": \"The user is likely trying to understand...\",\n  \"references\": [\n    {\n      \"url\": \"https://example.com\",\n      \"title\": \"Example Title\"\n    }\n  ],\n  \"searchSuggestions\": [\n    \"grok3 meaning\",\n    \"grok3 search intent\",\n    \"grok3 categories\",\n    \"grok3\"\n  ]\n}\n```\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "seo",
        "search",
        "keywords",
        "search intent",
        "search effectiveness",
        "search keywords"
      ],
      "category": "web-search"
    },
    "chat-prompt--gpters-search-mcp-server": {
      "owner": "chat-prompt",
      "name": "gpters-search-mcp-server",
      "url": "https://github.com/chat-prompt/gpters-search-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/chat-prompt.webp",
      "description": "Access knowledge from the GPTers AI study community efficiently through an MCP interface, enabling targeted searches based on creation date, author, and board categories. The server integrates with various MCP clients to deliver relevant insights from community content.",
      "stars": 0,
      "forks": 3,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-04-19T01:18:20Z",
      "readme_content": "# GPTers Search MCP Server\n\n[![smithery badge](https://smithery.ai/badge/@chat-prompt/gpters-search-mcp-server)](https://smithery.ai/server/@chat-prompt/gpters-search-mcp-server)\n\n본 MCP 서버는 GPTers AI 스터디 커뮤니티의 지식을 검색할 수 있는 MCP 서버입니다.\n\n## 소개\n\nGPTers Search MCP Server는 GPTers AI 스터디 커뮤니티의 지식을 검색하고, 이를 MCP를 통해 Claude Desktop이나 Cursor와 같은 MCP Client(Host)에게 제공합니다.  \n해당 서버는 GPTers 커뮤니티의 지식을 기반으로 한 다양한 응용 서비스에 활용될 수 있습니다.\n\n## 주요 기능\n\n- GPTers AI 스터디 커뮤니티 검색 API 연동\n- MCP 형식의 검색 결과 제공\n- 작성일시, 작성자, 게시판 기준 필터링 지원\n    - 작성일시 기준으로 검색 가능\n        - 프롬프트 예 : gpters에서 ~~를 찾아주고, 최근 1년 내의 사례 글에서 정리해줘.\n    - 작성자 기준으로 검색 가능\n        - 프롬프트 예 : gpters에서 작성자가 김태현이고 ~~ 관련한 게시물을 찾아서 정리해줘.\n    - 게시판 이름으로 검색 가능\n        - 프롬프트 예 : gpters에서 \"이미지 / 음악 / 영상\" 게시판에서 검색해서 ~~ 관련한 게시물을 찾아서 정리해줘.\n\n## 설치 방법\n\n### Smithery를 통한 설치\n\n[Smithery](https://smithery.ai/server/@chat-prompt/gpters-search-mcp-server)를 통해 GPTers Search MCP 서버를 자동으로 설치하려면 AI 클라이언트에 따라 다음 명령 중 하나를 사용하세요:\n\n> **Note**: API 키는 GPTers 커뮤니티 관리자에게 요청하여 발급받을 수 있습니다.\n\nClaude Desktop:\n\n```bash\nnpx -y @smithery/cli@latest install @chat-prompt/gpters-search-mcp-server --client claude --config '\"{\\\"apiSecretKey\\\":\\\"your_api_key\\\"}\"'\n```\n\nCursor:\n\n```bash\nnpx -y @smithery/cli@latest install @chat-prompt/gpters-search-mcp-server --client cursor --config '\"{\\\"apiSecretKey\\\":\\\"your_api_key\\\"}\"'\n```\n\n## 사용 예제\n\n### 기본 검색\n\n\n\n\n### 작성일시 기준 검색",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "searches",
        "search",
        "community",
        "search chat",
        "gpters search",
        "searches based"
      ],
      "category": "web-search"
    },
    "chenmingkong--bilibili-mcp-server": {
      "owner": "chenmingkong",
      "name": "bilibili-mcp-server",
      "url": "https://github.com/chenmingkong/bilibili-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/chenmingkong.webp",
      "description": "Seamlessly access Bilibili's API, enabling advanced searching and data retrieval operations for users, videos, live streams, and articles, as well as fetching video comments (danmaku). Offers functionalities for general searches and precise filtering of results across various content types.",
      "stars": 1,
      "forks": 2,
      "license": "MIT License",
      "language": "",
      "updated_at": "2025-09-07T15:31:21Z",
      "readme_content": "# Bilibili API MCP Server\r\n\r\n用于哔哩哔哩 API 的 MCP（模型上下文协议）服务器，支持多种操作。\r\n\r\n## 环境要求\r\n\r\n- [uv](https://docs.astral.sh/uv/) - 一个项目管理工具，可以很方便管理依赖。\r\n\r\n## 使用方法\r\n\r\n1. clone 本项目\r\n\r\n2. 使用 uv 安装依赖\r\n\r\n```bash\r\nuv sync\r\n```\r\n\r\n3. 在任意 mcp client 中配置本 Server\r\n\r\n```json\r\n{\r\n  \"mcpServers\": {\r\n    \"bilibili\": {\r\n      \"command\": \"uv\",\r\n      \"args\": [\r\n        \"--directory\",\r\n        \"/your-project-path/bilibili-mcp-server\",\r\n        \"run\",\r\n        \"bilibili.py\"\r\n      ]\r\n    }\r\n  }\r\n}\r\n```\r\n\r\n4. 在 client 中使用\r\n\r\n## 支持的操作\r\n\r\n支持以下操作：\r\n\r\n1. `general_search`: 基础搜索功能，使用关键词在哔哩哔哩进行搜索。\r\n2. `search_user`: 专门用于搜索哔哩哔哩用户的功能，可以按照粉丝数排序。\r\n3. `get_precise_results`: 精确搜索功能，可以过滤掉不必要的信息，支持多种搜索类型：\r\n   - 用户搜索 (`user`)：精确匹配用户名，只返回完全匹配的结果。例如搜索\"双雷\"只会返回用户名为\"双雷\"的账号信息，不会返回其他相关用户\r\n   - 视频搜索 (`video`)\r\n   - 直播搜索 (`live`)\r\n   - 专栏搜索 (`article`)\r\n返回结果包含 `exact_match` 字段，标识是否找到精确匹配的结果。\r\n4. `get_video_danmaku·`: 获取视频弹幕信息。\r\n\r\n## 如何为本项目做贡献\r\n\r\n1. Fork 本项目\r\n2. 新建分支，并在新的分支上做改动\r\n3. 提交 PR\r\n\r\n## License\r\n\r\nMIT\r\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "searches",
        "searching",
        "search",
        "web search",
        "search chenmingkong",
        "advanced searching"
      ],
      "category": "web-search"
    },
    "chenxilol--perplexity-mcp-go": {
      "owner": "chenxilol",
      "name": "perplexity-mcp-go",
      "url": "https://github.com/chenxilol/perplexity-mcp-go",
      "imageUrl": "/freedevtools/mcp/pfp/chenxilol.webp",
      "description": "Connect AI models to the Perplexity search API for real-time web searches, retrieving results with citations. Access updated information and related questions to enhance application capabilities.",
      "stars": 4,
      "forks": 0,
      "license": "MIT License",
      "language": "Go",
      "updated_at": "2025-03-29T15:54:54Z",
      "readme_content": "# Perplexity Search MCP Server\n\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n\nA Go implementation of a Perplexity Search MCP server that allows large language models (LLMs) to access the Perplexity search API through the [Model Context Protocol (MCP)](https://modelcontextprotocol.io/).\n\n## Features\n\n- **perplexity_search**: Perform web searches and return results, including citations\n  - **Parameters**:\n    - `query` (string, required): The search query\n    - `search_recency_filter` (string, optional): Filter results by time (`month`, `week`, `day`, `hour`)\n    - `max_tokens` (integer, optional): Maximum number of tokens to return\n    - `temperature` (number, optional, default: 0.2): Controls randomness in response\n    - `top_p` (number, optional, default: 0.9): Nucleus sampling threshold\n    - `search_domain_filter` (array, optional): List of domains to limit search results\n    - `return_images` (boolean, optional): Include image links in results\n    - `return_related_questions` (boolean, optional): Include related questions\n    - `top_k` (number, optional, default: 0): Number of tokens for top-k filtering\n    - `stream` (boolean, optional): Stream response incrementally\n    - `presence_penalty` (number, optional, default: 0): Adjust likelihood of new topics\n    - `frequency_penalty` (number, optional, default: 1): Reduce repetition\n    - `web_search_options` (object, optional): Configuration options for web search\n\n## Setup & Usage\n\n### Installing via Smithery\n\nTo install Perplexity Search Golang for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@chenxilol/perplexity-mcp-go):\n\n```bash\nnpx -y @smithery/cli install @chenxilol/perplexity-mcp-go --client claude\n```\n\n### Prerequisites\n\n- Go 1.23 or higher\n- Perplexity API key\n\n### Installation\n\n1. Clone the repository:\n\n```bash\ngit clone https://github.com/chenxilol/perplexity-mcp-go.git\ncd perplexity-mcp-go\n```\n\n2. Build the application:\n\n```bash\ngo build -o perplexity-search-mcp\n```\n\n### Running Locally\n\n1. Set your Perplexity API key:\n\n```bash\nexport PERPLEXITY_API_KEY=\"your-api-key-here\"\n```\n\n2. Run the server:\n\n```bash\n./perplexity-search-mcp\n```\n\n### Integrating with Claude\n\n1. Copy the provided `claude_desktop_config.json` to your Claude configuration directory:\n   - Windows: `%USERPROFILE%\\AppData\\Roaming\\Claude\\claude_desktop_config.json`\n   - macOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\n   - Linux: `~/.config/Claude/claude_desktop_config.json`\n\n2. Edit the configuration file to include your API key:\n\n```json\n{\n  \"mcpServers\": {\n    \"perplexity-search\": {\n      \"command\": \"/path/to/perplexity-search-mcp\",\n      \"env\": {\n        \"PERPLEXITY_API_KEY\": \"your-api-key-here\"\n      }\n    }\n  }\n}\n```\n\n### Docker Support\n\n1. Build the Docker image:\n\n```bash\ndocker build -t perplexity-search-mcp:latest .\n```\n\n2. Run the container:\n\n```bash\ndocker run -i --rm -e PERPLEXITY_API_KEY=your-api-key-here perplexity-search-mcp:latest\n```\n\n## Example Usage\n\nOnce configured, Claude can use the perplexity_search tool via MCP to perform real-time web searches.\n\nExample search with parameters:\n```json\n{\n  \"query\": \"latest AI research developments\",\n  \"search_recency_filter\": \"week\",\n  \"temperature\": 0.5,\n  \"return_related_questions\": true,\n  \"web_search_options\": {\n    \"search_context_size\": \"high\"\n  }\n}\n```\n\n## Troubleshooting\n\nIf you encounter issues:\n1. Verify your API key is correctly set\n2. Check network connectivity\n3. Examine stderr logs for error messages\n\n## License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n\n## Acknowledgments\n\n- [Model Context Protocol](https://modelcontextprotocol.io/) for the MCP specification\n- [MCP-Go](https://github.com/mark3labs/mcp-go) for the Go MCP implementation\n- [Perplexity](https://www.perplexity.ai/) for their search API \n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "searches",
        "search",
        "ai",
        "perplexity search",
        "web search",
        "search api"
      ],
      "category": "web-search"
    },
    "chiziuwaga--vercel-ai-sdk-mcp-project": {
      "owner": "chiziuwaga",
      "name": "vercel-ai-sdk-mcp-project",
      "url": "https://github.com/chiziuwaga/vercel-ai-sdk-mcp-project",
      "imageUrl": "/freedevtools/mcp/pfp/chiziuwaga.webp",
      "description": "Integrates Vercel AI SDK capabilities into AI development environments to facilitate the generation of objects, text, and UI components. Supports seamless interaction with other tools like Figma and Magic MCP, enhancing design and development workflows.",
      "stars": 0,
      "forks": 1,
      "license": "No License",
      "language": "TypeScript",
      "updated_at": "2025-04-12T22:03:15Z",
      "readme_content": "# Vercel AI SDK MCP Server Project\n\n[![smithery badge](https://smithery.ai/badge/@chiziuwaga/vercel-ai-sdk-mcp-project)](https://smithery.ai/server/@chiziuwaga/vercel-ai-sdk-mcp-project)\n\nThis repository contains a Model Context Protocol (MCP) server designed to expose capabilities of the Vercel AI SDK Core to AI development environments like Cursor. It allows leveraging features like `generateObject`, `generateText`, `streamText`, and UI generation alongside other MCP servers (like `mcp-figma` and `magic-mcp` via Smithery).\n\n## Core Features\n\n*   **Vercel AI SDK Integration:** Provides MCP tools wrapping core Vercel AI SDK functions (`generate_object`, `generate_ui_component`, etc.).\n*   **Tool Categorization:** Implements a `ToolManager` with a `set_tool_category` meta-tool to manage the number of active tools exposed to Cursor, keeping within reasonable limits.\n*   **Figma/Magic MCP Placeholders:** Includes placeholder connectors and tool registrations for `mcp-figma` and `magic-mcp`, intended for orchestration via Cursor AI (Pathway 2).\n*   **Smithery Deployment Ready:** Configured with `Dockerfile` and `smithery.yaml` for easy deployment on [Smithery.ai](https://smithery.ai/).\n*   **Cursor Integration:** Designed to be used within Cursor via the `.cursor/mcp.json` configuration.\n\n## Architectural Approach (Pathway 2 Orchestration)\n\nThis server is primarily designed to be one component in a multi-MCP workflow orchestrated by the AI within Cursor (Pathway 2).\n\nThe intended workflow involves:\n1.  Using prompts and Cursor Rules (`.cursor/rules/`) to guide the AI.\n2.  Making sequential calls to different MCP servers:\n    *   `mcp-figma` (via Smithery) for design extraction.\n    *   `magic-mcp` (via Smithery) for inspiration/component building.\n    *   This `vercel-ai-sdk-mcp` server for Vercel AI SDK specific tasks (like structured generation).\n3.  The AI combines context from each step to achieve the final goal.\n\nWhile a composite tool (`generate_enhanced_component_from_figma`) demonstrating direct server-to-server interaction (Pathway 1) exists in the code (`src/integrations/crossIntegration.ts`), it requires implementing functional MCP clients within the connectors and is not the primary intended usage pattern for this setup.\n\n## Prerequisites\n\n*   [Node.js](https://nodejs.org/) (v20 or later recommended)\n*   [npm](https://www.npmjs.com/)\n*   [Git](https://git-scm.com/)\n*   [Cursor](https://cursor.sh/)\n*   [Smithery Account](https://smithery.ai/) (for deployment)\n*   **API Keys:**\n    *   OpenAI API Key (Required)\n    *   Figma API Key (Required for *implementing* Figma integration)\n    *   21st Dev API Key (Required for *implementing* Magic MCP integration)\n\n## Local Setup\n\n1.  **Clone Repository:**\n    ```bash\n    git clone https://github.com/chiziuwaga/vercel-ai-sdk-mcp-project.git\n    cd vercel-ai-sdk-mcp-project\n    ```\n2.  **Install Dependencies:**\n    ```bash\n    npm install\n    ```\n3.  **Create `.env` File:**\n    Copy `.env.example` to `.env` and fill in your API keys:\n    ```dotenv\n    OPENAI_API_KEY=sk-your-openai-key\n    ANTHROPIC_API_KEY=sk-ant-your-anthropic-key # Optional\n    FIGMA_API_KEY=your-figma-key              # For future implementation\n    TWENTY_FIRST_API_KEY=your-21st-key        # For future implementation\n    TRANSPORT_TYPE=stdio                      # Keep as stdio for local\n    PORT=3000                                 # Only used if TRANSPORT_TYPE=sse\n    ```\n4.  **Build the Code:**\n    ```bash\n    npm run build\n    ```\n5.  **Run Locally:**\n    ```bash\n    npm run start\n    ```\n    The server will be running using stdio, waiting for connections.\n\n## Cursor Integration (Local)\n\nTo use the *local* server in Cursor:\n\n1.  Ensure `mcp-figma` and `magic-mcp` are runnable via `npx` locally.\n2.  Modify your workspace `.cursor/mcp.json` to run this server directly with Node:\n\n    ```json\n    {\n      \"mcpServers\": {\n        \"magic-mcp\": { ... }, // Keep existing Smithery config\n        \"mcp-figma\": { ... }, // Keep existing Smithery config\n        \"vercel-ai-sdk-mcp\": {\n          \"command\": \"node\",\n          \"args\": [\"dist/index.js\"], // Path relative to workspace root\n          \"env\": { // Pass keys directly for local run\n            \"OPENAI_API_KEY\": \"${OPENAI_API_KEY}\",\n            \"ANTHROPIC_API_KEY\": \"${ANTHROPIC_API_KEY}\",\n            \"FIGMA_API_KEY\": \"${FIGMA_API_KEY}\",\n            \"TWENTY_FIRST_API_KEY\": \"${TWENTY_FIRST_API_KEY}\",\n            \"TRANSPORT_TYPE\": \"stdio\"\n          }\n        }\n      }\n    }\n    ```\n3.  Make sure the `${API_KEY}` variables are accessible in your environment where Cursor can read them.\n\n## Usage Example (Pathway 2)\n\n1.  **Ensure MCP Servers are running** (locally or configured via Smithery in `.cursor/mcp.json`).\n2.  **Create Cursor Rules:** Add rule files in `.cursor/rules/` to guide the AI (see section below).\n3.  **Prompt Cursor AI:** Give a multi-step prompt like the User Story described previously, instructing the AI to call tools sequentially across `mcp-figma`, `magic-mcp`, and `vercel-ai-sdk-mcp`.\n\n    *Example Snippet:*\n    ```\n    \"First, use mcp-figma's extract_figma_design... Then use magic-mcp's inspiration tool... Finally, use vercel-ai-sdk-mcp's generate_ui_component with the combined context...\"\n    ```\n\n## Cursor Rules (`.cursor/rules/`)\n\nEffective use of the Pathway 2 orchestration relies on creating guidance rules for the Cursor AI. You **must** create a `.cursor/rules/` directory in your project root and add rule files (e.g., `figma.cursorule`, `magic.cursorule`, `vercel.cursorule`).\n\n*   These files should contain natural language instructions on:\n    *   Which tools to use from each MCP server for specific tasks.\n    *   How to structure prompts for those tools.\n    *   How to pass context (data) between sequential tool calls.\n    *   Standard workflows (e.g., Figma -> Magic -> Vercel).\n\nRefer to the [Cursor Rules Documentation](https://docs.cursor.com/context/rules-for-ai) for syntax and examples.\n\n## Deployment (Smithery)\n\n1.  **Push to GitHub:** Ensure your latest code, including `Dockerfile` and `smithery.yaml`, is pushed to the `main` branch on GitHub.\n2.  **Go to Smithery.ai:** Log in and find/add your `chiziuwaga/vercel-ai-sdk-mcp-project` server.\n3.  **Deploy:** Go to the \"Deployments\" tab and click \"Create Deployment\".\n4.  **Configure:** Provide the **required** API keys (`openaiApiKey`, etc.) when prompted by Smithery. These are stored securely.\n5.  **Launch:** Start the deployment process. Smithery builds the Docker image and runs the container.\n\n## Cursor Integration (Deployed)\n\nOnce deployed on Smithery:\n\n1.  Update your `.cursor/mcp.json` to use the Smithery CLI runner for your server (this should match the current content):\n\n    ```json\n    {\n      \"mcpServers\": {\n        \"magic-mcp\": { ... }, // Keep existing Smithery config\n        \"mcp-figma\": { ... }, // Keep existing Smithery config\n        \"vercel-ai-sdk-mcp\": {\n          \"command\": \"npx\",\n          \"args\": [\n            \"-y\", \"@smithery/cli@latest\", \"run\",\n            \"chiziuwaga/vercel-ai-sdk-mcp-project\",\n            \"--config\",\n            // Ensure these env vars are available to Cursor\n            \"{\"openaiApiKey\":\"${OPENAI_API_KEY}\", \"anthropicApiKey\":\"${ANTHROPIC_API_KEY}\", \"figmaApiKey\":\"${FIGMA_API_KEY}\", \"twentyFirstApiKey\":\"${TWENTY_FIRST_API_KEY}\", \"transportType\":\"stdio\"}\"\n          ]\n        }\n      }\n    }\n    ```\n2.  Ensure the `${API_KEY}` variables referenced in the `--config` JSON string are accessible to Cursor from your environment.\n\n## Configuration\n\nAPI Keys are required for full functionality:\n\n*   `OPENAI_API_KEY`: **Required** for Vercel AI SDK tools. Provide during Smithery deployment config or in `.env` for local runs.\n*   `ANTHROPIC_API_KEY`: Optional, for Anthropic models.\n*   `FIGMA_API_KEY`: Required **only** when `FigmaConnector` is implemented.\n*   `TWENTY_FIRST_API_KEY`: Required **only** when `MagicMcpConnector` is implemented.\n\n## Placeholders & Future Work\n\n*   **Implement Connectors:** The `src/integrations/figma/connector.ts` and `src/integrations/magicMcp/connector.ts` contain **placeholders**. They need to be implemented with actual API calls (for Figma) and MCP client logic (potentially for Magic MCP, depending on its interface) to enable the integration tools.\n*   **Add More Tools:** Implement the remaining Vercel AI SDK tools (text generation, streaming, chat, code gen, etc.) as outlined in the specs.\n*   **Error Handling:** Enhance error handling, especially around missing API keys.\n*   **Testing:** Add automated tests.\n\n## License\n\nISC License (as per `package.json`). ",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "vercel",
        "sdk",
        "ai",
        "vercel ai",
        "ai sdk",
        "ai development"
      ],
      "category": "web-search"
    },
    "chrishayuk--chuk-mcp-time-server": {
      "owner": "chrishayuk",
      "name": "chuk-mcp-time-server",
      "url": "https://github.com/chrishayuk/chuk-mcp-time-server",
      "imageUrl": "/freedevtools/mcp/pfp/chrishayuk.webp",
      "description": "Retrieve current times and convert between various global timezones using advanced utilities. Provides accurate and reliable time data through a microservice built with Python and asyncio.",
      "stars": 3,
      "forks": 4,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-08-25T03:08:56Z",
      "readme_content": "# MCP Time Server\n\n## Overview\n\nThe MCP Time Server is a sophisticated Python-based microservice designed to provide advanced time-related utilities across different timezones. It offers robust functionality for retrieving current times and converting times between various global timezones.\n\n## Project Details\n\n- **Version**: 0.1.1\n- **Python Compatibility**: Python 3.11+\n\n## Features\n\n- **Current Time Retrieval**: Get the current time for any IANA timezone\n- **Time Zone Conversion**: Convert times between different time zones\n- **Comprehensive Validation**: Robust input validation using Pydantic models\n- **Async Server Architecture**: Built with asyncio for efficient performance\n- **Flexible Configuration**: Configurable through environment variables and config files\n\n## Dependencies\n\nCore dependencies:\n- mcp (>=1.6.0)\n- pydantic (>=2.11.2)\n- PyYAML (>=6.0.2)\n- pyz (>=0.4.3)\n\nDevelopment dependencies:\n- pytest (>=8.3.5)\n\n## Installation\n\n### Prerequisites\n\n- Python 3.11 or higher\n- pip\n- (Optional) Virtual environment recommended\n\n### Install from PyPI\n\n```bash\npip install chuk-mcp-artifact-server\n```\n\n### Install from Source\n\n1. Clone the repository:\n```bash\ngit clone <repository-url>\ncd chuk-mcp-artifact-server\n```\n\n2. Create a virtual environment:\n```bash\npython -m venv venv\nsource venv/bin/activate  # On Windows, use `venv\\Scripts\\activate`\n```\n\n3. Install the package:\n```bash\npip install .  # Installs the package in editable mode\n```\n\n### Development Installation\n\nTo set up for development:\n```bash\npip install .[dev]  # Installs package with development dependencies\n```\n\n## Running the Server\n\n### Command-Line Interface\n\n```bash\nchuk-mcp-artifact-server\n```\n\n### Programmatic Usage\n\n```python\nfrom chuk_mcp_artifact_server.main import main\n\nif __name__ == \"__main__\":\n    main()\n```\n\n## Environment Variables\n\n- `NO_BOOTSTRAP`: Set to disable component bootstrapping\n- Other configuration options can be set in the configuration files\n\n## Available Tools\n\n### 1. Get Current Time\n\n**Input**:\n- `timezone`: IANA timezone name (e.g., 'America/New_York')\n\n**Example**:\n```python\nget_current_time('Europe/London')\n```\n\n**Returns**:\n- Current time in the specified timezone\n- Timezone details\n- Daylight Saving Time (DST) status\n\n### 2. Convert Time\n\n**Input**:\n- `source_timezone`: Source timezone (IANA format)\n- `time`: Time in HH:MM (24-hour) format\n- `target_timezone`: Target timezone (IANA format)\n\n**Example**:\n```python\nconvert_time('America/New_York', '14:30', 'Europe/Paris')\n```\n\n**Returns**:\n- Source time details\n- Target time details\n- Time difference between zones\n\n## Development\n\n### Code Formatting\n\n- Black is used for code formatting\n- isort is used for import sorting\n- Line length is set to 88 characters\n\n### Running Tests\n\n```bash\npytest\n```\n\n## Contributing\n\n1. Fork the repository\n2. Create your feature branch (`git checkout -b feature/AmazingFeature`)\n3. Ensure code passes formatting and testing\n4. Commit your changes (`git commit -m 'Add some AmazingFeature'`)\n5. Push to the branch (`git push origin feature/AmazingFeature`)\n6. Open a Pull Request\n\n## License\n\n[MIT License](LICENSE)",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "timezones",
        "microservice",
        "mcp",
        "timezones using",
        "global timezones",
        "time server"
      ],
      "category": "web-search"
    },
    "chuanmingliu--mcp-webresearch": {
      "owner": "chuanmingliu",
      "name": "mcp-webresearch",
      "url": "https://github.com/chuanmingliu/mcp-webresearch",
      "imageUrl": "/freedevtools/mcp/pfp/chuanmingliu.webp",
      "description": "Integrates real-time web research capabilities into workflows by enabling Google search, webpage content extraction, research session tracking, and screenshot capture.",
      "stars": 16,
      "forks": 1,
      "license": "MIT License",
      "language": "",
      "updated_at": "2025-08-31T08:24:56Z",
      "readme_content": "# MCP Web Research Server\n\nA Model Context Protocol (MCP) server for web research. \n\nBring real-time info into Claude and easily research any topic.\n\n## Features\n\n- Google search integration\n- Webpage content extraction\n- Research session tracking (list of visited pages, search queries, etc.)\n- Screenshot capture\n\n## Prerequisites\n\n- [Node.js](https://nodejs.org/) >= 18 (includes `npm` and `npx`)\n- [Claude Desktop app](https://claude.ai/download)\n\n## Installation\n\nFirst, ensure you've downloaded and installed the [Claude Desktop app](https://claude.ai/download) and you have npm installed.\n\nNext, add this entry to your `claude_desktop_config.json` (on Mac, found at `~/Library/Application\\ Support/Claude/claude_desktop_config.json`):\n\n```json\n{\n  \"mcpServers\": {\n    \"webresearch\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@mzxrai/mcp-webresearch@latest\"]\n    }\n  }\n}\n```\n\nThis config allows Claude Desktop to automatically start the web research MCP server when needed.\n\n## Usage\n\nSimply start a chat with Claude and send a prompt that would benefit from web research. If you'd like a prebuilt prompt customized for deeper web research, you can use the `agentic-research` prompt that we provide through this package. Access that prompt in Claude Desktop by clicking the Paperclip icon in the chat input and then selecting `Choose an integration` → `webresearch` → `agentic-research`.\n\n<img src=\"https://i.ibb.co/N6Y3C0q/Screenshot-2024-12-05-at-11-01-27-PM.png\" alt=\"Example screenshot of web research\" width=\"400\"/>\n\n### Tools\n\n1. `search_google`\n   - Performs Google searches and extracts results\n   - Arguments: `{ query: string }`\n\n2. `visit_page`\n   - Visits a webpage and extracts its content\n   - Arguments: `{ url: string, takeScreenshot?: boolean }`\n\n3. `take_screenshot`\n   - Takes a screenshot of the current page\n   - No arguments required\n\n### Prompts\n\n#### `agentic-research`\nA guided research prompt that helps Claude conduct thorough web research. The prompt instructs Claude to:\n- Start with broad searches to understand the topic landscape\n- Prioritize high-quality, authoritative sources\n- Iteratively refine the research direction based on findings\n- Keep you informed and let you guide the research interactively\n- Always cite sources with URLs\n\n### Resources\n\nWe expose two things as MCP resources: (1) captured webpage screenshots, and (2) the research session.\n\n#### Screenshots\n\nWhen you take a screenshot, it's saved as an MCP resource. You can access captured screenshots in Claude Desktop via the Paperclip icon.\n\n#### Research Session\n\nThe server maintains a research session that includes:\n- Search queries\n- Visited pages\n- Extracted content\n- Screenshots\n- Timestamps\n\n### Suggestions\n\nFor the best results, if you choose not to use the `agentic-research` prompt when doing your research, it may be helpful to suggest high-quality sources for Claude to use when researching general topics. For example, you could prompt `news today from reuters or AP` instead of `news today`.\n\n## Problems\n\nThis is very much pre-alpha code. And it is also AIGC, so expect bugs.\n\nIf you run into issues, it may be helpful to check Claude Desktop's MCP logs:\n\n```bash\ntail -n 20 -f ~/Library/Logs/Claude/mcp*.log\n```\n\n## Development\n\n```bash\n# Install dependencies\npnpm install\n\n# Build the project\npnpm build\n\n# Watch for changes\npnpm watch\n\n# Run in development mode\npnpm dev\n```\n\n## Requirements\n\n- Node.js >= 18\n- Playwright (automatically installed as a dependency)\n\n## Verified Platforms\n\n- [x] macOS\n- [ ] Linux\n\n## License\n\nMIT\n\n## Author\n\n[mzxrai](https://github.com/mzxrai) ",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "webresearch",
        "web",
        "webpage",
        "mcp webresearch",
        "web research",
        "webresearch integrates"
      ],
      "category": "web-search"
    },
    "chuanmingliu--servers": {
      "owner": "chuanmingliu",
      "name": "servers",
      "url": "https://github.com/chuanmingliu/servers",
      "imageUrl": "/freedevtools/mcp/pfp/chuanmingliu.webp",
      "description": "Fetches web content from specified URLs and converts it to markdown format for easier processing by language models. Supports content extraction with options for maximum length and starting index for truncation.",
      "stars": 1,
      "forks": 0,
      "license": "MIT License",
      "language": "",
      "updated_at": "2025-03-01T17:10:31Z",
      "readme_content": "# Model Context Protocol servers\n\nThis repository is a collection of *reference implementations* for the [Model Context Protocol](https://modelcontextprotocol.io/) (MCP), as well as references\nto community built servers and additional resources.\n\nThe servers in this repository showcase the versatility and extensibility of MCP, demonstrating how it can be used to give Large Language Models (LLMs) secure, controlled access to tools and data sources.\nEach MCP server is implemented with either the [Typescript MCP SDK](https://github.com/modelcontextprotocol/typescript-sdk) or [Python MCP SDK](https://github.com/modelcontextprotocol/python-sdk).\n\n> Note: Lists in this README are maintained in alphabetical order to minimize merge conflicts when adding new items.\n\n## 🌟 Reference Servers\n\nThese servers aim to demonstrate MCP features and the TypeScript and Python SDKs.\n\n- **[AWS KB Retrieval](src/aws-kb-retrieval-server)** - Retrieval from AWS Knowledge Base using Bedrock Agent Runtime\n- **[Brave Search](src/brave-search)** - Web and local search using Brave's Search API\n- **[EverArt](src/everart)** - AI image generation using various models\n- **[Everything](src/everything)** - Reference / test server with prompts, resources, and tools\n- **[Fetch](src/fetch)** - Web content fetching and conversion for efficient LLM usage\n- **[Filesystem](src/filesystem)** - Secure file operations with configurable access controls\n- **[Git](src/git)** - Tools to read, search, and manipulate Git repositories\n- **[GitHub](src/github)** - Repository management, file operations, and GitHub API integration\n- **[GitLab](src/gitlab)** - GitLab API, enabling project management\n- **[Google Drive](src/gdrive)** - File access and search capabilities for Google Drive\n- **[Google Maps](src/google-maps)** - Location services, directions, and place details\n- **[Memory](src/memory)** - Knowledge graph-based persistent memory system\n- **[PostgreSQL](src/postgres)** - Read-only database access with schema inspection\n- **[Puppeteer](src/puppeteer)** - Browser automation and web scraping\n- **[Sentry](src/sentry)** - Retrieving and analyzing issues from Sentry.io\n- **[Sequential Thinking](src/sequentialthinking)** - Dynamic and reflective problem-solving through thought sequences\n- **[Slack](src/slack)** - Channel management and messaging capabilities\n- **[Sqlite](src/sqlite)** - Database interaction and business intelligence capabilities\n- **[Time](src/time)** - Time and timezone conversion capabilities\n\n## 🤝 Third-Party Servers\n\n### 🎖️ Official Integrations\n\nOfficial integrations are maintained by companies building production ready MCP servers for their platforms.\n\n- <img height=\"12\" width=\"12\" src=\"https://www.21st.dev/favicon.ico\" alt=\"21st.dev Logo\" /> **[21st.dev Magic](https://github.com/21st-dev/magic-mcp)** - Create crafted UI components inspired by the best 21st.dev design engineers.\n- <img height=\"12\" width=\"12\" src=\"https://apify.com/favicon.ico\" alt=\"Apify Logo\" /> **[Apify](https://github.com/apify/actors-mcp-server)** - [Actors MCP Server](https://apify.com/apify/actors-mcp-server): Use 3,000+ pre-built cloud tools to extract data from websites, e-commerce, social media, search engines, maps, and more\n- <img height=\"12\" width=\"12\" src=\"https://axiom.co/favicon.ico\" alt=\"Axiom Logo\" /> **[Axiom](https://github.com/axiomhq/mcp-server-axiom)** - Query and analyze your Axiom logs, traces, and all other event data in natural language\n- <img height=\"12\" width=\"12\" src=\"https://browserbase.com/favicon.ico\" alt=\"Browserbase Logo\" /> **[Browserbase](https://github.com/browserbase/mcp-server-browserbase)** - Automate browser interactions in the cloud (e.g. web navigation, data extraction, form filling, and more)\n- <img alt=\"cloudflare\" height=\"12\" width=\"12\" src=\"https://cdn.simpleicons.org/cloudflare\" /> **[Cloudflare](https://github.com/cloudflare/mcp-server-cloudflare)** - Deploy, configure & interrogate your resources on the Cloudflare developer platform (e.g. Workers/KV/R2/D1)\n- <img height=\"12\" width=\"12\" src=\"https://e2b.dev/favicon.ico\" alt=\"E2B Logo\" /> **[E2B](https://github.com/e2b-dev/mcp-server)** - Run code in secure sandboxes hosted by [E2B](https://e2b.dev)\n- <img height=\"12\" width=\"12\" src=\"https://esignatures.com/favicon.ico\" alt=\"eSignatures Logo\" /> **[eSignatures](https://github.com/esignaturescom/mcp-server-esignatures)** - Contract and template management for drafting, reviewing, and sending binding contracts.\n- <img height=\"12\" width=\"12\" src=\"https://exa.ai/images/favicon-32x32.png\" alt=\"Exa Logo\" /> **[Exa](https://github.com/exa-labs/exa-mcp-server)** - Search Engine made for AIs by [Exa](https://exa.ai)\n- <img height=\"12\" width=\"12\" src=\"https://firecrawl.dev/favicon.ico\" alt=\"Firecrawl Logo\" /> **[Firecrawl](https://github.com/mendableai/firecrawl-mcp-server)** - Extract web data with [Firecrawl](https://firecrawl.dev)\n- <img height=\"12\" width=\"12\" src=\"https://fireproof.storage/favicon.ico\" alt=\"Fireproof Logo\" /> **[Fireproof](https://github.com/fireproof-storage/mcp-database-server)** - Immutable ledger database with live synchronization\n- <img height=\"12\" width=\"12\" src=\"https://grafana.com/favicon.ico\" alt=\"Grafana Logo\" /> **[Grafana](https://github.com/grafana/mcp-grafana)** - Search dashboards, investigate incidents and query datasources in your Grafana instance\n- **[IBM wxflows](https://github.com/IBM/wxflows/tree/main/examples/mcp/javascript)** - Tool platform by IBM to build, test and deploy tools for any data source\n- <img height=\"12\" width=\"12\" src=\"https://integration.app/favicon.ico\" alt=\"Integration App Icon\" /> **[Integration App](https://github.com/integration-app/mcp-server)** - Interact with any other SaaS applications on behalf of your customers. \n- <img alt=\"jetbrains\" height=\"12\" width=\"12\" src=\"https://cdn.simpleicons.org/jetbrains\" /> **[JetBrains](https://github.com/JetBrains/mcp-jetbrains)** – Work on your code with JetBrains IDEs\n- <img height=\"12\" width=\"12\" src=\"https://kagi.com/favicon.ico\" alt=\"Kagi Logo\" /> **[Kagi Search](https://github.com/kagisearch/kagimcp)** - Search the web using Kagi's search API\n- <img height=\"12\" width=\"12\" src=\"https://lingo.dev/favicon.ico\" alt=\"Lingo.dev Logo\" /> **[Lingo.dev](https://github.com/lingodotdev/lingo.dev/blob/main/mcp.md)** - Make your AI agent speak every language on the planet, using [Lingo.dev](https://lingo.dev) Localization Engine.\n- <img height=\"12\" width=\"12\" src=\"https://www.meilisearch.com/favicon.ico\" alt=\"Meilisearch Logo\" /> **[Meilisearch](https://github.com/meilisearch/meilisearch-mcp)** - Interact & query with Meilisearch (Full-text & semantic search API)\n-  **[Metoro](https://github.com/metoro-io/metoro-mcp-server)** - Query and interact with kubernetes environments monitored by Metoro\n- <img height=\"12\" width=\"12\" src=\"https://www.motherduck.com/favicon.ico\" alt=\"MotherDuck Logo\" /> **[MotherDuck](https://github.com/motherduckdb/mcp-server-motherduck)** - Query and analyze data with MotherDuck and local DuckDB\n- <img height=\"12\" width=\"12\" src=\"https://needle-ai.com/images/needle-logo-orange-2-rounded.png\" alt=\"Needle AI Logo\" /> **[Needle](https://github.com/needle-ai/needle-mcp)** - Production-ready RAG out of the box to search and retrieve data from your own documents.\n- <img height=\"12\" width=\"12\" src=\"https://neo4j.com/favicon.ico\" alt=\"Neo4j Logo\" /> **[Neo4j](https://github.com/neo4j-contrib/mcp-neo4j/)** - Neo4j graph database server (schema + read/write-cypher) and separate graph database backed memory\n- **[Neon](https://github.com/neondatabase/mcp-server-neon)** - Interact with the Neon serverless Postgres platform\n- <img height=\"12\" width=\"12\" src=\"https://oxylabs.io/favicon.ico\" alt=\"Oxylabs Logo\" /> **[Oxylabs](https://github.com/oxylabs/oxylabs-mcp)** - Scrape websites with Oxylabs Web API, supporting dynamic rendering and parsing for structured data extraction.\n- <img alt=\"logomark\" height=\"12\" width=\"12\" src=\"https://qdrant.tech/img/brand-resources-logos/logomark.svg\" /> **[Qdrant](https://github.com/qdrant/mcp-server-qdrant/)** - Implement semantic memory layer on top of the Qdrant vector search engine\n- **[Raygun](https://github.com/MindscapeHQ/mcp-server-raygun)** - Interact with your crash reporting and real using monitoring data on your Raygun account\n- <img height=\"12\" width=\"12\" src=\"https://riza.io/favicon.ico\" alt=\"Riza logo\" /> **[Riza](https://github.com/riza-io/riza-mcp)** - Arbitrary code execution and tool-use platform for LLMs by [Riza](https://riza.io)\n- <img alt=\"56912e614b35093426c515860f9f2234\" height=\"12\" width=\"12\" src=\"https://pics.fatwang2.com/56912e614b35093426c515860f9f2234.svg\" /> [Search1API](https://github.com/fatwang2/search1api-mcp) - One API for Search, Crawling, and Sitemaps\n- <img height=\"12\" width=\"12\" src=\"https://stripe.com/favicon.ico\" alt=\"Stripe Logo\" /> **[Stripe](https://github.com/stripe/agent-toolkit)** - Interact with Stripe API\n- <img height=\"12\" width=\"12\" src=\"https://tavily.com/favicon.ico\" alt=\"Tavily Logo\" /> **[Tavily](https://github.com/tavily-ai/tavily-mcp)** - Search engine for AI agents (search + extract) powered by [Tavily](https://tavily.com/)\n- <img height=\"12\" width=\"12\" src=\"https://www.tinybird.co/favicon.ico\" alt=\"Tinybird Logo\" /> **[Tinybird](https://github.com/tinybirdco/mcp-tinybird)** - Interact with Tinybird serverless ClickHouse platform\n- <img height=\"12\" width=\"12\" src=\"https://verodat.io/assets/favicon-16x16.png\" alt=\"Verodat Logo\" /> **[Verodat](https://github.com/Verodat/verodat-mcp-server)** - Interact with Verodat AI Ready Data platform\n\n### 🌎 Community Servers\n\nA growing set of community-developed and maintained servers demonstrates various applications of MCP across different domains.\n\n> **Note:** Community servers are **untested** and should be used at **your own risk**. They are not affiliated with or endorsed by Anthropic.\n\n- **[AWS S3](https://github.com/aws-samples/sample-mcp-server-s3)** - A sample MCP server for AWS S3 that flexibly fetches objects from S3 such as PDF documents\n- **[AWS](https://github.com/rishikavikondala/mcp-server-aws)** - Perform operations on your AWS resources using an LLM\n- **[Airtable](https://github.com/domdomegg/airtable-mcp-server)** - Read and write access to [Airtable](https://airtable.com/) databases, with schema inspection.\n- **[Airtable](https://github.com/felores/airtable-mcp)** - Airtable Model Context Protocol Server.\n- **[AlphaVantage](https://github.com/calvernaz/alphavantage)** - MCP server for stock market data API [AlphaVantage](https://www.alphavantage.co)\n- **[Anki](https://github.com/scorzeth/anki-mcp-server)** - An MCP server for interacting with your [Anki](https://apps.ankiweb.net) decks and cards.\n- **[Any Chat Completions](https://github.com/pyroprompts/any-chat-completions-mcp)** - Interact with any OpenAI SDK Compatible Chat Completions API like OpenAI, Perplexity, Groq, xAI and many more.\n- **[ArangoDB](https://github.com/ravenwits/mcp-server-arangodb)** - MCP Server that provides database interaction capabilities through [ArangoDB](https://arangodb.com/).\n- **[Atlassian](https://github.com/sooperset/mcp-atlassian)** - Interact with Atlassian Cloud products (Confluence and Jira) including searching/reading Confluence spaces/pages, accessing Jira issues, and project metadata.\n- **[Base Free USDC Transfer](https://github.com/magnetai/mcp-free-usdc-transfer)** - Send USDC on [Base](https://base.org) for free using Claude AI! Built with [Coinbase CDP](https://docs.cdp.coinbase.com/mpc-wallet/docs/welcome).\n- **[BigQuery](https://github.com/LucasHild/mcp-server-bigquery)** (by LucasHild) - This server enables LLMs to inspect database schemas and execute queries on BigQuery.\n- **[BigQuery](https://github.com/ergut/mcp-bigquery-server)** (by ergut) - Server implementation for Google BigQuery integration that enables direct BigQuery database access and querying capabilities\n- **[Calendar](https://github.com/GongRzhe/Calendar-MCP-Server)** - Google Calendar integration server enabling AI assistants to manage calendar events through natural language interactions.\n- **[CFBD API](https://github.com/lenwood/cfbd-mcp-server)** - An MCP server for the [College Football Data API](https://collegefootballdata.com/).\n- **[ChatMCP](https://github.com/AI-QL/chat-mcp)** – An Open Source Cross-platform GUI Desktop application compatible with Linux, macOS, and Windows, enabling seamless interaction with MCP servers across dynamically selectable LLMs, by **[AIQL](https://github.com/AI-QL)**\n- **[ChatSum](https://github.com/mcpso/mcp-server-chatsum)** - Query and Summarize chat messages with LLM. by [mcpso](https://mcp.so)\n- **[Chroma](https://github.com/privetin/chroma)** - Vector database server for semantic document search and metadata filtering, built on Chroma\n- **[ClaudePost](https://github.com/ZilongXue/claude-post)** - ClaudePost enables seamless email management for Gmail, offering secure features like email search, reading, and sending.\n- **[Cloudinary](https://github.com/felores/cloudinary-mcp-server)** - Cloudinary Model Context Protocol Server to upload media to Cloudinary and get back the media link and details.\n- **[code-executor](https://github.com/bazinga012/mcp_code_executor)** - An MCP server that allows LLMs to execute Python code within a specified Conda environment.\n- **[code-sandbox-mcp](https://github.com/Automata-Labs-team/code-sandbox-mcp)** - An MCP server to create secure code sandbox environment for executing code within Docker containers.\n- **[cognee-mcp](https://github.com/topoteretes/cognee/tree/main/cognee-mcp)** - GraphRAG memory server with customizable ingestion, data processing and search\n- **[coin_api_mcp](https://github.com/longmans/coin_api_mcp)** - Provides access to [coinmarketcap](https://coinmarketcap.com/) cryptocurrency data.\n- **[Contentful-mcp](https://github.com/ivo-toby/contentful-mcp)** - Read, update, delete, publish content in your [Contentful](https://contentful.com) space(s) from this MCP Server.\n- **[Data Exploration](https://github.com/reading-plus-ai/mcp-server-data-exploration)** - MCP server for autonomous data exploration on .csv-based datasets, providing intelligent insights with minimal effort. NOTE: Will execute arbitrary Python code on your machine, please use with caution!\n- **[Dataset Viewer](https://github.com/privetin/dataset-viewer)** - Browse and analyze Hugging Face datasets with features like search, filtering, statistics, and data export\n- **[DeepSeek MCP Server](https://github.com/DMontgomery40/deepseek-mcp-server)** - Model Context Protocol server integrating DeepSeek's advanced language models, in addition to [other useful API endpoints](https://github.com/DMontgomery40/deepseek-mcp-server?tab=readme-ov-file#features)\n- **[Deepseek_R1](https://github.com/66julienmartin/MCP-server-Deepseek_R1)** - A Model Context Protocol (MCP) server implementation connecting Claude Desktop with DeepSeek's language models (R1/V3)\n- **[Descope](https://github.com/descope-sample-apps/descope-mcp-server)** - An MCP server to integrate with [Descope](https://descope.com) to search audit logs, manage users, and more.\n- **[DevRev](https://github.com/kpsunil97/devrev-mcp-server)** - An MCP server to integrate with DevRev APIs to search through your DevRev Knowledge Graph where objects can be imported from diff. sources listed [here](https://devrev.ai/docs/import#available-sources).\n- **[Dify](https://github.com/YanxingLiu/dify-mcp-server)** - A simple implementation of an MCP server for dify workflows.\n- **[Discord](https://github.com/v-3/discordmcp)** - A MCP server to connect to Discord guilds through a bot and read and write messages in channels\n- **[Docker](https://github.com/ckreiling/mcp-server-docker)** - Integrate with Docker to manage containers, images, volumes, and networks.\n- **[Drupal](https://github.com/Omedia/mcp-server-drupal)** - Server for interacting with [Drupal](https://www.drupal.org/project/mcp) using STDIO transport layer.\n- **[Elasticsearch](https://github.com/cr7258/elasticsearch-mcp-server)** - MCP server implementation that provides Elasticsearch interaction.\n- **[ElevenLabs](https://github.com/mamertofabian/elevenlabs-mcp-server)** - A server that integrates with ElevenLabs text-to-speech API capable of generating full voiceovers with multiple voices.\n- **[Eunomia](https://github.com/whataboutyou-ai/eunomia-MCP-server)** - Extension of the Eunomia framework that connects Eunomia instruments with MCP servers\n- **[Everything Search](https://github.com/mamertofabian/mcp-everything-search)** - Fast file searching capabilities across Windows (using [Everything SDK](https://www.voidtools.com/support/everything/sdk/)), macOS (using mdfind command), and Linux (using locate/plocate command).\n- **[Fetch](https://github.com/zcaceres/fetch-mcp)** - A server that flexibly fetches HTML, JSON, Markdown, or plaintext.\n- **[FireCrawl](https://github.com/vrknetha/mcp-server-firecrawl)** - Advanced web scraping with JavaScript rendering, PDF support, and smart rate limiting\n- **[FlightRadar24](https://github.com/sunsetcoder/flightradar24-mcp-server)** - A Claude Desktop MCP server that helps you track flights in real-time using Flightradar24 data.\n- **[Glean](https://github.com/longyi1207/glean-mcp-server)** - A server that uses Glean API to search and chat.\n- **[Gmail](https://github.com/GongRzhe/Gmail-MCP-Server)** - A Model Context Protocol (MCP) server for Gmail integration in Claude Desktop with auto authentication support.\n- **[Goal Story](https://github.com/hichana/goalstory-mcp)** - a Goal Tracker and Visualization Tool for personal and professional development.\n- **[Golang Filesystem Server](https://github.com/mark3labs/mcp-filesystem-server)** - Secure file operations with configurable access controls built with Go!\n- **[Google Calendar](https://github.com/v-3/google-calendar)** - Integration with Google Calendar to check schedules, find time, and add/delete events\n- **[Google Calendar](https://github.com/nspady/google-calendar-mcp)** - Google Calendar MCP Server for managing Google calendar events. Also supports searching for events by attributes like title and location.\n- **[Google Custom Search](https://github.com/adenot/mcp-google-search)** - Provides Google Search results via the Google Custom Search API\n- **[Google Tasks](https://github.com/zcaceres/gtasks-mcp)** - Google Tasks API Model Context Protocol Server.\n- **[Holaspirit](https://github.com/syucream/holaspirit-mcp-server)** - Interact with [Holaspirit](https://www.holaspirit.com/).\n- **[Home Assistant](https://github.com/tevonsb/homeassistant-mcp)** - Interact with [Home Assistant](https://www.home-assistant.io/) including viewing and controlling lights, switches, sensors, and all other Home Assistant entities.\n- **[HubSpot](https://github.com/buryhuang/mcp-hubspot)** - HubSpot CRM integration for managing contacts and companies. Create and retrieve CRM data directly through Claude chat.\n- **[HuggingFace Spaces](https://github.com/evalstate/mcp-hfspace)** - Server for using HuggingFace Spaces, supporting Open Source Image, Audio, Text Models and more. Claude Desktop mode for easy integration.\n- **[Inoyu](https://github.com/sergehuber/inoyu-mcp-unomi-server)** - Interact with an Apache Unomi CDP customer data platform to retrieve and update customer profiles\n- **[iTerm MCP](https://github.com/ferrislucas/iterm-mcp)** - Integration with iTerm2 terminal emulator for macOS, enabling LLMs to execute and monitor terminal commands.\n- **[JavaFX](https://github.com/mcpso/mcp-server-javafx)** - Make drawings using a JavaFX canvas\n- **[JDBC](https://github.com/quarkiverse/quarkus-mcp-servers/tree/main/jdbc)** - Connect to any JDBC-compatible database and query, insert, update, delete, and more. Supports MySQL, PostgreSQL, Oracle, SQL Server, sqllite and [more](https://github.com/quarkiverse/quarkus-mcp-servers/tree/main/jdbc#supported-jdbc-variants).\n- **[JSON](https://github.com/GongRzhe/JSON-MCP-Server)** - JSON handling and processing server with advanced query capabilities using JSONPath syntax and support for array, string, numeric, and date operations.\n- **[Keycloak MCP](https://github.com/ChristophEnglisch/keycloak-model-context-protocol)** - This MCP server enables natural language interaction with Keycloak for user and realm management including creating, deleting, and listing users and realms.\n- **[Kibela](https://github.com/kiwamizamurai/mcp-kibela-server)** (by kiwamizamurai) - Interact with Kibela API.\n- **[kintone](https://github.com/macrat/mcp-server-kintone)** - Manage records and apps in [kintone](https://kintone.com) through LLM tools.\n- **[Kubernetes](https://github.com/Flux159/mcp-server-kubernetes)** - Connect to Kubernetes cluster and manage pods, deployments, and services.\n- **[Lightdash](https://github.com/syucream/lightdash-mcp-server)** - Interact with [Lightdash](https://www.lightdash.com/), a BI tool.\n- **[Linear](https://github.com/jerhadf/linear-mcp-server)** - Allows LLM to interact with Linear's API for project management, including searching, creating, and updating issues.\n- **[LlamaCloud](https://github.com/run-llama/mcp-server-llamacloud)** (by marcusschiesser) - Integrate the data stored in a managed index on [LlamaCloud](https://cloud.llamaindex.ai/)\n- **[llm-context](https://github.com/cyberchitta/llm-context.py)** - Provides a repo-packing MCP tool with configurable profiles that specify file inclusion/exclusion patterns and optional prompts.\n- **[MCP Compass](https://github.com/liuyoshio/mcp-compass)** - Suggest the right MCP server for your needs\n- **[MCP Installer](https://github.com/anaisbetts/mcp-installer)** - This server is a server that installs other MCP servers for you.\n- **[mcp-k8s-go](https://github.com/strowk/mcp-k8s-go)** - Golang-based Kubernetes server for MCP to browse pods and their logs, events, namespaces and more. Built to be extensible.\n- **[mcp-proxy](https://github.com/sparfenyuk/mcp-proxy)** - Connect to MCP servers that run on SSE transport, or expose stdio servers as an SSE server.\n- **[MSSQL](https://github.com/aekanun2020/mcp-server/)** - MSSQL database integration with configurable access controls and schema inspection\n- **[MSSQL](https://github.com/JexinSam/mssql_mcp_server)** (by jexin) - MCP Server for MSSQL database in Python\n- **[MSSQL-Python](https://github.com/amornpan/py-mcp-mssql)** (by amornpan) - A read-only Python implementation for MSSQL database access with enhanced security features, configurable access controls, and schema inspection capabilities. Focuses on safe database interaction through Python ecosystem.\n- **[Markdownify](https://github.com/zcaceres/mcp-markdownify-server)** - MCP to convert almost anything to Markdown (PPTX, HTML, PDF, Youtube Transcripts and more)\n- **[Minima](https://github.com/dmayboroda/minima)** - MCP server for RAG on local files\n- **[MongoDB](https://github.com/kiliczsh/mcp-mongo-server)** - A Model Context Protocol Server for MongoDB.\n- **[Monday.com](https://github.com/sakce/mcp-server-monday)** - MCP Server to interact with Monday.com boards and items.\n- **[MySQL](https://github.com/benborla/mcp-server-mysql)** (by benborla) - MySQL database integration in NodeJS with configurable access controls and schema inspection\n- **[MySQL](https://github.com/designcomputer/mysql_mcp_server)** (by DesignComputer) - MySQL database integration in Python with configurable access controls and schema inspection\n- **[NS Travel Information](https://github.com/r-huijts/ns-mcp-server)** - Access Dutch Railways (NS) real-time train travel information and disruptions through the official NS API.\n- **[Neo4j](https://github.com/da-okazaki/mcp-neo4j-server)** - A community built server that interacts with Neo4j Graph Database.\n- **[Neovim](https://github.com/bigcodegen/mcp-neovim-server)** - An MCP Server for your Neovim session.\n- **[Notion](https://github.com/suekou/mcp-notion-server)** (by suekou) - Interact with Notion API.\n- **[Notion](https://github.com/v-3/notion-server)** (by v-3) - Notion MCP integration. Search, Read, Update, and Create pages through Claude chat.\n- **[oatpp-mcp](https://github.com/oatpp/oatpp-mcp)** - C++ MCP integration for Oat++. Use [Oat++](https://oatpp.io) to build MCP servers.\n- **[Obsidian Markdown Notes](https://github.com/calclavia/mcp-obsidian)** - Read and search through your Obsidian vault or any directory containing Markdown notes\n- **[obsidian-mcp](https://github.com/StevenStavrakis/obsidian-mcp)** - (by Steven Stavrakis) An MCP server for Obsidian.md with tools for searching, reading, writing, and organizing notes.\n- **[OpenAPI](https://github.com/snaggle-ai/openapi-mcp-server)** - Interact with [OpenAPI](https://www.openapis.org/) APIs.\n- **[OpenCTI](https://github.com/Spathodea-Network/opencti-mcp)** - Interact with OpenCTI platform to retrieve threat intelligence data including reports, indicators, malware and threat actors.\n- **[OpenRPC](https://github.com/shanejonas/openrpc-mpc-server)** - Interact with and discover JSON-RPC APIs via [OpenRPC](https://open-rpc.org).\n- **[Open Strategy Partners Marketing Tools](https://github.com/open-strategy-partners/osp_marketing_tools)** - Content editing codes, value map, and positioning tools for product marketing.\n- **[Pandoc](https://github.com/vivekVells/mcp-pandoc)** - MCP server for seamless document format conversion using Pandoc, supporting Markdown, HTML, PDF, DOCX (.docx), csv and more.\n- **[PIF](https://github.com/hungryrobot1/MCP-PIF)** - A Personal Intelligence Framework (PIF), providing tools for file operations, structured reasoning, and journal-based documentation to support continuity and evolving human-AI collaboration across sessions.\n- **[Pinecone](https://github.com/sirmews/mcp-pinecone)** - MCP server for searching and uploading records to Pinecone. Allows for simple RAG features, leveraging Pinecone's Inference API.\n- **[Placid.app](https://github.com/felores/placid-mcp-server)** - Generate image and video creatives using Placid.app templates\n- **[Playwright](https://github.com/executeautomation/mcp-playwright)** - This MCP Server will help you run browser automation and webscraping using Playwright\n- **[Postman](https://github.com/shannonlal/mcp-postman)** - MCP server for running Postman Collections locally via Newman. Allows for simple execution of Postman Server and returns the results of whether the collection passed all the tests.\n- **[Qwen_Max](https://github.com/66julienmartin/MCP-server-Qwen_Max)** - A Model Context Protocol (MCP) server implementation for the Qwen models.\n- **[RabbitMQ](https://github.com/kenliao94/mcp-server-rabbitmq)** - The MCP server that interacts with RabbitMQ to publish and consume messages.\n- **[RAG Web Browser](https://github.com/apify/mcp-server-rag-web-browser)** An MCP server for Apify's open-source RAG Web Browser [Actor](https://apify.com/apify/rag-web-browser) to perform web searches, scrape URLs, and return content in Markdown.\n- **[Reaper](https://github.com/dschuler36/reaper-mcp-server)** - Interact with your [Reaper](https://www.reaper.fm/) (Digital Audio Workstation) projects.\n- **[Redis](https://github.com/GongRzhe/REDIS-MCP-Server)** - Redis database operations and caching microservice server with support for key-value operations, expiration management, and pattern-based key listing.\n- **[Redis](https://github.com/prajwalnayak7/mcp-server-redis)** MCP server to interact with Redis Server, AWS Memory DB, etc for caching or other use-cases where in-memory and key-value based storage is appropriate\n- **[Rememberizer AI](https://github.com/skydeckai/mcp-server-rememberizer)** - An MCP server designed for interacting with the Rememberizer data source, facilitating enhanced knowledge retrieval.\n- **[Replicate](https://github.com/deepfates/mcp-replicate)** - Search, run and manage machine learning models on Replicate through a simple tool-based interface. Browse models, create predictions, track their status, and handle generated images.\n- **[Rijksmuseum](https://github.com/r-huijts/rijksmuseum-mcp)** - Interface with the Rijksmuseum API to search artworks, retrieve artwork details, access image tiles, and explore user collections.\n- **[Salesforce MCP](https://github.com/smn2gnt/MCP-Salesforce)** - Interact with Salesforce Data and Metadata\n- **[Scholarly](https://github.com/adityak74/mcp-scholarly)** - A MCP server to search for scholarly and academic articles.\n- **[SearXNG](https://github.com/ihor-sokoliuk/mcp-searxng)** - A Model Context Protocol Server for [SearXNG](https://docs.searxng.org)\n- **[Snowflake](https://github.com/isaacwasserman/mcp-snowflake-server)** - This MCP server enables LLMs to interact with Snowflake databases, allowing for secure and controlled data operations.\n- **[Spotify](https://github.com/varunneal/spotify-mcp)** - This MCP allows an LLM to play and use Spotify.\n- **[Stripe](https://github.com/atharvagupta2003/mcp-stripe)** - This MCP allows integration with Stripe for handling payments, customers, and refunds.\n- **[TMDB](https://github.com/Laksh-star/mcp-server-tmdb)** - This MCP server integrates with The Movie Database (TMDB) API to provide movie information, search capabilities, and recommendations.\n- **[Tavily search](https://github.com/RamXX/mcp-tavily)** - An MCP server for Tavily's search & news API, with explicit site inclusions/exclusions\n- **[Ticketmaster](https://github.com/delorenj/mcp-server-ticketmaster)** - Search for events, venues, and attractions through the Ticketmaster Discovery API\n- **[Todoist](https://github.com/abhiz123/todoist-mcp-server)** - Interact with Todoist to manage your tasks.\n- **[Travel Planner](https://github.com/GongRzhe/TRAVEL-PLANNER-MCP-Server)** - Travel planning and itinerary management server integrating with Google Maps API for location search, place details, and route calculations.\n- **[Vega-Lite](https://github.com/isaacwasserman/mcp-vegalite-server)** - Generate visualizations from fetched data using the VegaLite format and renderer.\n- **[Video Editor](https://github.com/burningion/video-editing-mcp)** - A Model Context Protocol Server to add, edit, and search videos with [Video Jungle](https://www.video-jungle.com/).\n- **[WildFly MCP](https://github.com/wildfly-extras/wildfly-mcp)** - WildFly MCP server that enables LLM to interact with running WildFly servers (retrieve metrics, logs, invoke operations, ...).\n- **[Windows CLI](https://github.com/SimonB97/win-cli-mcp-server)** - MCP server for secure command-line interactions on Windows systems, enabling controlled access to PowerShell, CMD, and Git Bash shells.\n- **[World Bank data API](https://github.com/anshumax/world_bank_mcp_server)** - A server that fetches data indicators available with the World Bank as part of their data API\n- **[X (Twitter)](https://github.com/EnesCinr/twitter-mcp)** (by EnesCinr) - Interact with twitter API. Post tweets and search for tweets by query.\n- **[X (Twitter)](https://github.com/vidhupv/x-mcp)** (by vidhupv) - Create, manage and publish X/Twitter posts directly through Claude chat.\n- **[XMind](https://github.com/apeyroux/mcp-xmind)** - Read and search through your XMind directory containing XMind files.\n- **[YouTube](https://github.com/ZubeidHendricks/youtube-mcp-server)** - Comprehensive YouTube API integration for video management, Shorts creation, and analytics.\n\n## 📚 Frameworks\n\nThese are high-level frameworks that make it easier to build MCP servers or clients.\n\n### For servers\n\n* **[EasyMCP](https://github.com/zcaceres/easy-mcp/)** (TypeScript)\n* **[FastMCP](https://github.com/punkpeye/fastmcp)** (TypeScript)\n* **[Foxy Contexts](https://github.com/strowk/foxy-contexts)** – A library to build MCP servers in Golang by **[strowk](https://github.com/strowk)**\n* **[Quarkus MCP Server SDK](https://github.com/quarkiverse/quarkus-mcp-server)** (Java)\n\n### For clients\n\n* **[codemirror-mcp](https://github.com/marimo-team/codemirror-mcp)** - CodeMirror extension that implements the Model Context Protocol (MCP) for resource mentions and prompt commands\n\n## 📚 Resources\n\nAdditional resources on MCP.\n\n- **[AiMCP](https://www.aimcp.info)** - A collection of MCP clients&servers to find the right mcp tools by **[Hekmon](https://github.com/hekmon8)**\n- **[Awesome Crypto MCP Servers by badkk](https://github.com/badkk/awesome-crypto-mcp-servers)** - A curated list of MCP servers by **[Luke Fan](https://github.com/badkk)**\n- **[Awesome MCP Servers by appcypher](https://github.com/appcypher/awesome-mcp-servers)** - A curated list of MCP servers by **[Stephen Akinyemi](https://github.com/appcypher)**\n- **[Awesome MCP Servers by punkpeye](https://github.com/punkpeye/awesome-mcp-servers)** (**[website](https://glama.ai/mcp/servers)**) - A curated list of MCP servers by **[Frank Fiegel](https://github.com/punkpeye)**\n- **[Awesome MCP Servers by wong2](https://github.com/wong2/awesome-mcp-servers)** (**[website](https://mcpservers.org)**) - A curated list of MCP servers by **[wong2](https://github.com/wong2)**\n- **[Discord Server](https://glama.ai/mcp/discord)** – A community discord server dedicated to MCP by **[Frank Fiegel](https://github.com/punkpeye)**\n- **[MCP Badges](https://github.com/mcpx-dev/mcp-badges)** – Quickly highlight your MCP project with clear, eye-catching badges, by **[Ironben](https://github.com/nanbingxyz)**\n- **[MCP Servers Hub](https://github.com/apappascs/mcp-servers-hub)** (**[website](https://mcp-servers-hub-website.pages.dev/)**) - A curated list of MCP servers by **[apappascs](https://github.com/apappascs)**\n- **[MCP X Community](https://x.com/i/communities/1861891349609603310)** – A X community for MCP by **[Xiaoyi](https://x.com/chxy)**\n- **[mcp-cli](https://github.com/wong2/mcp-cli)** - A CLI inspector for the Model Context Protocol by **[wong2](https://github.com/wong2)**\n- **[mcp-get](https://mcp-get.com)** - Command line tool for installing and managing MCP servers by **[Michael Latman](https://github.com/michaellatman)**\n- **[mcp-manager](https://github.com/zueai/mcp-manager)** - Simple Web UI to install and manage MCP servers for Claude Desktop by **[Zue](https://github.com/zueai)**\n- **[MCPHub](https://github.com/Jeamee/MCPHub-Desktop)** – An Open Source MacOS & Windows GUI Desktop app for discovering, installing and managing MCP servers by **[Jeamee](https://github.com/jeamee)**\n- **[mcp.run](https://mcp.run)** - A hosted registry and control plane to install & run secure + portable MCP Servers.\n- **[Open-Sourced MCP Servers Directory](https://github.com/chatmcp/mcp-directory)** - A curated list of MCP servers by **[mcpso](https://mcp.so)**\n- <img height=\"12\" width=\"12\" src=\"https://opentools.com/favicon.ico\" alt=\"OpenTools Logo\" /> **[OpenTools](https://opentools.com)** - An open registry for finding, installing, and building with MCP servers by **[opentoolsteam](https://github.com/opentoolsteam)**\n- **[PulseMCP](https://www.pulsemcp.com)** ([API](https://www.pulsemcp.com/api)) - Community hub & weekly newsletter for discovering MCP servers, clients, articles, and news by **[Tadas Antanavicius](https://github.com/tadasant)**, **[Mike Coughlin](https://github.com/macoughl)**, and **[Ravina Patel](https://github.com/ravinahp)**\n- **[r/mcp](https://www.reddit.com/r/mcp)** – A Reddit community dedicated to MCP by **[Frank Fiegel](https://github.com/punkpeye)**\n- **[Smithery](https://smithery.ai/)** - A registry of MCP servers to find the right tools for your LLM agents by **[Henry Mao](https://github.com/calclavia)**\n- **[Toolbase](https://gettoolbase.ai)** - Desktop application that manages tools and MCP servers with just a few clicks - no coding required by **[gching](https://github.com/gching)**\n\n## 🚀 Getting Started\n\n### Using MCP Servers in this Repository\nTypescript-based servers in this repository can be used directly with `npx`.\n\nFor example, this will start the [Memory](src/memory) server:\n```sh\nnpx -y @modelcontextprotocol/server-memory\n```\n\nPython-based servers in this repository can be used directly with [`uvx`](https://docs.astral.sh/uv/concepts/tools/) or [`pip`](https://pypi.org/project/pip/). `uvx` is recommended for ease of use and setup.\n\nFor example, this will start the [Git](src/git) server:\n```sh\n# With uvx\nuvx mcp-server-git\n\n# With pip\npip install mcp-server-git\npython -m mcp_server_git\n```\n\nFollow [these](https://docs.astral.sh/uv/getting-started/installation/) instructions to install `uv` / `uvx` and [these](https://pip.pypa.io/en/stable/installation/) to install `pip`.\n\n### Using an MCP Client\nHowever, running a server on its own isn't very useful, and should instead be configured into an MCP client. For example, here's the Claude Desktop configuration to use the above server:\n\n```json\n{\n  \"mcpServers\": {\n    \"memory\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@modelcontextprotocol/server-memory\"]\n    }\n  }\n}\n```\n\nAdditional examples of using the Claude Desktop as an MCP client might look like:\n\n```json\n{\n  \"mcpServers\": {\n    \"filesystem\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@modelcontextprotocol/server-filesystem\", \"/path/to/allowed/files\"]\n    },\n    \"git\": {\n      \"command\": \"uvx\",\n      \"args\": [\"mcp-server-git\", \"--repository\", \"path/to/git/repo\"]\n    },\n    \"github\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@modelcontextprotocol/server-github\"],\n      \"env\": {\n        \"GITHUB_PERSONAL_ACCESS_TOKEN\": \"<YOUR_TOKEN>\"\n      }\n    },\n    \"postgres\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@modelcontextprotocol/server-postgres\", \"postgresql://localhost/mydb\"]\n    }\n  }\n}\n```\n\n## 🛠️ Creating Your Own Server\n\nInterested in creating your own MCP server? Visit the official documentation at [modelcontextprotocol.io](https://modelcontextprotocol.io/introduction) for comprehensive guides, best practices, and technical details on implementing MCP servers.\n\n## 🤝 Contributing\n\nSee [CONTRIBUTING.md](CONTRIBUTING.md) for information about contributing to this repository.\n\n## 🔒 Security\n\nSee [SECURITY.md](SECURITY.md) for reporting security vulnerabilities.\n\n## 📜 License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n\n## 💬 Community\n\n- [GitHub Discussions](https://github.com/orgs/modelcontextprotocol/discussions)\n\n## ⭐ Support\n\nIf you find MCP servers useful, please consider starring the repository and contributing new servers or improvements!\n\n---\n\nManaged by Anthropic, but built together with the community. The Model Context Protocol is open source and we encourage everyone to contribute their own servers and improvements!",
      "npm_url": "https://www.npmjs.com/package/servers",
      "npm_downloads": 2441,
      "keywords": [
        "chuanmingliu",
        "web",
        "search",
        "search chuanmingliu",
        "web search",
        "chuanmingliu servers"
      ],
      "category": "web-search"
    },
    "cline--cline": {
      "owner": "cline",
      "name": "cline",
      "url": "https://github.com/cline/cline",
      "imageUrl": "/freedevtools/mcp/pfp/cline.webp",
      "description": "Enable autonomous coding assistance within an IDE, facilitating file creation and editing, running terminal commands, and interactive debugging through a browser, all while managing user permissions. Extend functionality via custom tools using the Model Context Protocol (MCP) to automate and streamline software development tasks.",
      "stars": 50998,
      "forks": 7276,
      "license": "Apache License 2.0",
      "language": "TypeScript",
      "updated_at": "2025-10-03T07:45:46Z",
      "readme_content": "<div align=\"center\"><sub>\nEnglish | <a href=\"https://github.com/cline/cline/blob/main/locales/es/README.md\" target=\"_blank\">Español</a> | <a href=\"https://github.com/cline/cline/blob/main/locales/de/README.md\" target=\"_blank\">Deutsch</a> | <a href=\"https://github.com/cline/cline/blob/main/locales/ja/README.md\" target=\"_blank\">日本語</a> | <a href=\"https://github.com/cline/cline/blob/main/locales/zh-cn/README.md\" target=\"_blank\">简体中文</a> | <a href=\"https://github.com/cline/cline/blob/main/locales/zh-tw/README.md\" target=\"_blank\">繁體中文</a> | <a href=\"https://github.com/cline/cline/blob/main/locales/ko/README.md\" target=\"_blank\">한국어</a>\n</sub></div>\n\n# Cline – \\#1 on OpenRouter\n\n<p align=\"center\">\n  <img alt=\"demo\" src=\"https://media.githubusercontent.com/media/cline/cline/main/assets/docs/demo.gif\" width=\"100%\" />\n</p>\n\n<div align=\"center\">\n<table>\n<tbody>\n<td align=\"center\">\n<a href=\"https://marketplace.visualstudio.com/items?itemName=saoudrizwan.claude-dev\" target=\"_blank\"><strong>Download on VS Marketplace</strong></a>\n</td>\n<td align=\"center\">\n<a href=\"https://discord.gg/cline\" target=\"_blank\"><strong>Discord</strong></a>\n</td>\n<td align=\"center\">\n<a href=\"https://www.reddit.com/r/cline/\" target=\"_blank\"><strong>r/cline</strong></a>\n</td>\n<td align=\"center\">\n<a href=\"https://github.com/cline/cline/discussions/categories/feature-requests?discussions_q=is%3Aopen+category%3A%22Feature+Requests%22+sort%3Atop\" target=\"_blank\"><strong>Feature Requests</strong></a>\n</td>\n<td align=\"center\">\n<a href=\"https://docs.cline.bot/getting-started/for-new-coders\" target=\"_blank\"><strong>Getting Started</strong></a>\n</td>\n</tbody>\n</table>\n</div>\n\nMeet Cline, an AI assistant that can use your **CLI** a**N**d **E**ditor.\n\nThanks to [Claude Sonnet's agentic coding capabilities](https://www.anthropic.com/claude/sonnet), Cline can handle complex software development tasks step-by-step. With tools that let him create & edit files, explore large projects, use the browser, and execute terminal commands (after you grant permission), he can assist you in ways that go beyond code completion or tech support. Cline can even use the Model Context Protocol (MCP) to create new tools and extend his own capabilities. While autonomous AI scripts traditionally run in sandboxed environments, this extension provides a human-in-the-loop GUI to approve every file change and terminal command, providing a safe and accessible way to explore the potential of agentic AI.\n\n1. Enter your task and add images to convert mockups into functional apps or fix bugs with screenshots.\n2. Cline starts by analyzing your file structure & source code ASTs, running regex searches, and reading relevant files to get up to speed in existing projects. By carefully managing what information is added to context, Cline can provide valuable assistance even for large, complex projects without overwhelming the context window.\n3. Once Cline has the information he needs, he can:\n    - Create and edit files + monitor linter/compiler errors along the way, letting him proactively fix issues like missing imports and syntax errors on his own.\n    - Execute commands directly in your terminal and monitor their output as he works, letting him e.g., react to dev server issues after editing a file.\n    - For web development tasks, Cline can launch the site in a headless browser, click, type, scroll, and capture screenshots + console logs, allowing him to fix runtime errors and visual bugs.\n4. When a task is completed, Cline will present the result to you with a terminal command like `open -a \"Google Chrome\" index.html`, which you run with a click of a button.\n\n> [!TIP]\n> Use the `CMD/CTRL + Shift + P` shortcut to open the command palette and type \"Cline: Open In New Tab\" to open the extension as a tab in your editor. This lets you use Cline side-by-side with your file explorer, and see how he changes your workspace more clearly.\n\n---\n\n<img alt=\"3cf21e04_7ce9_4d22_a7b9_ba2c595e88a4\" align=\"right\" width=\"340\" src=\"https://github.com/user-attachments/assets/3cf21e04-7ce9-4d22-a7b9-ba2c595e88a4\">\n\n### Use any API and Model\n\nCline supports API providers like OpenRouter, Anthropic, OpenAI, Google Gemini, AWS Bedrock, Azure, GCP Vertex, Cerebras and Groq. You can also configure any OpenAI compatible API, or use a local model through LM Studio/Ollama. If you're using OpenRouter, the extension fetches their latest model list, allowing you to use the newest models as soon as they're available.\n\nThe extension also keeps track of total tokens and API usage cost for the entire task loop and individual requests, keeping you informed of spend every step of the way.\n\n<!-- Transparent pixel to create line break after floating image -->\n\n<img alt=\"ee14e6f7_20b8_4391_9091_8e8e25561929\" width=\"2000\" height=\"0\" src=\"https://github.com/user-attachments/assets/ee14e6f7-20b8-4391-9091-8e8e25561929\"><br>\n\n<img alt=\"81be79a8_1fdb_4028_9129_5fe055e01e76\" align=\"left\" width=\"370\" src=\"https://github.com/user-attachments/assets/81be79a8-1fdb-4028-9129-5fe055e01e76\">\n\n### Run Commands in Terminal\n\nThanks to the new [shell integration updates in VSCode v1.93](https://code.visualstudio.com/updates/v1_93#_terminal-shell-integration-api), Cline can execute commands directly in your terminal and receive the output. This allows him to perform a wide range of tasks, from installing packages and running build scripts to deploying applications, managing databases, and executing tests, all while adapting to your dev environment & toolchain to get the job done right.\n\nFor long running processes like dev servers, use the \"Proceed While Running\" button to let Cline continue in the task while the command runs in the background. As Cline works he’ll be notified of any new terminal output along the way, letting him react to issues that may come up, such as compile-time errors when editing files.\n\n<!-- Transparent pixel to create line break after floating image -->\n\n<img alt=\"ee14e6f7_20b8_4391_9091_8e8e25561929\" width=\"2000\" height=\"0\" src=\"https://github.com/user-attachments/assets/ee14e6f7-20b8-4391-9091-8e8e25561929\"><br>\n\n<img alt=\"c5977833_d9b8_491e_90f9_05f9cd38c588\" align=\"right\" width=\"400\" src=\"https://github.com/user-attachments/assets/c5977833-d9b8-491e-90f9-05f9cd38c588\">\n\n### Create and Edit Files\n\nCline can create and edit files directly in your editor, presenting you a diff view of the changes. You can edit or revert Cline's changes directly in the diff view editor, or provide feedback in chat until you're satisfied with the result. Cline also monitors linter/compiler errors (missing imports, syntax errors, etc.) so he can fix issues that come up along the way on his own.\n\nAll changes made by Cline are recorded in your file's Timeline, providing an easy way to track and revert modifications if needed.\n\n<!-- Transparent pixel to create line break after floating image -->\n\n<img alt=\"ee14e6f7_20b8_4391_9091_8e8e25561929\" width=\"2000\" height=\"0\" src=\"https://github.com/user-attachments/assets/ee14e6f7-20b8-4391-9091-8e8e25561929\"><br>\n\n<img alt=\"bc2e85ba_dfeb_4fe6_9942_7cfc4703cbe5\" align=\"left\" width=\"370\" src=\"https://github.com/user-attachments/assets/bc2e85ba-dfeb-4fe6-9942-7cfc4703cbe5\">\n\n### Use the Browser\n\nWith Claude Sonnet's new [Computer Use](https://www.anthropic.com/news/3-5-models-and-computer-use) capability, Cline can launch a browser, click elements, type text, and scroll, capturing screenshots and console logs at each step. This allows for interactive debugging, end-to-end testing, and even general web use! This gives him autonomy to fixing visual bugs and runtime issues without you needing to handhold and copy-pasting error logs yourself.\n\nTry asking Cline to \"test the app\", and watch as he runs a command like `npm run dev`, launches your locally running dev server in a browser, and performs a series of tests to confirm that everything works. [See a demo here.](https://x.com/sdrzn/status/1850880547825823989)\n\n<!-- Transparent pixel to create line break after floating image -->\n\n<img alt=\"ee14e6f7_20b8_4391_9091_8e8e25561929\" width=\"2000\" height=\"0\" src=\"https://github.com/user-attachments/assets/ee14e6f7-20b8-4391-9091-8e8e25561929\"><br>\n\n<img alt=\"ac0efa14_5c1f_4c26_a42d_9d7c56f5fadd\" align=\"right\" width=\"350\" src=\"https://github.com/user-attachments/assets/ac0efa14-5c1f-4c26-a42d-9d7c56f5fadd\">\n\n### \"add a tool that...\"\n\nThanks to the [Model Context Protocol](https://github.com/modelcontextprotocol), Cline can extend his capabilities through custom tools. While you can use [community-made servers](https://github.com/modelcontextprotocol/servers), Cline can instead create and install tools tailored to your specific workflow. Just ask Cline to \"add a tool\" and he will handle everything, from creating a new MCP server to installing it into the extension. These custom tools then become part of Cline's toolkit, ready to use in future tasks.\n\n-   \"add a tool that fetches Jira tickets\": Retrieve ticket ACs and put Cline to work\n-   \"add a tool that manages AWS EC2s\": Check server metrics and scale instances up or down\n-   \"add a tool that pulls the latest PagerDuty incidents\": Fetch details and ask Cline to fix bugs\n\n<!-- Transparent pixel to create line break after floating image -->\n\n<img alt=\"ee14e6f7_20b8_4391_9091_8e8e25561929\" width=\"2000\" height=\"0\" src=\"https://github.com/user-attachments/assets/ee14e6f7-20b8-4391-9091-8e8e25561929\"><br>\n\n<img alt=\"7fdf41e6_281a_4b4b_ac19_020b838b6970\" align=\"left\" width=\"360\" src=\"https://github.com/user-attachments/assets/7fdf41e6-281a-4b4b-ac19-020b838b6970\">\n\n### Add Context\n\n**`@url`:** Paste in a URL for the extension to fetch and convert to markdown, useful when you want to give Cline the latest docs\n\n**`@problems`:** Add workspace errors and warnings ('Problems' panel) for Cline to fix\n\n**`@file`:** Adds a file's contents so you don't have to waste API requests approving read file (+ type to search files)\n\n**`@folder`:** Adds folder's files all at once to speed up your workflow even more\n\n<!-- Transparent pixel to create line break after floating image -->\n\n<img alt=\"ee14e6f7_20b8_4391_9091_8e8e25561929\" width=\"2000\" height=\"0\" src=\"https://github.com/user-attachments/assets/ee14e6f7-20b8-4391-9091-8e8e25561929\"><br>\n\n<img alt=\"140c8606_d3bf_41b9_9a1f_4dbf0d4c90cb\" align=\"right\" width=\"350\" src=\"https://github.com/user-attachments/assets/140c8606-d3bf-41b9-9a1f-4dbf0d4c90cb\">\n\n### Checkpoints: Compare and Restore\n\nAs Cline works through a task, the extension takes a snapshot of your workspace at each step. You can use the 'Compare' button to see a diff between the snapshot and your current workspace, and the 'Restore' button to roll back to that point.\n\nFor example, when working with a local web server, you can use 'Restore Workspace Only' to quickly test different versions of your app, then use 'Restore Task and Workspace' when you find the version you want to continue building from. This lets you safely explore different approaches without losing progress.\n\n<!-- Transparent pixel to create line break after floating image -->\n\n<img alt=\"ee14e6f7_20b8_4391_9091_8e8e25561929\" width=\"2000\" height=\"0\" src=\"https://github.com/user-attachments/assets/ee14e6f7-20b8-4391-9091-8e8e25561929\"><br>\n\n## Contributing\n\nTo contribute to the project, start with our [Contributing Guide](CONTRIBUTING.md) to learn the basics. You can also join our [Discord](https://discord.gg/cline) to chat with other contributors in the `#contributors` channel. If you're looking for full-time work, check out our open positions on our [careers page](https://cline.bot/join-us)!\n\n## License\n\n[Apache 2.0 © 2025 Cline Bot Inc.](./LICENSE)\n",
      "npm_url": "https://www.npmjs.com/package/cline",
      "npm_downloads": 1328039,
      "keywords": [
        "cline",
        "automate",
        "tools",
        "search cline",
        "cline enable",
        "cline cline"
      ],
      "category": "web-search"
    },
    "clssck--researcher-mcp": {
      "owner": "clssck",
      "name": "researcher-mcp",
      "url": "https://github.com/clssck/researcher-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/clssck.webp",
      "description": "Integrates with intelligent search and documentation retrieval to assist coding by discovering APIs and modernizing code. Maintains context across queries using a database, while employing advanced query processing and intelligent rate management for effective responses.",
      "stars": 4,
      "forks": 0,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-09-24T09:03:43Z",
      "readme_content": "# MCP-researcher Server\n\nA powerful research assistant that integrates with Cline and Claude Desktop! Leverages Perplexity AI for intelligent search, documentation retrieval, API discovery, and code modernization assistance - all while you code.\n\n## Features\n\n- **Seamless Context Tracking**: Maintains conversation history in SQLite database to provide coherent responses across multiple queries\n- **Advanced Query Processing**: Uses Perplexity's Sonar models for sophisticated reasoning and detailed answers to complex questions\n- **Intelligent Rate Management**: Implements adaptive rate limiting with exponential backoff to maximize API usage without hitting limits\n- **High Performance Networking**: Optimizes API calls with connection pooling and automatic retry logic for reliable operation\n\n## Tools\n\n### 1. Search\n\nPerforms general search queries to get comprehensive information on any topic. The example shows how to use different detail levels (brief, normal, detailed) to get tailored responses.\n\n### 2. Get Documentation\n\nRetrieves documentation and usage examples for specific technologies, libraries, or APIs. The example demonstrates getting comprehensive documentation for React hooks, including best practices and common pitfalls.\n\n### 3. Find APIs\n\nDiscovers and evaluates APIs that could be integrated into a project. The example shows finding payment processing APIs with detailed analysis of features, pricing, and integration complexity.\n\n### 4. Check Deprecated Code\n\nAnalyzes code for deprecated features or patterns, providing migration guidance. The example demonstrates checking React class components and lifecycle methods for modern alternatives.\n\n## Installation\n\n### paste this part into claude directly if you want to, the ai can install it for you\n\n1. First install Node.js if not already installed (from nodejs.org)\n\n2. Clone the repo\n\n3. Install dependencies and build\n\n4. Get a Perplexity API key from [https://www.perplexity.ai/settings/api](https://www.perplexity.ai/settings/api)\n\n5. Create the MCP settings file in the appropriate location for your OS:\n\n6. To use with Claude Desktop, add the server config:\n\n7. To use with Cline, add into mcpServers:\n\n```json\n{\n  \"mcpServers\": {\n    \"perplexity-server\": {\n      \"command\": \"node\",\n      \"args\": [\"[path/to/researcher-mcp/build/index.js]\"],\n      \"env\": {\n        \"PERPLEXITY_API_KEY\": \"pplx-...\",\n        \"PERPLEXITY_MODEL\": \"sonar-reasoning\" // you can use different models\n      },\n      \"disabled\": false,\n      \"alwaysAllow\": [],\n      \"autoApprove\": [\n        \"search\",\n        \"get_documentation\",\n        \"find_apis\",\n        \"check_deprecated_code\",\n        \"get_request_status\"\n      ]\n    }\n  }\n}\n```\n\n8. Build the server:\n   npm run build\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "search",
        "retrieval",
        "queries",
        "search documentation",
        "search clssck",
        "intelligent search"
      ],
      "category": "web-search"
    },
    "cmann50--mcp-chrome-google-search": {
      "owner": "cmann50",
      "name": "mcp-chrome-google-search",
      "url": "https://github.com/cmann50/mcp-chrome-google-search",
      "imageUrl": "/freedevtools/mcp/pfp/cmann50.webp",
      "description": "Enables the integration of Google search capabilities and extraction of webpage content through the Chrome browser.",
      "stars": 21,
      "forks": 6,
      "license": "MIT License",
      "language": "HTML",
      "updated_at": "2025-08-26T00:18:26Z",
      "readme_content": "# MCP Chrome Google Search Tool\n\nMCP tool for Google search and webpage content extraction using Chrome browser. Works with Claude to enable Google search and content fetching capabilities.\n\n## Quick Installation\n\n1. **Configure Claude Desktop**\n   - Open Claude Desktop on Mac\n   - Go to Claude > Settings > Developer > Edit Config\n   - Add the following to your config file:\n   ```json\n   {\n     \"mcpServers\": {\n       \"mcp-chrome-google-search\": {\n         \"command\": \"npx\",\n         \"args\": [\n           \"-y\",\n           \"@cmann50/mcp-chrome-google-search\"\n         ]\n       }\n     }\n   }\n   ```\n   - Restart Claude Desktop\n\n2. **First Time Setup**\n   - **Grant Accessibility Permissions**\n     - On first run, approve macOS accessibility permissions prompt\n     - Navigate to: System Preferences > Security & Privacy > Privacy > Accessibility\n     - Add and enable permissions for your terminal app\n\n   - **Enable Chrome JavaScript from Apple Events**\n     - Open Chrome\n     - Navigate to: View > Developer > Allow JavaScript from Apple Events\n     - One-time setup only\n\nOnce configured, Claude will be able to perform Google searches and extract webpage content through Chrome when you make requests.\n\n## Key Advantages\n\n- Free to search google\n- Opens and small windows and uses your chrome browser, so should not get blocked\n- Since it is using your Chrome window it can access authenticated content.  Claude can just open the URL in your browser.\n\n## Platform Support\n- ✅ macOS\n- ❌ Windows (not supported)\n- ❌ Linux (not supported)\n\n## Requirements\n1. macOS\n2. Google Chrome\n3. Node.js 20 or higher\n\n## Alternative Installation Methods\n\n### NPX Installation\n```bash\nnpx mcp-chrome-google-search\n```\n\n\n### Custom Installation\n1. Checkout from git\n2. Run `npm run build`\n3. Add to Claude config (use absolute path):\n```json\n{\n    \"google-tools\": {\n        \"command\": \"node\",\n        \"args\": [\n            \"/your/checkout/path/mcp/mcp-chrome-google-search/dist/index.js\"\n        ]\n    }\n}\n```\n\n## Local development\n\nTo test changes locally bump package.json version and run\nto put it in edit mode:\n```\nnpm install -g .\n```\nThen just do `npm run build` and the files will go in dist where claude is monitoring\n\nThen press ctrl-R in claude desktop, no need to restart it\n\n## Debugging\n\n### Log Monitoring\n```bash\n# Follow logs in real-time\ntail -n 20 -F ~/Library/Logs/Claude/mcp*.log\n```\n\n### Dev Tools Access\n1. Enable developer settings:\n```bash\necho '{\"allowDevTools\": true}' > ~/Library/Application\\ Support/Claude/developer_settings.json\n```\n2. Open DevTools: Command-Option-Shift-i in Claude desktop\n3. Use ctrl-r in Claude desktop while tailing for better errors\n\n## Troubleshooting\n\n### Chrome JavaScript Error\nIf you see:\n```\nexecution error: Google Chrome got an error: Executing JavaScript through AppleScript \nis turned off. For more information: https://support.google.com/chrome/?p=applescript (12)\n```\n\nSolution:\n1. Open Chrome\n2. View > Developer > Allow JavaScript from Apple Events\n\n### Accessibility Permission Issues\nIf Chrome control fails:\n1. Open System Preferences\n2. Security & Privacy > Privacy > Accessibility\n3. Ensure terminal app is listed and enabled\n4. Use lock icon to make changes if needed\n\n## Implementation Details\n\n- Uses AppleScript for Chrome control\n- Visible automation - Chrome windows will open/navigate\n- Each request opens a new Chrome tab\n- Close unused tabs periodically for optimal performance\n- Only use with trusted Claude instances (has Chrome control access)\n\n## Support\n\n- Create GitHub issues for problems\n- Include macOS and Chrome version details\n\n## License\n\nMIT License - see LICENSE file for details",
      "npm_url": "https://www.npmjs.com/package/@cmann50/mcp-chrome-google-search",
      "npm_downloads": 4184,
      "keywords": [
        "chrome",
        "cmann50",
        "google",
        "mcp chrome",
        "search cmann50",
        "chrome google"
      ],
      "category": "web-search"
    },
    "code-yeongyu--perplexity-advanced-mcp": {
      "owner": "code-yeongyu",
      "name": "perplexity-advanced-mcp",
      "url": "https://github.com/code-yeongyu/perplexity-advanced-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/code-yeongyu.webp",
      "description": "Enhances query processing capabilities with advanced integration for AI models, enabling seamless connectivity between AI systems and various tools or data sources.",
      "stars": 24,
      "forks": 5,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-09-12T19:32:55Z",
      "readme_content": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/code-yeongyu-perplexity-advanced-mcp-badge.png)](https://mseep.ai/app/code-yeongyu-perplexity-advanced-mcp)\n\n<div align=\"center\">\n\n# Perplexity Advanced MCP\n\n[![GitHub](https://img.shields.io/badge/GitHub-100000?style=for-the-badge&logo=github&logoColor=white)](https://github.com/code-yeongyu/perplexity-advanced-mcp)\n[![PyPI](https://img.shields.io/badge/pypi-3775A9?style=for-the-badge&logo=pypi&logoColor=white)](https://pypi.org/project/perplexity-advanced-mcp)\n[![smithery badge](https://smithery.ai/badge/@code-yeongyu/perplexity-advanced-mcp)](https://smithery.ai/server/@code-yeongyu/perplexity-advanced-mcp)\n\n[한국어](README-ko.md)\n\n</div>\n\n---\n\n## Overview\n\nPerplexity Advanced MCP is an advanced integration package that leverages the [OpenRouter](https://openrouter.ai/) and [Perplexity](https://docs.perplexity.ai/home) APIs to provide enhanced query processing capabilities. With an intuitive command-line interface and a robust API client, this package facilitates seamless interactions with AI models for both simple and complex queries.\n\n## Comparison with [perplexity-mcp](https://github.com/jsonallen/perplexity-mcp)\n\nWhile [perplexity-mcp](https://github.com/jsonallen/perplexity-mcp) provides basic web search functionality using [Perplexity](https://docs.perplexity.ai/home) AI's API, Perplexity Advanced MCP offers several additional features:\n\n- **Multi-vendor Support:** Supports both [Perplexity](https://docs.perplexity.ai/home) and [OpenRouter](https://openrouter.ai/) APIs, giving you flexibility in choosing your provider\n- **Query Type Optimization:** Distinguishes between simple and complex queries, optimizing for cost and performance\n- **File Attachment Support:** Allows including file contents as context in your queries, enabling more precise and contextual responses\n- **Enhanced Retry Logic:** Implements robust retry mechanisms for improved reliability\n\nOverall, this is the most suitable MCP for handling codebases when integrated with editors like [Cline](https://cline.bot/) or [Cursor](https://www.cursor.com/).\n\n\n## Features\n\n- **Unified API Client:** Supports both [OpenRouter](https://openrouter.ai/) and [Perplexity](https://docs.perplexity.ai/home) APIs with configurable models for handling simple and complex queries.\n- **Command-Line Interface (CLI):** Manage API key configuration and run the MCP server using [Typer](https://typer.tiangolo.com/).\n- **Advanced Query Processing:** Incorporates file attachment processing, allowing you to include contextual data in your queries.\n- **Robust Retry Mechanism:** Utilizes Tenacity for retry logic to ensure consistent and reliable API communications.\n- **Customizable Logging:** Flexible logging configuration for detailed debugging and runtime monitoring.\n\n## Optimal AI Configuration\n\nFor the best experience with AI assistants (e.g., [Cursor](https://www.cursor.com/), [Claude for Desktop](https://claude.ai/download)), I recommend adding the following configuration to your project instructions or AI rules:\n\n```xml\n<perplexity-advanced-mcp>\n    <description>\n        Perplexity is an LLM that can search the internet, gather information, and answer users' queries.\n\n        For example, let's suppose we want to find out the latest version of Python.\n        1. You would search on Google.\n        2. Then read the top two or three results directly to verify.\n\n        Perplexity does that work for you.\n\n        To answer a user's query, Perplexity searches, opens the top search results, finds information on those websites, and then provides the answer.\n\n        Perplexity can be used with two types of queries: simple and complex. Choosing the right query type to fulfill the user's request is most important.\n    </description>\n    <simple-query>\n        <description>\n            It's cheap and fast. However, it's not suitable for complex queries. On average, it's more than 10 times cheaper and 3 times faster than complex queries.\n            Use it for simple questions such as \"What is the latest version of Python?\"\n        </description>\n        <pricing>\n            $1/M input tokens\n            $1/M output tokens\n        </pricing>\n    </simple-query>\n\n    <complex-query>\n        <description>\n            It's slower and more expensive. Compared to simple queries, it's on average more than 10 times more expensive and 3 times slower.\n            Use it for more complex requests like \"Analyze the attached code to examine the current status of a specific library and create a migration plan.\"\n        </description>\n        <pricing>\n            $1/M input tokens\n            $5/M output tokens\n        </pricing>\n    </complex-query>\n\n    <instruction>\n        When reviewing the user's request, if you find anything unexpected, uncertain, or questionable, **and you think you can get answer from the internet**, do not hesitate to use the \"ask_perplexity\" tool to consult Perplexity. However, if the internet is not required to satisfy users' request, it's meaningless to ask to perplexity.\n        Since Perplexity is also an LLM, prompt engineering techniques are paramount.\n        Remember the basics of prompt engineering, such as providing clear instructions, sufficient context, and examples\n        Include as much context and relevant files as possible to smoothly fulfill the user's request. When adding files as attachments, make sure they are absolute paths.\n    </instruction>\n</perplexity-advanced-mcp>\n```\n\nThis configuration helps AI assistants better understand when and how to use the Perplexity search functionality, optimizing for both cost and performance.\n\n## Usage\n\n### Installing via Smithery\n\nTo install Perplexity Advanced MCP for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@code-yeongyu/perplexity-advanced-mcp):\n\n```bash\nnpx -y @smithery/cli install @code-yeongyu/perplexity-advanced-mcp --client claude\n```\n\n### Quick Start with [uvx](https://docs.astral.sh/uv/guides/tools/)\n\nThe easiest way to run the MCP server is using [uvx](https://docs.astral.sh/uv/guides/tools/):\n\n```sh\nuvx perplexity-advanced-mcp -o <openrouter_api_key> # or -p <perplexity_api_key>\n```\n\nYou can also configure the API keys using environment variables:\n\n```sh\nexport OPENROUTER_API_KEY=\"your_key_here\"\n# or\nexport PERPLEXITY_API_KEY=\"your_key_here\"\n\nuvx perplexity-advanced-mcp\n```\n\nNote:\n- Providing both OpenRouter and Perplexity API keys simultaneously will result in an error\n- When both CLI arguments and environment variables are provided, CLI arguments take precedence\n\nThe CLI is built with [Typer](https://typer.tiangolo.com/), ensuring a user-friendly command-line experience.\n\n### MCP Search Tool\n\nThe package includes an MCP search tool integrated via the `ask_perplexity` function. It supports both simple and complex queries and processes file attachments to provide additional context.\n\n- **Simple Queries:** Provides fast, efficient responses.\n- **Complex Queries:** Engages in detailed reasoning and supports file attachments formatted as XML.\n\n## Configuration\n\n- **API Keys:** Configure either the `OPENROUTER_API_KEY` or `PERPLEXITY_API_KEY` through command-line options or environment variables.\n- **Model Selection:** The configuration (in `src/perplexity_advanced_mcp/config.py`) maps query types to specific models:\n  - **[OpenRouter](https://openrouter.ai/):**\n    - Simple Queries: `perplexity/sonar`\n    - Complex Queries: `perplexity/sonar-reasoning`\n  - **[Perplexity](https://docs.perplexity.ai/home):**\n    - Simple Queries: `sonar-pro`\n    - Complex Queries: `sonar-reasoning-pro`\n\n## Development Background & Philosophy\n\nThis project emerged from my personal curiosity and experimentation. Following the recent [\"vibe coding\"](https://x.com/karpathy/status/1886192184808149383) trend, over 95% of the code was written through [Cline](https://cline.bot/) + [Cursor](https://www.cursor.com/) IDE. They say \"talk is cheap, show me the code\" - well, with [Wispr Flow](https://wisprflow.ai/)'s speech-to-text magic, I literally just talked and the code showed up! Most of the development was done by me saying things like \"Write me the code for x y z, fix the bug here x y z.\" and pressing enter. Remarkably, creating this fully functional project took less than a few hours.\n\nFrom project scaffolding to file structure, everything was written and reviewed through LLM. Even the GitHub Actions workflow for PyPI publishing and the release approval process were handled through Cursor. As a human developer, my role was to:\n\n- Starting and stopping the MCP server to help AI conduct proper testing\n- Copying and providing error logs when issues occurred\n- Finding and providing [Python MCP SDK](https://github.com/modelcontextprotocol/python-sdk) documentation and examples from the internet\n- Requesting modifications for code that didn't seem correct\n\nIn today's world where many things can be automated and replaced, I hope this MCP can help developers like you who use it to discover value beyond just writing code. May this tool assist you in becoming a new era developer who can make higher-level decisions and considerations.\n\n## Development\n\nTo contribute or modify this package:\n\n### 1. **Clone the Repository:**\n\n```sh\ngh repo clone code-yeongyu/perplexity-advanced-mcp\n```\n\n### 2. **Install Dependencies:**\n\n```sh\nuv sync\n```\n\n### 3. **Contribute:**\n\nContributions are welcome! Please follow the existing code style and commit guidelines.\n\n## License\n\nThis project is licensed under the MIT License.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "search",
        "ai",
        "web",
        "search code",
        "web search",
        "query processing"
      ],
      "category": "web-search"
    },
    "codyde--mcp-firecrawl-tool": {
      "owner": "codyde",
      "name": "mcp-firecrawl-tool",
      "url": "https://github.com/codyde/mcp-firecrawl-tool",
      "imageUrl": "/freedevtools/mcp/pfp/codyde.webp",
      "description": "Scrape websites to extract structured data using Firecrawl's APIs, with features for error tracking and performance monitoring. Supports gathering content in various formats and custom schema data extraction.",
      "stars": 1,
      "forks": 2,
      "license": "No License",
      "language": "JavaScript",
      "updated_at": "2025-07-19T14:56:36Z",
      "readme_content": "# MCP Firecrawl Server\n\nThis is a simple MCP server that provides tools to scrape websites and extract structured data using Firecrawl's APIs.\n\n## Setup\n\n1. Install dependencies:\n```bash\nnpm install\n```\n\n2. Create a `.env` file in the root directory with the following variables:\n```\nFIRECRAWL_API_TOKEN=your_token_here\nSENTRY_DSN=your_sentry_dsn_here\n```\n\n- `FIRECRAWL_API_TOKEN` (required): Your Firecrawl API token\n- `SENTRY_DSN` (optional): Sentry DSN for error tracking and performance monitoring\n\n3. Start the server:\n```bash\nnpm start\n```\n\nAlternatively, you can set environment variables directly when running the server:\n```bash\nFIRECRAWL_API_TOKEN=your_token_here npm start\n```\n\n## Features\n\n- **Website Scraping**: Extract content from websites in various formats\n- **Structured Data Extraction**: Extract specific data points based on custom schemas\n- **Error Tracking**: Integrated with Sentry for error tracking and performance monitoring\n\n## Usage\n\nThe server exposes two tools:\n1. `scrape-website`: Basic website scraping with multiple format options\n2. `extract-data`: Structured data extraction based on prompts and schemas\n\n### Tool: scrape-website\n\nThis tool scrapes a website and returns its content in the requested formats.\n\nParameters:\n- `url` (string, required): The URL of the website to scrape\n- `formats` (array of strings, optional): Array of desired output formats. Supported formats are:\n  - `\"markdown\"` (default)\n  - `\"html\"`\n  - `\"text\"`\n\nExample usage with MCP Inspector:\n```bash\n# Basic usage (defaults to markdown)\nmcp-inspector --tool scrape-website --args '{\n  \"url\": \"https://example.com\"\n}'\n\n# Multiple formats\nmcp-inspector --tool scrape-website --args '{\n  \"url\": \"https://example.com\",\n  \"formats\": [\"markdown\", \"html\", \"text\"]\n}'\n```\n\n### Tool: extract-data\n\nThis tool extracts structured data from websites based on a provided prompt and schema.\n\nParameters:\n- `urls` (array of strings, required): Array of URLs to extract data from\n- `prompt` (string, required): The prompt describing what data to extract\n- `schema` (object, required): Schema definition for the data to extract\n\nThe schema definition should be an object where keys are field names and values are types. Supported types are:\n- `\"string\"`: For text fields\n- `\"boolean\"`: For true/false fields\n- `\"number\"`: For numeric fields\n- Arrays: Specified as `[\"type\"]` where type is one of the above\n- Objects: Nested objects with their own type definitions\n\nExample usage with MCP Inspector:\n```bash\n# Basic example extracting company information\nmcp-inspector --tool extract-data --args '{\n  \"urls\": [\"https://example.com\"],\n  \"prompt\": \"Extract the company mission, whether it supports SSO, and whether it is open source.\",\n  \"schema\": {\n    \"company_mission\": \"string\",\n    \"supports_sso\": \"boolean\",\n    \"is_open_source\": \"boolean\"\n  }\n}'\n\n# Complex example with nested data\nmcp-inspector --tool extract-data --args '{\n  \"urls\": [\"https://example.com/products\", \"https://example.com/pricing\"],\n  \"prompt\": \"Extract product information including name, price, and features.\",\n  \"schema\": {\n    \"products\": [{\n      \"name\": \"string\",\n      \"price\": \"number\",\n      \"features\": [\"string\"]\n    }]\n  }\n}'\n```\n\nBoth tools will return appropriate error messages if the scraping or extraction fails and automatically log errors to Sentry if configured.\n\n## Troubleshooting\n\nIf you encounter issues:\n\n1. Verify your Firecrawl API token is valid\n2. Check that the URLs you're trying to scrape are accessible\n3. For complex schemas, ensure they follow the supported format\n4. Review Sentry logs for detailed error information (if configured) ",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "firecrawl",
        "scrape",
        "apis",
        "firecrawl apis",
        "firecrawl tool",
        "scrape websites"
      ],
      "category": "web-search"
    },
    "coucya--mcp-server-requests": {
      "owner": "coucya",
      "name": "mcp-server-requests",
      "url": "https://github.com/coucya/mcp-server-requests",
      "imageUrl": "/freedevtools/mcp/pfp/coucya.webp",
      "description": "Enable AI models to fetch, process, and manipulate web content via HTTP requests, providing real-time access to online resources. Convert web data to Markdown and customize headers for enhanced interaction with web pages.",
      "stars": 4,
      "forks": 1,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-08T02:26:03Z",
      "readme_content": "[中文](README-zh.md)\n\n-----\n\n# mcp-server-requests\n\nAn MCP server that provides HTTP request capabilities, enabling LLMs to fetch and process web content.\n\n## Features\n- Supports converting web content to Markdown format\n- Supports filtering out content useless for LLMs\n- Supports custom User-Agent headers\n- Supports random User-Agent headers\n- Supports custom request headers in HTTP requests\n- Supports full HTTP methods (GET, POST, PUT, DELETE, PATCH)\n- LLMs can access complete HTTP response header information\n\n## Installation\n\n```bash\ngit clone https://github.com/coucya/mcp-server-requests.git\ncd mcp-server-requests\npip install .\n```\n\n## Usage\n\n### MCP Server Configuration\n\n```json\n{\n    \"mcpServers\": {\n        \"mcp-server-requests\": {\n            \"command\": \"python\",\n            \"args\": [\n                \"-m\",\n                \"mcp_server_requests\"\n            ]\n        }\n    }\n}\n```\n\n### Command Line\n\n### 0. **Start MCP Server**\n\nStart the MCP server directly:\n\n```bash\npython -m mcp_server_requests\n```\n\n#### Options\n- `--user-agent TEXT`: Specify custom User-Agent string\n- `--random-user-agent [browser=xxx;os=xxx]`: Use randomly generated User-Agent\n- `--force-user-agent`: Force using command line specified User-Agent, ignoring LLM provided UA\n- `--list-os-and-browser`: List available browsers and OS for random User-Agent generation\n\n#### Option Details\n- `--user-agent` and `--random-user-agent` are mutually exclusive and cannot be used together\n- User-Agent setup methods:\n  - Custom string: `--user-agent \"Mozilla/5.0 (...)\"`\n  - Fully random: `--random-user-agent`\n  - Conditional random generation:\n    - Specify browser type: `--random-user-agent browser=chrome`\n    - Specify OS: `--random-user-agent os=windows`\n    - Both browser and OS: `--random-user-agent browser=chrome;os=windows`\n    - Note: Browser and OS parameters are case insensitive\n\n- Use `--list-os-and-browser` to view available browsers and OS for `--random-user-agent`.\n\n- `--force-user-agent` controls User-Agent priority:\n  - When enabled: Prioritize command line specified User-Agent (via `--user-agent` or `--random-user-agent`), ignoring LLM provided UA\n  - When disabled:\n    - If LLM provides User-Agent, use that\n    - Otherwise use command line specified User-Agent\n\n---\n\n### 1. **fetch - Fetch Web Content**\n\nThe fetch subcommand is equivalent to the fetch tool functionality, demonstrating fetch capabilities.\n\n```bash\npython -m mcp_server_requests fetch <URL> [--return-content {raw,basic_clean,strict_clean,markdown}]\n```\n\nOptions:\n- `--return-content`: Return content type (default: markdown)\n  - **raw**: Return raw unprocessed HTML content\n  - **basic_clean**: Basic cleanup, removing non-display tags like script, style\n  - **strict_clean**: Strict cleanup, removing non-display tags and most HTML attributes\n  - **markdown**: Convert HTML to clean Markdown format\n\nExample:\n```\npython -m mcp_server_requests fetch https://example.com\n```\n\n---\n\n### 2. **get - Execute HTTP GET Request**\n\nThe get subcommand is equivalent to the http_get tool functionality, demonstrating http_get capabilities.\n\n```bash\npython -m mcp_server_requests get <URL> [--headers HEADERS]\n```\n\nOptions:\n- `--headers`: Custom request headers (format: \"key1=value1;key2=value2\")\n\n---\n\n### 3. **post - Execute HTTP POST Request**\n\nThe post subcommand is equivalent to the http_post tool functionality, demonstrating http_post capabilities.\n\n```bash\npython -m mcp_server_requests post <URL> [--headers HEADERS] [--data TEXT]\n```\n\nOptions:\n- `--headers`: Custom request headers\n- `--data`: Request body data\n\n---\n\n### 4. **put - Execute HTTP PUT Request**\n\nThe put subcommand is equivalent to the http_put tool functionality, demonstrating http_put capabilities.\n\n```bash\npython -m mcp_server_requests put <URL> [--headers HEADERS] [--data TEXT]\n```\n\nOptions: Same as POST method\n\n---\n\n### 5. **delete - Execute HTTP DELETE Request**\n\nThe delete subcommand is equivalent to the http_delete tool functionality, demonstrating http_delete capabilities.\n\n```bash\npython -m mcp_server_requests delete <URL> [--headers HEADERS] [--data TEXT]\n```\n\nOptions: Same as POST method\n\n---\n\n## Functionality\n\n### Available Tools\n\n1. **fetch** - Fetch web content\n   - Parameters:\n     - **url** (required): Target URL\n     - **return_content** (optional): Return content type ('raw', 'basic_clean', 'strict_clean', 'markdown')\n       - **raw**: Return raw HTML content\n       - **basic_clean**: Return filtered HTML content, removing non-display tags like script, style\n       - **strict_clean**: Return filtered HTML content, removing non-display tags and most useless HTML attributes\n       - **markdown**: Return HTML converted to Markdown\n\n2. **http_get** - Execute HTTP GET request\n   - Parameters:\n     - **url** (required): Target URL\n     - **query** (optional): Query parameter key-value pairs\n     - **headers** (optional): Custom request headers\n       - LLM may specify User-Agent in headers, whether to use it is controlled by `--force-user-agent` (same applies to other tools)\n\n3. **http_post** - Execute HTTP POST request\n   - Parameters:\n     - **url** (required): Target URL\n     - **query** (optional): Query parameter key-value pairs\n     - **headers** (optional): Custom request headers\n     - **data** (optional): Request body data (text)\n     - **json** (optional): Request body data (JSON)\n     - **data** and **json** cannot be used together\n\n4. **http_put** - Execute HTTP PUT request\n   - Parameters: Same as http_post\n\n5. **http_patch** - Execute HTTP PATCH request\n   - Parameters: Same as http_post\n\n6. **http_delete** - Execute HTTP DELETE request\n   - Parameters: Same as http_post\n\n## License\nMIT",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "web",
        "requests",
        "http",
        "web search",
        "web data",
        "web content"
      ],
      "category": "web-search"
    },
    "coyaSONG--youtube-mcp-server": {
      "owner": "coyaSONG",
      "name": "youtube-mcp-server",
      "url": "https://github.com/coyaSONG/youtube-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/coyaSONG.webp",
      "description": "Interact with YouTube data to search for videos, channels, and comments, and analyze performance metrics. Retrieve transcripts and generate summaries of video content.",
      "stars": 7,
      "forks": 0,
      "license": "No License",
      "language": "JavaScript",
      "updated_at": "2025-09-24T09:03:57Z",
      "readme_content": "# YouTube MCP Server\n\n[![smithery badge](https://smithery.ai/badge/@coyaSONG/youtube-mcp-server)](https://smithery.ai/server/@coyaSONG/youtube-mcp-server)\n\nA Model Context Protocol (MCP) server for interacting with YouTube data. This server provides resources and tools to query YouTube videos, channels, comments, and transcripts through a stdio interface.\n\n## Features\n\n- Search for YouTube videos with advanced filtering options\n- Get detailed information about specific videos and channels\n- Compare statistics across multiple videos\n- Discover trending videos by region and category\n- Analyze channel performance and video statistics\n- Retrieve video comments and transcripts/captions\n- Generate video analysis and transcript summaries\n\n## Prerequisites\n\n- Node.js (v16+)\n- YouTube Data API key\n\n## Installation\n\n### Installing via Smithery\n\nTo install YouTube MCP Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@coyaSONG/youtube-mcp-server):\n\n```bash\nnpx -y @smithery/cli install @coyaSONG/youtube-mcp-server --client claude\n```\n\n### Installing Manually\n1. Clone this repository:\n   ```bash\n   git clone https://github.com/coyaSONG/youtube-mcp-server.git\n   cd youtube-mcp-server\n   ```\n\n2. Install dependencies:\n   ```bash\n   npm install\n   ```\n\n3. Create a `.env` file in the root directory:\n   ```\n   YOUTUBE_API_KEY=your_youtube_api_key_here\n   PORT=3000\n   ```\n\n## Usage\n\n### Building and Running\n\n1. Build the project:\n   ```bash\n   npm run build\n   ```\n\n2. Run the server (HTTP transport):\n   ```bash\n   npm start\n   ```\n   The server will listen on port 3000 (or PORT environment variable) and accept MCP requests at `/mcp` endpoint.\n\n3. Run in development mode:\n   ```bash\n   npm run dev\n   ```\n\n4. Clean build artifacts:\n   ```bash\n   npm run clean\n   ```\n\n### HTTP Transport Migration\n\n**Migration Status**: ✅ **Complete** - Successfully migrated from STDIO to Streamable HTTP transport\n\nThis server has been updated to use the modern Streamable HTTP transport as required by Smithery hosting platform. The migration includes:\n\n- **Modern Protocol**: Uses Streamable HTTP transport (protocol version 2025-03-26)\n- **Express.js Framework**: Built on Express.js for robust HTTP handling\n- **Session Management**: Supports stateful operations with proper session tracking\n- **MCP Endpoint**: All requests handled at `/mcp` endpoint\n- **Backwards Compatibility**: Maintains full compatibility with all existing tools and resources\n- **Enhanced Performance**: Improved scalability and better error handling\n\n### Testing the Migration\n\n**Local Testing**:\n```bash\n# Start the server\nnpm start\n\n# Test with MCP Inspector\nnpx @modelcontextprotocol/inspector\n# Connect to: http://localhost:3000/mcp\n```\n\n**Smithery Integration**:\n- The server is fully compatible with Smithery's new hosting requirements\n- All existing Claude Desktop integrations will continue to work seamlessly\n- No changes required for end users\n\n## Docker Deployment\n\nThe project includes a Dockerfile for containerized deployment:\n\n```bash\n# Build the Docker image\ndocker build -t youtube-mcp-server .\n\n# Run the container with HTTP transport\ndocker run -p 3000:3000 --env-file .env youtube-mcp-server\n```\n\n**Important**: The container now exposes port 3000 for HTTP-based MCP communication instead of STDIO.\n\n## API Reference\n\n### Resources\n\n- `youtube://video/{videoId}` - Get detailed information about a specific video\n- `youtube://channel/{channelId}` - Get information about a specific channel\n- `youtube://transcript/{videoId}` - Get transcript for a specific video\n  - Optional query parameter: `?language=LANGUAGE_CODE` (e.g., `en`, `ko`, `ja`)\n\n### Tools\n\n#### Basic Tools\n- `search-videos` - Search for YouTube videos with advanced filtering options\n- `get-video-comments` - Get comments for a specific video\n- `get-video-transcript` - Get transcript for a specific video with optional language\n- `enhanced-transcript` - Advanced transcript extraction with filtering, search, and multi-video capabilities\n- `get-key-moments` - Extract key moments with timestamps from a video transcript for easier navigation\n- `get-segmented-transcript` - Divide a video transcript into segments for easier analysis\n\n#### Statistical Tools\n- `get-video-stats` - Get statistical information for a specific video\n- `get-channel-stats` - Get subscriber count, view count, and other channel statistics\n- `compare-videos` - Compare statistics across multiple videos\n\n#### Discovery Tools\n- `get-trending-videos` - Retrieve trending videos by region and category\n- `get-video-categories` - Get available video categories for a specific region\n\n#### Analysis Tools\n- `analyze-channel-videos` - Analyze performance trends of videos from a specific channel\n\n### Prompts\n\n- `video-analysis` - Generate an analysis of a YouTube video\n- `transcript-summary` - Generate a summary of a video based on its transcript with customizable length and keywords extraction\n- `segment-by-segment-analysis` - Provide detailed breakdown of content by analyzing each segment of the video\n\n## Examples\n\n### Accessing a Video Transcript\n\n```\nyoutube://transcript/dQw4w9WgXcQ\n```\n\n### Getting a Transcript in a Specific Language\n\n```\nyoutube://transcript/dQw4w9WgXcQ?language=en\n```\n\n### Using the Statistical Tools\n\n```javascript\n// Get video statistics\n{\n  \"type\": \"tool\",\n  \"name\": \"get-video-stats\",\n  \"parameters\": {\n    \"videoId\": \"dQw4w9WgXcQ\"\n  }\n}\n\n// Compare multiple videos\n{\n  \"type\": \"tool\",\n  \"name\": \"compare-videos\",\n  \"parameters\": {\n    \"videoIds\": [\"dQw4w9WgXcQ\", \"9bZkp7q19f0\"]\n  }\n}\n```\n\n### Using the Transcript Summary Prompt\n\n```javascript\n{\n  \"type\": \"prompt\",\n  \"name\": \"transcript-summary\",\n  \"parameters\": {\n    \"videoId\": \"dQw4w9WgXcQ\",\n    \"language\": \"en\"\n  }\n}\n```\n\n### Using the Enhanced Transcript Tool\n\n```javascript\n// Basic multi-video transcript extraction\n{\n  \"type\": \"tool\",\n  \"name\": \"enhanced-transcript\",\n  \"parameters\": {\n    \"videoIds\": [\"dQw4w9WgXcQ\", \"9bZkp7q19f0\"],\n    \"format\": \"timestamped\"\n  }\n}\n\n// With search and time filtering\n{\n  \"type\": \"tool\",\n  \"name\": \"enhanced-transcript\",\n  \"parameters\": {\n    \"videoIds\": [\"dQw4w9WgXcQ\"],\n    \"filters\": {\n      \"timeRange\": {\n        \"start\": 60,  // Start at 60 seconds\n        \"end\": 180    // End at 180 seconds\n      },\n      \"search\": {\n        \"query\": \"never gonna\",\n        \"contextLines\": 2\n      }\n    },\n    \"format\": \"merged\"\n  }\n}\n\n// With smart segmentation for easier analysis\n{\n  \"type\": \"tool\",\n  \"name\": \"enhanced-transcript\",\n  \"parameters\": {\n    \"videoIds\": [\"dQw4w9WgXcQ\"],\n    \"filters\": {\n      \"segment\": {\n        \"count\": 5,\n        \"method\": \"smart\"  // Breaks at natural pauses\n      }\n    },\n    \"format\": \"timestamped\",\n    \"language\": \"en\"\n  }\n}\n```\n\n### Using the Enhanced Transcript Analysis Features\n\n```javascript\n// Get key moments from a video\n{\n  \"type\": \"tool\",\n  \"name\": \"get-key-moments\",\n  \"parameters\": {\n    \"videoId\": \"dQw4w9WgXcQ\",\n    \"maxMoments\": \"5\"\n  }\n}\n\n// Get a segmented transcript\n{\n  \"type\": \"tool\",\n  \"name\": \"get-segmented-transcript\",\n  \"parameters\": {\n    \"videoId\": \"dQw4w9WgXcQ\",\n    \"segmentCount\": \"4\"\n  }\n}\n\n// Get a segment-by-segment analysis\n{\n  \"type\": \"prompt\",\n  \"name\": \"segment-by-segment-analysis\",\n  \"parameters\": {\n    \"videoId\": \"dQw4w9WgXcQ\",\n    \"segmentCount\": \"4\"\n  }\n}\n\n// Get customized transcript summary\n{\n  \"type\": \"prompt\",\n  \"name\": \"transcript-summary\",\n  \"parameters\": {\n    \"videoId\": \"dQw4w9WgXcQ\",\n    \"language\": \"en\",\n    \"summaryLength\": \"detailed\",\n    \"includeKeywords\": \"true\"\n  }\n}\n```\n\n## Error Handling\n\nThe server handles various error conditions, including:\n\n- Invalid API key\n- Video or channel not found\n- Transcript not available\n- Network issues\n\n## License\n\nMIT\n\n## Acknowledgements\n\n- [Model Context Protocol TypeScript SDK](https://github.com/modelcontextprotocol/typescript-sdk)\n- [YouTube Data API](https://developers.google.com/youtube/v3)\n- [YouTube Captions Scraper](https://github.com/algolia/youtube-captions-scraper)",
      "npm_url": "https://www.npmjs.com/package/youtube-mcp-server",
      "npm_downloads": 258,
      "keywords": [
        "youtube",
        "coyasong",
        "search",
        "coyasong youtube",
        "youtube data",
        "youtube mcp"
      ],
      "category": "web-search"
    },
    "cr7258--higress-ai-search-mcp-server": {
      "owner": "cr7258",
      "name": "higress-ai-search-mcp-server",
      "url": "https://github.com/cr7258/higress-ai-search-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/cr7258.webp",
      "description": "Enhances AI model responses through real-time search results from various online search engines and integrates internal knowledge bases for improved accuracy and relevance in responses.",
      "stars": 5,
      "forks": 5,
      "license": "Apache License 2.0",
      "language": "Python",
      "updated_at": "2025-08-25T06:21:06Z",
      "readme_content": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/cr7258-higress-ai-search-mcp-server-badge.png)](https://mseep.ai/app/cr7258-higress-ai-search-mcp-server)\n\n# Higress AI-Search MCP Server\n\n## Overview\n\nA Model Context Protocol (MCP) server that provides an AI search tool to enhance AI model responses with real-time search results from various search engines through [Higress](https://higress.cn/) [ai-search](https://github.com/alibaba/higress/blob/main/plugins/wasm-go/extensions/ai-search/README.md) feature.\n\n<a href=\"https://glama.ai/mcp/servers/gk0xde4wbp\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/gk0xde4wbp/badge\" alt=\"Higress AI-Search Server MCP server\" />\n</a>\n\n## Demo\n\n### Cline\n\nhttps://github.com/user-attachments/assets/60a06d99-a46c-40fc-b156-793e395542bb\n\n### Claude Desktop\n\nhttps://github.com/user-attachments/assets/5c9e639f-c21c-4738-ad71-1a88cc0bcb46\n\n## Features\n\n- **Internet Search**: Google, Bing, Quark - for general web information\n- **Academic Search**: Arxiv - for scientific papers and research\n- **Internal Knowledge Search**\n\n## Prerequisites\n\n- [uv](https://github.com/astral-sh/uv) for package installation.\n- Config Higress with [ai-search](https://github.com/alibaba/higress/blob/main/plugins/wasm-go/extensions/ai-search/README.md) plugin and [ai-proxy](https://github.com/alibaba/higress/blob/main/plugins/wasm-go/extensions/ai-proxy/README.md) plugin.\n\n## Configuration\n\nThe server can be configured using environment variables:\n\n- `HIGRESS_URL`(optional): URL for the Higress service (default: `http://localhost:8080/v1/chat/completions`).\n- `MODEL`(required): LLM model to use for generating responses.\n- `INTERNAL_KNOWLEDGE_BASES`(optional): Description of internal knowledge bases.\n\n### Option 1: Using uvx\n\nUsing uvx will automatically install the package from PyPI, no need to clone the repository locally.\n\n```bash\n{\n  \"mcpServers\": {\n    \"higress-ai-search-mcp-server\": {\n      \"command\": \"uvx\",\n      \"args\": [\n        \"higress-ai-search-mcp-server\"\n      ],\n      \"env\": {\n        \"HIGRESS_URL\": \"http://localhost:8080/v1/chat/completions\",\n        \"MODEL\": \"qwen-turbo\",\n        \"INTERNAL_KNOWLEDGE_BASES\": \"Employee handbook, company policies, internal process documents\"\n      }\n    }\n  }\n}\n```\n\n### Option 2: Using uv with local development\n\nUsing uv requires cloning the repository locally and specifying the path to the source code.\n\n```bash\n{\n  \"mcpServers\": {\n    \"higress-ai-search-mcp-server\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"--directory\",\n        \"path/to/src/higress-ai-search-mcp-server\",\n        \"run\",\n        \"higress-ai-search-mcp-server\"\n      ],\n      \"env\": {\n        \"HIGRESS_URL\": \"http://localhost:8080/v1/chat/completions\",\n        \"MODEL\": \"qwen-turbo\",\n        \"INTERNAL_KNOWLEDGE_BASES\": \"Employee handbook, company policies, internal process documents\"\n      }\n    }\n  }\n}\n```\n\n## License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "ai",
        "search",
        "web",
        "ai search",
        "search results",
        "ai model"
      ],
      "category": "web-search"
    },
    "crazyrabbitLTC--mpc-tally-api-server": {
      "owner": "crazyrabbitLTC",
      "name": "mpc-tally-api-server",
      "url": "https://github.com/crazyrabbitLTC/mpc-tally-api-server",
      "imageUrl": "/freedevtools/mcp/pfp/crazyrabbitLTC.webp",
      "description": "Access and retrieve information about DAOs, including governance data, proposals, and comprehensive metadata such as social links. Supports pagination for large result sets and offers sorting options by popularity or exploration status.",
      "stars": 6,
      "forks": 9,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-05-26T17:02:04Z",
      "readme_content": "# MPC Tally API Server\n\nA Model Context Protocol (MCP) server for interacting with the Tally API. This server allows AI agents to fetch information about DAOs, including their governance data, proposals, and metadata.\n\n## Features\n\n- List DAOs sorted by popularity or exploration status\n- Fetch comprehensive DAO metadata including social links and governance information\n- Pagination support for handling large result sets\n- Built with TypeScript and GraphQL\n- Full test coverage with Bun's test runner\n\n## Installation\n\n```bash\n# Clone the repository\ngit clone https://github.com/yourusername/mpc-tally-api-server.git\ncd mpc-tally-api-server\n\n# Install dependencies\nbun install\n\n# Build the project\nbun run build\n```\n\n## Configuration\n\n1. Create a `.env` file in the root directory:\n```env\nTALLY_API_KEY=your_api_key_here\n```\n\n2. Get your API key from [Tally](https://tally.xyz)\n\n⚠️ **Security Note**: Keep your API key secure:\n- Never commit your `.env` file\n- Don't expose your API key in logs or error messages\n- Rotate your API key if it's ever exposed\n- Use environment variables for configuration\n\n## Usage\n\n### Running the Server\n\n```bash\n# Start the server\nbun run start\n\n# Development mode with auto-reload\nbun run dev\n```\n\n### Claude Desktop Configuration\n\nAdd the following to your Claude Desktop configuration:\n\n```json\n{\n  \"tally\": {\n    \"command\": \"node\",\n    \"args\": [\n      \"/path/to/mpc-tally-api-server/build/index.js\"\n    ],\n    \"env\": {\n      \"TALLY_API_KEY\": \"your_api_key_here\"\n    }\n  }\n}\n```\n\n## Available Scripts\n\n- `bun run clean` - Clean the build directory\n- `bun run build` - Build the project\n- `bun run start` - Run the built server\n- `bun run dev` - Run in development mode with auto-reload\n- `bun test` - Run tests\n- `bun test --watch` - Run tests in watch mode\n- `bun test --coverage` - Run tests with coverage\n\n## API Functions\n\nThe server exposes the following MCP functions:\n\n### list_daos\nLists DAOs sorted by specified criteria.\n\nParameters:\n- `limit` (optional): Maximum number of DAOs to return (default: 20, max: 50)\n- `afterCursor` (optional): Cursor for pagination\n- `sortBy` (optional): How to sort the DAOs (default: popular)\n  - Options: \"id\", \"name\", \"explore\", \"popular\"\n\n## License\n\nMIT ",
      "npm_url": "https://www.npmjs.com/package/mpc-tally-api-server",
      "npm_downloads": 381,
      "keywords": [
        "mpc",
        "search",
        "tally",
        "search crazyrabbitltc",
        "mpc tally",
        "comprehensive metadata"
      ],
      "category": "web-search"
    },
    "cschmidt0121--splunkbase-mcp": {
      "owner": "cschmidt0121",
      "name": "splunkbase-mcp",
      "url": "https://github.com/cschmidt0121/splunkbase-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/cschmidt0121.webp",
      "description": "Programmatic access to Splunkbase functionality, enabling users to search for, download, and manage Splunkbase apps through a standardized interface.",
      "stars": 0,
      "forks": 2,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-03-27T04:17:00Z",
      "readme_content": "# splunkbase-mcp\n\nAn MCP server for Splunkbase\n\n## Description\n\nThis is a Machine Control Protocol (MCP) server that provides programmatic access to Splunkbase functionality. It allows you to search, download, and manage Splunkbase apps through a standardized interface.\n\n## Installation \n\nWarning: this will store your password on-disk in plaintext. Better methods may come about eventually.\n\n```\nuv run mcp install -v \"SPLUNKBASE_USERNAME=my_username\" -v \"SPLUNKBASE_PASSWORD=my_password\" splunkbase-mcp.py\n```\n\n## Usage\n\nSample prompt for Claude:\n\n```\nPlease do the following.\n1. Search the web to find what Splunk app is responsible for providing field extractions for the WinEventLog sourcetype \n2. Find the app on Splunkbase and grab its numerical app ID \n3. Use the download_app tool to grab the latest version of the app from Splunkbase and place it in /tmp/apps/\n```\n\n## Resources\n\n- `app://{app}/info` - Get detailed information about a Splunkbase app\n- `app://{app}/splunk_versions` - Get supported Splunk versions for an app\n\n## Available Tools\n\n### Search\n- **search(query: str)** - Search Splunkbase for apps\n  - Returns a list of search results\n\n### Version Management\n- **get_app_latest_version(app: str | int, splunk_version: str, is_cloud: bool = False)** - Get the latest compatible version of an app\n  - Parameters:\n    - `app`: App name or numeric ID\n    - `splunk_version`: Target Splunk version\n    - `is_cloud`: Whether to check Splunk Cloud compatibility\n  - Returns version information dictionary\n\n### Download\n- **download_app(app: str | int, output_dir: str, version: Optional[str] = None)** - Download a specific app version\n  - Parameters:\n    - `app`: App name or numeric ID\n    - `output_dir`: Directory to save the downloaded app\n    - `version`: Optional specific version to download (latest if not specified)\n  - Returns success message with download details\n\n## Dependencies\n\n- aiosplunkbase >= 0.1.3\n- mcp[cli]\n- aiofiles\n- Python >= 3.11 \n\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "splunkbase",
        "cschmidt0121",
        "search",
        "cschmidt0121 splunkbase",
        "splunkbase mcp",
        "splunkbase apps"
      ],
      "category": "web-search"
    },
    "cswkim--discogs-mcp-server": {
      "owner": "cswkim",
      "name": "discogs-mcp-server",
      "url": "https://github.com/cswkim/discogs-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/cswkim.webp",
      "description": "Facilitates interaction with the Discogs music database to perform catalog operations, execute searches, and manage music collections. Enables integration with clients for streamlined music data workflows.",
      "stars": 56,
      "forks": 4,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-10-02T22:12:02Z",
      "readme_content": "[![License](https://img.shields.io/github/license/cswkim/discogs-mcp-server)](LICENSE)\n[![GitHub Release](https://img.shields.io/github/v/release/cswkim/discogs-mcp-server)](https://github.com/cswkim/discogs-mcp-server/releases)\n[![GitHub Actions Workflow Status](https://img.shields.io/github/actions/workflow/status/cswkim/discogs-mcp-server/.github%2Fworkflows%2Fcheck-pr.yml)](https://github.com/cswkim/discogs-mcp-server/actions/workflows/check-pr.yml)\n[![NPM Downloads](https://img.shields.io/npm/d18m/discogs-mcp-server)](https://www.npmjs.com/package/discogs-mcp-server)\n[![Sponsor](https://img.shields.io/static/v1?label=sponsor&message=%E2%9D%A4&logo=GitHub&color=ff69b4)](https://github.com/sponsors/cswkim)\n\n# Discogs MCP Server\n\nMCP Server for the Discogs API, enabling music catalog operations, search functionality, and more.\n\n## Quickstart\n\nIf you just want to get started immediately using this MCP Server with the [Claude](https://claude.ai) desktop app and don't care about development or running the server yourself, then make sure you have [Node.js](https://nodejs.org/en) installed and your Discogs personal access token ready and skip straight to the [Claude configuration section](#claude-desktop-configuration). Use the `NPX` method from that section.\n\n## Table of Contents\n\n- [Acknowledgements](#acknowledgements)\n- [Available Tools](#available-tools)\n- [Caveats](#caveats)\n- [Prerequisites](#prerequisites)\n- [Setup](#setup)\n- [Running the Server](#running-the-server-locally)\n  - [Option 1: Local Development](#option-1-local-development)\n  - [Option 2: Docker](#option-2-docker)\n- [Inspection](#inspection)\n- [MCP Clients](#mcp-clients)\n  - [Claude Desktop Configuration](#claude-desktop-configuration)\n    - [NPX](#npx)\n    - [Local Node](#local-node)\n    - [Docker](#docker)\n  - [LibreChat](#librechat)\n  - [LM Studio](#lm-studio)\n- [TODO](#todo)\n- [License](#license)\n\n## Acknowledgements\n\nThis MCP server is built using [FastMCP](https://github.com/punkpeye/fastmcp), a typescript framework for building MCP servers. For more information about MCP and how to use MCP servers, please refer to the [FastMCP documentation](https://github.com/punkpeye/fastmcp/blob/main/README.md) and the [official MCP documentation](https://modelcontextprotocol.io).\n\n## Available Tools\n\nCheck out the list of available tools: [TOOLS.md](TOOLS.md)\n\n## Caveats\n\n- The [Discogs API documentation](https://www.discogs.com/developers) is not perfect and some endpoints may not be fully documented or may have inconsistencies.\n- Due to the vast number of API endpoints and response types, it's not feasible to verify type safety for every possible response. Please report any type-related issues you encounter.\n- This MCP server allows for editing data in your Discogs collection. Please use with caution and verify your actions before executing them.\n- The Discogs API `per_page` default is `50`, which can be too much data for some clients to process effectively, so within this project a `discogs.config.defaultPerPage` value has been set to `5`. You can request more data in your prompts, but be aware that some clients may struggle with larger responses.\n\n## Prerequisites\n\n- Node.js (tested with Node.js `20.x.x`, but `18.x.x` should work as well)\n  - Check your Node.js version with: `node --version`\n- Docker (optional, for running a local docker image without having to deal with Node or dependencies)\n\n## Setup\n\n1. Clone the repository\n2. Create a `.env` file in the root directory based on `.env.example`\n3. Set the following required environment variables in your `.env`:\n   - `DISCOGS_PERSONAL_ACCESS_TOKEN`: Your Discogs personal access token\n\nTo get your Discogs personal access token, go to your [Discogs Settings > Developers](https://www.discogs.com/settings/developers) page and find your token or generate a new one. **_DO NOT SHARE YOUR TOKEN_**. OAuth support will be added in a future release.\n\nThe other environment variables in `.env.example` are optional and have sensible defaults, so you don't need to set them unless you have specific requirements.\n\n## Running the Server Locally\n\n### Option 1: Local Development\n\n1. Install dependencies:\n   ```bash\n   pnpm install\n   ```\n\n2. Available commands:\n   - `pnpm run dev`: Start the development server with hot reloading\n   - `pnpm run dev:stream`: Start the development server with hot reloading in HTTP streaming mode\n   - `pnpm run build`: Build the production version\n   - `pnpm run start`: Run the production build\n   - `pnpm run inspect`: Run the MCP Inspector (see [Inspection](#inspection) section)\n   - `pnpm run format`: Check code formatting (prettier)\n   - `pnpm run lint`: Run linter (eslint)\n   - `pnpm run test`: Run vitest\n   - `pnpm run test:coverage`: Run vitest v8 coverage\n   - `pnpm run version:check`: Checks that the package.json version and src/version.ts match\n\n### Option 2: Docker\n\n1. Build the Docker image:\n   ```bash\n   docker build -t discogs-mcp-server:latest .\n   ```\n\n2. Run the container:\n   ```bash\n   docker run --env-file .env discogs-mcp-server:latest\n   ```\n\n   For HTTP Streaming transport mode:\n   ```bash\n   # The port should match what is in your .env file\n   docker run --env-file .env -p 3001:3001 discogs-mcp-server:latest stream\n   ```\n\n## Inspection\n\nRun the MCP Inspector to test your local MCP server:\n\n```bash\npnpm run inspect\n```\n\nThis will start the MCP Inspector at `http://127.0.0.1:6274`. Visit this URL in your browser to interact with your local MCP server.\n\nFor more information about the MCP Inspector, visit [the official documentation](https://modelcontextprotocol.io/docs/tools/inspector).\n\n## MCP Clients\n\nMore client examples will be added in the future. If you'd like configuration for a specific client, either\nrequest it by opening a new issue or creating the pull request to edit this section of the README yourself.\n\n### Claude Desktop Configuration\n\nFind your `claude_desktop_config.json` at `Claude > Settings > Developer > Edit Config` and depending on which option you'd like, add **JUST ONE** of the following:\n\n#### NPX\n\nRunning it straight from the npm registry.\n\n```json\n{\n  \"mcpServers\": {\n    \"discogs\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"discogs-mcp-server\"\n      ],\n      \"env\": {\n        \"DISCOGS_PERSONAL_ACCESS_TOKEN\": \"<YOUR_TOKEN>\"\n      }\n    }\n  }\n}\n```\n\n#### Local Node\n\nDependencies should have been installed before you use this method (`pnpm install`).\n\n```json\n{\n  \"mcpServers\": {\n    \"discogs\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"tsx\",\n        \"/PATH/TO/YOUR/PROJECT/FOLDER/src/index.ts\"\n      ],\n      \"env\": {\n        \"DISCOGS_PERSONAL_ACCESS_TOKEN\": \"<YOUR_TOKEN>\"\n      }\n    }\n  }\n}\n```\n\n#### Docker\n\nThe docker image should have been built before using this method.\n\n```json\n{\n  \"mcpServers\": {\n    \"discogs\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"--rm\",\n        \"-i\",\n        \"--env-file\",\n        \"/PATH/TO/YOUR/PROJECT/FOLDER/.env\",\n        \"discogs-mcp-server:latest\"\n      ]\n    }\n  }\n}\n```\n\nAny changes to local code will require Claude to be restarted to take effect. Also, Claude requires human-in-the-loop interaction to allow an MCP tool to be run, so everytime a new tool is accessed Claude will ask for permission. You usually only have to do this once per tool per chat. _If using the free version, long chats may result in more frequent errors trying to run tools as Claude limits the amount of context within a single chat._\n\n### LibreChat\n\nIn the `librechat.yaml` configuration file, add this under the `mcpServers` section:\n\n```yaml\ndiscogs:\n  type: stdio\n  command: npx\n  args: [\"-y\", \"discogs-mcp-server\"]\n  env:\n    DISCOGS_PERSONAL_ACCESS_TOKEN: YOUR_TOKEN_GOES_HERE\n```\n\n### LM Studio\n\nGet to the Chat `Settings`. In the `Program` tab there will be a dropdown with a default of `Install`. Select `Edit mcp.json`. Add this under the `mcpServers` section:\n\n```json\n\"discogs\": {\n  \"command\": \"npx\",\n  \"args\": [\n    \"-y\",\n    \"discogs-mcp-server\"\n  ],\n  \"env\": {\n    \"DISCOGS_PERSONAL_ACCESS_TOKEN\": \"YOUR_TOKEN_GOES_HERE\"\n  }\n}\n```\n\nAfter you Save, in the `Program` tab there should now be an `mcp/discogs` toggle to enable the server. Within every chat box there is an `Integrations` menu where you can also enable mcp servers.\n\n## TODO\n\n- OAuth support\n- Missing tools:\n  - Inventory uploading\n\n## License\n\nThis MCP server is licensed under the MIT License. This means you are free to use, modify, and distribute the software, subject to the terms and conditions of the MIT License. For more details, please see the LICENSE file in the project repository.\n",
      "npm_url": "https://www.npmjs.com/package/discogs-mcp-server",
      "npm_downloads": 4770,
      "keywords": [
        "cswkim",
        "discogs",
        "mcp",
        "cswkim discogs",
        "search cswkim",
        "discogs music"
      ],
      "category": "web-search"
    },
    "ctvidic--strava-mcp-server": {
      "owner": "ctvidic",
      "name": "strava-mcp-server",
      "url": "https://github.com/ctvidic/strava-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/ctvidic.webp",
      "description": "Access and analyze Strava data, including activities, athlete statistics, and social interactions through a language model interface, facilitating personalized insights and performance tracking.",
      "stars": 2,
      "forks": 1,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-06-29T20:40:02Z",
      "readme_content": "# Strava MCP Server\n\nA Model Context Protocol (MCP) server that provides access to the Strava API. This server enables language models to interact with Strava data, including activities, athlete information, and more.\n\n## Features\n\n- 🏃‍♂️ Activity tracking and analysis\n- 📊 Athlete statistics\n- 🗺️ Route visualization\n- 🏆 Achievement tracking\n- 🤝 Social features (kudos, comments)\n\n## Prerequisites\n\n- Python 3.12+\n- Strava API credentials\n- pip (Python package installer)\n\n## Installation\n\n1. Clone the repository:\n```bash\ngit clone https://github.com/yourusername/strava_mcp.git\ncd strava_mcp\n```\n\n2. Create a virtual environment:\n```bash\npython -m venv venv\nsource venv/bin/activate  # On Windows: .\\venv\\Scripts\\activate\n```\n\n3. Install dependencies:\n```bash\npip install -r requirements.txt\n```\n\n## Configuration\n\n1. Create a `config/.env` file with your Strava API credentials:\n```bash\nSTRAVA_CLIENT_ID=your_client_id\nSTRAVA_CLIENT_SECRET=your_client_secret\nSTRAVA_REFRESH_TOKEN=your_refresh_token\n```\n\n2. To obtain Strava API credentials:\n   - Go to https://www.strava.com/settings/api\n   - Create a new application\n   - Note down the Client ID and Client Secret\n   - Follow the OAuth 2.0 flow to get your refresh token\n\n## Usage\n\n### Using with Claude\n\nOnce connected, you can interact with your Strava data through Claude in various ways:\n\n#### Activity Queries\n- \"Show me my recent activities\"\n- \"Get details about my last run\"\n- \"What was my longest ride this month?\"\n- \"Show me activities where I set personal records\"\n- \"Display the route map for my latest activity\"\n\n#### Performance Analysis\n- \"What's my average running pace this year?\"\n- \"Compare my cycling performance between last month and this month\"\n- \"Show me my heart rate zones from yesterday's workout\"\n- \"What's my total elevation gain for all activities?\"\n- \"Calculate my weekly mileage for running\"\n\n#### Social Interactions\n- \"Who gave kudos on my latest activity?\"\n- \"Show me comments on my marathon run\"\n- \"List all my club activities\"\n- \"Find activities I did with friends\"\n\n#### Achievement Tracking\n- \"List all my segment achievements\"\n- \"Show my personal records on local segments\"\n- \"What achievements did I earn this week?\"\n- \"Display my progress on yearly goals\"\n\n#### Data Available Through Claude\n1. Activity Details:\n   - Distance, duration, pace\n   - Route maps and elevation profiles\n   - Heart rate, power, and cadence data\n   - Splits and lap information\n   - Weather conditions during activity\n\n2. Athlete Statistics:\n   - Year-to-date and all-time totals\n   - Personal records and achievements\n   - Training load and fitness trends\n   - Equipment usage and maintenance\n\n3. Social Data:\n   - Kudos and comments\n   - Club activities and leaderboards\n   - Friend activities and challenges\n   - Segment efforts and rankings\n\n4. Route Information:\n   - Detailed maps with elevation data\n   - Segment analysis\n   - Popular routes and segments\n   - Route planning and analysis\n\n### As an MCP Server\n\nUpdate your Claude Desktop configuration:\n\n```json\n{\n    \"mcpServers\": {\n        \"Strava\": {\n            \"command\": \"python\",\n            \"args\": [\"src/strava_server.py\"],\n            \"cwd\": \"/path/to/strava_mcp\",\n            \"env\": {\n                \"STRAVA_CLIENT_ID\": \"your_client_id\",\n                \"STRAVA_CLIENT_SECRET\": \"your_client_secret\",\n                \"STRAVA_REFRESH_TOKEN\": \"your_refresh_token\"\n            }\n        }\n    }\n}\n```\n\n### As an HTTP Server\n\n1. Start the server:\n```bash\n./run_server.sh\n```\n\n2. Access the API at `http://localhost:8000`\n\nAvailable endpoints:\n- GET `/activities/recent` - List recent activities\n- GET `/activities/{id}` - Get activity details\n- GET `/activities/{id}/map` - Get activity map visualization\n- GET `/athlete/stats` - Get athlete statistics\n\n## Development\n\n### Project Structure\n```\nstrava_mcp/\n├── src/\n│   ├── strava_server.py      # MCP server implementation\n│   ├── strava_http_server.py # HTTP API server\n│   ├── map_utils.py          # Map visualization utilities\n│   └── templates.py          # HTML templates\n├── config/\n│   └── .env                  # Environment variables (not in git)\n├── requirements.txt          # Python dependencies\n└── run_server.sh            # Server startup script\n```\n\n### Contributing\n\n1. Fork the repository\n2. Create a feature branch\n3. Commit your changes\n4. Push to the branch\n5. Create a Pull Request\n\n## Security\n\n- Never commit `.env` files or API credentials\n- The `.gitignore` file is configured to prevent sensitive data from being committed\n- Use environment variables for all sensitive configuration\n\n## License\n\nThis project is licensed under the MIT License - see the LICENSE file for details.\n\n## Acknowledgments\n\n- Strava API Documentation\n- Model Context Protocol (MCP) Specification\n- Contributors and maintainers ",
      "npm_url": "https://www.npmjs.com/package/strava-mcp-server",
      "npm_downloads": 199,
      "keywords": [
        "strava",
        "athlete",
        "search",
        "strava data",
        "analyze strava",
        "ctvidic strava"
      ],
      "category": "web-search"
    },
    "cyberchitta--scrapling-fetch-mcp": {
      "owner": "cyberchitta",
      "name": "scrapling-fetch-mcp",
      "url": "https://github.com/cyberchitta/scrapling-fetch-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/cyberchitta.webp",
      "description": "Accesses text content from bot-protected websites to retrieve documentation and reference materials. Optimized for low-volume retrieval, this server facilitates the extraction of text and HTML content that is otherwise inaccessible to AI assistants.",
      "stars": 53,
      "forks": 8,
      "license": "Apache License 2.0",
      "language": "Python",
      "updated_at": "2025-10-02T18:34:27Z",
      "readme_content": "# scrapling-fetch-mcp\n\n[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)\n[![PyPI version](https://img.shields.io/pypi/v/scrapling-fetch-mcp.svg)](https://pypi.org/project/scrapling-fetch-mcp/)\n\nAn MCP server that helps AI assistants access text content from websites that implement bot detection, bridging the gap between what you can see in your browser and what the AI can access.\n\n## Intended Use\n\nThis tool is optimized for low-volume retrieval of documentation and reference materials (text/HTML only) from websites that implement bot detection. It has not been designed or tested for general-purpose site scraping or data harvesting.\n\n> **Note**: This project was developed in collaboration with Claude Sonnets 3.7 and 4.5, using [LLM Context](https://github.com/cyberchitta/llm-context.py).\n\n## Installation\n\n### Requirements\n\n- Python 3.10+\n- [uv](https://github.com/astral-sh/uv) package manager\n\n### Install\n\n```bash\n# Install scrapling-fetch-mcp\nuv tool install scrapling-fetch-mcp\n\n# Install browser binaries (REQUIRED - large downloads)\nuvx --from scrapling-fetch-mcp scrapling install\n```\n\n**Important**: The browser installation downloads hundreds of MB of data and must complete before first use. If the MCP server times out on first use, the browsers may still be installing in the background. Wait a few minutes and try again.\n\n## Setup with Claude Desktop\n\nAdd this configuration to your Claude Desktop MCP settings:\n\n**MacOS**: `~/Library/Application Support/Claude/claude_desktop_config.json`  \n**Windows**: `%APPDATA%\\Claude\\claude_desktop_config.json`\n\n```json\n{\n  \"mcpServers\": {\n    \"scrapling-fetch\": {\n      \"command\": \"uvx\",\n      \"args\": [\"scrapling-fetch-mcp\"]\n    }\n  }\n}\n```\n\nAfter updating the config, restart Claude Desktop.\n\n## What It Does\n\nThis MCP server provides two tools that Claude can use automatically when you ask it to fetch web content:\n\n- **Page fetching**: Retrieves complete web pages with support for pagination\n- **Pattern extraction**: Finds and extracts specific content using regex patterns\n\nThe AI decides which tool to use based on your request. You just ask naturally:\n\n```\n\"Can you fetch the docs at https://example.com/api\"\n\"Find all mentions of 'authentication' on that page\"\n\"Get me the installation instructions from their homepage\"\n```\n\n## Protection Modes\n\nThe tools support three levels of bot detection bypass:\n\n- **basic**: Fast (1-2s), works for most sites\n- **stealth**: Moderate (3-8s), handles more protection\n- **max-stealth**: Maximum (10+s), for heavily protected sites\n\nClaude automatically starts with `basic` mode and escalates if needed.\n\n## Tips for Best Results\n\n- Just ask naturally - Claude handles the technical details\n- For large pages, Claude can page through content automatically\n- For specific searches, mention what you're looking for and Claude will use pattern matching\n- The metadata returned helps Claude decide whether to page or search\n\n## Limitations\n\n- Designed for text content only (documentation, articles, references)\n- Not for high-volume scraping or data harvesting\n- May not work with sites requiring authentication\n- Performance varies by site complexity and protection level\n\nBuilt with [Scrapling](https://github.com/D4Vinci/Scrapling) for web scraping with bot detection bypass.\n\n## License\n\nApache 2.0\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "cyberchitta",
        "retrieval",
        "search",
        "search cyberchitta",
        "cyberchitta scrapling",
        "content bot"
      ],
      "category": "web-search"
    },
    "dabidstudio--youtubeinsights-mcp-server": {
      "owner": "dabidstudio",
      "name": "youtubeinsights-mcp-server",
      "url": "https://github.com/dabidstudio/youtubeinsights-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/dabidstudio.webp",
      "description": "Extract insights from YouTube videos, analyze transcripts, and retrieve metadata and channel information. Enable keyword-based video discovery to support content strategy development.",
      "stars": 3,
      "forks": 4,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-08-07T05:32:51Z",
      "readme_content": "# YouTube Insights MCP Server\n\n\nA Model Context Protocol (MCP) server that enables insight extraction from YouTube videos, including subtitle parsing, keyword-based video discovery, and channel info retrieval.\n\n## Features\n\n- Extract transcripts from YouTube videos (multi-language)\n- Search videos by keyword and fetch metadata (views, likes, thumbnails, etc.)\n- Retrieve channel info and latest videos from any YouTube video URL\n- FastMCP-based server integration for easy deployment\n- MCP Tools for seamless agent workflows\n\n\n### Example usecases\n\n\n<details>\n<summary>Finding Trending Videos and Summarizing</summary>\n<img alt=\"60a97619_13cf_4aba_807e_0fad0a4f3b42\" src=\"https://github.com/user-attachments/assets/60a97619-13cf-4aba-807e-0fad0a4f3b42\" width=\"480\"/>\n</details>\n \n<details>\n<summary>Analyzing a Channel's Recent Performance</summary>\n<img alt=\"4f35a716_0c92_4368_8ba5_0b564613aae0\" src=\"https://github.com/user-attachments/assets/4f35a716-0c92-4368-8ba5-0b564613aae0\" width=\"480\"/>\n</details>\n\n\n\n## Installation\n\n\n### Installing via Smithery\n\nTo install youtubeinsights-mcp-server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@dabidstudio/youtubeinsights-mcp-server):\n\n```bash\nnpx -y @smithery/cli install @dabidstudio/youtubeinsights-mcp-server --client claude\n```\n\n### Using uvx (recommended)\n\nWhen using [`uvx`](https://docs.astral.sh/uv/guides/tools/), no specific installation is needed.\n\nAdd the following configuration to your MCP settings file (e.g., `claude_desktop_config.json` for Claude Desktop):\n\n```json\n{\n  \"mcpServers\": {\n    \"youtubeinsights\": {\n      \"command\": \"uvx\",\n      \"args\": [\"youtubeinsights-mcp-server\"],\n      \"env\": {\n        \"YOUTUBE_API_KEY\": \"your-api-key\",\n      }\n    }\n  }\n}\n```\n\n### Development Installation\n\n1. Clone this repository\n\n2. Copy `.env.example` to `.env` and fill in your youtube data api credentials\n\n    ```json\n    {\n      \"mcpServers\": {\n        \"youtubeinsights\": {\n          \"command\": \"uv\",\n          \"args\": [\n            \"--directory\",\n            \"path/to/youtubeinsights-mcp-server\",\n            \"run\",\n            \"youtubeinsights-mcp-server\"\n          ],\n          \"env\": {\n            \"YOUTUBE_API_KEY\": \"your-api-key\",\n          }\n        }\n      }\n    }\n    ```\n\n## Available MCP Tools\n\n- `get_youtube_transcript`: Extract full transcript (subtitles) from a YouTube video URL (supports `ko`, `en`)\n- `search_youtube_videos`: Search for videos on YouTube by keyword and retrieve key metadata\n- `get_channel_info`: Get channel metadata and recent uploads based on any YouTube video URL\n\n## Sample MCP Tool Descriptions\n\n```json\n{\n  \"tool\": \"get_youtube_transcript\",\n  \"description\": \"Extract subtitles from a given YouTube video URL.\"\n}\n```\n\n```json\n{\n  \"tool\": \"search_youtube_videos\",\n  \"description\": \"Search videos by keyword and return metadata including views, likes, and thumbnails.\"\n}\n```\n\n```json\n{\n  \"tool\": \"get_channel_info\",\n  \"description\": \"Retrieve channel info (title, subscriber count, latest uploads) based on a video URL.\"\n}\n```\n\n\n## License\n\nThis project is licensed under the MIT License. See the [LICENSE](LICENSE) file for details.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "youtubeinsights",
        "youtube",
        "videos",
        "youtubeinsights mcp",
        "dabidstudio youtubeinsights",
        "insights youtube"
      ],
      "category": "web-search"
    },
    "daheepk--arxiv-paper-mcp": {
      "owner": "daheepk",
      "name": "arxiv-paper-mcp",
      "url": "https://github.com/daheepk/arxiv-paper-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/daheepk.webp",
      "description": "Facilitates efficient search and exploration of research papers from arXiv.org, enabling users to access detailed metadata, conduct keyword and author-based searches, and generate prompts for summarization and comparison. Provides updates on trending topics across various research categories.",
      "stars": 8,
      "forks": 3,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-30T18:39:37Z",
      "readme_content": "# 🧠 arXiv Research Assistant MCP Server\n\n[![smithery badge](https://smithery.ai/badge/@daheepk/arxiv-paper-mcp)](https://smithery.ai/server/@daheepk/arxiv-paper-mcp)\n\n\nThis project is an MCP (Model Context Protocol) server built to interact with the vast arXiv.org paper database.\n\nIt allows clients like **Claude AI** to search, explore, and compare arXiv papers efficiently — all through a custom-built, local server. It’s built with **Python** and the **FastMCP** framework, and uses **uv** for lightweight package management.\n\n<table>\n  <tr>\n    <td>\n      <a href=\"https://glama.ai/mcp/servers/@daheepk/arxiv-mcp-server\">\n        <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@daheepk/arxiv-mcp-server/badge\" alt=\"arXiv Research Assistant Server MCP server\" />\n      </a>\n    </td>\n    <td style=\"vertical-align: top; padding-left: 10px;\">\n      <a href=\"https://mseep.ai/app/daheepk-arxiv-paper-mcp\">\n        <img src=\"https://mseep.net/pr/daheepk-arxiv-paper-mcp-badge.png\" alt=\"MseeP.ai Security Assessment Badge\" width=\"140\" />\n      </a>\n    </td>\n  </tr>\n</table>\n\n\n## ✨ Features\n\n- **🔍 Keyword-based Paper Search**  \n  Search arXiv papers by keywords, with options to sort by relevance or most recent.\n\n- **📚 Latest Papers by Category**  \n  Specify an arXiv category code (e.g., `cs.AI`, `math.AP`) to fetch the most recent papers in that field.\n\n- **📄 Paper Details Lookup**  \n  Fetch detailed metadata using a paper's arXiv ID: title, authors, abstract, categories, DOI, PDF link, and more.\n\n- **🧑‍🔬 Author-based Paper Search**  \n  Retrieve a list of papers published by a specific author.\n\n- **📊 Trend Analysis (Experimental)**  \n  Get an overview of trending keywords or topics based on recent papers in a category (currently uses mock data).\n\n- **📝 Summarization Prompt Generator**  \n  Dynamically generate prompts that help LLMs summarize a selected paper more effectively.\n\n- **🆚 Comparison Prompt Generator**  \n  Provide two paper IDs to generate a structured prompt for comparing their content.\n\n---\n\n## 🛠️ Tech Stack\n\n- Python 3.11+\n- [FastMCP](https://github.com/modelcontextprotocol/fastmcp)\n- uv (for dependency & environment management)\n- requests (for API communication)\n- xml.etree.ElementTree (for parsing XML responses)\n\n---\n\n## 🚀 Getting Started\n\n### Installing via Smithery\n\nTo install arXiv Research Assistant MCP Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/arxiv-paper-mcp):\n\n```bash\nnpx -y @smithery/cli install arxiv-paper-mcp --client claude\n```\n\n### Installation from PyPI\n\n```bash\nuv pip install arxiv-paper-mcp\n```\n\n### 🔧 Clone the repository (for development)\n```bash\ngit clone https://github.com/daheepk/arxiv-mcp-server.git\ncd arxiv-mcp-server\n```\n### 🔧 Install Dependencies (for development)\n\nUse `uv` to install all dependencies in editable mode:\n\n```bash\nuv pip install -e .\n```\n\n\n## ⚙️ How to Run\n\n### ▶️ Run the server (locally)\n\n```bash\narxiv-paper-mcp\n```\n\n## 🔌 Use with Claude\n\nTo use this MCP server with Claude, add the following JSON configuration to Claude's MCP settings:\n\n```json\n{\n  \"mcpServers\": {\n    \"arXivPaper\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"tool\",\n        \"run\",\n        \"arxiv-paper-mcp\"\n      ]\n    }\n  }\n}\n```\n\n## Project Structure\n```\narxiv-mcp-server/\n├── arxiv_mcp/              # Main package\n│   ├── __init__.py\n│   ├── app.py              # FastMCP app setup\n│   ├── server.py           # Server entry point\n│   ├── utils.py            # arXiv API communication logic\n│   ├── resources/          # MCP resources (categories, authors, etc.)\n│   ├── tools/              # MCP tools (search, detail lookup, trends)\n│   └── prompts/            # Prompt templates (summarize, compare)\n├── pyproject.toml          # Project config & dependencies\n└── README.md               # This file\n```\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "searches",
        "search",
        "research",
        "papers arxiv",
        "research papers",
        "arxiv paper"
      ],
      "category": "web-search"
    },
    "daniel-lxs--mcp-perplexity": {
      "owner": "daniel-lxs",
      "name": "mcp-perplexity",
      "url": "https://github.com/daniel-lxs/mcp-perplexity",
      "imageUrl": "/freedevtools/mcp/pfp/daniel-lxs.webp",
      "description": "Provides a Python-based interface to access the Perplexity API, enabling users to query responses, maintain chat history, and manage conversations while mimicking browser interactions with Perplexity Chat.",
      "stars": 63,
      "forks": 14,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-10-01T22:23:08Z",
      "readme_content": "# Perplexity Chat MCP Server\n\nThe Perplexity MCP Server provides a Python-based interface to the Perplexity API, offering tools for querying responses, maintaining chat history, and managing conversations. It supports model configuration via environment variables and stores chat data locally. Built with Python and setuptools, it's designed for integration with development environments.\n\nThe MCP Server is desined to mimick how users interact with the Perplexity Chat on their browser by allowing your models to ask questions, continue conversations, and list all your chats.\n\n[![smithery badge](https://smithery.ai/badge/@daniel-lxs/mcp-perplexity)](https://smithery.ai/server/@daniel-lxs/mcp-perplexity) [![Release and Publish](https://github.com/daniel-lxs/mcp-perplexity/actions/workflows/release.yml/badge.svg)](https://github.com/daniel-lxs/mcp-perplexity/actions/workflows/release.yml)\n\n\n\n<a href=\"https://glama.ai/mcp/servers/0nggjl0ohi\">\n  <img alt=\"badge\" width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/0nggjl0ohi/badge\" />\n</a>\n\n## Components\n\n### Tools\n\n- **ask_perplexity**: Request expert programming assistance through Perplexity. Focuses on coding solutions, error debugging, and technical explanations. Returns responses with source citations and alternative suggestions.\n- **chat_perplexity**: Maintains ongoing conversations with Perplexity AI. Creates new chats or continues existing ones with full history context. Returns chat ID for future continuation.\n- **list_chats_perplexity**: Lists all available chat conversations with Perplexity AI. Returns chat IDs, titles, and creation dates (displayed in relative time format, e.g., \"5 minutes ago\", \"2 days ago\"). Results are paginated with 50 chats per page.\n- **read_chat_perplexity**: Retrieves the complete conversation history for a specific chat. Returns the full chat history with all messages and their timestamps. No API calls are made to Perplexity - this only reads from local storage.\n\n## Key Features\n\n- **Model Configuration via Environment Variable:**  Allows you to specify the Perplexity model using the `PERPLEXITY_MODEL` environment variable for flexible model selection.\n\n  You can also specify `PERPLEXITY_MODEL_ASK` and `PERPLEXITY_MODEL_CHAT` to use different models for the `ask_perplexity` and `chat_perplexity` tools, respectively.\n\n  These will override `PERPLEXITY_MODEL`. You can check which models are available on the [Perplexity](https://docs.perplexity.ai/guides/model-cards) documentation.\n- **Persistent Chat History:** The `chat_perplexity` tool maintains ongoing conversations with Perplexity AI. Creates new chats or continues existing ones with full history context. Returns chat ID for future continuation.\n- **Streaming Responses with Progress Reporting:** Uses progress reporting to prevent timeouts on slow responses.\n\n## Quickstart\n\n### Prerequisites\n\nBefore using this MCP server, ensure you have:\n\n- Python 3.10 or higher\n- [uvx](https://docs.astral.sh/uv/#installation) package manager installed\n\nNote: Installation instructions for uvx are available [here](https://docs.astral.sh/uv/#installation).\n\n### Configuration for All Clients\n\nTo use this MCP server, configure your client with these settings (configuration method varies by client):\n\n```json\n\"mcpServers\": {\n  \"mcp-perplexity\": {\n    \"command\": \"uvx\",\n    \"args\": [\"mcp-perplexity\"],\n    \"env\": {\n      \"PERPLEXITY_API_KEY\": \"your-api-key\",\n      \"PERPLEXITY_MODEL\": \"sonar-pro\",\n      \"DB_PATH\": \"chats.db\"\n    }\n  }\n}\n```\n\n## Environment Variables\n\nConfigure the MCP Perplexity server using the following environment variables:\n\n| Variable | Description | Default Value | Required |\n|----------|-------------|---------------|----------|\n| `PERPLEXITY_API_KEY` | Your Perplexity API key | None | Yes |\n| `PERPLEXITY_MODEL` | Default model for interactions | `sonar-pro` | No |\n| `PERPLEXITY_MODEL_ASK` | Specific model for `ask_perplexity` tool | Uses `PERPLEXITY_MODEL` | No |\n| `PERPLEXITY_MODEL_CHAT` | Specific model for `chat_perplexity` tool | Uses `PERPLEXITY_MODEL` | No |\n| `DB_PATH` | Path to store chat history database | `chats.db` | No |\n| `WEB_UI_ENABLED` | Enable or disable web UI | `false` | No |\n| `WEB_UI_PORT` | Port for web UI | `8050` | No |\n| `WEB_UI_HOST` | Host for web UI | `127.0.0.1` | No |\n| `DEBUG_LOGS` | Enable detailed logging | `false` | No |\n\n#### Using Smithery CLI\n```bash\nnpx -y @smithery/cli@latest run @daniel-lxs/mcp-perplexity --config \"{\\\"perplexityApiKey\\\":\\\"pplx-abc\\\",\\\"perplexityModel\\\":\\\"sonar-pro\\\"}\"\n```\n\n## Usage\n\n### ask_perplexity\n\nThe `ask_perplexity` tool is used for specific questions, this tool doesn't maintain a chat history, every request is a new chat.\n\nThe tool will return a response from Perplexity AI using the `PERPLEXITY_MODEL_ASK` model if specified, otherwise it will use the `PERPLEXITY_MODEL` model.\n\n### chat_perplexity\n\nThe `chat_perplexity` tool is used for ongoing conversations, this tool maintains a chat history.\nA chat is identified by a chat ID, this ID is returned by the tool when a new chat is created. Chat IDs look like this: `wild-horse-12`.\n\nThis tool is useful for debugging, research, and any other task that requires a chat history.\n\nThe tool will return a response from Perplexity AI using the `PERPLEXITY_MODEL_CHAT` model if specified, otherwise it will use the `PERPLEXITY_MODEL` model.\n\n### list_chats_perplexity\nLists all available chat conversations.  It returns a paginated list of chats, showing the chat ID, title, and creation time (in relative format).  You can specify the page number using the `page` argument (defaults to 1, with 50 chats per page).\n\n### read_chat_perplexity\nRetrieves the complete conversation history for a given `chat_id`.  This tool returns all messages in the chat, including timestamps and roles (user or assistant). This tool does *not* make any API calls to Perplexity; it only reads from the local database.\n\n## Web UI\n\nThe MCP Perplexity server now includes a web interface for easier interaction and management of chats.\n\n### Features\n- Interactive chat interface\n- Chat history management\n- Real-time message display\n\n### Screenshots\n\n#### Chat List View\n![image](https://github.com/user-attachments/assets/a8aebd19-f58a-4d6c-988e-ea1c1ca7f174)\n\n#### Chat Interface\n![image](https://github.com/user-attachments/assets/627bfcdb-2214-47e6-a55e-3987737ad00f)\n\n### Accessing the Web UI\n\nWhen `WEB_UI_ENABLED` is set to `true`, the web UI will be available at `http://WEB_UI_HOST:WEB_UI_PORT`. \n\nBy default, this is `http://127.0.0.1:8050`.\n\n## Development\n\nThis project uses setuptools for development and builds. To get started:\n\n1. Create a virtual environment:\n   ```bash\n   python -m venv .venv\n   source .venv/bin/activate  # On Linux/macOS\n   # or\n   .venv\\Scripts\\activate  # On Windows\n   ```\n\n2. Install the project in editable mode with all dependencies:\n   ```bash\n   pip install -e .\n   ```\n\n3. Build the project:\n   ```bash\n   python -m build\n   ```\n\nThe virtual environment will contain all required dependencies for development.\n\n## Contributing\n\nThis project is open to contributions. Please see the [CONTRIBUTING.md](CONTRIBUTING.md) file for more information.\n\n## License\n\nThis project is licensed under the MIT License. See the [LICENSE](LICENSE) file for details.\n\n\n\n\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "perplexity",
        "python",
        "search",
        "perplexity api",
        "perplexity chat",
        "perplexity provides"
      ],
      "category": "web-search"
    },
    "datalayer--earthdata-mcp-server": {
      "owner": "datalayer",
      "name": "earthdata-mcp-server",
      "url": "https://github.com/datalayer/earthdata-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/datalayer.webp",
      "description": "Enables efficient discovery and retrieval of NASA Earth Data datasets and granules, facilitating geospatial analysis and data exploration. Integrates with workflows for AI-powered analysis of Earthdata datasets.",
      "stars": 21,
      "forks": 6,
      "license": "BSD 3-Clause \"New\" or \"Revised\" License",
      "language": "Python",
      "updated_at": "2025-09-19T09:33:45Z",
      "readme_content": "<!--\n  ~ Copyright (c) 2023-2024 Datalayer, Inc.\n  ~\n  ~ BSD 3-Clause License\n-->\n\n[![Datalayer](https://assets.datalayer.tech/datalayer-25.svg)](https://datalayer.io)\n\n[![Become a Sponsor](https://img.shields.io/static/v1?label=Become%20a%20Sponsor&message=%E2%9D%A4&logo=GitHub&style=flat&color=1ABC9C)](https://github.com/sponsors/datalayer)\n\n# 🪐 ✨ Earthdata MCP Server\n\n[![PyPI - Version](https://img.shields.io/pypi/v/earthdata-mcp-server)](https://pypi.org/project/earthdata-mcp-server)\n[![smithery badge](https://smithery.ai/badge/@datalayer/earthdata-mcp-server)](https://smithery.ai/server/@datalayer/earthdata-mcp-server)\n\nEarthdata MCP Server is a [Model Context Protocol](https://modelcontextprotocol.io/introduction) (MCP) server implementation that provides tools to interact with [NASA Earth Data](https://www.earthdata.nasa.gov/). It enables efficient dataset discovery, retrieval and analysis for Geospatial analysis.\n\n🚀 **NEW**: This server now includes all [Jupyter MCP Server](https://github.com/datalayer/jupyter-mcp-server) tools through composition, providing a unified interface for both Earth data discovery and analysis in Jupyter Notebooks.\n\n## 🚀 Key Features\n\n- **Efficient Data Retrieval**: Search and download Earthdata datasets\n- **Unified Interface**: Combines Earthdata research and Jupyter notebook manipulation tools for analysis\n\nThe following demo uses this MCP server to search for datasets and data granules on NASA Earthdata, download the data in Jupyter and run further analysis.\n\n<div>\n  <a href=\"https://www.loom.com/share/c2b5b05f548d4f1492d5c107f0c48dbc\">\n    <p>Analyzing Sea Level Rise with AI-Powered Geospatial Tools and Jupyter - Watch Video</p>\n  </a>\n  <a href=\"https://www.loom.com/share/c2b5b05f548d4f1492d5c107f0c48dbc\">\n    <img alt=\"c2b5b05f548d4f1492d5c107f0c48dbc_598a84f02de7e74e_full_play\" style=\"max-width:100%;\" src=\"https://cdn.loom.com/sessions/thumbnails/c2b5b05f548d4f1492d5c107f0c48dbc-598a84f02de7e74e-full-play.gif\">\n  </a>\n</div>\n\n## 🏁 Getting Started\n\nFor comprehensive setup instructions—including `Streamable HTTP` transport and advanced configuration—check out [the Jupyter MCP Server documentation](https://jupyter-mcp-server.datalayer.tech/). Or, get started quickly with `JupyterLab` and `stdio` transport here below.\n\n### 1. Set Up Your Environment\n\n```bash\npip install jupyterlab==4.4.1 jupyter-collaboration==4.0.2 ipykernel\npip uninstall -y pycrdt datalayer_pycrdt\npip install datalayer_pycrdt==0.12.17\n```\n\n### 2. Start JupyterLab\n\n```bash\n# make jupyterlab\njupyter lab --port 8888 --IdentityProvider.token MY_TOKEN --ip 0.0.0.0\n```\n\n### 3. Configure Your Preferred MCP Client\n\n> [!NOTE]\n>\n> Ensure the `port` of the `DOCUMENT_URL` and `RUNTIME_URL` match those used in the `jupyter lab` command.\n>\n> The `DOCUMENT_ID` which is the path to the notebook you want to connect to, should be relative to the directory where JupyterLab was started.\n>\n> In a basic setup, `DOCUMENT_URL` and `RUNTIME_URL` are the same. `DOCUMENT_TOKEN`, and `RUNTIME_TOKEN` are also the same and is actually the Jupyter Token.\n\n> [!NOTE]\n> \n> The `EARTHDATA_USERNAME` and `EARTHDATA_PASSWORD` environment variables are used for NASA Earthdata authentication to download datasets via the `earthaccess` library. See [NASA Earthdata Authentication](./docs/authentication.md) for more details.\n\n#### MacOS and Windows\n\n```json\n{\n  \"mcpServers\": {\n    \"earthdata\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"-i\",\n        \"--rm\",\n        \"-e\",\n        \"DOCUMENT_URL\",\n        \"-e\",\n        \"DOCUMENT_TOKEN\",\n        \"-e\",\n        \"DOCUMENT_ID\",\n        \"-e\",\n        \"RUNTIME_URL\",\n        \"-e\",\n        \"RUNTIME_TOKEN\",\n        \"datalayer/earthdata-mcp-server:latest\"\n      ],\n      \"env\": {\n        \"DOCUMENT_URL\": \"http://host.docker.internal:8888\",\n        \"DOCUMENT_TOKEN\": \"MY_TOKEN\",\n        \"DOCUMENT_ID\": \"notebook.ipynb\",\n        \"RUNTIME_URL\": \"http://host.docker.internal:8888\",\n        \"RUNTIME_TOKEN\": \"MY_TOKEN\",\n        \"EARTHDATA_USERNAME\": \"your_username\",\n        \"EARTHDATA_PASSWORD\": \"your_password\"\n      }\n    }\n  }\n}\n```\n\n#### Linux\n\n```json\n{\n  \"mcpServers\": {\n    \"earthdata\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"-i\",\n        \"--rm\",\n        \"-e\",\n        \"DOCUMENT_URL\",\n        \"-e\",\n        \"DOCUMENT_TOKEN\",\n        \"-e\",\n        \"DOCUMENT_ID\",\n        \"-e\",\n        \"RUNTIME_URL\",\n        \"-e\",\n        \"RUNTIME_TOKEN\",\n        \"--network=host\",\n        \"datalayer/earthdata-mcp-server:latest\"\n      ],\n      \"env\": {\n        \"DOCUMENT_URL\": \"http://localhost:8888\",\n        \"DOCUMENT_TOKEN\": \"MY_TOKEN\",\n        \"DOCUMENT_ID\": \"notebook.ipynb\",\n        \"RUNTIME_URL\": \"http://localhost:8888\",\n        \"RUNTIME_TOKEN\": \"MY_TOKEN\",\n        \"EARTHDATA_USERNAME\": \"your_username\",\n        \"EARTHDATA_PASSWORD\": \"your_password\"\n      }\n    }\n  }\n}\n```\n\n## Tools\n\nThe server offers **15 tools total**: 3 Earthdata-specific tools plus 12 Jupyter notebook manipulation tools.\n\n### Earthdata Tools\n\n#### `search_earth_datasets`\n\n- Search for datasets on NASA Earthdata.\n- Input:\n  - search_keywords (str): Keywords to search for in the dataset titles.\n  - count (int): Number of datasets to return.\n  - temporal (tuple): (Optional) Temporal range in the format (date_from, date_to).\n  - bounding_box (tuple): (Optional) Bounding box in the format (lower_left_lon, lower_left_lat, upper_right_lon, upper_right_lat).\n- Returns: List of dataset abstracts.\n\n#### `search_earth_datagranules`\n\n- Search for data granules on NASA Earthdata.\n- Input:\n  - short_name (str): Short name of the dataset.\n  - count (int): Number of data granules to return.\n  - temporal (tuple): (Optional) Temporal range in the format (date_from, date_to).\n  - bounding_box (tuple): (Optional) Bounding box in the format (lower_left_lon, lower_left_lat, upper_right_lon, upper_right_lat).\n- Returns: List of data granules.\n\n#### `download_earth_data_granules`\n\n- Download Earth data granules from NASA Earth Data and integrate with Jupyter notebooks.\n- This tool combines earthdata search capabilities with jupyter notebook manipulation to create a seamless download workflow.\n- **Authentication**: Requires NASA Earthdata Login credentials (see [Authentication section](#nasa-earthdata-authentication))\n- Input:\n  - folder_name (str): Local folder name to save the data.\n  - short_name (str): Short name of the Earth dataset to download.\n  - count (int): Number of data granules to download.\n  - temporal (tuple): (Optional) Temporal range in the format (date_from, date_to).\n  - bounding_box (tuple): (Optional) Bounding box in the format (lower_left_lon, lower_left_lat, upper_right_lon, upper_right_lat).\n- Returns: Success message with download code preparation details.\n\n### Jupyter Tools (Composed)\n\nThe following Jupyter notebook manipulation tools are available:\n\n- **`append_markdown_cell`**: Add markdown cells to notebooks\n- **`insert_markdown_cell`**: Insert markdown cells at specific positions\n- **`overwrite_cell_source`**: Modify existing cell content\n- **`append_execute_code_cell`**: Add and execute code cells\n- **`insert_execute_code_cell`**: Insert and execute code cells at specific positions\n- **`execute_cell_with_progress`**: Execute cells with progress monitoring\n- **`execute_cell_simple_timeout`**: Execute cells with timeout\n- **`execute_cell_streaming`**: Execute cells with streaming output\n- **`read_all_cells`**: Read all notebook cells\n- **`read_cell`**: Read specific notebook cells\n- **`get_notebook_info`**: Get notebook metadata\n- **`delete_cell`**: Delete notebook cells\n\nFor detailed documentation of the Jupyter tools, see the [Jupyter MCP Server documentation](https://github.com/datalayer/jupyter-mcp-server).\n\n## Prompts\n\n1. `download_analyze_global_sea_level` 🆕\n   - Generate a comprehensive workflow for downloading and analyzing Global Mean Sea Level Trend dataset.\n   - Uses both earthdata download tools and jupyter analysis capabilities.\n   - Returns: Detailed prompt for complete sea level analysis workflow.\n\n2. `sealevel_rise_dataset`\n   - Search for datasets related to sea level rise worldwide.\n   - Input:\n     - `start_year` (int): Start year to consider.\n      - `end_year` (int): End year to consider.\n   - Returns: Prompt correctly formatted.\n\n3. `ask_datasets_format`\n    - To ask about the format of the datasets.\n    - Returns: Prompt correctly formatted.\n\n## Building\n\n```bash\n# or run `docker build -t datalayer/earthdata-mcp-server .`\nmake build-docker\n```\n\nIf you prefer, you can pull the prebuilt images.\n\n```bash\nmake pull-docker\n```\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "earthdata",
        "datalayer",
        "datasets",
        "earthdata mcp",
        "earthdata datasets",
        "analysis earthdata"
      ],
      "category": "web-search"
    },
    "dazeb--markdown-downloader": {
      "owner": "dazeb",
      "name": "markdown-downloader",
      "url": "https://github.com/dazeb/markdown-downloader",
      "imageUrl": "/freedevtools/mcp/pfp/dazeb.webp",
      "description": "Download webpages and convert their content into markdown files. Features include a configurable download directory and automatic date-stamped filenames for organized storage.",
      "stars": 38,
      "forks": 15,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-10-01T23:57:02Z",
      "readme_content": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/dazeb-markdown-downloader-badge.jpg)](https://mseep.ai/app/dazeb-markdown-downloader)\n[![MseeP Badge](https://mseep.net/pr/dazeb-markdown-downloader-badge.jpg)](https://mseep.ai/app/dazeb-markdown-downloader)\n\n[![Verified on MseeP](https://mseep.ai/badge.svg)](https://mseep.ai/app/e85a9805-464e-46bd-a953-ccac0c4a5129)\n\n# Markdown Downloader MCP Server\n\n[![smithery badge](https://smithery.ai/badge/@dazeb/markdown-downloader)](https://smithery.ai/server/@dazeb/markdown-downloader)\n\n## Overview\n\nMarkdown Downloader is a powerful MCP (Model Context Protocol) server that allows you to download webpages as markdown files with ease. Leveraging the r.jina.ai service, this tool provides a seamless way to convert web content into markdown format.\n\n<a href=\"https://glama.ai/mcp/servers/jrki7zltg7\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/jrki7zltg7/badge\" alt=\"Markdown Downloader MCP server\" />\n</a>\n\n## Features\n\n- 🌐 Download webpages as markdown using r.jina.ai\n- 📁 Configurable download directory\n- 📝 Automatically generates date-stamped filenames\n- 🔍 List downloaded markdown files\n- 💾 Persistent configuration\n\n## Prerequisites\n\n- Node.js (version 16 or higher)\n- npm (Node Package Manager)\n\n## Installation\n\n### Installing via Smithery\n\nTo install Markdown Downloader for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@dazeb/markdown-downloader):\n\n```bash\nnpx -y @smithery/cli install @dazeb/markdown-downloader --client claude\n```\n\n### Installing manually\n\n1. Clone the repository:\n   ```bash\n   git clone https://github.com/your-username/markdown-downloader.git\n   cd markdown-downloader\n   ```\n\n2. Install dependencies:\n   ```bash\n   npm install\n   ```\n\n3. Build the project:\n   ```bash\n   npm run build\n   ```\n\n## Manually Add Server to Cline/Roo-Cline MCP Settings file\n\n### Linux/macOS\n```json\n{\n  \"mcpServers\": {\n    \"markdown-downloader\": {\n      \"command\": \"node\",\n      \"args\": [\n        \"/home/user/Documents/Cline/MCP/markdown-downloader/build/index.js\"\n      ],\n      \"disabled\": false,\n      \"alwaysAllow\": [\n        \"download_markdown\",\n        \"set_download_directory\"\n      ]\n    }\n  }\n}\n```\n\n### Windows\n```json\n{\n  \"mcpServers\": {\n    \"markdown-downloader\": {\n      \"command\": \"node\",\n      \"args\": [\n        \"C:\\\\Users\\\\username\\\\Documents\\\\Cline\\\\MCP\\\\markdown-downloader\\\\build\\\\index.js\"\n      ],\n      \"disabled\": false,\n      \"alwaysAllow\": [\n        \"download_markdown\",\n        \"set_download_directory\"\n      ]\n    }\n  }\n}\n```\n\n## Tools and Usage\n\n### 1. Set Download Directory\n\nChange the download directory:\n\n```bash\nuse set_download_directory /path/to/your/local/download/folder\n```\n\n- Validates directory exists and is writable\n- Persists the configuration for future use\n\n### 2. Download Markdown\n\nDownload a webpage as a markdown file:\n\n```bash\nuse tool download_markdown https://example.com/blog-post\n```\n\n- The URL will be prepended with `r.jina.ai`\n- Filename format: `{sanitized-url}-{date}.md`\n- Saved in the configured download directory\n\n### 3. List Downloaded Files\n\nList all downloaded markdown files:\n\n```bash\nuse list_downloaded_files\n```\n\n### 4. Get Download Directory\n\nRetrieve the current download directory:\n\n```bash\nuse get_download_directory\n```\n\n## Configuration\n\n### Linux/macOS\n- Configuration is stored in `~/.config/markdown-downloader/config.json`\n- Default download directory: `~/.markdown-downloads`\n\n### Windows\n- Configuration is stored in `%APPDATA%\\markdown-downloader\\config.json`\n- Default download directory: `%USERPROFILE%\\Documents\\markdown-downloads`\n\n## Troubleshooting\n\n- Ensure you have an active internet connection\n- Check that the URL is valid and accessible\n- Verify write permissions for the download directory\n\n## Security\n\n- The tool uses r.jina.ai to fetch markdown content\n- Local files are saved with sanitized filenames\n- Configurable download directory allows flexibility\n\n## Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n\n## License\n\nThis project is licensed under the MIT License. See the LICENSE file for details.\n\n## Disclaimer\n\nThis tool is provided as-is. Always review downloaded content for accuracy and appropriateness.\n\n## Support\n\nFor issues or feature requests, please open an issue on the GitHub repository.\n",
      "npm_url": "https://www.npmjs.com/package/markdown-downloader",
      "npm_downloads": 153,
      "keywords": [
        "markdown",
        "downloader",
        "dazeb",
        "markdown downloader",
        "dazeb markdown",
        "search dazeb"
      ],
      "category": "web-search"
    },
    "ddkang1--ddg-mcp": {
      "owner": "ddkang1",
      "name": "ddg-mcp",
      "url": "https://github.com/ddkang1/ddg-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/ddkang1.webp",
      "description": "Provides access to DuckDuckGo's search capabilities, including text, image, news, and video search. Summarizes search results and facilitates engaging conversations through AI chat.",
      "stars": 1,
      "forks": 0,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-06-05T02:02:28Z",
      "readme_content": "# ddg-mcp MCP server\n\nDuckDuckGo search API MCP - A server that provides DuckDuckGo search capabilities through the Model Context Protocol.\n\n## Components\n\n### Prompts\n\nThe server provides the following prompts:\n- **search-results-summary**: Creates a summary of DuckDuckGo search results\n  - Required \"query\" argument for the search term\n  - Optional \"style\" argument to control detail level (brief/detailed)\n\n### Tools\n\nThe server implements the following DuckDuckGo search tools:\n\n- **ddg-text-search**: Search the web for text results using DuckDuckGo\n  - Required: \"keywords\" - Search query keywords\n  - Optional: \"region\", \"safesearch\", \"timelimit\", \"max_results\"\n  \n- **ddg-image-search**: Search the web for images using DuckDuckGo\n  - Required: \"keywords\" - Search query keywords\n  - Optional: \"region\", \"safesearch\", \"timelimit\", \"size\", \"color\", \"type_image\", \"layout\", \"license_image\", \"max_results\"\n  \n- **ddg-news-search**: Search for news articles using DuckDuckGo\n  - Required: \"keywords\" - Search query keywords\n  - Optional: \"region\", \"safesearch\", \"timelimit\", \"max_results\"\n  \n- **ddg-video-search**: Search for videos using DuckDuckGo\n  - Required: \"keywords\" - Search query keywords\n  - Optional: \"region\", \"safesearch\", \"timelimit\", \"resolution\", \"duration\", \"license_videos\", \"max_results\"\n  \n- **ddg-ai-chat**: Chat with DuckDuckGo AI\n  - Required: \"keywords\" - Message or question to send to the AI\n  - Optional: \"model\" - AI model to use (options: \"gpt-4o-mini\", \"llama-3.3-70b\", \"claude-3-haiku\", \"o3-mini\", \"mistral-small-3\")\n\n## Installation\n\n### Prerequisites\n\n- Python 3.9 or higher\n- [uv](https://github.com/astral-sh/uv) (recommended) or pip\n\n### Install from PyPI\n\n```bash\n# Using uv\nuv install ddg-mcp\n\n# Using pip\npip install ddg-mcp\n```\n\n### Install from Source\n\n1. Clone the repository:\n```bash\ngit clone https://github.com/misanthropic-ai/ddg-mcp.git\ncd ddg-mcp\n```\n\n2. Install the package:\n```bash\n# Using uv\nuv install -e .\n\n# Using pip\npip install -e .\n```\n\n## Configuration\n\n### Required Dependencies\n\nThe server requires the `duckduckgo-search` package, which will be installed automatically when you install `ddg-mcp`.\n\nIf you need to install it manually:\n```bash\nuv install duckduckgo-search\n# or\npip install duckduckgo-search\n```\n\n## DuckDuckGo Search Parameters\n\n### Common Parameters\n\nThese parameters are available for most search types:\n\n- **region**: Region code for localized results (default: \"wt-wt\")\n  - Examples: \"us-en\" (US English), \"uk-en\" (UK English), \"ru-ru\" (Russian)\n  - See [DuckDuckGo regions](https://duckduckgo.com/params) for more options\n\n- **safesearch**: Content filtering level (default: \"moderate\")\n  - \"on\": Strict filtering\n  - \"moderate\": Moderate filtering\n  - \"off\": No filtering\n\n- **timelimit**: Time range for results\n  - \"d\": Last day\n  - \"w\": Last week\n  - \"m\": Last month\n  - \"y\": Last year (not available for news/videos)\n\n- **max_results**: Maximum number of results to return (default: 10)\n\n### Search Operators\n\nYou can use these operators in your search keywords:\n\n- `cats dogs`: Results about cats or dogs\n- `\"cats and dogs\"`: Results for exact term \"cats and dogs\"\n- `cats -dogs`: Fewer dogs in results\n- `cats +dogs`: More dogs in results\n- `cats filetype:pdf`: PDFs about cats (supported: pdf, doc(x), xls(x), ppt(x), html)\n- `dogs site:example.com`: Pages about dogs from example.com\n- `cats -site:example.com`: Pages about cats, excluding example.com\n- `intitle:dogs`: Page title includes the word \"dogs\"\n- `inurl:cats`: Page URL includes the word \"cats\"\n\n### Image Search Specific Parameters\n\n- **size**: \"Small\", \"Medium\", \"Large\", \"Wallpaper\"\n- **color**: \"color\", \"Monochrome\", \"Red\", \"Orange\", \"Yellow\", \"Green\", \"Blue\", \"Purple\", \"Pink\", \"Brown\", \"Black\", \"Gray\", \"Teal\", \"White\"\n- **type_image**: \"photo\", \"clipart\", \"gif\", \"transparent\", \"line\"\n- **layout**: \"Square\", \"Tall\", \"Wide\"\n- **license_image**: \"any\", \"Public\", \"Share\", \"ShareCommercially\", \"Modify\", \"ModifyCommercially\"\n\n### Video Search Specific Parameters\n\n- **resolution**: \"high\", \"standard\"\n- **duration**: \"short\", \"medium\", \"long\"\n- **license_videos**: \"creativeCommon\", \"youtube\"\n\n### AI Chat Models\n\n- **gpt-4o-mini**: OpenAI's GPT-4o mini model\n- **llama-3.3-70b**: Meta's Llama 3.3 70B model\n- **claude-3-haiku**: Anthropic's Claude 3 Haiku model\n- **o3-mini**: OpenAI's O3 mini model\n- **mistral-small-3**: Mistral AI's small model\n\n## Quickstart\n\n### Running the Server\n\nYou can run the server directly:\n\n```bash\nddg-mcp\n```\n\nBy default, it uses streamable-http transport. To use stdio transport:\n\n```bash\nddg-mcp --transport stdio\n```\n\nTo use streamable-http transport:\n\n```bash\nddg-mcp --transport streamable-http\n```\n\nYou can also specify host and port for SSE and streamable-http transports:\n\n```bash\nddg-mcp --host 0.0.0.0 --port 3001\n```\n\n### Install\n\n#### Claude Desktop\n\nOn MacOS: `~/Library/Application\\ Support/Claude/claude_desktop_config.json`\nOn Windows: `%APPDATA%/Claude/claude_desktop_config.json`\n\n<details>\n  <summary>Development/Unpublished Servers Configuration</summary>\n  ```\n  \"mcpServers\": {\n    \"ddg-mcp\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"--directory\",\n        \"/Users/shannon/Workspace/artivus/ddg-mcp\",\n        \"run\",\n        \"ddg-mcp\",\n        \"--transport\",\n        \"stdio\"\n      ]\n    }\n  }\n  ```\n</details>\n\n<details>\n  <summary>Published Servers Configuration</summary>\n  ```\n  \"mcpServers\": {\n    \"ddg-mcp\": {\n      \"command\": \"uvx\",\n      \"args\": [\n        \"ddg-mcp\",\n        \"--transport\",\n        \"stdio\"\n      ]\n    }\n  }\n  ```\n</details>\n\n## Usage Examples\n\n### Text Search\n\n```\nUse the ddg-text-search tool to search for \"climate change solutions\"\n```\n\nAdvanced example:\n```\nUse the ddg-text-search tool to search for \"renewable energy filetype:pdf site:edu\" with region \"us-en\", safesearch \"off\", timelimit \"y\", and max_results 20\n```\n\n### Image Search\n\n```\nUse the ddg-image-search tool to find images of \"renewable energy\" with color set to \"Green\"\n```\n\nAdvanced example:\n```\nUse the ddg-image-search tool to find images of \"mountain landscape\" with size \"Large\", color \"Blue\", type_image \"photo\", layout \"Wide\", and license_image \"Public\"\n```\n\n### News Search\n\n```\nUse the ddg-news-search tool to find recent news about \"artificial intelligence\" from the last day\n```\n\nAdvanced example:\n```\nUse the ddg-news-search tool to search for \"space exploration\" with region \"uk-en\", timelimit \"w\", and max_results 15\n```\n\n### Video Search\n\n```\nUse the ddg-video-search tool to find videos about \"machine learning tutorials\" with duration set to \"medium\"\n```\n\nAdvanced example:\n```\nUse the ddg-video-search tool to search for \"cooking recipes\" with resolution \"high\", duration \"short\", license_videos \"creativeCommon\", and max_results 10\n```\n\n### AI Chat\n\n```\nUse the ddg-ai-chat tool to ask \"What are the latest developments in quantum computing?\" using the claude-3-haiku model\n```\n\n### Search Results Summary\n\n```\nUse the search-results-summary prompt with query \"space exploration\" and style \"detailed\"\n```\n\n## Claude config\n\"ddg-mcp\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"--directory\",\n        \"/PATH/TO/YOUR/INSTALLATION/ddg-mcp\",\n        \"run\",\n        \"ddg-mcp\",\n        \"--transport\",\n        \"stdio\"\n      ]\n  },\n\n## Development\n\n### Building and Publishing\n\nTo prepare the package for distribution:\n\n1. Sync dependencies and update lockfile:\n```bash\nuv sync\n```\n\n2. Build package distributions:\n```bash\nuv build\n```\n\nThis will create source and wheel distributions in the `dist/` directory.\n\n3. Publish to PyPI:\n```bash\nuv publish\n```\n\nNote: You'll need to set PyPI credentials via environment variables or command flags:\n- Token: `--token` or `UV_PUBLISH_TOKEN`\n- Or username/password: `--username`/`UV_PUBLISH_USERNAME` and `--password`/`UV_PUBLISH_PASSWORD`\n\n### Automated Publishing with GitHub Actions\n\nThis repository includes a GitHub Actions workflow for automated publishing to PyPI. The workflow is triggered when:\n\n1. A new GitHub Release is created\n2. The workflow is manually triggered via the GitHub Actions interface\n\nTo set up automated publishing:\n\n1. Generate a PyPI API token:\n   - Go to https://pypi.org/manage/account/token/\n   - Create a new token with scope limited to the `ddg-mcp` project\n   - Copy the token value (you'll only see it once)\n\n2. Add the token to your GitHub repository secrets:\n   - Go to your repository on GitHub\n   - Navigate to Settings > Secrets and variables > Actions\n   - Click \"New repository secret\"\n   - Name: `PYPI_API_TOKEN`\n   - Value: Paste your PyPI token\n   - Click \"Add secret\"\n\n3. To publish a new version:\n   - Update the version number in `pyproject.toml`\n   - Create a new release on GitHub or manually trigger the workflow\n\n### Debugging\n\nSince MCP servers run over stdio, debugging can be challenging. For the best debugging\nexperience, we strongly recommend using the [MCP Inspector](https://github.com/modelcontextprotocol/inspector).\n\n\nYou can launch the MCP Inspector via [`npm`](https://docs.npmjs.com/downloading-and-installing-node-js-and-npm) with this command:\n\n```bash\nnpx @modelcontextprotocol/inspector uv --directory /path/to/your/ddg-mcp run ddg-mcp --transport stdio\n```\n\n\nUpon launching, the Inspector will display a URL that you can access in your browser to begin debugging.\n",
      "npm_url": "https://www.npmjs.com/package/ddg-mcp-server",
      "npm_downloads": 0,
      "keywords": [
        "duckduckgo",
        "ddkang1",
        "search",
        "duckduckgo search",
        "search ddkang1",
        "access duckduckgo"
      ],
      "category": "web-search"
    },
    "deepkl--mcp-searxng": {
      "owner": "deepkl",
      "name": "mcp-searxng",
      "url": "https://github.com/deepkl/mcp-searxng",
      "imageUrl": "/freedevtools/mcp/pfp/deepkl.webp",
      "description": "Integrates web search capabilities with support for executing general queries, news, and articles, allowing for pagination to tailor results. Enhances applications with robust search functionalities that can control result sizes and counts.",
      "stars": 1,
      "forks": 0,
      "license": "MIT License",
      "language": "",
      "updated_at": "2025-04-08T07:33:15Z",
      "readme_content": "# SearXNG MCP Server\n\nAn [MCP server](https://modelcontextprotocol.io/introduction) implementation that integrates the SearxNG API, providing web search capabilities.\n\n<a href=\"https://glama.ai/mcp/servers/0j7jjyt7m9\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/0j7jjyt7m9/badge\" alt=\"SearXNG Server MCP server\" /></a>\n\n[![smithery badge](https://smithery.ai/badge/@ihor-sokoliuk/server-searxng)](https://smithery.ai/server/@ihor-sokoliuk/server-searxng)\n\n## Features\n\n- **Web Search**: General queries, news, articles, with pagination.\n- **Pagination**: Control return size and result counts options.\n\n## Tools\n\n- **searxng_web_search**\n  - Execute web searches with pagination\n  - Inputs:\n    - `query` (string): Search terms\n    - `count` (number, optional): Results per page (default 20)\n    - `offset` (number, optional): Pagination offset (default 0)\n\n## Configuration\n\n### Setting the SEARXNG_URL\n\n1. Choose a SearxNG instance from the [list of public instances](https://searx.space/) or use your local environment.\n2. Set the `SEARXNG_URL` environment variable to the instance URL.\n3. The default `SEARXNG_URL` value is `http://localhost:8080`.\n\n### Usage with Claude Desktop\n\n### Installing via Smithery\n\nTo install SearxNG Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@ihor-sokoliuk/server-searxng):\n\n```bash\nnpx -y @smithery/cli install @ihor-sokoliuk/server-searxng --client claude\n```\n\n### NPX\n\n```json\n{\n  \"mcpServers\": {\n    \"searxng\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\"\n        \"<full path to mcp-searxng repo>/\"\n      ],\n      \"env\": {\n        \"SEARXNG_URL\": \"YOUR_SEARXNG_INSTANCE_URL\"\n      }\n    }\n  }\n}\n```\n\n### Docker\n\n#### Build\n\n```bash\ndocker build -t mcp-server-searxng:latest -f Dockerfile .\n```\n\n#### Use\n\nAdd this to your `claude_desktop_config.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"searxng\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"-i\",\n        \"--rm\",\n        \"-e\",\n        \"SEARXNG_URL\",\n        \"mcp-server-searxng:latest\"\n      ],\n      \"env\": {\n        \"SEARXNG_URL\": \"YOUR_SEARXNG_INSTANCE_URL\"\n      }\n    }\n  }\n}\n```\n\n\n## License\n\nThis MCP server is licensed under the MIT License. This means you are free to use, modify, and distribute the software, subject to the terms and conditions of the MIT License. For more details, please see the LICENSE file in the project repository.\n",
      "npm_url": "https://www.npmjs.com/package/mcp-searxng",
      "npm_downloads": 27918,
      "keywords": [
        "searxng",
        "search",
        "pagination",
        "search deepkl",
        "web search",
        "queries news"
      ],
      "category": "web-search"
    },
    "delorenj--mcp-server-ticketmaster": {
      "owner": "delorenj",
      "name": "mcp-server-ticketmaster",
      "url": "https://github.com/delorenj/mcp-server-ticketmaster",
      "imageUrl": "/freedevtools/mcp/pfp/delorenj.webp",
      "description": "Provides tools for discovering events, venues, and attractions via the Ticketmaster Discovery API, enabling flexible searches based on keywords, dates, and locations, with structured outputs for programmatic access or human-readable formats.",
      "stars": 22,
      "forks": 11,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-09-20T15:06:18Z",
      "readme_content": "# MCP Server for Ticketmaster\n[![smithery badge](https://smithery.ai/badge/mcp-server-ticketmaster)](https://smithery.ai/server/mcp-server-ticketmaster)\n\nA Model Context Protocol server that provides tools for discovering events, venues, and attractions through the Ticketmaster Discovery API.\n\n<a href=\"https://glama.ai/mcp/servers/u91gv8f3on\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/u91gv8f3on/badge\" alt=\"Server for Ticketmaster Events MCP server\" /></a>\n\n## Features\n\n- Search for events, venues, and attractions with flexible filtering:\n  - Keyword search\n  - Date range for events\n  - Location (city, state, country)\n  - Venue-specific searches\n  - Attraction-specific searches\n  - Event classifications/categories\n- Output formats:\n  - Structured JSON data for programmatic use\n  - Human-readable text for direct consumption\n- Comprehensive data including:\n  - Names and IDs\n  - Dates and times (for events)\n  - Price ranges (for events)\n  - URLs\n  - Images\n  - Locations and addresses (for venues)\n  - Classifications (for attractions)\n\n## Installation\n\n### Installing via Smithery\n\nTo install mcp-server-ticketmaster for Claude Desktop automatically via [Smithery](https://smithery.ai/server/mcp-server-ticketmaster):\n\n```bash\nnpx -y @smithery/cli install mcp-server-ticketmaster --client claude\n```\n\n### Manual Installation\n```bash\nnpx -y install @delorenj/mcp-server-ticketmaster\n```\n\n## Configuration\n\nThe server requires a Ticketmaster API key. You can get one by:\n1. Going to https://developer.ticketmaster.com/\n2. Creating an account or signing in\n3. Going to \"My Apps\" in your account\n4. Creating a new app to get your API key\n\nSet your API key in your MCP settings file:\n\n```json\n{\n  \"mcpServers\": {\n    \"ticketmaster\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@delorenj/mcp-server-ticketmaster\"],\n      \"env\": {\n        \"TICKETMASTER_API_KEY\": \"your-api-key-here\"\n      }\n    }\n  }\n}\n```\n\n## Usage\n\nThe server provides a tool called `search_ticketmaster` that accepts:\n\n### Required Parameters\n- `type`: Type of search ('event', 'venue', or 'attraction')\n\n### Optional Parameters\n- `keyword`: Search term\n- `startDate`: Start date in YYYY-MM-DD format (for events)\n- `endDate`: End date in YYYY-MM-DD format (for events)\n- `city`: City name\n- `stateCode`: State code (e.g., 'NY')\n- `countryCode`: Country code (e.g., 'US')\n- `venueId`: Specific venue ID\n- `attractionId`: Specific attraction ID\n- `classificationName`: Event category (e.g., 'Sports', 'Music')\n- `format`: Output format ('json' or 'text', defaults to 'json')\n\n### Examples\n\n#### Structured JSON Output (Default)\n```\n<use_mcp_tool>\n<server_name>ticketmaster</server_name>\n<tool_name>search_ticketmaster</tool_name>\n<arguments>\n{\n  \"type\": \"event\",\n  \"keyword\": \"concert\",\n  \"startDate\": \"2025-02-01\",\n  \"endDate\": \"2025-02-28\",\n  \"city\": \"New York\",\n  \"stateCode\": \"NY\"\n}\n</arguments>\n</use_mcp_tool>\n```\n\n#### Human-Readable Text Output\n```\n<use_mcp_tool>\n<server_name>ticketmaster</server_name>\n<tool_name>search_ticketmaster</tool_name>\n<arguments>\n{\n  \"type\": \"event\",\n  \"keyword\": \"concert\",\n  \"startDate\": \"2025-02-01\",\n  \"endDate\": \"2025-02-28\",\n  \"city\": \"New York\",\n  \"stateCode\": \"NY\",\n  \"format\": \"text\"\n}\n</arguments>\n</use_mcp_tool>\n```\n\n## Development\n\n1. Clone the repository\n2. Copy the example environment file:\n   ```bash\n   cp .env.example .env\n   ```\n3. Add your Ticketmaster API key to `.env`\n4. Install dependencies:\n   ```bash\n   npm install\n   ```\n5. Build the project:\n   ```bash\n   npm run build\n   ```\n6. Test with the inspector:\n   ```bash\n   npm run inspector\n   ```\n\n## Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request. For major changes, please open an issue first to discuss what you would like to change.\n\n## License\n\nMIT License - see [LICENSE](LICENSE) file for details\n",
      "npm_url": "https://www.npmjs.com/package/mcp-server-ticketmaster",
      "npm_downloads": 393,
      "keywords": [
        "ticketmaster",
        "searches",
        "search",
        "ticketmaster discovery",
        "server ticketmaster",
        "ticketmaster provides"
      ],
      "category": "web-search"
    },
    "devabdultech--hn-mcp": {
      "owner": "devabdultech",
      "name": "hn-mcp",
      "url": "https://github.com/devabdultech/hn-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/devabdultech.webp",
      "description": "Access real-time stories, comments, and user profiles from Hacker News, allowing search and retrieval of data through a robust integration with the Model Context Protocol.",
      "stars": 15,
      "forks": 3,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-08-17T17:33:20Z",
      "readme_content": "# Hacker News MCP Server\n\n[![smithery badge](https://smithery.ai/badge/@devabdultech/hn-mcp)](https://smithery.ai/server/@devabdultech/hn-mcp)\nOfficial Hacker News MCP Server - Adds powerful Hacker News integration to Cursor, Claude, and any other LLM clients. Access stories, comments, user profiles, and search functionality through the Model Context Protocol.\n\n<a href=\"https://glama.ai/mcp/servers/73uji99mwg\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/73uji99mwg/badge\" alt=\"Hacker News Server MCP server\" />\n</a>\n\n## Features\n\n- Search stories and comments using Algolia's HN Search API\n- Get stories by type (top, new, best, ask, show, job)\n- Get individual stories with comments\n- Get comment trees and user discussions\n- Get user profiles and submissions\n- Real-time access to Hacker News data\n\n## Set Up\n\n### Running on Claude Desktop\n\nAdd this to your `claude_desktop_config.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"hackernews\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@devabdultech/hn-mcp-server\"]\n    }\n  }\n}\n```\n\n### Installing via Smithery\n\nTo install Hacker News MCP Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@devabdultech/hn-mcp):\n\n```bash\nnpx -y @smithery/cli install @devabdultech/hn-mcp --client claude\n```\n\n## Tools\n\n1. `search`\n   * Search for stories and comments on Hacker News using Algolia's search API\n   * Inputs:\n         * `query` (string): Search query\n         * `type` (optional string): Filter by type ('story' or 'comment')\n         * `page` (optional number): Page number for pagination\n         * `hitsPerPage` (optional number): Results per page (max 100)\n   * Returns: Search results with stories and comments\n\n2. `getStories`\n   * Get multiple stories by type (top, new, best, ask, show, job)\n   * Inputs:\n         * `type` (string): Type of stories to fetch ('top', 'new', 'best', 'ask', 'show', 'job')\n         * `limit` (optional number): Number of stories to fetch (max 100)\n   * Returns: Array of story objects\n\n3. `getStoryWithComments`\n   * Get a story along with its comment thread\n   * Inputs:\n         * `id` (number): Story ID\n   * Returns: Story details with nested comments\n\n4. `getCommentTree`\n   * Get the full comment tree for a story\n   * Inputs:\n         * `storyId` (number): ID of the story\n   * Returns: Hierarchical comment tree structure\n\n5. `getUser`\n   * Get a user's profile information\n   * Inputs:\n         * `id` (string): Username\n   * Returns: User profile details including karma, created date, and about text\n\n6. `getUserSubmissions`\n   * Get a user's submissions (stories and comments)\n   * Inputs:\n         * `id` (string): Username\n   * Returns: Array of user's submitted stories and comments\n\n\n### Contributing\n\n1. Fork the repository\n2. Create your feature branch\n3. Commit your changes\n4. Push to the branch\n5. Create a new Pull Request\n\n## License\n\nThis MCP server is licensed under the MIT License. See the LICENSE file for details.\n\n## About\n\nThis MCP server is built and maintained by [devabdultech](https://github.com/devabdultech). It uses the official Hacker News API and Algolia Search API to provide comprehensive access to Hacker News data through the Model Context Protocol.\n",
      "npm_url": "https://www.npmjs.com/package/hn-mcp",
      "npm_downloads": 221,
      "keywords": [
        "search",
        "hacker",
        "web",
        "hacker news",
        "web search",
        "search retrieval"
      ],
      "category": "web-search"
    },
    "divslingerx--mcp-server": {
      "owner": "divslingerx",
      "name": "mcp-server",
      "url": "https://github.com/divslingerx/mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/divslingerx.webp",
      "description": "Provides web search capabilities using Puppeteer, returning structured JSON results from Google searches in a lightweight and stateless design.",
      "stars": 0,
      "forks": 4,
      "license": "No License",
      "language": "JavaScript",
      "updated_at": "2025-01-20T18:03:10Z",
      "readme_content": "# Memory Store MCP Server\n\nA Model Context Protocol (MCP) server that provides web search capabilities using Puppeteer.\n\n## Features\n\n- Web search functionality via Google\n- Structured JSON results\n- Lightweight and stateless design\n- Easy integration with MCP-enabled systems\n\n## Installation\n\n1. Clone the repository:\n\n   ```bash\n   git clone https://github.com/yourusername/mcp-server.git\n   cd mcp-server\n   ```\n\n2. Install dependencies:\n\n   ```bash\n   npm install\n   ```\n\n3. Build the project:\n   ```bash\n   npm run build\n   ```\n\n## Configuration\n\nCreate a `.env` file in the project root with the following environment variables:\n\n```env\n# Puppeteer configuration\nPUPPETEER_EXECUTABLE_PATH=/path/to/chrome\nPUPPETEER_HEADLESS=true\n\n# Server settings\nPORT=3000\n```\n\n## Usage\n\nStart the server:\n\n```bash\nnpm start\n```\n\nThe server will be available to MCP clients. Example usage through MCP:\n\n```json\n{\n  \"tool\": \"search_web\",\n  \"arguments\": {\n    \"query\": \"example search\"\n  }\n}\n```\n\n## Development\n\n### Building the Project\n\n```bash\nnpm run build\n```\n\n### Running Tests\n\n```bash\nnpm test\n```\n\n### Linting\n\n```bash\nnpm run lint\n```\n\n### Formatting\n\n```bash\nnpm run format\n```\n\n## License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n",
      "npm_url": "https://www.npmjs.com/package/mcp-server",
      "npm_downloads": 29732,
      "keywords": [
        "puppeteer",
        "searches",
        "search",
        "using puppeteer",
        "search divslingerx",
        "searches lightweight"
      ],
      "category": "web-search"
    },
    "doomdagadiggiedahdah--iacr-mcp-server": {
      "owner": "doomdagadiggiedahdah",
      "name": "iacr-mcp-server",
      "url": "https://github.com/doomdagadiggiedahdah/iacr-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/doomdagadiggiedahdah.webp",
      "description": "Provides a programmatic interface to retrieve and search for cryptographic research papers from the IACR Cryptology ePrint Archive, along with access to paper metadata.",
      "stars": 2,
      "forks": 7,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-05-16T06:16:51Z",
      "readme_content": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/doomdagadiggiedahdah-iacr-mcp-server-badge.png)](https://mseep.ai/app/doomdagadiggiedahdah-iacr-mcp-server)\n\n# IACR Cryptology ePrint Archive MCP Server\n\n[![smithery badge](https://smithery.ai/badge/iacr-mcp-server)](https://smithery.ai/server/iacr-mcp-server)\n\n## Overview\n\nThis Model Context Protocol (MCP) server provides a programmatic interface to the IACR Cryptology ePrint Archive, enabling efficient retrieval of cryptographic research papers.\n\n<a href=\"https://glama.ai/mcp/servers/e2oh3a96de\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/e2oh3a96de/badge\" alt=\"IACR Server MCP server\" /></a>\n\n## Features\n\n- 🔍 Search cryptographic papers\n- 📋 Retrieve paper metadata\n- 🔒 Secure access to research publications\n\n## Prerequisites\n\n- Node.js (v16+)\n- npm or yarn\n\n## Installation\n\n### Installing via Smithery\n\nTo install IACR Cryptology ePrint Archive for Claude Desktop automatically via [Smithery](https://smithery.ai/server/iacr-mcp-server):\n\n```bash\nnpx -y @smithery/cli install iacr-mcp-server --client claude\n```\n\n### Manual Installation\n```bash\ngit clone https://github.com/yourusername/iacr-mcp-server.git\ncd iacr-mcp-server\nnpm install\n```\n\n## Configuration\n\nNo additional configuration is required. The server uses the IACR ePrint Archive's RSS feed for data retrieval.\n\n## Usage\n\n### Available Tools\n\n1. `search_papers`: Search for papers\n   - Parameters:\n     - `query`: Search term (required)\n     - `year`: Publication year (optional)\n     - `max_results`: Maximum number of results (default: 20)\n\n2. `get_paper_details`: Retrieve details for a specific paper\n   - Parameters:\n     - `paper_id`: Unique paper identifier (required)\n\n## Contributing\n\n1. Fork the repository\n2. Create a feature branch\n3. Commit your changes\n4. Push to the branch\n5. Create a Pull Request\n\n## Disclaimer\n\nThis is an unofficial tool. Always refer to the original IACR Cryptology ePrint Archive for the most accurate and up-to-date research publications.\n\n## Contact\n\nFor issues, questions, or suggestions, please open a GitHub issue.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "cryptology",
        "search",
        "archive",
        "search cryptographic",
        "iacr cryptology",
        "cryptology eprint"
      ],
      "category": "web-search"
    },
    "doronaviguy--mpc-0x": {
      "owner": "doronaviguy",
      "name": "mpc-0x",
      "url": "https://github.com/doronaviguy/mpc-0x",
      "imageUrl": "/freedevtools/mcp/pfp/doronaviguy.webp",
      "description": "Provides real-time information about Ethereum addresses across multiple chains and fetches address data seamlessly through an easy-to-use API.",
      "stars": 0,
      "forks": 1,
      "license": "No License",
      "language": "JavaScript",
      "updated_at": "2025-03-16T08:01:48Z",
      "readme_content": "# MCP Ethereum Address Info Server\n\nThis server provides information about Ethereum addresses across multiple chains using the Model Context Protocol (MCP). It includes a Server-Sent Events (SSE) endpoint for real-time updates.\n\n## Table of Contents\n\n- [Setup](#setup)\n- [Running the Server](#running-the-server)\n- [Available Endpoints](#available-endpoints)\n- [Using the SSE Endpoint](#using-the-sse-endpoint)\n- [Testing with Curl](#testing-with-curl)\n- [Example Workflow](#example-workflow)\n\n## Setup\n\n1. Clone the repository:\n   ```bash\n   git clone <repository-url>\n   cd mcp-0x-address\n   ```\n\n2. Install dependencies:\n   ```bash\n   npm install\n   ```\n\n3. Create a `.env` file with the following variables:\n   ```\n   MCP_PORT=3002\n   ```\n\n## Running the Server\n\nStart the HTTP MCP server:\n\n```bash\nnpm run start:http\n```\n\nThis will start the server on port 3002 (or the port specified in your `.env` file).\n\n## Available Endpoints\n\nThe server provides the following endpoints:\n\n- `GET /health` - Server health check\n- `POST /mcp` - MCP endpoint for tool calls\n- `GET /sse` - Server-Sent Events endpoint for real-time updates\n- `GET /sse/clients` - Get information about connected SSE clients\n- `POST /sse/subscribe/:clientId` - Subscribe to address updates\n- `POST /sse/unsubscribe/:clientId` - Unsubscribe from address updates\n\n## Using the SSE Endpoint\n\nThe SSE endpoint allows clients to receive real-time updates from the server. Here's how to use it:\n\n1. Connect to the SSE endpoint\n2. Get your client ID from the connection response\n3. Subscribe to specific addresses\n4. Receive real-time updates for those addresses\n\n## Testing with Curl\n\n### 1. Connect to the SSE Endpoint\n\n```bash\ncurl -N http://localhost:3002/sse\n```\n\nThis will establish a connection to the SSE endpoint and start receiving events. The connection will remain open until you manually terminate it.\n\n### 2. Check Connected Clients\n\n```bash\ncurl http://localhost:3002/sse/clients\n```\n\n### 3. Subscribe to Address Updates\n\nAfter connecting to the SSE endpoint, you'll receive a client ID. Use that ID to subscribe to address updates:\n\n```bash\ncurl -X POST \\\n  http://localhost:3002/sse/subscribe/YOUR_CLIENT_ID \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"addresses\": [\"0x742d35Cc6634C0532925a3b844Bc454e4438f44e\", \"0xC02aaA39b223FE8D0A0e5C4F27eAD9083C756Cc2\"]}'\n```\n\nReplace `YOUR_CLIENT_ID` with the client ID you received when connecting to the SSE endpoint.\n\n### 4. Unsubscribe from Address Updates\n\n```bash\ncurl -X POST \\\n  http://localhost:3002/sse/unsubscribe/YOUR_CLIENT_ID \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"addresses\": [\"0x742d35Cc6634C0532925a3b844Bc454e4438f44e\"]}'\n```\n\n### 5. Trigger an Address Update\n\nTo trigger an address update (which will be sent to subscribed clients), call the `get-address-info` tool:\n\n```bash\ncurl -X POST \\\n  http://localhost:3002/mcp \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"jsonrpc\": \"2.0\",\n    \"id\": 1,\n    \"method\": \"tools/call\",\n    \"params\": {\n      \"name\": \"get-address-info\",\n      \"arguments\": {\n        \"address\": \"0x742d35Cc6634C0532925a3b844Bc454e4438f44e\"\n      }\n    }\n  }'\n```\n\n### 6. Check Server Health\n\n```bash\ncurl http://localhost:3002/health\n```\n\n### 7. Test the Ping Tool\n\n```bash\ncurl -X POST \\\n  http://localhost:3002/mcp \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"jsonrpc\": \"2.0\",\n    \"id\": 1,\n    \"method\": \"tools/call\",\n    \"params\": {\n      \"name\": \"ping\",\n      \"arguments\": {}\n    }\n  }'\n```\n\n## Example Workflow\n\nHere's a complete workflow for testing the SSE functionality:\n\n1. Start the server:\n   ```bash\n   npm run start:http\n   ```\n\n2. In a new terminal, connect to the SSE endpoint:\n   ```bash\n   curl -N http://localhost:3002/sse\n   ```\n\n   You'll receive a response like:\n   ```\n   data: {\"type\":\"connection\",\"clientId\":\"client-1234567890abcdef\",\"message\":\"Connected to MCP SSE endpoint\",\"timestamp\":\"2023-01-01T00:00:00.000Z\"}\n   ```\n\n3. Note the `clientId` from the response.\n\n4. In another terminal, subscribe to address updates:\n   ```bash\n   curl -X POST \\\n     http://localhost:3002/sse/subscribe/client-1234567890abcdef \\\n     -H \"Content-Type: application/json\" \\\n     -d '{\"addresses\": [\"0x742d35Cc6634C0532925a3b844Bc454e4438f44e\"]}'\n   ```\n\n5. Trigger an address update:\n   ```bash\n   curl -X POST \\\n     http://localhost:3002/mcp \\\n     -H \"Content-Type: application/json\" \\\n     -d '{\n       \"jsonrpc\": \"2.0\",\n       \"id\": 1,\n       \"method\": \"tools/call\",\n       \"params\": {\n         \"name\": \"get-address-info\",\n         \"arguments\": {\n           \"address\": \"0x742d35Cc6634C0532925a3b844Bc454e4438f44e\"\n         }\n       }\n     }'\n   ```\n\n6. In the terminal where you're connected to the SSE endpoint, you'll see updates for the address.\n\n## Automated Testing Script\n\nFor a more automated test, you can use this bash script:\n\n```bash\n#!/bin/bash\n\n# Start SSE connection in the background and capture the output\ncurl -N http://localhost:3002/sse > sse_output.txt &\nSSE_PID=$!\n\n# Wait a moment for the connection to establish\nsleep 2\n\n# Extract the client ID from the output\nCLIENT_ID=$(grep -o '\"clientId\":\"[^\"]*\"' sse_output.txt | head -1 | cut -d'\"' -f4)\n\nif [ -z \"$CLIENT_ID\" ]; then\n  echo \"Failed to get client ID\"\n  kill $SSE_PID\n  exit 1\nfi\n\necho \"Connected with client ID: $CLIENT_ID\"\n\n# Subscribe to an address\ncurl -X POST \\\n  http://localhost:3002/sse/subscribe/$CLIENT_ID \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"addresses\": [\"0x742d35Cc6634C0532925a3b844Bc454e4438f44e\"]}'\n\necho \"Subscribed to address. Waiting for updates...\"\necho \"Press Ctrl+C to stop\"\n\n# Keep the script running to see updates\ntail -f sse_output.txt\n\n# Clean up on exit\ntrap \"kill $SSE_PID; rm sse_output.txt\" EXIT\n```\n\nSave this as `test_sse.sh`, make it executable with `chmod +x test_sse.sh`, and run it with `./test_sse.sh`.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "addresses",
        "mpc",
        "address",
        "address data",
        "ethereum addresses",
        "doronaviguy mpc"
      ],
      "category": "web-search"
    },
    "dragons96--mcp-undetected-chromedriver": {
      "owner": "dragons96",
      "name": "mcp-undetected-chromedriver",
      "url": "https://github.com/dragons96/mcp-undetected-chromedriver",
      "imageUrl": "/freedevtools/mcp/pfp/dragons96.webp",
      "description": "Automates control of the Chrome browser while successfully bypassing anti-bot detection mechanisms using undetected-chromedriver. Provides a comprehensive interface for various automated web interactions.",
      "stars": 6,
      "forks": 2,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-21T03:20:52Z",
      "readme_content": "# MCP-Undetected-Chromedriver\n\n[![smithery badge](https://smithery.ai/badge/@dragons96/mcp-undetected-chromedriver)](https://smithery.ai/server/@dragons96/mcp-undetected-chromedriver)\n\nAn MCP service built on undetected-chromedriver, providing a comprehensive interface for automating Chrome browser control while bypassing anti-bot detection.\n\n[中文文档](README_ZH.md)\n\n## Installation\n\n\nTo install MCP-Undetected-Chromedriver for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@dragons96/mcp-undetected-chromedriver):\n\n```bash\nnpx -y @smithery/cli install @dragons96/mcp-undetected-chromedriver --client claude\n```\n\n## Configuration to use Undetected Chromedriver Server\n\nHere's the Claude Desktop configuration to use the Undetected-chromedriver server:\n\n```json\n{\n  \"mcpServers\": {\n    \"mcp-undetected-chromedriver\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"@smithery/cli@latest\",\n        \"run\",\n        \"@dragons96/mcp-undetected-chromedriver\",\n        \"--config\",\n        \"{}\"\n      ]\n    }\n  }\n}\n```\n\n\n### Requirements\n\n- Python >= 3.11\n- Chrome browser\n\n### Installation with uv\n\n```bash\n# Create virtual environment\nuv venv\n\n# Activate virtual environment\n# Windows\n.venv\\Scripts\\activate\n# Linux/MacOS\nsource .venv/bin/activate\n\n# Install dependencies\nuv pip install -e .\n```\n\n## Project Introduction\n\nMCP-Undetected-Chromedriver is an MCP (Multi Channel Protocol) service that wraps the functionality of the undetected-chromedriver library into a series of easy-to-use APIs. This project is particularly suitable for scenarios that require bypassing modern website anti-bot detection mechanisms in automated testing, data scraping, or web automation scripts.\n\n### Key Features\n\n- Based on undetected-chromedriver, effectively bypassing website anti-bot detection\n- Provides rich browser operation API interfaces\n- Supports screenshots, PDF export, and other functionalities\n- Supports complex page interaction operations such as clicking, form filling, dragging, etc.\n- Seamlessly integrates with other tools in the MCP ecosystem\n\n## Todo List\n\n- [ ] Optimize browser driver management and handle driver interruptions\n- [ ] Extend API capabilities\n- [ ] Add more comprehensive error handling and logging\n- [ ] Improve documentation with more usage examples\n- [ ] Add support for browser profiles and extensions\n\n## Usage\n\n### Starting the Service\n\n```bash\nmcp-server-undetected-chromedriver\n```\n\n### Available APIs\n\nThe service provides the following main API interfaces:\n\n- `browser_navigate`: Navigate to a specified URL\n- `browser_screenshot`: Take a screenshot of the current page\n- `browser_click`: Click on page elements\n- `browser_iframe_click`: Click on elements within an iframe\n- `browser_fill`: Fill content in input fields\n- `browser_select`: Select options in dropdown selection boxes\n- `browser_hover`: Hover the mouse over elements\n- `browser_evalute`: Execute JavaScript code\n- `browser_close`: Close the browser\n- `browser_get_visible_text`: Get visible text on the page\n- `browser_get_visible_html`: Get visible HTML on the page\n- `browser_go_back`: Navigate backward in browser history\n- `browser_go_forward`: Navigate forward in browser history\n- `browser_drag`: Drag elements\n- `browser_press_key`: Simulate key presses\n- `browser_save_as_pdf`: Save the page as a PDF\n\n### Code Example\n\n```python\nfrom mcp.client import Client\n\n# Create MCP client\nclient = Client()\nclient.start(\"undetected-chromedriver-mcp-server\")\n\n# Navigate to website\nresponse = client.call(\"browser_navigate\", {\"url\": \"https://example.com\"})\nprint(response)\n\n# Take a screenshot\nresponse = client.call(\"browser_screenshot\", {\"name\": \"example\"})\nprint(response)\n\n# Get page text\nresponse = client.call(\"browser_get_visible_text\")\nprint(response.content[0].text)\n\n# Close the browser\nclient.call(\"browser_close\")\n```\n\n## How It Works\n\nThis service uses the undetected-chromedriver library to create a specialized Chrome browser instance that effectively evades common anti-bot detection mechanisms. The service wraps these features through the MCP protocol, providing an easy-to-use API interface that makes automated testing and web scraping more convenient.\n\n## License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n\n## Contribution Guidelines\n\nBug reports and feature requests are welcome on the GitHub Issues page. If you want to contribute code, please create an issue to discuss your ideas first.\n\n## FAQ\n\n**Q: Why choose undetected-chromedriver instead of the standard selenium webdriver?**\n\nA: undetected-chromedriver is specifically designed to bypass anti-bot detection mechanisms of modern websites, such as Cloudflare, Distil Networks, etc., making it more reliable for data scraping and automated testing scenarios.\n\n**Q: How does the service handle browser instances?**\n\nA: The service maintains a global browser instance, which is automatically created when an API requiring a browser is first called. The browser can be explicitly closed using the `browser_close` API.\n\n**Q: How to handle elements within iframes?**\n\nA: The `browser_iframe_click` API can directly operate on elements within iframes, without the need to manually switch frame contexts.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "chromedriver",
        "chrome",
        "browser",
        "chromedriver automates",
        "chromedriver provides",
        "control chrome"
      ],
      "category": "web-search"
    },
    "enemyrr--mcp-server-pagespeed": {
      "owner": "enemyrr",
      "name": "mcp-server-pagespeed",
      "url": "https://github.com/enemyrr/mcp-server-pagespeed",
      "imageUrl": "/freedevtools/mcp/pfp/enemyrr.webp",
      "description": "Analyze webpage performance using Google PageSpeed Insights to provide metrics and suggestions for optimization.",
      "stars": 8,
      "forks": 5,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-09-09T09:05:24Z",
      "readme_content": "# @enemyrr/mcp-server-pagespeed\n\nA Model Context Protocol server that provides Google PageSpeed Insights analysis. This server enables AI models to analyze webpage performance through a standardized interface.\n\n<a href=\"https://glama.ai/mcp/servers/wes81w8il2\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/wes81w8il2/badge\" alt=\"Server Pagespeed MCP server\" /></a>\n\n## Installation & Setup for Cursor IDE\n\n1. Clone and build the project:\n```bash\ngit clone https://github.com/enemyrr/mcp-server-pagespeed.git\ncd mcp-server-pagespeed\nnpm install\nnpm run build\n```\n\n2. Add the server in Cursor IDE settings:\n   - Open Command Palette (Cmd/Ctrl + Shift + P)\n   - Search for \"MCP: Add Server\"\n   - Fill in the fields:\n     - Name: `pagespeed`\n     - Type: `command`\n     - Command: `node /absolute/path/to/mcp-server-pagespeed/build/index.js`\n\n> **Note**: Replace `/absolute/path/to/` with the actual path where you cloned and built the project.\n\n## Command-line Usage\n\nJust run:\n\n```bash\nnpx mcp-server-pagespeed\n```\n\n## Available Tools\n\n### analyze_pagespeed\nAnalyze a webpage using Google PageSpeed Insights API.\n\n```typescript\nuse_mcp_tool({\n  server_name: \"pagespeed\",\n  tool_name: \"analyze_pagespeed\",\n  arguments: {\n    url: \"https://example.com\"\n  }\n});\n```\n\nThe tool returns:\n- Overall performance score (0-100)\n- Loading experience metrics\n  - First Contentful Paint\n  - First Input Delay\n- Top 5 improvement suggestions with:\n  - Title\n  - Description\n  - Potential impact\n  - Current value\n\n## Features\n\n- Real-time webpage performance analysis\n- Detailed loading experience metrics\n- Prioritized improvement suggestions\n- Comprehensive error handling\n- TypeScript support\n\n## Error Handling\n\nThe server provides detailed error messages for:\n- Invalid URLs\n- API request failures\n- Connection issues\n- Invalid tool calls\n\n## Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request to https://github.com/enemyrr/mcp-server-pagespeed\n\n## License\n\nMIT \n",
      "npm_url": "https://www.npmjs.com/package/mcp-server-pagespeed",
      "npm_downloads": 240,
      "keywords": [
        "pagespeed",
        "performance",
        "webpage",
        "pagespeed insights",
        "pagespeed analyze",
        "google pagespeed"
      ],
      "category": "web-search"
    },
    "envykernel--LinkedinMCPServer": {
      "owner": "envykernel",
      "name": "LinkedinMCPServer",
      "url": "https://github.com/envykernel/LinkedinMCPServer",
      "imageUrl": "/freedevtools/mcp/pfp/envykernel.webp",
      "description": "Integrate LinkedIn data and functionalities into applications using a standardized protocol for programmatic interaction. Utilize job search capabilities and automate LinkedIn interactions to enhance productivity and efficiency.",
      "stars": 0,
      "forks": 0,
      "license": "No License",
      "language": "C#",
      "updated_at": "2025-05-01T20:40:53Z",
      "readme_content": "# LinkedIn MCP Server\n\nA Model Context Protocol (MCP) server implementation for LinkedIn integration, built with .NET. This server provides a bridge between AI agents and LinkedIn's functionality, allowing for programmatic interaction with LinkedIn's features through a standardized protocol.\n\n## 🚀 Features\n\n- LinkedIn job search integration\n- Standardized MCP protocol implementation\n- Docker containerization support\n- .NET Core implementation\n\n## 📋 Prerequisites\n\n- .NET 6.0 or later\n- Docker (optional, for containerized deployment)\n- LinkedIn Developer Account (for API access)\n\n## 🛠️ Installation\n\n### Local Development\n\n1. Clone the repository:\n```bash\ngit clone https://github.com/envykernel/LinkedinMCPServer.git\ncd LinkedinMCPServer\n```\n\n2. Install dependencies:\n```bash\ndotnet restore\n```\n\n3. Build the project:\n```bash\ndotnet build\n```\n\n### Docker Deployment\n\nBuild and run using Docker:\n\n```bash\ndocker build -t linkedin-mcp-server .\ndocker run -p 5000:80 linkedin-mcp-server\n```\n\n## ⚙️ Configuration\n\nThe server requires the following configuration:\n\n1. LinkedIn API credentials (set in environment variables or configuration file)\n2. Server port settings\n3. MCP protocol settings\n\nCreate a `appsettings.json` file or set environment variables according to your needs.\n\n## 🚦 Usage\n\nRun the server locally:\n\n```bash\ndotnet run --project LinkedinMCPServer\n```\n\nThe server will start and listen for MCP protocol requests on the configured port.\n\n## 📝 API Documentation\n\nThe server implements the following MCP endpoints:\n\n- `SearchJobs`: Search for jobs on LinkedIn\n  - Parameters:\n    - `keywords`: Keywords to search for in job titles and descriptions\n    - `locationId`: Location ID for the search\n    - `datePosted`: When the job was posted (anyTime, past24Hours, pastWeek, pastMonth)\n    - `sort`: Sort order (mostRelevant, mostRecent)\n\n## 🤝 Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n\n1. Fork the repository\n2. Create your feature branch (`git checkout -b feature/AmazingFeature`)\n3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)\n4. Push to the branch (`git push origin feature/AmazingFeature`)\n5. Open a Pull Request\n\n## 📄 License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n\n## 🙏 Acknowledgments\n\n- LinkedIn API Documentation\n- Model Context Protocol Specification\n- .NET Community\n\n## 📞 Support\n\nFor support, please open an issue in the GitHub repository or contact the maintainers.\n\n---\n\nMade with ❤️ by [envykernel](https://github.com/envykernel)",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "linkedin",
        "linkedinmcpserver",
        "envykernel",
        "automate linkedin",
        "linkedin interactions",
        "envykernel linkedinmcpserver"
      ],
      "category": "web-search"
    },
    "erithwik--mcp-hn": {
      "owner": "erithwik",
      "name": "mcp-hn",
      "url": "https://github.com/erithwik/mcp-hn",
      "imageUrl": "/freedevtools/mcp/pfp/erithwik.webp",
      "description": "Fetch stories, comments, and user information from Hacker News, allowing users to access trending and relevant content directly from the platform.",
      "stars": 53,
      "forks": 15,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-17T07:45:41Z",
      "readme_content": "# Hacker News MCP Server\n\n[![smithery badge](https://smithery.ai/badge/mcp-hn)](https://smithery.ai/server/mcp-hn)\n\nA Model Context Protocol (MCP) server that provides tools for fetching information from Hacker News.\n\n<a href=\"https://glama.ai/mcp/servers/e0rco8dfgt\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/e0rco8dfgt/badge\" alt=\"mcp-hn MCP server\" /></a>\n\n## Tools\n\n- `get_stories` Fetching (top, new, ask_hn, show_hn) stories\n- `get_story_info` Fetching comments associated with a story\n- `search_stories` Searching for stories by query\n- `get_user_info` Fetching user info\n\n## Example Usage\n\nUse prompts like the following:\n\n```\nUser: Get the top stories of today\n  Output: Uses `get_stories` tool and returns a story about AI\nUser: What does the details of the story today that talks about the future of AI\n  Output: Uses `get_story_info` tool based on the results of the previous tool\nUser: What has the user `pg` been up to?\n  Output: Uses `get_user_info` tool and returns a summary of the user's activity\nUser: What does hackernews say about careers in AI?\n  Output: Uses `search_stories` tool and returns a summary of the comments\n```\n\nA more detailed example with the puppeteer MCP server:\n\n```\nUser: What are the top stories of today?\n  Output: Uses `get_stories` tool and returns a story about AI\nUser: Can you use the puppeteer tool to read the article about <AI> and also use the hackernews tool to view the comments and give me a summary of what the main comments are about the article?\n  Output: Uses puppeteer tool to read the article about AI and then uses the `get_story_info` hn tool to get the comments and returns a summary of the comments\n```\n\n## Quickstart\n\n### Installing via Smithery\n\nTo install Hacker News MCP Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/mcp-hn):\n\n```bash\nnpx -y @smithery/cli install mcp-hn --client claude\n```\n\n### Claude Desktop:\n\nUpdate the following:\n\nOn MacOS: `~/Library/Application\\ Support/Claude/claude_desktop_config.json`\n\nOn Windows: `%APPDATA%/Claude/claude_desktop_config.json`\n\nWith the following for production:\n\n```json\n{\n  \"mcpServers\": {\n    \"mcp-hn\": {\n      \"command\": \"uvx\",\n      \"args\": [\"mcp-hn\"]\n    }\n  }\n}\n```\n\n\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "web",
        "erithwik",
        "search",
        "hacker news",
        "search erithwik",
        "web search"
      ],
      "category": "web-search"
    },
    "evalstate--mcp-webcam": {
      "owner": "evalstate",
      "name": "mcp-webcam",
      "url": "https://github.com/evalstate/mcp-webcam",
      "imageUrl": "/freedevtools/mcp/pfp/evalstate.webp",
      "description": "Streams live images from a webcam to an MCP Client, supporting both capturing frames and taking screenshots.",
      "stars": 91,
      "forks": 10,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-09-21T20:37:09Z",
      "readme_content": "# ⭐⭐ mcp-webcam 0.2.0 - the 50 Star Update ⭐⭐ \n\nIn celebration of getting 52 GitHub stars, `mcp-webcam 0.2.0` is here! Now supports streamable-http!! No installation required! - try it now at [`https://webcam.fast-agent.ai/`](https://webcam.fast-agent.ai/). You can specify your own UserID by adding `?user=<YOUR_USER_ID>` after the URL. Note this shared instance is for fun, not security - see below for instructions how to run your own copy locally.\n\nIn streamable-http mode multiple clients can connect simultaneously, and you can choose which is used for Sampling.\n\n![mcp_webcam_020_thumb](https://github.com/user-attachments/assets/041e3091-71e5-4aa1-9170-ee20177485ef)\n\nIf we get to 100 stars I'll add another feature 😊.\n\n## Multi-user Mode\n\nWhen run in Streaming mode, if you set an MCP_HOST environment variable the host name is used as a prefix in URL construction, and 5 character UserIDs are automatically generated when the User lands on the webpage. \n\n![image](https://github.com/user-attachments/assets/30d06cc2-59b6-485b-989d-7030b39c287d)\n\n\n## mcp-webcam\n\nMCP Server that provides access to your WebCam. Provides `capture` and `screenshot` tools to take an image from the Webcam, or take a screenshot. The current image is also available as a Resource.\n\n### MCP Sampling\n\n`mcp-webcam` supports \"sampling\"! Press the \"Sample\" button to send a sampling request to the Client along with your entered message. \n\n> [!TIP]\n> Claude Desktop does not currently support Sampling. If you want a Client that can handle multi-modal sampling request, try https://github.com/evalstate/fast-agent/ or VSCode (more details below).\n\n## Installation and Running\n\n### NPX\n\nInstall a recent version of [NodeJS](https://nodejs.org/en/download) for your platform. The NPM package is `@llmindset/mcp-webcam`. \n\nTo start in **STDIO** mode: `npx @llmindset/mcp-webcam`. This starts the `mcp-webcam` UI on port 3333. Point your browser at `http://localhost:3333` to get started.\n\nTo change the port: `npx @llmindset/mcp-webcam 9999`. This starts `mcp-webcam` the UI on port 9999.\n\nFor **Streaming HTTP** mode: `npx @llmindset/mcp-webcam --streaming`. This will make the UI available at `http://localhost:3333` and the MCP Server available at `http://localhost:3333/mcp`.\n\n### Docker\n\nYou can run `mcp-webcam` using Docker. By default, it starts in **streaming mode**:\n\n```bash\ndocker run -p 3333:3333 ghcr.io/evalstate/mcp-webcam:latest\n```\n\n#### Environment Variables\n\n- `MCP_TRANSPORT_MODE` - Set to `stdio` for STDIO mode, defaults to `streaming`\n- `PORT` - The port to run on (default: `3333`)\n- `BIND_HOST` - Network interface to bind the server to (default: `localhost`)\n- `MCP_HOST` - Public-facing URL for user instructions and MCP client connections (default: `http://localhost:3333`)\n\n#### Examples\n\n```bash\n# STDIO mode\ndocker run -p 3333:3333 -e MCP_TRANSPORT_MODE=stdio ghcr.io/evalstate/mcp-webcam:latest\n\n# Custom port\ndocker run -p 8080:8080 -e PORT=8080 ghcr.io/evalstate/mcp-webcam:latest\n\n# For cloud deployments with custom domain (e.g., Hugging Face Spaces)\ndocker run -p 3333:3333 -e MCP_HOST=https://evalstate-mcp-webcam.hf.space ghcr.io/evalstate/mcp-webcam:latest\n\n# Complete cloud deployment example\ndocker run -p 3333:3333 -e MCP_HOST=https://your-domain.com ghcr.io/evalstate/mcp-webcam:latest\n```\n\n## Clients\n\nIf you want a Client that supports sampling try:\n\n### fast-agent\n\nStart the `mcp-webcam` in streaming mode, install [`uv`](https://docs.astral.sh/uv/) and connect with:\n\n`uvx fast-agent-mcp go --url http://localhost:3333/mcp`\n\n`fast-agent` currently uses Haiku as its default model, so set an `ANTHROPIC_API_KEY`. If you want to use a different model, you can add `--model` on the command line. More instructions for installation and configuration are available here: https://fast-agent.ai/models/.\n\nTo start the server in STDIO mode, add the following to your `fastagent.config.yaml`\n\n```yaml\nwebcam_local:\n   command: \"npx\"\n   args: [\"@llmindset/mcp-webcam\"]\n```\n\n### VSCode\n\nVSCode versions 1.101.0 and above support MCP Sampling. Simply start `mcp-webcam` in streaming mode, and add `http://localhost:3333/mcp` as an MCP Server to get started.\n\n### Claude Desktop\n\nClaude Desktop does **NOT** support Sampling. To run `mcp-webcam` from Claude Desktop, add the following to the `mcpServers` section of your `claude_desktop_config.json` file:\n\n```json\n    \"webcam\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"@llmindset/mcp-webcam\"\n      ]\n    }\n```\n\nStart Claude Desktop, and connect to `http://localhost:3333`. You can then ask Claude to `get the latest picture from my webcam`, or `Claude, take a look at what I'm holding` or `what colour top am i wearing?`. You can \"freeze\" the current image and that will be returned to Claude rather than a live capture. \n\nYou can ask for Screenshots - navigate to the browser so that you can guide the capture area when the request comes in. Screenshots are automatically resized to be manageable for Claude (useful if you have a 4K Screen). The button is there to allow testing of your platform specific Screenshot UX - it doesn't do anything other than prepare you for a Claude intiated request. NB this does not **not** work on Safari as it requires human initiation.\n\n## Other notes\n\nThat's it really. \n\nThis MCP Server was built to demonstrate exposing a User Interface on an MCP Server, and serving live resources back to Claude Desktop.\n\nThis project might prove useful if you want to build a local, interactive MCP Server.\n\nThanks to  https://github.com/tadasant for help with testing and setup. \n\nPlease read the article at [https://llmindset.co.uk/posts/2025/01/resouce-handling-mcp](https://llmindset.co.uk/posts/2025/01/mcp-files-resources-part1/) for more details about handling files and resources in LLM / MCP Chat Applications, and why you might want to do this.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "webcam",
        "capturing",
        "mcp",
        "mcp webcam",
        "webcam mcp",
        "webcam streams"
      ],
      "category": "web-search"
    },
    "everford--fetcher-mcp": {
      "owner": "everford",
      "name": "fetcher-mcp",
      "url": "https://github.com/everford/fetcher-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/everford.webp",
      "description": "Retrieve web page content using a Playwright headless browser to navigate and extract information efficiently. Designed for easy setup and configuration, it leverages AI to streamline web scraping tasks.",
      "stars": 7,
      "forks": 6,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-10-03T07:56:09Z",
      "readme_content": "# 🚀 Fetcher MCP - Playwright Headless Browser Server\n\nWelcome to the Fetcher MCP GitHub repository! This repository hosts the MCP server for fetching web page content using the Playwright headless browser.\n\n## 🧠 About\n\nThe Fetcher MCP is designed to leverage artificial intelligence capabilities to efficiently retrieve web page content. By utilizing the Playwright headless browser, this server can navigate through web pages and extract desired information with ease.\n\n## 🎯 Key Features\n\n🤖 AI-Powered Content Fetching  \n🔗 Playwright Integration  \n🚀 Fast and Efficient  \n🌟 Easy Setup and Configuration  \n\n## 📚 Repository Details\n\n- **Name**: fetcher-mcp\n- **Description**: MCP server for fetch web page content using Playwright headless browser\n- **Topics**: AI, MCP, Playwright\n\n## 📦 Latest Release\n\nYou can download the latest version of the Fetcher MCP server from the following link:  \n[![Download Fetcher MCP](https://github.com/everford/fetcher-mcp/releases)](https://github.com/everford/fetcher-mcp/releases)\n\n### :information_source: Note:\nThe provided link leads directly to the application file. Please make sure to launch the application after downloading.\n\nIf the link is not accessible or does not work, you can check the \"Releases\" section of this repository for alternative download options.\n\n## 🚀 Get Started\n\nTo start using the Fetcher MCP server for content fetching, follow these simple steps:\n\n1. Download the latest version from the link above.\n2. Unzip the downloaded file to your desired location.\n3. Launch the application.\n4. Configure the server settings as needed.\n5. Start fetching web page content effortlessly!\n\n## 🌐 Additional Resources\n\nFor more information, resources, or support regarding the Fetcher MCP server, feel free to visit the official website at [https://github.com/everford/fetcher-mcp/releases](https://github.com/everford/fetcher-mcp/releases).\n\n## 📝 Contribution Guidelines\n\nWe welcome contributions to enhance the Fetcher MCP server and make it even more powerful and efficient. If you have any ideas, suggestions, or improvements, please submit a pull request following our guidelines.\n\n## 🙌 Join Our Community\n\nConnect with other developers, share insights, and stay updated on the latest news related to the Fetcher MCP server by joining our community:\n\n👥 [Slack Channel](https://github.com/everford/fetcher-mcp/releases)  \n🐦 [Twitter](https://github.com/everford/fetcher-mcp/releases)  \n📧 [Newsletter](https://github.com/everford/fetcher-mcp/releases)\n\n---\n\n🚀 Start using the Fetcher MCP server today for seamless web page content fetching with AI-powered capabilities. Effortlessly extract the information you need using the Playwright headless browser integration. Happy Fetching! 🌟\n\n---\n\nRemember, the Fetcher MCP server simplifies the process of web page content retrieval, making it faster and more efficient than ever before. Download the latest version now and experience the power of AI and Playwright in action. Happy fetching! 🚀",
      "npm_url": "https://www.npmjs.com/package/fetcher-mcp",
      "npm_downloads": 50960,
      "keywords": [
        "scraping",
        "fetcher",
        "search",
        "web scraping",
        "retrieve web",
        "everford fetcher"
      ],
      "category": "web-search"
    },
    "falahgs--Brave-Gemini-Research-MCP-Server": {
      "owner": "falahgs",
      "name": "Brave-Gemini-Research-MCP-Server",
      "url": "https://github.com/falahgs/Brave-Gemini-Research-MCP-Server",
      "imageUrl": "/freedevtools/mcp/pfp/falahgs.webp",
      "description": "Enables AI assistants to conduct web searches and analyze research papers using Brave Search and Google's Gemini model. Supports general and local web search functionalities alongside academic analysis capabilities.",
      "stars": 5,
      "forks": 3,
      "license": "No License",
      "language": "TypeScript",
      "updated_at": "2025-08-28T01:16:07Z",
      "readme_content": "# Brave-Gemini Research MCP Server\r\n\r\nA modern MCP (Model Context Protocol) server implementation that provides AI assistants with web search capabilities via the Brave Search API and advanced research paper analysis with Google's Gemini model.\r\n\r\n## Overview\r\n\r\nThis project enables AI assistants like Claude to perform web searches and analyze research papers directly through a standardized API interface. The MCP server exposes three main tools:\r\n\r\n1. **Web Search** - For general internet searches and information retrieval\r\n2. **Local Search** - For finding businesses, locations, and places of interest\r\n3. **Research Paper Analysis** - For in-depth analysis of academic papers using Google's Gemini model\r\n\r\n## Features\r\n\r\n- 🔍 **Web Search API** - Find information across the web\r\n- 🏢 **Local Search API** - Discover businesses and places\r\n- 📑 **Research Paper Analysis** - Analyze academic papers with Gemini AI\r\n- 🤖 **Claude Integration** - Seamless connection with Claude Desktop\r\n- 🛠️ **Extensible Design** - Easy to add new tools and capabilities\r\n\r\n## Setup and Installation\r\n\r\n### Prerequisites\r\n\r\n- Node.js v18+ recommended\r\n- Brave Search API key ([Get one here](https://brave.com/search/api/))\r\n- Google API key for Gemini integration (required for research paper analysis)\r\n- Claude Desktop for AI assistant integration (optional)\r\n\r\n### Installation\r\n\r\n1. Clone the repository:\r\n   ```bash\r\n   git clone https://github.com/falahgs/brave-gemini-research-mcp.git\r\n   cd brave-gemini-research-mcp\r\n   ```\r\n\r\n2. Install dependencies:\r\n   ```bash\r\n   npm install\r\n   ```\r\n\r\n3. Create a `.env` file with your API keys:\r\n   ```\r\n   BRAVE_API_KEY=your_brave_api_key\r\n   GOOGLE_API_KEY=your_google_api_key\r\n   ```\r\n\r\n### Building\r\n\r\nCompile the TypeScript code to JavaScript:\r\n\r\n```bash\r\nnpm run build\r\n# or manually\r\nnpx tsc\r\n```\r\n\r\n### Running the Server\r\n\r\nSet environment variables and start the server:\r\n\r\n**PowerShell:**\r\n```powershell\r\n$env:BRAVE_API_KEY=\"your_brave_api_key\"\r\n$env:GOOGLE_API_KEY=\"your_google_api_key\"\r\nnode dist/index.js\r\n```\r\n\r\n**Command Prompt:**\r\n```\r\nSET BRAVE_API_KEY=your_brave_api_key\r\nSET GOOGLE_API_KEY=your_google_api_key\r\nnode dist/index.js\r\n```\r\n\r\n**Bash/Linux/macOS:**\r\n```bash\r\nBRAVE_API_KEY=your_brave_api_key GOOGLE_API_KEY=your_google_api_key node dist/index.js\r\n```\r\n\r\n## Claude Desktop Integration\r\n\r\nFollow these steps to integrate the MCP server with Claude Desktop:\r\n\r\n1. Ensure you have Claude Desktop installed ([Download here](https://claude.ai/desktop))\r\n\r\n2. Locate your Claude Desktop configuration file:\r\n   - Windows: `C:\\Users\\<username>\\AppData\\Roaming\\Claude\\claude_desktop_config.json`\r\n   - macOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\r\n\r\n3. Add the Brave-Gemini Research MCP configuration:\r\n\r\n```json\r\n{\r\n  \"mcpServers\": {\r\n    \"Brave-Gemini Research\": {\r\n      \"command\": \"node\",\r\n      \"args\": [\"G:\\\\path\\\\to\\\\your\\\\brave-gemini-research-mcp\\\\dist\\\\index.js\"],\r\n      \"cwd\": \"G:\\\\path\\\\to\\\\your\\\\brave-gemini-research-mcp\",\r\n      \"timeoutMs\": 120000,\r\n      \"env\": {\r\n        \"BRAVE_API_KEY\": \"your_brave_api_key\",\r\n        \"GOOGLE_API_KEY\": \"your_google_api_key\",\r\n        \"NODE_ENV\": \"production\",\r\n        \"DEBUG\": \"mcp:*\"\r\n      }\r\n    }\r\n  }\r\n}\r\n```\r\n\r\n4. Important notes:\r\n   - Use **absolute paths** with double backslashes (Windows) in the `args` and `cwd` fields\r\n   - Replace `G:\\\\path\\\\to\\\\your\\\\brave-gemini-research-mcp` with the actual path to your project\r\n   - Replace `your_brave_api_key` and `your_google_api_key` with your actual API keys\r\n   - The `timeoutMs` setting helps prevent timeout issues during initialization\r\n\r\n5. Save the file and restart Claude Desktop\r\n\r\n### Using with Claude\r\n\r\nAfter configuration, you can ask Claude to search the web or analyze research papers with prompts like:\r\n\r\n- \"Search the web for the latest AI research papers\"\r\n- \"Find coffee shops in San Francisco\"\r\n- \"Analyze this research paper on quantum computing: [paper content]\"\r\n\r\nClaude will use the MCP server to perform these searches and analyses, returning the results directly in your conversation.\r\n\r\n## Tool Capabilities\r\n\r\n### Web Search Tool\r\n\r\nThe web search tool enables general internet searches:\r\n\r\n- **Function**: `brave_web_search`\r\n- **Parameters**:\r\n  - `query` (required): Search query (max 400 chars)\r\n  - `count` (optional): Number of results (1-20, default 10)\r\n  - `offset` (optional): Pagination offset (max 9, default 0)\r\n\r\n### Local Search Tool\r\n\r\nThe local search tool finds businesses and locations:\r\n\r\n- **Function**: `brave_local_search`\r\n- **Parameters**:\r\n  - `query` (required): Local search query (e.g., \"pizza near Central Park\")\r\n  - `count` (optional): Number of results (1-20, default 5)\r\n\r\n### Research Paper Analysis Tool\r\n\r\nThe research paper analysis tool provides in-depth analysis of academic papers using Google's Gemini model:\r\n\r\n- **Function**: `gemini_research_paper_analysis`\r\n- **Parameters**:\r\n  - `paperContent` (required): The full text of the research paper to analyze\r\n  - `analysisType` (optional): Type of analysis to perform\r\n    - Options: \"summary\", \"critique\", \"literature review\", \"key findings\", \"comprehensive\" (default)\r\n  - `additionalContext` (optional): Specific questions or context to guide the analysis\r\n\r\n**Analysis Types:**\r\n- **Summary**: Comprehensive overview including research question, methodology, key findings, and conclusions\r\n- **Critique**: Critical evaluation of methodology, validity, limitations, and suggestions for improvement\r\n- **Literature Review**: Analysis of how the paper fits within the broader research landscape\r\n- **Key Findings**: Extraction and explanation of the most significant findings and implications\r\n- **Comprehensive**: Complete analysis covering all aspects (default)\r\n\r\n### Example Analysis Result\r\n\r\nWhen using the Research Paper Analysis tool with Gemini, you'll receive a structured, comprehensive analysis depending on the analysis type selected. For example, with a \"comprehensive\" analysis, you might get:\r\n\r\n```\r\n## Research Paper Analysis: Comprehensive\r\n\r\n### Overview\r\n[Summary of paper's main topic and research objectives]\r\n\r\n### Methodology Assessment\r\n[Evaluation of the research methods and design]\r\n\r\n### Key Findings\r\n[Breakdown of the most significant discoveries and results]\r\n\r\n### Limitations\r\n[Analysis of constraints and weaknesses in the research]\r\n\r\n### Significance & Implications\r\n[Discussion of the paper's importance to the field]\r\n\r\n### Recommendations\r\n[Suggestions for future research or applications]\r\n```\r\n\r\nThe Gemini model provides expert-level analysis that helps researchers, students, and professionals quickly understand and evaluate complex academic content.\r\n\r\n## Troubleshooting\r\n\r\n### Common Issues\r\n\r\n1. **Module Not Found Errors**:\r\n   - Ensure all imports include `.js` extensions in TypeScript files\r\n   - Run `npx tsc` to recompile after fixing imports\r\n   - Check the generated `dist` directory structure\r\n\r\n2. **Timeout Errors**:\r\n   - Increase the `timeoutMs` in Claude Desktop configuration (120000 ms recommended)\r\n   - Check that environment variables are properly set\r\n\r\n3. **API Key Issues**:\r\n   - Verify your API keys are correctly set in the environment\r\n   - Check for rate limiting or usage restrictions\r\n\r\n4. **Gemini Model Issues**:\r\n   - Ensure your Google API key has access to Gemini models\r\n   - Check if the paper content exceeds token limits (try shorter excerpts)\r\n   - Verify the analysis type is one of the supported options\r\n\r\n5. **Windows-Specific Issues**:\r\n   - Use PowerShell for more reliable environment variable handling\r\n   - For Windows paths in JSON config, use double backslashes (e.g., `G:\\\\path\\\\to\\\\file`)\r\n   - Consider using absolute paths if relative paths aren't working\r\n\r\n### Debugging\r\n\r\nFor detailed debugging output:\r\n\r\n```bash\r\n# Set environment variables\r\nDEBUG=mcp:* NODE_ENV=development node dist/index.js\r\n```\r\n\r\n## Testing Your Setup\r\n\r\nTo verify your MCP server is working correctly:\r\n\r\n1. **Manual Test**:\r\n   - Run the server using the command line instructions above\r\n   - Check the console output for \"Brave-Gemini Research MCP Server running on stdio\"\r\n   - No error messages should appear\r\n\r\n2. **Claude Desktop Test**:\r\n   - After configuring Claude Desktop, open a new conversation\r\n   - Ask Claude to \"Search for latest developments in AI\"\r\n   - Claude should respond with search results from Brave Search\r\n   - Ask Claude to analyze a research paper\r\n   - Claude should respond with a detailed analysis from Gemini\r\n\r\n## Technical Details\r\n\r\n### MCP Protocol\r\n\r\nThe Model Context Protocol allows AI models to access external tools through a standardized interface. Key components include:\r\n\r\n- **Tools**: Functions with defined schemas\r\n- **Transports**: Communication channels between clients and servers\r\n- **Handlers**: Logic to process requests and return responses\r\n\r\n### Project Structure\r\n\r\n```\r\n├── dist/               # Compiled JavaScript files\r\n├── src/\r\n│   ├── config.ts       # Server configuration\r\n│   ├── server.ts       # MCP server implementation\r\n│   ├── tools/          # Tool definitions and handlers\r\n│   └── utils/          # Utility functions and API clients\r\n├── index.ts            # Server entry point\r\n├── tsconfig.json       # TypeScript configuration\r\n└── package.json        # Project dependencies\r\n```\r\n\r\n## Citation\r\n\r\nIf you use this tool in your research or project, please cite it as:\r\n\r\n```\r\nSalieh, F. G. (2025). Brave-Gemini Research MCP Server: A tool for AI assistants to search the web and analyze research papers. \r\nhttps://github.com/yourusername/brave-gemini-research-mcp\r\n```\r\n\r\n## License\r\n\r\nMIT\r\n\r\n## Copyright\r\n\r\n© 2025 Falah G. Salieh, Baghdad, Iraq. All rights reserved.\r\n\r\n---\r\n\r\nMade with ❤️ for enhancing AI capabilities\r\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "searches",
        "search",
        "google",
        "brave search",
        "web search",
        "web searches"
      ],
      "category": "web-search"
    },
    "falcosan--mcp": {
      "owner": "falcosan",
      "name": "mcp",
      "url": "https://github.com/falcosan/mcp",
      "imageUrl": "/freedevtools/mcp/pfp/falcosan.webp",
      "description": "Bridge between AI models and the Meilisearch search engine, enabling advanced search functionalities and real-time communication. Facilitates AI-driven data retrieval and management through seamless access to Meilisearch's powerful indexing capabilities.",
      "stars": 0,
      "forks": 0,
      "license": "No License",
      "language": "TypeScript",
      "updated_at": "2025-10-01T17:26:40Z",
      "readme_content": "# MCP Meilisearch API Server\n\nA Model Context Protocol (MCP) server implementation that provides a bridge between AI models and the Meilisearch search engine.\n\n## Overview\n\n- **MCP Server**: Exposes Meilisearch APIs as tools using the Model Context Protocol.\n- **Web Client Demo**: A demo interface showcasing search functionalities.\n- **AI Inference**: Intelligent tool selection based on user queries.\n\n## Key Features\n\n- **Multiple Transport Options**: Supports both STDIO and StreamableHTTP transports.\n- **Meilisearch API Support**: Full access to Meilisearch functionalities.\n- **Web Client Demo**: Updated interface showcasing search capabilities and features.\n- **AI Inference**: Leverages LLMs from providers such as OpenAI, HuggingFace, OpenRouter, and Ollama to intelligently determine and utilize the most suitable tool for user queries.\n\n## Getting Started\n\n### Prerequisites\n\n- Node.js v20 or higher.\n- A running Meilisearch instance (local or remote).\n- API key for Meilisearch (if required).\n- AI provider API key (if using AI inference).\n\n### Installation\n\nInstall the package:\n\n```bash\n# Using npm\nnpm install mcp-meilisearch\n\n# Using yarn\nyarn add mcp-meilisearch\n\n# Using pnpm\npnpm add mcp-meilisearch\n```\n\n### Options\n\n#### Meilisearch Connection Options\n\n- `meilisearchHost`: URL of the Meilisearch instance (Default: \"http://localhost:7700\")\n- `meilisearchApiKey`: API key for authenticating with Meilisearch (Default: \"\")\n\n#### MCP Server Options\n\n- `transport`: Transport type for MCP server (\"http\" | \"stdio\") (Default: \"http\")\n- `httpPort`: HTTP port for MCP server (Default: 4995)\n- `mcpEndpoint`: MCP endpoint path (Default: \"/mcp\")\n\n#### Session Options\n\n- `sessionTimeout`: Session timeout in milliseconds (Default: 3600000)\n- `sessionCleanupInterval`: Session cleanup interval in milliseconds (Default: 60000)\n\n#### AI Inference Options\n\n- `aiProviderName`: Name of the AI provider (\"openai\" | \"huggingface\" | \"openrouter\" | \"ollama\") (Default: \"openai\")\n- `aiProviderApiKey`: AI provider API key for AI inference\n- `llmModel`: AI model to use (Default: \"gpt-3.5-turbo\")\n\nPlease be aware that not all models support function calling, which is required for proper AI inference in this package. Make sure to select a model that supports the tools parameter.\n\n| Provider    | Supported Models                                                                              |\n| ----------- | --------------------------------------------------------------------------------------------- |\n| OpenAI      | [List of supported models](https://platform.openai.com/docs/models)                           |\n| OpenRouter  | [List of supported models](https://openrouter.ai/models?fmt=cards&supported_parameters=tools) |\n| HuggingFace | [List of supported models](https://huggingface.co/models?other=function+calling)              |\n| Ollama      | [List of supported models](https://ollama.com/search?c=tools)                                 |\n\n#### Example server setup\n\n```typescript\nawait mcpMeilisearchServer({\n  meilisearchHost: \"http://localhost:7700\",\n  aiProviderName: \"openrouter\",\n  aiProviderApiKey: \"your_openrouter_api_key\",\n  llmModel: \"anthropic/claude-3-opus\", // Make sure to use a model that supports function calling\n});\n```\n\n### Using the MCPClient\n\nThe package exports the MCPClient class for client-side integration:\n\n```typescript\nimport { MCPClient } from \"mcp-meilisearch/client\";\n\nconst client = new MCPClient(\"mcp-meilisearch-client\");\n\nawait client.connectToServer(\"http://localhost:4995/mcp\");\n\nconst result = await client.callTool(\"global-search\", {\n  q: \"search kiosco antonio\",\n});\n```\n\n#### Client Methods\n\n##### `callTool(name, args)`\n\nCalls a specific tool on the MCP server with optional arguments.\n\n**Parameters:**\n\n- `name`: String - The name of the tool to call\n- `args`: Object (Optional) - Arguments to pass to the tool\n\n##### `processSummary(query)`\n\nProcesses data using AI to generate a human-readable summary.\n\n**Parameters:**\n\n- `query`: Any - The data to be summarized\n\n##### `callToolWithAI(query, options)`\n\nProcesses a user query through AI to determine and execute the most appropriate tool.\n\n**Parameters:**\n\n- `query`: String - The user's query or request to be processed\n- `options`: Object (Optional) - Configuration options\n  - `specificTools`: String[] (Optional) - Restricts tool selection to this list of tool names\n  - `justReasoning`: Boolean (Optional) - When set to `true`, returns only the AI's reasoning without executing the selected tool\n  - `provideSummary`: Boolean (Optional) - When set to `true`, generates a concise summary of the search results along with the regular response\n\n### Starting the Server\n\nYou can start the server programmatically:\n\n```typescript\nimport mcpMeilisearchServer from \"mcp-meilisearch\";\n\nawait mcpMeilisearchServer({\n  meilisearchHost: \"http://localhost:7700\",\n  meilisearchApiKey: \"your_meilisearch_api_key\",\n  aiProviderName: \"openai\",\n  aiProviderApiKey: \"your_ai_provider_api_key\",\n  llmModel: \"gpt-4\",\n});\n```\n\n## Tools\n\nThe MCP server exposes various tools that allow you to interact with Meilisearch functionalities. Each tool corresponds to a specific Meilisearch API endpoint, enabling you to perform operations such as searching, indexing, and managing documents.\n\n### Tool Categories\n\n1. **System Tools**: Health checks, version info, server stats.\n2. **Index Tools**: Manage indexes (create, update, delete, list).\n3. **Document Tools**: Add, update, delete, and retrieve documents.\n4. **Search Tools**: Advanced search, including vector search.\n5. **Settings Tools**: Configure index settings.\n6. **Task Tools**: Manage asynchronous tasks.\n7. **Vector Tools**: Experimental vector search capabilities.\n\n### System Tools\n\n#### health\n\n- **Description**: Check if the Meilisearch server is healthy.\n\n#### version\n\n- **Description**: Get the version information of the Meilisearch server.\n\n#### info\n\n- **Description**: Get the system information of the Meilisearch server.\n\n#### stats\n\n- **Description**: Get statistics about all indexes or a specific index.\n- **Parameters**:\n  - `indexUid` (string, optional): Unique identifier of the index.\n\n#### get-tasks\n\n- **Description**: Get information about tasks with optional filtering.\n- **Parameters**:\n  - `limit` (number, optional): Maximum number of tasks to return.\n  - `from` (number, optional): Task uid from which to start fetching.\n  - `status` (string, optional): Status of tasks to return.\n  - `type` (string, optional): Type of tasks to return.\n  - `indexUids` (string[], optional): UIDs of the indexes on which tasks were performed.\n\n#### delete-tasks\n\n- **Description**: Delete tasks based on provided filters.\n- **Parameters**:\n  - `statuses` (string[], optional): Statuses of tasks to delete.\n  - `types` (string[], optional): Types of tasks to delete.\n  - `indexUids` (string[], optional): UIDs of the indexes on which tasks to delete were performed.\n  - `uids` (number[], optional): UIDs of the tasks to delete.\n  - `canceledBy` (number[], optional): UIDs of the tasks that canceled tasks to delete.\n  - `beforeUid` (number, optional): Delete tasks whose uid is before this value.\n  - `beforeStartedAt` (string, optional): Delete tasks that started processing before this date (ISO 8601 format).\n  - `beforeFinishedAt` (string, optional): Delete tasks that finished processing before this date (ISO 8601 format).\n\n### Index Tools\n\n#### list-indexes\n\n- **Description**: List all indexes in the Meilisearch instance.\n- **Parameters**:\n  - `limit` (number, optional): Maximum number of indexes to return.\n  - `offset` (number, optional): Number of indexes to skip.\n\n#### get-index\n\n- **Description**: Get information about a specific Meilisearch index.\n- **Parameters**:\n  - `indexUid` (string, required): Unique identifier of the index.\n\n#### create-index\n\n- **Description**: Create a new Meilisearch index.\n- **Parameters**:\n  - `indexUid` (string, required): Unique identifier for the new index.\n  - `primaryKey` (string, optional): Primary key for the index.\n\n#### update-index\n\n- **Description**: Update a Meilisearch index (currently only supports updating the primary key).\n- **Parameters**:\n  - `indexUid` (string, required): Unique identifier of the index.\n  - `primaryKey` (string, required): New primary key for the index.\n\n#### delete-index\n\n- **Description**: Delete a Meilisearch index.\n- **Parameters**:\n  - `indexUid` (string, required): Unique identifier of the index to delete.\n\n#### swap-indexes\n\n- **Description**: Swap two or more indexes in Meilisearch.\n- **Parameters**:\n  - `indexes` (string, required): JSON array of index pairs to swap, e.g. [[\"movies\", \"movies_new\"]].\n\n### Document Tools\n\n#### get-documents\n\n- **Description**: Get documents from a Meilisearch index.\n- **Parameters**:\n  - `indexUid` (string, required): Unique identifier of the index.\n  - `limit` (number, optional): Maximum number of documents to return (default: 20).\n  - `offset` (number, optional): Number of documents to skip (default: 0).\n  - `fields` (string[], optional): Fields to return in the documents.\n  - `filter` (string, optional): Filter query to apply.\n\n#### get-document\n\n- **Description**: Get a document by its ID from a Meilisearch index.\n- **Parameters**:\n  - `indexUid` (string, required): Unique identifier of the index.\n  - `documentId` (string, required): ID of the document to retrieve.\n  - `fields` (string[], optional): Fields to return in the document.\n\n#### add-documents\n\n- **Description**: Add documents to a Meilisearch index.\n- **Parameters**:\n  - `indexUid` (string, required): Unique identifier of the index.\n  - `documents` (string, required): JSON array of documents to add.\n  - `primaryKey` (string, optional): Primary key for the documents.\n\n#### update-documents\n\n- **Description**: Update documents in a Meilisearch index.\n- **Parameters**:\n  - `indexUid` (string, required): Unique identifier of the index.\n  - `documents` (string, required): JSON array of documents to update.\n  - `primaryKey` (string, optional): Primary key for the documents.\n\n#### delete-document\n\n- **Description**: Delete a document by its ID from a Meilisearch index.\n- **Parameters**:\n  - `indexUid` (string, required): Unique identifier of the index.\n  - `documentId` (string, required): ID of the document to delete.\n\n#### delete-documents\n\n- **Description**: Delete multiple documents by their IDs from a Meilisearch index.\n- **Parameters**:\n  - `indexUid` (string, required): Unique identifier of the index.\n  - `documentIds` (string, required): JSON array of document IDs to delete.\n\n#### delete-all-documents\n\n- **Description**: Delete all documents in a Meilisearch index.\n- **Parameters**:\n  - `indexUid` (string, required): Unique identifier of the index.\n\n### Search Tools\n\n#### search\n\n- **Description**: Search for documents in a Meilisearch index.\n- **Parameters**:\n  - `indexUid` (string, required): Unique identifier of the index.\n  - `q` (string, required): Search query.\n  - `limit` (number, optional): Maximum number of results to return (default: 20).\n  - `offset` (number, optional): Number of results to skip (default: 0).\n  - `filter` (string, optional): Filter query to apply.\n  - `sort` (string[], optional): Attributes to sort by, e.g. [\"price:asc\"].\n  - `facets` (string[], optional): Facets to return.\n  - `attributesToRetrieve` (string[], optional): Attributes to include in results.\n  - `attributesToCrop` (string[], optional): Attributes to crop.\n  - `cropLength` (number, optional): Length at which to crop cropped attributes.\n  - `attributesToHighlight` (string[], optional): Attributes to highlight.\n  - `highlightPreTag` (string, optional): Tag to insert before highlighted text.\n  - `highlightPostTag` (string, optional): Tag to insert after highlighted text.\n  - `showMatchesPosition` (boolean, optional): Whether to include match positions in results.\n  - `matchingStrategy` (string, optional): Matching strategy: 'all' or 'last'.\n\n#### multi-search\n\n- **Description**: Perform multiple searches in one request.\n- **Parameters**:\n  - `queries` (string, required): JSON array of search queries, each containing the same parameters as the `search` tool.\n\n#### global-search\n\n- **Description**: Search for a term across all available Meilisearch indexes and return combined results.\n- **Parameters**:\n  - `q` (string, required): Search query.\n  - `limit` (number, optional): Maximum number of results to return per index (default: 20).\n  - `attributesToRetrieve` (string[], optional): Attributes to include in results.\n\n#### facet-search\n\n- **Description**: Search for facet values matching specific criteria.\n- **Parameters**:\n  - `indexUid` (string, required): Unique identifier of the index.\n  - `facetName` (string, required): Name of the facet to search.\n  - `facetQuery` (string, optional): Query to match against facet values.\n  - `filter` (string, optional): Filter to apply to the base search.\n\n### Settings Tools\n\n#### get-settings\n\n- **Description**: Get all settings for a Meilisearch index.\n- **Parameters**:\n  - `indexUid` (string, required): Unique identifier of the index.\n\n#### update-settings\n\n- **Description**: Update settings for a Meilisearch index.\n- **Parameters**:\n  - `indexUid` (string, required): Unique identifier of the index.\n  - `settings` (string, required): JSON object containing settings to update.\n\n#### reset-settings\n\n- **Description**: Reset all settings for a Meilisearch index to their default values.\n- **Parameters**:\n  - `indexUid` (string, required): Unique identifier of the index.\n\n#### Get Settings Tools\n\nThe following tools retrieve specific settings for a Meilisearch index:\n\n- **get-displayed-attributes**\n- **get-searchable-attributes**\n- **get-filterable-attributes**\n- **get-sortable-attributes**\n- **get-ranking-rules**\n- **get-stop-words**\n- **get-synonyms**\n- **get-typo-tolerance**\n- **get-pagination**\n- **get-faceting**\n- **get-dictionary**\n- **get-proximity-precision**\n- **get-separator-tokens**\n- **get-non-separator-tokens**\n- **get-word-dictionary**\n\nAll these tools have the same parameter:\n\n- `indexUid` (string, required): Unique identifier of the index.\n\n#### Update Settings Tools\n\nThe following tools update specific settings for a Meilisearch index:\n\n- **update-displayed-attributes**\n\n  - `indexUid` (string, required): Unique identifier of the index.\n  - `displayedAttributes` (string, required): JSON array of attributes to display, e.g. [\"title\", \"description\"].\n\n- **update-searchable-attributes**\n\n  - `indexUid` (string, required): Unique identifier of the index.\n  - `searchableAttributes` (string, required): JSON array of attributes that can be searched, e.g. [\"title\", \"description\"].\n\n- **update-filterable-attributes**\n\n  - `indexUid` (string, required): Unique identifier of the index.\n  - `filterableAttributes` (string, required): JSON array of attributes that can be used as filters, e.g. [\"genre\", \"director\"].\n\n- **update-sortable-attributes**\n\n  - `indexUid` (string, required): Unique identifier of the index.\n  - `sortableAttributes` (string, required): JSON array of attributes that can be used for sorting, e.g. [\"price\", \"date\"].\n\n- **update-ranking-rules**\n\n  - `indexUid` (string, required): Unique identifier of the index.\n  - `rankingRules` (string, required): JSON array of ranking rules, e.g. [\"typo\", \"words\", \"proximity\", \"attribute\", \"sort\", \"exactness\"].\n\n- **update-stop-words**\n\n  - `indexUid` (string, required): Unique identifier of the index.\n  - `stopWords` (string, required): JSON array of words to ignore in search queries, e.g. [\"the\", \"a\", \"an\"].\n\n- **update-synonyms**\n\n  - `indexUid` (string, required): Unique identifier of the index.\n  - `synonyms` (string, required): JSON object mapping words to their synonyms, e.g. {\"movie\": [\"film\"]}.\n\n- **update-typo-tolerance**\n\n  - `indexUid` (string, required): Unique identifier of the index.\n  - `typoTolerance` (string, required): JSON object with typo tolerance configuration, e.g. {\"enabled\": true, \"minWordSizeForTypos\": {\"oneTypo\": 5, \"twoTypos\": 9}}.\n\n- **update-pagination**\n\n  - `indexUid` (string, required): Unique identifier of the index.\n  - `pagination` (string, required): JSON object with pagination configuration, e.g. {\"maxTotalHits\": 1000}.\n\n- **update-faceting**\n\n  - `indexUid` (string, required): Unique identifier of the index.\n  - `faceting` (string, required): JSON object with faceting configuration, e.g. {\"maxValuesPerFacet\": 100}.\n\n- **update-dictionary**\n\n  - `indexUid` (string, required): Unique identifier of the index.\n  - `dictionary` (string, required): JSON array of words to consider as a single word, e.g. [\"San Francisco\", \"New York\"].\n\n- **update-proximity-precision**\n\n  - `indexUid` (string, required): Unique identifier of the index.\n  - `proximityPrecision` (string, required): String with proximity precision value, can be 'byWord' or 'byAttribute'.\n\n- **update-separator-tokens**\n\n  - `indexUid` (string, required): Unique identifier of the index.\n  - `separatorTokens` (string, required): JSON array of tokens that should be considered as word separators, e.g. [\"-\", \"_\"].\n\n- **update-non-separator-tokens**\n\n  - `indexUid` (string, required): Unique identifier of the index.\n  - `nonSeparatorTokens` (string, required): JSON array of tokens that should not be considered as word separators, e.g. [\"@\", \".\"].\n\n- **update-word-dictionary**\n  - `indexUid` (string, required): Unique identifier of the index.\n  - `wordDictionary` (string, required): JSON array of custom words to add to the dictionary, e.g. [\"cbuilder\", \"meilisearch\"].\n\n#### Reset Settings Tools\n\nThe following tools reset specific settings for a Meilisearch index to their default values:\n\n- **reset-displayed-attributes**\n- **reset-searchable-attributes**\n- **reset-filterable-attributes**\n- **reset-sortable-attributes**\n- **reset-ranking-rules**\n- **reset-stop-words**\n- **reset-synonyms**\n- **reset-typo-tolerance**\n- **reset-pagination**\n- **reset-faceting**\n- **reset-dictionary**\n- **reset-proximity-precision**\n- **reset-separator-tokens**\n- **reset-non-separator-tokens**\n- **reset-word-dictionary**\n\nAll these reset tools have the same parameter:\n\n- `indexUid` (string, required): Unique identifier of the index.\n\n### Task Tools\n\n#### list-tasks\n\n- **Description**: List tasks with optional filtering.\n- **Parameters**:\n  - `limit` (number, optional): Maximum number of tasks to return.\n  - `from` (number, optional): Task uid from which to start fetching.\n  - `statuses` (string[], optional): Statuses of tasks to return.\n  - `types` (string[], optional): Types of tasks to return.\n  - `indexUids` (string[], optional): UIDs of the indexes on which tasks were performed.\n  - `uids` (number[], optional): UIDs of specific tasks to return.\n\n#### get-task\n\n- **Description**: Get information about a specific task.\n- **Parameters**:\n  - `taskUid` (number, required): Unique identifier of the task.\n\n#### cancel-tasks\n\n- **Description**: Cancel tasks based on provided filters.\n- **Parameters**:\n  - `statuses` (string[], optional): Statuses of tasks to cancel.\n  - `types` (string[], optional): Types of tasks to cancel.\n  - `indexUids` (string[], optional): UIDs of the indexes on which tasks to cancel were performed.\n  - `uids` (number[], optional): UIDs of the tasks to cancel.\n\n#### wait-for-task\n\n- **Description**: Wait for a specific task to complete.\n- **Parameters**:\n  - `taskUid` (number, required): Unique identifier of the task to wait for.\n  - `timeoutMs` (number, optional): Maximum time to wait in milliseconds (default: 5000).\n  - `intervalMs` (number, optional): Polling interval in milliseconds (default: 500).\n\n### Vector Tools\n\n#### enable-vector-search\n\n- **Description**: Enable the vector search experimental feature in Meilisearch.\n\n#### get-experimental-features\n\n- **Description**: Get the status of experimental features in Meilisearch.\n\n#### update-embedders\n\n- **Description**: Configure embedders for vector search.\n- **Parameters**:\n  - `indexUid` (string, required): Unique identifier of the index.\n  - `embedders` (string, required): JSON object containing embedder configurations.\n\n#### get-embedders\n\n- **Description**: Get the embedders configuration for an index.\n- **Parameters**:\n  - `indexUid` (string, required): Unique identifier of the index.\n\n#### reset-embedders\n\n- **Description**: Reset the embedders configuration for an index.\n- **Parameters**:\n  - `indexUid` (string, required): Unique identifier of the index.\n\n#### vector-search\n\n- **Description**: Perform a vector search in a Meilisearch index.\n- **Parameters**:\n  - `indexUid` (string, required): Unique identifier of the index.\n  - `vector` (string, required): JSON array representing the vector to search for.\n  - `limit` (number, optional): Maximum number of results to return (default: 20).\n  - `offset` (number, optional): Number of results to skip (default: 0).\n  - `filter` (string, optional): Filter to apply (e.g., 'genre = horror AND year > 2020').\n  - `embedder` (string, optional): Name of the embedder to use (if omitted, a 'vector' must be provided).\n  - `attributes` (string[], optional): Attributes to include in the vector search.\n  - `query` (string, optional): Text query to search for (if using 'embedder' instead of 'vector').\n  - `hybrid` (boolean, optional): Whether to perform a hybrid search (combining vector and text search).\n  - `hybridRatio` (number, optional): Ratio of vector vs text search in hybrid search (0-1, default: 0.5).\n",
      "npm_url": "https://www.npmjs.com/package/mcp-mcp",
      "npm_downloads": 83,
      "keywords": [
        "indexing",
        "search",
        "retrieval",
        "meilisearch search",
        "search engine",
        "advanced search"
      ],
      "category": "web-search"
    },
    "farzad528--mcp-server-azure-ai-agents": {
      "owner": "farzad528",
      "name": "mcp-server-azure-ai-agents",
      "url": "https://github.com/farzad528/mcp-server-azure-ai-agents",
      "imageUrl": "/freedevtools/mcp/pfp/farzad528.webp",
      "description": "Integrates Azure AI services with Claude Desktop for enhanced search capabilities, enabling intelligent searches across indexed documents and the web while providing source citations. Offers two implementations: one that utilizes the Azure AI Agent Service for document and web searches, and another for direct access to Azure AI Search.",
      "stars": 52,
      "forks": 12,
      "license": "Other",
      "language": "Python",
      "updated_at": "2025-09-15T14:33:32Z",
      "readme_content": "# Azure AI Agent Service + Azure AI Search MCP Server\n\nA Model Context Protocol (MCP) server that enables Claude Desktop to search your content using Azure AI services. Choose between Azure AI Agent Service (with both document search and web search) or direct Azure AI Search integration.\n\n\n\n---\n\n## Overview\n\nThis project provides two MCP server implementations to connect Claude Desktop with Azure search capabilities:\n\n1. **Azure AI Agent Service Implementation (Recommended)** - Uses the powerful Azure AI Agent Service to provide:\n   - **Azure AI Search Tool** - Search your indexed documents with AI-enhanced results\n   - **Bing Web Grounding Tool** - Search the web with source citations\n\n2. **Direct Azure AI Search Implementation** - Connects directly to Azure AI Search with three methods:\n   - **Keyword Search** - Exact lexical matches\n   - **Vector Search** - Semantic similarity using embeddings\n   - **Hybrid Search** - Combination of keyword and vector searches\n\n---\n\n## Features\n\n- **AI-Enhanced Search** - Azure AI Agent Service optimizes search results with intelligent processing\n- **Multiple Data Sources** - Search both your private documents and the public web\n- **Source Citations** - Web search results include citations to original sources\n- **Flexible Implementation** - Choose between Azure AI Agent Service or direct Azure AI Search integration\n- **Seamless Claude Integration** - All search capabilities accessible through Claude Desktop's interface\n- **Customizable** - Easy to extend or modify search behavior\n\n---\n\n## Quick Links\n\n- [Get Started with Azure AI Search](https://learn.microsoft.com/en-us/azure/search/search-get-started-portal)\n- [Azure AI Agent Service Quickstart](https://learn.microsoft.com/en-us/azure/ai-studio/how-to/agent-quickstart)\n\n---\n\n## Requirements\n\n- **Python:** Version 3.10 or higher\n- **Claude Desktop:** Latest version\n- **Azure Resources:** \n  - Azure AI Search service with an index containing vectorized text data\n  - For Agent Service: Azure AI Project with Azure AI Search and Bing connections\n- **Operating System:** Windows or macOS (instructions provided for Windows, but adaptable)\n\n---\n\n## Azure AI Agent Service Implementation (Recommended)\n\n### Setup Guide\n\n1. **Project Directory:**\n\n   ```bash\n   mkdir mcp-server-azure-ai-search\n   cd mcp-server-azure-ai-search\n   ```\n\n2. **Create a `.env` File:**\n\n   ```bash\n   echo \"PROJECT_CONNECTION_STRING=your-project-connection-string\" > .env\n   echo \"MODEL_DEPLOYMENT_NAME=your-model-deployment-name\" >> .env\n   echo \"AI_SEARCH_CONNECTION_NAME=your-search-connection-name\" >> .env\n   echo \"BING_CONNECTION_NAME=your-bing-connection-name\" >> .env\n   echo \"AI_SEARCH_INDEX_NAME=your-index-name\" >> .env\n   ```\n\n3. **Set Up Virtual Environment:**\n\n   ```bash\n   uv venv\n   .venv\\Scripts\\activate\n   uv pip install \"mcp[cli]\" azure-identity python-dotenv azure-ai-projects\n   ```\n\n4. **Use the `azure_ai_agent_service_server.py` script** for integration with Azure AI Agent Service.\n\n### Azure AI Agent Service Setup\n\nBefore using the implementation, you need to:\n\n1. **Create an Azure AI Project:**\n   - Go to the Azure Portal and create a new Azure AI Project\n   - Note the project connection string and model deployment name\n\n2. **Create an Azure AI Search Connection:**\n   - In your Azure AI Project, add a connection to your Azure AI Search service\n   - Note the connection name and index name\n\n3. **Create a Bing Web Search Connection:**\n   - In your Azure AI Project, add a connection to Bing Search service\n   - Note the connection name\n\n4. **Authenticate with Azure:**\n   ```bash\n   az login\n   ```\n\n### Configuring Claude Desktop\n\n```json\n{\n  \"mcpServers\": {\n    \"azure-ai-agent\": {\n      \"command\": \"C:\\\\path\\\\to\\\\.venv\\\\Scripts\\\\python.exe\",\n      \"args\": [\"C:\\\\path\\\\to\\\\azure_ai_agent_service_server.py\"],\n      \"env\": {\n        \"PROJECT_CONNECTION_STRING\": \"your-project-connection-string\",\n        \"MODEL_DEPLOYMENT_NAME\": \"your-model-deployment-name\",\n        \"AI_SEARCH_CONNECTION_NAME\": \"your-search-connection-name\",\n        \"BING_CONNECTION_NAME\": \"your-bing-connection-name\",\n        \"AI_SEARCH_INDEX_NAME\": \"your-index-name\"\n      }\n    }\n  }\n}\n```\n\n> **Note:** Replace path placeholders with your actual project paths.\n\n---\n\n## Direct Azure AI Search Implementation\n\nFor those who prefer direct Azure AI Search integration without the Agent Service:\n\n1. **Create a different `.env` File:**\n\n   ```bash\n   echo \"AZURE_SEARCH_SERVICE_ENDPOINT=https://your-service-name.search.windows.net\" > .env\n   echo \"AZURE_SEARCH_INDEX_NAME=your-index-name\" >> .env\n   echo \"AZURE_SEARCH_API_KEY=your-api-key\" >> .env\n   ```\n\n2. **Install Dependencies:**\n\n   ```bash\n   uv pip install \"mcp[cli]\" azure-search-documents==11.5.2 azure-identity python-dotenv\n   ```\n\n3. **Use the `azure_search_server.py` script** for direct integration with Azure AI Search.\n\n4. **Configure Claude Desktop:**\n\n   ```json\n   {\n     \"mcpServers\": {\n       \"azure-search\": {\n         \"command\": \"C:\\\\path\\\\to\\\\.venv\\\\Scripts\\\\python.exe\",\n         \"args\": [\"C:\\\\path\\\\to\\\\azure_search_server.py\"],\n         \"env\": {\n           \"AZURE_SEARCH_SERVICE_ENDPOINT\": \"https://your-service-name.search.windows.net\",\n           \"AZURE_SEARCH_INDEX_NAME\": \"your-index-name\",\n           \"AZURE_SEARCH_API_KEY\": \"your-api-key\"\n         }\n       }\n     }\n   }\n   ```\n\n---\n\n## Testing the Server\n\n1. **Restart Claude Desktop** to load the new configuration\n2. Look for the MCP tools icon (hammer icon) in the bottom-right of the input field\n3. Try queries such as:\n   - \"Search for information about AI in my Azure Search index\"\n   - \"Search the web for the latest developments in LLMs\"\n   - \"Find information about neural networks using hybrid search\"\n\n---\n\n## Troubleshooting\n\n- **Server Not Appearing:**\n  - Check Claude Desktop logs (located at `%APPDATA%\\Claude\\logs\\mcp*.log` on Windows)\n  - Verify file paths and environment variables in the configuration\n  - Test running the server directly: `python azure_ai_agent_service_server.py` or `uv run python azure_ai_agent_service_server.py`\n\n- **Azure AI Agent Service Issues:**\n  - Ensure your Azure AI Project is correctly configured\n  - Verify that connections exist and are properly configured\n  - Check your Azure authentication status\n\n---\n\n## Customizing Your Server\n\n- **Modify Tool Instructions:** Adjust the instructions provided to each agent to change how they process queries\n- **Add New Tools:** Use the `@mcp.tool()` decorator to integrate additional tools\n- **Customize Response Formatting:** Edit how responses are formatted and returned to Claude Desktop\n- **Adjust Web Search Parameters:** Modify the web search tool to focus on specific domains\n\n---\n\n## License\n\nThis project is licensed under the MIT License.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "azure",
        "searches",
        "search",
        "azure ai",
        "ai search",
        "ai services"
      ],
      "category": "web-search"
    },
    "fatwang2--search1api-mcp": {
      "owner": "fatwang2",
      "name": "search1api-mcp",
      "url": "https://github.com/fatwang2/search1api-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/fatwang2.webp",
      "description": "Provides search and crawl functionality utilizing the Search1API to retrieve and index data from various sources.",
      "stars": 156,
      "forks": 36,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-10-03T04:12:20Z",
      "readme_content": "# Search1API MCP Server\n\n[中文文档](./README_zh.md)\n\nA Model Context Protocol (MCP) server that provides search and crawl functionality using Search1API.\n\n## Prerequisites\n\n- Node.js >= 18.0.0\n- A valid Search1API API key (See **Setup Guide** below on how to obtain and configure)\n\n## Installation (Standalone / General)\n\n1.  **Clone the repository:**\n    ```bash\n    git clone https://github.com/fatwang2/search1api-mcp.git\n    cd search1api-mcp\n    ```\n\n2.  **Configure API Key:** Before building, you need to provide your Search1API key. See the **Setup Guide** section below for different methods (e.g., using a `.env` file or environment variables).\n\n3.  **Install dependencies and build:**\n    ```bash\n    npm install\n    npm run build\n    ```\n    *Note: If using the project's `.env` file method for the API key, ensure it exists before this step.*\n\n## Usage (Standalone / General)\n\nEnsure your API key is configured (see **Setup Guide**).\n\nStart the server:\n```bash\nnpm start\n```\n\nThe server will then be ready to accept connections from MCP clients.\n\n## Setup Guide\n\n### 1. Get Search1API Key\n\n1.  Register at [Search1API](https://www.search1api.com/?utm_source=mcp)\n2.  Get your API key from your dashboard.\n\n### 2. Configure API Key\n\nYou need to make your API key available to the server. Choose **one** of the following methods:\n\n**Method A: Project `.env` File (Recommended for Standalone or LibreChat)**\n\nThis method is required if integrating with the current version of LibreChat (see specific section below).\n\n1.  In the `search1api-mcp` project root directory, create a file named `.env`:\n    ```bash\n    # In the search1api-mcp directory\n    echo \"SEARCH1API_KEY=your_api_key_here\" > .env\n    ```\n2.  Replace `your_api_key_here` with your actual key.\n3.  Make sure this file exists **before** running `npm install && npm run build`.\n\n**Method B: Environment Variable (Standalone Only)**\n\nSet the `SEARCH1API_KEY` environment variable before starting the server.\n\n```bash\nexport SEARCH1API_KEY=\"your_api_key_here\"\nnpm start\n```\n\n**Method C: MCP Client Configuration (Advanced)**\n\nSome MCP clients allow specifying environment variables directly in their configuration. This is useful for clients like Cursor, VS Code extensions, etc.\n\n```json\n{\n  \"mcpServers\": {\n    \"search1api\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"search1api-mcp\"\n      ],\n      \"env\": {\n        \"SEARCH1API_KEY\": \"YOUR_SEARCH1API_KEY\"\n      }\n    }\n  }\n}\n```\n\n**Note for LibreChat Users:** Due to current limitations in LibreChat, Method A (Project `.env` File) is the **required** method. See the dedicated integration section below for full instructions.\n\n## Integration with LibreChat (Docker)\n\nThis section details the required steps for integrating with LibreChat via Docker.\n\n**Overview:**\n\n1.  Clone this server's repository into a location accessible by your LibreChat `docker-compose.yml`.\n2.  Configure the required API key using the **Project `.env` File method** within this server's directory.\n3.  Build this server.\n4.  Tell LibreChat how to run this server by editing `librechat.yaml`.\n5.  Make sure the built server code is available inside the LibreChat container via a Docker volume bind.\n6.  Restart LibreChat.\n\n**Step-by-Step:**\n\n1.  **Clone the Repository:**\n    Navigate to the directory on your host machine where you manage external services for LibreChat (this is often alongside your `docker-compose.yml`). A common location is a dedicated `mcp-server` directory.\n    ```bash\n    # Example: Navigate to where docker-compose.yml lives, then into mcp-server\n    cd /path/to/your/librechat/setup/mcp-server\n    git clone https://github.com/fatwang2/search1api-mcp.git\n    ```\n\n2.  **Navigate into the Server Directory:**\n    ```bash\n    cd search1api-mcp\n    ```\n\n3.  **Configure API Key (Project `.env` File Method - Required for LibreChat):**\n    ```bash\n    # Create the .env file\n    echo \"SEARCH1API_KEY=your_api_key_here\" > .env\n    # IMPORTANT: Replace 'your_api_key_here' with your actual Search1API key\n    ```\n\n4.  **Install Dependencies and Build:**\n    This step compiles the server code into the `build` directory.\n    ```bash\n    npm install\n    npm run build\n    ```\n\n5.  **Configure `librechat.yaml`:**\n    Edit your main `librechat.yaml` file to tell LibreChat how to execute this MCP server. Add an entry under `mcp_servers`:\n    ```yaml\n    # In your main librechat.yaml\n    mcp_servers:\n      # You can add other MCP servers here too\n      search1api:\n        # Optional: Display name for the server in LibreChat UI\n        # name: Search1API Tools\n\n        # Command tells LibreChat to use 'node'\n        command: node\n\n        # Args specify the script for 'node' to run *inside the container*\n        args:\n          - /app/mcp-server/search1api-mcp/build/index.js\n    ```\n    *   The `args` path (`/app/...`) is the location *inside* the LibreChat API container where the built server will be accessed (thanks to the volume bind in the next step).\n\n6.  **Configure Docker Volume Bind:**\n    Edit your `docker-compose.yml` (or more likely, your `docker-compose.override.yml`) to map the `search1api-mcp` directory from your host machine into the LibreChat API container. Find the `volumes:` section for the `api:` service:\n    ```yaml\n    # In your docker-compose.yml or docker-compose.override.yml\n    services:\n      api:\n        # ... other service config ...\n        volumes:\n          # ... other volumes likely exist here ...\n\n          # Add this volume bind:\n          - ./mcp-server/search1api-mcp:/app/mcp-server/search1api-mcp\n    ```\n    *   **Host Path (`./mcp-server/search1api-mcp`):** This is the path on your host machine *relative* to where your `docker-compose.yml` file is located. Adjust it if you cloned the repo elsewhere.\n    *   **Container Path (`:/app/mcp-server/search1api-mcp`):** This is the path *inside* the container. It **must match** the directory structure used in the `librechat.yaml` `args` path.\n\n7.  **Restart LibreChat:**\n    Apply the changes by rebuilding (if you modified `docker-compose.yml`) and restarting your LibreChat stack.\n    ```bash\n    docker compose down && docker compose up -d --build\n    # Or: docker compose restart api (if only librechat.yaml changed)\n    ```\n\nNow, the Search1API server should be available as a tool provider within LibreChat.\n\n## Features\n\n- Web search functionality\n- News search functionality\n- Web page content extraction\n- Website sitemap extraction\n- Deep thinking and complex problem solving with DeepSeek R1\n- Seamless integration with Claude Desktop, Cursor, Windsurf, Cline and other MCP clients\n\n## Tools\n\n### 1. Search Tool\n- Name: `search`\n- Description: Search the web using Search1API\n- Parameters:\n  * `query` (required): Search query in natural language. Be specific and concise for better results\n  * `max_results` (optional, default: 10): Number of results to return\n  * `search_service` (optional, default: \"google\"): Search service to use (google, bing, duckduckgo, yahoo, x, reddit, github, youtube, arxiv, wechat, bilibili, imdb, wikipedia)\n  * `crawl_results` (optional, default: 0): Number of results to crawl for full webpage content\n  * `include_sites` (optional): List of sites to include in search\n  * `exclude_sites` (optional): List of sites to exclude from search\n  * `time_range` (optional): Time range for search results (\"day\", \"month\", \"year\")\n\n### 2. News Tool\n- Name: `news`\n- Description: Search for news articles using Search1API\n- Parameters:\n  * `query` (required): Search query in natural language. Be specific and concise for better results\n  * `max_results` (optional, default: 10): Number of results to return\n  * `search_service` (optional, default: \"bing\"): Search service to use (google, bing, duckduckgo, yahoo, hackernews)\n  * `crawl_results` (optional, default: 0): Number of results to crawl for full webpage content\n  * `include_sites` (optional): List of sites to include in search\n  * `exclude_sites` (optional): List of sites to exclude from search\n  * `time_range` (optional): Time range for search results (\"day\", \"month\", \"year\")\n\n### 3. Crawl Tool\n- Name: `crawl`\n- Description: Extract content from a URL using Search1API\n- Parameters:\n  * `url` (required): URL to crawl\n\n### 4. Sitemap Tool\n- Name: `sitemap`\n- Description: Get all related links from a URL\n- Parameters:\n  * `url` (required): URL to get sitemap\n\n### 5. Reasoning Tool\n- Name: `reasoning`\n- Description: A tool for deep thinking and complex problem solving with fast deepseek r1 model and web search ability(You can change to any other model in search1api website but the speed is not guaranteed)\n- Parameters:\n  * `content` (required): The question or problem that needs deep thinking\n\n### 6. Trending Tool\n- Name: `trending`\n- Description: Get trending topics from popular platforms\n- Parameters:\n  * `search_service` (required): Specify the platform to get trending topics from (github, hackernews)\n  * `max_results` (optional, default: 10): Maximum number of trending items to return\n\n## Version History\n\n- v0.2.0: Added fallback `.env` support for LibreChat integration and updated dependencies.\n- v0.1.8: Added X(Twitter) and Reddit search services\n- v0.1.7: Added Trending tool for GitHub and Hacker News\n- v0.1.6: Added Wikipedia search service\n- v0.1.5: Added new search parameters (include_sites, exclude_sites, time_range) and new search services (arxiv, wechat, bilibili, imdb)\n- v0.1.4: Added reasoning tool with deepseek r1 and updated the Cursor and Windsurf configuration guide\n- v0.1.3: Added news search functionality\n- v0.1.2: Added sitemap functionality\n- v0.1.1: Added web crawling functionality\n- v0.1.0: Initial release with search functionality\n\n## License\n\nThis project is licensed under the MIT License - see the LICENSE file for details.\n",
      "npm_url": "https://www.npmjs.com/package/search1api-mcp",
      "npm_downloads": 9263,
      "keywords": [
        "search1api",
        "search",
        "fatwang2",
        "search1api mcp",
        "utilizing search1api",
        "search1api retrieve"
      ],
      "category": "web-search"
    },
    "fefergrgrgrg--smileyCoinDev": {
      "owner": "fefergrgrgrg",
      "name": "smileyCoinDev",
      "url": "https://github.com/fefergrgrgrg/smileyCoinDev",
      "imageUrl": "/freedevtools/mcp/pfp/fefergrgrgrg.webp",
      "description": "Provides web development tools to capture screenshots and programmatically list available screens, helping to streamline development tasks and automate operations.",
      "stars": 15,
      "forks": 1,
      "license": "MIT License",
      "language": "C++",
      "updated_at": "2025-05-16T13:54:28Z",
      "readme_content": "Litecoin Core integration/staging tree\n=====================================\n\n[![Build Status](https://travis-ci.org/litecoin-project/litecoin.svg?branch=master)](https://travis-ci.org/litecoin-project/litecoin)\n\nhttps://litecoin.org\n\nWhat is Litecoin?\n----------------\n\nLitecoin is an experimental digital currency that enables instant payments to\nanyone, anywhere in the world. Litecoin uses peer-to-peer technology to operate\nwith no central authority: managing transactions and issuing money are carried\nout collectively by the network. Litecoin Core is the name of open source\nsoftware which enables the use of this currency.\n\nFor more information, as well as an immediately useable, binary version of\nthe Litecoin Core software, see [https://litecoin.org](https://litecoin.org).\n\nLicense\n-------\n\nLitecoin Core is released under the terms of the MIT license. See [COPYING](COPYING) for more\ninformation or see https://opensource.org/licenses/MIT.\n\nDevelopment Process\n-------------------\n\nThe `master` branch is regularly built and tested, but is not guaranteed to be\ncompletely stable. [Tags](https://github.com/litecoin-project/litecoin/tags) are created\nregularly to indicate new official, stable release versions of Litecoin Core.\n\nThe contribution workflow is described in [CONTRIBUTING.md](CONTRIBUTING.md)\nand useful hints for developers can be found in [doc/developer-notes.md](doc/developer-notes.md).\n\nThe developer [mailing list](https://groups.google.com/forum/#!forum/litecoin-dev)\nshould be used to discuss complicated or controversial changes before working\non a patch set.\n\nDeveloper IRC can be found on Freenode at #litecoin-dev.\n\nTesting\n-------\n\nTesting and code review is the bottleneck for development; we get more pull\nrequests than we can review and test on short notice. Please be patient and help out by testing\nother people's pull requests, and remember this is a security-critical project where any mistake might cost people\nlots of money.\n\n### Automated Testing\n\nDevelopers are strongly encouraged to write [unit tests](src/test/README.md) for new code, and to\nsubmit new unit tests for old code. Unit tests can be compiled and run\n(assuming they weren't disabled in configure) with: `make check`. Further details on running\nand extending unit tests can be found in [/src/test/README.md](/src/test/README.md).\n\nThere are also [regression and integration tests](/test), written\nin Python, that are run automatically on the build server.\nThese tests can be run (if the [test dependencies](/test) are installed) with: `test/functional/test_runner.py`\n\nThe Travis CI system makes sure that every pull request is built for Windows, Linux, and macOS, and that unit/sanity tests are run automatically.\n\n### Manual Quality Assurance (QA) Testing\n\nChanges should be tested by somebody other than the developer who wrote the\ncode. This is especially important for large or high-risk changes. It is useful\nto add a test plan to the pull request description if testing the changes is\nnot straightforward.\n\nTranslations\n------------\n\nWe only accept translation fixes that are submitted through [Bitcoin Core's Transifex page](https://www.transifex.com/projects/p/bitcoin/).\nTranslations are converted to Litecoin periodically.\n\nTranslations are periodically pulled from Transifex and merged into the git repository. See the\n[translation process](doc/translation_process.md) for details on how this works.\n\n**Important**: We do not accept translation changes as GitHub pull requests because the next\npull from Transifex would automatically overwrite them again.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "smileycoindev",
        "fefergrgrgrg",
        "screenshots",
        "fefergrgrgrg smileycoindev",
        "smileycoindev provides",
        "capture screenshots"
      ],
      "category": "web-search"
    },
    "fengin--search-server": {
      "owner": "fengin",
      "name": "search-server",
      "url": "https://github.com/fengin/search-server",
      "imageUrl": "/freedevtools/mcp/pfp/fengin.webp",
      "description": "Provides online and local search capabilities with support for multiple search engines including Brave Search, Metaso Search, and Bocha Search. Designed for integration with AI models like Cursor and Claude Desktop, enabling enhanced content retrieval.",
      "stars": 73,
      "forks": 3,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-19T03:59:51Z",
      "readme_content": "# Search MCP Server\n\n\n\n一个基于MCP协议的搜索服务实现，提供多种搜索引擎支持，Cursor和Claude Desktop能与之无缝集成。\n\n使用Python开发，支持异步处理和高并发请求，目前支持三种搜索引擎选择：\n\n- Brave Search ：国外一家专业提供搜索接口服务产品\n\n- 秘塔（Metaso）搜索：秘塔AI搜索的逆向实现接口，非官方接口\n\n- 博查（bocha）搜索：国内Search API市场占有率最高的搜索API产品\n\n更多MCP知识，见AI全书([ 一文看懂什么是MCP(大模型上下文)？用来干什么的？怎么用它？](https://aibook.ren/archives/mcp-course))\n\n**作者**： 凌封（微信fengin）\n\n**网址**：[https://aibook.ren]()（AI全书）\n\n## 使用示例\n\n\n\n## 功能特点\n\n- **多搜索引擎支持**: \n  - Brave Search: 提供网络搜索和位置搜索\n  - Metaso搜索: 提供网络搜索和学术搜索，支持简洁和深入两种模式\n  - 博查搜索: 提供网络搜索，支持时间范围过滤、详细摘要和图片搜索\n- **适用场景**: Claude Desktop或者Cursor无缝集成使用，大大扩展工具的内容获取能力\n- **模块化设计**: 每个搜索引擎都是独立的模块，也可以单独拷出去其他地方使用\n\n## 三种搜索的选择\n\n<mark>运行时只能生效一种搜索引擎</mark>，为了方便大家选择配置哪个上线，我列了下大致的对比：\n\n| 搜索引擎   | 国内/外 | 需魔法 | 自带总结 | 质量  | 免费    | 官方  | 速度      | 注册门槛 |\n| ------ | ---- | --- | ---- | --- | ----- | --- | ------- | ---- |\n| Brave  | 国外   | 是   | 否    | 高   | 是(限量) | 是   | 中       | 很高   |\n| Metaso | 国内   | 否   | 是    | 中   | 是     | 否   | 慢(AI总结) | 低    |\n| Bocha  | 国内   | 否   | 否    | 高   | 否     | 是   | 极快      | 低    |\n\n## 安装和使用\n\n### 1. 环境要求\n\n- Python 3.10+\n- uv 0.24.0+\n- node.js v20.15.0\n- cursor >=0.45.10 (低于该版本mcp server配置老是连不上)\n- 科学上网（仅使用Brave Search需要）\n\n#### 1.1 安装浏览器驱动(仅Metaso需要)\n\n```\n# 安装Playwright框架\npip install playwright>=1.35.0\n# 安装浏览器驱动,仅安装chromium\nplaywright install chromium\n```\n\n\n\n### 2.下载代码\n\n```bash\ngit clone https://github.com/fengin/search-server.git\n```\n\n### 3. 启用你要的搜索引擎\n\n打开项目根目录，修改server.py以下代码选择启用类型：\n\n```python\n# 搜索引擎配置\nSEARCH_ENGINE = os.getenv(\"SEARCH_ENGINE\", \"bocha\")\n```\n\n其中值分别对应有brave、metaso、bocha，也可以通过配置环境变量SEARCH_ENGIN\n\n### 4\\. 配置对应的搜索模块\n\n以下三个模块目录下都对应有一个config.py文件：\n\n- src\\search\\proxy\\brave\n\n- src\\search\\proxy\\metaso\n\n- src\\search\\proxy\\bocha\n\n根据你的选择，修改对应的config.py文件配置\n\n#### 4.1 brave search配置\n\n```python\n# 检查API密钥\nBRAVE_API_KEY = os.getenv(\"BRAVE_API_KEY\")\nif not BRAVE_API_KEY:\n    BRAVE_API_KEY = \"你申请的 brave_api_key\"\n```\n\n在Claude Desktop里面使用的话，也可以在Claude Desktop里配置通过环境变量传这个参数，但是Cursor目前不支持环境变量，只能在这文件里修改\n\nAPI KEY 申请地址：[Brave Search - API](https://api-dashboard.search.brave.com/login)\n\n<mark>申请门槛比较高</mark>，要求：\n\n- 魔法（使用时也需要）\n\n- 邮箱验证\n\n- 信用卡（可以用虚拟的：[https://cardgenerator.org/](https://cardgenerator.org/)）\n\n#### 4.2 秘塔(metaso)配置\n\n```python\n# 认证信息\nMETASO_UID = os.getenv(\"METASO_UID\")\nMETASO_SID = os.getenv(\"METASO_SID\")\nif not METASO_UID or not METASO_SID:\n    METASO_UID = \"你获取的 metaso_uid\"\n    METASO_SID = \"你获取的 metaso_sid\"\n```\n\n同样Claude Desktop使用可以通过 MCP Servers配置里环境变量；\n\n**uid和sid获取方式：**\n\n进入秘塔AI搜索，登录账号（<mark>建议登录账号，否则可能遭遇奇怪的限制</mark>），然后F12打开开发者工具，从Application > Cookies中找到`uid`和`sid`的值。\n\n\n\n**多账号接入**\n\n<mark>注意：目前怀疑秘塔对IP地址的总搜索次数有限制，建议加入IP轮换</mark>\n\n你可以通过提供多个账号的uid-sid并使用`,`修改下相关的使用代码，每次请求服务会从中挑选一个，本人后续再考虑。\n\n#### 4.3 博查(bocha)配置\n\n```python\nBOCHA_API_KEY = os.getenv(\"BOCHA_API_KEY\", \"\")\nif not BOCHA_API_KEY:\n    BOCHA_API_KEY=\"你申请的 bocha_api_key\"\n```\n\n注册申请地址：https://open.bochaai.com/\n\n调用按次数收费，不便宜，但是搜索质量确实比较好，我这有少量的免费试用码，有需要的微信联系我；\n\n### 5. AI工具配置\n\n#### 5.1 Cursor里配置\n\n\n\n- name:  search\n\n- type:  cmd\n\n- command: uv --directory D:\\\\code\\\\search-server run search\n\n其中 “D:\\code\\search-server” 就是你拉下来源代码目录\n\n#### 5.2 Claude Desktop配置\n\n找到配置文件\n\n**方法一**\n\n```\n# widnows\nC:\\Users\\{用户}\\AppData\\Roaming\\Claude\\claude_desktop_config.json\n# mac/linux 应该在用户家目录下找\n```\n\n**方法二**\n\n打开Claude Desktop应用进入查看：\nClaude Desktop—>菜单—>Settings—>Developer—>Edit Config\n\n编辑增加以下MCP Server:\n\n```json\n{\n  \"mcpServers\": {\n    \"search\": {\n            \"command\": \"uv\",\n            \"args\": [\n                \"--directory\",\n                \"D:\\\\code\\\\search-server\",\n                \"run\",\n                \"search\"\n            ],\n            \"env\": {\n                \"BRAVE_API_KEY\": \"你申请的API KEY\"\n            }\n        }\n  }\n}\n```\n\n环境变量看你需要，如果代码改了这就没必要配置了\n\n<mark>Cursor会弹出一个黑窗口，不要关闭，不要关闭</mark>，这是启动的MCP Server进程，目前没办法解决不弹出来。\n\nClaude Desktop配置完一定要重启应用才生效。\n\n#### 5.4 问题排错\n\n配置Cursor后很多人遇到，MCP Servers里面配置完，还是显示状态红点，Tools Not Found，使用时也不会去调用，这是因为根本没配置好。\n\n大部分可能出现的情况是：\n\n1. 环境没准备好，包括需要的软件及版本要求，详见环境章节\n\n2. 准备的环境不对，像windows有cmd终端、powershell终端、还有可能装了gitbash终端，你打开cmd终端（cursor一般是这个）检查下环境，直接运行下uv --directory D:\\code\\search-server run search\n\n3. 配置路径/命令不对，可以打开终端运行命令看下：uv --directory D:\\code\\search-server run search\n\n4. 把黑窗口关闭了，再次开启需要重启Cursor\n\n5. Cursor版本太旧\n\n6. 运行时报以下错，原因是没有安装chromium，解决办法见环境准备1.1章节\n   \n   ```shell\n   错误:搜索执行错误:BrowserType.launch persistent context:Executable doesn't exist atC:\\Users\\fengi\\AppDatalLocal\\ms-playwright\\chromium headless shell-1155\\chrome-winlheadless shell.exe\n   ```\n\n### 6. 使用\n\n<mark>在你的Claude Desktop 或者 Cursor 里直接你正常的工作就行了，必要时它会自动调用搜索接口获取内容</mark>，比如说，你整理下网络上2025年技术发展方向作为软件什么内容，它就会去调用搜索工具获取网络信息：\n\n- 配置好工具后，它的信息里面就知道有这个工具\n\n- 根据你的要求，它自动会分析判断需要用到搜索工具\n\n- 根据需求，提取关键词，调用搜索工具\n\n- 根据搜索返回内容，组织你要的结果\n\n有一点需要注意的，<mark>在Cursor里面，必须启用composer的agent模式工作才会生效</mark>，调用工具时，也需要你点下执行；\n\n## 技术内幕\n\n### 项目结构\n\n```shell\nsearch/\n├── __init__.py\n├── server.py              # MCP服务器实现\n└── proxy/                 # 搜索引擎代理\n    ├── brave/             # Brave搜索模块\n    │   ├── __init__.py\n    │   ├── client.py      # 核心客户端实现\n    │   ├── config.py      # 配置和速率限制\n    │   └── exceptions.py  # 异常定义\n    ├── metaso/            # Metaso搜索模块\n    │   ├── __init__.py\n    │   ├── client.py      # 核心客户端实现\n    │   ├── config.py      # 配置和速率限制\n    │   └── exceptions.py  # 异常定义\n    ├── bocha/             # 博查搜索模块\n    │   ├── __init__.py\n    │   ├── client.py      # 核心客户端实现\n    │   ├── config.py      # 配置和速率限制\n    │   └── exceptions.py  # 异常定义\n    ├── brave_search.py    # Brave MCP工具实现\n    ├── metaso_search.py   # Metaso MCP工具实现\n    └── bocha_search.py    # 博查搜索MCP工具实现\n```\n\n### 接口参数\n\n#### Brave Search引擎\n\n- **search**\n  \n  - 执行网络搜索，支持分页和过滤\n  - 输入参数:\n    - `query` (string): 搜索关键词\n    - `count` (number, 可选): 每页结果数量(最大20)\n    - `offset` (number, 可选): 分页偏移量(最大9)\n\n- **location_search**\n  \n  - 搜索地理位置相关信息（商家、餐厅等）\n  - 输入参数:\n    - `query` (string): 位置搜索关键词\n    - `count` (number, 可选): 结果数量(最大20)\n  - 无相关结果时自动切换到网络搜索\n\n#### Metaso搜索引擎\n\n- **search**\n  \n  - 执行网络搜索，支持多种模式\n  - 输入参数:\n    - `query` (string): 搜索关键词\n    - `mode` (string, 可选): 搜索模式\n      - `concise`: 简洁模式，回答简短精炼\n      - `detail`: 深入模式，回答详细全面（默认）\n      - `research`: 研究模式，回答深度分析（<mark>目前暂不支持，逆向没有成功</mark>）\n\n- **scholar_search**\n  \n  - 执行学术搜索，专门用于查找学术资源\n  - 输入参数:\n    - `query` (string): 学术搜索关键词\n    - `mode` (string, 可选): 搜索模式，同上\n\n#### 博查搜索引擎\n\n- **search**\n  - 执行网络搜索，支持时间范围过滤和详细摘要\n  - 输入参数:\n    - `query` (string): 搜索关键词\n    - `count` (number, 可选): 结果数量(1-10，默认10)\n    - `page` (number, 可选): 页码，从1开始\n    - `freshness` (string, 可选): 时间范围\n      - `noLimit`: 不限时间（默认）\n      - `oneDay`: 一天内\n      - `oneWeek`: 一周内\n      - `oneMonth`: 一月内\n      - `oneYear`: 一年内\n    - `summary` (boolean, 可选): 是否显示详细摘要，默认false\n  - 返回内容:\n    - 搜索统计信息（总结果数、当前页/总页数、本页结果数）\n    - 网页搜索结果（标题、URL、来源、摘要、发布时间）\n    - 相关图片信息（尺寸、来源、URL）\n\n##",
      "npm_url": "https://www.npmjs.com/package/mcp-search-server",
      "npm_downloads": 0,
      "keywords": [
        "search",
        "fengin",
        "retrieval",
        "search fengin",
        "fengin search",
        "search server"
      ],
      "category": "web-search"
    },
    "fr0ziii--perplexity-mcp-server": {
      "owner": "fr0ziii",
      "name": "perplexity-mcp-server",
      "url": "https://github.com/fr0ziii/perplexity-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/null.webp",
      "description": "Integrate with Perplexity AI for chatting, searching, and retrieving documentation from various sources. Provides a standardized access method to Perplexity AI's capabilities for MCP-based systems.",
      "stars": 0,
      "forks": 0,
      "license": "Unknown",
      "language": "Unknown",
      "updated_at": "",
      "readme_content": "",
      "npm_url": "https://www.npmjs.com/package/perplexity-mcp-server",
      "npm_downloads": 1579,
      "keywords": [
        "search",
        "searching",
        "perplexity",
        "perplexity ai",
        "perplexity mcp",
        "chatting searching"
      ],
      "category": "web-search"
    },
    "funwarioisii--cosense-mcp-server": {
      "owner": "funwarioisii",
      "name": "cosense-mcp-server",
      "url": "https://github.com/funwarioisii/cosense-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/funwarioisii.webp",
      "description": "Retrieve page data efficiently and integrate with Claude Desktop for streamlined information management. The server enhances application capabilities by providing real-time context through an MCP interface.",
      "stars": 1,
      "forks": 2,
      "license": "No License",
      "language": "JavaScript",
      "updated_at": "2025-04-29T13:47:55Z",
      "readme_content": "# cosense-mcp-server MCP Server\n\nMCP server for [cosense](https://cosen.se)\n\n## Features\n\n- Get Page\n\n## Development\n\nInstall dependencies:\n\n```bash\nnpm install\n```\n\nBuild the server:\n\n```bash\nnpm run build\n```\n\nFor development with auto-rebuild:\n\n```bash\nnpm run watch\n```\n\n## Installation\n\n```bash\ngit clone https://github.com/funwarioisii/cosense-mcp-server.git\ncd cosense-mcp-server\nnpm run install\nnpm run build\n```\n\nTo use with Claude Desktop, add the server config:\n\nOn MacOS: `~/Library/Application\\ Support/Claude/claude_desktop_config.json`\nOn Windows: `%APPDATA%/Claude/claude_desktop_config.json`\n\n```json\n{\n  \"mcpServers\": {\n    \"cosense-mcp-server\": {\n      \"command\": \"node\",\n      \"args\": [\"/path/to/cosense-mcp-server/build/index.js\"],\n      \"env\": {\n        \"COSENSE_PROJECT_NAME\": \"your_project_name\",\n        \"COSENSE_SID\": \"your_sid\"\n      }\n    }\n  }\n}\n```\n\n`COSENSE_SID` is optional.\nIf you want to use this server towards a private project, you need to set `COSENSE_SID`.\n\n### Debugging\n\nSince MCP servers communicate over stdio, debugging can be challenging. We recommend using the [MCP Inspector](https://github.com/modelcontextprotocol/inspector), which is available as a package script:\n\n```bash\nnpm run inspector\n```\n\nThe Inspector will provide a URL to access debugging tools in your browser.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "web",
        "search",
        "page",
        "web search",
        "mcp server",
        "page data"
      ],
      "category": "web-search"
    },
    "gabrimatic--mcp-web-search-tool": {
      "owner": "gabrimatic",
      "name": "mcp-web-search-tool",
      "url": "https://github.com/gabrimatic/mcp-web-search-tool",
      "imageUrl": "/freedevtools/mcp/pfp/gabrimatic.webp",
      "description": "Provides real-time web search capabilities, enabling AI assistants to retrieve current information from various sources through a modular architecture. Returns structured JSON results for enhanced context-aware responses in areas like weather, news, sports, and finance.",
      "stars": 13,
      "forks": 2,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-08-20T16:43:48Z",
      "readme_content": "# MCP Web Search Tool\n\nA powerful Model Context Protocol (MCP) server providing real-time web search capabilities through pluggable search providers. Currently integrated with the [Brave Search API](https://api-dashboard.search.brave.com/app/documentation/web-search/get-started).\n\n\n\n## ✨ Features\n\n- **Real-Time Information Access**: Enables AI assistants to retrieve up-to-date information from the web\n- **Pluggable Search Providers**: Modular architecture allows for easy switching between different search engines\n- **Structured Output Format**: Returns search results in a clean, consistent JSON format\n- **Smart Query Handling**: Automatically categorizes queries and provides AI assistants with context-aware guidance\n\n## 📋 Requirements\n\n- **Node.js**: v16.x or newer\n- **npm**: v7.x or newer\n- **Brave Search API Key**: Required for accessing the Brave Search API\n\n## 🚀 Installation\n\n1. **Clone the Repository**:\n   ```bash\n   git clone https://github.com/gabrimatic/mcp-web-search-tool.git\n   cd mcp-web-search-tool\n   ```\n\n2. **Install Dependencies**:\n   ```bash\n   npm install\n   ```\n\n3. **Configure Environment Variables**:\n   Create a `.env` file in the project root:\n   ```\n   BRAVE_API_KEY=your_api_key_here\n   MAX_RESULTS=10 # Optional: Default is 10\n   REQUEST_TIMEOUT=10000 # Optional: Default is 10000ms\n   ```\n\n4. **Build the Project**:\n   ```bash\n   npm run build\n   ```\n\n## 💻 Usage\n\n### Starting the Server\n```bash\nnpm start\n```\n\n### Testing the Server\n```bash\nnode test-server.js\n```\n\n## Integration with Claude Desktop App _(optional)_\n\nOne of the most exciting aspects of this project is its seamless integration with the Claude Desktop app. This integration allows users to ask Claude questions that require real-time information, and Claude will automatically use the web search tool to provide up-to-date answers.\n\n### Configuration\n\n1. Create a `claude_desktop_config.json` file:\n   ```json\n   {\n     \"mcpServers\": {\n       \"mcp-web-search\": {\n         \"command\": \"node\",\n         \"args\": [\n           \"/path/to/your/mcp-web-search-tool/build/index.js\"\n         ]\n       }\n     }\n   }\n   ```\n\n2. Launch Claude Desktop with your configuration file.\n\n3. Ask Claude questions requiring real-time information, and it will automatically use the web search tool.\n\n###  🎥 YouTube Video\n\nWatch Claude in action using the MCP Web Search Tool for real-time AI browsing!\n\n[📺 Claude + MCP Web Search – Live Demo](https://youtu.be/6jAnjJSCL30?si=4n0-NtTyG_3SVaFh)\n\n## Example Queries:\n- \"What are analysts saying about the MVP race after tonight’s NBA games?\"\n- \"What are the latest news about artificial intelligence?\"\n- \"What's the weather like in New York today?\"\n- \"How is the stock market performing right now?\"\n\n## 🛠️ Available Tools\n\n#### Web Search\n- **Tool Name**: `web_search`\n- **Description**: Search the web for REAL-TIME information\n- **Necessarily For**: Weather, current events, sports scores, stock market updates\n- **Parameters**:\n  - `search_term` (string): The search term to look up\n  - `provider` (string, optional): Search provider (defaults to Brave)\n\n### Query Categories\nThe tool automatically categorizes queries into:\n- Weather information\n- Current events and news\n- Sports scores and results\n- Stock market and financial data\n- Time-sensitive information\n- General information seeking\n\n## 📜 License\n\n[MIT License](LICENSE)\n\n## 👨‍💻 Developer\nBy [Hossein Yousefpour](https://gabrimatic.info \"Hossein Yousefpour\")\n\n&copy; All rights reserved.\n\n## 📝 Medium Article\n\nRead more about the MCP Web Search Tool, its capabilities, and how it enhances AI-driven web search in our detailed Medium article:\n📖 [Deep Dive into MCP Web Search Tool](https://medium.com/@gabrimatic/introducing-mcp-web-search-tool-bridging-ai-assistants-to-real-time-web-information-5df9ab92ad02)\n\n\n## ☕ Support\n<a href=\"https://www.buymeacoffee.com/gabrimatic\" target=\"_blank\"><img src=\"https://www.buymeacoffee.com/assets/img/custom_images/orange_img.png\" alt=\"Buy Me A Book\" style=\"height: 41px !important;width: 174px !important;box-shadow: 0px 3px 2px 0px rgba(190, 190, 190, 0.5) !important;-webkit-box-shadow: 0px 3px 2px 0px rgba(190, 190, 190, 0.5) !important;\" ></a>",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "search",
        "web",
        "gabrimatic",
        "search gabrimatic",
        "web search",
        "mcp web"
      ],
      "category": "web-search"
    },
    "galihfr09--quran_cloud_mcp_server": {
      "owner": "galihfr09",
      "name": "quran_cloud_mcp_server",
      "url": "https://github.com/galihfr09/quran_cloud_mcp_server",
      "imageUrl": "/freedevtools/mcp/pfp/galihfr09.webp",
      "description": "Connects AI models to the Quran API for precise access to Quranic text, minimizing hallucination by retrieving only necessary data upon request. Enables seamless integration with applications such as Claude desktop for enhanced reliability when handling sensitive religious content.",
      "stars": 0,
      "forks": 0,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-05-07T07:39:09Z",
      "readme_content": "# quran_cloud_mcp_server\nMCP server to help LLMs to get access to Quran API (https://alquran.cloud/api) to prevent the hallucination with Quran text.\n\nhallucination is a big problem specially when you are working on sensitive data that each character is important.\n\none way of reducing the hallucination is by providing the context to your LLM but of course with large chunk of text like the holy Quran it's not efficient if you put all text in each request.\n\nSo, in this repo I have created an MCP server that's connect your LLM to a free API https://alquran.cloud/api that enables your model to retrieve only the data he needs.\n\nAlso, I will show to you how we can connect this MCP server to Claude desktop application.\n\n## Example of Claude the original response\n![Claude original response](https://github.com/marwanWaly/quran_cloud_mcp_server/blob/main/imgs/original_claude_response.png?raw=true)\n\n## Example of Claude the new response after connecting to Search-Quran MCP server \n![Claude New response](https://github.com/marwanWaly/quran_cloud_mcp_server/blob/main/imgs/new_claude_response.png?raw=true)\n\n## Installation\nmake sure you have python 3.13 & pip\n\nOpen your terminal and write these commands\n\n### Cloning\n```\ngit clone https://github.com/marwanWaly/quran_cloud_mcp_server.git\n```\n\n### Move to project directory\n```\ncd quran_cloud_mcp_server\n```\n\n### Create virtual environment\n```\npython -m venv .venv\n```\n\n### Activate venv\nOn Windows\n```\n.\\venv\\Scripts\\activate\n```\n\nOn Mac or linux\n```\nsource .venv/bin/activate\n```\n\n### Python packages installation\nUse the package manager [pip](https://pip.pypa.io/en/stable/) to install requirements.txt.\n```\npip install -r requirements.txt\n```\n\n### Create .env file\n```\nOPENAI_API_KEY=Your-secret-key\n```\n\n### Run in terminal\n```\npython client.py\n```\n\nnow you can directly chat with GPT4o in your terminal\n\n![server running in terminal](https://github.com/marwanWaly/quran_cloud_mcp_server/blob/main/imgs/run_server_in_terminal.PNG)\n\n## Connect the server to Claude Desktop\nDownload [Claude](https://claude.ai/download) desktop and open it\n\n## Step 1\nSelect setting from the file menu\n\n![step 1](https://github.com/marwanWaly/quran_cloud_mcp_server/blob/main/imgs/add_mcp_to_claude_step-1.png)\n\n## Step 2\nClick on `Developer` then `Edit Config`\n\n![step 2](https://github.com/marwanWaly/quran_cloud_mcp_server/blob/main/imgs/add_mcp_to_claude_step-2.png)\n\n## Step 3\nOpen `claude_desktop_config.json`\n\n![step 3](https://github.com/marwanWaly/quran_cloud_mcp_server/blob/main/imgs/add_mcp_to_claude_step-3.png)\n\n## Step 4\nWrite this configuration in the file\n\n```\n{\n  \"mcpServers\": {\n    \"Search-Quran\": {\n      \"command\": \"python\",\n      \"args\": [\n        \"PROJECT_PATH_ON_YOUR_PC\\\\server.py\"\n      ],\n      \"host\": \"127.0.0.1\",\n      \"port\": 8080,\n      \"timeout\": 30000\n    }\n  }\n}\n```\n\nDon't forget to replace `PROJECT_PATH_ON_YOUR_PC` with the absolute path to your project server\n\n![step 4](https://github.com/marwanWaly/quran_cloud_mcp_server/blob/main/imgs/add_mcp_to_claude_step-4.png)\n\n## Step 5\nRestart Claude app (make sure it's completely closed from your taskbar by right click on Claude icon and select `Quit`)\n\nCheck if the new MCP has been added\n\n![step 5](https://github.com/marwanWaly/quran_cloud_mcp_server/blob/main/imgs/add_mcp_to_claude_step-5.png)\n\nClick on tools icon \n\n![step 6](https://github.com/marwanWaly/quran_cloud_mcp_server/blob/main/imgs/add_mcp_to_claude_step-6.png)\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "quran_cloud_mcp_server",
        "quran",
        "quranic",
        "quran api",
        "access quranic",
        "galihfr09 quran_cloud_mcp_server"
      ],
      "category": "web-search"
    },
    "garylab--serper-mcp-server": {
      "owner": "garylab",
      "name": "serper-mcp-server",
      "url": "https://github.com/garylab/serper-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/garylab.webp",
      "description": "Provides Google search results to LLMs through Serper integration, enabling dynamic retrieval of up-to-date search information.",
      "stars": 15,
      "forks": 7,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-09-18T17:00:37Z",
      "readme_content": "# Serper MCP Server\n[![smithery badge](https://smithery.ai/badge/@garylab/serper-mcp-server)](https://smithery.ai/server/@garylab/serper-mcp-server)\n\nA Model Context Protocol server that provides **Google Search via Serper**. This server enables LLMs to get search result information from Google.\n\n## Available Tools\n\n- `google_search` - Set [all the parameters](src/serper_mcp_server/schemas.py#L15)\n- `google_search_images` - Set [all the parameters](src/serper_mcp_server/schemas.py#L15)\n- `google_search_videos` - Set [all the parameters](src/serper_mcp_server/schemas.py#L15)\n- `google_search_places` - Set [all the parameters](src/serper_mcp_server/schemas.py#L20)\n- `google_search_maps` - Set [all the parameters](src/serper_mcp_server/schemas.py#L24)\n- `google_search_reviews` - Set [all the parameters](src/serper_mcp_server/schemas.py#L34)\n- `google_search_news` - Set [all the parameters](src/serper_mcp_server/schemas.py#L15)\n- `google_search_shopping` - Set [all the parameters](src/serper_mcp_server/schemas.py#L45)\n- `google_search_lens` - Set [all the parameters](src/serper_mcp_server/schemas.py#L50)\n- `google_search_scholar` - Set [all the parameters](src/serper_mcp_server/schemas.py#L20)\n- `google_search_parents` - Set [all the parameters](src/serper_mcp_server/schemas.py#L56)\n- `google_search_autocomplete` - Set [all the parameters](src/serper_mcp_server/schemas.py#L20)\n- `webpage_scrape` - Set [all the parameters](src/serper_mcp_server/schemas.py#L62)\n\n\n## Usage\n\n### Installing via Smithery\n\nTo install Serper MCP Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@garylab/serper-mcp-server):\n\n```bash\nnpx -y @smithery/cli install @garylab/serper-mcp-server --client claude\n```\n\n### Using `uv` (recommended)\n\n1. Make sure you had installed [`uv`](https://docs.astral.sh/uv/) on your os system.\n\n2. In your MCP client code configuration or **Claude** settings (file `claude_desktop_config.json`) add `serper` mcp server:\n    ```json\n    {\n        \"mcpServers\": {\n            \"serper\": {\n                \"command\": \"uvx\",\n                \"args\": [\"serper-mcp-server\"],\n                \"env\": {\n                    \"SERPER_API_KEY\": \"<Your Serper API key>\"\n                }\n            }\n        }\n    }\n    ```\n    `uv` will download mcp server automatically using `uvx` from [pypi.org](https://pypi.org/project/serper-mcp-server/) and apply to your MCP client.\n\n### Using `pip` for project\n1. Add `serper-mcp-server` to your MCP client code `requirements.txt` file.\n    ```txt\n    serper-mcp-server\n    ```\n\n2. Install the dependencies.\n    ```shell\n    pip install -r requirements.txt\n    ```\n\n3. Add the configuration for you client:\n    ```json\n    {\n        \"mcpServers\": {\n            \"serper\": {\n                \"command\": \"python3\",\n                \"args\": [\"-m\", \"serper_mcp_server\"],\n                \"env\": {\n                    \"SERPER_API_KEY\": \"<Your Serper API key>\"\n                }\n            }\n        }\n    }\n    ```\n\n\n### Using `pip` for globally usage\n\n1. Make sure the `pip` or `pip3` is in your os system.\n    ```bash\n    pip install serper-mcp-server\n    # or\n    pip3 install serper-mcp-server\n    ```\n\n2. MCP client code configuration or **Claude** settings, add `serper` mcp server:\n    ```json\n    {\n        \"mcpServers\": {\n            \"serper\": {\n                \"command\": \"python3\",\n                \"args\": [\"serper-mcp-server\"],\n                \"env\": {\n                    \"SERPER_API_KEY\": \"<Your Serper API key>\"\n                }\n            }\n        }\n    }\n    ```\n\n\n## Debugging\n\nYou can use the MCP inspector to debug the server. For `uvx` installations:\n\n```bash\nnpx @modelcontextprotocol/inspector uvx serper-mcp-server\n```\n\nOr if you've installed the package in a specific directory or are developing on it:\n\n```bash\ngit clone https://github.com/garylab/serper-mcp-server.git\ncd serper-mcp-server\nnpx @modelcontextprotocol/inspector uv run serper-mcp-server -e SERPER_API_KEY=<the key>\n```\n\n\n## License\n\nserper-mcp-server is licensed under the MIT License. This means you are free to use, modify, and distribute the software, subject to the terms and conditions of the MIT License. For more details, please see the LICENSE file in the project repository.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "serper",
        "search",
        "google",
        "llms serper",
        "web search",
        "search results"
      ],
      "category": "web-search"
    },
    "geezerrrr--exa-mcp-server": {
      "owner": "geezerrrr",
      "name": "exa-mcp-server",
      "url": "https://github.com/geezerrrr/exa-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/geezerrrr.webp",
      "description": "Provides real-time web search capabilities using Exa's robust search API, enabling AI models to retrieve and cache web information in a secure manner while offering customizable search parameters.",
      "stars": 0,
      "forks": 0,
      "license": "MIT License",
      "language": "",
      "updated_at": "2025-03-22T08:47:51Z",
      "readme_content": "# Exa MCP Server 🔍\n[![npm version](https://badge.fury.io/js/exa-mcp-server.svg)](https://www.npmjs.com/package/exa-mcp-server)\n[![smithery badge](https://smithery.ai/badge/exa)](https://smithery.ai/server/exa)\n\nA Model Context Protocol (MCP) server lets AI assistants like Claude use the Exa AI Search API for web searches. This setup allows AI models to get real-time web information in a safe and controlled way.\n\nDemo video https://www.loom.com/share/ac676f29664e4c6cb33a2f0a63772038?sid=0e72619f-5bfc-415d-a705-63d326373f60\n\n\n## What is MCP? 🤔\n\nThe Model Context Protocol (MCP) is a system that lets AI apps, like Claude Desktop, connect to external tools and data sources. It gives a clear and safe way for AI assistants to work with local services and APIs while keeping the user in control.\n\n## What does this server do? 🚀\n\nThe Exa MCP server:\n- Enables AI assistants to perform web searches using Exa's powerful search API\n- Provides structured search results including titles, URLs, and content snippets\n- Caches recent searches as resources for reference\n- Handles rate limiting and error cases gracefully\n- Supports real-time web crawling for fresh content\n\n\n## Prerequisites 📋\n\nBefore you begin, ensure you have:\n\n- [Node.js](https://nodejs.org/) (v18 or higher)\n- [Claude Desktop](https://claude.ai/download) installed\n- An [Exa API key](https://dashboard.exa.ai/api-keys)\n- Git installed\n\nYou can verify your Node.js installation by running:\n```bash\nnode --version  # Should show v18.0.0 or higher\n```\n\n## Installation 🛠️\n\n### NPM Installation\n\n```bash\nnpm install -g exa-mcp-server\n```\n\n### Using Smithery\n\nTo install the Exa MCP server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/exa):\n\n```bash\nnpx -y @smithery/cli install exa --client claude\n```\n\n### Manual Installation\n\n1. Clone the repository:\n\n```bash\ngit clone https://github.com/exa-labs/exa-mcp-server.git\ncd exa-mcp-server\n```\n\n2. Install dependencies:\n\n```bash\nnpm install\n```\n\n3. Build the project:\n\n```bash\nnpm run build\n```\n\n4. Create a global link (this makes the server executable from anywhere):\n\n```bash\nnpm link\n```\n\n## Configuration ⚙️\n\n### 1. Configure Claude Desktop to recognize the Exa MCP server\n\nYou can find claude_desktop_config.json inside the settings of Claude Desktop app:\n\nOpen the Claude Desktop app and enable Developer Mode from the top-left menu bar. \n\nOnce enabled, open Settings (also from the top-left menu bar) and navigate to the Developer Option, where you'll find the Edit Config button. Clicking it will open the claude_desktop_config.json file, allowing you to make the necessary edits. \n\nOR (if you want to open claude_desktop_config.json from terminal)\n\n#### For macOS:\n\n1. Open your Claude Desktop configuration:\n\n```bash\ncode ~/Library/Application\\ Support/Claude/claude_desktop_config.json\n```\n\n#### For Windows:\n\n1. Open your Claude Desktop configuration:\n\n```powershell\ncode %APPDATA%\\Claude\\claude_desktop_config.json\n```\n\n### 2. Add the Exa server configuration:\n\n```json\n{\n  \"mcpServers\": {\n    \"exa\": {\n      \"command\": \"npx\",\n      \"args\": [\"/path/to/exa-mcp-server/build/index.js\"],\n      \"env\": {\n        \"EXA_API_KEY\": \"your-api-key-here\"\n      }\n    }\n  }\n}\n```\n\nReplace `your-api-key-here` with your actual Exa API key from [dashboard.exa.ai/api-keys](https://dashboard.exa.ai/api-keys).\n\n### 3. Restart Claude Desktop\n\nFor the changes to take effect:\n\n1. Completely quit Claude Desktop (not just close the window)\n2. Start Claude Desktop again\n3. Look for the 🔌 icon to verify the Exa server is connected\n\n## Usage 🎯\n\nOnce configured, you can ask Claude to perform web searches. Here are some example prompts:\n\n```\nCan you search for recent developments in quantum computing?\n```\n\n```\nSearch for and summarize the latest news about artificial intelligence startups in new york.\n```\n\n```\nFind and analyze recent research papers about climate change solutions.\n```\n\n```\nSearch for today's breaking news about tech.\n```\n\n```\nSearch for the top 10 AI research papers from 2023, and only use live crawling as a fallback.\n```\n\n```\nSearch for electric vehicles and return 3 results, always using live crawling.\n```\n\nThe server will:\n\n1. Process the search request\n2. Query the Exa API with optimal settings (including live crawling)\n3. Return formatted results to Claude\n4. Cache the search for future reference\n\n## Features ✨\n\n* **Simplified Web Search Tool**: Enables Claude to search the web with just a query parameter\n* **Customizable Search Parameters**: Control the number of results and live crawling strategy\n* **Automatic Live Crawling**: Uses real-time crawling based on specified strategy\n* **Preset Optimal Parameters**: Uses best defaults for result count and character limits\n* **Search Caching**: Saves recent searches as resources for reference\n* **Error Handling**: Gracefully handles API errors and rate limits\n* **Type Safety**: Full TypeScript implementation with Zod validation\n* **MCP Compliance**: Fully implements the latest MCP protocol specification\n\n## Testing with MCP Inspector 🔍\n\nYou can test the server directly using the MCP Inspector:\n\n```bash\nnpx @modelcontextprotocol/inspector node ./build/index.js\n```\n\nThis opens an interactive interface where you can explore the server's capabilities, execute search queries, and view cached search results.\n\n## Troubleshooting 🔧\n\n### Common Issues\n\n1. **Server Not Found**\n   * Verify the npm link is correctly set up\n   * Check Claude Desktop configuration syntax\n   * Ensure Node.js is properly installed\n\n2. **API Key Issues**\n   * Confirm your EXA_API_KEY is valid\n   * Check the EXA_API_KEY is correctly set in the Claude Desktop config\n   * Verify no spaces or quotes around the API key\n\n3. **Connection Issues**\n   * Restart Claude Desktop completely\n   * Check Claude Desktop logs:\n   \n   ```bash\n   # macOS\n   tail -n 20 -f ~/Library/Logs/Claude/mcp*.log\n   \n   # Windows\n   type \"%APPDATA%\\Claude\\logs\\mcp*.log\"\n   ```\n\n### Getting Help\n\nIf you encounter issues, review the [MCP Documentation](https://modelcontextprotocol.io) or visit the [GitHub discussions](https://github.com/orgs/modelcontextprotocol/discussions) for community support.\n\n## Acknowledgments 🙏\n\n* [Exa AI](https://exa.ai) for their powerful search API\n* [Model Context Protocol](https://modelcontextprotocol.io) for the MCP specification\n* [Anthropic](https://anthropic.com) for Claude Desktop\n",
      "npm_url": "https://www.npmjs.com/package/exa-mcp-server",
      "npm_downloads": 97542,
      "keywords": [
        "search",
        "exa",
        "web",
        "search geezerrrr",
        "geezerrrr exa",
        "web search"
      ],
      "category": "web-search"
    },
    "georgeck--hn-companion-mcp": {
      "owner": "georgeck",
      "name": "hn-companion-mcp",
      "url": "https://github.com/georgeck/hn-companion-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/georgeck.webp",
      "description": "Summarizes discussions from Hacker News by fetching and processing comments, ensuring the hierarchical structure and associated metadata are maintained for effective summarization by Claude.",
      "stars": 0,
      "forks": 2,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-04-16T17:51:58Z",
      "readme_content": "# Hacker News Companion MCP\n[![smithery badge](https://smithery.ai/badge/@georgeck/hn-companion-mcp)](https://smithery.ai/server/@georgeck/hn-companion-mcp)\n\n<a href=\"https://glama.ai/mcp/servers/@georgeck/hn-companion-mcp\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@georgeck/hn-companion-mcp/badge\" alt=\"Hacker News Companion MCP server\" />\n</a>\n\nA Model Context Protocol (MCP) for summarizing Hacker News discussions using Claude.\n\n## Overview\n\nThis MCP fetches and processes Hacker News discussions, preparing them in a format that Claude can use to generate high-quality summaries. It handles both the hierarchical structure of comments and their metadata (scores, downvotes, etc.) to help Claude understand the relative importance and relationships of different comments.\n\n## Features\n\n- Process Hacker News URLs or post IDs\n- Download and analyze comment structure from HN\n- Score comments based on community engagement\n- Format data optimized for Claude's summarization\n\n## Installation\n\n### Installing via Smithery\n\nTo install Hacker News Companion for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@georgeck/hn-companion-mcp):\n\n```bash\nnpx -y @smithery/cli install @georgeck/hn-companion-mcp --client claude\n```\n\n### Manual Installation\n\n1. Clone the repository:\n   ```\n   git clone https://github.com/yourusername/hn-companion-mcp.git\n   cd hn-companion-mcp\n   ```\n\n2. Install dependencies:\n   ```\n   npm install\n   ```\n\n## Usage\n\n### CLI\n\n```bash\nnode index.js <post-id-or-url>\n```\n\nExample:\n```bash\nnode index.js 43448075\n# or\nnode index.js https://news.ycombinator.com/item?id=43448075\n```\n\n### API Server\n\nStart the server:\n```bash\nnpm start\n```\n\nMake a request:\n```bash\ncurl -X POST http://localhost:3000/api/summarize \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"input\": \"https://news.ycombinator.com/item?id=43448075\"}'\n```\n\n## API Reference\n\n### `POST /api/summarize`\n\nRequest body:\n```json\n{\n  \"input\": \"https://news.ycombinator.com/item?id=43448075\"\n}\n```\n\nResponse:\n```json\n{\n  \"status\": \"success\",\n  \"data\": {\n    \"systemPrompt\": \"...\",\n    \"userPrompt\": \"...\",\n    \"commentPathIdMapping\": { ... },\n    \"postTitle\": \"...\",\n    \"postId\": \"...\",\n    \"commentCount\": 123\n  }\n}\n```\n\n## Integration with Claude\n\nThis MCP is designed to prepare data for Claude to summarize. When a user asks Claude to summarize a Hacker News discussion, \nClaude can call this MCP to get the formatted data and then generate a summary based on the provided system and user prompts.\n\n```json\n\"hn-companion\": {\n      \"command\": \"node\",\n      \"args\": [\"<full path to src>/hn-companion-mcp/server.js\"]\n    }\n  }\n```\n\n## License\n\nMIT",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "summarization",
        "summarizes",
        "discussions",
        "summarizes discussions",
        "discussions hacker",
        "mcp summarizes"
      ],
      "category": "web-search"
    },
    "gianlucamazza--mcp-duckduckgo": {
      "owner": "gianlucamazza",
      "name": "mcp-duckduckgo",
      "url": "https://github.com/gianlucamazza/mcp-duckduckgo",
      "imageUrl": "/freedevtools/mcp/pfp/gianlucamazza.webp",
      "description": "Provides web search functionality using DuckDuckGo with advanced navigation and content exploration features, allowing users to retrieve detailed search results and related queries.",
      "stars": 6,
      "forks": 4,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-10-02T13:15:10Z",
      "readme_content": "# MCP DuckDuckGo Search Plugin\n\nA DuckDuckGo search plugin for Model Context Protocol (MCP), compatible with Claude Code. Provides web search functionality with advanced navigation and content exploration features.\n\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![GitHub](https://img.shields.io/github/stars/gianlucamazza/mcp-duckduckgo?style=social)](https://github.com/gianlucamazza/mcp-duckduckgo)\n\n## Description\n\nThis project implements a Model Context Protocol (MCP) server that provides web search functionality using DuckDuckGo. The plugin is designed to work seamlessly with Claude Code or any other client that supports MCP, offering not just basic search capabilities but also advanced navigation and result exploration features.\n\n## Features\n\n- **Web Search Tool**: Perform web searches using DuckDuckGo\n- **Detailed Results**: Get detailed information about specific search results\n- **Related Searches**: Discover related search queries based on your original search\n- **Pagination Support**: Navigate through multiple pages of search results\n- **Domain Extraction**: View domain information for each search result\n- **Advanced Filtering**: Filter results by site and time period\n- **Enhanced Content Extraction**: Extract rich content from webpages including metadata, structure, and snippets\n- **Basic Web Spidering**: Follow links from search results to explore related content (configurable depth)\n- **Metadata Extraction**: Extract titles, authors, keywords, publication dates, and more\n- **Social Media Detection**: Identify and extract social media links from webpages\n- **Content Structure Analysis**: Extract headings and sections to understand webpage structure\n- **Search Documentation**: Access comprehensive documentation about the search functionality\n- **Search Assistant**: Get help formulating effective search queries\n- **Parameterized Resource**: Retrieve formatted search results for specific queries\n\n## Requirements\n\n- Python 3.9 or higher\n- Package manager: `uv` (recommended) or `pip`\n- Python packages listed in `pyproject.toml`\n\n## Installation\n\n### From PyPI\n\n*Note: This package is not yet published to PyPI. Please install from source below.*\n\nIn the future, once published, you'll be able to install with:\n\n```bash\npip install mcp-duckduckgo\n```\n\n### From Source\n\n#### Using uv (Recommended)\n\n[uv](https://github.com/astral-sh/uv) is a fast Python package manager that provides better dependency resolution and faster installs.\n\n1. Install uv if you haven't already:\n   ```bash\n   curl -LsSf https://astral.sh/uv/install.sh | sh\n   ```\n\n2. Clone and install as a tool:\n   ```bash\n   git clone https://github.com/gianlucamazza/mcp-duckduckgo.git\n   cd mcp-duckduckgo\n   uv tool install .\n   ```\n\n   Or install directly from the repository:\n   ```bash\n   uv tool install git+https://github.com/gianlucamazza/mcp-duckduckgo.git\n   ```\n\n#### Using pip\n\n1. Clone this repository:\n\n   ```bash\n   git clone https://github.com/gianlucamazza/mcp-duckduckgo.git\n   cd mcp-duckduckgo\n   ```\n\n2. Install the package in development mode:\n\n   ```bash\n   pip install -e .\n   ```\n\n   Or use the provided script:\n\n   ```bash\n   ./scripts/install_dev.sh\n   ```\n\n   Or use Make:\n\n   ```bash\n   make install\n   ```\n\n## Usage\n\n### Starting the Server Manually\n\nTo start the MCP server:\n\n```bash\nmcp-duckduckgo\n```\n\nOr with custom parameters:\n\n```bash\nmcp-duckduckgo --port 8000\n```\n\nOr use the provided script for development:\n\n```bash\n./scripts/run.sh\n```\n\nOr use Make:\n\n```bash\nmake run\n```\n\n### Environment Variables\n\nThe MCP server can be configured using environment variables:\n\n- `MCP_PORT`: Set the port number for the server (default: 3000)\n\nExample usage:\n\n```bash\n# Set port via environment variable\nexport MCP_PORT=8080\nmcp-duckduckgo\n\n# Or set it inline\nMCP_PORT=8080 mcp-duckduckgo\n```\n\nNote: The `--port` command-line argument takes precedence over the `MCP_PORT` environment variable.\n\n### Using with Claude Code\n\n1. Install the package from source as described above.\n\n2. Configure Claude Code to use the plugin:\n\n   ```bash\n   claude mcp add duckduckgo-search -- mcp-duckduckgo\n   ```\n\n3. For global configuration (available in all projects):\n\n   ```bash\n   claude mcp add duckduckgo-search --scope user -- mcp-duckduckgo\n   ```\n\n4. Start Claude Code:\n\n   ```bash\n   claude\n   ```\n\n5. Now you can use the DuckDuckGo search functionality within Claude Code.\n\n## Available Endpoints\n\nThe plugin provides the following endpoints:\n\n### Tool: `duckduckgo_web_search`\n\nPerforms a web search using DuckDuckGo with the following parameters:\n\n- `query` (required): The search query (max 400 characters, 50 words)\n- `count` (optional, default: 10): Number of results per page (1-20)\n- `page` (optional, default: 1): Page number for pagination\n- `site` (optional): Limit results to a specific site (e.g., 'example.com')\n- `time_period` (optional): Filter results by time period ('day', 'week', 'month', 'year')\n\nExample usage in Claude Code:\n\n```text\nSearch for \"artificial intelligence latest developments\"\n```\n\n### Tool: `duckduckgo_get_details`\n\nRetrieves detailed information about a specific search result:\n\n- `url` (required): URL of the result to get details for\n\nExample usage in Claude Code:\n\n```text\nGet details for \"https://example.com/article\"\n```\n\n### Tool: `duckduckgo_related_searches`\n\nSuggests related search queries based on the original query:\n\n- `query` (required): Original search query (max 400 characters)\n- `count` (optional, default: 5): Number of related searches to return (1-10)\n\nExample usage in Claude Code:\n\n```text\nFind related searches for \"renewable energy\"\n```\n\n### Resource: `docs://search`\n\nProvides comprehensive documentation about the search functionality.\n\nExample usage in Claude Code:\n\n```text\nShow me the documentation for the DuckDuckGo search\n```\n\n### Prompt: `search_assistant`\n\nHelps formulate effective search queries.\n\nExample usage in Claude Code:\n\n```text\nHelp me formulate a search query about climate change solutions\n```\n\n### Resource: `search://{query}`\n\nRetrieves formatted search results for a specific query.\n\nExample usage in Claude Code:\n\n```text\nGet search results for \"quantum computing breakthroughs\"\n```\n\n## Using the Navigation Features\n\nThe plugin provides several features to help navigate and explore search results:\n\n### Pagination\n\nTo navigate through multiple pages of search results:\n\n```text\nSearch for \"climate change solutions\" with 5 results per page, page 2\n```\n\n### Filtering Results\n\nTo filter results by specific site:\n\n```text\nSearch for \"machine learning tutorials\" on \"tensorflow.org\"\n```\n\nTo filter results by time period:\n\n```text\nSearch for \"latest news\" from the past week\n```\n\n### Exploring Result Details\n\nTo get more information about a specific search result:\n\n```text\nGet details for \"https://example.com/article-found-in-search\"\n```\n\n### Finding Related Searches\n\nTo discover related search queries:\n\n```text\nFind related searches for \"electric vehicles\"\n```\n\nThese navigation features can be combined with Claude's natural language capabilities to create a powerful search and exploration experience. For example:\n\n```text\nSearch for \"python machine learning libraries\", then get details on the top result, and finally show me related search terms\n```\n\n## Implementation Notes\n\nThis implementation uses DuckDuckGo's public web interface and parses the HTML response to extract results. This approach is used for demonstration purposes, as DuckDuckGo does not offer an official search API. In a production environment, it's recommended to use a search service with an official API.\n\n## Enhanced Content Extraction\n\nThe DuckDuckGo plugin includes advanced content extraction capabilities that go beyond simple search results:\n\n### Content Extraction Features\n\n- **Full Webpage Analysis**: Extract and parse HTML content from search result URLs\n- **Intelligent Content Targeting**: Identify and extract main content areas from different types of websites\n- **Rich Metadata Extraction**: Extract titles, descriptions, authors, keywords, and publication dates\n- **Image Detection**: Identify and extract main images and media from webpages\n- **Social Media Integration**: Detect and extract links to social media profiles\n- **Content Structure Analysis**: Extract headings and sections to understand webpage organization\n- **Official Source Detection**: Identify whether a source is official based on domain and content signals\n\n### Web Spidering Capabilities\n\nThe plugin includes basic web spidering functionality:\n\n- **Configurable Depth**: Follow links from 0 to 3 levels deep from the original URL\n- **Link Limitation**: Control the maximum number of links to follow per page (1-5)\n- **Domain Restriction**: Option to only follow links within the same domain\n- **Related Content Discovery**: Find and analyze content related to the original search\n\n### Using Enhanced Content Extraction\n\nTo use the enhanced content extraction features:\n\n```text\nGet details for \"https://example.com/article\" with spider depth 1\n```\n\nTo control spidering behavior:\n\n```text\nGet details for \"https://example.com/article\" with spider depth 2, max links 3, same domain only\n```\n\n## Development\n\nThe project includes several utility scripts in the `scripts` directory to help with development:\n\n- `install_dev.sh`: Sets up the development environment\n- `run.sh`: Runs the MCP server with development settings\n- `test.sh`: Runs tests with coverage reporting\n- `lint.sh`: Runs linting and code formatting\n- `publish.sh`: Builds and publishes the package to PyPI\n\nFor convenience, a Makefile is also provided with the following targets:\n\n```bash\nmake install  # Install the package in development mode\nmake test     # Run tests with coverage\nmake lint     # Run linting and code formatting\nmake run      # Run the MCP server\nmake publish  # Build and publish the package to PyPI\nmake clean    # Clean build artifacts\nmake all      # Run install, lint, and test (default)\nmake help     # Show help message\n```\n\n### Testing\n\nThe project includes a comprehensive test suite covering all major functionality. Tests are located in the `tests/` directory.\n\n#### Installing Test Dependencies\n\nBefore running the tests, install the test dependencies:\n\n```bash\npip install -e \".[test]\"\n```\n\n#### Running Tests\n\nYou can run all tests with:\n\n```bash\npytest\n```\n\nTo run tests with coverage reporting:\n\n```bash\npytest --cov=mcp_duckduckgo\n```\n\nTo run a specific test file:\n\n```bash\npytest tests/test_models.py\n```\n\nTo run tests with verbose output:\n\n```bash\npytest -v\n```\n\nOr use the provided script:\n\n```bash\n./scripts/test.sh\n```\n\nOr use Make:\n\n```bash\nmake test\n```\n\n#### Test Structure\n\nThe test suite is organized as follows:\n\n- `conftest.py` - Shared fixtures and configurations for tests\n- `test_models.py` - Tests for data models\n- `test_search.py` - Tests for search functionality\n- `test_tools.py` - Tests for MCP tools\n- `test_resources.py` - Tests for MCP resources\n- `test_integration.py` - End-to-end integration tests\n- `test_server.py` - Server lifecycle tests\n\nFor more details about testing, see the [tests/README.md](tests/README.md) file.\n\n### Code Formatting and Linting\n\n```bash\nblack mcp_duckduckgo\nisort mcp_duckduckgo\nmypy mcp_duckduckgo\n```\n\nOr use the provided script:\n\n```bash\n./scripts/lint.sh\n```\n\nOr use Make:\n\n```bash\nmake lint\n```\n\n### Publishing to PyPI\n\nIf you want to publish the package to PyPI:\n\n1. Update the version in `pyproject.toml`\n2. Ensure you have the necessary credentials and tools:\n   ```bash\n   pip install build twine\n   ```\n3. Build and publish:\n   ```bash\n   python -m build\n   twine upload dist/*\n   ```\n\nOr use the provided script if available:\n\n```bash\n./scripts/publish.sh\n```\n\nOr use Make:\n\n```bash\nmake publish\n```\n\n## Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n\n## License\n\n[MIT](LICENSE)\n\n## Repository\n\n[GitHub Repository](https://github.com/gianlucamazza/mcp-duckduckgo)\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "duckduckgo",
        "search",
        "gianlucamazza",
        "duckduckgo provides",
        "using duckduckgo",
        "duckduckgo advanced"
      ],
      "category": "web-search"
    },
    "giorgos3215--ultimate-cursor-mcp": {
      "owner": "giorgos3215",
      "name": "ultimate-cursor-mcp",
      "url": "https://github.com/giorgos3215/ultimate-cursor-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/giorgos3215.webp",
      "description": "Integrates advanced web, code, and file operations with Supabase for database management, providing tools for web scraping, code analysis, and file handling alongside AI-powered functionalities for intelligent analysis and self-improvement based on usage patterns.",
      "stars": 0,
      "forks": 2,
      "license": "No License",
      "language": "JavaScript",
      "updated_at": "2025-07-23T16:40:11Z",
      "readme_content": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/giorgos3215-ultimate-cursor-mcp-badge.png)](https://mseep.ai/app/giorgos3215-ultimate-cursor-mcp)\n\n# Ultimate Self-Evolving Cursor MCP\n\nA comprehensive MCP (Model Context Protocol) implementation for Cursor, featuring advanced tools for web, code, file operations, and Supabase database management.\n\n[![Smithery.ai](https://img.shields.io/badge/Smithery.ai-Available-blue.svg)](https://smithery.ai/package/ultimate-cursor-mcp)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n\n## Features\n\n- Advanced web tools (scraping, crawling, semantic search)\n- Powerful code analysis and refactoring tools\n- File operations with batch processing and watching capabilities\n- AI-powered capabilities (LLM queries, image analysis)\n- **Full Supabase integration** for database operations and management\n- Self-improvement mechanism with usage analytics\n- Memory persistence for better context understanding\n\n## Installation\n\n### Easy Setup (Recommended)\n\nRun the setup script which will install both the Ultimate Cursor MCP and optionally the Supabase MCP:\n\n```bash\n./setup.sh\n```\n\nThe script will:\n1. Install the Ultimate Cursor MCP\n2. Ask if you want to set up Supabase integration\n3. Guide you through providing Supabase credentials if needed\n4. Configure everything automatically\n\n### Manual Installation\n\n#### Ultimate Cursor MCP\n\n```bash\npython3 tools/mcp_installer.py local .\n```\n\n#### Supabase MCP (Optional)\n\n```bash\npython3 tools/mcp_installer.py supabase --url \"https://yourproject.supabase.co\" --key \"your-api-key\"\n```\n\n### Smithery.ai Installation\n\nIf you prefer to install via smithery.ai:\n\n```bash\ncursor smithery install ultimate-cursor-mcp\n```\n\n## Supabase Integration\n\nThe Supabase integration provides:\n\n- SQL query execution with safety controls (read-only by default)\n- Database schema inspection tools\n- Management API access with safety classifications\n- Auth Admin tools for user management\n\n### Benefits of Supabase MCP\n\n- **Safety features**: Starts in read-only mode; requires explicit mode switching for write operations\n- **Comprehensive database tools**: Schema inspection, table information, detailed structure\n- **Full SQL support**: Execute any PostgreSQL query with transaction handling\n- **Advanced Management API access**: Send arbitrary requests with auto-injection of project ref\n- **Auth Admin tools**: User creation, deletion, invitation and management\n\n[Read the complete Supabase integration guide](./docs/supabase.md)\n\n## Testing\n\nAfter installation, you can test the functionality:\n\n```bash\n./test-client.js\n```\n\n## Configuration\n\nThe configuration is stored in `~/.cursor/mcp.json`. After installation, restart Cursor for the changes to take effect.\n\n## Development\n\n### Project Structure\n\n- `src/` - TypeScript implementation of the MCP server\n  - `enhanced-mcp.js` - Main MCP server\n  - `tools/` - Tool implementations\n    - `web-tools.js` - Web scraping and search tools\n    - `code-tools.js` - Code analysis tools\n    - `file-tools.js` - File operation tools\n    - `ai-tools.js` - LLM and image analysis tools\n- `tools/` - Helper scripts\n  - `mcp_installer.py` - Installation utility\n\n## Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n\n1. Fork the repository\n2. Create your feature branch (`git checkout -b feature/amazing-feature`)\n3. Commit your changes (`git commit -m 'Add some amazing feature'`)\n4. Push to the branch (`git push origin feature/amazing-feature`)\n5. Open a Pull Request\n\n## License\n\nMIT\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "supabase",
        "cursor",
        "giorgos3215",
        "search giorgos3215",
        "supabase database",
        "cursor mcp"
      ],
      "category": "web-search"
    },
    "goswamig--fetch-mcp": {
      "owner": "goswamig",
      "name": "fetch-mcp",
      "url": "https://github.com/goswamig/fetch-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/goswamig.webp",
      "description": "Fetches web content from specified URLs in formats such as HTML, JSON, plain text, and Markdown, allowing for customizable request headers.",
      "stars": 3,
      "forks": 0,
      "license": "No License",
      "language": "TypeScript",
      "updated_at": "2025-04-28T04:25:11Z",
      "readme_content": "# Fetch MCP Server\n[![smithery badge](https://smithery.ai/badge/@goswamig/fetch-mcp)](https://smithery.ai/server/@goswamig/fetch-mcp)\n\n\n\nThis MCP server provides functionality to fetch web content in various formats, including HTML, JSON, plain text, and Markdown.\n\n## Components\n\n### Tools\n\n- **fetch_html**\n  - Fetch a website and return the content as HTML\n  - Input:\n    - `url` (string, required): URL of the website to fetch\n    - `headers` (object, optional): Custom headers to include in the request\n  - Returns the raw HTML content of the webpage\n\n- **fetch_json**\n  - Fetch a JSON file from a URL\n  - Input:\n    - `url` (string, required): URL of the JSON to fetch\n    - `headers` (object, optional): Custom headers to include in the request\n  - Returns the parsed JSON content\n\n- **fetch_txt**\n  - Fetch a website and return the content as plain text (no HTML)\n  - Input:\n    - `url` (string, required): URL of the website to fetch\n    - `headers` (object, optional): Custom headers to include in the request\n  - Returns the text content of the webpage with HTML tags, scripts, and styles removed\n\n- **fetch_markdown**\n  - Fetch a website and return the content as Markdown\n  - Input:\n    - `url` (string, required): URL of the website to fetch\n    - `headers` (object, optional): Custom headers to include in the request\n  - Returns the content of the webpage converted to Markdown format\n\n### Resources\n\nThis server does not provide any persistent resources. It's designed to fetch and transform web content on demand.\n\n## Getting started\n\n1. Clone the repository\n2. Install dependencies: `npm install`\n3. Build the server: `npm run build`\n\n### Installing via Smithery\n\nTo install fetch-mcp for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@goswamig/fetch-mcp):\n\n```bash\nnpx -y @smithery/cli install @goswamig/fetch-mcp --client claude\n```\n\n### Usage\n\nTo use the server, you can run it directly:\n\n```bash\nnpm start\n```\n\nThis will start the Fetch MCP Server running on stdio.\n\n### Usage with Desktop App\n\nTo integrate this server with a desktop app, add the following to your app's server configuration:\n\n```json\n{\n  \"mcpServers\": {\n    \"fetch\": {\n      \"command\": \"node\",\n      \"args\": [\n        \"{ABSOLUTE PATH TO FILE HERE}/dist/index.js\"\n      ]\n    }\n  }\n}\n```\n\n## Features\n\n- Fetches web content using modern fetch API\n- Supports custom headers for requests\n- Provides content in multiple formats: HTML, JSON, plain text, and Markdown\n- Uses JSDOM for HTML parsing and text extraction\n- Uses TurndownService for HTML to Markdown conversion\n\n## Development\n\n- Run `npm run dev` to start the TypeScript compiler in watch mode\n- Use `npm test` to run the test suite\n\n## License\n\nThis project is licensed under the MIT License.",
      "npm_url": "https://www.npmjs.com/package/fetch-mcp",
      "npm_downloads": 36810,
      "keywords": [
        "fetch",
        "fetches",
        "headers",
        "fetches web",
        "goswamig fetch",
        "mcp fetches"
      ],
      "category": "web-search"
    },
    "gradusnikov--google-search-mcp-server": {
      "owner": "gradusnikov",
      "name": "google-search-mcp-server",
      "url": "https://github.com/gradusnikov/google-search-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/gradusnikov.webp",
      "description": "Integrate Google Custom Search to retrieve and display search results directly in applications. Enhance applications with the ability to access vast amounts of online information efficiently.",
      "stars": 6,
      "forks": 3,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-09-25T10:02:29Z",
      "readme_content": "# Google Search MCP Server\n\n[![smithery badge](https://smithery.ai/badge/@gradusnikov/google-search-mcp-server)](https://smithery.ai/server/@gradusnikov/google-search-mcp-server)\n\nA Model Context Protocol (MCP) server that provides Google Custom Search functionality.\n\n## Installation\n\n### Installing via Smithery\n\nTo install google-search-mcp-server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@gradusnikov/google-search-mcp-server):\n\n```bash\nnpx -y @smithery/cli install @gradusnikov/google-search-mcp-server --client claude\n```\n\n### Manual Installation\n1. Clone the repository:\n```bash\ngit clone https://github.com/gradusnikov/google-search-mpc-server.git\ncd google-search-mpc-server\n```\n\n2. Install dependencies:\n```bash\npip install fastmcp google-api-python-client python-dotenv\n```\n\n## Configuration\n\nCreate a `.env` file in the project root with the following variables:\n\n```\nGOOGLE_API_KEY=your_google_api_key\nGOOGLE_CSE_ID=your_custom_search_engine_id\n```\n\nTo obtain these credentials:\n1. Create a Google Cloud project and enable the Custom Search API\n2. Generate an API key from the Google Cloud Console\n3. Create a Custom Search Engine at https://cse.google.com/cse/all and get its ID\n\n## Usage\n\nStart the server using MCP:\n\n```bash\nmcp run google_search_mcp_server.py\n```\n\nor add the server to Claude Desktop app *CLAUDE_DIRECTORY/claude_desktop_config.json*. For example if you are using Windows Subsystem for Linux (WSL) it may look like this:\n\n```\n\"google-search\": {\n            \"command\": \"wsl.exe\",\n            \"args\": [\n                \"bash\",\n                \"-c\",\n                \"source /home/[user]/anaconda3/etc/profile.d/conda.sh && conda activate mcp && /home/[user]/anaconda3/bin/mcp run /home/[user]/google-search-mpc-server/google_search_mcp_server.py\"\n            ]\n        },\n```\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "search",
        "google",
        "web",
        "search results",
        "custom search",
        "web search"
      ],
      "category": "web-search"
    },
    "gradusnikov--pubmed-search-mcp-server": {
      "owner": "gradusnikov",
      "name": "pubmed-search-mcp-server",
      "url": "https://github.com/gradusnikov/pubmed-search-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/gradusnikov.webp",
      "description": "Search and retrieve academic papers from the PubMed database, providing access to titles, authors, journals, abstracts, and DOIs.",
      "stars": 6,
      "forks": 5,
      "license": "BSD 3-Clause \"New\" or \"Revised\" License",
      "language": "Python",
      "updated_at": "2025-07-08T04:10:26Z",
      "readme_content": "# PubMedSearch MCP Server\n[![smithery badge](https://smithery.ai/badge/@gradusnikov/pubmed-search-mcp-server)](https://smithery.ai/server/@gradusnikov/pubmed-search-mcp-server)\n\nA Model Content Protocol server that provides tools to search and retrieve academic papers from PubMed database.\n\n## Features\n\n- Search PubMed by keywords in title/abstract or author names\n- Retrieve detailed information including title, authors, journal, abstract, DOI and more\n\n### Installing via Smithery\n\nTo install pubmed-search-mcp-server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@gradusnikov/pubmed-search-mcp-server):\n\n```bash\nnpx -y @smithery/cli install @gradusnikov/pubmed-search-mcp-server --client claude\n```\n\n### Installing Manually\n1. Clone this repository:\n   ```\n   git clone <repository-url>\n   cd pubmed-search-mcp-server\n   ```\n\n2. Install dependencies:\n   ```\n   pip install fastmcp requests python-dotenv\n   ```\n\n3. Create a `.env` file in the project root (if needed for configuration)\n\n## Usage\n\n1. Start the server in development mode:\n   ```\n   mcp dev pubmed_search_mcp_server.py\n   ```\n\n2. or add the server to Claude Desktop app *CLAUDE_DIRECTORY/claude_desktop_config.json*. For example if you are using Windows Subsystem for Linux (WSL) it may look like this:\n\n   ```\n   \"pubmed-search\": {\n               \"command\": \"wsl.exe\",\n               \"args\": [\n                   \"bash\",\n                   \"-c\",\n                   \"source /home/[user]/anaconda3/etc/profile.d/conda.sh && conda activate mcp && mcp run /home/[user]/pubmed-search-mpc-server/pubmed_search_mcp_server.py\"\n               ]\n           },\n   ```\n\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "pubmed",
        "journals",
        "search",
        "pubmed search",
        "gradusnikov pubmed",
        "pubmed database"
      ],
      "category": "web-search"
    },
    "gregkop--sketchfab-mcp-server": {
      "owner": "gregkop",
      "name": "sketchfab-mcp-server",
      "url": "https://github.com/gregkop/sketchfab-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/gregkop.webp",
      "description": "Search, view details, and download 3D models from Sketchfab, utilizing keywords, tags, and categories to find models and obtain comprehensive information about them. Supports downloading in multiple formats for creative projects.",
      "stars": 28,
      "forks": 8,
      "license": "No License",
      "language": "JavaScript",
      "updated_at": "2025-09-25T16:12:28Z",
      "readme_content": "# Sketchfab MCP Server\n\nA Model Context Protocol (MCP) server for interacting with Sketchfab's 3D model platform. This MCP allows you to search, view details, and download 3D models from Sketchfab directly through Claude or Cursor.\n\n## Features\n\n- **Search for 3D Models**: Find models on Sketchfab using keywords, tags, and categories\n- **View Model Details**: Get comprehensive information about specific models\n- **Download Models**: Download models in various formats (gltf, glb, usdz, source)\n\n## Prerequisites\n\n- Node.js 18 or higher\n- A Sketchfab API key (for authentication)\n\n## Installation\n\n1. Clone this repository\n2. Install dependencies:\n   ```\n   npm install\n   ```\n3. Build the project:\n   ```\n   npm run build\n   ```\n\n## Usage\n\n### Running the MCP Server\n\n```\nnpm start\n```\n\nTo provide your Sketchfab API key, use the `--api-key` parameter:\n\n```\nnode build/index.js --api-key YOUR_API_KEY\n```\n\nAlternatively, you can set the `SKETCHFAB_API_KEY` environment variable:\n\n```\nexport SKETCHFAB_API_KEY=YOUR_API_KEY\nnpm start\n```\n\n### Available Tools\n\n#### 1. sketchfab-search\n\nSearch for 3D models on Sketchfab based on keywords and filters.\n\nParameters:\n- `query` (optional): Text search query (e.g., \"car\", \"house\", \"character\")\n- `tags` (optional): Filter by specific tags (e.g., [\"animated\", \"rigged\", \"pbr\"])\n- `categories` (optional): Filter by categories (e.g., [\"characters\", \"architecture\", \"vehicles\"])\n- `downloadable` (optional): Set to true to show only downloadable models\n- `limit` (optional): Maximum number of results to return (1-24, default: 10)\n\n#### 2. sketchfab-model-details\n\nGet detailed information about a specific Sketchfab model.\n\nParameters:\n- `modelId`: The unique ID of the Sketchfab model\n\n#### 3. sketchfab-download\n\nDownload a 3D model from Sketchfab.\n\nParameters:\n- `modelId`: The unique ID of the Sketchfab model to download\n- `format` (optional): Preferred format to download the model in (gltf, glb, usdz, source)\n- `outputPath` (optional): Local directory or file path to save the downloaded file\n\n## Using with Cursor\n\n1. Go to Cursor Settings -> MCP -> Add new MCP server\n2. Configure your MCP:\n   - Name: Sketchfab MCP\n   - Type: command\n   - Command: `node /path/to/build/index.js --api-key YOUR_API_KEY`\n\n## Using with Claude Desktop\n\nAdd the following MCP config to your Claude Desktop configuration:\n\n```json\n{\n  \"mcpServers\": {\n    \"sketchfab\": {\n      \"command\": \"node\",\n      \"args\": [\"/path/to/build/index.js\", \"--api-key\", \"YOUR_API_KEY\"]\n    }\n  }\n}\n```\n\n## Environment Variables\n\nYou can set the following environment variables:\n\n- `SKETCHFAB_API_KEY`: Your Sketchfab API key (alternative to passing it with the --api-key parameter)\n\n## License\n\nISC\n",
      "npm_url": "https://www.npmjs.com/package/sketchfab-mcp-server",
      "npm_downloads": 3623,
      "keywords": [
        "sketchfab",
        "3d",
        "models",
        "models sketchfab",
        "sketchfab mcp",
        "sketchfab utilizing"
      ],
      "category": "web-search"
    },
    "happyhackerbird--jobspy-mcp-server": {
      "owner": "happyhackerbird",
      "name": "jobspy-mcp-server",
      "url": "https://github.com/happyhackerbird/jobspy-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/happyhackerbird.webp",
      "description": "Facilitates job searching on LinkedIn by utilizing the unofficial LinkedIn API for credential management and job scraping. Matches job opportunities using JobSpy for enhanced search capabilities.",
      "stars": 1,
      "forks": 1,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-06-24T14:09:31Z",
      "readme_content": "# MCP Server for Job Searching\n## Overview\nA Model Context Protocol (MCP) server designed to facilitate seamless job searching on LinkedIn, leveraging the unofficial LinkedIn API for client credential management. Based off [JobSpy](https://github.com/cullenwatson/JobSpy). \n\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "linkedin",
        "jobspy",
        "happyhackerbird",
        "happyhackerbird jobspy",
        "searching linkedin",
        "linkedin api"
      ],
      "category": "web-search"
    },
    "harimkang--mcp-korea-tourism-api": {
      "owner": "harimkang",
      "name": "mcp-korea-tourism-api",
      "url": "https://github.com/harimkang/mcp-korea-tourism-api",
      "imageUrl": "/freedevtools/mcp/pfp/harimkang.webp",
      "description": "Access information about South Korea's tourism attractions, including festivals, temples, restaurants, and accommodations through an API powered by the Korea Tourism Organization.",
      "stars": 6,
      "forks": 1,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-08-12T06:18:19Z",
      "readme_content": "# Korea Tourism API MCP Server ✈️\n\n<!-- Badges -->\n\n[![smithery badge](https://smithery.ai/badge/@harimkang/mcp-korea-tourism-api)](https://smithery.ai/interface/@harimkang/mcp-korea-tourism-api)\n[![Verified on MseeP](https://mseep.ai/badge.svg)](https://mseep.ai/app/85b16552-af4c-4029-9d47-a4586438ec02)\n[![PyPI version](https://badge.fury.io/py/mcp-korea-tourism-api.svg)](https://badge.fury.io/py/mcp-korea-tourism-api)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![CI Tests](https://github.com/harimkang/mcp-korea-tourism-api/actions/workflows/ci.yml/badge.svg)](https://github.com/harimkang/mcp-korea-tourism-api/actions/workflows/ci.yml)\n\nUnlock the wonders of South Korean tourism directly within your AI assistant! This project provides a Model Context Protocol (MCP) server powered by the official Korea Tourism Organization (KTO) API. Equip your AI with the ability to discover vibrant festivals, serene temples, delicious restaurants, comfortable accommodations, and much more across Korea.\n\n**Links:**\n\n- **PyPI Package:** [https://pypi.org/project/mcp-korea-tourism-api/](https://pypi.org/project/mcp-korea-tourism-api/)\n- **GitHub Repository:** [https://github.com/harimkang/mcp-korea-tourism-api](https://github.com/harimkang/mcp-korea-tourism-api)\n- **Releases:** [https://github.com/harimkang/mcp-korea-tourism-api/releases](https://github.com/harimkang/mcp-korea-tourism-api/releases)\n\n## ✨ Features\n\n- **Comprehensive Search:** Find tourist spots, cultural sites, events, food, lodging, and shopping via keywords, area, or location.\n- **Rich Details:** Access descriptions, operating hours, admission fees, photos, addresses, and contact information.\n- **Location-Aware:** Discover attractions near specific GPS coordinates.\n- **Timely Information:** Find festivals and events based on date ranges.\n- **Multilingual Support:** Get information in various languages supported by the KTO API (including English).\n  - **Supported Languages**: English, Japanese, Simplified Chinese, Traditional Chinese, Russian, Spanese, German, French\n- **Efficient & Resilient:**\n  - **Response Caching:** Uses time-to-live (TTL) caching to store results and reduce redundant API calls, improving speed.\n  - **Rate Limiting:** Respects API usage limits to prevent errors.\n  - **Automatic Retries:** Automatically retries requests in case of temporary network or server issues.\n- **MCP Standard:** Seamlessly integrates with AI assistants supporting the Model Context Protocol.\n\n## ⚠️ Prerequisites\n\nBefore you begin, you **must** obtain an API key from the **Korea Tourism Organization (KTO) Data Portal**.\n\n1.  Visit the [KTO Data Portal](https://www.data.go.kr/) (or the specific portal for the tourism API if available).\n2.  Register and request an API key for the \"TourAPI\" services (you might need to look for services providing information like `areaBasedList`, `searchKeyword`, `detailCommon`, etc.).\n3.  Keep your **Service Key (API Key)** safe. It will be required during installation or runtime.\n\n> You need to apply for the API below to make a request for each language.\n>\n> - English: https://www.data.go.kr/data/15101753/openapi.do\n> - Japanese: https://www.data.go.kr/data/15101760/openapi.do\n> - Simplified Chinese: https://www.data.go.kr/data/15101764/openapi.do\n> - Traditional Chinese: https://www.data.go.kr/data/15101769/openapi.do\n> - Russian: https://www.data.go.kr/data/15101831/openapi.do\n> - Spanese: https://www.data.go.kr/data/15101811/openapi.do\n> - German: https://www.data.go.kr/data/15101805/openapi.do\n> - French: https://www.data.go.kr/data/15101808/openapi.do\n\n## 🚀 Installation & Running\n\nYou can run this MCP server using either `uv` (a fast Python package installer and runner) or `Docker`.\n\n### Installing via Smithery\n\nTo install Korea Tourism API MCP Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@harimkang/mcp-korea-tourism-api):\n\n```bash\nnpx -y @smithery/cli install @harimkang/mcp-korea-tourism-api --client claude\n```\n\n### Option 1: Using `uv` (Recommended for local development)\n\n1.  **Clone the repository:**\n    ```bash\n    git clone https://github.com/harimkang/mcp-korea-tourism-api.git\n    cd mcp-korea-tourism-api\n    ```\n2.  **Set the API Key Environment Variable:**\n    Replace `\"YOUR_KTO_API_KEY\"` with the actual key you obtained.\n\n    ```bash\n    # On macOS/Linux\n    export KOREA_TOURISM_API_KEY=\"YOUR_KTO_API_KEY\"\n\n    # On Windows (Command Prompt)\n    # set KOREA_TOURISM_API_KEY=\"YOUR_KTO_API_KEY\"\n\n    # On Windows (PowerShell)\n    # $env:KOREA_TOURISM_API_KEY=\"YOUR_KTO_API_KEY\"\n    ```\n\n    _Note: For persistent storage, add this line to your shell's configuration file (e.g., `.zshrc`, `.bashrc`, or use system environment variable settings)._\n\n3.  **Install dependencies and run the server:**\n    This command uses `uv` to install dependencies based on `uv.lock` (if available) or `pyproject.toml` and then runs the server module.\n\n    ```bash\n    # Install Dependency with uv\n    uv sync\n\n    # Default: stdio transport (for MCP clients)\n    uv run -m mcp_tourism.server\n\n    # HTTP transport for web applications\n    uv run -m mcp_tourism.server --transport streamable-http --host 127.0.0.1 --port 8000\n\n    # SSE transport for real-time applications\n    uv run -m mcp_tourism.server --transport sse --host 127.0.0.1 --port 8080\n\n    # Using environment variables\n    export MCP_TRANSPORT=streamable-http\n    export MCP_HOST=0.0.0.0\n    export MCP_PORT=3000\n    uv run -m mcp_tourism.server\n    ```\n\n    The server will start and listen for MCP requests via the specified transport protocol.\n\n### Option 2: Using Docker (Recommended for isolated environment/deployment)\n\n1.  **Clone the repository:**\n    ```bash\n    git clone https://github.com/harimkang/mcp-korea-tourism-api.git\n    cd mcp-korea-tourism-api\n    ```\n2.  **Build the Docker Image:**\n    You can build the image with different transport configurations:\n\n    ```bash\n    # Default build (stdio transport)\n    docker build -t mcp-korea-tourism-api .\n\n    # Build with HTTP transport configuration\n    docker build -t mcp-korea-tourism-api \\\n      --build-arg MCP_TRANSPORT=streamable-http \\\n      --build-arg MCP_HOST=0.0.0.0 \\\n      --build-arg MCP_PORT=8000 \\\n      --build-arg MCP_PATH=/mcp \\\n      --build-arg MCP_LOG_LEVEL=INFO \\\n      .\n\n    # Build with SSE transport configuration\n    docker build -t mcp-korea-tourism-api \\\n      --build-arg MCP_TRANSPORT=sse \\\n      --build-arg MCP_HOST=0.0.0.0 \\\n      --build-arg MCP_PORT=8080 \\\n      .\n    ```\n\n3.  **Run the Docker Container:**\n    You can run the container with different transport configurations:\n    - **Stdio Transport (Default - for MCP clients):**\n\n      ```bash\n      docker run --rm -it \\\n        -e KOREA_TOURISM_API_KEY=\"YOUR_KTO_API_KEY\" \\\n        mcp-korea-tourism-api\n      ```\n\n    - **HTTP Transport (for web applications):**\n\n      ```bash\n      # Using runtime environment variables\n      docker run --rm -p 8000:8000 \\\n        -e KOREA_TOURISM_API_KEY=\"YOUR_KTO_API_KEY\" \\\n        -e MCP_TRANSPORT=streamable-http \\\n        -e MCP_HOST=0.0.0.0 \\\n        -e MCP_PORT=8000 \\\n        mcp-korea-tourism-api\n\n      # Check health: curl http://localhost:8000/health\n      ```\n\n    - **SSE Transport (for real-time applications):**\n\n      ```bash\n      docker run --rm -p 8080:8080 \\\n        -e KOREA_TOURISM_API_KEY=\"YOUR_KTO_API_KEY\" \\\n        -e MCP_TRANSPORT=sse \\\n        -e MCP_HOST=0.0.0.0 \\\n        -e MCP_PORT=8080 \\\n        mcp-korea-tourism-api\n      ```\n\n    - **Using Docker Compose (Recommended):**\n\n      ```bash\n      # Copy and configure environment variables\n      cp docker.env.example .env\n      # Edit .env file with your API key and preferred settings\n\n      # Run with HTTP transport (default profile)\n      docker-compose up mcp-tourism-http\n\n      # Run with SSE transport\n      docker-compose --profile sse up mcp-tourism-sse\n\n      # Run development setup with debug logging\n      docker-compose --profile dev up mcp-tourism-dev\n      ```\n\n## 🔧 Transport Configuration\n\nThe Korea Tourism API MCP Server supports multiple transport protocols to accommodate different use cases:\n\n### Available Transports\n\n1. **`stdio`** (Default): Standard input/output transport for direct MCP client integration\n   - Best for: Claude Desktop, Cursor, and other MCP-compatible AI assistants\n   - Configuration: No additional setup required\n\n2. **`streamable-http`**: HTTP-based transport for web applications\n   - Best for: Web applications, REST API integration, load balancers\n   - Features: HTTP endpoints, health checks, JSON responses\n   - Default endpoint: `http://localhost:8000/mcp`\n\n3. **`sse`**: Server-Sent Events transport for real-time applications\n   - Best for: Real-time web applications, event-driven architectures\n   - Features: Real-time streaming, persistent connections\n   - Default endpoint: `http://localhost:8080/mcp`\n\n### Configuration Options\n\nYou can configure the server using command line arguments or environment variables:\n\n| Setting   | CLI Argument  | Environment Variable | Default     | Description                      |\n| --------- | ------------- | -------------------- | ----------- | -------------------------------- |\n| Transport | `--transport` | `MCP_TRANSPORT`      | `stdio`     | Transport protocol to use        |\n| Host      | `--host`      | `MCP_HOST`           | `127.0.0.1` | Host address for HTTP transports |\n| Port      | `--port`      | `MCP_PORT`           | `8000`      | Port for HTTP transports         |\n| Path      | `--path`      | `MCP_PATH`           | `/mcp`      | Path for HTTP endpoints          |\n| Log Level | `--log-level` | `MCP_LOG_LEVEL`      | `INFO`      | Logging level                    |\n\n### Command Line Examples\n\n```bash\n# Get help for all available options\npython -m mcp_tourism.server --help\n\n# Run with HTTP transport on custom port\npython -m mcp_tourism.server --transport streamable-http --port 3000 --log-level DEBUG\n\n# Run with SSE transport\npython -m mcp_tourism.server --transport sse --host 0.0.0.0 --port 8080\n```\n\n### Environment Variable Examples\n\n```bash\n# Set environment variables\nexport MCP_TRANSPORT=streamable-http\nexport MCP_HOST=0.0.0.0\nexport MCP_PORT=8000\nexport MCP_LOG_LEVEL=INFO\nexport KOREA_TOURISM_API_KEY=\"your_api_key_here\"\n\n# Run the server\npython -m mcp_tourism.server\n```\n\n### Health Check\n\nFor HTTP and SSE transports, a health check endpoint is available at `/health`:\n\n```bash\n# Check server health\ncurl http://localhost:8000/health\n\n# Example response\n{\n  \"status\": \"healthy\",\n  \"service\": \"Korea Tourism API MCP Server\",\n  \"transport\": \"streamable-http\",\n  \"timestamp\": 1640995200.0\n}\n```\n\n## 🛠️ Integrating with Cursor\n\nTo use this MCP server within Cursor:\n\n1.  **Ensure the Docker container is runnable:** Follow the Docker installation steps above to build the image (`mcp-korea-tourism-api`). You don't need to manually run the container; Cursor will do that.\n2.  **Locate your `mcp.json` file:** This file configures MCP tools for Cursor. You can usually find it via Cursor's settings or potentially in a path like `~/.cursor/mcp.json` or similar.\n3.  **Add or Update the MCP Configuration:** Add the following JSON object to the list within your `mcp.json` file. If you already have an entry for this tool, update its `command`. Replace `\"YOUR_KTO_API_KEY\"` with your actual key.\n    \n\n    ```json\n    {\n      \"mcpServers\": {\n        \"korea-tourism\": {\n          \"command\": \"docker\",\n          \"args\": [\n            \"run\",\n            \"--rm\",\n            \"-i\",\n            \"-e\",\n            \"KOREA_TOURISM_API_KEY=YOUR_KTO_API_KEY\",\n            \"mcp-korea-tourism-api\"\n          ]\n        }\n      }\n    }\n    ```\n\n    OR Use uv [local directory]\n\n    ```json\n    {\n      \"mcpServers\": {\n        \"korea-tourism\": {\n          \"command\": \"uv\",\n          \"args\": [\n            \"--directory\",\n            \"{LOCAL_PATH}/mcp-korea-tourism-api\",\n            \"run\",\n            \"-m\",\n            \"mcp_tourism.server\"\n          ],\n          \"env\": {\n            \"KOREA_TOURISM_API_KEY\": \"YOUR_KTO_API_KEY\"\n          }\n        }\n      }\n    }\n    ```\n\n4.  **Save `mcp.json`**.\n5.  **Restart Cursor or Reload MCP Tools:** Cursor should now detect the tool and use Docker to run it when needed.\n\n## 🛠️ MCP Tools Provided\n\nThis server exposes the following tools for AI assistants:\n\n1.  `search_tourism_by_keyword`: Search for tourism information using keywords (e.g., \"Gyeongbokgung\", \"Bibimbap\"). Filter by content type, area code.\n    \n2.  `get_tourism_by_area`: Browse tourism information by geographic area codes (e.g., Seoul='1'). Filter by content type, district code.\n    \n3.  `find_nearby_attractions`: Discover tourism spots near specific GPS coordinates (longitude, latitude). Filter by radius and content type.\n    \n4.  `search_festivals_by_date`: Find festivals occurring within a specified date range (YYYYMMDD). Filter by area code.\n    \n5.  `find_accommodations`: Search for hotels, guesthouses, etc. Filter by area and district code.\n    \n6.  `get_detailed_information`: Retrieve comprehensive details (overview, usage time, parking, etc.) for a specific item using its Content ID. Filter by content type.\n    \n7.  `get_tourism_images`: Get image URLs associated with a specific tourism item using its Content ID.\n    \n8.  `get_area_codes`: Retrieve area codes (for cities/provinces) and optionally sub-area (district) codes.\n    \n\n## ⚙️ Requirements (for `uv` method)\n\n- Python 3.12+\n- `uv` installed (`pip install uv`)\n\n## Example Usage\n\nAn AI assistant integrated with this MCP could handle queries like:\n\n- \"Find restaurants near Myeongdong station.\"\n- \"Show me pictures of Bulguksa Temple.\"\n- \"Are there any festivals in Busan next month?\"\n- \"Tell me more about Gyeongbokgung Palace, content ID 264337.\"\n\n[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/harimkang-mcp-korea-tourism-api-badge.png)](https://mseep.ai/app/harimkang-mcp-korea-tourism-api)",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "harimkang",
        "korea",
        "mcp",
        "korea tourism",
        "mcp korea",
        "harimkang mcp"
      ],
      "category": "web-search"
    },
    "heehee365--MediaCrawler": {
      "owner": "heehee365",
      "name": "MediaCrawler",
      "url": "https://github.com/heehee365/MediaCrawler",
      "imageUrl": "/freedevtools/mcp/pfp/heehee365.webp",
      "description": "Scrapes data from popular social media platforms such as Xiaohongshu, Douyin, Kuaishou, Bilibili, and Weibo. Extracts videos, images, comments, likes, and shares using Playwright for efficient data collection.",
      "stars": 1,
      "forks": 0,
      "license": "Other",
      "language": "",
      "updated_at": "2025-08-25T09:59:59Z",
      "readme_content": "> **免责声明：**\n> \n> 大家请以学习为目的使用本仓库，爬虫违法违规的案件：https://github.com/HiddenStrawberry/Crawler_Illegal_Cases_In_China  <br>\n>\n>本仓库的所有内容仅供学习和参考之用，禁止用于商业用途。任何人或组织不得将本仓库的内容用于非法用途或侵犯他人合法权益。本仓库所涉及的爬虫技术仅用于学习和研究，不得用于对其他平台进行大规模爬虫或其他非法行为。对于因使用本仓库内容而引起的任何法律责任，本仓库不承担任何责任。使用本仓库的内容即表示您同意本免责声明的所有条款和条件。\n\n> 点击查看更为详细的免责声明。[点击跳转](#disclaimer)\n# 仓库描述\n\n**小红书爬虫**，**抖音爬虫**， **快手爬虫**， **B站爬虫**， **微博爬虫**...。  \n目前能抓取小红书、抖音、快手、B站、微博的视频、图片、评论、点赞、转发等信息。\n\n原理：利用[playwright](https://playwright.dev/)搭桥，保留登录成功后的上下文浏览器环境，通过执行JS表达式获取一些加密参数\n通过使用此方式，免去了复现核心加密JS代码，逆向难度大大降低\n\n## 视频教程\n> 如果你想很快入门这个项目，或者想了具体实现原理，我推荐你看看这个课程，从设计出发一步步带你如何使用，门槛大大降低，同时也是对我开源的支持，如果你能支持我的课程，我将会非常开心～<br>\n> 课程售价非常非常的便宜，几杯咖啡的事儿.<br>\n> 课程介绍飞书文档链接：https://relakkes.feishu.cn/wiki/JUgBwdhIeiSbAwkFCLkciHdAnhh\n\n\n## 感谢下列Sponsors对本仓库赞助\n<a href=\"https://sider.ai/ad-land-redirect?source=github&p1=mi&p2=kk\">通过注册这个款免费的GPT助手，帮我获取GPT4额度作为支持。也是我每天在用的一款chrome AI助手插件</a>\n<a href=\"https://sider.ai/ad-land-redirect?source=github&p1=mi&p2=kk\" target=\"_blank\"><img alt=\"jK8drZ2bxTg67q9\" src=\"https://s2.loli.net/2024/04/01/jK8drZ2bxTg67q9.png\" ></a>\n\n成为赞助者，展示你的产品在这里，联系作者：relakkes@gmail.com\n\n## 功能列表\n| 平台  | Cookie 登录 | 二维码登录 | 手机号登录 | 关键词搜索 | 指定视频/帖子 ID 爬取 | 登录状态缓存 | 数据保存 | IP 代理池 | 滑块验证码 |\n|:---:|:---------:|:-----:|:-----:|:-----:|:-------------:|:------:|:----:|:------:|:-----:|\n| 小红书 |     ✅     |   ✅   | ✅     |   ✅   |       ✅       |   ✅    |  ✅   |   ✅    |   ✕   |\n| 抖音  |     ✅     |   ✅   | ✅     |   ✅   |       ✅       |   ✅    |  ✅   |   ✅    |   ✅   |\n| 快手  |     ✅     |   ✅   | ✕     |   ✅   |       ✅       |   ✅    |  ✅   |   ✅    |    ✕   |\n| B 站 |     ✅     |   ✅   | ✕     |   ✅   |       ✅       |   ✅    |  ✅   |   ✅    |   ✕   |\n| 微博  |     ✅      |   ✅    | ✕     |   ✅    |       ✅        |    ✅    |   ✅   |    ✅    |   ✕   |\n\n\n## 使用方法\n\n### 创建并激活 python 虚拟环境\n   ```shell   \n   # 进入项目根目录\n   cd MediaCrawler\n   \n   # 创建虚拟环境\n   # 注意python 版本需要3.7 - 3.9 \n   python -m venv venv\n   \n   # macos & linux 激活虚拟环境\n   source venv/bin/activate\n\n   # windows 激活虚拟环境\n   venv\\Scripts\\activate\n\n   ```\n\n### 安装依赖库\n\n   ```shell\n   pip3 install -r requirements.txt\n   ```\n\n### 安装 playwright浏览器驱动\n\n   ```shell\n   playwright install\n   ```\n\n### 运行爬虫程序\n\n   ```shell\n   # 从配置文件中读取关键词搜索相关的帖子并爬取帖子信息与评论\n   python main.py --platform xhs --lt qrcode --type search\n   \n   # 从配置文件中读取指定的帖子ID列表获取指定帖子的信息与评论信息\n   python main.py --platform xhs --lt qrcode --type detail\n  \n   # 打开对应APP扫二维码登录\n     \n   # 其他平台爬虫使用示例，执行下面的命令查看\n   python main.py --help    \n   ```\n\n\n### 数据保存\n- 支持保存到关系型数据库（Mysql、PgSQL等）\n- 支持保存到csv中（data/目录下）\n- 支持保存到json中（data/目录下）\n\n## MediaCrawler爬虫项目交流群：\n> 7天有效期，自动更新, 如果人满了可以加作者wx拉进群: yzglan，备注来自github.\n\n<div style=\"max-width: 200px\">  \n<p></p>\n</div>\n\n\n## 打赏\n免费开源不易，如果项目帮到你了，可以给我打赏哦，您的支持就是我最大的动力！\n<div style=\"display: flex;justify-content: space-between;width: 100%\">\n    <p></p>\n    <p></p>\n</div>\n\n## 爬虫入门课程\n我新开的爬虫教程Github仓库 [CrawlerTutorial](https://github.com/NanmiCoder/CrawlerTutorial) ，感兴趣的朋友可以关注一下，持续更新，主打一个免费.\n\n\n## 运行报错常见问题Q&A\n> 遇到问题先自行搜索解决下，现在AI很火，用ChatGPT大多情况下能解决你的问题 [免费的ChatGPT](https://sider.ai/ad-land-redirect?source=github&p1=mi&p2=kk)  \n\n➡️➡️➡️ [常见问题](docs/常见问题.md)\n\n\n## 项目代码结构\n➡️➡️➡️ [项目代码结构说明](docs/项目代码结构.md)\n\n## 手机号登录说明\n➡️➡️➡️ [手机号登录说明](docs/手机号登录说明.md)\n\n\n\n## star 趋势图\n- 如果该项目对你有帮助，star一下 ❤️❤️❤️\n\n[![Star History Chart](https://api.star-history.com/svg?repos=NanmiCoder/MediaCrawler&type=Date)](https://star-history.com/#NanmiCoder/MediaCrawler&Date)\n\n\n\n\n## 参考\n\n- xhs客户端 [ReaJason的xhs仓库](https://github.com/ReaJason/xhs)\n- 短信转发 [参考仓库](https://github.com/pppscn/SmsForwarder)\n- 内网穿透工具 [ngrok](https://ngrok.com/docs/)\n\n\n\n## 免责声明\n<div id=\"disclaimer\"> \n\n### 1. 项目目的与性质\n本项目（以下简称“本项目”）是作为一个技术研究与学习工具而创建的，旨在探索和学习网络数据采集技术。本项目专注于自媒体平台的数据爬取技术研究，旨在提供给学习者和研究者作为技术交流之用。\n\n### 2. 法律合规性声明\n本项目开发者（以下简称“开发者”）郑重提醒用户在下载、安装和使用本项目时，严格遵守中华人民共和国相关法律法规，包括但不限于《中华人民共和国网络安全法》、《中华人民共和国反间谍法》等所有适用的国家法律和政策。用户应自行承担一切因使用本项目而可能引起的法律责任。\n\n### 3. 使用目的限制\n本项目严禁用于任何非法目的或非学习、非研究的商业行为。本项目不得用于任何形式的非法侵入他人计算机系统，不得用于任何侵犯他人知识产权或其他合法权益的行为。用户应保证其使用本项目的目的纯属个人学习和技术研究，不得用于任何形式的非法活动。\n\n### 4. 免责声明\n开发者已尽最大努力确保本项目的正当性及安全性，但不对用户使用本项目可能引起的任何形式的直接或间接损失承担责任。包括但不限于由于使用本项目而导致的任何数据丢失、设备损坏、法律诉讼等。\n\n### 5. 知识产权声明\n本项目的知识产权归开发者所有。本项目受到著作权法和国际著作权条约以及其他知识产权法律和条约的保护。用户在遵守本声明及相关法律法规的前提下，可以下载和使用本项目。\n\n### 6. 最终解释权\n关于本项目的最终解释权归开发者所有。开发者保留随时更改或更新本免责声明的权利，恕不另行通知。\n</div>",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "mediacrawler",
        "heehee365",
        "xiaohongshu",
        "heehee365 mediacrawler",
        "mediacrawler scrapes",
        "search heehee365"
      ],
      "category": "web-search"
    },
    "hellokaton--unsplash-mcp-server": {
      "owner": "hellokaton",
      "name": "unsplash-mcp-server",
      "url": "https://github.com/hellokaton/unsplash-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/hellokaton.webp",
      "description": "Connects to Unsplash's image library to perform advanced searches and apply filters on keywords for rich, high-quality image retrieval.",
      "stars": 175,
      "forks": 19,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-10-02T20:22:26Z",
      "readme_content": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/hellokaton-unsplash-mcp-server-badge.png)](https://mseep.ai/app/hellokaton-unsplash-mcp-server)\n\n# Unsplash MCP Server\n\nEnglish | [简体中文](README_zh.md)\n\n> A simple MCP server for seamless Unsplash image integration and search capabilities.\n\n[![Python 3.9+](https://img.shields.io/badge/python-3.9+-blue.svg)](https://www.python.org/downloads/)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![smithery badge](https://smithery.ai/badge/@hellokaton/unsplash-mcp-server)](https://smithery.ai/server/@hellokaton/unsplash-mcp-server)\n\n## 📋 Overview\n\nUnsplash MCP Server is used for searching rich, high-quality images. It's ideal for developers who want to integrate Unsplash functionality into their own applications.\n\n## ✨ Features\n\n- **Advanced Image Search**: Search Unsplash's extensive photo library with filters for:\n  - Keyword relevance\n  - Color schemes\n  - Orientation options\n  - Custom sorting and pagination\n\n## 🔑 Obtaining Unsplash Access Key\n\nBefore installing this server, you'll need to obtain an Unsplash API Access Key:\n\n1. Create a developer account at [Unsplash](https://unsplash.com/developers)\n2. Register a new application\n3. Get your Access Key from the application details page\n4. Use this key in the configuration steps below\n\nFor more details, refer to the [official Unsplash API documentation](https://unsplash.com/documentation).\n\n## 🚀 Installation\n\nTo install Unsplash Image Integration Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@hellokaton/unsplash-mcp-server):\n\n### IDE Setup\n\n**Cursor IDE**\n\n```bash\nnpx -y @smithery/cli@latest install @hellokaton/unsplash-mcp-server --client cursor --key 7558c683-****-****\n```\n\n**Windsurf**\n\n```bash\nnpx -y @smithery/cli@latest install @hellokaton/unsplash-mcp-server --client windsurf --key 7558c683-****-****\n```\n\n**Cline**\n\n```bash\nnpx -y @smithery/cli@latest install @hellokaton/unsplash-mcp-server --client cline --key 7558c683-****-****\n```\n\n### Manual Installation\n\n```bash\n# Clone the repository\ngit clone https://github.com/hellokaton/unsplash-mcp-server.git\n\n# Navigate to project directory\ncd unsplash-mcp-server\n\n# Create virtual environment\nuv venv\n\n# Install dependencies\nuv pip install .\n```\n\n**Cursor Editor Integration**\n\nAdd the following configuration to your Cursor editor's `settings.json`:\n\n⚠️ **Note:** Please adjust the following configuration according to your actual installation:\n\n- If `uv` is not in your system PATH, use an absolute path (e.g., `/path/to/uv`)\n- `./server.py` should be modified to the actual location of your server script (can use absolute path or path relative to workspace)\n\n\n\n```json\n{\n  \"mcpServers\": {\n    \"unsplash\": {\n      \"command\": \"uv\",\n      \"args\": [\"run\", \"--with\", \"fastmcp\", \"fastmcp\", \"run\", \"./server.py\"],\n      \"env\": {\n        \"UNSPLASH_ACCESS_KEY\": \"${YOUR_ACCESS_KEY}\"\n      }\n    }\n  }\n}\n```\n\n### Using in Cursor\n\n\n\n## 🛠️ Available Tools\n\n### Search Photos\n\n```json\n{\n  \"tool\": \"search_photos\",\n  \"query\": \"mountain\",\n  \"per_page\": 5,\n  \"orientation\": \"landscape\"\n}\n```\n\n## 🔄 Other Implementations\n\n- Golang: [unsplash-mcp-server](https://github.com/douglarek/unsplash-mcp-server)\n- Java: [unsplash-mcp-server](https://github.com/JavaProgrammerLB/unsplash-mcp-server)\n\n## 📄 License\n\n[MIT License](LICENSE)\n\n## 📬 Contact\n\n- [Twitter/X](https://x.com/hellokaton)\n- [GitHub Issues](https://github.com/hellokaton/unsplash-mcp-server/issues)",
      "npm_url": "https://www.npmjs.com/package/unsplash-mcp-server",
      "npm_downloads": 315,
      "keywords": [
        "unsplash",
        "mcp",
        "searches",
        "unsplash mcp",
        "unsplash image",
        "image retrieval"
      ],
      "category": "web-search"
    },
    "hesreallyhim--mcp-server-isitdown": {
      "owner": "hesreallyhim",
      "name": "mcp-server-isitdown",
      "url": "https://github.com/hesreallyhim/mcp-server-isitdown",
      "imageUrl": "/freedevtools/mcp/pfp/hesreallyhim.webp",
      "description": "Checks the status of websites to determine if they are currently down and provides information on recent downtime events.",
      "stars": 1,
      "forks": 5,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-07-05T10:25:53Z",
      "readme_content": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/hesreallyhim-mcp-server-isitdown-badge.png)](https://mseep.ai/app/hesreallyhim-mcp-server-isitdown)\n[![Verified on MseeP](https://mseep.ai/badge.svg)](https://mseep.ai/app/24e83a9a-3b58-48c5-a490-04d6a631ed1e)\n# mcp-server-isitdown\n\nAn MCP server that checks if a website is currently down by querying [www.isitdownrightnow.com](https://www.isitdownrightnow.com).\n\n[![smithery badge](https://smithery.ai/badge/@hesreallyhim/mcp-server-isitdown)](https://smithery.ai/server/@hesreallyhim/mcp-server-isitdown)\n<a href=\"https://glama.ai/mcp/servers/1wx4z4amkm\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/1wx4z4amkm/badge\" alt=\"IsItDown Server MCP server\" />\n</a>\n\n## Overview\n\nThis MCP server provides a simple tool to check if a website is experiencing downtime, and can provide some information about recent downtime events.\n\n## Tools\n\nThe following tools are implemented:\n\n* **`get_website_status`**: Checks if a website is currently down or not.\n  * **`Input`**: `root_domain` (string): The root domain of the website to check (e.g., \"example.com\")\n  * **`Output`**: A string message indicating whether the website is up or down, with the last recorded downtime information\n\n## Installation\n\n### Installing via Smithery\n\nTo install IsItDown Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@hesreallyhim/mcp-server-isitdown):\n\n```bash\nnpx -y @smithery/cli install @hesreallyhim/mcp-server-isitdown --client claude\n```\n\n> **Note**: This package is not currently published to a public registry. Installation is only available from source.\n\n### From Source\n\n```bash\n# Clone the repository \ngit clone https://github.com/yourusername/mcp-server-isitdown.git\ncd mcp-server-isitdown\n\n# Using uv (recommended)\nuv pip install -e .\n\n# Using pip\npip install -e .\n```\n\n## Configuration for Claude Desktop\n\nAdd this configuration to your `claude_desktop_config.json` file:\n\n```json\n\"isitdown\": {\n  \"command\": \"/path/to/uv\",\n  \"args\": [\n    \"--directory\",\n    \"/path/to/cloned/repo/src\",\n    \"run\",\n    \"mcp_server_isitdown\"\n  ]\n}\n```\n\n## Usage\n\n### Run as a standalone MCP server\n\n```bash\n# Using the installed script\nmcp-server-isitdown\n\n# Using the Python module\npython -m mcp_server_isitdown\n```\n\n### Example usage with Claude for Dekstop:\n\n* \"Is wikipedia down right now?\"\n* \"When was the last time reddit was down?\"\n\n### Use as a library\n\n```python\nfrom mcp_server_isitdown.server import get_website_status\n\n# Check if a website is down (async function)\nasync def check_website():\n    result = await get_website_status(\"example.com\")\n    print(result)  # Prints status message with up/down status\n```\n\n## Development\n\n```bash\n# Type checking\nuvx mypy .\n\n# Run all pre-commit hooks\nuv pre-commit run --all-files\n\n# Install in development mode\nuv pip install -e \".[dev]\"\n\n# Run the Inspector\nmcp dev src/mcp_server_isitdown/server.py\n```\n\n## Build\n\n```bash\n# Build the package\nuv build\n\n# Install the built package\nuv pip install dist/mcp_isitdown_service-*.whl\n```\n\n## License\n\nMIT\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "isitdown",
        "downtime",
        "mcp",
        "server isitdown",
        "isitdown checks",
        "downtime events"
      ],
      "category": "web-search"
    },
    "hhw67865--tripadvisor-mcp-server": {
      "owner": "hhw67865",
      "name": "tripadvisor-mcp-server",
      "url": "https://github.com/hhw67865/tripadvisor-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/hhw67865.webp",
      "description": "Access TripAdvisor data to plan vacations by discovering attractions, restaurants, and hotels, while viewing photos and reviews. Offers an interactive prompt for vacation planning and detailed location information.",
      "stars": 2,
      "forks": 5,
      "license": "Apache License 2.0",
      "language": "Python",
      "updated_at": "2025-09-23T08:57:58Z",
      "readme_content": "# TripAdvisor Vacation Planner MCP Server\n\nThis MCP server provides access to TripAdvisor data for planning vacations, finding attractions, restaurants, and hotels.\n\n## Features\n\n- Search for locations by name and category\n- Get detailed information about specific locations\n- Find nearby attractions, restaurants, and hotels\n- View photos and reviews\n- Interactive vacation planning prompt\n\n## Setup Instructions\n\n### Prerequisites\n\n- Python 3.10 or higher\n- [uv](https://github.com/astral-sh/uv) - Fast Python package installer and resolver\n- TripAdvisor API key (get one from [TripAdvisor Developer Portal](https://developer.tripadvisor.com/))\n- Claude Desktop\n- Google Maps MCP Server (https://github.com/modelcontextprotocol/servers/tree/main/src/google-maps)\n\n### Installation with uv\n\n1. Clone this repository\n2. Create and activate a virtual environment:\n   ```bash\n   uv venv\n   # On Windows\n   .venv\\Scripts\\activate\n   # On macOS/Linux\n   source .venv/bin/activate\n   ```\n3. Install the required dependencies:\n   ```bash\n   uv add \"mcp[cli]\"\n   ```\n\n### Running the Server\n\nYou can run the server directly with:\n\n```bash\n# Set your API key as an environment variable\nexport TRIPADVISOR_API_KEY=your_api_key_here  # Linux/macOS\nset TRIPADVISOR_API_KEY=your_api_key_here     # Windows Command Prompt\n$env:TRIPADVISOR_API_KEY=\"your_api_key_here\"  # Windows PowerShell\n\n# Run the server\nmcp run server.py\n```\n\n### Setting up for Claude Desktop\n\nSet up the MCP Server with:\n\n```bash\nmcp install server.py\n```\n\n### Configuring Claude Desktop\n\n1. Open Claude Desktop\n2. Go to Settings > MCP Servers\n3. Add a new server with the following configuration:\n   ```json\n   {\n     \"tripadvisor\": {\n       \"command\": \"uv\",\n       \"args\": [\n         \"run\",\n         \"--with\",\n         \"mcp[cli]\",\n         \"mcp\",\n         \"run\",\n         \"PATH_TO_YOUR_PROJECT\\\\server.py\"\n       ],\n       \"env\": {\n         \"TRIPADVISOR_API_KEY\": \"YOUR_API_KEY_HERE\"\n       }\n     }\n   }\n   ```\n4. Replace `PATH_TO_YOUR_PROJECT` with the absolute path to your project directory\n5. Replace `YOUR_API_KEY_HERE` with your actual TripAdvisor API key\n\n### Using the Vacation Planner\n\n1. Start a new conversation in Claude\n2. Just prompt anything with \"Vacation Planner\" prompt\n3. Follow the interactive prompts to plan your perfect vacation!\n\n## API Endpoints Used\n\n- Location Search: Find locations by name and category\n- Location Details: Get comprehensive information about a location\n- Location Photos: View photos of a location\n- Location Reviews: Read reviews of a location\n- Nearby Search: Find locations near a specific point\n\n## Troubleshooting\n\n- If you see 401 Unauthorized errors, check that your API key is correct and that your IP is whitelisted in the TripAdvisor Developer Portal\n- For issues with Claude Desktop integration, verify your configuration settings and ensure the path to server.py is correct\n- If Claude is failing to complete, then there is a high chance that you are using too many input tokens. get_location_details_tool is usually the culprit.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "tripadvisor",
        "mcp",
        "vacations",
        "tripadvisor mcp",
        "vacation planning",
        "tripadvisor data"
      ],
      "category": "web-search"
    },
    "highlight-ing--highlight-youtube-mcp": {
      "owner": "highlight-ing",
      "name": "highlight-youtube-mcp",
      "url": "https://github.com/highlight-ing/highlight-youtube-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/highlight-ing.webp",
      "description": "Extract transcripts from YouTube videos by providing a video URL. The server supports multiple URL formats and returns the transcript text in a structured array format.",
      "stars": 1,
      "forks": 3,
      "license": "No License",
      "language": "JavaScript",
      "updated_at": "2025-03-10T02:12:46Z",
      "readme_content": "# YouTube Integration\n\nThe YouTube MCP server provides functionality to extract transcripts from YouTube videos.\n\n## Available Tools\n\n### get_youtube_transcript\nRetrieves the transcript text from a YouTube video.\n\n**Parameters**:\n- `videoUrl`: Full YouTube video URL (supports standard, shortened, and embed URLs)\n\n**Returns**: Object containing:\n- `content`: Array with transcript text\n\n## URL Support\n\nHandles multiple YouTube URL formats:\n- Standard: `https://www.youtube.com/watch?v=VIDEO_ID`\n- Shortened: `https://youtu.be/VIDEO_ID`\n- Embed: `https://www.youtube.com/embed/VIDEO_ID`\n\n## Error Handling\n\nThe server implements standard error handling:\n- Invalid URLs return `ErrorCode.InvalidParams`\n- Missing URL returns `ErrorCode.InvalidParams`\n- Failed transcript fetches return formatted error messages\n- Graceful shutdown on SIGINT\n\n## Technical Details\n\n- Built using the Highlight AI MCP SDK\n- Uses youtube-transcript library\n- Input validation via Zod\n- Runs as a stdio-based MCP server\n- Supports Node.js >=18.0.0\n\n## Limitations\n\n- Only works with videos that have captions enabled\n- Currently only returns English transcripts\n- Rate limits depend on YouTube's API restrictions\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "transcript",
        "transcripts",
        "youtube",
        "transcripts youtube",
        "highlight youtube",
        "youtube mcp"
      ],
      "category": "web-search"
    },
    "hiromitsusasaki--raindrop-io-mcp-server": {
      "owner": "hiromitsusasaki",
      "name": "raindrop-io-mcp-server",
      "url": "https://github.com/hiromitsusasaki/raindrop-io-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/hiromitsusasaki.webp",
      "description": "Interact with Raindrop.io bookmarks by creating, searching, and filtering them using a user-friendly interface. Integrates seamlessly with AI models to enhance bookmark management.",
      "stars": 61,
      "forks": 11,
      "license": "No License",
      "language": "TypeScript",
      "updated_at": "2025-10-02T06:15:06Z",
      "readme_content": "# Raindrop.io MCP Server\n[![smithery badge](https://smithery.ai/badge/@hiromitsusasaki/raindrop-io-mcp-server)](https://smithery.ai/server/@hiromitsusasaki/raindrop-io-mcp-server)\n\nAn integration that allows LLMs to interact with Raindrop.io bookmarks using the Model Context Protocol (MCP).\n\n<a href=\"https://glama.ai/mcp/servers/@hiromitsusasaki/raindrop-io-mcp-server\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@hiromitsusasaki/raindrop-io-mcp-server/badge\" alt=\"Raindrop.io Server MCP server\" />\n</a>\n\n## Features\n\n- Create bookmarks\n- Search bookmarks\n- Filter by tags\n\n## Requirements\n\n- Node.js 16 or higher\n- Raindrop.io account and API token\n\n## Setup\n\n### Installing via Smithery\n\nTo install Raindrop.io Integration for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@hiromitsusasaki/raindrop-io-mcp-server):\n\n```bash\nnpx -y @smithery/cli install @hiromitsusasaki/raindrop-io-mcp-server --client claude\n```\n\n### Manual Installation\n\n1. Clone the repository:\n```bash\ngit clone https://github.com/hiromitsusasaki/raindrop-io-mcp-server\ncd raindrop-io-mcp-server\n```\n\n2. Install dependencies:\n```bash\nnpm install\n```\n\n3. Set up environment variables:\n- Create a `.env` file and set your Raindrop.io API token\n```\nRAINDROP_TOKEN=your_access_token_here\n```\n\n4. Build:\n```bash\nnpm run build\n```\n\n## Using with Claude for Desktop\n\n1. Open Claude for Desktop configuration file:\n- macOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\n- Windows: `%APPDATA%\\Claude\\claude_desktop_config.json`\n\n2. Add the following configuration:\n```json\n{\n  \"mcpServers\": {\n    \"raindrop\": {\n      \"command\": \"node\",\n      \"args\": [\"PATH_TO_BUILD/index.js\"],\n      \"env\": {\n        \"RAINDROP_TOKEN\": \"your_access_token_here\"\n      }\n    }\n  }\n}\n```\n\n3. Restart Claude for Desktop\n\n## Available Tools\n\n### create-bookmark\nCreates a new bookmark.\n\n**Parameters:**\n- `url`: URL to bookmark (required)\n- `title`: Title for the bookmark (optional)\n- `tags`: Array of tags (optional)\n- `collection`: Collection ID (optional)\n\n### search-bookmarks\nSearches through bookmarks.\n\n**Parameters:**\n- `query`: Search query (required)\n- `tags`: Array of tags to filter by (optional)\n\n## Development\n\n```bash\n# Build for development\nnpm run build\n\n# Start server\nnpm start\n```\n\n## Security Notes\n\n- Always manage API tokens using environment variables\n- Set appropriate permissions for Claude for Desktop configuration files\n- Restrict unnecessary file access\n\n## Open Source\n\nThis is an open source MCP server that anyone can use and contribute to. The project is released under the MIT License.\n\n## Contributing\n\nContributions are welcome! Feel free to submit issues, feature requests, or pull requests to help improve this project.\n\n## Related Links\n\n- [Model Context Protocol](https://modelcontextprotocol.io/)\n- [Raindrop.io API Documentation](https://developer.raindrop.io/)",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "bookmarks",
        "bookmark",
        "raindrop",
        "io bookmarks",
        "bookmark management",
        "raindrop io"
      ],
      "category": "web-search"
    },
    "hks-anakin--openapi-mcp-server": {
      "owner": "hks-anakin",
      "name": "openapi-mcp-server",
      "url": "https://github.com/hks-anakin/openapi-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/hks-anakin.webp",
      "description": "Explores and summarizes OpenAPI specifications, providing overviews and specific endpoint details without requiring authentication. Facilitates understanding of APIs by generating code snippets and offering insights into operations.",
      "stars": 0,
      "forks": 0,
      "license": "MIT License",
      "language": "",
      "updated_at": "2025-04-22T13:38:33Z",
      "readme_content": "# OpenAPI MCP Server\n\n[![smithery badge](https://smithery.ai/badge/@janwilmake/openapi-mcp-server)](https://smithery.ai/server/@janwilmake/openapi-mcp-server) [![janwilmake/openapi-mcp-server context](https://badge.forgithub.com/janwilmake/openapi-mcp-server?excludePathPatterns=README-v1.md&excludePathPatterns=*.yaml)](https://uithub.com/janwilmake/openapi-mcp-server?excludePathPatterns=README-v1.md&excludePathPatterns=*.yaml)\n\nA Model Context Protocol (MCP) server for Claude/Cursor that enables searching and exploring OpenAPI specifications through oapis.org.\n\n- Demo: https://x.com/janwilmake/status/1903497808134496583\n- HN Thread: https://news.ycombinator.com/item?id=43447278\n- OpenAPISearch: https://github.com/janwilmake/openapisearch\n- OAPIS: https://github.com/janwilmake/oapis\n\nThe MCP works by applying a 3 step process :\n\n1. It figures out the openapi identifier you need\n2. It requests a summary of that in simple language\n3. It determines which endpoints you need, and checks out how exactly they work (again, in simple language)\n\n> [!IMPORTANT]\n> OpenAPI MCP has found a [new owner](https://github.com/janwilmake) and has been migrated from v1.2 to v2, which works different to the previous version. You can still access any version prior to v2.0.0 and their README is [here](README-v1.md)\n>\n> OpenAPI MCP v2 is a Work In Progress and focuses on exploration and providing context about APIs. It **does not** allow executing the endpoints as tools directly, as authentication isn't a solved problem with MCP yet. However, it's great for codegen!\n>\n> Expect bugs. Open To Contributers, [DM](https://x.com/janwilmake)\n\n## Features\n\n- Get an overview of any OpenAPI specification\n- Retrieve details about specific API operations\n- Support for both JSON and YAML formats\n- Tested with Claude Desktop and Cursor\n\n## Installation\n\n### Installing via Smithery\n\nTo install openapi-mcp-server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@janwilmake/openapi-mcp-server):\n\n```bash\nnpx -y @smithery/cli install @janwilmake/openapi-mcp-server --client claude\n```\n\n### Installing via npx\n\nRun and follow instructions:\n\n```bash\nnpx openapi-mcp-server@latest init\n```\n\n## Usage in Claude\n\nOnce installed, you can ask Claude to:\n\n- \"Find information about the Stripe API\"\n- \"Explain how to use the GitHub API's repository endpoints\"\n\nClaude will use the MCP server to:\n\n1. First get an overview of the requested API\n2. Then retrieve specific operation details as needed\n\n## Requirements\n\n- Node.js >= 16.17.0\n- Claude Desktop, Cursor, or any other MCP client.\n\n## License\n\nMIT\n",
      "npm_url": "https://www.npmjs.com/package/openapi-mcp-server",
      "npm_downloads": 10386,
      "keywords": [
        "openapi",
        "apis",
        "endpoint",
        "openapi specifications",
        "openapi mcp",
        "summarizes openapi"
      ],
      "category": "web-search"
    },
    "huanongfish--arxiv-mcp": {
      "owner": "huanongfish",
      "name": "arxiv-mcp",
      "url": "https://github.com/huanongfish/arxiv-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/huanongfish.webp",
      "description": "Search and access arXiv research papers through a simple interface, facilitating the retrieval and download of academic content for analysis. Streamline research with local storage and efficient access to extensive academic resources.",
      "stars": 6,
      "forks": 3,
      "license": "Apache License 2.0",
      "language": "Python",
      "updated_at": "2025-05-02T06:37:47Z",
      "readme_content": "[![Twitter Follow](https://img.shields.io/twitter/follow/JoeBlazick?style=social)](https://twitter.com/JoeBlazick)\n[![smithery badge](https://smithery.ai/badge/arxiv-mcp-server)](https://smithery.ai/server/arxiv-mcp-server)\n[![Python Version](https://img.shields.io/badge/python-3.11+-blue.svg)](https://www.python.org/downloads/)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![PyPI Downloads](https://img.shields.io/pypi/dm/arxiv-mcp-server.svg)](https://pypi.org/project/arxiv-mcp-server/)\n[![PyPI Version](https://img.shields.io/pypi/v/arxiv-mcp-server.svg)](https://pypi.org/project/arxiv-mcp-server/)\n\n# ArXiv MCP Server\n\n> 🔍 Enable AI assistants to search and access arXiv papers through a simple MCP interface.\n\nThe ArXiv MCP Server provides a bridge between AI assistants and arXiv's research repository through the Message Control Protocol (MCP). It allows AI models to search for papers and access their content in a programmatic way.\n\n<div align=\"center\">\n  \n🤝 **[Contribute](https://github.com/blazickjp/arxiv-mcp-server/blob/main/CONTRIBUTING.md)** • \n📝 **[Report Bug](https://github.com/blazickjp/arxiv-mcp-server/issues)**\n\n</div>\n\n## ✨ Core Features\n\n- 🔎 **Paper Search**: Query arXiv papers with filters for date ranges and categories\n- 📄 **Paper Access**: Download and read paper content\n- 📋 **Paper Listing**: View all downloaded papers\n- 🗃️ **Local Storage**: Papers are saved locally for faster access\n- 📝 **Prompts**: A Set of Research Prompts\n\n## 🚀 Quick Start\n\n### Installing via Smithery\n\nTo install ArXiv Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/arxiv-mcp-server):\n\n```bash\nnpx -y @smithery/cli install arxiv-mcp-server --client claude\n```\n\n### Installing Manually\nInstall using uv:\n\n```bash\nuv tool install arxiv-mcp-server\n```\n\nFor development:\n\n```bash\n# Clone and set up development environment\ngit clone https://github.com/blazickjp/arxiv-mcp-server.git\ncd arxiv-mcp-server\n\n# Create and activate virtual environment\nuv venv\nsource .venv/bin/activate\n\n# Install with test dependencies\nuv pip install -e \".[test]\"\n```\n\n### 🔌 MCP Integration\n\nAdd this configuration to your MCP client config file:\n\n```json\n{\n    \"mcpServers\": {\n        \"arxiv-mcp-server\": {\n            \"command\": \"uv\",\n            \"args\": [\n                \"tool\",\n                \"run\",\n                \"arxiv-mcp-server\",\n                \"--storage-path\", \"/path/to/paper/storage\"\n            ]\n        }\n    }\n}\n```\n\nFor Development:\n\n```json\n{\n    \"mcpServers\": {\n        \"arxiv-mcp-server\": {\n            \"command\": \"uv\",\n            \"args\": [\n                \"--directory\",\n                \"path/to/cloned/arxiv-mcp-server\",\n                \"run\",\n                \"arxiv-mcp-server\",\n                \"--storage-path\", \"/path/to/paper/storage\"\n            ]\n        }\n    }\n}\n```\n\n## 💡 Available Tools\n\nThe server provides four main tools:\n\n### 1. Paper Search\nSearch for papers with optional filters:\n\n```python\nresult = await call_tool(\"search_papers\", {\n    \"query\": \"transformer architecture\",\n    \"max_results\": 10,\n    \"date_from\": \"2023-01-01\",\n    \"categories\": [\"cs.AI\", \"cs.LG\"]\n})\n```\n\n### 2. Paper Download\nDownload a paper by its arXiv ID:\n\n```python\nresult = await call_tool(\"download_paper\", {\n    \"paper_id\": \"2401.12345\"\n})\n```\n\n### 3. List Papers\nView all downloaded papers:\n\n```python\nresult = await call_tool(\"list_papers\", {})\n```\n\n### 4. Read Paper\nAccess the content of a downloaded paper:\n\n```python\nresult = await call_tool(\"read_paper\", {\n    \"paper_id\": \"2401.12345\"\n})\n```\n\n## ⚙️ Configuration\n\nConfigure through environment variables:\n\n| Variable | Purpose | Default |\n|----------|---------|---------|\n| `ARXIV_STORAGE_PATH` | Paper storage location | ~/.arxiv-mcp-server/papers |\n\n## 🧪 Testing\n\nRun the test suite:\n\n```bash\npython -m pytest\n```\n\n## 📄 License\n\nReleased under the MIT License. See the LICENSE file for details.\n\n---\n\n<div align=\"center\">\n\nMade with ❤️ by the Pear Labs Team\n\n<a href=\"https://glama.ai/mcp/servers/04dtxi5i5n\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/04dtxi5i5n/badge\" alt=\"ArXiv Server MCP server\" /></a>\n</div>\n",
      "npm_url": "https://www.npmjs.com/package/arxiv-mcp-server",
      "npm_downloads": 800,
      "keywords": [
        "retrieval",
        "search",
        "arxiv",
        "arxiv research",
        "search huanongfish",
        "huanongfish arxiv"
      ],
      "category": "web-search"
    },
    "iamadk--reference-servers": {
      "owner": "iamadk",
      "name": "reference-servers",
      "url": "https://github.com/iamadk/reference-servers",
      "imageUrl": "/freedevtools/mcp/pfp/iamadk.webp",
      "description": "Retrieve and process web content by converting HTML to markdown, enabling easier extraction of information in manageable chunks. Supports starting extraction from a specified character index for better navigation of web content.",
      "stars": 0,
      "forks": 0,
      "license": "MIT License",
      "language": "",
      "updated_at": "2025-03-12T12:21:35Z",
      "readme_content": "# Model Context Protocol servers\n\nThis repository is a collection of *reference implementations* for the [Model Context Protocol](https://modelcontextprotocol.io/) (MCP), as well as references\nto community built servers and additional resources.\n\nThe servers in this repository showcase the versatility and extensibility of MCP, demonstrating how it can be used to give Large Language Models (LLMs) secure, controlled access to tools and data sources.\nEach MCP server is implemented with either the [Typescript MCP SDK](https://github.com/modelcontextprotocol/typescript-sdk) or [Python MCP SDK](https://github.com/modelcontextprotocol/python-sdk).\n\n> Note: Lists in this README are maintained in alphabetical order to minimize merge conflicts when adding new items.\n\n## 🌟 Reference Servers\n\nThese servers aim to demonstrate MCP features and the Typescript and Python SDK.\n\n- **[AWS KB Retrieval](src/aws-kb-retrieval-server)** - Retrieval from AWS Knowledge Base using Bedrock Agent Runtime\n- **[Brave Search](src/brave-search)** - Web and local search using Brave's Search API\n- **[EverArt](src/everart)** - AI image generation using various models\n- **[Everything](src/everything)** - Reference / test server with prompts, resources, and tools\n- **[Fetch](src/fetch)** - Web content fetching and conversion for efficient LLM usage\n- **[Filesystem](src/filesystem)** - Secure file operations with configurable access controls\n- **[Git](src/git)** - Tools to read, search, and manipulate Git repositories\n- **[GitHub](src/github)** - Repository management, file operations, and GitHub API integration\n- **[GitLab](src/gitlab)** - GitLab API, enabling project management\n- **[Google Drive](src/gdrive)** - File access and search capabilities for Google Drive\n- **[Google Maps](src/google-maps)** - Location services, directions, and place details\n- **[Memory](src/memory)** - Knowledge graph-based persistent memory system\n- **[PostgreSQL](src/postgres)** - Read-only database access with schema inspection\n- **[Puppeteer](src/puppeteer)** - Browser automation and web scraping\n- **[Sentry](src/sentry)** - Retrieving and analyzing issues from Sentry.io\n- **[Sequential Thinking](src/sequentialthinking)** - Dynamic and reflective problem-solving through thought sequences\n- **[Slack](src/slack)** - Channel management and messaging capabilities\n- **[Sqlite](src/sqlite)** - Database interaction and business intelligence capabilities\n- **[Time](src/time)** - Time and timezone conversion capabilities\n\n## 🤝 Third-Party Servers\n\n### 🎖️ Official Integrations\n\nOfficial integrations are maintained by companies building production ready MCP servers for their platforms.\n\n- <img height=\"12\" width=\"12\" src=\"https://axiom.co/favicon.ico\" alt=\"Axiom Logo\" /> **[Axiom](https://github.com/axiomhq/mcp-server-axiom)** - Query and analyze your Axiom logs, traces, and all other event data in natural language\n- <img height=\"12\" width=\"12\" src=\"https://browserbase.com/favicon.ico\" alt=\"Browserbase Logo\" /> **[Browserbase](https://github.com/browserbase/mcp-server-browserbase)** - Automate browser interactions in the cloud (e.g. web navigation, data extraction, form filling, and more)\n- <img alt=\"cloudflare\" height=\"12\" width=\"12\" src=\"https://cdn.simpleicons.org/cloudflare\" /> **[Cloudflare](https://github.com/cloudflare/mcp-server-cloudflare)** - Deploy, configure & interrogate your resources on the Cloudflare developer platform (e.g. Workers/KV/R2/D1)\n- <img height=\"12\" width=\"12\" src=\"https://e2b.dev/favicon.ico\" alt=\"E2B Logo\" /> **[E2B](https://github.com/e2b-dev/mcp-server)** - Run code in secure sandboxes hosted by [E2B](https://e2b.dev)\n- <img height=\"12\" width=\"12\" src=\"https://exa.ai/images/favicon-32x32.png\" alt=\"Exa Logo\" /> **[Exa](https://github.com/exa-labs/exa-mcp-server)** - Search Engine made for AIs by [Exa](https://exa.ai)\n- <img height=\"12\" width=\"12\" src=\"https://fireproof.storage/favicon.ico\" alt=\"Fireproof Logo\" /> **[Fireproof](https://github.com/fireproof-storage/mcp-database-server)** - Immutable ledger database with live synchronization\n- <img alt=\"jetbrains\" height=\"12\" width=\"12\" src=\"https://cdn.simpleicons.org/jetbrains\" /> **[JetBrains](https://github.com/JetBrains/mcp-jetbrains)** – Work on your code with JetBrains IDEs\n- <img height=\"12\" width=\"12\" src=\"https://kagi.com/favicon.ico\" alt=\"Kagi Logo\" /> **[Kagi Search](https://github.com/kagisearch/kagimcp)** - Search the web using Kagi's search API\n- <img height=\"12\" width=\"12\" src=\"https://www.meilisearch.com/favicon.ico\" alt=\"Meilisearch Logo\" /> **[Meilisearch](https://github.com/meilisearch/meilisearch-mcp)** - Interact & query with Meilisearch (Full-text & semantic search API)\n-  **[Metoro](https://github.com/metoro-io/metoro-mcp-server)** - Query and interact with kubernetes environments monitored by Metoro\n- <img height=\"12\" width=\"12\" src=\"https://www.motherduck.com/favicon.ico\" alt=\"MotherDuck Logo\" /> **[MotherDuck](https://github.com/motherduckdb/mcp-server-motherduck)** - Query and analyze data with MotherDuck and local DuckDB\n- <img height=\"12\" width=\"12\" src=\"https://needle-ai.com/images/needle-logo-orange-2-rounded.png\" alt=\"Needle AI Logo\" /> **[Needle](https://github.com/needle-ai/needle-mcp)** - Production-ready RAG out of the box to search and retrieve data from your own documents.\n- <img height=\"12\" width=\"12\" src=\"https://neo4j.com/favicon.ico\" alt=\"Neo4j Logo\" /> **[Neo4j](https://github.com/neo4j-contrib/mcp-neo4j/)** - Neo4j graph database server (schema + read/write-cypher) and separate graph database backed memory\n- **[Neon](https://github.com/neondatabase/mcp-server-neon)** - Interact with the Neon serverless Postgres platform\n- <img alt=\"logomark\" height=\"12\" width=\"12\" src=\"https://qdrant.tech/img/brand-resources-logos/logomark.svg\" /> **[Qdrant](https://github.com/qdrant/mcp-server-qdrant/)** - Implement semantic memory layer on top of the Qdrant vector search engine\n- **[Raygun](https://github.com/MindscapeHQ/mcp-server-raygun)** - Interact with your crash reporting and real using monitoring data on your Raygun account\n- <img alt=\"56912e614b35093426c515860f9f2234\" height=\"12\" width=\"12\" src=\"https://pics.fatwang2.com/56912e614b35093426c515860f9f2234.svg\" /> [Search1API](https://github.com/fatwang2/search1api-mcp) - One API for Search, Crawling, and Sitemaps\n- <img height=\"12\" width=\"12\" src=\"https://www.tinybird.co/favicon.ico\" alt=\"Tinybird Logo\" /> **[Tinybird](https://github.com/tinybirdco/mcp-tinybird)** - Interact with Tinybird serverless ClickHouse platform\n\n### 🌎 Community Servers\n\nA growing set of community-developed and maintained servers demonstrates various applications of MCP across different domains.\n\n> **Note:** Community servers are **untested** and should be used at **your own risk**. They are not affiliated with or endorsed by Anthropic.\n\n- **[AWS S3](https://github.com/aws-samples/sample-mcp-server-s3)** - A sample MCP server for AWS S3 that flexibly fetches objects from S3 such as PDF documents\n- **[AWS](https://github.com/rishikavikondala/mcp-server-aws)** - Perform operations on your AWS resources using an LLM\n- **[Airtable](https://github.com/domdomegg/airtable-mcp-server)** - Read and write access to [Airtable](https://airtable.com/) databases, with schema inspection.\n- **[Airtable](https://github.com/felores/airtable-mcp)** - Airtable Model Context Protocol Server.\n- **[AlphaVantage](https://github.com/calvernaz/alphavantage)** - MCP server for stock market data API [AlphaVantage](https://www.alphavantage.co)\n- **[Anki](https://github.com/scorzeth/anki-mcp-server)** - An MCP server for interacting with your [Anki](https://apps.ankiweb.net) decks and cards.\n- **[Any Chat Completions](https://github.com/pyroprompts/any-chat-completions-mcp)** - Interact with any OpenAI SDK Compatible Chat Completions API like OpenAI, Perplexity, Groq, xAI and many more.\n- **[Atlassian](https://github.com/sooperset/mcp-atlassian)** - Interact with Atlassian Cloud products (Confluence and Jira) including searching/reading Confluence spaces/pages, accessing Jira issues, and project metadata.\n- **[BigQuery](https://github.com/LucasHild/mcp-server-bigquery)** (by LucasHild) - This server enables LLMs to inspect database schemas and execute queries on BigQuery.\n- **[BigQuery](https://github.com/ergut/mcp-bigquery-server)** (by ergut) - Server implementation for Google BigQuery integration that enables direct BigQuery database access and querying capabilities\n- **[ChatMCP](https://github.com/AI-QL/chat-mcp)** – An Open Source Cross-platform GUI Desktop application compatible with Linux, macOS, and Windows, enabling seamless interaction with MCP servers across dynamically selectable LLMs, by **[AIQL](https://github.com/AI-QL)**\n- **[ChatSum](https://github.com/mcpso/mcp-server-chatsum)** - Query and Summarize chat messages with LLM. by [mcpso](https://mcp.so)\n- **[Chroma](https://github.com/privetin/chroma)** - Vector database server for semantic document search and metadata filtering, built on Chroma\n- **[Cloudinary](https://github.com/felores/cloudinary-mcp-server)** - Cloudinary Model Context Protocol Server to upload media to Cloudinary and get back the media link and details.\n- **[cognee-mcp](https://github.com/topoteretes/cognee-mcp-server)** - GraphRAG memory server with customizable ingestion, data processing and search\n- **[coin_api_mcp](https://github.com/longmans/coin_api_mcp)** - Provides access to [coinmarketcap](https://coinmarketcap.com/) cryptocurrency data.\n- **[Contentful-mcp](https://github.com/ivo-toby/contentful-mcp)** - Read, update, delete, publish content in your [Contentful](https://contentful.com) space(s) from this MCP Server.\n- **[Data Exploration](https://github.com/reading-plus-ai/mcp-server-data-exploration)** - MCP server for autonomous data exploration on .csv-based datasets, providing intelligent insights with minimal effort. NOTE: Will execute arbitrary Python code on your machine, please use with caution!\n- **[Dataset Viewer](https://github.com/privetin/dataset-viewer)** - Browse and analyze Hugging Face datasets with features like search, filtering, statistics, and data export\n- **[DevRev](https://github.com/kpsunil97/devrev-mcp-server)** - An MCP server to integrate with DevRev APIs to search through your DevRev Knowledge Graph where objects can be imported from diff. sources listed [here](https://devrev.ai/docs/import#available-sources).\n- **[Dify](https://github.com/YanxingLiu/dify-mcp-server)** - A simple implementation of an MCP server for dify workflows.\n- **[Docker](https://github.com/ckreiling/mcp-server-docker)** - Integrate with Docker to manage containers, images, volumes, and networks.\n- **[Drupal](https://github.com/Omedia/mcp-server-drupal)** - Server for interacting with [Drupal](https://www.drupal.org/project/mcp) using STDIO transport layer.\n- **[Elasticsearch](https://github.com/cr7258/elasticsearch-mcp-server)** - MCP server implementation that provides Elasticsearch interaction.\n- **[Fetch](https://github.com/zcaceres/fetch-mcp)** - A server that flexibly fetches HTML, JSON, Markdown, or plaintext.\n- **[FireCrawl](https://github.com/vrknetha/mcp-server-firecrawl)** - Advanced web scraping with JavaScript rendering, PDF support, and smart rate limiting\n- **[FlightRadar24](https://github.com/sunsetcoder/flightradar24-mcp-server)** - A Claude Desktop MCP server that helps you track flights in real-time using Flightradar24 data.\n- **[Glean](https://github.com/longyi1207/glean-mcp-server)** - A server that uses Glean API to search and chat.\n- **[Google Calendar](https://github.com/v-3/google-calendar)** - Integration with Google Calendar to check schedules, find time, and add/delete events\n- **[Google Tasks](https://github.com/zcaceres/gtasks-mcp)** - Google Tasks API Model Context Protocol Server.\n- **[Home Assistant](https://github.com/tevonsb/homeassistant-mcp)** - Interact with [Home Assistant](https://www.home-assistant.io/) including viewing and controlling lights, switches, sensors, and all other Home Assistant entities.\n- **[HuggingFace Spaces](https://github.com/evalstate/mcp-hfspace)** - Server for using HuggingFace Spaces, supporting Open Source Image, Audio, Text Models and more. Claude Desktop mode for easy integration.\n- **[Inoyu](https://github.com/sergehuber/inoyu-mcp-unomi-server)** - Interact with an Apache Unomi CDP customer data platform to retrieve and update customer profiles\n- **[Keycloak MCP](https://github.com/ChristophEnglisch/keycloak-model-context-protocol)** - This MCP server enables natural language interaction with Keycloak for user and realm management including creating, deleting, and listing users and realms.\n- **[Kubernetes](https://github.com/Flux159/mcp-server-kubernetes)** - Connect to Kubernetes cluster and manage pods, deployments, and services.\n- **[Linear](https://github.com/jerhadf/linear-mcp-server)** - Allows LLM to interact with Linear's API for project management, including searching, creating, and updating issues.\n- **[LlamaCloud](https://github.com/run-llama/mcp-server-llamacloud)** (by marcusschiesser) - Integrate the data stored in a managed index on [LlamaCloud](https://cloud.llamaindex.ai/)\n- **[MCP Installer](https://github.com/anaisbetts/mcp-installer)** - This server is a server that installs other MCP servers for you.\n- **[mcp-k8s-go](https://github.com/strowk/mcp-k8s-go)** - Golang-based Kubernetes server for MCP to browse pods and their logs, events, namespaces and more. Built to be extensible.\n- **[MSSQL](https://github.com/aekanun2020/mcp-server/)** - MSSQL database integration with configurable access controls and schema inspection\n- **[Markdownify](https://github.com/zcaceres/mcp-markdownify-server)** - MCP to convert almost anything to Markdown (PPTX, HTML, PDF, Youtube Transcripts and more)\n- **[Minima](https://github.com/dmayboroda/minima)** - MCP server for RAG on local files\n- **[MongoDB](https://github.com/kiliczsh/mcp-mongo-server)** - A Model Context Protocol Server for MongoDB.\n- **[MySQL](https://github.com/benborla/mcp-server-mysql)** (by benborla) - MySQL database integration in NodeJS with configurable access controls and schema inspection\n- **[MySQL](https://github.com/designcomputer/mysql_mcp_server)** (by DesignComputer) - MySQL database integration in Python with configurable access controls and schema inspection\n- **[NS Travel Information](https://github.com/r-huijts/ns-mcp-server)** - Access Dutch Railways (NS) real-time train travel information and disruptions through the official NS API.\n- **[Notion](https://github.com/suekou/mcp-notion-server)** (by suekou) - Interact with Notion API.\n- **[Notion](https://github.com/v-3/notion-server)** (by v-3) - Notion MCP integration. Search, Read, Update, and Create pages through Claude chat.\n- **[oatpp-mcp](https://github.com/oatpp/oatpp-mcp)** - C++ MCP integration for Oat++. Use [Oat++](https://oatpp.io) to build MCP servers.\n- **[Obsidian Markdown Notes](https://github.com/calclavia/mcp-obsidian)** - Read and search through your Obsidian vault or any directory containing Markdown notes\n- **[OpenAPI](https://github.com/snaggle-ai/openapi-mcp-server)** - Interact with [OpenAPI](https://www.openapis.org/) APIs.\n- **[OpenCTI](https://github.com/Spathodea-Network/opencti-mcp)** - Interact with OpenCTI platform to retrieve threat intelligence data including reports, indicators, malware and threat actors.\n- **[OpenRPC](https://github.com/shanejonas/openrpc-mpc-server)** - Interact with and discover JSON-RPC APIs via [OpenRPC](https://open-rpc.org).\n- **[Pandoc](https://github.com/vivekVells/mcp-pandoc)** - MCP server for seamless document format conversion using Pandoc, supporting Markdown, HTML, and plain text, with other formats like PDF, csv and docx in development.\n- **[Pinecone](https://github.com/sirmews/mcp-pinecone)** - MCP server for searching and uploading records to Pinecone. Allows for simple RAG features, leveraging Pinecone's Inference API.\n- **[Placid.app](https://github.com/felores/placid-mcp-server)** - Generate image and video creatives using Placid.app templates\n- **[Playwright](https://github.com/executeautomation/mcp-playwright)** - This MCP Server will help you run browser automation and webscraping using Playwright\n- **[Postman](https://github.com/shannonlal/mcp-postman)** - MCP server for running Postman Collections locally via Newman. Allows for simple execution of Postman Server and returns the results of whether the collection passed all the tests.\n- **[RAG Web Browser](https://github.com/apify/mcp-server-rag-web-browser)** An MCP server for Apify's RAG Web Browser Actor to perform web searches, scrape URLs, and return content in Markdown.\n- **[Rememberizer AI](https://github.com/skydeckai/mcp-server-rememberizer)** - An MCP server designed for interacting with the Rememberizer data source, facilitating enhanced knowledge retrieval.\n- **[Salesforce MCP](https://github.com/smn2gnt/MCP-Salesforce)** - Interact with Salesforce Data and Metadata\n- **[Scholarly](https://github.com/adityak74/mcp-scholarly)** - A MCP server to search for scholarly and academic articles.\n- **[Snowflake](https://github.com/isaacwasserman/mcp-snowflake-server)** - This MCP server enables LLMs to interact with Snowflake databases, allowing for secure and controlled data operations.\n- **[Spotify](https://github.com/varunneal/spotify-mcp)** - This MCP allows an LLM to play and use Spotify.\n- **[TMDB](https://github.com/Laksh-star/mcp-server-tmdb)** - This MCP server integrates with The Movie Database (TMDB) API to provide movie information, search capabilities, and recommendations.\n- **[Tavily search](https://github.com/RamXX/mcp-tavily)** - An MCP server for Tavily's search & news API, with explicit site inclusions/exclusions\n- **[Todoist](https://github.com/abhiz123/todoist-mcp-server)** - Interact with Todoist to manage your tasks.\n- **[Vega-Lite](https://github.com/isaacwasserman/mcp-vegalite-server)** - Generate visualizations from fetched data using the VegaLite format and renderer.\n- **[Windows CLI](https://github.com/SimonB97/win-cli-mcp-server)** - MCP server for secure command-line interactions on Windows systems, enabling controlled access to PowerShell, CMD, and Git Bash shells.\n- **[X (Twitter)](https://github.com/EnesCinr/twitter-mcp)** (by EnesCinr) - Interact with twitter API. Post tweets and search for tweets by query.\n- **[X (Twitter)](https://github.com/vidhupv/x-mcp)** (by vidhupv) - Create, manage and publish X/Twitter posts directly through Claude chat.\n- **[XMind](https://github.com/apeyroux/mcp-xmind)** - Read and search through your XMind directory containing XMind files.\n\n## 📚 Frameworks\n\nThese are high-level frameworks that make it easier to build MCP servers.\n\n* [EasyMCP](https://github.com/zcaceres/easy-mcp/) (TypeScript)\n* [FastMCP](https://github.com/punkpeye/fastmcp) (TypeScript)\n\n## 📚 Resources\n\nAdditional resources on MCP.\n\n- **[AiMCP](https://www.aimcp.info)** - A collection of MCP clients&servers to find the right mcp tools by **[Hekmon](https://github.com/hekmon8)**\n- **[Awesome Crypto MCP Servers by badkk](https://github.com/badkk/awesome-crypto-mcp-servers)** - A curated list of MCP servers by **[Luke Fan](https://github.com/badkk)**\n- **[Awesome MCP Servers by appcypher](https://github.com/appcypher/awesome-mcp-servers)** - A curated list of MCP servers by **[Stephen Akinyemi](https://github.com/appcypher)**\n- **[Awesome MCP Servers by punkpeye](https://github.com/punkpeye/awesome-mcp-servers)** (**[website](https://glama.ai/mcp/servers)**) - A curated list of MCP servers by **[Frank Fiegel](https://github.com/punkpeye)**\n- **[Awesome MCP Servers by wong2](https://github.com/wong2/awesome-mcp-servers)** (**[website](https://mcpservers.org)**) - A curated list of MCP servers by **[wong2](https://github.com/wong2)**\n- **[Discord Server](https://glama.ai/mcp/discord)** – A community discord server dedicated to MCP by **[Frank Fiegel](https://github.com/punkpeye)**\n- **[MCP Badges](https://github.com/mcpx-dev/mcp-badges)** – Quickly highlight your MCP project with clear, eye-catching badges, by **[Ironben](https://github.com/nanbingxyz)**\n- **[MCP X Community](https://x.com/i/communities/1861891349609603310)** – A X community for MCP by **[Xiaoyi](https://x.com/chxy)**\n- **[mcp-cli](https://github.com/wong2/mcp-cli)** - A CLI inspector for the Model Context Protocol by **[wong2](https://github.com/wong2)**\n- **[mcp-get](https://mcp-get.com)** - Command line tool for installing and managing MCP servers by **[Michael Latman](https://github.com/michaellatman)**\n- **[mcp-manager](https://github.com/zueai/mcp-manager)** - Simple Web UI to install and manage MCP servers for Claude Desktop by **[Zue](https://github.com/zueai)**\n- **[MCPHub](https://github.com/Jeamee/MCPHub-Desktop)** – An Open Source MacOS & Windows GUI Desktop app for discovering, installing and managing MCP servers by **[Jeamee](https://github.com/jeamee)**\n- **[mcp.run](https://mcp.run)** - A hosted registry and control plane to install & run secure + portable MCP Servers.\n- **[Open-Sourced MCP Servers Directory](https://github.com/chatmcp/mcp-directory)** - A curated list of MCP servers by **[mcpso](https://mcp.so)**\n- **[PulseMCP](https://www.pulsemcp.com)** ([API](https://www.pulsemcp.com/api)) - Community hub & weekly newsletter for discovering MCP servers, clients, articles, and news by **[Tadas Antanavicius](https://github.com/tadasant)**, **[Mike Coughlin](https://github.com/macoughl)**, and **[Ravina Patel](https://github.com/ravinahp)**\n- **[r/mcp](https://www.reddit.com/r/mcp)** – A Reddit community dedicated to MCP by **[Frank Fiegel](https://github.com/punkpeye)**\n- **[Smithery](https://smithery.ai/)** - A registry of MCP servers to find the right tools for your LLM agents by **[Henry Mao](https://github.com/calclavia)**\n\n## 🚀 Getting Started\n\n### Using MCP Servers in this Repository\nTypescript-based servers in this repository can be used directly with `npx`.\n\nFor example, this will start the [Memory](src/memory) server:\n```sh\nnpx -y @modelcontextprotocol/server-memory\n```\n\nPython-based servers in this repository can be used directly with [`uvx`](https://docs.astral.sh/uv/concepts/tools/) or [`pip`](https://pypi.org/project/pip/). `uvx` is recommended for ease of use and setup.\n\nFor example, this will start the [Git](src/git) server:\n```sh\n# With uvx\nuvx mcp-server-git\n\n# With pip\npip install mcp-server-git\npython -m mcp_server_git\n```\n\nFollow [these](https://docs.astral.sh/uv/getting-started/installation/) instructions to install `uv` / `uvx` and [these](https://pip.pypa.io/en/stable/installation/) to install `pip`.\n\n### Using an MCP Client\nHowever, running a server on its own isn't very useful, and should instead be configured into an MCP client. For example, here's the Claude Desktop configuration to use the above server:\n\n```json\n{\n  \"mcpServers\": {\n    \"memory\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@modelcontextprotocol/server-memory\"]\n    }\n  }\n}\n```\n\nAdditional examples of using the Claude Desktop as an MCP client might look like:\n\n```json\n{\n  \"mcpServers\": {\n    \"filesystem\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@modelcontextprotocol/server-filesystem\", \"/path/to/allowed/files\"]\n    },\n    \"git\": {\n      \"command\": \"uvx\",\n      \"args\": [\"mcp-server-git\", \"--repository\", \"path/to/git/repo\"]\n    },\n    \"github\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@modelcontextprotocol/server-github\"],\n      \"env\": {\n        \"GITHUB_PERSONAL_ACCESS_TOKEN\": \"<YOUR_TOKEN>\"\n      }\n    },\n    \"postgres\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@modelcontextprotocol/server-postgres\", \"postgresql://localhost/mydb\"]\n    }\n  }\n}\n```\n\n## 🛠️ Creating Your Own Server\n\nInterested in creating your own MCP server? Visit the official documentation at [modelcontextprotocol.io](https://modelcontextprotocol.io/introduction) for comprehensive guides, best practices, and technical details on implementing MCP servers.\n\n## 🤝 Contributing\n\nSee [CONTRIBUTING.md](CONTRIBUTING.md) for information about contributing to this repository.\n\n## 🔒 Security\n\nSee [SECURITY.md](SECURITY.md) for reporting security vulnerabilities.\n\n## 📜 License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n\n## 💬 Community\n\n- [GitHub Discussions](https://github.com/orgs/modelcontextprotocol/discussions)\n\n## ⭐ Support\n\nIf you find MCP servers useful, please consider starring the repository and contributing new servers or improvements!\n\n---\n\nManaged by Anthropic, but built together with the community. The Model Context Protocol is open source and we encourage everyone to contribute their own servers and improvements!",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "markdown",
        "html",
        "search",
        "search iamadk",
        "web search",
        "web content"
      ],
      "category": "web-search"
    },
    "ibrooksSDX--mcp-server-opensearch": {
      "owner": "ibrooksSDX",
      "name": "mcp-server-opensearch",
      "url": "https://github.com/ibrooksSDX/mcp-server-opensearch",
      "imageUrl": "/freedevtools/mcp/pfp/ibrooksSDX.webp",
      "description": "Create a semantic memory layer on top of OpenSearch to facilitate LLM applications by integrating external data sources and tools.",
      "stars": 4,
      "forks": 3,
      "license": "Apache License 2.0",
      "language": "Python",
      "updated_at": "2025-06-05T06:01:06Z",
      "readme_content": "# mcp-server-opensearch: An OpenSearch MCP Server\n[![smithery badge](https://smithery.ai/badge/@ibrooksSDX/mcp-server-opensearch)](https://smithery.ai/server/@ibrooksSDX/mcp-server-opensearch)\n\n> The [Model Context Protocol (MCP)](https://modelcontextprotocol.io/introduction) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.\n\nThis repository is an example of how to create a MCP server for [OpenSearch](https://opensearch.org/), a distributed search and analytics engine.\n\n# Under Contruction \n\n\n\n\n\n## Current Blocker - Async Client from OpenSearch isn't installing\n\n[Open Search Async Client Docs](https://github.com/opensearch-project/opensearch-py/blob/main/guides/async.m) \n\n```shell\npip install opensearch-py[async]\nzsh: no matches found: opensearch-py[async]\n```\n\n## Overview \n\nA basic Model Context Protocol server for keeping and retrieving memories in the OpenSearch engine.\nIt acts as a semantic memory layer on top of the OpenSearch database.\n\n## Components\n\n### Tools\n\n1. `search-openSearch`\n   - Store a memory in the OpenSearch database\n   - Input:\n     - `query` (json): prepared json query message\n   - Returns: Confirmation message\n\n## Installation\n\n### Installing via Smithery\n\nTo install mcp-server-opensearch for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@ibrooksSDX/mcp-server-opensearch):\n\n```bash\nnpx -y @smithery/cli install @ibrooksSDX/mcp-server-opensearch --client claude\n```\n\n### Using uv (recommended)\n\nWhen using [`uv`](https://docs.astral.sh/uv/) no specific installation is needed to directly run *mcp-server-opensearch*.\n\n```shell\nuv run mcp-server-opensearch \\\n  --opensearch-url \"http://localhost:9200\" \\\n  --index-name \"my_index\" \\\n```\nor \n\n```shell\nuv run fastmcp run demo.py:main\n```\n\n## Testing - Local Open Search Client\n\n\n\n```shell\nuv run python src/mcp-server-opensearch/test_opensearch.py\n```\n## Testing - MCP Server Connection to Open Search Client\n\n\n\n\n```shell\ncd src/mcp-server-opensearch\nuv run fastmcp dev demo.py\n```\n\n## Usage with Claude Desktop\n\nTo use this server with the Claude Desktop app, add the following configuration to the \"mcpServers\" section of your `claude_desktop_config.json`:\n\n```json\n{\n  \"opensearch\": {\n    \"command\": \"uvx\",\n    \"args\": [\n      \"mcp-server-opensearch\",\n      \"--opensearch-url\",\n      \"http://localhost:9200\",\n      \"--opensearch-api-key\",\n      \"your_api_key\",\n      \"--index-name\",\n      \"your_index_name\"\n    ]\n  }, \"Demo\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"run\",\n        \"--with\",\n        \"fastmcp\",\n        \"--with\",\n        \"opensearch-py\",\n        \"fastmcp\",\n        \"run\",\n        \"/Users/ibrooks/Documents/GitHub/mcp-server-opensearch/src/mcp-server-opensearch/demo.py\"\n      ]\n    }\n}\n```\n\nOr use the FastMCP UI to install the server to Claude\n\n```shell\nuv run fastmcp install demo.py\n```\n\n## Environment Variables\n\nThe configuration of the server can be also done using environment variables:\n\n- `OPENSEARCH_HOST`: URL of the OpenSearch server, e.g. `http://localhost`\n- `OPENSEARCH_HOSTPORT`: Port of the host of the OpenSearch server `9200`\n- `INDEX_NAME`: Name of the index to use",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "opensearch",
        "search",
        "llm",
        "opensearch facilitate",
        "server opensearch",
        "layer opensearch"
      ],
      "category": "web-search"
    },
    "icraft2170--youtube-data-mcp-server": {
      "owner": "icraft2170",
      "name": "youtube-data-mcp-server",
      "url": "https://github.com/icraft2170/youtube-data-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/icraft2170.webp",
      "description": "Interact with YouTube content to retrieve video details, manage captions, and analyze trends using the YouTube Data API for real-time insights.",
      "stars": 47,
      "forks": 23,
      "license": "No License",
      "language": "JavaScript",
      "updated_at": "2025-09-22T05:15:44Z",
      "readme_content": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/icraft2170-youtube-data-mcp-server-badge.png)](https://mseep.ai/app/icraft2170-youtube-data-mcp-server)\n\n# YouTube MCP Server\n[![smithery badge](https://smithery.ai/badge/@icraft2170/youtube-data-mcp-server)](https://smithery.ai/server/@icraft2170/youtube-data-mcp-server)\n\nA Model Context Protocol (MCP) server implementation utilizing the YouTube Data API. It allows AI language models to interact with YouTube content through a standardized interface.\n\n## Key Features\n\n### Video Information\n* Retrieve detailed video information (title, description, duration, statistics)\n* Search for videos by keywords\n* Get related videos based on a specific video\n* Calculate and analyze video engagement ratios\n\n### Transcript/Caption Management\n* Retrieve video captions with multi-language support\n* Specify language preferences for transcripts\n* Access time-stamped captions for precise content reference\n\n### Channel Analysis\n* View detailed channel statistics (subscribers, views, video count)\n* Get top-performing videos from a channel\n* Analyze channel growth and engagement metrics\n\n### Trend Analysis\n* View trending videos by region and category\n* Compare performance metrics across multiple videos\n* Discover popular content in specific categories\n\n## Available Tools\n\nThe server provides the following MCP tools:\n\n| Tool Name | Description | Required Parameters |\n|-----------|-------------|---------------------|\n| `getVideoDetails` | Get detailed information about multiple YouTube videos including metadata, statistics, and content details | `videoIds` (array) |\n| `searchVideos` | Search for videos based on a query string | `query`, `maxResults` (optional) |\n| `getTranscripts` | Retrieve transcripts for multiple videos | `videoIds` (array), `lang` (optional) |\n| `getRelatedVideos` | Get videos related to a specific video based on YouTube's recommendation algorithm | `videoId`, `maxResults` (optional) |\n| `getChannelStatistics` | Retrieve detailed metrics for multiple channels including subscriber count, view count, and video count | `channelIds` (array) |\n| `getChannelTopVideos` | Get the most viewed videos from a specific channel | `channelId`, `maxResults` (optional) |\n| `getVideoEngagementRatio` | Calculate engagement metrics for multiple videos (views, likes, comments, and engagement ratio) | `videoIds` (array) |\n| `getTrendingVideos` | Get currently popular videos by region and category | `regionCode` (optional), `categoryId` (optional), `maxResults` (optional) |\n| `compareVideos` | Compare statistics across multiple videos | `videoIds` (array) |\n\n## Installation\n\n### Automatic Installation via Smithery\n\nAutomatically install YouTube MCP Server for Claude Desktop via [Smithery](https://smithery.ai/server/@icraft2170/youtube-data-mcp-server):\n\n```bash\nnpx -y @smithery/cli install @icraft2170/youtube-data-mcp-server --client claude\n```\n\n### Manual Installation\n```bash\n# Install from npm\nnpm install youtube-data-mcp-server\n\n# Or clone repository\ngit clone https://github.com/icraft2170/youtube-data-mcp-server.git\ncd youtube-data-mcp-server\nnpm install\n```\n\n## Environment Configuration\nSet the following environment variables:\n* `YOUTUBE_API_KEY`: YouTube Data API key (required)\n* `YOUTUBE_TRANSCRIPT_LANG`: Default caption language (optional, default: 'ko')\n\n## MCP Client Configuration\nAdd the following to your Claude Desktop configuration file:\n\n```json\n{\n  \"mcpServers\": {\n    \"youtube\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"youtube-data-mcp-server\"],\n      \"env\": {\n        \"YOUTUBE_API_KEY\": \"YOUR_API_KEY_HERE\",\n        \"YOUTUBE_TRANSCRIPT_LANG\": \"ko\"\n      }\n    }\n  }\n}\n```\n\n## YouTube API Setup\n1. Access Google Cloud Console\n2. Create a new project or select an existing one\n3. Enable YouTube Data API v3\n4. Create API credentials (API key)\n5. Use the generated API key in your environment configuration\n\n## Development\n\n```bash\n# Install dependencies\nnpm install\n\n# Run in development mode\nnpm run dev\n\n# Build\nnpm run build\n```\n\n## Network Configuration\n\nThe server exposes the following ports for communication:\n- HTTP: 3000\n- gRPC: 3001\n\n## System Requirements\n- Node.js 18.0.0 or higher\n\n## Security Considerations\n- Always keep your API key secure and never commit it to version control systems\n- Manage your API key through environment variables or configuration files\n- Set usage limits for your API key to prevent unauthorized use\n\n## License\nThis project is licensed under the MIT License. See the LICENSE file for details. \n",
      "npm_url": "https://www.npmjs.com/package/youtube-data-mcp-server",
      "npm_downloads": 41607,
      "keywords": [
        "youtube",
        "icraft2170",
        "data",
        "youtube data",
        "icraft2170 youtube",
        "youtube content"
      ],
      "category": "web-search"
    },
    "ihor-sokoliuk--mcp-searxng": {
      "owner": "ihor-sokoliuk",
      "name": "mcp-searxng",
      "url": "https://github.com/ihor-sokoliuk/mcp-searxng",
      "imageUrl": "/freedevtools/mcp/pfp/ihor-sokoliuk.webp",
      "description": "Integrates the SearXNG API to provide web search functionalities including retrieving general queries, news, and articles with advanced filtering options.",
      "stars": 247,
      "forks": 48,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-10-03T04:28:35Z",
      "readme_content": "# SearXNG MCP Server\n\nAn [MCP server](https://modelcontextprotocol.io/introduction) implementation that integrates the [SearXNG](https://docs.searxng.org) API, providing web search capabilities.\n\n[![https://nodei.co/npm/mcp-searxng.png?downloads=true&downloadRank=true&stars=true](https://nodei.co/npm/mcp-searxng.png?downloads=true&downloadRank=true&stars=true)](https://www.npmjs.com/package/mcp-searxng)\n\n[![https://badgen.net/docker/pulls/isokoliuk/mcp-searxng](https://badgen.net/docker/pulls/isokoliuk/mcp-searxng)](https://hub.docker.com/r/isokoliuk/mcp-searxng)\n\n<a href=\"https://glama.ai/mcp/servers/0j7jjyt7m9\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/0j7jjyt7m9/badge\" alt=\"SearXNG Server MCP server\" /></a>\n\n## Features\n\n- **Web Search**: General queries, news, articles, with pagination.\n- **URL Content Reading**: Advanced content extraction with pagination, section filtering, and heading extraction.\n- **Intelligent Caching**: URL content is cached with TTL (Time-To-Live) to improve performance and reduce redundant requests.\n- **Pagination**: Control which page of results to retrieve.\n- **Time Filtering**: Filter results by time range (day, month, year).\n- **Language Selection**: Filter results by preferred language.\n- **Safe Search**: Control content filtering level for search results.\n\n## Tools\n\n- **searxng_web_search**\n  - Execute web searches with pagination\n  - Inputs:\n    - `query` (string): The search query. This string is passed to external search services.\n    - `pageno` (number, optional): Search page number, starts at 1 (default 1)\n    - `time_range` (string, optional): Filter results by time range - one of: \"day\", \"month\", \"year\" (default: none)\n    - `language` (string, optional): Language code for results (e.g., \"en\", \"fr\", \"de\") or \"all\" (default: \"all\")\n    - `safesearch` (number, optional): Safe search filter level (0: None, 1: Moderate, 2: Strict) (default: instance setting)\n\n- **web_url_read**\n  - Read and convert the content from a URL to markdown with advanced content extraction options\n  - Inputs:\n    - `url` (string): The URL to fetch and process\n    - `startChar` (number, optional): Starting character position for content extraction (default: 0)\n    - `maxLength` (number, optional): Maximum number of characters to return\n    - `section` (string, optional): Extract content under a specific heading (searches for heading text)\n    - `paragraphRange` (string, optional): Return specific paragraph ranges (e.g., '1-5', '3', '10-')\n    - `readHeadings` (boolean, optional): Return only a list of headings instead of full content\n\n## Configuration\n\n### Setting the SEARXNG_URL\n\nThe `SEARXNG_URL` environment variable defines which SearxNG instance to connect to.\n\n#### Environment Variable Format\n```bash\nSEARXNG_URL=<protocol>://<hostname>[:<port>]\n```\n\n#### Examples\n```bash\n# Local development (default)\nSEARXNG_URL=http://localhost:8080\n\n# Public instance\nSEARXNG_URL=https://search.example.com\n\n# Custom port\nSEARXNG_URL=http://my-searxng.local:3000\n```\n\n#### Setup Instructions\n1. Choose a SearxNG instance from the [list of public instances](https://searx.space/) or use your local environment\n2. Set the `SEARXNG_URL` environment variable to the complete instance URL\n3. If not specified, the default value `http://localhost:8080` will be used\n\n### Using Authentication (Optional)\n\nIf you are using a password protected SearxNG instance you can set a username and password for HTTP Basic Auth:\n\n- Set the `AUTH_USERNAME` environment variable to your username\n- Set the `AUTH_PASSWORD` environment variable to your password\n\n**Note:** Authentication is only required for password-protected SearxNG instances. See the usage examples below for how to configure authentication with different installation methods.\n\n### Proxy Support (Optional)\n\nThe server supports HTTP and HTTPS proxies through environment variables. This is useful when running behind corporate firewalls or when you need to route traffic through a specific proxy server.\n\n#### Proxy Environment Variables\n\nSet one or more of these environment variables to configure proxy support:\n\n- `HTTP_PROXY`: Proxy URL for HTTP requests\n- `HTTPS_PROXY`: Proxy URL for HTTPS requests  \n- `http_proxy`: Alternative lowercase version for HTTP requests\n- `https_proxy`: Alternative lowercase version for HTTPS requests\n\n#### Proxy URL Formats\n\nThe proxy URL can be in any of these formats:\n\n```bash\n# Basic proxy\nexport HTTP_PROXY=http://proxy.company.com:8080\nexport HTTPS_PROXY=http://proxy.company.com:8080\n\n# Proxy with authentication\nexport HTTP_PROXY=http://username:password@proxy.company.com:8080\nexport HTTPS_PROXY=http://username:password@proxy.company.com:8080\n```\n\n**Note:** If no proxy environment variables are set, the server will make direct connections as normal. See the usage examples below for how to configure proxy settings with different installation methods.\n\n### [NPX](https://www.npmjs.com/package/mcp-searxng)\n\n```json\n{\n  \"mcpServers\": {\n    \"searxng\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"mcp-searxng\"],\n      \"env\": {\n        \"SEARXNG_URL\": \"YOUR_SEARXNG_INSTANCE_URL\"\n      }\n    }\n  }\n}\n```\n\n<details>\n<summary>Additional NPX Configuration Options</summary>\n\n#### With Authentication\n```json\n{\n  \"mcpServers\": {\n    \"searxng\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"mcp-searxng\"],\n      \"env\": {\n        \"SEARXNG_URL\": \"YOUR_SEARXNG_INSTANCE_URL\",\n        \"AUTH_USERNAME\": \"your_username\",\n        \"AUTH_PASSWORD\": \"your_password\"\n      }\n    }\n  }\n}\n```\n\n#### With Proxy Support\n```json\n{\n  \"mcpServers\": {\n    \"searxng\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"mcp-searxng\"],\n      \"env\": {\n        \"SEARXNG_URL\": \"YOUR_SEARXNG_INSTANCE_URL\",\n        \"HTTP_PROXY\": \"http://proxy.company.com:8080\",\n        \"HTTPS_PROXY\": \"http://proxy.company.com:8080\"\n      }\n    }\n  }\n}\n```\n\n#### With Authentication and Proxy Support\n```json\n{\n  \"mcpServers\": {\n    \"searxng\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"mcp-searxng\"],\n      \"env\": {\n        \"SEARXNG_URL\": \"YOUR_SEARXNG_INSTANCE_URL\",\n        \"AUTH_USERNAME\": \"your_username\",\n        \"AUTH_PASSWORD\": \"your_password\",\n        \"HTTP_PROXY\": \"http://proxy.company.com:8080\",\n        \"HTTPS_PROXY\": \"http://proxy.company.com:8080\"\n      }\n    }\n  }\n}\n```\n\n</details>\n\n### [NPM](https://www.npmjs.com/package/mcp-searxng)\n\n```bash\nnpm install -g mcp-searxng\n```\n\n```json\n{\n  \"mcpServers\": {\n    \"searxng\": {\n      \"command\": \"mcp-searxng\",\n      \"env\": {\n        \"SEARXNG_URL\": \"YOUR_SEARXNG_INSTANCE_URL\"\n      }\n    }\n  }\n}\n```\n\n<details>\n<summary>Additional NPM Configuration Options</summary>\n\n#### With Authentication\n```json\n{\n  \"mcpServers\": {\n    \"searxng\": {\n      \"command\": \"mcp-searxng\",\n      \"env\": {\n        \"SEARXNG_URL\": \"YOUR_SEARXNG_INSTANCE_URL\",\n        \"AUTH_USERNAME\": \"your_username\",\n        \"AUTH_PASSWORD\": \"your_password\"\n      }\n    }\n  }\n}\n```\n\n#### With Proxy Support\n```json\n{\n  \"mcpServers\": {\n    \"searxng\": {\n      \"command\": \"mcp-searxng\",\n      \"env\": {\n        \"SEARXNG_URL\": \"YOUR_SEARXNG_INSTANCE_URL\",\n        \"HTTP_PROXY\": \"http://proxy.company.com:8080\",\n        \"HTTPS_PROXY\": \"http://proxy.company.com:8080\"\n      }\n    }\n  }\n}\n```\n\n#### With Authentication and Proxy Support\n```json\n{\n  \"mcpServers\": {\n    \"searxng\": {\n      \"command\": \"mcp-searxng\",\n      \"env\": {\n        \"SEARXNG_URL\": \"YOUR_SEARXNG_INSTANCE_URL\",\n        \"AUTH_USERNAME\": \"your_username\",\n        \"AUTH_PASSWORD\": \"your_password\",\n        \"HTTP_PROXY\": \"http://proxy.company.com:8080\",\n        \"HTTPS_PROXY\": \"http://proxy.company.com:8080\"\n      }\n    }\n  }\n}\n```\n\n</details>\n\n### Docker\n\n#### Using [Pre-built Image from Docker Hub](https://hub.docker.com/r/isokoliuk/mcp-searxng)\n\n```bash\ndocker pull isokoliuk/mcp-searxng:latest\n```\n\n```json\n{\n  \"mcpServers\": {\n    \"searxng\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\", \"-i\", \"--rm\",\n        \"-e\", \"SEARXNG_URL\",\n        \"isokoliuk/mcp-searxng:latest\"\n      ],\n      \"env\": {\n        \"SEARXNG_URL\": \"YOUR_SEARXNG_INSTANCE_URL\"\n      }\n    }\n  }\n}\n```\n\n<details>\n<summary>Additional Docker Configuration Options</summary>\n\n#### With Authentication\n```json\n{\n  \"mcpServers\": {\n    \"searxng\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\", \"-i\", \"--rm\",\n        \"-e\", \"SEARXNG_URL\",\n        \"-e\", \"AUTH_USERNAME\",\n        \"-e\", \"AUTH_PASSWORD\",\n        \"isokoliuk/mcp-searxng:latest\"\n      ],\n      \"env\": {\n        \"SEARXNG_URL\": \"YOUR_SEARXNG_INSTANCE_URL\",\n        \"AUTH_USERNAME\": \"your_username\",\n        \"AUTH_PASSWORD\": \"your_password\"\n      }\n    }\n  }\n}\n```\n\n#### With Proxy Support\n```json\n{\n  \"mcpServers\": {\n    \"searxng\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\", \"-i\", \"--rm\",\n        \"-e\", \"SEARXNG_URL\",\n        \"-e\", \"HTTP_PROXY\",\n        \"-e\", \"HTTPS_PROXY\",\n        \"isokoliuk/mcp-searxng:latest\"\n      ],\n      \"env\": {\n        \"SEARXNG_URL\": \"YOUR_SEARXNG_INSTANCE_URL\",\n        \"HTTP_PROXY\": \"http://proxy.company.com:8080\",\n        \"HTTPS_PROXY\": \"http://proxy.company.com:8080\"\n      }\n    }\n  }\n}\n```\n\n#### With Authentication and Proxy Support\n```json\n{\n  \"mcpServers\": {\n    \"searxng\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\", \"-i\", \"--rm\",\n        \"-e\", \"SEARXNG_URL\",\n        \"-e\", \"AUTH_USERNAME\",\n        \"-e\", \"AUTH_PASSWORD\",\n        \"-e\", \"HTTP_PROXY\",\n        \"-e\", \"HTTPS_PROXY\",\n        \"isokoliuk/mcp-searxng:latest\"\n      ],\n      \"env\": {\n        \"SEARXNG_URL\": \"YOUR_SEARXNG_INSTANCE_URL\",\n        \"AUTH_USERNAME\": \"your_username\",\n        \"AUTH_PASSWORD\": \"your_password\",\n        \"HTTP_PROXY\": \"http://proxy.company.com:8080\",\n        \"HTTPS_PROXY\": \"http://proxy.company.com:8080\"\n      }\n    }\n  }\n}\n```\n\n</details>\n\n#### Build Locally\n\n```bash\ndocker build -t mcp-searxng:latest -f Dockerfile .\n```\n\n```json\n{\n  \"mcpServers\": {\n    \"searxng\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\", \"-i\", \"--rm\",\n        \"-e\", \"SEARXNG_URL\",\n        \"mcp-searxng:latest\"\n      ],\n      \"env\": {\n        \"SEARXNG_URL\": \"YOUR_SEARXNG_INSTANCE_URL\"\n      }\n    }\n  }\n}\n```\n\n<details>\n<summary>Additional Build Locally Configuration Options</summary>\n\n#### With Authentication\n```json\n{\n  \"mcpServers\": {\n    \"searxng\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\", \"-i\", \"--rm\",\n        \"-e\", \"SEARXNG_URL\",\n        \"-e\", \"AUTH_USERNAME\",\n        \"-e\", \"AUTH_PASSWORD\",\n        \"mcp-searxng:latest\"\n      ],\n      \"env\": {\n        \"SEARXNG_URL\": \"YOUR_SEARXNG_INSTANCE_URL\",\n        \"AUTH_USERNAME\": \"your_username\",\n        \"AUTH_PASSWORD\": \"your_password\"\n      }\n    }\n  }\n}\n```\n\n#### With Proxy Support\n```json\n{\n  \"mcpServers\": {\n    \"searxng\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\", \"-i\", \"--rm\",\n        \"-e\", \"SEARXNG_URL\",\n        \"-e\", \"HTTP_PROXY\",\n        \"-e\", \"HTTPS_PROXY\",\n        \"mcp-searxng:latest\"\n      ],\n      \"env\": {\n        \"SEARXNG_URL\": \"YOUR_SEARXNG_INSTANCE_URL\",\n        \"HTTP_PROXY\": \"http://proxy.company.com:8080\",\n        \"HTTPS_PROXY\": \"http://proxy.company.com:8080\"\n      }\n    }\n  }\n}\n```\n\n#### With Authentication and Proxy Support\n```json\n{\n  \"mcpServers\": {\n    \"searxng\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\", \"-i\", \"--rm\",\n        \"-e\", \"SEARXNG_URL\",\n        \"-e\", \"AUTH_USERNAME\",\n        \"-e\", \"AUTH_PASSWORD\",\n        \"-e\", \"HTTP_PROXY\",\n        \"-e\", \"HTTPS_PROXY\",\n        \"mcp-searxng:latest\"\n      ],\n      \"env\": {\n        \"SEARXNG_URL\": \"YOUR_SEARXNG_INSTANCE_URL\",\n        \"AUTH_USERNAME\": \"your_username\",\n        \"AUTH_PASSWORD\": \"your_password\",\n        \"HTTP_PROXY\": \"http://proxy.company.com:8080\",\n        \"HTTPS_PROXY\": \"http://proxy.company.com:8080\"\n      }\n    }\n  }\n}\n```\n\n</details>\n\n#### Docker Compose\n\nCreate a `docker-compose.yml` file:\n\n```yaml\nservices:\n  mcp-searxng:\n    image: isokoliuk/mcp-searxng:latest\n    stdin_open: true\n    environment:\n      - SEARXNG_URL=YOUR_SEARXNG_INSTANCE_URL\n```\n\nThen configure your MCP client:\n\n```json\n{\n  \"mcpServers\": {\n    \"searxng\": {\n      \"command\": \"docker-compose\",\n      \"args\": [\"run\", \"--rm\", \"mcp-searxng\"]\n    }\n  }\n}\n```\n\n<details>\n<summary>Additional Docker Compose Configuration Options</summary>\n\n#### With Authentication\n```yaml\nservices:\n  mcp-searxng:\n    image: isokoliuk/mcp-searxng:latest\n    stdin_open: true\n    environment:\n      - SEARXNG_URL=YOUR_SEARXNG_INSTANCE_URL\n      - AUTH_USERNAME=your_username\n      - AUTH_PASSWORD=your_password\n```\n\n#### With Proxy Support\n```yaml\nservices:\n  mcp-searxng:\n    image: isokoliuk/mcp-searxng:latest\n    stdin_open: true\n    environment:\n      - SEARXNG_URL=YOUR_SEARXNG_INSTANCE_URL\n      - HTTP_PROXY=http://proxy.company.com:8080\n      - HTTPS_PROXY=http://proxy.company.com:8080\n```\n\n#### With Authentication and Proxy Support\n```yaml\nservices:\n  mcp-searxng:\n    image: isokoliuk/mcp-searxng:latest\n    stdin_open: true\n    environment:\n      - SEARXNG_URL=YOUR_SEARXNG_INSTANCE_URL\n      - AUTH_USERNAME=your_username\n      - AUTH_PASSWORD=your_password\n      - HTTP_PROXY=http://proxy.company.com:8080\n      - HTTPS_PROXY=http://proxy.company.com:8080\n```\n\n#### Using Local Build\n```yaml\nservices:\n  mcp-searxng:\n    build: .\n    stdin_open: true\n    environment:\n      - SEARXNG_URL=YOUR_SEARXNG_INSTANCE_URL\n```\n\n</details>\n\n### HTTP Transport (Optional)\n\nThe server supports both STDIO (default) and HTTP transports:\n\n#### STDIO Transport (Default)\n- **Best for**: Claude Desktop and most MCP clients\n- **Usage**: Automatic - no additional configuration needed\n\n#### HTTP Transport  \n- **Best for**: Web-based applications and remote MCP clients\n- **Usage**: Set the `MCP_HTTP_PORT` environment variable\n\n```json\n{\n  \"mcpServers\": {\n    \"searxng-http\": {\n      \"command\": \"mcp-searxng\",\n      \"env\": {\n        \"SEARXNG_URL\": \"YOUR_SEARXNG_INSTANCE_URL\",\n        \"MCP_HTTP_PORT\": \"3000\"\n      }\n    }\n  }\n}\n```\n\n<details>\n<summary>Additional HTTP Transport Configuration Options</summary>\n\n#### HTTP Server with Authentication\n```json\n{\n  \"mcpServers\": {\n    \"searxng-http\": {\n      \"command\": \"mcp-searxng\",\n      \"env\": {\n        \"SEARXNG_URL\": \"YOUR_SEARXNG_INSTANCE_URL\",\n        \"MCP_HTTP_PORT\": \"3000\",\n        \"AUTH_USERNAME\": \"your_username\",\n        \"AUTH_PASSWORD\": \"your_password\"\n      }\n    }\n  }\n}\n```\n\n#### HTTP Server with Proxy Support\n```json\n{\n  \"mcpServers\": {\n    \"searxng-http\": {\n      \"command\": \"mcp-searxng\",\n      \"env\": {\n        \"SEARXNG_URL\": \"YOUR_SEARXNG_INSTANCE_URL\",\n        \"MCP_HTTP_PORT\": \"3000\",\n        \"HTTP_PROXY\": \"http://proxy.company.com:8080\",\n        \"HTTPS_PROXY\": \"http://proxy.company.com:8080\"\n      }\n    }\n  }\n}\n```\n\n#### HTTP Server with Authentication and Proxy Support\n```json\n{\n  \"mcpServers\": {\n    \"searxng-http\": {\n      \"command\": \"mcp-searxng\",\n      \"env\": {\n        \"SEARXNG_URL\": \"YOUR_SEARXNG_INSTANCE_URL\",\n        \"MCP_HTTP_PORT\": \"3000\",\n        \"AUTH_USERNAME\": \"your_username\",\n        \"AUTH_PASSWORD\": \"your_password\",\n        \"HTTP_PROXY\": \"http://proxy.company.com:8080\",\n        \"HTTPS_PROXY\": \"http://proxy.company.com:8080\"\n      }\n    }\n  }\n}\n```\n\n</details>\n\n**HTTP Endpoints:**\n- **MCP Protocol**: `POST/GET/DELETE /mcp` \n- **Health Check**: `GET /health`\n- **CORS**: Enabled for web clients\n\n**Testing HTTP Server:**\n```bash\n# Start HTTP server\nMCP_HTTP_PORT=3000 SEARXNG_URL=http://localhost:8080 mcp-searxng\n\n# Check health\ncurl http://localhost:3000/health\n```\n\n## Running evals\n\nThe evals package loads an mcp client that then runs the src/index.ts file, so there is no need to rebuild between tests. You can see the full documentation [here](https://www.mcpevals.io/docs).\n\n```bash\nSEARXNG_URL=SEARXNG_URL OPENAI_API_KEY=your-key npx mcp-eval evals.ts src/index.ts\n```\n\n## For Developers\n\n### Contributing to the Project\n\nWe welcome contributions! Here's how to get started:\n\n#### 0. Coding Guidelines\n\n- Use TypeScript for type safety\n- Follow existing error handling patterns\n- Keep error messages concise but informative\n- Write unit tests for new functionality\n- Ensure all tests pass before submitting PRs\n- Maintain test coverage above 90%\n- Test changes with the MCP inspector\n- Run evals before submitting PRs\n\n#### 1. Fork and Clone\n\n```bash\n# Fork the repository on GitHub, then clone your fork\ngit clone https://github.com/YOUR_USERNAME/mcp-searxng.git\ncd mcp-searxng\n\n# Add the original repository as upstream\ngit remote add upstream https://github.com/ihor-sokoliuk/mcp-searxng.git\n```\n\n#### 2. Development Setup\n\n```bash\n# Install dependencies\nnpm install\n\n# Start development with file watching\nnpm run watch\n```\n\n#### 3. Development Workflow\n\n1. **Create a feature branch:**\n   ```bash\n   git checkout -b feature/your-feature-name\n   ```\n\n2. **Make your changes** in `src/` directory\n   - Main server logic: `src/index.ts`\n   - Error handling: `src/error-handler.ts`\n\n3. **Build and test:**\n   ```bash\n   npm run build               # Build the project\n   npm test                     # Run unit tests\n   npm run test:coverage       # Run tests with coverage report\n   npm run inspector            # Run MCP inspector\n   ```\n\n4. **Run evals to ensure functionality:**\n   ```bash\n   SEARXNG_URL=http://localhost:8080 OPENAI_API_KEY=your-key npx mcp-eval evals.ts src/index.ts\n   ```\n\n#### 4. Submitting Changes\n\n```bash\n# Commit your changes\ngit add .\ngit commit -m \"feat: description of your changes\"\n\n# Push to your fork\ngit push origin feature/your-feature-name\n\n# Create a Pull Request on GitHub\n```\n\n### Testing\n\nThe project includes comprehensive unit tests with excellent coverage.\n\n#### Running Tests\n\n```bash\n# Run all tests\nnpm test\n\n# Run with coverage reporting\nnpm run test:coverage\n\n# Watch mode for development\nnpm run test:watch\n```\n\n#### Test Statistics\n- **Unit tests** covering all core modules\n- **100% success rate** with dynamic coverage reporting via c8\n- **HTML coverage reports** generated in `coverage/` directory\n\n#### What's Tested\n- Error handling (network, server, configuration errors)\n- Type validation and schema guards\n- Proxy configurations and environment variables\n- Resource generation and logging functionality\n- All module imports and function availability\n\n## License\n\nThis MCP server is licensed under the MIT License. This means you are free to use, modify, and distribute the software, subject to the terms and conditions of the MIT License. For more details, please see the LICENSE file in the project repository.\n",
      "npm_url": "https://www.npmjs.com/package/mcp-searxng",
      "npm_downloads": 27918,
      "keywords": [
        "searxng",
        "search",
        "filtering",
        "searxng api",
        "integrates searxng",
        "searxng integrates"
      ],
      "category": "web-search"
    },
    "ilyazub--serpapi-mcp-server": {
      "owner": "ilyazub",
      "name": "serpapi-mcp-server",
      "url": "https://github.com/ilyazub/serpapi-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/ilyazub.webp",
      "description": "Retrieve and parse search engine results from multiple platforms such as Google and Bing, allowing access to both live and archived data. Integrate with MCP clients for enhanced search functionalities.",
      "stars": 16,
      "forks": 10,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-10-01T07:17:53Z",
      "readme_content": "# SerpApi MCP Server\n\n[![Build](https://github.com/ilyazub/serpapi-mcp-server/actions/workflows/python-package.yml/badge.svg)](https://github.com/ilyazub/serpapi-mcp-server/actions/workflows/python-package.yml)\n\nBuild an MCP server that:\n\n- Get parsed search engines results pages via SerpApi using an API key, *fast*\n\nThis MCP (Model Context Protocol) server integrates with [SerpApi](https://serpapi.com) to perform searches across various search engines and retrieve both live and archived results. It exposes tools and resources for seamless interaction with MCP clients or hosts, such as Grok or Claude for Desktop.\n\n---\n\n## Installation\n\nTo set up the SerpApi MCP server, install the required Python libraries:\n\n```bash\npip install mcp serpapi python-dotenv\n```\n\nYou’ll also need a [SerpApi API key](https://serpapi.com/manage-api-key). Sign up at SerpApi to get one.\n\n## Quick Start\n\n1. Save the Server Code: Place the server code in a file, e.g., server.py.\n\n2. Configure the API Key: Create a .env file in the same directory with your SerpApi API key:\n```plaintext\nSERPAPI_API_KEY=your_api_key_here\n```\n\n3. Run the Server: Start the server with:\n\n```bash\npython server.py\n```\n\n4. Integrate with an MCP Client: Connect the server to an MCP client or host (e.g., Claude for Desktop). For Claude, update Claude_desktop_config.json:\n\n```json\n{\n  \"mcpServers\": {\n    \"serpapi\": {\n      \"command\": \"python\",\n      \"args\": [\"path/to/server.py\"]\n    }\n  }\n}\n```\n\nRestart the client to load the server.\n\n## Features\n- Supported Engines: Google, Google Light, Bing, Walmart, Yahoo, eBay, YouTube, DuckDuckGo, Yandex, Baidu\n\n- **Tools**:\n* search: Perform a search on a specified engine with a query and optional parameters.\n\n\n- **Resources**:\n* locations: Find Google Locations.\n\n## Usage Examples\n\nThese examples assume an MCP client (e.g., written in Python using the MCP client SDK) is connected to the server.\nListing Supported Engines\nRetrieve the list of supported search engines:\n\n```python\n\nengines = await session.read_resource(\"locations\")\nprint(engines)\n```\n\nPerforming a Search\nSearch for \"coffee\" on Google with a location filter:\n\n```python\n\nresult = await session.call_tool(\"search\", {\n    \"query\": \"coffee\",\n    \"engine\": \"google\",\n    \"location\": \"Austin, TX\"\n})\n```\nprint(result)\n\n## Configuration\nAPI Key: Set your SerpApi API key in the `.env` file as `SERPAPI_API_KEY`.\n\n### Running the Server\n\nProduction Mode: Launch the server with:\n```bash\n\npython server.py\n```\n\nDevelopment Mode: Use the MCP Inspector for debugging:\n\n```bash\nmcp dev server.py\n```\n\n## Testing\n\nTest the server using the MCP Inspector or an MCP client. For Claude for Desktop, configure the server in `Claude_desktop_config.json`, restart the app, and use the hammer icon to explore and test available tools.\n\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "bing",
        "search",
        "serpapi",
        "search engine",
        "google bing",
        "web search"
      ],
      "category": "web-search"
    },
    "imprvhub--mcp-claude-spotify": {
      "owner": "imprvhub",
      "name": "mcp-claude-spotify",
      "url": "https://github.com/imprvhub/mcp-claude-spotify",
      "imageUrl": "/freedevtools/mcp/pfp/imprvhub.webp",
      "description": "Integrate with Spotify to perform actions such as authentication, searching for music, controlling playback, managing playlists, and providing personalized music recommendations. Access and manage your Spotify library effortlessly from the LLM environment.",
      "stars": 16,
      "forks": 6,
      "license": "Mozilla Public License 2.0",
      "language": "TypeScript",
      "updated_at": "2025-09-14T00:35:02Z",
      "readme_content": "# MCP Claude Spotify\n[![Trust Score](https://archestra.ai/mcp-catalog/api/badge/quality/imprvhub/mcp-claude-spotify)](https://archestra.ai/mcp-catalog/imprvhub__mcp-claude-spotify)\n[![Verified on MseeP](https://mseep.ai/badge.svg)](https://mseep.ai/app/99039f16-4abd-4af8-8873-ae2844e7dd65)\n[![smithery badge](https://smithery.ai/badge/@imprvhub/mcp-claude-spotify)](https://smithery.ai/server/@imprvhub/mcp-claude-spotify)\n\n<table style=\"border-collapse: collapse; width: 100%;\">\n<tr>\n<td style=\"padding: 15px; vertical-align: middle; border: none; text-align: center;\">\n  <a href=\"https://mseep.ai/app/imprvhub-mcp-claude-spotify\">\n    <img src=\"https://mseep.net/pr/imprvhub-mcp-claude-spotify-badge.png\" alt=\"MseeP.ai Security Assessment Badge\" />\n  </a>\n</td>  \n<td style=\"width: 50%; padding: 15px; vertical-align: middle; border: none;\">An integration that allows Claude Desktop to interact with Spotify using the Model Context Protocol (MCP).</td>\n<td style=\"width: 50%; padding: 0; vertical-align: middle; border: none;\"><a href=\"https://glama.ai/mcp/servers/@imprvhub/mcp-claude-spotify\"><img src=\"https://glama.ai/mcp/servers/@imprvhub/mcp-claude-spotify/badge\" alt=\"Claude Spotify MCP server\" style=\"max-width: 100%;\" /></a></td>\n</tr>\n</table>\n\n## Features\n\n- Spotify authentication\n- Search for tracks, albums, artists, and playlists\n- Playback control (play, pause, next, previous)\n- Create and manage playlists\n- Get personalized recommendations\n- Access user's top played tracks over different time periods\n\n## Demo\n\n<p>\n  <a href=\"https://www.youtube.com/watch?v=WNw5H9epZfc\">\n    \n  </a>\n</p>\n\n## Requirements\n\n- Node.js 16 or higher\n- Spotify account\n- Claude Desktop\n- Spotify API credentials (Client ID and Client Secret)\n\n## Installation\n\n### Installing via Smithery\n\nTo install MCP Claude Spotify for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@imprvhub/mcp-claude-spotify):\n\n```bash\nnpx -y @smithery/cli install @imprvhub/mcp-claude-spotify --client claude\n```\n\n### Installing Manually\n1. Clone or download this repository:\n```bash\ngit clone https://github.com/imprvhub/mcp-claude-spotify\ncd claude-spotify-mcp\n```\n\n2. Install dependencies:\n```bash\nnpm install\n```\n\n3. Build the project (if you want to modify the source code):\n```bash\nnpm run build\n```\n\nThe repository already includes pre-built files in the `build` directory, so you can skip step 3 if you don't plan to modify the source code.\n\n## Setting up Spotify Credentials\n\nTo use this MCP, you need to obtain Spotify API credentials:\n\n1. Go to [Spotify Developer Dashboard](https://developer.spotify.com/dashboard)\n2. Log in with your Spotify account\n3. Click \"Create App\"\n4. Fill in your app information:\n   - App name: \"MCP Claude Spotify\" (or whatever you prefer)\n   - App description: \"Spotify integration for Claude Desktop\"\n   - Website: You can leave this blank or put any URL\n   - Redirect URI: **Important** - Add `http://127.0.0.1:8888/callback`\n5. Accept the terms and conditions and click \"Create\"\n6. In your app dashboard, you'll see the \"Client ID\"\n7. Click \"Show Client Secret\" to reveal your \"Client Secret\"\n\nSave these credentials as you'll need them for configuration.\n\n## Running the MCP Server\n\nThere are two ways to run the MCP server:\n\n### Option 1: Running manually (recommended for first-time setup and troubleshooting)\n\n1. Open a terminal or command prompt\n2. Navigate to the project directory\n3. Run the server directly:\n\n```bash\nnode build/index.js\n```\n\nKeep this terminal window open while using Claude Desktop. The server will run until you close the terminal.\n\n### Option 2: Auto-starting with Claude Desktop (recommended for regular use)\n\nThe Claude Desktop can automatically start the MCP server when needed. To set this up:\n\n#### Configuration\n\nThe Claude Desktop configuration file is located at:\n\n- **macOS**: `~/Library/Application Support/Claude/claude_desktop_config.json`\n- **Windows**: `%APPDATA%\\Claude\\claude_desktop_config.json`\n\nEdit this file to add the Spotify MCP configuration. If the file doesn't exist, create it:\n\n```json\n{\n  \"mcpServers\": {\n    \"spotify\": {\n      \"command\": \"node\",\n      \"args\": [\"ABSOLUTE_PATH_TO_DIRECTORY/mcp-claude-spotify/build/index.js\"],\n      \"env\": {\n        \"SPOTIFY_CLIENT_ID\": \"your_client_id_here\",\n        \"SPOTIFY_CLIENT_SECRET\": \"your_client_secret_here\"\n      }\n    }\n  }\n}\n```\n\n**Important**: Replace:\n- `ABSOLUTE_PATH_TO_DIRECTORY` with the **complete absolute path** where you installed the MCP\n  - macOS/Linux example: `/Users/username/mcp-claude-spotify`\n  - Windows example: `C:\\\\Users\\\\username\\\\mcp-claude-spotify`\n- `your_client_id_here` with the Client ID you obtained from Spotify\n- `your_client_secret_here` with the Client Secret you obtained from Spotify\n\nIf you already have other MCPs configured, simply add the \"spotify\" section inside the \"mcpServers\" object.\n\n#### Setting up auto-start scripts (Optional)\n\nFor a more reliable experience, you can set up auto-start scripts:\n\n<details>\n<summary><b>Windows auto-start instructions</b></summary>\n\n1. Create a file named `start-spotify-mcp.bat` in the project directory with the following content:\n```\n@echo off\ncd %~dp0\nnode build/index.js\n```\n\n2. Create a shortcut to this BAT file\n3. Press `Win+R`, type `shell:startup` and press Enter\n4. Move the shortcut to this folder to have it start with Windows\n</details>\n\n<details>\n<summary><b>macOS auto-start instructions</b></summary>\n\n1. Create a file named `com.spotify.mcp.plist` in `~/Library/LaunchAgents/` with the following content:\n```xml\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<!DOCTYPE plist PUBLIC \"-//Apple//DTD PLIST 1.0//EN\" \"http://www.apple.com/DTDs/PropertyList-1.0.dtd\">\n<plist version=\"1.0\">\n<dict>\n    <key>Label</key>\n    <string>com.spotify.mcp</string>\n    <key>ProgramArguments</key>\n    <array>\n        <string>/usr/local/bin/node</string>\n        <string>ABSOLUTE_PATH_TO_DIRECTORY/mcp-claude-spotify/build/index.js</string>\n    </array>\n    <key>RunAtLoad</key>\n    <true/>\n    <key>KeepAlive</key>\n    <true/>\n    <key>StandardErrorPath</key>\n    <string>/tmp/spotify-mcp.err</string>\n    <key>StandardOutPath</key>\n    <string>/tmp/spotify-mcp.out</string>\n    <key>EnvironmentVariables</key>\n    <dict>\n        <key>SPOTIFY_CLIENT_ID</key>\n        <string>your_client_id_here</string>\n        <key>SPOTIFY_CLIENT_SECRET</key>\n        <string>your_client_secret_here</string>\n    </dict>\n</dict>\n</plist>\n```\n\n2. Replace the path and credentials with your actual values\n3. Load the agent with: `launchctl load ~/Library/LaunchAgents/com.spotify.mcp.plist`\n</details>\n\n<details>\n<summary><b>Linux auto-start instructions</b></summary>\n\n1. Create a file named `spotify-mcp.service` in `~/.config/systemd/user/` (create the directory if it doesn't exist):\n```\n[Unit]\nDescription=Spotify MCP Server for Claude Desktop\nAfter=network.target\n\n[Service]\nType=simple\nExecStart=/usr/bin/node ABSOLUTE_PATH_TO_DIRECTORY/mcp-claude-spotify/build/index.js\nRestart=on-failure\nEnvironment=\"SPOTIFY_CLIENT_ID=your_client_id_here\"\nEnvironment=\"SPOTIFY_CLIENT_SECRET=your_client_secret_here\"\n\n[Install]\nWantedBy=default.target\n```\n\n2. Replace the path and credentials with your actual values\n3. Enable and start the service:\n```bash\nsystemctl --user enable spotify-mcp.service\nsystemctl --user start spotify-mcp.service\n```\n\n4. Check status with:\n```bash\nsystemctl --user status spotify-mcp.service\n```\n</details>\n\n## Usage\n\n1. Restart Claude Desktop after modifying the configuration\n2. In Claude, use the `auth-spotify` command to start the authentication process\n3. A browser window will open for you to authorize the application\n4. Log in with your Spotify account and authorize the application\n5. **Important**: After successful authentication, restart Claude Desktop to properly initialize the MCP's tool registry and WebSocket session token cache\n6. After restarting, all Spotify MCP tools will be properly registered and available for use\n\nThe MCP server runs as a child process managed by Claude Desktop. When Claude is running, it automatically starts and manages the Node.js server process based on the configuration in `claude_desktop_config.json`.\n\n## Available Tools\n\n### auth-spotify\nInitiates the Spotify authentication process.\n\n### search-spotify\nSearches for tracks, albums, artists, or playlists.\n\n**Parameters:**\n- `query`: Search text\n- `type`: Type of search (track, album, artist, playlist)\n- `limit`: Number of results (1-50)\n\n### play-track\nPlays a specific track.\n\n**Parameters:**\n- `trackId`: Spotify track ID\n- `deviceId`: (Optional) Spotify device ID to play on\n\n### get-current-playback\nGets information about the current playback.\n\n### pause-playback\nPauses the playback.\n\n### next-track\nSkips to the next track.\n\n### previous-track\nReturns to the previous track.\n\n### get-user-playlists\nGets the user's playlists.\n\n### create-playlist\nCreates a new playlist.\n\n**Parameters:**\n- `name`: Playlist name\n- `description`: (Optional) Description\n- `public`: (Optional) Whether it's public or private\n\n### add-tracks-to-playlist\nAdds tracks to a playlist.\n\n**Parameters:**\n- `playlistId`: Playlist ID\n- `trackIds`: Array of track IDs\n\n### get-recommendations\nGets recommendations based on seeds.\n\n**Parameters:**\n- `seedTracks`: (Optional) Array of track IDs\n- `seedArtists`: (Optional) Array of artist IDs\n- `seedGenres`: (Optional) Array of genres\n- `limit`: (Optional) Number of recommendations (1-100)\n\n### get-top-tracks\nGets the user's most played tracks over a specified time range.\n\n**Parameters:**\n- `limit`: (Optional) Number of tracks to return (1-50, default: 20)\n- `offset`: (Optional) Index of the first track to return (default: 0)\n- `time_range`: (Optional) Time frame for calculating affinity:\n  - `short_term`: Approximately last 4 weeks\n  - `medium_term`: Approximately last 6 months (default)\n  - `long_term`: Several years of data\n\n## Troubleshooting\n\n### \"Server disconnected\" error\nIf you see the error \"MCP Spotify: Server disconnected\" in Claude Desktop:\n\n1. **Verify the server is running**:\n   - Open a terminal and manually run `node build/index.js` from the project directory\n   - If the server starts successfully, use Claude while keeping this terminal open\n\n2. **Check your configuration**:\n   - Ensure the absolute path in `claude_desktop_config.json` is correct for your system\n   - Double-check that you've used double backslashes (`\\\\`) for Windows paths\n   - Verify you're using the complete path from the root of your filesystem\n\n3. **Try the auto-start option**:\n   - Set up the auto-start script for your operating system as described in the \"Setting up auto-start scripts\" section\n   - This ensures the server is always running when you need it\n\n### Browser doesn't open automatically\nIf the browser doesn't open automatically during authentication, manually visit:\n`http://127.0.0.1:8888/login`\n\n### Authentication error\nMake sure you've correctly configured the redirect URI in your Spotify Developer dashboard:\n`http://127.0.0.1:8888/callback`\n\n### Server startup error\nVerify that:\n- Environment variables are correctly configured in your `claude_desktop_config.json` or launch script\n- Node.js is installed and compatible (v16+)\n- Required ports (8888) are available and not blocked by firewall\n- You have permission to run the script in the specified location\n\n### Tools not appearing in Claude\nIf the Spotify tools don't appear in Claude after authentication:\n- Make sure you've restarted Claude Desktop after successful authentication\n- Check the Claude Desktop logs for any MCP communication errors\n- Ensure the MCP server process is running (run it manually to confirm)\n- Verify that the MCP server is correctly registered in the Claude Desktop MCP registry\n\n### Checking if the server is running\nTo check if the server is running:\n\n- **Windows**: Open Task Manager, go to the \"Details\" tab, and look for \"node.exe\"\n- **macOS/Linux**: Open Terminal and run `ps aux | grep node`\n\nIf you don't see the server running, start it manually or use the auto-start method.\n\n## Testing\n\nThis project includes automated tests to ensure code quality and functionality. The test suite uses Jest with TypeScript support and covers:\n\n- Zod schema validation - verifies all input schemas correctly validate data\n- Spotify API interactions - tests API request handling and error handling\n- MCP server functionality - ensures proper registration and execution of tools\n\n### Running Tests\n\nFirst, make sure all development dependencies are installed:\n\n```bash\nnpm install\n```\n\nTo run all tests:\n\n```bash\nnpm test\n```\n\nTo run a specific test file:\n\n```bash\nnpm test -- --testMatch=\"**/tests/schemas.test.ts\"\n```\n\nIf you encounter issues with ESM modules, make sure you're using Node.js v16 or higher and that the NODE_OPTIONS environment variable includes the `--experimental-vm-modules` flag as configured in the package.json.\n\n### Test Structure\n\n- `tests/schemas.test.ts`: Tests for input validation schemas\n- `tests/spotify-api.test.ts`: Tests for Spotify API interactions\n- `tests/server.test.ts`: Tests for MCP server functionality\n\n### Adding New Tests\n\nWhen adding new functionality, please include corresponding tests:\n\n1. For new schemas, add validation tests in `schemas.test.ts`\n2. For Spotify API functions, add tests in `spotify-api.test.ts` \n3. For MCP tools, add tests in `server.test.ts`\n\nAll tests should be written using Jest and the ESM module format with TypeScript.\n\n## Security Notes\n\n- Never share your Client ID and Client Secret\n- Access token is now stored in the user's home directory at `~/.spotify-mcp/tokens.json` to enable persistence between sessions and multiple instances\n- No user data is stored on disk\n\n### Revoking Application Access\n\nFor security reasons, you may want to revoke the application's access to your Spotify account when:\n- You no longer use this integration\n- You suspect unauthorized access\n- You're troubleshooting authentication issues\n\nTo revoke access:\n\n1. Go to your [Spotify Account page](https://www.spotify.com/account)\n2. Navigate to \"Apps\" in the menu\n3. Find \"MCP Claude Spotify\" (or the name you chose for your app)\n4. Click \"REMOVE ACCESS\"\n\nThis immediately invalidates all access and refresh tokens. The next time you use the `auth-spotify` command, you'll need to authorize the application again.\n\n## Contributing\n\nContributions are welcome! Here are some guidelines to follow:\n\n### Development Workflow\n\n1. Fork the repository\n2. Create a feature branch (`git checkout -b feature/amazing-feature`)\n3. Make your changes\n4. Run tests to ensure they pass (`npm test`)\n5. Commit your changes (`git commit -m 'Add some amazing feature'`)\n6. Push to the branch (`git push origin feature/amazing-feature`)\n7. Open a Pull Request\n\n### Code Style Guidelines\n\nThis project follows these coding standards:\n\n- Use TypeScript with strict type checking\n- Follow ESM module format\n- Use 2 spaces for indentation\n- Use camelCase for variables and functions\n- Use PascalCase for classes and interfaces\n- Document functions with JSDoc comments\n- Keep line length under 100 characters\n\n### Project Structure\n\nThe project follows this structure:\n\n```\nmcp-claude-spotify/\n├── src/               # Source code\n├── build/             # Compiled JavaScript\n├── tests/             # Test files\n├── public/            # Public assets\n└── ...\n```\n\n### Pull Request Process\n\n1. Ensure your code follows the style guidelines\n2. Update documentation if needed\n3. Add tests for new functionality\n4. Make sure all tests pass\n5. Your PR will be reviewed by maintainers\n\n## Related Links\n\n- [Model Context Protocol](https://modelcontextprotocol.io/)\n- [Spotify Web API Documentation](https://developer.spotify.com/documentation/web-api)\n- [Claude Desktop](https://claude.ai/download)\n- [MCP Series](https://github.com/mcp-series)\n\n## License\n\nThis project is licensed under the Mozilla Public License 2.0 - see the [LICENSE](https://github.com/imprvhub/mcp-claude-spotify/blob/main/LICENSE) file for details.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "spotify",
        "imprvhub",
        "playlists",
        "integrate spotify",
        "spotify integrate",
        "spotify library"
      ],
      "category": "web-search"
    },
    "imprvhub--mcp-rss-aggregator": {
      "owner": "imprvhub",
      "name": "mcp-rss-aggregator",
      "url": "https://github.com/imprvhub/mcp-rss-aggregator",
      "imageUrl": "/freedevtools/mcp/pfp/imprvhub.webp",
      "description": "Fetch and read content from RSS feeds, organize and filter them by categories, and import subscriptions via OPML. Retrieve well-formatted articles with titles, snippets, and links using simple commands or natural language queries.",
      "stars": 10,
      "forks": 6,
      "license": "Mozilla Public License 2.0",
      "language": "TypeScript",
      "updated_at": "2025-09-27T11:11:34Z",
      "readme_content": "# MCP RSS Aggregator\n[![Trust Score](https://archestra.ai/mcp-catalog/api/badge/quality/imprvhub/mcp-rss-aggregator)](https://archestra.ai/mcp-catalog/imprvhub__mcp-rss-aggregator)\n[![Verified on MseeP](https://mseep.ai/badge.svg)](https://mseep.ai/app/51854dcf-6cb2-4bd0-a37f-5b87ba25d7c7)\n[![smithery badge](https://smithery.ai/badge/@imprvhub/mcp-rss-aggregator)](https://smithery.ai/server/@imprvhub/mcp-rss-aggregator)\n\n<table style=\"border-collapse: collapse; width: 100%; table-layout: fixed;\">\n<tr>\n<td style=\"padding: 15px; vertical-align: middle; border: none; text-align: center;\">\n  <a href=\"https://mseep.ai/app/imprvhub-mcp-rss-aggregator\">\n    <img src=\"https://mseep.net/pr/imprvhub-mcp-rss-aggregator-badge.png\" alt=\"MseeP.ai Security Assessment Badge\" />\n  </a>\n</td>\n<td style=\"width: 40%; padding: 15px; vertical-align: middle; border: none;\">An integration that allows Claude Desktop to fetch and read content from your favorite RSS feeds using the Model Context Protocol (MCP).</td>\n<td style=\"width: 60%; padding: 0; vertical-align: middle; border: none; min-width: 300px; text-align: center;\"><a href=\"https://glama.ai/mcp/servers/@imprvhub/mcp-rss-aggregator\">\n  <img style=\"max-width: 100%; height: auto; min-width: 300px;\" src=\"https://glama.ai/mcp/servers/@imprvhub/mcp-rss-aggregator/badge\" alt=\"RSS Aggregator MCP server\" />\n</a></td>\n</tr>\n</table>\n\n## Features\n\n- Read articles from your favorite RSS feeds directly in Claude Desktop\n- Support for OPML files to import your existing feed subscriptions\n- Organize feeds by categories\n- Get the latest articles across all your feeds\n- Filter articles by feed source or category\n- Well-formatted article presentation with titles, snippets, and links\n\n## Demo\n\n<p>\n  <a href=\"https://youtu.be/9pvm078fHkQ\">\n    \n  </a>\n</p>\n\n<details>\n<summary> Timestamps </summary>\n\nClick on any timestamp to jump to that section of the video\n\n[00:00](https://youtu.be/9pvm078fHkQ&t=0s) - **Sample RSS Feed Demonstration**:\nUsing the default 'sample-feeds.opml' file included in the repository. This segment displays how Claude processes and presents news content from sources like TechCrunch, The Verge, and other technology publications through the MCP (Model Context Protocol).\n\n[01:05](https://youtu.be/9pvm078fHkQ&t=65s) - **Configuration File Editing Process**:\nStep-by-step walkthrough of accessing and modifying the claude_desktop_config.json file to change the OPML file path reference from the default sample to a customized 'my-feeds.opml' file.\n\n[01:15](https://youtu.be/9pvm078fHkQ&t=75s) - **Application Restart Procedure**:\nIllustrating the necessary step of closing and reopening the Claude Desktop application to properly load and apply the modified OPML file configuration changes.\n\n[01:25](https://youtu.be/9pvm078fHkQ&t=85s) - **Custom RSS Feed Results**:\nDemonstration of the results after implementing the custom OPML file. This section highlights the expanded and more diverse news sources now available through Claude Desktop, including Spanish-language content.\n</details>\n\n## Requirements\n\n- Node.js 16 or higher\n- Claude Desktop\n- Internet connection to access RSS feeds\n\n## Installation\n\n### Installing Manually\n1. Clone or download this repository:\n```bash\ngit clone https://github.com/imprvhub/mcp-rss-aggregator\ncd mcp-rss-aggregator\n```\n\n2. Install dependencies:\n```bash\nnpm install\n```\n\n3. Build the project:\n```bash\nnpm run build\n```\n\n## Feed Configuration\n\nThe RSS Aggregator supports both OPML and JSON formats for feed configuration.\n\n### Using OPML (Recommended)\n\nOPML (Outline Processor Markup Language) is a standard format used by most RSS readers to export and import feed subscriptions. \n\nA sample OPML file with popular feeds is included in the `public/sample-feeds.opml` file. You can:\n\n1. Use this file as-is\n2. Edit it to add your own feeds\n3. Replace it with an export from your existing RSS reader\n\nMost RSS readers allow you to export your subscriptions as an OPML file.\n\n### Using JSON\n\nAlternatively, you can define your feeds in a JSON file with the following format:\n\n```json\n[\n  {\n    \"title\": \"Hacker News\",\n    \"url\": \"https://news.ycombinator.com/rss\",\n    \"htmlUrl\": \"https://news.ycombinator.com/\",\n    \"category\": \"Tech News\"\n  },\n  {\n    \"title\": \"TechCrunch\",\n    \"url\": \"https://techcrunch.com/feed/\",\n    \"htmlUrl\": \"https://techcrunch.com/\",\n    \"category\": \"Tech News\"\n  }\n]\n```\n\n## Running the MCP Server\n\nThere are two ways to run the MCP server:\n\n### Option 1: Running manually\n\n1. Open a terminal or command prompt\n2. Navigate to the project directory\n3. Run the server directly:\n\n```bash\nnode build/index.js\n```\n\nKeep this terminal window open while using Claude Desktop. The server will run until you close the terminal.\n\n### Option 2: Auto-starting with Claude Desktop (recommended for regular use)\n\nThe Claude Desktop can automatically start the MCP server when needed. To set this up:\n\n#### Configuration\n\nThe Claude Desktop configuration file is located at:\n\n- **macOS**: `~/Library/Application Support/Claude/claude_desktop_config.json`\n- **Windows**: `%APPDATA%\\Claude\\claude_desktop_config.json`\n- **Linux**: `~/.config/Claude/claude_desktop_config.json`\n\nEdit this file to add the RSS Aggregator MCP configuration. If the file doesn't exist, create it:\n\n```json\n{\n  \"mcpServers\": {\n    \"rssAggregator\": {\n      \"command\": \"node\",\n      \"args\": [\"ABSOLUTE_PATH_TO_DIRECTORY/mcp-rss-aggregator/build/index.js\"],\n      \"feedsPath\": \"ABSOLUTE_PATH_TO_YOUR_FEEDS_FILE.opml\"\n    }\n  }\n}\n```\n\n**Important Notes**: \n- Replace `ABSOLUTE_PATH_TO_DIRECTORY` with the **complete absolute path** where you installed the MCP\n  - macOS/Linux example: `/Users/username/mcp-rss-aggregator`\n  - Windows example: `C:\\\\Users\\\\username\\\\mcp-rss-aggregator`\n- Replace `ABSOLUTE_PATH_TO_YOUR_FEEDS_FILE.opml` with the path to your OPML or JSON file\n  - If omitted, the sample feeds file will be used\n\nIf you already have other MCPs configured, simply add the \"rssAggregator\" section inside the \"mcpServers\" object:\n\n```json\n{\n  \"mcpServers\": {\n    \"otherMcp1\": {\n      \"command\": \"...\",\n      \"args\": [\"...\"]\n    },\n    \"rssAggregator\": {\n      \"command\": \"node\",\n      \"args\": [\n        \"ABSOLUTE_PATH_TO_DIRECTORY/mcp-rss-aggregator/build/index.js\"\n      ],\n      \"feedsPath\": \"ABSOLUTE_PATH_TO_YOUR_FEEDS_FILE.opml\"\n    }\n  }\n}\n```\n\nThe MCP server will automatically start when Claude Desktop needs it, based on the configuration in your `claude_desktop_config.json` file.\n\n## Usage\n\n1. Restart Claude Desktop after modifying the configuration\n2. In Claude, use the `rss` command to interact with the RSS Aggregator MCP Server\n3. The MCP server runs as a subprocess managed by Claude Desktop\n\n## Available Commands\n\nThe RSS Aggregator MCP provides a tool named `rss` with several commands:\n\n| Command | Description | Parameters | Example |\n|---------|-------------|------------|---------|\n| `latest` | Show latest articles from all feeds | Optional limit (--N) | `rss latest --20` |\n| `top` or `best` | Show top articles from all feeds | Optional limit (--N) | `rss top --15` |\n| `list` | List all available feeds | None | `rss list` |\n| `--[feed-id]` | Show articles from a specific feed | Optional limit (--N) | `rss --hackernews --10` |\n| `[category]` | Show articles from a specific category | Optional limit (--N) | `rss \"Tech News\" --20` |\n| `set-feeds-path --[path]` | Set path to OPML/JSON file | Path to file | `rss set-feeds-path --/path/to/feeds.opml` |\n\n## Example Usage\n\nHere are various examples of how to use the RSS Aggregator with Claude:\n\n### Direct Commands:\n\n```\nrss latest\nrss top --20\nrss list\nrss \"Tech News\"\nrss --hackernews\nrss --techcrunch --15\n```\n\n### Natural Language Queries:\n\nYou can also interact with the MCP using natural language. Claude will interpret these requests and use the appropriate commands:\n\n- \"What are the latest news on Hacker News?\"\n- \"Show me the top tech articles today\"\n- \"Fetch the latest articles from my programming feeds\"\n- \"List all my RSS feeds\"\n\n## Extended Usage Examples\n\n### Daily News Briefing\n\nGet your news briefing from all your sources:\n\n```\nrss latest --25\n```\n\nThis will fetch the 25 most recent articles across all your feeds, giving you a quick overview of the latest news.\n\n### Exploring Top Content\n\nFind the most important or popular articles:\n\n```\nrss top --20\n```\n\n### Category-Based Reading\n\nFocus on specific content categories:\n\n```\nrss \"Tech News\" --30\nrss \"Politics\" --15\nrss \"Science\" --10\n```\n\n### Source-Specific Updates\n\nRead updates from specific sources you follow:\n\n```\nrss --hackernews --20\nrss --nytimes\nrss --techcrunch --15\n```\n\n### Discover Your Available Feeds\n\nFind out what feeds you have configured:\n\n```\nrss list\n```\n\n### Combining Multiple Requests\n\nYou can make multiple sequential requests to build a comprehensive view:\n\n```\nrss \"Tech News\" --10\nrss \"Finance\" --10\nrss top --5\n```\n\n### Practical Workflows\n\n1. **Morning Routine**:\n   ```\n   rss top --10\n   rss \"News\" --5\n   ```\n\n2. **Industry Research**:\n   ```\n   rss \"Industry News\" --15\n   rss --bloomberg --5\n   ```\n\n3. **Tech Updates**:\n   ```\n   rss --hackernews --10\n   rss --techcrunch --5\n   ```\n\n### Working with Claude\n\nYou can ask Claude to analyze or summarize the articles:\n\n1. After running: `rss latest --10`\n   Ask: \"Can you summarize these articles?\"\n\n2. After running: `rss \"Tech News\" --15`\n   Ask: \"What are the key trends in these tech articles?\"\n\n3. After running: `rss --nytimes --washingtonpost --10`\n   Ask: \"Compare how these sources cover current events\"\n\n## Troubleshooting\n\n### \"Server disconnected\" error\nIf you see the error \"MCP RSS Aggregator: Server disconnected\" in Claude Desktop:\n\n1. **Verify the server is running**:\n   - Open a terminal and manually run `node build/index.js` from the project directory\n   - If the server starts successfully, use Claude while keeping this terminal open\n\n2. **Check your configuration**:\n   - Ensure the absolute path in `claude_desktop_config.json` is correct for your system\n   - Double-check that you've used double backslashes (`\\\\`) for Windows paths\n   - Verify you're using the complete path from the root of your filesystem\n\n### Tools not appearing in Claude\nIf the RSS Aggregator tools don't appear in Claude:\n- Make sure you've restarted Claude Desktop after configuration\n- Check the Claude Desktop logs for any MCP communication errors\n- Ensure the MCP server process is running (run it manually to confirm)\n\n### Feeds not loading\nIf your feeds aren't loading properly:\n- Make sure your OPML/JSON file is correctly formatted\n- Check if the `feedsPath` in your configuration is correct\n- Try running the server manually with a known good feeds file\n\n## Contributing\n\nContributions to improve the RSS Aggregator are welcome! Here are some ways you can contribute:\n\n1. Add support for more feed formats\n2. Improve feed parsing and error handling\n3. Add more visualization options for articles\n4. Improve categorization and filtering capabilities\n\n## License\n\nThis project is licensed under the Mozilla Public License 2.0 - see the [LICENSE](https://github.com/imprvhub/mcp-rss-aggregator/blob/main/LICENSE) file for details.\n\n## Related Links\n\n- [Model Context Protocol](https://modelcontextprotocol.io/)\n- [Claude Desktop](https://claude.ai/download)\n- [MCP Series](https://github.com/mcp-series)",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "feeds",
        "rss",
        "aggregator",
        "rss aggregator",
        "rss feeds",
        "content rss"
      ],
      "category": "web-search"
    },
    "instructa--mcp-youtube-music": {
      "owner": "instructa",
      "name": "mcp-youtube-music",
      "url": "https://github.com/instructa/mcp-youtube-music",
      "imageUrl": "/freedevtools/mcp/pfp/instructa.webp",
      "description": "Search for and play tracks on YouTube Music, enabling quick access to music via simple commands. Integrates music capabilities seamlessly into workflows.",
      "stars": 9,
      "forks": 2,
      "license": "No License",
      "language": "TypeScript",
      "updated_at": "2025-09-20T14:00:34Z",
      "readme_content": "# YouTube Music MCP 🎵\n\nThis is a simple MCP server that allows you to search for and play tracks on YouTube Music directly from your AI assistant like Cursor or Claude Desktop.\n\nBuilt with:\n\n- [YouTube Music](https://music.youtube.com/)\n- [Anthropic MCP](https://docs.anthropic.com/en/docs/agents-and-tools/mcp)\n- [Cursor](https://cursor.so/)\n\n## Available Tools\n\n- `searchTrack`: Search for tracks on YouTube Music by name.\n- `playTrack`: Play tracks directly by searching and opening them in your default browser.\n\n\n# Installation\n\n## 1. Get a key\n\nTo make this work you need a valid [Google Youtube API Key](https://console.cloud.google.com/marketplace/product/google/youtube.googleapis.com)\n\n## 2. Add to cursor\n\nAdd the following MCP configuration to your Cursor `.cursor/mcp.json` settings:\n\n```json\n{\n  \"mcpServers\": {\n    \"youtube-music-mcp\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@instructa/mcp-youtube-music\"],\n      \"env\": {\n        \"YOUTUBE_API_KEY\": \"<INSERT_API_KEY_HERE>\"\n      }\n    }\n  }\n}\n```\n\n**Develop**\n\nThis MCP is typically run directly using `npx` and doesn't require local installation or building unless you intend to modify the source code. If you want to develop it locally, you would typically clone the source repository (if available) and follow its specific contribution guidelines.\n\n## License\n\nThis project is licensed under the MIT License - see the LICENSE file for details.\n\n---\n\n## Links\n\n- X/Twitter: [@kregenrek](https://x.com/kregenrek)\n- Bluesky: [@kevinkern.dev](https://bsky.app/profile/kevinkern.dev)\n\n## Courses\n- Learn Cursor AI: [Ultimate Cursor Course](https://www.instructa.ai/en/cursor-ai)\n- Learn to build software with AI: [instructa.ai](https://www.instructa.ai)\n\n## See my other projects:\n\n* [AI Prompts](https://github.com/instructa/ai-prompts/blob/main/README.md) - Curated AI Prompts for Cursor AI, Cline, Windsurf and Github Copilot\n* [codefetch](https://github.com/regenrek/codefetch) - Turn code into Markdown for LLMs with one simple terminal command\n* [aidex](https://github.com/regenrek/aidex) A CLI tool that provides detailed information about AI language models, helping developers choose the right model for their needs.\n* [codetie](https://github.com/codetie-ai/codetie) - XCode CLI",
      "npm_url": "https://www.npmjs.com/package/@instructa/mcp-youtube-music",
      "npm_downloads": 445,
      "keywords": [
        "music",
        "youtube",
        "mcp",
        "youtube music",
        "music search",
        "mcp youtube"
      ],
      "category": "web-search"
    },
    "ipfind--ipfind-mcp-server": {
      "owner": "ipfind",
      "name": "ipfind-mcp-server",
      "url": "https://github.com/ipfind/ipfind-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/ipfind.webp",
      "description": "Retrieve geolocation information for IP addresses using the IP Find API, enabling AI assistants to easily access IP data for location lookups.",
      "stars": 1,
      "forks": 5,
      "license": "No License",
      "language": "JavaScript",
      "updated_at": "2025-08-14T14:18:19Z",
      "readme_content": "# IP Find MCP Server\n\nA Model Context Protocol server that enables AI assistants to use IP Find. \n\n## How It Works\n\nThe MCP server:\n\n-   Connects to your IP Find API and allows AI Assistants to get locations of IP Addresses.\n\n## Usage with Claude Desktop\n\n### Prerequisites\n\n-   NodeJS\n-   MCP Client (like Claude Desktop App)\n-   IP Find API Key\n\n### Installation\n\nTo use this server with the Claude Desktop app, add the following configuration to the \"mcpServers\" section of your `claude_desktop_config.json`:\n\n```json\n{\n    \"mcpServers\": {\n        \"ipfind\": {\n            \"command\": \"npx\",\n            \"args\": [\"-y\", \"@ipfind/ipfind-mcp-server\"],\n            \"env\": {\n                \"IPFIND_API_KEY\": \"<API KEY GOES HERE>\"\n            }\n        }\n    }\n}\n```\n\n-   `IPFIND_API_KEY` - You can generate an API key at IPfind.com.\n\n\nCertified by [MCPHub](https://mcphub.com/mcp-servers/ipfind/ipfind-mcp-server)\n",
      "npm_url": "https://www.npmjs.com/package/@ipfind/ipfind-mcp-server",
      "npm_downloads": 355,
      "keywords": [
        "ipfind",
        "geolocation",
        "ip",
        "search ipfind",
        "ipfind ipfind",
        "location lookups"
      ],
      "category": "web-search"
    },
    "isnow890--naver-search-mcp": {
      "owner": "isnow890",
      "name": "naver-search-mcp",
      "url": "https://github.com/isnow890/naver-search-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/isnow890.webp",
      "description": "Connects to the Naver Search API for searching diverse content types such as news, blogs, and shopping. Analyzes search and shopping trends using the DataLab API, offering insights into consumer behavior patterns by category, device, gender, and age group.",
      "stars": 40,
      "forks": 15,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-09-30T12:55:28Z",
      "readme_content": "# Naver Search MCP Server\n\n[![한국어](https://img.shields.io/badge/한국어-README-yellow)](README-ko.md)\n\n[![Trust Score](https://archestra.ai/mcp-catalog/api/badge/quality/isnow890/naver-search-mcp)](https://archestra.ai/mcp-catalog/isnow890__naver-search-mcp)\n[![smithery badge](https://smithery.ai/badge/@isnow890/naver-search-mcp)](https://smithery.ai/server/@isnow890/naver-search-mcp)\n[![MCP.so](https://img.shields.io/badge/MCP.so-Naver%20Search%20MCP-blue)](https://mcp.so/server/naver-search-mcp/isnow890)\n\n#### Version History\n\n###### 1.0.45 (2025-09-28)\n\n- Smithery compatibility issues resolved - Now available through Smithery with latest features\n- Excel compatibility issues fixed in category search - Replaced with JSON functionality\n- Web Korean search (`search_webkr`) functionality restored\n- Full compatibility with Smithery platform installations\n\n###### 1.0.44 (2025-08-31)\n\n- `get_current_korean_time` tool added - Essential time context tool for Korean timezone\n- Enhanced all existing tool descriptions to reference time tool for temporal queries\n- Improved temporal context handling for \"today\", \"now\", \"current\" searches\n- Comprehensive Korean time formatting with multiple output formats\n\n###### 1.0.4 (2025-08-21)\n\n- `find_category` tool added - with fuzzy matching and ranking system support\n- Enhanced parameter validation with Zod schema\n- Improved category search workflow\n\n###### 1.0.30 (2025-08-04)\n\n- MCP SDK upgraded to 1.17.1\n- Fixed compatibility issues with Smithery specification changes\n- Added comprehensive DataLab shopping category code documentation\n\n###### 1.0.2 (2025-04-26)\n\n- README updated: cafe article search tool and version history section improved\n\n###### 1.0.1 (2025-04-26)\n\n- Cafe article search feature added\n- Shopping category info added to zod\n- Source code refactored\n\n###### 1.0.0 (2025-04-08)\n\n- Initial release\n\n#### Prerequisites\n\n- Naver Developers API Key (Client ID and Secret)\n- Node.js 18 or higher\n- NPM 8 or higher\n- Docker (optional, for container deployment)\n\n#### Getting API Keys\n\n1. Visit [Naver Developers](https://developers.naver.com/apps/#/register)\n2. Click \"Register Application\"\n3. Enter application name and select ALL of the following APIs:\n   - Search (for blog, news, book search, etc.)\n   - DataLab (Search Trends)\n   - DataLab (Shopping Insight)\n4. Set the obtained Client ID and Client Secret as environment variables\n\n## Tool Details\n\n### Available tools:\n\n#### 🕐 Time & Context Tools\n\n- **get_current_korean_time**: Get current Korean time (KST) with comprehensive date/time information. Essential for understanding \"today\", \"now\", or \"current\" context in Korean timezone. Always use this tool when temporal context is needed for searches or analysis.\n\n#### 🆕 Category Search\n\n- **find_category**: Category search tool - No more need to manually check category numbers via URL for trend and shopping insight searches. The LLM will find it out as you say.\n\n#### Search Tools\n\n- **search_webkr**: Search Naver web documents\n- **search_news**: Search Naver news\n- **search_blog**: Search Naver blogs\n- **search_cafearticle**: Search Naver cafe articles\n- **search_shop**: Search Naver shopping\n- **search_image**: Search Naver images\n- **search_kin**: Search Naver KnowledgeiN\n- **search_book**: Search Naver books\n- **search_encyc**: Search Naver encyclopedia\n- **search_academic**: Search Naver academic papers\n- **search_local**: Search Naver local places\n\n#### DataLab Tools\n\n- **datalab_search**: Analyze search term trends\n- **datalab_shopping_category**: Analyze shopping category trends\n- **datalab_shopping_by_device**: Analyze shopping trends by device\n- **datalab_shopping_by_gender**: Analyze shopping trends by gender\n- **datalab_shopping_by_age**: Analyze shopping trends by age group\n- **datalab_shopping_keywords**: Analyze shopping keyword trends\n- **datalab_shopping_keyword_by_device**: Analyze shopping keyword trends by device\n- **datalab_shopping_keyword_by_gender**: Analyze shopping keyword trends by gender\n- **datalab_shopping_keyword_by_age**: Analyze shopping keyword trends by age group\n\n#### Complete Category List:\n\nFor a complete list of category codes, you can download from Naver Shopping Partner Center or extract them by browsing Naver Shopping categories.\n\n### 🎯 Business Use Cases & Scenarios\n\n#### 🛍️ E-commerce Market Research\n\n```javascript\n// Fashion trend discovery\nfind_category(\"fashion\") → Check top fashion categories and codes\ndatalab_shopping_category → Analyze seasonal fashion trends\ndatalab_shopping_age → Identify fashion target demographics\ndatalab_shopping_keywords → Compare \"dress\" vs \"jacket\" vs \"coat\"\n```\n\n#### 📱 Digital Marketing Strategy\n\n```javascript\n// Beauty industry analysis\nfind_category(\"cosmetics\") → Find beauty categories\ndatalab_shopping_gender → 95% female vs 5% male shoppers\ndatalab_shopping_device → Mobile dominance in beauty shopping\ndatalab_shopping_keywords → \"tint\" vs \"lipstick\" keyword performance\n```\n\n#### 🏢 Business Intelligence & Competitive Analysis\n\n```javascript\n// Tech product insights\nfind_category(\"smartphone\") → Check electronics categories\ndatalab_shopping_category → Track iPhone vs Galaxy trends\ndatalab_shopping_age → 20-30s as main smartphone buyers\ndatalab_shopping_device → PC vs mobile shopping behavior\n```\n\n#### 📊 Seasonal Business Planning\n\n```javascript\n// Holiday shopping analysis\nfind_category(\"gift\") → Gift categories\ndatalab_shopping_category → Black Friday, Christmas trends\ndatalab_shopping_keywords → \"Mother's Day gift\" vs \"birthday gift\"\ndatalab_shopping_age → Age-based gift purchasing patterns\n```\n\n#### 🎯 Customer Persona Development\n\n```javascript\n// Fitness market analysis\nfind_category(\"exercise\") → Sports/fitness categories\ndatalab_shopping_gender → Male vs female fitness spending\ndatalab_shopping_age → Primary fitness demographics (20-40s)\ndatalab_shopping_keywords → \"home workout\" vs \"gym\" trend analysis\n```\n\n### 📈 Advanced Analysis Scenarios\n\n#### Market Entry Strategy\n\n1. **Category Discovery**: Use `find_category` to explore market segments\n2. **Trend Analysis**: Identify growing vs declining categories\n3. **Demographic Targeting**: Age/gender analysis for customer targeting\n4. **Competitive Intelligence**: Keyword performance comparison\n5. **Device Strategy**: Mobile vs PC shopping optimization\n\n#### Product Launch Planning\n\n1. **Market Validation**: Category growth trends and seasonality\n2. **Target Customers**: Demographic analysis for product positioning\n3. **Marketing Channels**: Device preferences for advertising strategy\n4. **Competitive Landscape**: Keyword competition and opportunities\n5. **Pricing Strategy**: Category performance and price correlation\n\n#### Performance Monitoring\n\n1. **Category Health**: Monitor product category trends\n2. **Keyword Tracking**: Track brand and product keyword performance\n3. **Demographic Shifts**: Monitor changing customer demographics\n4. **Seasonal Patterns**: Plan inventory and marketing campaigns\n5. **Competitive Benchmarking**: Compare performance against category averages\n\n### Quick Reference: Popular Category Codes\n\n| Category            | Code     | Korean        |\n| ------------------- | -------- | ------------- |\n| Fashion/Clothing    | 50000000 | 패션의류      |\n| Cosmetics/Beauty    | 50000002 | 화장품/미용   |\n| Digital/Electronics | 50000003 | 디지털/가전   |\n| Sports/Leisure      | 50000004 | 스포츠/레저   |\n| Food/Beverages      | 50000008 | 식품/음료     |\n| Health/Medical      | 50000009 | 건강/의료용품 |\n\n💡 **Tip**: Use `find_category` with fuzzy searches like \"beauty\", \"fashion\", \"electronics\" to easily find categories.\n\n## Installation\n\n### Method 1: NPX Installation (Recommended)\n\nThe most reliable way to use this MCP server is through direct NPX installation. For detailed package information, see the [NPM package page](https://www.npmjs.com/package/@isnow890/naver-search-mcp).\n\n#### Claude Desktop Configuration\n\nAdd to Claude Desktop config file (`%APPDATA%\\Claude\\claude_desktop_config.json` on Windows, `~/Library/Application Support/Claude/claude_desktop_config.json` on macOS/Linux):\n\n```json\n{\n  \"mcpServers\": {\n    \"naver-search\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@isnow890/naver-search-mcp\"],\n      \"env\": {\n        \"NAVER_CLIENT_ID\": \"your_client_id\",\n        \"NAVER_CLIENT_SECRET\": \"your_client_secret\"\n      }\n    }\n  }\n}\n```\n\n#### Cursor AI Configuration\n\nAdd to `mcp.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"naver-search\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@isnow890/naver-search-mcp\"],\n      \"env\": {\n        \"NAVER_CLIENT_ID\": \"your_client_id\",\n        \"NAVER_CLIENT_SECRET\": \"your_client_secret\"\n      }\n    }\n  }\n}\n```\n\n### Method 2: Smithery Installation (Alternative - Known Issues)\n\n⚠️ **Important Notice**: Smithery installation may experience connection timeouts and hanging issues due to WebSocket relay infrastructure problems. This is a known Smithery platform issue, not a problem with this MCP server code. **NPX installation (Method 1) is strongly recommended for reliable operation.**\n\n#### Known Issues with Smithery:\n- Server initialization hangs or times out\n- `Error -32001: Request timed out`\n- WebSocket connection drops after handshake\n- Server shuts down unexpectedly before processing requests\n\n#### If you still want to try Smithery:\n\n##### For Claude Desktop:\n\n```bash\nnpx -y @smithery/cli@latest install @isnow890/naver-search-mcp --client claude\n```\n\n##### For other AI clients:\n\n```bash\n# Cursor\nnpx -y @smithery/cli@latest install @isnow890/naver-search-mcp --client cursor\n\n# Windsurf\nnpx -y @smithery/cli@latest install @isnow890/naver-search-mcp --client windsurf\n\n# Cline\nnpx -y @smithery/cli@latest install @isnow890/naver-search-mcp --client cline\n```\n\n**If you experience timeout issues with Smithery, please switch to Method 1 (NPX) for stable operation.**\n\n### Method 3: Local Installation\n\nFor local development or custom modifications:\n\n#### Step 1: Download and Build Source Code\n\n##### Clone with Git\n\n```bash\ngit clone https://github.com/isnow890/naver-search-mcp.git\ncd naver-search-mcp\nnpm install\nnpm run build\n```\n\n##### Or Download ZIP File\n\n1. Download the latest version from [GitHub Releases](https://github.com/isnow890/naver-search-mcp/releases)\n2. Extract the ZIP file to your desired location\n3. Navigate to the extracted folder in terminal:\n\n```bash\ncd /path/to/naver-search-mcp\nnpm install\nnpm run build\n```\n\n⚠️ **Important**: You must run `npm run build` after installation to generate the `dist` folder that contains the compiled JavaScript files.\n\n#### Step 2: Claude Desktop Configuration\n\nAfter building, you'll need the following information:\n\n- **NAVER_CLIENT_ID**: Client ID from Naver Developers\n- **NAVER_CLIENT_SECRET**: Client Secret from Naver Developers\n- **Installation Path**: Absolute path to the downloaded folder\n\n##### Windows Configuration\n\nAdd to Claude Desktop config file (`%APPDATA%\\Claude\\claude_desktop_config.json`):\n\n```json\n{\n  \"mcpServers\": {\n    \"naver-search\": {\n      \"type\": \"stdio\",\n      \"command\": \"cmd\",\n      \"args\": [\n        \"/c\",\n        \"node\",\n        \"C:\\\\path\\\\to\\\\naver-search-mcp\\\\dist\\\\src\\\\index.js\"\n      ],\n      \"cwd\": \"C:\\\\path\\\\to\\\\naver-search-mcp\",\n      \"env\": {\n        \"NAVER_CLIENT_ID\": \"your-naver-client-id\",\n        \"NAVER_CLIENT_SECRET\": \"your-naver-client-secret\"\n      }\n    }\n  }\n}\n```\n\n##### macOS/Linux Configuration\n\nAdd to Claude Desktop config file (`~/Library/Application Support/Claude/claude_desktop_config.json`):\n\n```json\n{\n  \"mcpServers\": {\n    \"naver-search\": {\n      \"type\": \"stdio\",\n      \"command\": \"node\",\n      \"args\": [\"/path/to/naver-search-mcp/dist/src/index.js\"],\n      \"cwd\": \"/path/to/naver-search-mcp\",\n      \"env\": {\n        \"NAVER_CLIENT_ID\": \"your-naver-client-id\",\n        \"NAVER_CLIENT_SECRET\": \"your-naver-client-secret\"\n      }\n    }\n  }\n}\n```\n\n##### Path Configuration Important Notes\n\n⚠️ **Important**: You must change the following paths in the above configuration to your actual installation paths:\n\n- **Windows**: Change `C:\\\\path\\\\to\\\\naver-search-mcp` to your actual downloaded folder path\n- **macOS/Linux**: Change `/path/to/naver-search-mcp` to your actual downloaded folder path\n- **Build Path**: Make sure the path points to `dist/src/index.js` (not just `index.js`)\n\nFinding your path:\n\n```bash\n# Check current location\npwd\n\n# Absolute path examples\n# Windows: C:\\Users\\username\\Downloads\\naver-search-mcp\n# macOS: /Users/username/Downloads/naver-search-mcp\n# Linux: /home/username/Downloads/naver-search-mcp\n```\n\n#### Step 3: Restart Claude Desktop\n\nAfter completing the configuration, completely close and restart Claude Desktop to activate the Naver Search MCP server.\n\n---\n\n## Alternative Installation Methods\n\n### Method 4: Docker Installation\n\nFor containerized deployment:\n\n```bash\ndocker run -i --rm \\\n  -e NAVER_CLIENT_ID=your_client_id \\\n  -e NAVER_CLIENT_SECRET=your_client_secret \\\n  mcp/naver-search\n```\n\nDocker configuration for Claude Desktop:\n\n```json\n{\n  \"mcpServers\": {\n    \"naver-search\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"-i\",\n        \"--rm\",\n        \"-e\",\n        \"NAVER_CLIENT_ID=your_client_id\",\n        \"-e\",\n        \"NAVER_CLIENT_SECRET=your_client_secret\",\n        \"mcp/naver-search\"\n      ]\n    }\n  }\n}\n```\n\n## Build\n\nDocker build:\n\n```bash\ndocker build -t mcp/naver-search .\n```\n\n## License\n\nMIT License\n",
      "npm_url": "https://www.npmjs.com/package/naver-search-mcp",
      "npm_downloads": 0,
      "keywords": [
        "searching",
        "search",
        "naver",
        "naver search",
        "search shopping",
        "search api"
      ],
      "category": "web-search"
    },
    "it-beard--tavily-server": {
      "owner": "it-beard",
      "name": "tavily-server",
      "url": "https://github.com/it-beard/tavily-server",
      "imageUrl": "/freedevtools/mcp/pfp/it-beard.webp",
      "description": "Facilitates comprehensive web searches and retrieves up-to-date information through AI-powered search capabilities, with features like result scoring, caching, and search history storage. Provides access to rich search results including titles, URLs, content snippets, and AI-generated summaries.",
      "stars": 6,
      "forks": 0,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-04-09T18:09:08Z",
      "readme_content": "# Tavily MCP Server\n\nA Model Context Protocol (MCP) server that provides AI-powered search capabilities using the Tavily API. This server enables AI assistants to perform comprehensive web searches and retrieve relevant, up-to-date information.\n\n## Features\n\n- AI-powered search functionality\n- Support for basic and advanced search depths\n- Rich search results including titles, URLs, and content snippets\n- AI-generated summaries of search results\n- Result scoring and response time tracking\n- Comprehensive search history storage with caching\n- MCP Resources for flexible data access\n\n## Prerequisites\n\n- Node.js (v16 or higher)\n- npm (Node Package Manager)\n- Tavily API key (Get one at [Tavily's website](https://tavily.com))\n- An MCP client (e.g., Cline, Claude Desktop, or your own implementation)\n\n## Installation\n\n1. Clone the repository:\n```bash\ngit clone https://github.com/it-beard/tavily-server.git\ncd tavily-mcp-server\n```\n\n2. Install dependencies:\n```bash\nnpm install\n```\n\n3. Build the project:\n```bash\nnpm run build\n```\n\n## Configuration\n\nThis server can be used with any MCP client. Below are configuration instructions for popular clients:\n\n### Cline Configuration\n\nIf you're using Cline (the VSCode extension for Claude), create or modify the MCP settings file at:\n- macOS: `~/Library/Application Support/Cursor/User/globalStorage/saoudrizwan.claude-dev/settings/cline_mcp_settings.json`\n- Windows: `%APPDATA%\\Cursor\\User\\globalStorage\\saoudrizwan.claude-dev\\settings\\cline_mcp_settings.json`\n- Linux: `~/.config/Cursor/User/globalStorage/saoudrizwan.claude-dev\\settings\\cline_mcp_settings.json`\n\nAdd the following configuration (replace paths and API key with your own):\n```json\n{\n  \"mcpServers\": {\n    \"tavily\": {\n      \"command\": \"node\",\n      \"args\": [\"/path/to/tavily-server/build/index.js\"],\n      \"env\": {\n        \"TAVILY_API_KEY\": \"your-api-key-here\"\n      }\n    }\n  }\n}\n```\n\n### Claude Desktop Configuration\n\nIf you're using the Claude Desktop app, modify the configuration file at:\n- macOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\n- Windows: `%APPDATA%\\Claude\\claude_desktop_config.json`\n- Linux: `~/.config/Claude/claude_desktop_config.json`\n\nUse the same configuration format as shown above.\n\n### Other MCP Clients\n\nFor other MCP clients, consult their documentation for the correct configuration file location and format. The server configuration should include:\n1. Command to run the server (typically `node`)\n2. Path to the compiled server file\n3. Environment variables including the Tavily API key\n\n## Usage\n\n### Tools\n\nThe server provides a single tool named `search` with the following parameters:\n\n#### Required Parameters\n- `query` (string): The search query to execute\n\n#### Optional Parameters\n- `search_depth` (string): Either \"basic\" (faster) or \"advanced\" (more comprehensive)\n\n#### Example Usage\n\n```typescript\n// Example using the MCP SDK\nconst result = await mcpClient.callTool(\"tavily\", \"search\", {\n  query: \"latest developments in artificial intelligence\",\n  search_depth: \"basic\"\n});\n```\n\n### Resources\n\nThe server provides both static and dynamic resources for flexible data access:\n\n#### Static Resources\n- `tavily://last-search/result`: Returns the results of the most recent search query\n  - Persisted to disk in the data directory\n  - Survives server restarts\n  - Returns a 'No search has been performed yet' error if no search has been done\n\n#### Dynamic Resources (Resource Templates)\n- `tavily://search/{query}`: Access search results for any query\n  - Replace {query} with your URL-encoded search term\n  - Example: `tavily://search/artificial%20intelligence`\n  - Returns cached results if the query was previously made\n  - Performs and stores new search if query hasn't been searched before\n  - Returns the same format as the search tool but through a resource interface\n\nResources in MCP provide an alternative way to access data compared to tools:\n- Tools are for executing operations (like performing a new search)\n- Resources are for accessing data (like retrieving existing search results)\n- Resource URIs can be stored and accessed later\n- Resources support both static (fixed) and dynamic (templated) access patterns\n\n#### Response Format\n\n```typescript\ninterface SearchResponse {\n  query: string;\n  answer: string;\n  results: Array<{\n    title: string;\n    url: string;\n    content: string;\n    score: number;\n  }>;\n  response_time: number;\n}\n```\n\n### Persistent Storage\n\nThe server implements comprehensive persistent storage for search results:\n\n#### Storage Location\n- Data is stored in the `data` directory\n- `data/searches.json` contains all historical search results\n- Data persists between server restarts\n- Storage is automatically initialized on server start\n\n#### Storage Features\n- Stores complete search history\n- Caches all search results for quick retrieval\n- Automatic saving of new search results\n- Disk-based persistence\n- JSON format for easy debugging\n- Error handling for storage operations\n- Automatic directory creation\n\n#### Caching Behavior\n- All search results are cached automatically\n- Subsequent requests for the same query return cached results\n- Caching improves response time and reduces API calls\n- Cache persists between server restarts\n- Last search is tracked for quick access\n\n## Development\n\n### Project Structure\n\n```\ntavily-server/\n├── src/\n│   └── index.ts    # Main server implementation\n├── data/           # Persistent storage directory\n│   └── searches.json  # Search history and cache storage\n├── build/          # Compiled JavaScript files\n├── package.json    # Project dependencies and scripts\n└── tsconfig.json   # TypeScript configuration\n```\n\n### Available Scripts\n\n- `npm run build`: Compile TypeScript and make the output executable\n- `npm run start`: Start the MCP server (after building)\n- `npm run dev`: Run the server in development mode\n\n## Error Handling\n\nThe server provides detailed error messages for common issues:\n- Invalid API key\n- Network errors\n- Invalid search parameters\n- API rate limiting\n- Resource not found\n- Invalid resource URIs\n- Storage read/write errors\n\n## Contributing\n\n1. Fork the repository\n2. Create your feature branch (`git checkout -b feature/amazing-feature`)\n3. Commit your changes (`git commit -m 'Add some amazing feature'`)\n4. Push to the branch (`git push origin feature/amazing-feature`)\n5. Open a Pull Request\n\n## License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n\n## Acknowledgments\n\n- [Model Context Protocol (MCP)](https://github.com/modelcontextprotocol/protocol) for the server framework\n- [Tavily API](https://tavily.com) for providing the search capabilities\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "searches",
        "search",
        "web",
        "web search",
        "web searches",
        "search results"
      ],
      "category": "web-search"
    },
    "jaacob--perplexity-mcp": {
      "owner": "jaacob",
      "name": "perplexity-mcp",
      "url": "https://github.com/jaacob/perplexity-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/jaacob.webp",
      "description": "Provides web search capabilities through the Perplexity API, allowing access to real-time information from the web.",
      "stars": 4,
      "forks": 4,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-07-31T12:55:27Z",
      "readme_content": "# Perplexity MCP Server\n\n[![smithery badge](https://smithery.ai/badge/@jaacob/perplexity-mcp)](https://smithery.ai/server/@jaacob/perplexity-mcp)\n\nAn MCP server that provides web search capabilities using Perplexity's API.\n\n<a href=\"https://glama.ai/mcp/servers/97nsl3drhq\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/97nsl3drhq/badge\" alt=\"Perplexity Server MCP server\" /></a>\n\n## Prerequisites\n\n- Node.js (v14 or higher)\n- A Perplexity API key (get one at <https://www.perplexity.ai/settings/api>)\n- Claude Desktop App\n\n## Installation\n\n### Installing via Smithery\n\nTo install Perplexity Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@jaacob/perplexity-mcp):\n\n```bash\nnpx -y @smithery/cli install @jaacob/perplexity-mcp --client claude\n```\n\n### Manual Installation\n1. Clone this repository:\n\n    ```bash\n    git clone https://github.com/jaacob/perplexity-mcp\n    cd perplexity-mcp\n    ```\n\n2. Install dependencies:\n\n    ```bash\n    npm install\n    ```\n\n3. Build the server:\n\n    ```bash\n    npm run build\n    ```\n\n## Configuration\n\n1. Get your Perplexity API key from <https://www.perplexity.ai/settings/api>\n\n2. Add the server to Claude's config file at `~/Library/Application Support/Claude/claude_desktop_config.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"perplexity-server\": {\n      \"command\": \"node\",\n      \"args\": [\n        \"/absolute/path/to/perplexity-mcp/build/index.js\"\n      ],\n      \"env\": {\n        \"PERPLEXITY_API_KEY\": \"your-api-key-here\",\n        \"PERPLEXITY_MODEL\": \"sonar\"\n      }\n    }\n  }\n}\n```\n\nReplace `/absolute/path/to` with the actual path to where you cloned the repository.\n\n### Available Models\n\nYou can specify which model to use by setting the `PERPLEXITY_MODEL` environment variable. Available options:\n\n- `sonar-reasoning-pro` - Most capable model with enhanced reasoning\n- `sonar-reasoning` - Enhanced reasoning capabilities\n- `sonar-pro` - Faster response times\n- `sonar` - Default model (used if no model is specified)\n\nFor up-to-date model pricing and availability, visit: <https://docs.perplexity.ai/guides/pricing>\n\n## Usage\n\nAfter configuring the server and restarting Claude, you can simply ask Claude to search for information. For example:\n\n- \"What's the latest news about SpaceX?\"\n- \"Search for the best restaurants in Chicago\"\n- \"Find information about the history of jazz music\"\n\nClaude will automatically use the Perplexity search tool to find and return relevant information.\n\nIf for whatever reason it decides not to, you can force the issue by prepending your prompt with \"Search the web\".\n\n## Development\n\nTo modify the server:\n\n1. Edit `src/index.ts`\n2. Rebuild with `npm run build`\n3. Restart Claude to load the changes\n\n## License\n\nMIT\n",
      "npm_url": "https://www.npmjs.com/package/perplexity-mcp",
      "npm_downloads": 8172,
      "keywords": [
        "search",
        "mcp",
        "perplexity",
        "perplexity api",
        "search jaacob",
        "perplexity mcp"
      ],
      "category": "web-search"
    },
    "jae-jae--fetcher-mcp": {
      "owner": "jae-jae",
      "name": "fetcher-mcp",
      "url": "https://github.com/jae-jae/fetcher-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/jae-jae.webp",
      "description": "Fetch web page content using a headless browser, capable of executing JavaScript to handle dynamic web pages and modern web applications. Extracts the main content intelligently using a Readability algorithm and supports output in both HTML and Markdown formats.",
      "stars": 879,
      "forks": 71,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-10-02T02:55:44Z",
      "readme_content": "<div align=\"center\">\n  <img src=\"https://raw.githubusercontent.com/jae-jae/fetcher-mcp/refs/heads/main/icon.svg\" width=\"100\" height=\"100\" alt=\"Fetcher MCP Icon\" />\n</div>\n\n[中文](https://www.readme-i18n.com/jae-jae/fetcher-mcp?lang=zh) |\n[Deutsch](https://www.readme-i18n.com/jae-jae/fetcher-mcp?lang=de) |\n[Español](https://www.readme-i18n.com/jae-jae/fetcher-mcp?lang=es) |\n[français](https://www.readme-i18n.com/jae-jae/fetcher-mcp?lang=fr) |\n[日本語](https://www.readme-i18n.com/jae-jae/fetcher-mcp?lang=ja) |\n[한국어](https://www.readme-i18n.com/jae-jae/fetcher-mcp?lang=ko) |\n[Português](https://www.readme-i18n.com/jae-jae/fetcher-mcp?lang=pt) |\n[Русский](https://www.readme-i18n.com/jae-jae/fetcher-mcp?lang=ru)\n\n# Fetcher MCP\n\nMCP server for fetch web page content using Playwright headless browser.\n\n> 🌟 **Recommended**: [OllaMan](https://ollaman.com/) - Powerful Ollama AI Model Manager.\n\n## Advantages\n\n- **JavaScript Support**: Unlike traditional web scrapers, Fetcher MCP uses Playwright to execute JavaScript, making it capable of handling dynamic web content and modern web applications.\n\n- **Intelligent Content Extraction**: Built-in Readability algorithm automatically extracts the main content from web pages, removing ads, navigation, and other non-essential elements.\n\n- **Flexible Output Format**: Supports both HTML and Markdown output formats, making it easy to integrate with various downstream applications.\n\n- **Parallel Processing**: The `fetch_urls` tool enables concurrent fetching of multiple URLs, significantly improving efficiency for batch operations.\n\n- **Resource Optimization**: Automatically blocks unnecessary resources (images, stylesheets, fonts, media) to reduce bandwidth usage and improve performance.\n\n- **Robust Error Handling**: Comprehensive error handling and logging ensure reliable operation even when dealing with problematic web pages.\n\n- **Configurable Parameters**: Fine-grained control over timeouts, content extraction, and output formatting to suit different use cases.\n\n## Quick Start\n\nRun directly with npx:\n\n```bash\nnpx -y fetcher-mcp\n```\n\nFirst time setup - install the required browser by running the following command in your terminal:\n\n```bash\nnpx playwright install chromium\n```\n\n### HTTP and SSE Transport\n\nUse the `--transport=http` parameter to start both Streamable HTTP endpoint and SSE endpoint services simultaneously:\n\n```bash\nnpx -y fetcher-mcp --log --transport=http --host=0.0.0.0 --port=3000\n```\n\nAfter startup, the server provides the following endpoints:\n\n- `/mcp` - Streamable HTTP endpoint (modern MCP protocol)\n- `/sse` - SSE endpoint (legacy MCP protocol)\n\nClients can choose which method to connect based on their needs.\n\n### Debug Mode\n\nRun with the `--debug` option to show the browser window for debugging:\n\n```bash\nnpx -y fetcher-mcp --debug\n```\n\n## Configuration MCP\n\nConfigure this MCP server in Claude Desktop:\n\nOn MacOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\n\nOn Windows: `%APPDATA%/Claude/claude_desktop_config.json`\n\n```json\n{\n  \"mcpServers\": {\n    \"fetcher\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"fetcher-mcp\"]\n    }\n  }\n}\n```\n\n## Docker Deployment\n\n### Running with Docker\n\n```bash\ndocker run -p 3000:3000 ghcr.io/jae-jae/fetcher-mcp:latest\n```\n\n### Deploying with Docker Compose\n\nCreate a `docker-compose.yml` file:\n\n```yaml\nversion: \"3.8\"\n\nservices:\n  fetcher-mcp:\n    image: ghcr.io/jae-jae/fetcher-mcp:latest\n    container_name: fetcher-mcp\n    restart: unless-stopped\n    ports:\n      - \"3000:3000\"\n    environment:\n      - NODE_ENV=production\n    # Using host network mode on Linux hosts can improve browser access efficiency\n    # network_mode: \"host\"\n    volumes:\n      # For Playwright, may need to share certain system paths\n      - /tmp:/tmp\n    # Health check\n    healthcheck:\n      test: [\"CMD\", \"wget\", \"--spider\", \"-q\", \"http://localhost:3000\"]\n      interval: 30s\n      timeout: 10s\n      retries: 3\n```\n\nThen run:\n\n```bash\ndocker-compose up -d\n```\n\n## Features\n\n- `fetch_url` - Retrieve web page content from a specified URL\n\n  - Uses Playwright headless browser to parse JavaScript\n  - Supports intelligent extraction of main content and conversion to Markdown\n  - Supports the following parameters:\n    - `url`: The URL of the web page to fetch (required parameter)\n    - `timeout`: Page loading timeout in milliseconds, default is 30000 (30 seconds)\n    - `waitUntil`: Specifies when navigation is considered complete, options: 'load', 'domcontentloaded', 'networkidle', 'commit', default is 'load'\n    - `extractContent`: Whether to intelligently extract the main content, default is true\n    - `maxLength`: Maximum length of returned content (in characters), default is no limit\n    - `returnHtml`: Whether to return HTML content instead of Markdown, default is false\n    - `waitForNavigation`: Whether to wait for additional navigation after initial page load (useful for sites with anti-bot verification), default is false\n    - `navigationTimeout`: Maximum time to wait for additional navigation in milliseconds, default is 10000 (10 seconds)\n    - `disableMedia`: Whether to disable media resources (images, stylesheets, fonts, media), default is true\n    - `debug`: Whether to enable debug mode (showing browser window), overrides the --debug command line flag if specified\n\n- `fetch_urls` - Batch retrieve web page content from multiple URLs in parallel\n  - Uses multi-tab parallel fetching for improved performance\n  - Returns combined results with clear separation between webpages\n  - Supports the following parameters:\n    - `urls`: Array of URLs to fetch (required parameter)\n    - Other parameters are the same as `fetch_url`\n\n- `browser_install` - Install Playwright Chromium browser binary automatically\n\n  - Installs required Chromium browser binary when not available\n  - Automatically suggested when browser installation errors occur\n  - Supports the following parameters:\n    - `withDeps`: Install system dependencies required by Chromium browser, default is false\n    - `force`: Force installation even if Chromium is already installed, default is false\n\n## Tips\n\n### Handling Special Website Scenarios\n\n#### Dealing with Anti-Crawler Mechanisms\n\n- **Wait for Complete Loading**: For websites using CAPTCHA, redirects, or other verification mechanisms, include in your prompt:\n\n  ```\n  Please wait for the page to fully load\n  ```\n\n  This will use the `waitForNavigation: true` parameter.\n\n- **Increase Timeout Duration**: For websites that load slowly:\n  ```\n  Please set the page loading timeout to 60 seconds\n  ```\n  This adjusts both `timeout` and `navigationTimeout` parameters accordingly.\n\n#### Content Retrieval Adjustments\n\n- **Preserve Original HTML Structure**: When content extraction might fail:\n\n  ```\n  Please preserve the original HTML content\n  ```\n\n  Sets `extractContent: false` and `returnHtml: true`.\n\n- **Fetch Complete Page Content**: When extracted content is too limited:\n\n  ```\n  Please fetch the complete webpage content instead of just the main content\n  ```\n\n  Sets `extractContent: false`.\n\n- **Return Content as HTML**: When HTML format is needed instead of default Markdown:\n  ```\n  Please return the content in HTML format\n  ```\n  Sets `returnHtml: true`.\n\n### Debugging and Authentication\n\n#### Enabling Debug Mode\n\n- **Dynamic Debug Activation**: To display the browser window during a specific fetch operation:\n  ```\n  Please enable debug mode for this fetch operation\n  ```\n  This sets `debug: true` even if the server was started without the `--debug` flag.\n\n#### Using Custom Cookies for Authentication\n\n- **Manual Login**: To login using your own credentials:\n\n  ```\n  Please run in debug mode so I can manually log in to the website\n  ```\n\n  Sets `debug: true` or uses the `--debug` flag, keeping the browser window open for manual login.\n\n- **Interacting with Debug Browser**: When debug mode is enabled:\n\n  1. The browser window remains open\n  2. You can manually log into the website using your credentials\n  3. After login is complete, content will be fetched with your authenticated session\n\n- **Enable Debug for Specific Requests**: Even if the server is already running, you can enable debug mode for a specific request:\n  ```\n  Please enable debug mode for this authentication step\n  ```\n  Sets `debug: true` for this specific request only, opening the browser window for manual login.\n\n## Development\n\n### Install Dependencies\n\n```bash\nnpm install\n```\n\n### Install Playwright Browser\n\nInstall the browsers needed for Playwright:\n\n```bash\nnpm run install-browser\n```\n\n### Build the Server\n\n```bash\nnpm run build\n```\n\n## Debugging\n\nUse MCP Inspector for debugging:\n\n```bash\nnpm run inspector\n```\n\nYou can also enable visible browser mode for debugging:\n\n```bash\nnode build/index.js --debug\n```\n\n## Related Projects\n\n- [g-search-mcp](https://github.com/jae-jae/g-search-mcp): A powerful MCP server for Google search that enables parallel searching with multiple keywords simultaneously. Perfect for batch search operations and data collection.\n\n## License\n\nLicensed under the [MIT License](https://choosealicense.com/licenses/mit/)\n\n[![Powered by DartNode](https://dartnode.com/branding/DN-Open-Source-sm.png)](https://dartnode.com \"Powered by DartNode - Free VPS for Open Source\")\n",
      "npm_url": "https://www.npmjs.com/package/fetcher-mcp",
      "npm_downloads": 50960,
      "keywords": [
        "fetcher",
        "html",
        "fetch",
        "fetch web",
        "web search",
        "dynamic web"
      ],
      "category": "web-search"
    },
    "jae-jae--g-search-mcp": {
      "owner": "jae-jae",
      "name": "g-search-mcp",
      "url": "https://github.com/jae-jae/g-search-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/jae-jae.webp",
      "description": "Enables efficient parallel searching on Google using multiple keywords, automatically handling verification challenges and returning structured search results in JSON format for analysis.",
      "stars": 210,
      "forks": 22,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-10-02T14:10:50Z",
      "readme_content": "<div align=\"center\">\n  <img src=\"https://github.com/jae-jae/g-search-mcp/raw/main/icon.svg\" width=\"120\" height=\"120\" alt=\"g-search-mcp Logo\" />\n</div>\n\n# G-Search MCP\n\nA powerful MCP server for Google search that enables parallel searching with multiple keywords simultaneously.\n\n> This project is modified from [google-search](https://github.com/web-agent-master/google-search).\n\n> 🌟 **Recommended**: [OllaMan](https://ollaman.com/) - Powerful Ollama AI Model Manager.\n\n## Advantages\n\n- **Parallel Searching**: Supports searching with multiple keywords on Google simultaneously, improving search efficiency\n- **Browser Optimization**: Opens multiple tabs in a single browser instance for efficient parallel searching\n- **Automatic Verification Handling**: Intelligently detects CAPTCHA and enables visible browser mode for user verification when needed\n- **User Behavior Simulation**: Simulates real user browsing patterns to reduce the possibility of detection by search engines\n- **Structured Data**: Returns structured search results in JSON format for easy processing and analysis\n- **Configurable Parameters**: Supports various parameter configurations such as search result limits, timeout settings, locale settings, etc.\n\n## Quick Start\n\nRun directly with npx:\n\n```bash\nnpx -y g-search-mcp\n```\n\nFirst time setup - install the required browser by running the following command in your terminal:\n\n```bash\nnpx playwright install chromium\n```\n\n### Debug Mode\n\nUse the `--debug` option to run in debug mode (showing browser window):\n\n```bash\nnpx -y g-search-mcp --debug\n```\n\n## Configure MCP\n\nConfigure this MCP server in Claude Desktop:\n\nMacOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\nWindows: `%APPDATA%/Claude/claude_desktop_config.json`\n\n```json\n{\n  \"mcpServers\": {\n    \"g-search\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"g-search-mcp\"]\n    }\n  }\n}\n```\n\n## Features\n\n- `search` - Execute Google searches with multiple keywords and return results\n  - Uses Playwright browser to perform searches\n  - Supports the following parameters:\n    - `queries`: Array of search queries to execute (required parameter)\n    - `limit`: Maximum number of results to return per query, default is 10\n    - `timeout`: Page loading timeout in milliseconds, default is 60000 (60 seconds)\n    - `noSaveState`: Whether to avoid saving browser state, default is false\n    - `locale`: Locale setting for search results, default is en-US\n    - `debug`: Whether to enable debug mode (showing browser window), overrides the --debug flag in command line\n\n**Example usage**:\n\n```\nUse the search tool to search for \"machine learning\" and \"artificial intelligence\" on Google\n```\n\n**Example response**:\n\n```json\n{\n  \"searches\": [\n    {\n      \"query\": \"machine learning\",\n      \"results\": [\n        {\n          \"title\": \"What is Machine Learning? | IBM\",\n          \"link\": \"https://www.ibm.com/topics/machine-learning\",\n          \"snippet\": \"Machine learning is a branch of artificial intelligence (AI) and computer science which focuses on the use of data and algorithms to imitate the way that humans learn, gradually improving its accuracy.\"\n        },\n        ...\n      ]\n    },\n    {\n      \"query\": \"artificial intelligence\",\n      \"results\": [\n        {\n          \"title\": \"What is Artificial Intelligence (AI)? | IBM\",\n          \"link\": \"https://www.ibm.com/topics/artificial-intelligence\",\n          \"snippet\": \"Artificial intelligence leverages computers and machines to mimic the problem-solving and decision-making capabilities of the human mind.\"\n        },\n        ...\n      ]\n    }\n  ]\n}\n```\n\n## Usage Tips\n\n### Handling Special Website Scenarios\n\n#### Adjusting Search Parameters\n\n- **Search Result Quantity**: For more search results:\n\n  ```\n  Please return the top 20 search results for each keyword\n  ```\n\n  This will set the `limit: 20` parameter.\n\n- **Increase Timeout Duration**: For slow loading situations:\n  ```\n  Please set the page loading timeout to 120 seconds\n  ```\n  This will adjust the `timeout` parameter to 120000 milliseconds.\n\n#### Locale Settings Adjustment\n\n- **Change Search Region**: Specify a different locale setting:\n  ```\n  Please use Chinese locale (zh-CN) for searching\n  ```\n  This will set the `locale: \"zh-CN\"` parameter.\n\n### Debugging and Troubleshooting\n\n#### Enable Debug Mode\n\n- **Dynamic Debug Activation**: To display the browser window during a specific search operation:\n  ```\n  Please enable debug mode for this search operation\n  ```\n  This sets `debug: true` even if the server was started without the `--debug` flag.\n\n## Installation\n\n### Prerequisites\n\n- Node.js 18 or higher\n- NPM or Yarn\n\n### Install from Source\n\n1. Clone the repository:\n\n```bash\ngit clone https://github.com/jae-jae/g-search-mcp.git\ncd g-search-mcp\n```\n\n2. Install dependencies:\n\n```bash\nnpm install\n```\n\n3. Install Playwright browser:\n\n```bash\nnpm run install-browser\n```\n\n4. Build the server:\n\n```bash\nnpm run build\n```\n\n## Development\n\n### Auto Rebuild (Development Mode)\n\n```bash\nnpm run watch\n```\n\n### Using MCP Inspector for Debugging\n\n```bash\nnpm run inspector\n```\n\n## Related Projects\n\n- [fetcher-mcp](https://github.com/jae-jae/fetcher-mcp): A powerful MCP server for fetching web page content using Playwright headless browser. Features intelligent content extraction, parallel processing, resource optimization, and more, making it an ideal tool for web content scraping.\n\n## License\n\nLicensed under the [MIT License](https://choosealicense.com/licenses/mit/)\n",
      "npm_url": "https://www.npmjs.com/package/g-search-mcp",
      "npm_downloads": 12269,
      "keywords": [
        "searching",
        "search",
        "google",
        "parallel searching",
        "searching google",
        "web search"
      ],
      "category": "web-search"
    },
    "jaipandya--producthunt-mcp-server": {
      "owner": "jaipandya",
      "name": "producthunt-mcp-server",
      "url": "https://github.com/jaipandya/producthunt-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/jaipandya.webp",
      "description": "Connects to Product Hunt's API to access posts, comments, collections, topics, and user data. Facilitates the integration of AI assistants and chatbots with Product Hunt's features.",
      "stars": 31,
      "forks": 9,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-09-02T14:59:46Z",
      "readme_content": "# 🚀 Product Hunt MCP Server\n\n[![PyPI version](https://img.shields.io/pypi/v/product-hunt-mcp.svg)](https://pypi.org/project/product-hunt-mcp/)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)\n[![Python 3.10+](https://img.shields.io/badge/python-3.10%2B-blue.svg)](https://www.python.org/)\n[![Docker Ready](https://img.shields.io/badge/docker-ready-blue)](Dockerfile)\n[![MCP Compatible](https://img.shields.io/badge/MCP-compatible-brightgreen)](https://modelcontextprotocol.io/)\n\n> **A plug-and-play [MCP](https://modelcontextprotocol.io/) server for Product Hunt**\n\n---\n\n## 📦 Quick Install\n\n```bash\npip install product-hunt-mcp\n```\n\n## 🏃‍♂️ Quick Start Example\n\n```bash\n# Run the MCP server (requires PRODUCT_HUNT_TOKEN environment variable)\nexport PRODUCT_HUNT_TOKEN=your_token_here\nproduct-hunt-mcp\n```\n\n---\n\n## ✨ What is this?\n\n**Product Hunt MCP Server** connects Product Hunt's API to any LLM or agent that speaks the [Model Context Protocol (MCP)](https://modelcontextprotocol.io/). Perfect for AI assistants, chatbots, or your own automations!\n\n- 🔍 Get posts, collections, topics, users\n- 🗳️ Get votes, comments, and more\n- 🛠️ Use with Claude Desktop, Cursor, or any MCP client\n\n---\n\n## 🛠️ Features\n\n- Get detailed info on posts, comments, collections, topics, users\n- Search/filter by topic, date, votes, etc.\n- Paginated comments, user upvotes, and more\n- Built with [FastMCP](https://github.com/jlowin/fastmcp) for speed and compatibility\n\n---\n\n## 🧑‍💻 Who is this for?\n\n- **AI/LLM users**: Plug into Claude Desktop, Cursor, or your own agent\n- **Developers**: Build bots, dashboards, or automations with Product Hunt data\n- **Tinkerers**: Explore the MCP ecosystem and build your own tools\n\n---\n\n## 🏁 Setup\n\n### Prerequisites\n\n- Python 3.10+\n- Product Hunt API token ([get one here](https://www.producthunt.com/v2/oauth/applications))\n  - You'll need to create an account on Product Hunt\n  - Navigate to the API Dashboard and create a new application\n  - Use the `Developer Token` for the token\n\n> **Note:** When creating a new application on Product Hunt, you will be asked for a `redirect_uri`. While the MCP server does not use the redirect URI, it is a required field. You can enter any valid URL, such as `https://localhost:8424/callback`.\n\n### Installation\n\n**Preferred: [uv](https://github.com/astral-sh/uv) (fast, modern Python installer)**\n\n```bash\n# Install uv if you don't have it\npip install uv\n```\n\n#### Install from PyPI (recommended)\n```bash\nuv pip install product-hunt-mcp\n# or\npip install product-hunt-mcp\n```\n\n#### Install from GitHub (latest main branch)\n```bash\nuv pip install 'git+https://github.com/jaipandya/producthunt-mcp-server.git'\n# or\npip install 'git+https://github.com/jaipandya/producthunt-mcp-server.git'\n```\n\n#### Install locally from source\n```bash\nuv pip install .\n# or\npip install .\n```\n\n---\n\n## 🚀 Usage with Claude Desktop & Cursor\n\nOnce installed, the `product-hunt-mcp` command will be available. Add it to your Claude Desktop or Cursor configuration:\n\n```json\n{\n  \"mcpServers\": {\n    \"product-hunt\": {\n      \"command\": \"product-hunt-mcp\",\n      \"env\": {\n        \"PRODUCT_HUNT_TOKEN\": \"your_token_here\"\n      }\n    }\n  }\n}\n```\n\n- Replace `your_token_here` with your actual Product Hunt API token.\n- The token **must** be set as an environment variable in your Claude Desktop or Cursor config for the server to authenticate.\n- Always restart your client (Claude Desktop/Cursor) after editing the config file.\n\n> **Tip:** On macOS, Claude Desktop may not always find the `product-hunt-mcp` command if it's not in the default PATH. If you encounter issues, you can provide the full path to the executable. After installing, run:\n>\n> ```bash\n> which product-hunt-mcp\n> ```\n>\n> Use the output path in your Claude Desktop config, replacing `\"command\": \"product-hunt-mcp\"` with the full path (e.g., `\"command\": \"/Users/youruser/.local/bin/product-hunt-mcp\"`).\n\n### Finding your configuration file\n\n- **Claude Desktop**: \n  - Windows: `%APPDATA%\\claude-desktop\\config.json`\n  - macOS: `~/Library/Application Support/claude-desktop/config.json`\n  - Linux: `~/.config/claude-desktop/config.json`\n\n- **Cursor**:\n  - Windows: `%APPDATA%\\Cursor\\User\\settings.json`\n  - macOS: `~/Library/Application Support/Cursor/User/settings.json`\n  - Linux: `~/.config/Cursor/User/settings.json`\n\n### Docker\n\nYou can also run the server using Docker:\n\n```bash\n# Build the Docker image\ndocker build -t product-hunt-mcp .\n\n# Run the Docker container (interactive for MCP)\ndocker run -i --rm -e PRODUCT_HUNT_TOKEN=your_token_here product-hunt-mcp\n```\n\nFor Claude Desktop/Cursor integration with Docker, use this configuration:\n\n```json\n{\n  \"mcpServers\": {\n    \"product-hunt\": {\n      \"command\": \"docker\",\n      \"args\": [\"run\", \"-i\", \"--rm\", \"-e\", \"PRODUCT_HUNT_TOKEN=your_token_here\", \"product-hunt-mcp\"],\n      \"env\": {}\n    }\n  }\n}\n```\n\n> **Security Note:** Your `PRODUCT_HUNT_TOKEN` is sensitive. Do not share it or commit it to version control.\n\n---\n\n## 🛠️ MCP Tools\n\n| Tool                | Description                                 | Key Parameters |\n|---------------------|---------------------------------------------|----------------|\n| get_post_details    | Get info about a specific post              | `id` or `slug`, `comments_count`, `comments_after` |\n| get_posts           | Get posts with filters                      | `topic`, `order`, `count`, `featured`, `posted_before`, `posted_after` |\n| get_comment         | Get info about a specific comment           | `id` (required) |\n| get_post_comments   | Get comments for a post                     | `post_id` or `slug`, `order`, `count`, `after` |\n| get_collection      | Get info about a collection                 | `id` or `slug` |\n| get_collections     | Get collections with filters                | `featured`, `user_id`, `post_id`, `order`, `count` |\n| get_topic           | Get info about a topic                      | `id` or `slug` |\n| search_topics       | Search topics                               | `query`, `followed_by_user_id`, `order`, `count` |\n| get_user            | Get info about a user                       | `id` or `username`, `posts_type`, `posts_count` |\n| get_viewer          | Get info about the authenticated user       | None |\n| check_server_status | Check server/API status & authentication    | None |\n\n---\n\n## 🏗️ Project Structure\n\n```\nproduct-hunt-mcp/\n├── src/\n│   └── product_hunt_mcp/ # Main package directory\n│       ├── __init__.py\n│       ├── cli.py        # Command-line entry point\n│       ├── api/          # API clients & queries\n│       ├── schemas/      # Data validation schemas\n│       ├── tools/        # MCP tool definitions\n│       └── utils/        # Utility functions\n├── pyproject.toml      # Project metadata, dependencies, build config\n├── README.md\n├── CONTRIBUTING.md\n├── CHANGELOG.md\n├── Dockerfile\n└── ... (config files, etc.)\n```\n\n---\n\n## 🔄 Rate Limiting\n\nThe Product Hunt API has rate limits that this client respects. If you encounter rate limit errors, the client will inform you when the rate limit resets. You can check your current rate limit status using the `get_api_rate_limits` or `check_server_status` tools.\n\n---\n\n## 🐛 Troubleshooting\n\n- **Missing token**: Ensure your `PRODUCT_HUNT_TOKEN` is correctly set as an environment variable.\n- **Connection issues**: Verify your internet connection and that the Product Hunt API is accessible.\n- **Rate limiting**: If you hit rate limits, wait until the reset time or reduce your query frequency.\n- **Claude Desktop/Cursor not finding the server**: Verify the path to your Python executable and restart the client.\n\n---\n\n## 🤝 Contributing\n\n- PRs and issues welcome!\n- Please follow [PEP8](https://peps.python.org/pep-0008/) and use [ruff](https://github.com/charliermarsh/ruff) for linting.\n- See `pyproject.toml` for dev dependencies.\n\n---\n\n## 🌐 Links\n\n- [Model Context Protocol (MCP)](https://modelcontextprotocol.io/)\n- [FastMCP](https://github.com/jlowin/fastmcp)\n- [Product Hunt API Docs](https://www.producthunt.com/v2/docs)\n- [MCP Inspector](https://modelcontextprotocol.io/docs/tools/inspector)\n- [Awesome MCP Servers](https://github.com/punkpeye/awesome-mcp-servers)\n\n---\n\n## 📝 Notes\n\n- This project is not affiliated with Product Hunt.\n- The Product Hunt API is subject to change.\n\n---\n\n## 📜 License\n\nMIT",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "producthunt",
        "chatbots",
        "search",
        "producthunt mcp",
        "jaipandya producthunt",
        "chatbots product"
      ],
      "category": "web-search"
    },
    "jakeyShakey--umami_mcp_server": {
      "owner": "jakeyShakey",
      "name": "umami_mcp_server",
      "url": "https://github.com/jakeyShakey/umami_mcp_server",
      "imageUrl": "/freedevtools/mcp/pfp/jakeyShakey.webp",
      "description": "Connects Claude to Umami analytics data to analyze user behavior and track website performance. Provides tools for monitoring real-time visitor activity, capturing webpage content, and generating insights from historical data.",
      "stars": 6,
      "forks": 5,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-08-12T10:56:38Z",
      "readme_content": "# Umami Analytics MCP Server\n\nA Model Context Protocol (MCP) server that enhances Claude's capabilities by providing access to website analytics data from Umami. This server allows Claude to analyze user behavior, track website performance, and provide data-driven insights.\n\nThe codebase has been generated from start to finish using Claude Sonnet 3.5 and Cursor.\n\n\n\n## What It Does\n\nThis server connects Claude to your Umami analytics platform, enabling it to:\n- Analyze user journeys and behavior patterns\n- Track website performance metrics\n- Monitor real-time visitor activity\n- Capture and analyze webpage content\n- Generate insights from historical analytics data\n\n## How It Works\n\nThe server provides the following tools to Claude to analyze website data:\n\n### Available Tools\n- **get_websites**: Retrieve a list of websites and their IDs in your Umami account\n- **get_website_stats**: Get key metrics like pageviews, visitors, bounce rate for a website\n- **get_website_metrics**: Analyze specific metrics like URLs, referrers, browsers, countries\n- **get_pageview_series**: Get time-series pageview data with customizable intervals\n- **get_active_visitors**: Monitor current number of active visitors on a website\n- **get_session_ids**: Retrieve session IDs for specific events or time periods\n- **get_tracking_data**: Get detailed activity data for a specific session ID\n- **get_docs**: Perform semantic search on many user journeys, returning the most relevant chunks for a given question\n- **get_screenshot**: Capture visual snapshots of webpages\n- **get_html**: Retrieve and analyze webpage HTML source code\n\nEach tool has a description and a list of arguments that can be passed to it. These are used to provide context and information to allow Claude to effectively select the right tool/s for the job, and provide the correct paremeters.\n\nMost of these tools pull data directly from the Umami API into Claude Desktop, however get_docs adds in a semantic search step to avoid context window issues with Claude as well as saving on token usage. All of the user journeys for a given event are retrieved using the Umami API and then these are chunked into smaller sections and embedded using an open soruce sentence transformer model from hugging face. Then, based on the question, the most relevant chunks are retrieved and returned to Claude, allowing for analysis of specific actions and behaviours performed by users on the website, something hard to replicate with traditional data visualisation tools. The implementation of this embedding and semantic search is in the `src/analytics_service/embeddings.py` file.\n\nAdditoanlly, the get_screenshot and get_html tools use the open source [Crawl4AI](https://github.com/unclecode/crawl4ai) web crawler to retrieve the HTML source code and screenshot of a given website. The screenshots have to be downsampled to reduce their size to avoid context window issues with Claude. This allows you to provide context to Claude about how the website is structured and how it looks, allowing for more accurate and relevant recomendations on improving site performance. The implementation of the web crawler is in the `src/analytics_service/crawler.py` file.\n\n\n\n## Setup Guide\n\n### Prerequisites\n\n- install uv: `pip install uv`\n\n1. **Claude Desktop Configuration**\n   \n   Add the following to your Claude Desktop config file:\n   - MacOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\n   - Windows: `%APPDATA%/Claude/claude_desktop_config.json`\n\n   ```json\n   {\n     \"mcpServers\": {\n       \"analytics_service\": {\n         \"command\": \"uv\",\n         \"args\": [\n           \"--directory\",\n           \"/path/to/analytics_service\",\n           \"run\",\n           \"analytics-service\"\n         ],\n         \"env\": {\n          \"UMAMI_API_URL\": \"https://example.com\",\n          \"UMAMI_USERNAME\": \"yourUmamiUsername\",\n          \"UMAMI_PASSWORD\": \"yourUmamiPassword\", \n          \"UMAMI_TEAM_ID\": \"yourUmamiTeamId\"\n        }\n       }\n     }\n   }\n   ```\n   Replace `/path/to/analytics_service` with the actual path to your analytics_service directory.\n\n   For the UMAMI_API_URL, replace `https://example.com` with the URL of the version of Umami you are using (either self-hosted or hosted on Umami Cloud). \n   For the UMAMI_USERNAME and UMAMI_PASSWORD, replace `yourUmamiUsername` and `yourUmamiPassword` with your Umami account credentials.\n   For the UMAMI_TEAM_ID, replace `yourUmamiTeamId` with the ID of the team you want to analyse.\n\n2. **Opening Claude Desktop**\n\n  When you open Claude Desktop, it will automatically begin to connect to the analytics_service MCP server. Allow up to a few minutes as the server is initialised and the correct packages are installed. When the server is ready, you will see 10 MCP tools available in the bottom right of the chat window. This is signified with a small hammer icon and the number 10 next to it.\n\n  \n\n  Additionally, if you haven't already, it is strongly recommended that you enable the \"Analysis tool\" in Feature Preview within Claude Desktop. This will allow Claude to build you dashboards as well as other visualisations for your data. To do this, in the left-hand side panel, find the \"Feature Preview\" tab and in it, enable the \"Analysis tool\". LaTeX Rendering can also be enabled in the same section.\n\n  \n\n  ## How to Use the Server\n  ### Getting Started\n\n  The simplest way to start is to use the **Create Dashboard Prompt** provided by the server. This is selected by clicking the \"Attach from MCP\" attachment button at the bottom left of the chat window. Then selecting choose implementation and then the Create Dashboard Prompt.\n\n  \n\n  This will guide you through the process of creating a dashboard for your website, asking for:\n  1. The name of the website you want to analyze\n  2. The start and end dates for the analysis\n  3. The timezone of the website\n\n  Once you have provided this information, the server will generate a txt file instructing Claude how to build the dashboard.\n  Press enter in the chat window and Claude will do the rest. You can then ask Claude to make any changes to the dashboard or add any other visualisations.\n\n  \n\n  ### Natural Language Usage\n\n  For a more customisable experience, you can talk directly to Claude and specify your own requirements such as what data you want to see on the dashboard and what visualisations you want to use. Additionally you can analyse user journeys to pinpoint specific pain points and add screenshots from your site to give Claude extra context\n\n  The necessary tools to complete your request will be used by Claude automatically. Simply make your request in natural language and Claude will decide which tools to use. If you want to see a list of all the tools available, you can ask Claude to list them for you or click on the hammer icon in the bottom right of the chat window.\n  \n  \n  \n  ### Create your own prompts\n\n  You can also create your own prompts for workflows that you use regularly. To do this, you will need to:\n\n1. **Define Your Prompt Structure**\n   Create a prompt definition that includes:\n   - `name`: A unique identifier for your prompt\n   - `description`: A clear explanation of what the prompt does\n   - `arguments`: List of input parameters your prompt needs\n\n   Add this to the `list_prompts()` function in `src/analytics_service/server.py`:\n   \n   Example structure:\n   ```python\n   @app.list_prompts()\n   async def list_prompts():\n       return [\n           # ... existing prompts ...\n           {\n               \"name\": \"Your Prompt Name\",\n               \"description\": \"Your prompt description\",\n               \"arguments\": [\n                   {\n                       \"name\": \"Parameter Name 1\",\n                       \"description\": \"Parameter description\",\n                       \"required\": True/False\n                   },\n                   {\n                       \"name\": \"Parameter Name 2\",\n                       \"description\": \"Parameter description\",\n                       \"required\": True/False\n                   }\n               ]\n           }\n       ]\n   ```\n\n2. **Implement the Prompt**\n   Add your prompt handling logic in the `get_prompt()` function in `src/analytics_service/server.py`:\n   ```python\n   @app.get_prompt()\n   async def get_prompt(name: str, arguments: Any):\n    # ... existing prompts ...\n       if name == \"Your Prompt Name\":\n           return {\n               \"messages\": [\n                   {\n                       \"role\": \"user\",\n                       \"content\": {\n                           \"type\": \"text\",\n                           \"text\": f\"Your prompt template with {arguments['Parameter Name']}\"\n                       }\n                   }\n               ]\n           }\n   ```\n\n   When defining messages in your prompt, the `role` field is crucial for structuring the conversation:\n   - Use `\"role\": \"user\"` for messages that simulate user input or questions\n   - Use `\"role\": \"assistant\"` for messages that represent Claude's responses or instructions\n   - Use `\"role\": \"system\"` for messages that set context or provide high-level instructions\n   \n   The `content` field in each message must specify a `type`. Available types are:\n   - `\"type\": \"text\"` - For plain text content\n   - `\"type\": \"resource\"` - For including external resources like files, logs, or other data. Must include a `resource` object with:\n     - `uri`: The resource identifier\n     - `text`: The actual content\n     - `mimeType`: The MIME type of the content (e.g., \"text/plain\", \"text/x-python\")\n\n   While resources do include their content in the `text` field, using the `resource` type provides several important benefits:\n   1. **Content Type Awareness**: The `mimeType` field tells Claude how to interpret the content (e.g., as Python code, plain text, or other formats)\n   2. **Source Tracking**: The `uri` field maintains a reference to where the content came from, which can be useful for:\n      - Tracking the origin of data\n      - Enabling updates if the source changes\n      - Providing context about the resource's location and purpose\n   3. **Structured Data Handling**: The resource format allows for consistent handling of different types of content while maintaining metadata about each resource\n\n   Here's an example showing different roles and content types:\n   ```python\n   \"messages\": [\n       {\n           \"role\": \"system\",\n           \"content\": {\n               \"type\": \"text\",\n               \"text\": \"Analyze the following log file and code for potential issues.\"\n           }\n       },\n       {\n           \"role\": \"user\",\n           \"content\": {\n               \"type\": \"resource\",\n               \"resource\": {\n                   \"uri\": \"logs://recent\",\n                   \"text\": \"[2024-03-14 15:32:11] ERROR: Connection timeout\",\n                   \"mimeType\": \"text/plain\"\n               }\n           }\n       },\n       {\n           \"role\": \"assistant\",\n           \"content\": {\n               \"type\": \"text\",\n               \"text\": \"I notice a connection timeout error. Let me examine the related code.\"\n           }\n       },\n       {\n           \"role\": \"user\",\n           \"content\": {\n               \"type\": \"resource\",\n               \"resource\": {\n                   \"uri\": \"file:///code.py\",\n                   \"text\": \"def example():\\n    pass\",\n                   \"mimeType\": \"text/x-python\"\n               }\n           }\n       }\n   ]\n   ```\n\n   For most prompts, the text type with user role is more than sufficient and allows Claude more control and creativity in its responses. For more complex workflows however, multiple messages with different roles and types allow for a more structured conversation flow and more user control over the response.\n\n3. **Best Practices for Creating Prompts**\n   - Make your prompts focused and specific\n   - Include clear validation requirements for arguments\n   - Use descriptive names for parameters\n   - Include example values in parameter descriptions\n   - Structure the prompt template to guide Claude effectively\n   - Consider error handling and edge cases\n   - Test the prompt with various inputs",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "umami_mcp_server",
        "umami",
        "analytics",
        "umami analytics",
        "jakeyshakey umami_mcp_server",
        "umami_mcp_server connects"
      ],
      "category": "web-search"
    },
    "jamiew--spotify-mcp": {
      "owner": "jamiew",
      "name": "spotify-mcp",
      "url": "https://github.com/jamiew/spotify-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/jamiew.webp",
      "description": "Connect to Spotify for controlling playback and managing your music library, including playlist management support. Features include searching for tracks, getting information about music entities, and managing the playback queue.",
      "stars": 4,
      "forks": 0,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-22T07:47:05Z",
      "readme_content": "# spotify-mcp MCP server\n\n[![smithery badge](https://smithery.ai/badge/@jamiew/spotify-mcp)](https://smithery.ai/server/@jamiew/spotify-mcp)\n\nMCP server connecting Claude with Spotify. This fork of [varunneal/spotify-mcp](https://github.com/varunneal/spotify-mcp) adds smart-batching tools and advanced playlist features that optimize API usage.\n\n## Features\n\n### Core Functionality\n- **Playback Control**: Start, pause, skip tracks, manage queue\n- **Search & Discovery**: Find tracks, albums, artists, playlists with pagination  \n- **Real-time State**: Live user profile and playback status\n\n### Enhanced Playlist Tools (New in this fork)\n- **Smart Batch Operations**: Add/remove up to 100 tracks in single API calls\n- **Large Playlist Support**: Efficiently handle playlists with 1000+ tracks using pagination\n- **Advanced Playlist Management**: Create, modify details, bulk track operations\n- **API-Optimized Workflows**: Intelligent batching reduces API calls by 60-80%\n\n## Installation\n\n### 1. Get Spotify API Keys\n1. Create account at [developer.spotify.com](https://developer.spotify.com/)\n2. Create app with redirect URI: `http://localhost:8888`\n\n### 2. Install via Smithery (Recommended)\n```bash\nnpx -y @smithery/cli install @jamiew/spotify-mcp --client claude\n```\n\n### 3. Manual Installation\n```bash\ngit clone https://github.com/jamiew/spotify-mcp.git\n```\n\nAdd to Claude Desktop config (`~/Library/Application Support/Claude/claude_desktop_config.json`):\n```json\n\"spotify\": {\n  \"command\": \"uv\",\n  \"args\": [\"--directory\", \"/path/to/spotify_mcp\", \"run\", \"spotify-mcp\"],\n  \"env\": {\n    \"SPOTIFY_CLIENT_ID\": \"YOUR_CLIENT_ID\",\n    \"SPOTIFY_CLIENT_SECRET\": \"YOUR_CLIENT_SECRET\",\n    \"SPOTIFY_REDIRECT_URI\": \"http://localhost:8888\"\n  }\n}\n```\n\n**Requirements**: Spotify Premium account, `uv` >= 0.54\n\n## Usage Examples\n\n- **\"Create a chill study playlist with 20 tracks\"** → Search + playlist creation + bulk track addition\n- **\"Show me the first 50 tracks from my 'Liked Songs'\"** → Pagination for large playlists  \n- **\"Find similar artists to Radiohead and add their top tracks to my queue\"** → Search + artist info + queue management\n\n## Development\n\nBuilt with **FastMCP framework** featuring 13 focused tools, type-safe APIs, and comprehensive test coverage.\n\n**Debug with MCP Inspector:**\n```bash\nnpx @modelcontextprotocol/inspector uv --directory /path/to/spotify_mcp run spotify-mcp\n```\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "spotify",
        "playlist",
        "playback",
        "spotify mcp",
        "jamiew spotify",
        "spotify controlling"
      ],
      "category": "web-search"
    },
    "jasonjmcghee--WebMCP": {
      "owner": "jasonjmcghee",
      "name": "WebMCP",
      "url": "https://github.com/jasonjmcghee/WebMCP",
      "imageUrl": "/freedevtools/mcp/pfp/jasonjmcghee.webp",
      "description": "A protocol for websites to share tools, resources, and prompts with client-side LLMs, enhancing user interaction without the need for API keys. It allows multiple WebMCP-enabled sites to be connected for a richer user experience.",
      "stars": 59,
      "forks": 11,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-10-02T15:25:20Z",
      "readme_content": "# WebMCP\n\nA proposal and code for websites to support client side LLMs\n\n[![NPM Version](https://img.shields.io/npm/v/%40jason.today%2Fwebmcp)](https://www.npmjs.com/package/@jason.today/webmcp) [![MIT licensed](https://img.shields.io/npm/l/%40jason.today%2Fwebmcp)](./LICENSE)\n\nWebMCP allows websites to share tools, resources, prompts, etc. to LLMs. In other words, WebMCP allows a website to be an MCP server. No sharing API Keys. Use any model you want.\n\n[Here's a simple website I built that is WebMCP-enabled](https://webmcp.dev)\n\nIt comes in the form of a widget that a website owner can put on their site and expose tools to give client-side LLMs what they need to provide a great UX for the user or agent.\n\n_The look, feel, how it's used, and security are all absolutely open for contribution / constructive criticism. MCP Clients directly building WebMCP functionality seems like an ideal outcome._\n\nAn end-user can connect to any number of websites at a time - and tools are \"scoped\" (by name) based on the domain to simplify organization.\n\n### Super Quick Demo (20 seconds, Sound on 🔊)\n\nhttps://github.com/user-attachments/assets/61229470-1242-401e-a7d9-c0d762d7b519\n\n## Getting started (using your LLM with websites using WebMCP)\n\n#### Installation\n\nJust specify your MCP client (`claude`, `cursor`, `cline`, `windsurf`, or a path to json)\n\n```bash\nnpx -y @jason.today/webmcp@latest --config claude\n```\n\n_If you're interested in setting it up manually, use the command `npx -y @jason.today/webmcp@latest --mcp`._\n\n_Auto-install was inspired by Smithery, but their code is AGPL so I wrote this myself. If it doesn't work for you or you don't see your mcp client, please file an issue._\n\n#### Using WebMCP\n\nWhen you're ready to connect to a website, you can ask your model to generate you an mcp token.\n\nCopy the token and paste it to the website's input. As soon as the website registers with it, it's thrown away and cannot be used for subsequent registrations or anything else. The website will receive its own session token for making requests.\n\nIf you'd rather your model / service never see the token, you can manually execute `npx @jason.today/webmcp --new` instead.\n\nSome MCP clients, including Claude Desktop, need to be restarted to get access to new tools. (at least at time of writing)\n\nTo disconnect, you can close the browser tab, click \"disconnect\", or shut down the server with `npx @jason.today/webmcp -q`.\n\nAll configuration files are stored in `~/.webmcp` directory.\n\n## Getting started (adding WebMCP to your website)\n\nTo use WebMCP, simply include [`webmcp.js`](https://github.com/jasonjmcghee/WebMCP/releases) on your page (via src or directly):\n\n```\n<script src=\"webmcp.js\"></script>\n```\n\nThe WebMCP widget will automatically initialize and appear in the bottom right corner of your page. Clicking on it will ask for a webmcp token which the end-user will generate.\n\n### Full Demo (3 minutes)\n\nhttps://github.com/user-attachments/assets/43ad160a-846d-48ad-9af9-f6d537e78473\n\n## More Info About How It Works\n\nThe bridge between the MCP client and the website is a localhost-only (not accessible to requests outside your computer) websocket server. Because it is configured to allow requests from your local web browser, authentication / token exchange is required, in case you visit a website attempting to abuse this.\n\n_Ideally the web browser itself would have an explicit permission for this, like webcam or microphone use._\n\n1. The MCP client connects to the `/mcp` path using the server token from `.env` (auto-generated)\n2. The server generates a registration token (instigated via the built-in mcp tool by a model or the `--new` command)\n3. Web clients connect to the `/register` endpoint with this token and its domain.\n4. Web pages connect to their assigned channel based on their domain.\n5. When an LLM wants to use a tool / resource / prompt, the request flows from:\n   - MCP Client → MCP Server → WebSocket Server → Web Page with the tool / resource / prompt\n   - (similar for requesting a list of tools / resources / prompts)\n6. The web page performs the request (e.g. call tool) and sends the result back through the same path\n7. Multiple web pages can be connected simultaneously, each with their own set of tools and tokens\n8. The MCP client sees all tools as a unified list, with channel prefixes to avoid name collisions\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant MCP as MCP Client\n    participant Server as MCP Server\n    participant WS as WebSocket Server\n    participant Web as Website\n    \n    %% Initial connection\n    MCP->>Server: Connect to /mcp with internal server token\n    \n    %% Website registration token\n    User->>MCP: Request registration token\n    MCP->>Server: Request registration token\n    Server-->>MCP: Return registration token\n    MCP-->>User: Display registration token\n    \n    %% Website registration\n    User->>Web: Paste registration token\n    Web->>WS: Connect to /register with token & domain (registration token deleted)\n    WS-->>Web: Assign channel & session token\n    Web->>WS: Connect to assigned channel\n    \n    %% Tool interaction\n    MCP->>Server: Request tools list\n    Server->>WS: Forward request\n    WS->>Web: Request tools\n    Web-->>WS: Return tools list\n    WS-->>Server: Forward tools list\n    Server-->>MCP: Return tools list\n    \n    %% Tool execution\n    MCP->>Server: Tool request\n    Server->>WS: Forward request\n    WS->>Web: Execute tool\n    Web-->>WS: Return result\n    WS-->>Server: Forward result\n    Server-->>MCP: Return result\n    \n    %% Disconnection\n    User->>Web: Disconnect\n    Web->>WS: Close connection\n```\n\n## Security\n\nThis is a super early project. I'm very interested in hardening security to prevent malicious extensions etc. from being\nable to perform prompt injection attacks and similar. If you have constructive ideas, please reach out or open an issue.\n\n## Built in tools\n\n- Token generator (for connecting to WebMCP websites)\n- MCP Tool Definer (to simplify building the schema of a tool for use with MCP)\n  - You can ask for the javascript (if relevant) in a follow-up message for use with WebMCP\n\n## Docker\n\nThere is a `Dockerfile` specifically for Smithery deployment.\n\nIf you'd like to use docker to run the websocket server, I've added a `docker-compose.yml` for demonstration purposes.\n\nIf `--docker` is provided to the mcp client config alongside `--mcp`, it will assume the server is running. This will allow you to dockerize the main process (websocket server), and your mcp client will connect to your docker container via websocket. Similarly, websites will communicate with your docker container.\n",
      "npm_url": "https://www.npmjs.com/package/webmcp",
      "npm_downloads": 126,
      "keywords": [
        "webmcp",
        "web",
        "client",
        "webmcp protocol",
        "webmcp enabled",
        "jasonjmcghee webmcp"
      ],
      "category": "web-search"
    },
    "jayanthbagare--mcp-scholarly": {
      "owner": "jayanthbagare",
      "name": "mcp-scholarly",
      "url": "https://github.com/jayanthbagare/mcp-scholarly",
      "imageUrl": "/freedevtools/mcp/pfp/jayanthbagare.webp",
      "description": "Search for accurate academic articles by keyword to facilitate research workflows. Integrates with applications to provide seamless retrieval of scholarly papers.",
      "stars": 0,
      "forks": 0,
      "license": "MIT License",
      "language": "",
      "updated_at": "2025-05-06T10:07:43Z",
      "readme_content": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/adityak74-mcp-scholarly-badge.png)](https://mseep.ai/app/adityak74-mcp-scholarly)\n\n# mcp-scholarly MCP server\n[![smithery badge](https://smithery.ai/badge/mcp-scholarly)](https://smithery.ai/server/mcp-scholarly)\n\nA MCP server to search for accurate academic articles. More scholarly vendors will be added soon.\n\n\n\n![image](https://github.com/user-attachments/assets/13202184-bc12-4530-b7c1-2ee698f3e1cc)\n\n<a href=\"https://glama.ai/mcp/servers/aq05b2p0ql\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/aq05b2p0ql/badge\" alt=\"Scholarly Server MCP server\" /></a>\n\n![star-history-202551](https://github.com/user-attachments/assets/f22d5796-017c-4c53-b230-101a09a28118)\n\n\n## Components\n\n### Tools\n\nThe server implements one tool:\n- search-arxiv: Search arxiv for articles related to the given keyword.\n  - Takes \"keyword\" as required string arguments\n\n## Quickstart\n\n### Install\n\n#### Claude Desktop\n\nOn MacOS: `~/Library/Application\\ Support/Claude/claude_desktop_config.json`\nOn Windows: `%APPDATA%/Claude/claude_desktop_config.json`\n\n<details>\n  <summary>Development/Unpublished Servers Configuration</summary>\n  ```\n  \"mcpServers\": {\n    \"mcp-scholarly\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"--directory\",\n        \"/Users/adityakarnam/PycharmProjects/mcp-scholarly/mcp-scholarly\",\n        \"run\",\n        \"mcp-scholarly\"\n      ]\n    }\n  }\n  ```\n</details>\n\n<details>\n  <summary>Published Servers Configuration</summary>\n  ```\n  \"mcpServers\": {\n    \"mcp-scholarly\": {\n      \"command\": \"uvx\",\n      \"args\": [\n        \"mcp-scholarly\"\n      ]\n    }\n  }\n  ```\n</details>\n\nor if you are using Docker\n\n<details>\n  <summary>Published Docker Servers Configuration</summary>\n  ```\n  \"mcpServers\": {\n    \"mcp-scholarly\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\", \"--rm\", \"-i\",\n        \"mcp/scholarly\"\n      ]\n    }\n  }\n  ```\n</details>\n\n### Installing via Smithery\n\nTo install mcp-scholarly for Claude Desktop automatically via [Smithery](https://smithery.ai/server/mcp-scholarly):\n\n```bash\nnpx -y @smithery/cli install mcp-scholarly --client claude\n```\n\n## Development\n\n### Building and Publishing\n\nTo prepare the package for distribution:\n\n1. Sync dependencies and update lockfile:\n```bash\nuv sync\n```\n\n2. Build package distributions:\n```bash\nuv build\n```\n\nThis will create source and wheel distributions in the `dist/` directory.\n\n3. Publish to PyPI:\n```bash\nuv publish\n```\n\nNote: You'll need to set PyPI credentials via environment variables or command flags:\n- Token: `--token` or `UV_PUBLISH_TOKEN`\n- Or username/password: `--username`/`UV_PUBLISH_USERNAME` and `--password`/`UV_PUBLISH_PASSWORD`\n\n### Debugging\n\nSince MCP servers run over stdio, debugging can be challenging. For the best debugging\nexperience, we strongly recommend using the [MCP Inspector](https://github.com/modelcontextprotocol/inspector).\n\n\nYou can launch the MCP Inspector via [`npm`](https://docs.npmjs.com/downloading-and-installing-node-js-and-npm) with this command:\n\n```bash\nnpx @modelcontextprotocol/inspector uv --directory /Users/adityakarnam/PycharmProjects/mcp-scholarly/mcp-scholarly run mcp-scholarly\n```\n\n\nUpon launching, the Inspector will display a URL that you can access in your browser to begin debugging.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "scholarly",
        "search",
        "research",
        "scholarly search",
        "retrieval scholarly",
        "scholarly papers"
      ],
      "category": "web-search"
    },
    "jbdamask--mcp-nih-reporter": {
      "owner": "jbdamask",
      "name": "mcp-nih-reporter",
      "url": "https://github.com/jbdamask/mcp-nih-reporter",
      "imageUrl": "/freedevtools/mcp/pfp/jbdamask.webp",
      "description": "Search for NIH-funded research projects and publications using various criteria such as fiscal years, investigator names, organization details, and funding information. Retrieve detailed project information, including abstracts, in a conversational format.",
      "stars": 0,
      "forks": 2,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-03-19T10:49:39Z",
      "readme_content": "# NIH RePORTER MCP\n\nA Model Context Protocol [(MCP)](https://modelcontextprotocol.io/introduction) server for chatting with [NIH RePORTER](https://reporter.nih.gov/). Search for NIH-funded research projects and publications in a conversational manner.\nAccompanying blog post [here](https://open.substack.com/pub/johndamask/p/building-an-mcp-server-over-nihs?r=2ee1b&utm_campaign=post&utm_medium=web&showWelcomeOnShare=true).\n\n\n\n\n## Features\n\n- Search NIH-funded research projects with various criteria:\n  - Fiscal years\n  - Principal Investigator names\n  - Organization details (name, state, city, type, department)\n  - Funding amounts\n  - COVID-19 response status\n  - Funding mechanism\n  - Institute/Center codes\n  - RCDC terms\n  - Date ranges\n- Search publications associated with NIH projects\n- Combined search functionality for both projects and publications\n- Detailed project and publication information including abstracts\n- Configurable result limits\n\n## Prerequisites\n\n- Python 3.12 or higher\n- UV package manager (recommended for faster dependency installation)\n\n## Installation\n\n1. Clone the repository:\n```bash\ngit clone <repository-url>\ncd mcp-nih-reporter\n```\n\n2. Create and activate a virtual environment:\n```bash\npython -m venv .venv\nsource .venv/bin/activate  # On Windows, use `.venv\\Scripts\\activate`\n```\n\n3. Install dependencies using UV:\n```bash\nuv pip install -e .\n```\n## Usage\n\nThis MCP server provides access to the NIH RePORTER API through several tools:\n\n- `search_projects`: Search for NIH-funded research projects\n- `search_publications`: Search for publications associated with NIH projects\n- `search_combined`: Combined search for both projects and publications\n- `test_connection`: Test the API connection\n\nYou can use this MCP with any MCP-compatible client, such as:\n- Claude Desktop\n- Cursor\n- Other MCP-enabled tools\n\n### Example claude_desktop_config.json\n```\n{\n  \"mcpServers\": {\n\t \"nih-reporter\": {\n\t      \"command\": \"<fully qualified path to>/uv\",\n\t      \"args\": [\n\t        \"run\",\n\t        \"--with\",\n\t        \"mcp[cli]\",\n\t        \"mcp\",\n\t        \"run\",\n\t        \"<fully qualified path to>/mcp-nih-reporter/mcp-nih-reporter.py\"\n\t      ]\n\t    }\n  }\n}\n```\n\nThe search results will be returned in a structured format containing project details including:\n- Project title and abstract\n- Principal Investigator information\n- Organization details\n- Funding information\n- Project dates and status\n\n## Debugging\n\nA log file will be created in the root folder when the MCP attempts to run in a client (e.g. Claude Desktop). Check there if you're having trouble.\n\n## Development\n\nThe project uses:\n- `httpx` for async HTTP requests\n- `mcp` for the Mission Control Protocol implementation\n- `python-dotenv` for environment variable management\n- `uv` for dependency management\n\n## Logging\n\nLogs are written to `mcp-nih-reporter.log` in the project root directory. The logging level is set to INFO by default.\n\n## Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request. For major changes, please open an issue first to discuss what you would like to change.\n\n1. Fork the repository\n2. Create your feature branch (`git checkout -b feature/amazing-feature`)\n3. Commit your changes (`git commit -m 'Add some amazing feature'`)\n4. Push to the branch (`git push origin feature/amazing-feature`)\n5. Open a Pull Request\n\nPlease make sure to update tests as appropriate and follow the existing code style.\n\n## License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "research",
        "projects",
        "publications",
        "nih funded",
        "funded research",
        "search nih"
      ],
      "category": "web-search"
    },
    "jedrazb--elastic-semantic-search-mcp-server": {
      "owner": "jedrazb",
      "name": "elastic-semantic-search-mcp-server",
      "url": "https://github.com/jedrazb/elastic-semantic-search-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/jedrazb.webp",
      "description": "Enables semantic search capabilities for blog posts indexed in Elasticsearch, facilitating efficient content crawling and indexing to enhance search functionality and retrieval of relevant information.",
      "stars": 3,
      "forks": 6,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-06-08T19:17:13Z",
      "readme_content": "# MCP Server: Elasticsearch semantic search tool\n\nDemo repo for: https://j.blaszyk.me/tech-blog/mcp-server-elasticsearch-semantic-search/\n\n## Table of Contents\n- [Overview](#overview)\n- [Running the MCP Server](#running-the-mcp-server)\n- [Integrating with Claude Desktop](#integrating-with-claude-desktop)\n- [Crawling Search Labs Blog Posts](#crawling-search-labs-blog-posts)\n  - [1. Verify Crawler Setup](#1-verify-crawler-setup)\n  - [2. Configure Elasticsearch](#2-configure-elasticsearch)\n  - [3. Update Index Mapping for Semantic Search](#3-update-index-mapping-for-semantic-search)\n  - [4. Start Crawling](#4-start-crawling)\n  - [5. Verify Indexed Documents](#5-verify-indexed-documents)\n\n---\n\n## Overview\nThis repository provides a **Python implementation of an MCP server** for **semantic search** through **Search Labs blog posts** indexed in **Elasticsearch**.\n\nIt assumes you've crawled the blog posts and stored them in the `search-labs-posts` index using **Elastic Open Crawler**.\n\n---\n\n## Running the MCP Server\n\nAdd `ES_URL` and `ES_AP_KEY` into `.env` file, (take a look [here](#2-configure-elasticsearch) for generating api key with minimum permissions)\n\nStart the server in **MCP Inspector**:\n\n```sh\nmake dev\n```\n\nOnce running, access the MCP Inspector at: [http://localhost:5173](http://localhost:5173)\n\n---\n\n## Integrating with Claude Desktop\n\nTo add the MCP server to **Claude Desktop**:\n\n```sh\nmake install-claude-config\n```\n\nThis updates `claude_desktop_config.json` in your home directory. On the next restart, the Claude app will detect the server and load the declared tool.\n\n---\n\n## Crawling Search Labs Blog Posts\n\n### 1. Verify Crawler Setup\nTo check if the **Elastic Open Crawler** works, run:\n\n```sh\ndocker run --rm \\\n  --entrypoint /bin/bash \\\n  -v \"$(pwd)/crawler-config:/app/config\" \\\n  --network host \\\n  docker.elastic.co/integrations/crawler:latest \\\n  -c \"bin/crawler crawl config/test-crawler.yml\"\n```\n\nThis should print crawled content from a **single page**.\n\n---\n\n### 2. Configure Elasticsearch\nSet up **Elasticsearch URL and API Key**.\n\nGenerate an API key with **minimum crawler permissions**:\n\n```sh\nPOST /_security/api_key\n{\n  \"name\": \"crawler-search-labs\",\n  \"role_descriptors\": {\n    \"crawler-search-labs-role\": {\n      \"cluster\": [\"monitor\"],\n      \"indices\": [\n        {\n          \"names\": [\"search-labs-posts\"],\n          \"privileges\": [\"all\"]\n        }\n      ]\n    }\n  },\n  \"metadata\": {\n    \"application\": \"crawler\"\n  }\n}\n```\n\nCopy the `encoded` value from the response and set it as `API_KEY`.\n\n---\n\n### 3. Update Index Mapping for Semantic Search\n\nEnsure the `search-labs-posts` index exists. If not, create it:\n\n```sh\nPUT search-labs-posts\n```\n\nUpdate the **mapping** to enable **semantic search**:\n\n```sh\nPUT search-labs-posts/_mappings\n{\n  \"properties\": {\n    \"body\": {\n      \"type\": \"text\",\n      \"copy_to\": \"semantic_body\"\n    },\n    \"semantic_body\": {\n      \"type\": \"semantic_text\",\n      \"inference_id\": \".elser-2-elasticsearch\"\n    }\n  }\n}\n```\n\nThe `body` field is indexed as **semantic text** using **Elasticsearch’s ELSER model**.\n\n---\n\n### 4. Start Crawling\n\nRun the crawler to populate the index:\n\n```sh\ndocker run --rm \\\n  --entrypoint /bin/bash \\\n  -v \"$(pwd)/crawler-config:/app/config\" \\\n  --network host \\\n  docker.elastic.co/integrations/crawler:latest \\\n  -c \"bin/crawler crawl config/elastic-search-labs-crawler.yml\"\n```\n> [!TIP]\n> **If using a fresh Elasticsearch cluster**, wait for the **ELSER model** to start before indexing.\n\n---\n\n### 5. Verify Indexed Documents\nCheck if the documents were indexed:\n\n```sh\nGET search-labs-posts/_count\n```\n\nThis will return the total document count in the index. You can also verify in **Kibana**.\n\n---\n\n **Done!** You can now perform **semantic searches** on **Search Labs blog posts**\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "elasticsearch",
        "blog",
        "indexing",
        "semantic search",
        "indexed elasticsearch",
        "elasticsearch facilitating"
      ],
      "category": "web-search"
    },
    "jeong-sik--kakao-api-mcp-server": {
      "owner": "jeong-sik",
      "name": "kakao-api-mcp-server",
      "url": "https://github.com/jeong-sik/kakao-api-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/jeong-sik.webp",
      "description": "Provides access to Kakao Map and Daum Search APIs for fetching location data, performing place searches, converting coordinates to addresses, and finding routes. Enables integration of various search functionalities and mapping features through a standardized interface.",
      "stars": 14,
      "forks": 2,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-09-04T05:55:11Z",
      "readme_content": "# 카카오 API MCP 서버\n\n카카오맵 API 및 Daum 검색 API를 [Model Context Protocol](https://github.com/anthropics/model-context-protocol)(MCP)을 통해 활용할 수 있는 서버입니다. 이 서버를 통해 AI 모델이 카카오맵의 지도 관련 기능과 Daum의 다양한 검색 기능을 활용할 수 있습니다.\n\n## 주의 사항\n\n*   **카카오 로그인, 카카오톡 메시지 보내기 등 사용자 계정 관련 기능은 포함되어 있지 않습니다.** 이 서버는 공개된 카카오 및 Daum의 Open API만을 사용합니다.\n*   본 서버를 사용하기 위해서는 유효한 **카카오 REST API 키**가 필요합니다. [카카오 디벨로퍼스](https://developers.kakao.com/)에서 앱을 생성하고 REST API 키를 발급받으세요.\n\n## 주요 기능\n\n### 카카오맵 API\n\n1. **장소 검색 (`mcp_kakao_map_search_places`)**\n   - 키워드로 카카오맵에서 장소를 검색\n   - 위치, 카테고리, 연락처 정보 제공\n\n2. **좌표-주소 변환 (`mcp_kakao_map_coord_to_address`)**\n   - 경위도 좌표를 실제 주소로 변환\n   - 도로명 주소 및 지번 주소 정보 제공\n\n3. **길찾기 (`mcp_kakao_map_find_route`)**\n   - 출발지에서 목적지까지의 경로 검색\n   - 거리, 소요 시간, 예상 택시 요금 등 제공\n   - 교통 상황 정보 포함 (선택적)\n\n### Daum 검색 API\n\n1. **웹 문서 검색 (`mcp_kakao_map_search_web`)**\n   - 키워드로 다음 웹 문서 검색\n   - 페이지 정렬 및 검색 결과 개수 조정 가능\n\n2. **이미지 검색 (`mcp_kakao_map_search_image`)**\n   - 키워드로 다음 이미지 검색\n   - 이미지 URL 및 관련 정보 제공\n\n3. **블로그 검색 (`mcp_kakao_map_search_blog`)**\n   - 키워드로 다음 블로그 글 검색\n   - 블로그 이름, 포스트 제목, 내용 요약 제공\n\n4. **카페 검색 (`mcp_kakao_map_search_cafe`)**\n   - 키워드로 다음 카페 글 검색\n   - 카페 이름, 게시물 제목, 내용 요약 제공\n\n## 도구 사용 예시 (MCP)\n\n아래는 MCP 클라이언트(예: AI 모델)가 이 서버의 도구를 호출하는 방법과 예상되는 응답 형식입니다.\n\n### 카카오맵 API\n\n#### 1. 장소 검색 (`mcp_kakao_map_search_places`)\n\n**호출 (Request):**\n```json\n{\n  \"tool_name\": \"mcp_kakao_map_search_places\",\n  \"parameters\": {\n    \"keyword\": \"판교역 현대백화점\"\n  }\n}\n```\n\n**응답 (Response - 예시):**\n```json\n{\n  \"tool_name\": \"mcp_kakao_map_search_places\",\n  \"result\": \"장소 검색 결과 (결과 수: 15, 총 18개):\\n이름: 현대백화점 판교점\\n주소: 경기 성남시 분당구 백현동 541\\n카테고리: 쇼핑,유통 > 백화점 > 현대백화점\\n전화번호: 031-5170-2233\\n상세정보: http://place.map.kakao.com/18757447\\n---\\n... (추가 결과)\"\n}\n```\n\n#### 2. 좌표-주소 변환 (`mcp_kakao_map_coord_to_address`)\n\n**호출 (Request):**\n```json\n{\n  \"tool_name\": \"mcp_kakao_map_coord_to_address\",\n  \"parameters\": {\n    \"x\": 127.1120278,\n    \"y\": 37.3955833\n  }\n}\n```\n\n**응답 (Response - 예시):**\n```json\n{\n  \"tool_name\": \"mcp_kakao_map_coord_to_address\",\n  \"result\": \"주소 변환 결과:\\n도로명: 경기 성남시 분당구 판교역로146번길 20\\n지번: 경기 성남시 분당구 백현동 535\"\n}\n```\n\n#### 3. 길찾기 (`mcp_kakao_map_find_route`)\n\n**호출 (Request):**\n```json\n{\n  \"tool_name\": \"mcp_kakao_map_find_route\",\n  \"parameters\": {\n    \"origin\": \"판교역\",\n    \"destination\": \"정자역\",\n    \"transportation_type\": \"car\",\n    \"traffic_info\": true\n  }\n}\n```\n\n**응답 (Response - 예시):**\n```json\n{\n  \"tool_name\": \"mcp_kakao_map_find_route\",\n  \"result\": \"🗺️ 길찾기 결과\\n\\n출발지: 판교역 신분당선 (경기 성남시 분당구 삼평동)\\n\\n목적지: 정자역 신분당선 (경기 성남시 분당구 정자동)\\n\\n이동 수단: 자동차\\n\\n총 거리: 3.6km\\n예상 소요 시간: 10분\\n예상 택시 요금: 5,600원\\n\\n📊 교통 상황 요약:\\n\\n카카오맵에서 보기: https://map.kakao.com/?sName=%ED%8C%90%EA%B5%90%EC%97%AD&eName=%EC%A0%95%EC%9E%90%EC%97%AD\\n\"\n}\n```\n\n### Daum 검색 API\n\n#### 1. 웹 문서 검색 (`mcp_kakao_map_search_web`)\n\n**호출 (Request):**\n```json\n{\n  \"tool_name\": \"mcp_kakao_map_search_web\",\n  \"parameters\": {\n    \"query\": \"카카오브레인 칼로\",\n    \"size\": 2\n  }\n}\n```\n\n**응답 (Response - 예시):**\n```json\n{\n  \"tool_name\": \"mcp_kakao_map_search_web\",\n  \"result\": \"웹 문서 검색 결과 (총 2083개 중 2개 표시):\\n\\n1. 카카오브레인 | 칼로 Karlo\\n   내용: 카카오브레인의 이미지 생성 모델 Karlo는 사용자가 입력한 문장(Text)을 이해하여, 세상에 단 하나뿐인 이미지를 만들어내는 인공지능 화가입니다. 수백만 장 규모의...\\n   URL: https://kakaobrain.com/karlo\\n   날짜: 2024. 1. 1.\\n\\n2. 카카오브레인, AI 아티스트 '칼로 2.0' 공개 - 테크레시피\\n   내용: 카카오브레인이 초거대 인공지능(AI) 이미지 생성 모델 '칼로(Karlo) 2.0'을 공개했다고 11일 밝혔다. 칼로 2.0은 약 3억 장 규모의 텍스트-이미지 데이터셋을 학습한 모델이...\\n   URL: https://techrecipe.co.kr/posts/56513\\n   날짜: 2023. 7. 11.\\n\\n현재 페이지가 마지막 페이지가 아닙니다. 더 많은 결과를 보려면 page 매개변수를 증가시키세요.\\n\"\n}\n```\n\n#### 2. 이미지 검색 (`mcp_kakao_map_search_image`)\n\n**호출 (Request):**\n```json\n{\n  \"tool_name\": \"mcp_kakao_map_search_image\",\n  \"parameters\": {\n    \"query\": \"고양이\",\n    \"size\": 1\n  }\n}\n```\n\n**응답 (Response - 예시):**\n```json\n{\n  \"tool_name\": \"mcp_kakao_map_search_image\",\n  \"result\": \"이미지 검색 결과 (총 8715385개 중 1개 표시):\\n\\n1. 컬렉션 이름: Daum 백과\\n   문서 URL: http://100.daum.net/encyclopedia/view/172XX61300001\\n   이미지 URL: https://t1.daumcdn.net/thumb/R1024x0/?fname=http%3A%2F%2Ft1.daumcdn.net%2Fencyclop%2F172%2F613%2F172XX61300001\\n   썸네일 URL: https://search1.kakaocdn.net/thumb/R100x100/?fname=http%3A%2F%2Ft1.daumcdn.net%2Fencyclop%2F172%2F613%2F172XX61300001&token=1579057346066cfd0b2e0c671d07c433\\n   크기: 가로 1024px, 세로 682px\\n   표시 URL: 100.daum.net\\n   날짜: 2014. 11. 6.\\n\\n현재 페이지가 마지막 페이지가 아닙니다. 더 많은 결과를 보려면 page 매개변수를 증가시키세요.\\n\"\n}\n```\n\n#### 3. 블로그 검색 (`mcp_kakao_map_search_blog`)\n\n**호출 (Request):**\n```json\n{\n  \"tool_name\": \"mcp_kakao_map_search_blog\",\n  \"parameters\": {\n    \"query\": \"판교 맛집\",\n    \"size\": 1\n  }\n}\n```\n\n**응답 (Response - 예시):**\n```json\n{\n  \"tool_name\": \"mcp_kakao_map_search_blog\",\n  \"result\": \"블로그 검색 결과 (총 215893개 중 1개 표시):\\n\\n1. 블로그명: 짱돌의 일상다반사\\n   제목: 판교 맛집 추천 | 유스페이스몰 가성비 좋은 점심 맛집\\n   내용: 판교테크노밸리 유스페이스몰은 늘 점심시간마다 직장인들로 인산인해를 이루는 곳이다. 오늘은 판교 점심 맛집으로 괜찮은 곳 두 군데를 소개해 본다. 1.... \\n   URL: http://jdcamping.tistory.com/1374\\n   썸네일: https://search2.kakaocdn.net/thumb/R180x180/?fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FcQv0tX%2FbtrOfR4oUu3%2FdKQGkK0kY6kKk40f4kYkYK%2Fimg.jpg&token=1c251bb24ae4bb01657303012e2641ac\\n   날짜: 2024. 12. 17.\\n\\n현재 페이지가 마지막 페이지가 아닙니다. 더 많은 결과를 보려면 page 매개변수를 증가시키세요.\\n\"\n}\n```\n\n#### 4. 카페 검색 (`mcp_kakao_map_search_cafe`)\n\n**호출 (Request):**\n```json\n{\n  \"tool_name\": \"mcp_kakao_map_search_cafe\",\n  \"parameters\": {\n    \"query\": \"코딩 스터디\",\n    \"size\": 1\n  }\n}\n```\n\n**응답 (Response - 예시):**\n```json\n{\n  \"tool_name\": \"mcp_kakao_map_search_cafe\",\n  \"result\": \"카페 검색 결과 (총 18335개 중 1개 표시):\\n\\n1. 카페명: 독취사-취업,대학생,대기업,공기업,NCS,인적성,취업카페\\n   제목: [스터디] 웹개발/코딩 기초 스터디 구해요\\n   내용: 안녕하세요! 웹개발 및 코딩 기초를 함께 공부할 스터디원을 모집합니다. 현재 2명이며, 최대 4명까지 생각하고 있습니다. 장소는 주로 강남/사당에서 진행하고, 온라...\\n   URL: http://cafe.daum.net/breakjob/DldL/12345\\n   썸네일: https://search1.kakaocdn.net/thumb/P180x180/?fname=https%3A%2F%2Ft1.daumcdn.net%2Fcafe_image%2F%2Fconfig%2Fimg_default_profile%3Fver%3D1&token=de43b9d06222d0a2192f9f70fcb0f134\\n   날짜: 2025. 3. 28.\\n\\n현재 페이지가 마지막 페이지가 아닙니다. 더 많은 결과를 보려면 page 매개변수를 증가시키세요.\\n\"\n}\n```\n\n## 설치 및 설정\n\n1. **저장소 복제 및 종속성 설치:**\n```bash\ngit clone https://github.com/yousleepwhen/kakao-api-mcp-server.git # 저장소 URL을 실제 URL로 변경해주세요\ncd kakao-api-mcp-server\nyarn install\n```\n*   이 프로젝트는 `yarn` 패키지 매니저 사용을 권장합니다.\n\n2. **카카오 REST API 키 설정:**\n   - 프로젝트 루트 디렉토리에 `.env` 파일을 생성합니다.\n   - `.env` 파일 안에 다음과 같이 카카오 디벨로퍼스에서 발급받은 REST API 키를 입력합니다:\n     ```\n     KAKAO_REST_API_KEY=여기에_카카오_REST_API_키_입력\n     ```\n   - 또는, 서버 실행 시 `--kakao-api-key` 인자를 통해 직접 전달할 수도 있습니다.\n\n## 실행 방법\n\n서버를 실행하기 전에 코드를 빌드해야 합니다. `start` 관련 스크립트에 빌드 과정이 포함되어 있으므로 별도로 `yarn build`를 실행할 필요는 없습니다.\n\n### HTTP 모드 (기본)\n\n다른 서비스나 도구와 HTTP를 통해 통신할 때 사용합니다.\n\n```bash\nyarn start\n```\n\n기본적으로 3000번 포트를 사용합니다. 포트를 변경하려면 `--port` 인자를 사용하세요:\n\n```bash\nyarn start --port 8080\n```\n\n### stdio 모드\n\n터미널의 표준 입출력(stdin/stdout)을 통해 MCP 메시지를 주고받을 때 사용합니다.\n\n```bash\nyarn start:stdio\n```\n\n### 개발 모드\n\n개발 중 코드 변경 시 자동으로 빌드하고 서버를 재시작하려면 (nodemon 등 별도 설정 필요) `dev` 스크립트를 활용할 수 있습니다. 현재 `dev` 스크립트는 `start`와 동일하게 동작합니다.\n\n```bash\nyarn dev\n```\n\n## 라이선스\n\n이 프로젝트는 MIT 라이선스 하에 배포됩니다.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "kakao",
        "searches",
        "search",
        "kakao api",
        "kakao map",
        "search apis"
      ],
      "category": "web-search"
    },
    "jerpint--paperpal": {
      "owner": "jerpint",
      "name": "paperpal",
      "url": "https://github.com/jerpint/paperpal",
      "imageUrl": "/freedevtools/mcp/pfp/jerpint.webp",
      "description": "Enhance literature review processes by enabling searches and discussions about machine learning papers from arXiv and Hugging Face. Facilitate natural conversations with language models to discover new research and organize ideas effectively.",
      "stars": 10,
      "forks": 3,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-17T07:29:54Z",
      "readme_content": "🚨 Development has moved to https://github.com/milatechtransfer/paperpal\n\n# paperpal\n\nMCP Extension to aid you in searching and writing literature reviews\n\n> Check out this [conversation with Claude](https://claude.ai/share/0572fbd9-3ba2-4143-9f7f-5cae205c6d0d) to see what it can do\n\n## How it works\n\n`paperpal` gives your LLMs access to [arxiv](https://www.arxiv.org) and [Hugging Face papers](https://huggingface.co/papers).\nYou can then have a natural conversation with your favourite LLMs (e.g. Claude) and have it guide you.\n\nYou can:\n\n* Discuss papers\n* Look for new papers\n* Organize ideas for liteature reviews\n* etc.\n\nOf course, this tool is as good as the sum of its parts. LLMs can still hallucinate, and semantic search is never perfect.\n\n## Quickstart\n\nThere are many different ways with which you can interact with an MCP server.\n\n### Claude Desktop App\n\n> If this is your first time using an MCP server for Claude Desktop App, see https://modelcontextprotocol.io/quickstart/user\n\nFirst, clone this repository locally:\n\n    git clone https://github.com/jerpint/paperpal\n\nNext, add the extension to your app. Open your configuration file (on macOS this should be `~/Library/Application Support/Claude/claude_desktop_config.json`) and and add the following to the extension:\n\nFor example on MacOS:\n\n```python\n{\n  \"mcpServers\": {\n    \"paperpal\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"--directory\",\n        \"/Users/<username>/paperpal\",\n        \"run\",\n        \"paperpal.py\"\n      ]\n    }\n  }\n}\n```\n\nRestart your Claude Desktop App and you should see it appear.\n\n\n### Cursor\n\n> If this is your first time using an MCP server for Cursor, see https://docs.cursor.com/context/model-context-protocol#remote-development\n\nFirst, clone this repository locally:\n\n    git clone https://github.com/jerpint/paperpal\n\n\nAdd this to the root of the project in a `.cursor/mcp.json` file:\n\n```\n{\n  \"mcpServers\": {\n    \"paperpal\": {\n      \"command\": \"/Users/jeremypinto/.cargo/bin/uv\",\n      \"args\": [\n        \"--directory\",\n        \"/Users/jeremypinto/paperpal\",\n        \"run\",\n        \"paperpal.py\"\n      ]\n    }\n  }\n}\n```\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "paperpal",
        "searches",
        "research",
        "searches discussions",
        "learning papers",
        "jerpint paperpal"
      ],
      "category": "web-search"
    },
    "jharkins--mcp-time-srv": {
      "owner": "jharkins",
      "name": "mcp-time-srv",
      "url": "https://github.com/jharkins/mcp-time-srv",
      "imageUrl": "/freedevtools/mcp/pfp/jharkins.webp",
      "description": "Provides accurate current time information and timezone conversions. Queries can be made for the current time in any IANA timezone and for converting times between different timezones.",
      "stars": 0,
      "forks": 1,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-04-26T02:01:48Z",
      "readme_content": "# MCP Time Server (`mcp-time-srv`)\n\nA simple Model Context Protocol (MCP) server implemented in TypeScript that provides tools for time-related queries, including getting the current time in various timezones and converting times between timezones.\n\nThis server supports both modern Streamable HTTP and legacy HTTP+SSE MCP transport protocols.\n\n## Features\n\nProvides the following MCP tools:\n\n*   **`get_current_time`**: Returns the current time in a specified IANA timezone.\n*   **`convert_time`**: Converts a given time from a source IANA timezone to a target IANA timezone.\n\n## Prerequisites\n\n*   [Node.js](https://nodejs.org/) (v18 or later recommended)\n*   [npm](https://www.npmjs.com/) (usually comes with Node.js)\n*   [Docker](https://www.docker.com/) (Optional, for running in a container)\n\n## Setup\n\n1.  **Clone the repository (if you haven't already):**\n    ```bash\n    # git clone <your-repo-url>\n    # cd mcp-time-srv\n    ```\n\n2.  **Install dependencies:**\n    ```bash\n    npm install\n    ```\n\n## Running Locally\n\n1.  **Build the TypeScript code:**\n    ```bash\n    npm run build\n    ```\n    This compiles the TypeScript source in `src/` to JavaScript in `dist/`.\n\n2.  **Run the server:**\n    You can run the server using `ts-node` (for development) or directly with `node` after building.\n\n    *   **Using `ts-node`:**\n        ```bash\n        npx ts-node src/server.ts\n        ```\n    *   **Using `node` (after building):**\n        ```bash\n        node dist/server.js\n        ```\n\nThe server will start, typically listening on port 3000.\n```\nMCP Time server listening on http://localhost:3000\n```\n\n## Running with Docker (Optional)\n\nA `Dockerfile` is provided for building and running the server in a container.\n\n1.  **Build the Docker image:**\n    ```bash\n    docker build -t mcp-time-srv .\n    ```\n\n2.  **Run the container:**\n    ```bash\n    docker run -d -p 3000:3000 --name my-mcp-server mcp-time-srv\n    ```\n    *   `-d`: Run in detached mode (in the background).\n    *   `-p 3000:3000`: Map port 3000 on your host to port 3000 in the container.\n    *   `--name my-mcp-server`: Assign a name to the container for easier management.\n\nThe server will be running inside the container, accessible at `http://localhost:3000`.\n\nTo stop the container:\n```bash\ndocker stop my-mcp-server\n```\nTo view logs:\n```bash\ndocker logs my-mcp-server\n```\n\n## Testing with the Client\n\nA simple test client script (`src/client.ts`) is included to demonstrate interacting with the server's tools.\n\n1.  **Ensure the server is running** (either locally or in Docker).\n2.  **Run the client:**\n    ```bash\n    npx ts-node src/client.ts\n    ```\nThe client will connect to the server (using SSE transport by default), list the available tools, call each tool with example arguments (including some designed to test error handling), and print the results.\n\n## Tool Details\n\n### `get_current_time`\n\nReturns the current time in the specified timezone.\n\n*   **Input Argument:**\n    *   `timezone` (string, optional): An IANA timezone name (e.g., `America/New_York`, `Europe/London`). If omitted, defaults to the server's local timezone.\n*   **Output:** A JSON object containing:\n    *   `timezone` (string): The effective timezone used.\n    *   `datetime` (string): The current time in ISO 8601 format with offset (e.g., `2025-04-26T01:39:15Z`).\n\n### `convert_time`\n\nConverts a time from a source timezone to a target timezone.\n\n*   **Input Arguments:**\n    *   `source_timezone` (string, optional): Source IANA timezone name. Defaults to server's local timezone if omitted.\n    *   `time` (string, required): The time to convert in 24-hour HH:MM format (e.g., `14:30`).\n    *   `target_timezone` (string, optional): Target IANA timezone name. Defaults to server's local timezone if omitted.\n*   **Output:** A JSON object containing:\n    *   `source` (object): Details of the time in the source timezone (`timezone`, `datetime`).\n    *   `target` (object): Details of the converted time in the target timezone (`timezone`, `datetime`).\n    *   `time_difference` (string): The difference between the target and source timezone offsets (e.g., `+8h`, `-5h`, `+5.75h`). \n\n## License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details. ",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "timezones",
        "timezone",
        "mcp",
        "timezone conversions",
        "information timezone",
        "time srv"
      ],
      "category": "web-search"
    },
    "jikime--py-mcp-naver-search": {
      "owner": "jikime",
      "name": "py-mcp-naver-search",
      "url": "https://github.com/jikime/py-mcp-naver-search",
      "imageUrl": "/freedevtools/mcp/pfp/jikime.webp",
      "description": "Access diverse Naver search APIs for retrieving structured data across multiple categories including blogs, news, books, images, and shopping items. The server facilitates pagination, adult content filtering, and keyboard input error correction for enhanced search functionality.",
      "stars": 3,
      "forks": 6,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-05-16T05:41:13Z",
      "readme_content": "# Naver Search MCP Server\n\n[![smithery badge](https://smithery.ai/badge/@jikime/py-mcp-naver-search)](https://smithery.ai/server/@jikime/py-mcp-naver-search) ![](https://badge.mcpx.dev?type=server 'MCP Server') ![Version](https://img.shields.io/badge/version-1.1.10-green) ![License](https://img.shields.io/badge/license-MIT-blue)\n\nThis MCP (Multi-platform Communication Protocol) server provides access to Naver Search APIs, allowing AI agents to search for various types of content on Naver.\n\n## Overview\n\n- Search for blogs, news, books, images, shopping items, and more\n- Multiple search categories with pagination support\n- Structured text responses optimized for LLM consumption\n- Check for adult content\n- Convert keyboard input errors (errata)\n\n## Table of Contents\n\n- [Prerequisites](#prerequisites)\n- [Installation](#installation)\n- [Configure MCP Settings](#configure-mcp-settings)\n- [API Reference](#api-reference)\n- [Acknowledgements](#acknowledgements)\n- [License](#license)\n\n## Setup\n\n### Prerequisites\n\n- Python 3.12+\n- Naver Developer API credentials\n  - You can obtain these credentials by signing up at the [Naver Developers](https://developers.naver.com/apps/#/register) portal.\n  - And You can check my blog [Naver Search API MCP Server](https://devway.tistory.com/55), too.\n\n### Installation\n\n1. Clone the repository:\n```bash\ngit clone https://github.com/jikime/py-mcp-naver-search.git\ncd py-mcp-naver-search\n```\n\n2. uv installation\n```bash\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n```\n\n3. Create a virtual environment and install dependencies:\n```bash\nuv venv -p 3.12\nsource .venv/bin/activate\npip install -r requirements.txt\n```\n\n4. Create a `.env` file with your Naver API credentials:\n```\ncp env.example .env\nvi .env\n\nNAVER_CLIENT_ID=your_client_id_here\nNAVER_CLIENT_SECRET=your_client_secret_here\n```\n\n\n#### Using Docker\n\n1. Build the Docker image:\n```bash\ndocker build -t py-mcp-naver-search .\n```\n\n2. Run the container:\n```bash\ndocker run py-mcp-naver-search\n```\n\n#### Using Local\n\n1. Run the server:\n```bash\nmcp run server.py\n```\n2. Run the MCP Inspector\n```bash\nmcp dev server.py\n```\n\n## Configure MCP Settings\nAdd the server configuration to your MCP settings file:\n\n#### Claude desktop app \n1. To install automatically via [Smithery](https://smithery.ai/server/@jikime/py-mcp-naver-search):\n\n```bash\nnpx -y @smithery/cli install @jikime/py-mcp-naver-search --client claude\n```\n\n2. To install manually\nopen `~/Library/Application Support/Claude/claude_desktop_config.json`\n\nAdd this to the `mcpServers` object:\n```json\n{\n  \"mcpServers\": {\n    \"Google Toolbox\": {\n      \"command\": \"/path/to/bin/uv\",\n      \"args\": [\n        \"--directory\",\n        \"/path/to/py-mcp-naver-search\",\n        \"run\",\n        \"server.py\"\n      ]\n    }\n  }\n}\n```\n\n#### Cursor IDE \nopen `~/.cursor/mcp.json`\n\nAdd this to the `mcpServers` object:\n```json\n{\n  \"mcpServers\": {\n    \"Google Toolbox\": {\n      \"command\": \"/path/to/bin/uv\",\n      \"args\": [\n        \"--directory\",\n        \"/path/to/py-mcp-naver-search\",\n        \"run\",\n        \"server.py\"\n      ]\n    }\n  }\n}\n```\n\n#### for Docker\n```json\n{\n  \"mcpServers\": {\n    \"Google Toolbox\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"-i\",\n        \"--rm\",\n        \"py-mcp-naver-search\"\n      ]\n    }\n  }\n}\n```\n\n### Using the Client\n\nThe repository includes a client script for testing:\n\n```bash\n# Basic search\nuv run client.py blog \"Python programming\" display=5 page=1\n\n# News search with sorting\nuv run client.py news \"AI\" display=10 page=1 sort=date\n\n# Image search with filtering\nuv run client.py image \"cat\" display=10 filter=large\n\n# Check for adult content\nuv run client.py adult \"your query\"\n\n# Errata correction\nuv run client.py errata \"spdlqj\"\n```\n\n## Available Search Categories\n\nThe server supports the following search categories:\n\n1. `blog` - Blog posts\n2. `news` - News articles\n3. `book` - Books\n4. `adult` - Adult content check\n5. `encyc` - Encyclopedia entries\n6. `cafe_article` - Cafe articles\n7. `kin` - Knowledge iN Q&A\n8. `local` - Local business information\n9. `errata` - Keyboard input error correction\n10. `shop` - Shopping items\n11. `doc` - Academic papers and documents\n12. `image` - Images\n13. `webkr` - Web documents\n\n## API Reference\n\n### Tools\n\n#### Search Blog\n```\nsearch_blog(query: str, display: int = 10, page: int = 1, sort: str = \"sim\") -> str\n```\nSearches for blogs on Naver using the given keyword.\n\n#### Search News\n```\nsearch_news(query: str, display: int = 10, page: int = 1, sort: str = \"sim\") -> str\n```\nSearches for news on Naver using the given keyword.\n\n#### Search Book\n```\nsearch_book(query: str, display: int = 10, page: int = 1, sort: str = \"sim\") -> str\n```\nSearches for book information on Naver using the given keyword.\n\n#### Check Adult Query\n```\ncheck_adult_query(query: str) -> str\n```\nDetermines if the input query is an adult search term.\n\n#### Search Encyclopedia\n```\nsearch_encyclopedia(query: str, display: int = 10, page: int = 1, sort: str = \"sim\") -> str\n```\nSearches for encyclopedia information on Naver using the given keyword.\n\n#### Search Cafe Article\n```\nsearch_cafe_article(query: str, display: int = 10, page: int = 1, sort: str = \"sim\") -> str\n```\nSearches for cafe articles on Naver using the given keyword.\n\n#### Search KnowledgeiN\n```\nsearch_kin(query: str, display: int = 10, page: int = 1, sort: str = \"sim\") -> str\n```\nSearches for Knowledge iN Q&A on Naver using the given keyword.\n\n#### Search Local\n```\nsearch_local(query: str, display: int = 5, page: int = 1, sort: str = \"random\") -> str\n```\nSearches for local business information using the given keyword.\n\n#### Correct Errata\n```\ncorrect_errata(query: str) -> str\n```\nConverts Korean/English keyboard input errors.\n\n#### Search Shop\n```\nsearch_shop(query: str, display: int = 10, page: int = 1, sort: str = \"sim\") -> str\n```\nSearches for shopping product information on Naver using the given keyword.\n\n#### Search Document\n```\nsearch_doc(query: str, display: int = 10, page: int = 1) -> str\n```\nSearches for academic papers, reports, etc. using the given keyword.\n\n#### Search Image\n```\nsearch_image(query: str, display: int = 10, page: int = 1, sort: str = \"sim\", filter: str = \"all\") -> str\n```\nSearches for images using the given keyword.\n\n#### Search Web Document\n```\nsearch_webkr(query: str, display: int = 10, page: int = 1) -> str\n```\nSearches for web documents using the given keyword.\n\n### Resources\n\n#### Available Search Categories\n```\nGET naver://available-search-categories\n```\nReturns a list of Naver search categories available on this MCP server.\n\n## Response Format\n\nAll tools return responses in structured text format, optimized for LLM processing:\n\n```\nNaver Blog search results (total 12,345 of 1~10):\n\n### Result 1\nTitle(title): Sample Blog Post\nLink(link): https://blog.example.com/post1\nDescription(description): This is a sample blog post about...\nBlogger name(bloggername): John Doe\nBlogger link(bloggerlink): https://blog.example.com\nPost date(postdate): 20250429\n\n### Result 2\n...\n```\n\n## Acknowledgements\n- [Naver Search API MCP Server Blog](https://devway.tistory.com/55)\n- [Naver Open API](https://developers.naver.com/docs/search/blog/)\n- [MCP Protocol](https://github.com/mcp-foundation/mcp-spec)\n\n## License\n\nThis project is licensed under the MIT License - see the LICENSE file for details.\n\n\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "search",
        "apis",
        "naver",
        "search apis",
        "naver search",
        "search functionality"
      ],
      "category": "web-search"
    },
    "jikime--py-mcp-youtube-toolbox": {
      "owner": "jikime",
      "name": "py-mcp-youtube-toolbox",
      "url": "https://github.com/jikime/py-mcp-youtube-toolbox",
      "imageUrl": "/freedevtools/mcp/pfp/jikime.webp",
      "description": "Interact with YouTube to perform video searches, extract transcripts, retrieve comments, and obtain detailed information about videos and channels. Provides advanced filtering options and access to trending videos and transcript analysis.",
      "stars": 20,
      "forks": 5,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-09-25T09:20:54Z",
      "readme_content": "# py-mcp-youtube-toolbox\n[![smithery badge](https://smithery.ai/badge/@jikime/py-mcp-youtube-toolbox)](https://smithery.ai/server/@jikime/py-mcp-youtube-toolbox) ![](https://badge.mcpx.dev?type=server 'MCP Server') ![Version](https://img.shields.io/badge/version-1.0.0-green) ![License](https://img.shields.io/badge/license-MIT-blue)\n\nAn MCP server that provides AI assistants with powerful tools to interact with YouTube, including video searching, transcript extraction, comment retrieval, and more.\n\n<a href=\"https://glama.ai/mcp/servers/@jikime/py-mcp-youtube-toolbox\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@jikime/py-mcp-youtube-toolbox/badge\" alt=\"YouTube Toolbox MCP server\" />\n</a>\n\n## Overview\n\npy-mcp-youtube-toolbox provides the following YouTube-related functionalities:\n\n- Search YouTube videos with advanced filtering options\n- Get detailed information about videos and channels\n- Retrieve video comments with sorting options\n- Extract video transcripts and captions in multiple languages\n- Find related videos for a given video\n- Get trending videos by region\n- Generate summaries of video content based on transcripts\n- Advanced transcript analysis with filtering, searching, and multi-video capabilities\n\n## Table of Contents\n\n- [Prerequisites](#prerequisites)\n- [Installation](#installation)\n- [Configure MCP Settings](#configure-mcp-settings)\n- [Tools Documentation](#tools-documentation)\n  - [Video Tools](#video-tools)\n  - [Channel Tools](#channel-tools)\n  - [Transcript Tools](#transcript-tools)\n  - [Prompt Tools](#prompt-tools)\n  - [Resource Tools](#resource-tools)\n- [Development](#development)\n- [License](#license)\n\n## Prerequisites\n1. **Python**: Install Python 3.12 or higher\n2. **YouTube API Key**:\n   - Go to [Google Cloud Console](https://console.cloud.google.com/)\n   - Create a new project or select an existing one\n   - Enable the YouTube Data API v3:\n     1. Go to \"APIs & Services\" > \"Library\"\n     2. Search for and enable \"YouTube Data API v3\"\n   - Create credentials:\n     1. Go to \"APIs & Services\" > \"Credentials\"\n     2. Click \"Create Credentials\" > \"API key\"\n     3. Note down your API key\n\n## Installation\n#### Git Clone\n```bash\ngit clone https://github.com/jikime/py-mcp-youtube-toolbox.git\ncd py-mcp-youtube-toolbox\n```\n\n#### Configuration \n1. Install UV package manager:\n```bash\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n```\n\n2. Create and activate virtual environment:\n```bash\nuv venv -p 3.12\nsource .venv/bin/activate  # On MacOS/Linux\n# or\n.venv\\Scripts\\activate  # On Windows\n```\n\n3. Install dependencies:\n```bash\nuv pip install -r requirements.txt\n```\n\n4. Environment variables:\n```bash\ncp env.example .env\nvi .env\n# Update with your YouTube API key\nYOUTUBE_API_KEY=your_youtube_api_key\n```\n\n#### Using Docker\n\n1. Build the Docker image:\n```bash\ndocker build -t py-mcp-youtube-toolbox .\n```\n\n2. Run the container:\n```bash\ndocker run -e YOUTUBE_API_KEY=your_youtube_api_key py-mcp-youtube-toolbox\n```\n\n#### Using Local\n\n1. Run the server:\n```bash\nmcp run server.py\n```\n\n2. Run the MCP Inspector:\n```bash\nmcp dev server.py\n```\n\n## Configure MCP Settings\nAdd the server configuration to your MCP settings file:\n\n#### Claude desktop app \n1. To install automatically via [Smithery](https://smithery.ai/server/@jikime/py-mcp-youtube-toolbox):\n\n```bash\nnpx -y @smithery/cli install @jikime/py-mcp-youtube-toolbox --client claude\n```\n\n2. To install manually\nopen `~/Library/Application Support/Claude/claude_desktop_config.json`\n\nAdd this to the `mcpServers` object:\n```json\n{\n  \"mcpServers\": {\n    \"YouTube Toolbox\": {\n      \"command\": \"/path/to/bin/uv\",\n      \"args\": [\n        \"--directory\",\n        \"/path/to/py-mcp-youtube-toolbox\",\n        \"run\",\n        \"server.py\"\n      ],\n      \"env\": {\n        \"YOUTUBE_API_KEY\": \"your_youtube_api_key\"\n      }\n    }\n  }\n}\n```\n\n#### Cursor IDE \nopen `~/.cursor/mcp.json`\n\nAdd this to the `mcpServers` object:\n```json\n{\n  \"mcpServers\": {\n    \"YouTube Toolbox\": {\n      \"command\": \"/path/to/bin/uv\",\n      \"args\": [\n        \"--directory\",\n        \"/path/to/py-mcp-youtube-toolbox\",\n        \"run\",\n        \"server.py\"\n      ],\n      \"env\": {\n        \"YOUTUBE_API_KEY\": \"your_youtube_api_key\"\n      }\n    }\n  }\n}\n```\n\n#### for Docker\n```json\n{\n  \"mcpServers\": {\n    \"YouTube Toolbox\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"-i\",\n        \"--rm\",\n        \"-e\", \"YOUTUBE_API_KEY=your_youtube_api_key\",\n        \"py-mcp-youtube-toolbox\"\n      ]\n    }\n  }\n}\n```\n\n## Tools Documentation\n\n### Video Tools\n\n- `search_videos`: Search for YouTube videos with advanced filtering options (channel, duration, region, etc.)\n- `get_video_details`: Get detailed information about a specific YouTube video (title, channel, views, likes, etc.)\n- `get_video_comments`: Retrieve comments from a YouTube video with sorting options\n- `get_related_videos`: Find videos related to a specific YouTube video\n- `get_trending_videos`: Get trending videos on YouTube by region\n\n### Channel Tools\n\n- `get_channel_details`: Get detailed information about a YouTube channel (name, subscribers, views, etc.)\n\n### Transcript Tools\n\n- `get_video_transcript`: Extract transcripts/captions from YouTube videos in specified languages\n- `get_video_enhanced_transcript`: Advanced transcript extraction with filtering, search, and multi-video capabilities\n\n### Prompt Tools\n\n- `transcript_summary`: Generate summaries of YouTube video content based on transcripts with customizable options\n\n### Resource Tools\n\n- `youtube://available-youtube-tools`: Get a list of all available YouTube tools\n- `youtube://video/{video_id}`: Get detailed information about a specific video\n- `youtube://channel/{channel_id}`: Get information about a specific channel\n- `youtube://transcript/{video_id}?language={language}`: Get transcript for a specific video\n\n## Development\n\nFor local testing, you can use the included client script:\n\n```bash\n# Example: Search videos\nuv run client.py search_videos query=\"MCP\" max_results=5\n\n# Example: Get video details\nuv run client.py get_video_details video_id=zRgAEIoZEVQ\n\n# Example: Get channel details\nuv run client.py get_channel_details channel_id=UCRpOIr-NJpK9S483ge20Pgw\n\n# Example: Get video comments\nuv run client.py get_video_comments video_id=zRgAEIoZEVQ max_results=10 order=time\n\n# Example: Get video transcript\nuv run client.py get_video_transcript video_id=zRgAEIoZEVQ language=ko\n\n# Example: Get related videos\nuv run client.py get_related_videos video_id=zRgAEIoZEVQ max_results=5\n\n# Example: Get trending videos\nuv run client.py get_trending_videos region_code=ko max_results=10\n\n# Example: Advanced transcript extraction\nuv run client.py get_video_enhanced_transcript video_ids=zRgAEIoZEVQ language=ko format=timestamped include_metadata=true start_time=100 end_time=200 query=에이전트 case_sensitive=true segment_method=equal segment_count=2\n\n# Example: \n```\n\n## License\n\nMIT License",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "youtube",
        "jikime",
        "py",
        "youtube toolbox",
        "jikime py",
        "mcp youtube"
      ],
      "category": "web-search"
    },
    "jkingsman--qanon-mcp-server": {
      "owner": "jkingsman",
      "name": "qanon-mcp-server",
      "url": "https://github.com/jkingsman/qanon-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/jkingsman.webp",
      "description": "Provides access to a dataset of Q-Anon posts for research purposes, enabling users to search, filter, and analyze the content for sociological insights.",
      "stars": 1,
      "forks": 1,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-06-20T16:40:02Z",
      "readme_content": "## QAnon is a dangerous cult. This archive is for research purposes only, and I do _not_ endorse any material in this repo.\r\n\r\n# Q-Anon Posts/Drops MCP Server\r\n\r\n![](https://badge.mcpx.dev?type=server 'qanon-mcp')\r\n[![smithery badge](https://smithery.ai/badge/@jkingsman/qanon-mcp-server)](https://smithery.ai/server/@jkingsman/qanon-mcp-server)\r\n\r\nAn MCP (Model Context Protocol) server that provides access to a dataset of Q-Anon posts for anthropological/sociological research. This server allows AI assistants like Claude to search, filter, and analyze the Q-Anon drops.\r\n\r\nPosts are drawn from https://github.com/jkingsman/JSON-QAnon. You can learn more about how the source data was composed there, as well as find alternate formats, schemas, etc.\r\n\r\n### Warning: This tool was entirely vibe coded. Use at your own risk.\r\n\r\n## Prerequisites\r\n\r\n- Python 3.10 or higher\r\n- `uv` package manager\r\n- Claude Desktop (for Claude integration)\r\n\r\n## Installation\r\n\r\nThis tool is compatible with `uvx` and doesn't need to be cloned/installed.\r\n\r\n### Installing via Smithery\r\n\r\nTo install qanon-mcp-server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@jkingsman/qanon-mcp-server):\r\n\r\n```bash\r\nnpx -y @smithery/cli install @jkingsman/qanon-mcp-server --client claude\r\n```\r\n\r\n### Manual\r\n\r\n1. Clone or download this repository to your local machine\r\n2. Install the required packages using `uv`:\r\n\r\n```bash\r\nuv pip install -e .\r\n```\r\n\r\n## Usage\r\n\r\nYou can run the server directly with `uvx`:\r\n\r\n```bash\r\nuvx qanon_mcp\r\n```\r\n\r\n## Claude Desktop Integration\r\n\r\nTo use this MCP server with Claude Desktop:\r\n\r\n1. Make sure you have [Claude Desktop](https://claude.ai/download) installed\r\n2. Open the Claude menu and select \"Settings...\"\r\n3. Click on \"Developer\" in the left-hand bar and then \"Edit Config\"\r\n4. Add the following configuration to the `claude_desktop_config.json` file:\r\n\r\n```json\r\n{\r\n  \"mcpServers\": {\r\n    \"qanon_mcp\": {\r\n      \"command\": \"uvx\",\r\n      \"args\": [\r\n        \"qanon_mcp\"\r\n      ]\r\n    }\r\n  }\r\n}\r\n```\r\n\r\nor, if you don't have `uvx` installed:\r\n\r\n```json\r\n{\r\n  \"mcpServers\": {\r\n    \"qanon_mcp\": {\r\n      \"command\": \"uv\",\r\n      \"args\": [\r\n        \"tool\",\r\n        \"run\",\r\n        \"qanon_mcp\"\r\n      ]\r\n    }\r\n  }\r\n}\r\n```\r\n\r\n\r\n5. Save the file and restart Claude Desktop\r\n6. Start a new conversation in Claude Desktop\r\n7. You should see a hammer icon in the input box, indicating that tools are available\r\n\r\n## Features\r\n\r\n### Resources\r\n\r\n- `qanon://posts/count` - Get the total number of posts\r\n- `qanon://posts/{post_id}` - Access a specific post by ID\r\n- `qanon://posts/raw/{post_id}` - Get the raw JSON data for a specific post\r\n- `qanon://authors` - List all unique authors\r\n- `qanon://stats` - Get dataset statistics\r\n\r\n### Tools\r\n\r\n- **get_post_by_id_tool** - Retrieve a specific post by its ID\r\n- **search_posts** - Find posts containing specific keywords or phrases\r\n- **get_posts_by_date** - Retrieve posts from a specific date range\r\n- **get_posts_by_author_id** - Find posts by a specific author ID\r\n- **analyze_post** - Get detailed analysis of a specific post including references and context\r\n- **get_timeline_summary** - Generate a chronological timeline, optionally within a date range\r\n- **word_cloud_by_post_ids** - Generate a word frequency analysis for posts within a specified ID range\r\n- **word_cloud_by_date_range** - Generate a word frequency analysis for posts within a specified date range\r\n\r\n## Example Queries for Claude\r\n\r\nOnce the MCP server is connected to Claude Desktop, you can ask questions like:\r\n\r\n- \"How many Q-Anon posts are in the dataset?\"\r\n- \"Search for posts that mention 'storm'\"\r\n- \"Show me posts from October 2020\"\r\n- \"Analyze post #3725\"\r\n- \"Create a timeline of Q-Anon posts from 2018\"\r\n- \"Generate a word cloud for Q-Anon posts between January and March 2019\"\r\n- \"Get the raw data for post #4500\"\r\n- \"What are the most common words used in posts #1000-2000?\"\r\n\r\n## Troubleshooting\r\n\r\n- If Claude Desktop doesn't show the hammer icon, check your configuration and restart Claude Desktop\r\n- Ensure the `posts.json` file is in the same directory as the script\r\n- Check the output in the terminal for any error messages\r\n- Make sure you're using the absolute path to the script in your Claude Desktop configuration\r\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "search",
        "qanon",
        "sociological",
        "jkingsman qanon",
        "search jkingsman",
        "posts research"
      ],
      "category": "web-search"
    },
    "jlgrimes--ptcg-mcp": {
      "owner": "jlgrimes",
      "name": "ptcg-mcp",
      "url": "https://github.com/jlgrimes/ptcg-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/jlgrimes.webp",
      "description": "Search and display detailed information about Pokemon Trading Card Game cards, including attributes, legality, and statistics. Access high-resolution images to enhance gameplay effectively.",
      "stars": 8,
      "forks": 3,
      "license": "No License",
      "language": "TypeScript",
      "updated_at": "2025-08-01T18:03:13Z",
      "readme_content": "# Pokemon TCG Card Search MCP\n\nThis Model Context Protocol (MCP) server allows Claude to search and display Pokemon Trading Card Game cards.\n\n## Setup Instructions\n\n1. Update your Claude configuration file:\n\n   - Open `/Users/ABSOLUTE_PATH_HERE/Library/Application Support/Claude/claude_desktop_config.json`\n   - Add the following configuration (remove any existing MCP configurations):\n\n   ```json\n   {\n     \"mcpServers\": {\n       \"ptcg-mcp\": {\n         \"command\": \"node\",\n         \"args\": [\"ABSOLUTE_PATH_HERE/dist/index.js\"]\n       }\n     }\n   }\n   ```\n\n2. Quit Claude:\n\n   - Open Task Manager\n   - Find and quit Claude completely\n\n3. Restart Claude:\n   - The Pokemon TCG Card Search MCP will be automatically loaded\n   - You can now ask Claude questions about Pokemon cards\n\n## Usage\n\nOnce configured, you can ask Claude questions about Pokemon cards such as:\n\n- \"Show me standard-legal basic Pokemon with free retreat\"\n- \"Find water-type Pokemon with more than 120 HP\"\n- \"Search for Pikachu cards\"\n\nClaude will display the matching cards with their images and relevant information.\n\n## Features\n\n- Search cards by name, type, subtype, legality, and more\n- View high-resolution card images\n- Filter by various card attributes:\n  - Name (supports exact matching with `!` and wildcards with `*`)\n  - Subtypes (e.g., Basic, EX, GX, V, VMAX, etc.)\n  - Legalities (Standard, Expanded, Unlimited)\n  - Types (Water, Fire, Grass, etc.)\n  - Retreat cost\n  - HP\n  - National Pokedex numbers\n  - And more!\n\n## Example Queries\n\nHere are some example queries you can try:\n\n- \"Show me standard-legal basic Pokemon with free retreat\"\n- \"Find water-type Pokemon with more than 120 HP\"\n- \"Search for cards with 'char\\*' in their name\"\n- \"Show me banned cards in Standard format\"\n- \"Find EX Pokemon that evolve from Charmander\"\n\n## Query Syntax\n\n### Name Search\n\n- Regular search: `name:pikachu`\n- Exact match: `!name:pikachu`\n- Wildcard: `name:char*`\n- Preserve hyphens: `name:chien-pao`\n\n### Filters\n\n- Types: `types:water` or `-types:water` (exclude)\n- Subtypes: `subtypes:basic`\n- Legalities: `legalities.standard:legal`\n- HP: `hp:[100 TO 200]`\n- Retreat Cost: `convertedRetreatCost:0`\n\n### Range Queries\n\nUse `[` and `]` for inclusive ranges, `{` and `}` for exclusive ranges:\n\n- `hp:[100 TO 200]` - HP between 100 and 200 (inclusive)\n- `hp:{100 TO 200}` - HP between 100 and 200 (exclusive)\n- `hp:[* TO 100]` - HP up to 100\n- `hp:[100 TO *]` - HP 100 or higher\n\n## Response Format\n\nThe MCP returns card information including:\n\n- Card name\n- Set name\n- High-resolution card image\n- Card legalities\n- Other card details as requested\n\n## Notes\n\n- The MCP uses the Pokemon TCG API to fetch card data\n- Images are displayed directly from the Pokemon TCG API's CDN\n- All queries are case-insensitive\n- Multiple filters can be combined in a single query\n",
      "npm_url": "https://www.npmjs.com/package/ptcg-mcp",
      "npm_downloads": 429,
      "keywords": [
        "ptcg",
        "cards",
        "card",
        "pokemon trading",
        "trading card",
        "game cards"
      ],
      "category": "web-search"
    },
    "jmagar--yarr": {
      "owner": "jmagar",
      "name": "yarr",
      "url": "https://github.com/jmagar/yarr",
      "imageUrl": "/freedevtools/mcp/pfp/null.webp",
      "description": "Automate and control your media services using natural language commands.",
      "stars": 0,
      "forks": 0,
      "license": "Unknown",
      "language": "Unknown",
      "updated_at": "",
      "readme_content": "",
      "npm_url": "https://www.npmjs.com/package/yarr",
      "npm_downloads": 1212,
      "keywords": [
        "jmagar",
        "automate",
        "search",
        "search jmagar",
        "web search",
        "media services"
      ],
      "category": "web-search"
    },
    "jmarcher--tasty-tales": {
      "owner": "jmarcher",
      "name": "tasty-tales",
      "url": "https://github.com/jmarcher/tasty-tales",
      "imageUrl": "/freedevtools/mcp/pfp/jmarcher.webp",
      "description": "Build and manage rich, interactive recipes with features for searching, filtering, and discovering culinary content. Includes dynamic serving size calculators and interactive timers for a customized cooking experience.",
      "stars": 0,
      "forks": 0,
      "license": "MIT License",
      "language": "",
      "updated_at": "2025-02-06T16:15:41Z",
      "readme_content": "# 🍽️ TastyTales\n\n<div align=\"center\">\n\n[![Nuxt 3](https://img.shields.io/badge/Nuxt-3-00DC82?style=for-the-badge&logo=nuxt.js)](https://nuxt.com)\n[![Vue 3](https://img.shields.io/badge/Vue-3-4FC08D?style=for-the-badge&logo=vue.js)](https://vuejs.org)\n[![TailwindCSS](https://img.shields.io/badge/Tailwind-CSS-38B2AC?style=for-the-badge&logo=tailwind-css)](https://tailwindcss.com)\n[![TypeScript](https://img.shields.io/badge/TypeScript-007ACC?style=for-the-badge&logo=typescript)](https://www.typescriptlang.org)\n[![License](https://img.shields.io/badge/license-MIT-blue.svg?style=for-the-badge)](LICENSE)\n\nA modern, feature-rich recipe platform built with Nuxt 3, Vue 3, and TailwindCSS. Perfect for culinary enthusiasts, food bloggers, and recipe creators.\n\n[Demo](https://demo-link) · [Report Bug](https://github.com/yourusername/tastytales/issues) · [Request Feature](https://github.com/yourusername/tastytales/issues)\n\n</div>\n\n## 💝 Support My Work\n\n<div align=\"center\">\n  <a href=\"https://patreon.com/0xExile\">\n    <img src=\"https://img.shields.io/badge/Sponsor_on-Patreon-FF424D?style=for-the-badge&logo=patreon&logoColor=white\" alt=\"Sponsor on Patreon\" />\n  </a>\n</div>\n\n<p align=\"center\">\n  <strong>🌟 Get Exclusive Access to Premium Content! 🌟</strong><br>\n  Sponsor me on Patreon and get exclusive access to full Nuxt Apps with stunning designs,<br>components, and exclusive content!\n</p>\n\n## ✨ Features\n\n### Core Features\n\n- 📝 **Rich Recipe Management**\n  - Step-by-step instructions\n  - Interactive timers with notifications\n  - Dynamic serving size calculator\n  - Ingredient management\n- 🎨 **Modern UI/UX**\n  - Clean, minimalist design\n  - Smooth animations and transitions\n  - Responsive layout for all devices\n  - Dark mode support\n- 🔍 **Search & Discovery**\n  - Full-text search\n  - Category and tag filtering\n  - Recipe recommendations\n\n## 🧩 Modules\n\nTastyTales uses several Nuxt modules to enhance functionality:\n\n- [@nuxt/content v3](https://content.nuxtjs.org) - File-based CMS\n- [@pinia/nuxt](https://pinia.vuejs.org/ssr/nuxt.html) - State Management\n- [@nuxtjs/tailwindcss](https://tailwindcss.nuxtjs.org) - CSS Framework\n- [@nuxtjs/google-fonts](https://google-fonts.nuxtjs.org) - Web Fonts\n- [nuxt-icon](https://github.com/nuxt-modules/icon) - Icon System\n\n## 📝 Creating Content\n\n### Adding a New Recipe\n\n1. Create a new JSON file in `content/recipes/` with the following structure:\n\n```json\n{\n  \"title\": \"Classic Margherita Pizza\",\n  \"description\": \"Traditional Neapolitan pizza with fresh ingredients\",\n  \"image\": \"/images/recipes/margherita-pizza.jpg\",\n  \"prepTime\": \"15 minutes\",\n  \"cookTime\": \"12 minutes\",\n  \"servings\": 4,\n  \"difficulty\": \"medium\",\n  \"tags\": [\"italian\", \"pizza\", \"vegetarian\"],\n  \"ingredients\": [\n    \"Pizza dough\",\n    \"San Marzano tomatoes\",\n    \"Fresh mozzarella\",\n    \"Fresh basil\",\n    \"Olive oil\",\n    \"Salt\"\n  ],\n  \"instructions\": [\n    { \"instruction\": \"Preheat oven to 500°F (260°C)\", \"timer\": 1 },\n    { \"instruction\": \"Roll out the pizza dough\", \"timer\": 5 },\n    \"Spread tomato sauce\",\n    \"Add fresh mozzarella\",\n    \"Bake for 12-15 minutes\",\n    \"Garnish with fresh basil\"\n  ]\n}\n```\n\n### Adding a Blog Post\n\n1. Create a new Markdown file in `content/blog/` with the following frontmatter:\n\n```md\n---\ntitle: 'The Art of Pizza Making'\ndescription: 'Learn the secrets of making authentic Neapolitan pizza at home'\ndate: '2025-02-03'\nimage: '/images/blog/pizza-making.jpg'\nauthor:\n  name: 'John Doe'\n  avatar: '/images/authors/john.jpg'\ntags: ['tips', 'techniques', 'italian']\n---\n\n# The Art of Pizza Making\n\nYour blog content goes here in Markdown format...\n\n## Tips for Perfect Pizza\n\n1. Use high-quality ingredients\n2. Let the dough rest properly\n3. Preheat your oven thoroughly\n\n## Common Mistakes to Avoid\n\n- Don't overload toppings\n- Avoid using cold ingredients\n- Don't skip the resting time\n```\n\n### Working with Nuxt Content\n\nQuery your content in components/pages:\n\n```vue\n<script setup>\n// Fetch all recipes\nconst { data: recipes } = await useAsyncData('recipes', () =>\n  queryCollection('recipes').all()\n);\n\n// Fetch a single recipe\nconst { data: recipe } = await useAsyncData('pizza', () =>\n  queryCollection('recipes')\n    .where('title', '=', 'Classic Margherita Pizza')\n    .first()\n);\n\n// Search recipes by tag\nconst { data: italianRecipes } = await useAsyncData('italian', () =>\n  queryContent('recipes').where('tags', 'LIKE', 'italian').all()\n);\n</script>\n```\n\n## 🚀 Quick Start\n\n### Prerequisites\n\n- Node.js 16.x or later\n- pnpm 7.x or later\n\n### Installation\n\n1. Clone the repository\n\n```bash\ngit clone https://github.com/florianjs/tasty-tales.git\ncd tastytales\n```\n\n2. Install dependencies\n\n```bash\npnpm install\n```\n\n3. Start development server\n\n```bash\npnpm dev\n```\n\n4. Build for production\n\n```bash\npnpm build\n```\n\n## 📁 Project Structure\n\n```\ntastytales/\n├── components/           # Reusable Vue components\n│   ├── recipe/          # Recipe-specific components\n│   └── ui/              # UI components\n├── composables/         # Composable functions\n├── content/            # Recipe content (JSON/Markdown)\n├── layouts/            # Page layouts\n├── pages/              # Application routes\n├── public/             # Static assets\n├── stores/             # Pinia stores\n├── types/              # TypeScript types\n├── app.vue             # Application entry\n├── nuxt.config.ts      # Nuxt configuration\n└── tailwind.config.ts  # TailwindCSS configuration\n```\n\n## 🎨 Customization\n\n### Theme\n\nThe default theme uses a warm, inviting color scheme:\n\n```typescript\ncolors: {\n  primary: colors.orange,\n  accent: colors.amber,\n  // Customize in tailwind.config.ts\n}\n```\n\n### Typography\n\n- **Headings**: Playfair Display (elegant serif)\n- **Body**: Inter (modern sans-serif)\n\nCustomize fonts in `nuxt.config.ts`:\n\n```typescript\ngoogleFonts: {\n  families: {\n    'Playfair+Display': [500, 600, 700],\n    'Inter': [400, 500, 600]\n  }\n}\n```\n\n## ☁️ Deployment\n\n### Recommended: Cloudflare Pages\n\nThis project is optimized for deployment on Cloudflare Pages, which provides:\n\n- Global CDN distribution\n- Automatic HTTPS\n- Zero cold starts\n- Easy deployment process\n\n### Deployment Steps\n\nBasic deployment steps:\n\n1. Connect your repository to Cloudflare Pages\n2. Set build command: `pnpm run build`\n3. Set build output directory: `.output/public`\n4. Add a D1 SQL Database from Cloudflare\n5. Bind your database to your Cloudflare Page\n\nFor detailed deployment instructions, follow the [Nuxt Content deployment guide for Cloudflare](https://content.nuxt.com/docs/deploy/serverless).\n\n## 🤝 Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request. For major changes, please open an issue first to discuss what you would like to change.\n\n1. Fork the Project\n2. Create your Feature Branch (`git checkout -b feature/AmazingFeature`)\n3. Commit your Changes (`git commit -m 'Add some AmazingFeature'`)\n4. Push to the Branch (`git push origin feature/AmazingFeature`)\n5. Open a Pull Request\n\n## 📝 License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n\n## 🙏 Acknowledgments\n\n- [Nuxt.js](https://nuxt.com) - The Intuitive Vue Framework\n- [TailwindCSS](https://tailwindcss.com) - A utility-first CSS framework\n- [Pinia](https://pinia.vuejs.org) - The Vue Store that you will enjoy using\n\n---\n\n<div align=\"center\">\n\nMade with ❤️ by [0xExile](https://github.com/florianjs)\n\n</div>\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "jmarcher",
        "recipes",
        "culinary",
        "jmarcher tasty",
        "search jmarcher",
        "interactive recipes"
      ],
      "category": "web-search"
    },
    "jmh108--MCP-server-readability-python": {
      "owner": "jmh108",
      "name": "MCP-server-readability-python",
      "url": "https://github.com/jmh108/MCP-server-readability-python",
      "imageUrl": "/freedevtools/mcp/pfp/jmh108.webp",
      "description": "Extracts and transforms webpage content into clean, LLM-optimized Markdown, removing ads and non-essential elements for improved readability and processing by language models.",
      "stars": 3,
      "forks": 3,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-29T05:37:20Z",
      "readme_content": "# MCP Server Readability Parser (Python / FastMCP)\n\n## Credits/Reference\nThis project is based on the original [server-moz-readability](https://github.com/emzimmer/server-moz-readability) implementation of [emzimmer](https://github.com/emzimmer). (For the original README documentation, please refer to the [original README.md](https://github.com/emzimmer/server-moz-readability/blob/main/readme.md).)\n\nThis Python implementation adapts the original concept to run as python based MCP using [FastMCP](https://github.com/jlowin/fastmcp)\n\n\n\n# Mozilla Readability Parser MCP Server\n\nA Python implementation of the [Model Context Protocol (MCP)](https://github.com/modelcontextprotocol) server that extracts and transforms webpage content into clean, LLM-optimized Markdown.\n\n## Table of Contents\n- [Features](#features)\n- [Why Not Just Fetch?](#why-not-just-fetch)\n- [Installation](#installation)\n- [Quick Start](#quick-start)\n- [Tool Reference](#tool-reference)\n- [Dependencies](#dependencies)\n- [License](#license)\n\n## Features\n- Removes ads, navigation, footers and other non-essential content\n- Converts clean HTML into well-formatted Markdown\n- Handles errors gracefully\n- Optimized for LLM processing\n- Lightweight and fast\n\n## Why Not Just Fetch?\nUnlike simple fetch requests, this server:\n- Extracts only relevant content using Readability algorithm\n- Eliminates noise like ads, popups, and navigation menus\n- Reduces token usage by removing unnecessary HTML/CSS\n- Provides consistent Markdown formatting for better LLM processing\n- Handles complex web pages with dynamic content\n\n## Installation\n\n1. Clone the repository:\n```bash\ngit clone https://github.com/jmh108/MCP-server-readability-python.git\ncd MCP-server-readability-python\n```\n\n2. Create and activate a virtual environment:\n```bash\npython -m venv venv\nsource venv/bin/activate  # On Windows use: venv\\Scripts\\activate\n```\n\n3. Install dependencies:\n```bash\npip install -r requirements.txt\n```\n\n## Quick Start\n\n1. Start the server:\n```bash\nfastmcp run server.py\n```\n\n2. Example request:\n```bash\ncurl -X POST http://localhost:8000/tools/extract_content \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"url\": \"https://example.com/article\"}'\n```\n\n## Tool Reference\n\n### `extract_content`\nFetches and transforms webpage content into clean Markdown.\n\n**Arguments:**\n```json\n{\n  \"url\": {\n    \"type\": \"string\",\n    \"description\": \"The website URL to parse\",\n    \"required\": true\n  }\n}\n```\n\n**Returns:**\n```json\n{\n  \"content\": \"Markdown content...\"\n}\n```\n\n## MCP Server Configuration\n\nTo configure the MCP server, add the following to your MCP settings file:\n\n```json\n{\n  \"mcpServers\": {\n    \"readability\": {\n      \"command\": \"fastmcp\",\n      \"args\": [\"run\", \"server.py\"],\n      \"env\": {}\n    }\n  }\n}\n```\n\nThe server can then be started using the MCP protocol and accessed via the `parse` tool.\n\n## Dependencies\n- [readability-lxml](https://github.com/buriy/python-readability) - Content extraction\n- [html2text](https://github.com/Alir3z4/html2text) - HTML to Markdown conversion\n- [beautifulsoup4](https://www.crummy.com/software/BeautifulSoup/) - DOM parsing\n- [requests](https://docs.python-requests.org/) - HTTP requests\n\n## License\nMIT License - See [LICENSE](LICENSE) for details.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "markdown",
        "web",
        "webpage",
        "search jmh108",
        "webpage content",
        "jmh108 mcp"
      ],
      "category": "web-search"
    },
    "jmh108--md-webcrawl-mcp": {
      "owner": "jmh108",
      "name": "md-webcrawl-mcp",
      "url": "https://github.com/jmh108/md-webcrawl-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/jmh108.webp",
      "description": "Extracts website content and saves it as markdown files while mapping website structures and links efficiently, enabling batch processing of multiple URLs.",
      "stars": 3,
      "forks": 5,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-07-19T18:57:32Z",
      "readme_content": "# MD MCP Webcrawler Project\n\nA Python-based MCP (https://modelcontextprotocol.io/introduction) web crawler for extracting and saving website content. \n\n## Features\n- Extract website content and save as markdown files\n- Map website structure and links\n- Batch processing of multiple URLs\n- Configurable output directory\n\n## Installation\n\n1. Clone the repository:\n```bash\ngit clone https://github.com/yourusername/webcrawler.git\ncd webcrawler\n```\n\n2. Install dependencies:\n```bash\npip install -r requirements.txt\n```\n\n3. Optional: Configure environment variables:\n```bash\nexport OUTPUT_PATH=./output  # Set your preferred output directory\n```\n\n## Output\nCrawled content is saved in markdown format in the specified output directory.\n\n## Configuration\nThe server can be configured through environment variables:\n\n- `OUTPUT_PATH`: Default output directory for saved files\n- `MAX_CONCURRENT_REQUESTS`: Maximum parallel requests (default: 5)\n- `REQUEST_TIMEOUT`: Request timeout in seconds (default: 30)\n\n## Claude Set-Up\nInstall with FastMCP \n``` fastmcp install server.py ```\n\nor user custom settings to run with fastmcp directly\n\n````\n\"Crawl Server\": {\n      \"command\": \"fastmcp\",\n      \"args\": [\n        \"run\",\n        \"/Users/mm22/Dev_Projekte/servers-main/src/Webcrawler/server.py\"\n      ],\n      \"env\": {\n        \"OUTPUT_PATH\": \"/Users/user/Webcrawl\"\n      }\n```` \n\n\n\n## Development\n\n### Live Development\n```bash\nfastmcp dev server.py --with-editable .\n```\n### Debug \nIt helps to use https://modelcontextprotocol.io/docs/tools/inspector for debugging\n\n## Examples\n\n### Example 1: Extract and Save Content\n```bash\nmcp call extract_content --url \"https://example.com\" --output_path \"example.md\"\n```\n\n### Example 2: Create Content Index\n```bash\nmcp call scan_linked_content --url \"https://example.com\" | \\\n  mcp call create_index --content_map - --output_path \"index.md\"\n```\n\n## Contributing\n1. Fork the repository\n2. Create a feature branch (`git checkout -b feature/AmazingFeature`)\n3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)\n4. Push to the branch (`git push origin feature/AmazingFeature`)\n5. Open a Pull Request\n\n## License\n\nDistributed under the MIT License. See `LICENSE` for more information.\n\n## Requirements\n\n- Python 3.7+\n- FastMCP (uv pip install fastmcp)\n- Dependencies listed in requirements.txt\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "webcrawl",
        "markdown",
        "urls",
        "md webcrawl",
        "webcrawl mcp",
        "search jmh108"
      ],
      "category": "web-search"
    },
    "jobsonlook--xhs-mcp": {
      "owner": "jobsonlook",
      "name": "xhs-mcp",
      "url": "https://github.com/jobsonlook/xhs-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/jobsonlook.webp",
      "description": "Facilitates access to Xiaohongshu's API using JavaScript reverse engineering to bypass the need for heavyweight tools like Playwright. Supports searching notes, retrieving note content, fetching comments, and posting comments.",
      "stars": 334,
      "forks": 79,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-10-03T02:51:21Z",
      "readme_content": "# 小红书MCP服务\n[![smithery badge](https://smithery.ai/badge/@jobsonlook/xhs-mcp)](https://smithery.ai/server/@jobsonlook/xhs-mcp)\n[![PyPI version](https://badge.fury.io/py/jobson-xhs-mcp.svg)](https://badge.fury.io/py/jobson-xhs-mcp)\n\n一个用于小红书API的MCP（Model Context Protocol）服务器，支持搜索笔记、获取内容、查看评论和发表评论等功能。\n## 特点\n- [x] 采用js逆向出x-s,x-t,直接请求http接口,无须笨重的playwright\n- [x] 搜索笔记\n- [x] 获取笔记内容\n- [x] 获取笔记的评论\n- [x] 发表评论\n\n![特性](https://raw.githubusercontent.com/jobsonlook/xhs-mcp/master/docs/feature.png)\n\n## 快速开始\n\n### 方法一：使用uvx（推荐）\n\n#### 1. 环境要求\n- Python 3.12+\n- uv (安装方法: `pip install uv`)\n\n#### 2. 获取小红书的cookie\n[打开web小红书](https://www.xiaohongshu.com/explore)\n登录后，获取cookie，将cookie配置到下一步的 XHS_COOKIE 环境变量中\n![cookie](https://raw.githubusercontent.com/jobsonlook/xhs-mcp/master/docs/cookie.png)\n\n#### 3. 配置MCP服务器\n\n在你的MCP客户端配置文件中添加以下配置：\n\n```json\n{\n    \"mcpServers\": {\n        \"xhs-mcp\": {\n            \"command\": \"uvx\",\n            \"args\": [\n                \"--from\",\n                \"jobson-xhs-mcp\",\n                \"xhs-mcp\"\n            ],\n            \"env\": {\n                \"XHS_COOKIE\": \"你的小红书cookie\"\n            }\n        }\n    }\n}\n```\n\n#### 4. 测试运行\n```bash\n# 设置环境变量\nexport XHS_COOKIE=\"你的小红书cookie\"\n\n# 直接运行测试\nuvx --from jobson-xhs-mcp xhs-mcp --help\n```\n\n### 方法二：从源码安装\n\n#### 1. 环境要求\n- node\n- python 3.12\n- uv (pip install uv)\n\n#### 2. 克隆并安装\n```sh\ngit clone git@github.com:jobsonlook/xhs-mcp.git\ncd xhs-mcp\nuv sync\n```\n\n#### 3. 获取小红书的cookie\n[打开web小红书](https://www.xiaohongshu.com/explore)\n登录后，获取cookie，将cookie配置到下一步的 XHS_COOKIE 环境变量中\n![cookie](https://raw.githubusercontent.com/jobsonlook/xhs-mcp/master/docs/cookie.png)\n\n#### 4. 配置MCP服务器\n\n```json\n{\n    \"mcpServers\": {\n        \"xhs-mcp\": {\n            \"command\": \"uv\",\n            \"args\": [\n                \"--directory\",\n                \"/path/to/xhs-mcp\",\n                \"run\",\n                \"xhs_mcp/__main__.py\"\n            ],\n            \"env\": {\n                \"XHS_COOKIE\": \"你的小红书cookie\"\n            }\n        }\n    }\n}\n```\n\n## 可用工具\n\n本MCP服务器提供以下工具：\n\n- `check_cookie()` - 检测cookie是否失效\n- `home_feed()` - 获取首页推荐笔记\n- `search_notes(keywords)` - 根据关键词搜索笔记\n- `get_note_content(url)` - 获取笔记内容（需要带xsec_token的完整URL）\n- `get_note_comments(url)` - 获取笔记评论（需要带xsec_token的完整URL）\n- `post_comment(comment, note_id)` - 发布评论到指定笔记\n\n## 使用示例\n\n### 在Claude Desktop中使用\n\n1. 打开Claude Desktop的设置\n2. 找到MCP服务器配置\n3. 添加上述JSON配置\n4. 重启Claude Desktop\n5. 现在你可以在对话中使用小红书相关功能了\n\n### 常见问题\n\n**Q: Cookie如何获取？**\nA: 在浏览器中登录小红书网页版，打开开发者工具，在Network标签页中找到任意请求，复制Cookie头的值。\n\n**Q: 为什么提示cookie失效？**\nA: 小红书的cookie有时效性，需要定期更新。重新登录网页版获取新的cookie即可。\n\n**Q: uvx命令找不到？**\nA: 请先安装uv：`pip install uv`，然后确保PATH环境变量包含uv的安装路径。\n\n## 免责声明\n本项目仅用于学习交流，禁止用于其他用途，任何涉及商业盈利目的均不得使用，否则风险自负。\n\n",
      "npm_url": "https://www.npmjs.com/package/xhs-mcp",
      "npm_downloads": 1311,
      "keywords": [
        "xiaohongshu",
        "jobsonlook",
        "xhs",
        "xiaohongshu api",
        "jobsonlook xhs",
        "search jobsonlook"
      ],
      "category": "web-search"
    },
    "josemartinrodriguezmortaloni--webSearch-Tools": {
      "owner": "josemartinrodriguezmortaloni",
      "name": "webSearch-Tools",
      "url": "https://github.com/josemartinrodriguezmortaloni/webSearch-Tools",
      "imageUrl": "/freedevtools/mcp/pfp/josemartinrodriguezmortaloni.webp",
      "description": "Perform intelligent web searches and extract specific information from web pages using natural language prompts. Utilize advanced web scraping and crawling capabilities for enhanced content analysis and retrieval tasks.",
      "stars": 1,
      "forks": 6,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-03-18T05:59:54Z",
      "readme_content": "# WebSearch - Advanced Web Search and Content Extraction Tool\n\n![License](https://img.shields.io/badge/license-MIT-blue.svg)\n![Python Version](https://img.shields.io/badge/python-3.8%2B-blue)\n![Firecrawl](https://img.shields.io/badge/firecrawl-latest-green)\n![uv](https://img.shields.io/badge/uv-latest-purple)\n\nA powerful web search and content extraction tool built with Python, leveraging the Firecrawl API for advanced web scraping, searching, and content analysis capabilities.\n\n## 🚀 Features\n\n- **Advanced Web Search**: Perform intelligent web searches with customizable parameters\n- **Content Extraction**: Extract specific information from web pages using natural language prompts\n- **Web Crawling**: Crawl websites with configurable depth and limits\n- **Web Scraping**: Scrape web pages with support for various output formats\n- **MCP Integration**: Built as a Model Context Protocol (MCP) server for seamless integration\n\n## 📋 Prerequisites\n\n- Python 3.8 or higher\n- uv package manager\n- Firecrawl API key\n- OpenAI API key (optional, for enhanced features)\n- Tavily API key (optional, for additional search capabilities)\n\n## 🛠️ Installation\n\n1. Install uv:\n\n```bash\n# On Windows (using pip)\npip install uv\n\n# On Unix/MacOS\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n\n# Add uv to PATH (Unix/MacOS)\nexport PATH=\"$HOME/.local/bin:$PATH\"\n\n# Add uv to PATH (Windows - add to Environment Variables)\n# Add: %USERPROFILE%\\.local\\bin\n```\n\n2. Clone the repository:\n\n```bash\ngit clone https://github.com/yourusername/websearch.git\ncd websearch\n```\n\n3. Create and activate a virtual environment with uv:\n\n```bash\n# Create virtual environment\nuv venv\n\n# Activate on Windows\n.\\.venv\\Scripts\\activate.ps1\n\n# Activate on Unix/MacOS\nsource .venv/bin/activate\n```\n\n4. Install dependencies with uv:\n\n```bash\n# Install from requirements.txt\nuv sync\n```\n\n5. Set up environment variables:\n\n```bash\n# Create .env file\ntouch .env\n\n# Add your API keys\nFIRECRAWL_API_KEY=your_firecrawl_api_key\nOPENAI_API_KEY=your_openai_api_key\n```\n\n## 🎯 Usage\n\n### Setting Up With Claude for Desktop\n\nInstead of running the server directly, you can configure Claude for Desktop to access the WebSearch tools:\n\n1. Locate or create your Claude for Desktop configuration file:\n\n   - Windows: `%env:AppData%\\Claude\\claude_desktop_config.json`\n   - macOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\n\n2. Add the WebSearch server configuration to the `mcpServers` section:\n\n```json\n{\n  \"mcpServers\": {\n    \"websearch\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"--directory\",\n        \"D:\\\\ABSOLUTE\\\\PATH\\\\TO\\\\WebSearch\",\n        \"run\",\n        \"main.py\"\n      ]\n    }\n  }\n}\n```\n\n3. Make sure to replace the directory path with the absolute path to your WebSearch project folder.\n\n4. Save the configuration file and restart Claude for Desktop.\n\n5. Once configured, the WebSearch tools will appear in the tools menu (hammer icon) in Claude for Desktop.\n\n### Available Tools\n\n1. **Search**\n\n2. **Extract Information**\n\n3. **Crawl Websites**\n\n4. **Scrape Content**\n\n## 📚 API Reference\n\n### Search\n\n- `query` (str): The search query\n- Returns: Search results in JSON format\n\n### Extract\n\n- `urls` (List[str]): List of URLs to extract information from\n- `prompt` (str): Instructions for extraction\n- `enableWebSearch` (bool): Enable supplementary web search\n- `showSources` (bool): Include source references\n- Returns: Extracted information in specified format\n\n### Crawl\n\n- `url` (str): Starting URL\n- `maxDepth` (int): Maximum crawl depth\n- `limit` (int): Maximum pages to crawl\n- Returns: Crawled content in markdown/HTML format\n\n### Scrape\n\n- `url` (str): Target URL\n- Returns: Scraped content with optional screenshots\n\n## 🔧 Configuration\n\n### Environment Variables\n\nThe tool requires certain API keys to function. We provide a `.env.example` file that you can use as a template:\n\n1. Copy the example file:\n\n```bash\n# On Unix/MacOS\ncp .env.example .env\n\n# On Windows\ncopy .env.example .env\n```\n\n2. Edit the `.env` file with your API keys:\n\n```env\n# OpenAI API key - Required for AI-powered features\nOPENAI_API_KEY=your_openai_api_key_here\n\n# Firecrawl API key - Required for web scraping and searching\nFIRECRAWL_API_KEY=your_firecrawl_api_key_here\n```\n\n### Getting the API Keys\n\n1. **OpenAI API Key**:\n\n   - Visit [OpenAI's platform](https://platform.openai.com/)\n   - Sign up or log in\n   - Navigate to API keys section\n   - Create a new secret key\n\n2. **Firecrawl API Key**:\n   - Visit [Firecrawl's website](https://docs.firecrawl.dev/)\n   - Create an account\n   - Navigate to your dashboard\n   - Generate a new API key\n\nIf everything is configured correctly, you should receive a JSON response with search results.\n\n### Troubleshooting\n\nIf you encounter errors:\n\n1. Ensure all required API keys are set in your `.env` file\n2. Verify the API keys are valid and have not expired\n3. Check that the `.env` file is in the root directory of the project\n4. Make sure the environment variables are being loaded correctly\n\n## 🤝 Contributing\n\n1. Fork the repository\n2. Create your feature branch (`git checkout -b feature/AmazingFeature`)\n3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)\n4. Push to the branch (`git push origin feature/AmazingFeature`)\n5. Open a Pull Request\n\n## 📝 License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n\n## 🙏 Acknowledgments\n\n- [Firecrawl](https://docs.firecrawl.dev/) for their powerful web scraping API\n- [OpenAI](https://openai.com/) for AI capabilities\n- [MCP](https://modelcontextprotocol.io/introduction)The MCP community for the protocol specification\n\n## 📬 Contact\n\nJosé Martín Rodriguez Mortaloni - [@m4s1t425](https://x.com/m4s1t425) - jmrodriguezm13@gmail.com\n\n---\n\nMade with ❤️ using Python and Firecrawl\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "websearch",
        "scraping",
        "searches",
        "josemartinrodriguezmortaloni websearch",
        "websearch tools",
        "web scraping"
      ],
      "category": "web-search"
    },
    "jsonallen--perplexity-mcp": {
      "owner": "jsonallen",
      "name": "perplexity-mcp",
      "url": "https://github.com/jsonallen/perplexity-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/jsonallen.webp",
      "description": "Provides web search functionality leveraging Perplexity AI's API, allowing users to search for recent information on various topics.",
      "stars": 254,
      "forks": 33,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-10-03T04:12:32Z",
      "readme_content": "# perplexity-mcp MCP server\n\n[![smithery badge](https://smithery.ai/badge/perplexity-mcp)](https://smithery.ai/server/perplexity-mcp)\n\nA Model Context Protocol (MCP) server that provides web search functionality using [Perplexity AI's](https://www.perplexity.ai/) API. Works with the [Anthropic](https://www.anthropic.com/news/model-context-protocol) Claude desktop client.\n\n## Example\n\nLet's you use prompts like, \"Search the web to find out what's new at Anthropic in the past week.\"\n\n## Glama Scores\n\n<a href=\"https://glama.ai/mcp/servers/ebg0za4hn9\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/ebg0za4hn9/badge\" alt=\"Perplexity Server MCP server\" /></a>\n\n## Components\n\n### Prompts\n\nThe server provides a single prompt:\n\n- perplexity_search_web: Search the web using Perplexity AI\n  - Required \"query\" argument for the search query\n  - Optional \"recency\" argument to filter results by time period:\n    - 'day': last 24 hours\n    - 'week': last 7 days\n    - 'month': last 30 days (default)\n    - 'year': last 365 days\n  - Uses Perplexity's API to perform web searches\n\n### Tools\n\nThe server implements one tool:\n\n- perplexity_search_web: Search the web using Perplexity AI\n  - Takes \"query\" as a required string argument\n  - Optional \"recency\" parameter to filter results (day/week/month/year)\n  - Returns search results from Perplexity's API\n\n## Installation\n\n### Installing via Smithery\n\nTo install Perplexity MCP for Claude Desktop automatically via [Smithery](https://smithery.ai/server/perplexity-mcp):\n\n```bash\nnpx -y @smithery/cli install perplexity-mcp --client claude\n```\n\n### Requires [UV](https://github.com/astral-sh/uv) (Fast Python package and project manager)\n\nIf uv isn't installed.\n\n```bash\n# Using Homebrew on macOS\nbrew install uv\n```\n\nor\n\n```bash\n# On macOS and Linux.\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n\n# On Windows.\npowershell -ExecutionPolicy ByPass -c \"irm https://astral.sh/uv/install.ps1 | iex\"\n```\n\n### Environment Variables\n\nThe following environment variable is required in your claude_desktop_config.json. You can obtain an API key from [Perplexity](https://perplexity.ai)\n\n- `PERPLEXITY_API_KEY`: Your Perplexity AI API key\n\nOptional environment variables:\n\n- `PERPLEXITY_MODEL`: The Perplexity model to use (defaults to \"sonar\" if not specified)\n\n  Available models:\n\n  - `sonar-deep-research`: 128k context - Enhanced research capabilities\n  - `sonar-reasoning-pro`: 128k context - Advanced reasoning with professional focus\n  - `sonar-reasoning`: 128k context - Enhanced reasoning capabilities\n  - `sonar-pro`: 200k context - Professional grade model\n  - `sonar`: 128k context - Default model\n  - `r1-1776`: 128k context - Alternative architecture\n\nAnd updated list of models is avaiable (here)[https://docs.perplexity.ai/guides/model-cards]\n\n### Cursor & Claude Desktop Installation\n\nAdd this tool as a mcp server by editing the Cursor/Claude config file.\n\n```json\n  \"perplexity-mcp\": {\n    \"env\": {\n      \"PERPLEXITY_API_KEY\": \"XXXXXXXXXXXXXXXXXXXX\",\n      \"PERPLEXITY_MODEL\": \"sonar\"\n    },\n    \"command\": \"uvx\",\n    \"args\": [\n      \"perplexity-mcp\"\n    ]\n  }\n```\n\n#### Cursor\n- On MacOS: `/Users/your-username/.cursor/mcp.json`\n- On Windows: `C:\\Users\\your-username\\.cursor\\mcp.json`\n\nIf everything is working correctly, you should now be able to call the tool from Cursor.\n\n\n#### Claude Desktop\n- On MacOS: `~/Library/Application\\ Support/Claude/claude_desktop_config.json`\n- On Windows: `%APPDATA%/Claude/claude_desktop_config.json`\n\nTo verify the server is working. Open the Claude client and use a prompt like \"search the web for news about openai in the past week\". You should see an alert box open to confirm tool usage. Click \"Allow for this chat\".\n\n  <img width=\"600\" alt=\"mcp_screenshot\" src=\"https://github.com/user-attachments/assets/922d8f6a-8c9a-4978-8be6-788e70b4d049\" />",
      "npm_url": "https://www.npmjs.com/package/perplexity-mcp",
      "npm_downloads": 8172,
      "keywords": [
        "search",
        "perplexity",
        "jsonallen",
        "search jsonallen",
        "leveraging perplexity",
        "jsonallen perplexity"
      ],
      "category": "web-search"
    },
    "jtucker--mcp-untappd-server": {
      "owner": "jtucker",
      "name": "mcp-untappd-server",
      "url": "https://github.com/jtucker/mcp-untappd-server",
      "imageUrl": "/freedevtools/mcp/pfp/jtucker.webp",
      "description": "Query the Untappd API to find beers and retrieve detailed information about them. Supports beer searches and accessing specific beer details using their unique identifiers.",
      "stars": 0,
      "forks": 2,
      "license": "No License",
      "language": "JavaScript",
      "updated_at": "2025-02-28T01:19:19Z",
      "readme_content": "# Untappd Model Context Protocol Server\n\nThis is a simple MCP server written in `node` to query the [Untappd API](https://untappd.com/api/docs). Unfortunately they are no longer accepting registrations for new API keys. \n\nCurrently only supports 3 tools:\n\n- `search_beer`: This will search Untappd for a beer\n- `get_beer_info`: This get's the detailed beer info via the `beer_id` returned from a search.\n- `get_user_checkins`: _currently does not work_ This will return a users checkin's. \n\n## Development\n\nInstall dependencies:\n```bash\nnpm install\n```\n\nBuild the server:\n```bash\nnpm run build\n```\n\nFor development with auto-rebuild:\n```bash\nnpm run watch\n```\n\n## Installation\n\nTo use with Claude Desktop, add the server config:\n\nOn MacOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\nOn Windows: `%APPDATA%/Claude/claude_desktop_config.json`\n\n```json\n{\n  \"mcpServers\": {\n    \"untappd-server\": {\n      \"command\": \"/path/to/untappd-server/build/index.js\"\n    }\n  }\n}\n```\n\n### Debugging\n\nSince MCP servers communicate over stdio, debugging can be challenging. We recommend using the [MCP Inspector](https://github.com/modelcontextprotocol/inspector), which is available as a package script:\n\n```bash\nnpm run inspector\n```\n\nThe Inspector will provide a URL to access debugging tools in your browser.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "searches",
        "search",
        "retrieve",
        "beer searches",
        "search jtucker",
        "api beers"
      ],
      "category": "web-search"
    },
    "junmer--mcp-server-lottiefiles": {
      "owner": "junmer",
      "name": "mcp-server-lottiefiles",
      "url": "https://github.com/junmer/mcp-server-lottiefiles",
      "imageUrl": "/freedevtools/mcp/pfp/junmer.webp",
      "description": "Search and retrieve Lottie animations from LottieFiles, access detailed animation information, and discover popular animations for projects.",
      "stars": 6,
      "forks": 3,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-09-11T10:15:46Z",
      "readme_content": "# LottieFiles MCP Server\n[![smithery badge](https://smithery.ai/badge/mcp-server-lottiefiles)](https://smithery.ai/server/mcp-server-lottiefiles)\n\nA Model Context Protocol (MCP) server for searching and retrieving Lottie animations from LottieFiles.\n\n## Features\n\n- Search Lottie animations\n- Get animation details\n- Get popular animations list\n\n## Installation\n\n### Installing via Smithery\n\nTo install LottieFiles Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/mcp-server-lottiefiles):\n\n```bash\nnpx -y smithery install mcp-server-lottiefiles --client claude\n```\n\n### Manual Installation\n```bash\nnpm install\n```\n\n## Usage\n\n1. Start the server:\n\n```bash\nnpm start\n```\n\n2. Connect using an MCP client\n\n## API Tools\n\n### Search Animations\n\nSearch for Lottie animations by keywords.\n\nParameters:\n- `query`: Search keywords\n- `page`: Page number (optional, default: 1)\n- `limit`: Items per page (optional, default: 20)\n\n### Get Animation Details\n\nGet detailed information about a specific Lottie animation.\n\nParameters:\n- `id`: Unique identifier of the animation\n\n### Get Popular Animations\n\nGet a list of currently popular Lottie animations.\n\nParameters:\n- `page`: Page number (optional, default: 1)\n- `limit`: Items per page (optional, default: 20)\n\n## Development\n\n```bash\n# Build\nnpm run build\n```\n\n## License\n\nMIT\n",
      "npm_url": "https://www.npmjs.com/package/mcp-server-lottiefiles",
      "npm_downloads": 587,
      "keywords": [
        "lottiefiles",
        "lottie",
        "animations",
        "animations lottiefiles",
        "lottie animations",
        "lottiefiles search"
      ],
      "category": "web-search"
    },
    "jutalik--originforge-generate-nft-mcp": {
      "owner": "jutalik",
      "name": "originforge-generate-nft-mcp",
      "url": "https://github.com/jutalik/originforge-generate-nft-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/jutalik.webp",
      "description": "Fetch and display NFT data from the Origin Forge API, save SVG images and JSON metadata, and enhance the viewing experience of NFTs. Provides various functions to retrieve NFT information, attributes, and color palettes.",
      "stars": 1,
      "forks": 1,
      "license": "No License",
      "language": "JavaScript",
      "updated_at": "2025-07-19T11:21:32Z",
      "readme_content": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/jutalik-originforge-generate-nft-mcp-badge.png)](https://mseep.ai/app/jutalik-originforge-generate-nft-mcp)\n\n[![smithery badge](https://smithery.ai/badge/@jutalik/originforge-generate-nft-mcp)](https://smithery.ai/server/@jutalik/originforge-generate-nft-mcp)\n\n# NFT 데이터 뷰어\n[![smithery badge](https://smithery.ai/badge/@jutalik/originforge-generate-nft-mcp)](https://smithery.ai/server/@jutalik/originforge-generate-nft-mcp)\n\n이 프로젝트는 [Origin Forge API](https://api.origin-forge.com)에서 NFT 데이터를 가져와 표시하고 저장하는 도구입니다.\n\n## 기능\n\n- NFT 데이터 가져오기\n- 속성 및 색상 팔레트 표시\n- SVG 이미지 및 JSON 메타데이터 저장\n- MCP(Model Context Protocol) 서버 기능\n\n## 사용 방법\n\n### 간단한 데이터 뷰어\n\n```bash\nnode src/simple-nft.js\n```\n\n### 이미지 저장 기능이 있는 강화된 뷰어\n\n```bash\nnode src/enhanced-nft.js\n```\n\n### MCP 서버 실행\n\n```bash\nnode build/index.js\n```\n\n## MCP 서버 기능\n\nMCP 서버는 다음과 같은 도구들을 제공합니다:\n\n1. `get-nft-data` - 기본 NFT 정보 가져오기\n2. `get-nft-image` - NFT 이미지 데이터 가져오기\n3. `get-nft-attributes` - 상세 NFT 속성 가져오기 \n4. `get-color-palette` - NFT 색상 팔레트 가져오기\n5. `get-enhanced-nft-view` - 향상된 NFT 뷰 표시\n6. `save-nft-files` - NFT 이미지와 JSON 데이터를 파일로 저장 (매개변수: outputDir)\n7. `get-random-nfts` - 여러 개의 랜덤 NFT 가져오기 (매개변수: count)\n\n### 파일 저장 예시\n`save-nft-files` 도구를 사용하면 다음 파일들이 저장됩니다:\n- SVG 이미지 파일\n- JSON 메타데이터 파일\n- 원본 API 응답 데이터\n\n기본 저장 경로는 `nft-output` 디렉토리이며, `outputDir` 매개변수로 변경할 수 있습니다.\n\n### 여러 NFT 가져오기\n`get-random-nfts` 도구를 사용하면 한 번에 여러 개의 랜덤 NFT를 가져올 수 있습니다.\n기본값은 3개이며, `count` 매개변수로 1-5개 사이에서 지정할 수 있습니다.\n\n## 이미지 예시\n\n저장된 이미지는 `nft-output` 디렉토리에서 찾을 수 있습니다. 각 이미지는 SVG 형식과 JSON 메타데이터를 포함합니다.\n\n## 개발자\n\n이 프로젝트는 NFT 데이터를 쉽게 가져오고 표시하기 위해 개발되었습니다.\n\n## 라이선스\n\nISC \n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "nft",
        "nfts",
        "forge",
        "nfts provides",
        "retrieve nft",
        "nft information"
      ],
      "category": "web-search"
    },
    "jzhang17--prospect-research-mcp": {
      "owner": "jzhang17",
      "name": "prospect-research-mcp",
      "url": "https://github.com/jzhang17/prospect-research-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/jzhang17.webp",
      "description": "Provides tools for conducting prospect research, including semantic search, webpage scraping, and batch processing of search queries. Enhances research tasks with a focus on understanding context and intent behind queries.",
      "stars": 1,
      "forks": 2,
      "license": "No License",
      "language": "TypeScript",
      "updated_at": "2025-08-18T20:44:16Z",
      "readme_content": "\n# Prospect Research MCP Server\n\n[![smithery badge](https://smithery.ai/badge/@jzhang17/prospect-research-mcp)](https://smithery.ai/server/@jzhang17/prospect-research-mcp)\n\nA Model Context Protocol (MCP) server implementation focused on prospect research tools, deployed on Smithery Web infrastructure.\n\n## Features\n\n- **Semantic Search**: Contextual search that understands meaning and intent behind queries\n- **Webpage Scraping**: Extract and process content from multiple web pages\n- **Batch Search Processing**: Execute multiple search queries in parallel\n- **Comprehensive Coverage**: Combine different search approaches for thorough research\n\n## Tools\n\n- **web-search**\n  - A semantic search engine (Tavily) that understands the contextual meaning and intent behind queries\n  - Inputs:\n    - `query` (string): The search query to look up\n\n- **scrape-webpages**\n  - Scrape the provided web pages for detailed information\n  - Inputs:\n    - `links` (array): A list of URLs to scrape (optimally less than 10)\n  - Processes content to remove images and returns combined content from provided URLs\n\n- **batch-web-search**\n  - Traditional keyword-based search (Google via Search1API) that processes multiple queries simultaneously\n  - Inputs:\n    - `queries` (array): List of search queries to process in parallel (optimally less than 30)\n  - Executes multiple distinct search queries in parallel\n\n## Prompts\n\n- `simple-assist` - A basic prompt for general queries\n- `research` - A prompt for detailed research questions\n- `review-code` - A prompt for code review\n\n## Configuration\n\n### Required API Keys\nThis server requires the following API keys:\n- `TAVILY_API_KEY` - For semantic web search functionality\n- `JINA_API_KEY` - For webpage scraping\n- `SEARCH1API_KEY` - For batch web search\n\nThese are configured in the Smithery Web environment for the deployed version.\n\n### Usage with Claude Desktop\nAdd this to your `claude_desktop_config.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"prospect-research\": {\n      \"transport\": \"sse\",\n      \"url\": \"https://smithery.ai/server/@jzhang17/prospect-research-mcp\",\n      \"env\": {\n        \"TAVILY_API_KEY\": \"YOUR_TAVILY_API_KEY\",\n        \"JINA_API_KEY\": \"YOUR_JINA_API_KEY\",\n        \"SEARCH1API_KEY\": \"YOUR_SEARCH1API_KEY\"\n      }\n    }\n  }\n}\n```\n\n### For Other MCP Clients\nConfigure your client to connect to the server using the SSE transport type and the Smithery-hosted URL.\n\n\n## Structure\n\n- `/src/index.ts` - Main server entrypoint\n- `/src/tools/` - MCP tool implementations (web search, webpage scraping, batch search)\n- `/src/prompts/` - MCP prompt implementations\n- `/src/types/` - TypeScript type definitions\n\n## Deployment\n\nThis server is deployed to Smithery Web platform. To access the deployed server:\n\n1. Visit [Smithery.ai](https://smithery.ai/server/@jzhang17/prospect-research-mcp)\n2. The server is available at the URL provided by Smithery Web\n\n## References\n\n- [Model Context Protocol](https://modelcontextprotocol.io)\n- [MCP TypeScript SDK](https://github.com/modelcontextprotocol/typescript-sdk)\n- [Smithery AI Platform](https://smithery.ai)\n- [MCP Client List](https://modelcontextprotocol.io/clients)\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "search",
        "mcp",
        "prospect",
        "semantic search",
        "prospect research",
        "web search"
      ],
      "category": "web-search"
    },
    "kagisearch--kagimcp": {
      "owner": "kagisearch",
      "name": "kagimcp",
      "url": "https://github.com/kagisearch/kagimcp",
      "imageUrl": "/freedevtools/mcp/pfp/kagisearch.webp",
      "description": "Access Kagi Search API for retrieving information from the web, allowing users to perform web searches and gather data from online sources.",
      "stars": 198,
      "forks": 21,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-10-02T17:10:45Z",
      "readme_content": "# Kagi MCP server\n\n[![smithery badge](https://smithery.ai/badge/kagimcp)](https://smithery.ai/server/kagimcp)\n\n<a href=\"https://glama.ai/mcp/servers/xabrrs4bka\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/xabrrs4bka/badge\" alt=\"Kagi Server MCP server\" />\n</a>\n\n## Setup Intructions\n> Before anything, unless you are just using non-search tools, ensure you have access to the search API. It is currently in closed beta and available upon request. Please reach out to support@kagi.com for an invite.\n\nInstall uv first.\n\nMacOS/Linux:\n```bash\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n```\n\nWindows:\n```\npowershell -ExecutionPolicy ByPass -c \"irm https://astral.sh/uv/install.ps1 | iex\"\n```\n### Installing via Smithery\n\nAlternatively, you can install Kagi for Claude Desktop via [Smithery](https://smithery.ai/server/kagimcp):\n\n```bash\nnpx -y @smithery/cli install kagimcp --client claude\n```\n\n### Setup with Claude\n#### Claude Desktop\n```json\n// claude_desktop_config.json\n// Can find location through:\n// Hamburger Menu -> File -> Settings -> Developer -> Edit Config\n{\n  \"mcpServers\": {\n    \"kagi\": {\n      \"command\": \"uvx\",\n      \"args\": [\"kagimcp\"],\n      \"env\": {\n        \"KAGI_API_KEY\": \"YOUR_API_KEY_HERE\",\n        \"KAGI_SUMMARIZER_ENGINE\": \"YOUR_ENGINE_CHOICE_HERE\" // Defaults to \"cecil\" engine if env var not present\n      }\n    }\n  }\n}\n```\n#### Claude Code\nAdd the Kagi mcp server with the following command (setting summarizer engine optional):\n\n```bash\nclaude mcp add kagi -e KAGI_API_KEY=\"YOUR_API_KEY_HERE\" KAGI_SUMMARIZER_ENGINE=\"YOUR_ENGINE_CHOICE_HERE\" -- uvx kagimcp\n```\n\nNow claude code can use the Kagi mcp server. However, claude code comes with its own web search functionality by default, which may conflict with Kagi. You can disable claude's web search functionality with the following in your claude code settings file (`~/.claude/settings.json`):\n\n```json\n{\n  \"permissions\": {\n    \"deny\": [\n      \"WebSearch\"\n    ]\n  }\n}\n```\n\n### Pose query that requires use of a tool\ne.g. \"Who was time's 2024 person of the year?\" for search, or \"summarize this video: https://www.youtube.com/watch?v=jNQXAC9IVRw\" for summarizer.\n\n### Debugging\nRun:\n```bash\nnpx @modelcontextprotocol/inspector uvx kagimcp\n```\n\n## Local/Dev Setup Instructions\n\n### Clone repo\n`git clone https://github.com/kagisearch/kagimcp.git`\n\n### Install dependencies\nInstall uv first.\n\nMacOS/Linux:\n```bash\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n```\n\nWindows:\n```\npowershell -ExecutionPolicy ByPass -c \"irm https://astral.sh/uv/install.ps1 | iex\"\n```\n\nThen install MCP server dependencies:\n```bash\ncd kagimcp\n\n# Create virtual environment and activate it\nuv venv\n\nsource .venv/bin/activate # MacOS/Linux\n# OR\n.venv/Scripts/activate # Windows\n\n# Install dependencies\nuv sync\n```\n### Setup with Claude Desktop\n\n#### Using MCP CLI SDK\n```bash\n# `pip install mcp[cli]` if you haven't\nmcp install /ABSOLUTE/PATH/TO/PARENT/FOLDER/kagimcp/src/kagimcp/server.py -v \"KAGI_API_KEY=API_KEY_HERE\"\n```\n\n#### Manually\n```json\n# claude_desktop_config.json\n# Can find location through:\n# Hamburger Menu -> File -> Settings -> Developer -> Edit Config\n{\n  \"mcpServers\": {\n    \"kagi\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"--directory\",\n        \"/ABSOLUTE/PATH/TO/PARENT/FOLDER/kagimcp\",\n        \"run\",\n        \"kagimcp\"\n      ],\n      \"env\": {\n        \"KAGI_API_KEY\": \"YOUR_API_KEY_HERE\",\n        \"KAGI_SUMMARIZER_ENGINE\": \"YOUR_ENGINE_CHOICE_HERE\" // Defaults to \"cecil\" engine if env var not present\n      }\n    }\n  }\n}\n```\n\n### Pose query that requires use of a tool\ne.g. \"Who was time's 2024 person of the year?\" for search, or \"summarize this video: https://www.youtube.com/watch?v=jNQXAC9IVRw\" for summarizer.\n\n### Debugging\nRun:\n```bash\n# If mcp cli installed (`pip install mcp[cli]`)\nmcp dev /ABSOLUTE/PATH/TO/PARENT/FOLDER/kagimcp/src/kagimcp/server.py\n\n# If not\nnpx @modelcontextprotocol/inspector \\\n      uv \\\n      --directory /ABSOLUTE/PATH/TO/PARENT/FOLDER/kagimcp \\\n      run \\\n      kagimcp\n```\nThen access MCP Inspector at `http://localhost:5173`. You may need to add your Kagi API key in the environment variables in the inspector under `KAGI_API_KEY`.\n\n# Advanced Configuration\n- Level of logging is adjustable through the `FASTMCP_LOG_LEVEL` environment variable (e.g. `FASTMCP_LOG_LEVEL=\"ERROR\"`)\n  - Relevant issue: https://github.com/kagisearch/kagimcp/issues/4\n- Summarizer engine can be customized using the `KAGI_SUMMARIZER_ENGINE` environment variable (e.g. `KAGI_SUMMARIZER_ENGINE=\"daphne\"`)\n  - Learn about the different summarization engines [here](https://help.kagi.com/kagi/api/summarizer.html#summarization-engines)\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "kagisearch",
        "kagi",
        "searches",
        "kagi search",
        "search kagisearch",
        "kagisearch kagimcp"
      ],
      "category": "web-search"
    },
    "kazuph--mcp-fetch": {
      "owner": "kazuph",
      "name": "mcp-fetch",
      "url": "https://github.com/kazuph/mcp-fetch",
      "imageUrl": "/freedevtools/mcp/pfp/kazuph.webp",
      "description": "Fetch web content and process images to facilitate efficient interaction with online resources. Supports integration with MCP clients like Claude Desktop for seamless content management.",
      "stars": 30,
      "forks": 18,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-09-29T16:56:16Z",
      "readme_content": "# MCP Fetch\n\nModel Context Protocol server for fetching web content and processing images. This allows Claude Desktop (or any MCP client) to fetch web content and handle images appropriately.\n\n<a href=\"https://glama.ai/mcp/servers/5mknfdhyrg\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/5mknfdhyrg/badge\" alt=\"@kazuph/mcp-fetch MCP server\" /></a>\n\n## Quick Start (For Users)\n\nTo use this tool with Claude Desktop, simply add the following to your Claude Desktop configuration (`~/Library/Application Support/Claude/claude_desktop_config.json`):\n\n```json\n{\n  \"tools\": {\n    \"imageFetch\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@kazuph/mcp-fetch\"]\n    }\n  }\n}\n```\n\nThis will automatically download and run the latest version of the tool when needed.\n\n### Required Setup\n\n1. Enable Accessibility for Claude:\n   - Open System Settings\n   - Go to Privacy & Security > Accessibility\n   - Click the \"+\" button\n   - Add Claude from your Applications folder\n   - Turn ON the toggle for Claude\n\nThis accessibility setting is required for automated clipboard operations (Cmd+V) to work properly.\n\n## Features\n\n- **Web Content Extraction**: Automatically extracts and formats web content as markdown\n- **Article Title Extraction**: Extracts and displays the title of the article\n- **Image Processing**: Optional processing of images from web pages with optimization (disabled by default, enable with `enableFetchImages: true`)\n- **File Saving**: Images are automatically saved to `~/Downloads/mcp-fetch/YYYY-MM-DD/` directory when processed\n- **Dual Output**: Both file saving and optional Base64 encoding for AI display\n- **Pagination Support**: Supports pagination for both text and images\n- **JPEG Optimization**: Automatically optimizes images as JPEG for better performance\n- **GIF Support**: Extracts first frame from animated GIFs\n\n## For Developers\n\nThe following sections are for those who want to develop or modify the tool.\n\n## Prerequisites\n\n- Node.js 18+\n- macOS (for clipboard operations)\n- Claude Desktop (install from https://claude.ai/desktop)\n- tsx (install via `npm install -g tsx`)\n\n## Installation\n\n```bash\ngit clone https://github.com/kazuph/mcp-fetch.git\ncd mcp-fetch\nnpm install\nnpm run build\n```\n\n## Image Processing Specifications\n\nWhen processing images from web content, the following optimizations are applied:\n\n- Images are converted to JPEG format with quality control\n- Maximum width limited to 1200px by default\n- Maximum height limited to 1600px by default\n- Chroma subsampling (4:2:0) for better compression\n- MozJPEG optimization for smaller file sizes\n\n## Configuration\n\n1. Make sure Claude Desktop is installed and running.\n\n2. Install tsx globally if you haven't:\n```bash\nnpm install -g tsx\n# or\npnpm add -g tsx\n```\n\n3. Modify your Claude Desktop config located at:\n`~/Library/Application Support/Claude/claude_desktop_config.json`\n\nYou can easily find this through the Claude Desktop menu:\n1. Open Claude Desktop\n2. Click Claude on the Mac menu bar\n3. Click \"Settings\"\n4. Click \"Developer\"\n\nAdd the following to your MCP client's configuration:\n\n```json\n{\n  \"tools\": {\n    \"imageFetch\": {\n      \"args\": [\"tsx\", \"/path/to/mcp-fetch/index.ts\"]\n    }\n  }\n}\n```\n\n## Available Parameters\n\n- `url` (required): The URL to fetch\n- `maxLength` (default: 20000): Maximum length of content to return\n- `startIndex` (default: 0): Starting position in content\n- `imageStartIndex` (default: 0): Starting position for image collection\n- `raw` (default: false): Return raw content instead of processed markdown\n- `imageMaxCount` (default: 3): Maximum number of images to process per request\n- `imageMaxHeight` (default: 4000): Maximum height of merged image\n- `imageMaxWidth` (default: 1000): Maximum width of merged image\n- `imageQuality` (default: 80): JPEG quality (1-100)\n- `enableFetchImages` (default: false): Enable fetching and processing of images. FOR CLAUDE USERS ONLY: YOU ARE ALLOWED TO ENABLE THIS FEATURE!!! CLAUDE CAN FETCH AND PROCESS IMAGES!!! ENJOY THIS CAPABILITY!!!!!\n- `allowCrossOriginImages` (default: true): When true, also fetch images hosted on different origins from the page\n- `saveImages` (default: true): Save processed images to local files\n- `returnBase64` (default: false): Return base64 encoded images for AI display\n- `ignoreRobotsTxt` (default: false): Ignore robots.txt restrictions\n\n### Security Hardening (v1.5.1)\n\n- Only `http://` and `https://` URLs are allowed for page and image fetches\n- Blocks private/loopback/link-local IPs and local hostnames (e.g., `localhost`, `.local`)\n- Manual redirect handling with validation (max 3 hops)\n- Request timeouts (default 12s, configurable via `MCP_FETCH_TIMEOUT_MS`)\n- Response size limits: HTML up to 2MB, images up to 10MB (tunable via env)\n\nEnvironment variables:\n\n- `MCP_FETCH_TIMEOUT_MS` (default: 12000)\n- `MCP_FETCH_MAX_REDIRECTS` (default: 3)\n- `MCP_FETCH_MAX_HTML_BYTES` (default: 2000000)\n- `MCP_FETCH_MAX_IMAGE_BYTES` (default: 10000000)\n\n## Examples\n\n### Basic Content Fetching (No Images)\n```json\n{\n  \"url\": \"https://example.com\"\n}\n```\n\n### Fetching with Images (File Saving Only)\n```json\n{\n  \"url\": \"https://example.com\",\n  \"enableFetchImages\": true,\n  \"imageMaxCount\": 3\n}\n```\n\n### Fetching with Images for AI Display\n```json\n{\n  \"url\": \"https://example.com\",\n  \"enableFetchImages\": true,\n  \"returnBase64\": true,\n  \"imageMaxCount\": 3\n}\n```\n\n### Paginating Through Images\n```json\n{\n  \"url\": \"https://example.com\",\n  \"enableFetchImages\": true,\n  \"imageStartIndex\": 3,\n  \"imageMaxCount\": 3\n}\n```\n\n## Notes\n\n- This tool is designed for macOS only due to its dependency on macOS-specific clipboard operations.\n- Images are processed using Sharp for optimal performance and quality.\n- When multiple images are found, they are merged vertically with consideration for size limits.\n- Animated GIFs are automatically handled by extracting their first frame.\n- **File Saving**: Images are automatically saved to `~/Downloads/mcp-fetch/YYYY-MM-DD/` with filename format `hostname_HHMMSS_index.jpg`\n- **Tool Name**: The tool name has been changed from `fetch` to `imageFetch` to avoid conflicts with native fetch functions.\n\n## Changelog\n\n### v1.2.0\n- **BREAKING CHANGE**: Tool name changed from `fetch` to `imageFetch` to avoid conflicts\n- **NEW**: Automatic file saving - Images are now saved to `~/Downloads/mcp-fetch/YYYY-MM-DD/` by default\n- **NEW**: Added `saveImages` parameter (default: true) to control file saving\n- **NEW**: Added `returnBase64` parameter (default: false) for AI image display\n- **BEHAVIOR CHANGE**: Default behavior now saves files instead of only returning base64\n- Improved AI assistant integration with clear instructions for base64 option\n- Enhanced file organization with date-based directories and structured naming\n\n### v1.1.3\n- Changed default behavior: Images are not fetched by default (`enableFetchImages: false`)\n- Removed `disableImages` in favor of `enableFetchImages` parameter\n\n### v1.1.0\n- Added article title extraction feature\n- Improved response formatting to include article titles\n- Fixed type issues with MCP response content\n\n### v1.0.0\n- Initial release\n- Web content extraction\n- Image processing and optimization\n- Pagination support\n",
      "npm_url": "https://www.npmjs.com/package/mcp-fetch",
      "npm_downloads": 1627,
      "keywords": [
        "kazuph",
        "fetch",
        "web",
        "mcp fetch",
        "kazuph mcp",
        "content management"
      ],
      "category": "web-search"
    },
    "kbyk004-diy--playwright-lighthouse-mcp": {
      "owner": "kbyk004-diy",
      "name": "playwright-lighthouse-mcp",
      "url": "https://github.com/kbyk004-diy/playwright-lighthouse-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/kbyk004-diy.webp",
      "description": "Analyze website performance and capture screenshots using Playwright and Lighthouse. Gain insights into web application performance metrics with real-time analysis and visual feedback.",
      "stars": 4,
      "forks": 5,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-08-01T17:15:32Z",
      "readme_content": "# Playwright-Lighthouse MCP Server\n\nA MCP server that analyzes web site performance using Playwright and Lighthouse. Through the Model Context Protocol (MCP), LLMs can perform web site performance analysis.\n\n## Features\n\n- Performance analysis with Lighthouse\n- Screenshot capture\n\n## Setup\n\n### Prerequisites\n\n- Node.js 18 or higher\n- npm\n\n### Installation\n\n```bash\n# Clone the repository\ngit clone https://github.com/kbyk004/playwright-lighthouse-mcp.git\ncd playwright-lighthouse-mcp\n\n# Install dependencies\nnpm install\nnpx playwright install\n\n# Build\nnpm run build\n```\n\n## Usage\n\n### Debugging MCP Server\n\n```bash\nnpm run inspector\n```\n\n### Integration with MCP Clients\n\nThis server is designed to be used with clients that support the Model Context Protocol (MCP). For example, it can be integrated with Claude for Desktop.\n\n#### Configuration Example for Claude for Desktop\n\nAdd the following to the Claude for Desktop configuration file (`~/Library/Application Support/Claude/claude_desktop_config.json`):\n\n```json\n{\n  \"mcpServers\": {\n    \"playwright-lighthouse\": {\n      \"command\": \"node\",\n      \"args\": [\n        \"/path-to/playwright-lighthouse-mcp/build/index.js\"\n      ]\n    }\n  }\n}\n```\n\n## Available Tools\n\n### 1. run-lighthouse\n\nRuns a Lighthouse performance analysis on the currently open page.\n\nParameters:\n- `url`: The URL of the website you want to analyze\n- `categories`: Array of categories to analyze (default: [\"performance\"])\n  - Available categories: \"performance\", \"accessibility\", \"best-practices\", \"seo\", \"pwa\"\n- `maxItems`: Maximum number of improvement items to display for each category (default: 3, max: 5)\n\n### 2. take-screenshot\n\nTakes a screenshot of the currently open page.\n\nParameters:\n- `url`: The URL of the website you want to capture\n- `fullPage`: If true, captures a screenshot of the entire page (default: false)\n\n## Output Format\n\nThe analysis results include:\n\n- Overall scores for each selected category with color indicators\n- Key improvement areas grouped by category\n- Path to the saved report file\n\n## License\n\nMIT License - see [LICENSE](LICENSE) for details",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "performance",
        "playwright",
        "web",
        "website performance",
        "playwright lighthouse",
        "analyze website"
      ],
      "category": "web-search"
    },
    "kevinwatt--mcp-server-searxng": {
      "owner": "kevinwatt",
      "name": "mcp-server-searxng",
      "url": "https://github.com/kevinwatt/mcp-server-searxng",
      "imageUrl": "/freedevtools/mcp/pfp/kevinwatt.webp",
      "description": "Integrates with SearXNG to provide privacy-focused meta search capabilities, combining results from multiple search engines without tracking or profiling users. Supports various categories, languages, and filtering options, ensuring reliable and safe search experiences.",
      "stars": 21,
      "forks": 6,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-09-06T22:59:13Z",
      "readme_content": "# SearXNG MCP Server\n[![smithery badge](https://smithery.ai/badge/@kevinwatt/mcp-server-searxng)](https://smithery.ai/server/@kevinwatt/mcp-server-searxng)\n\nAn MCP server implementation that integrates with SearXNG, providing privacy-focused meta search capabilities.\n\n## Features\n\n- **Meta Search**: Combines results from multiple search engines\n- **Privacy-Focused**: No tracking, no user profiling\n- **Multiple Categories**: Support for general, news, science, files, images, videos, and more\n- **Language Support**: Search in specific languages or all languages\n- **Time Range Filtering**: Filter results by day, week, month, or year\n- **Safe Search**: Three levels of safe search filtering\n- **Fallback Support**: Multiple SearXNG instances for reliability\n\n## Installation\n\n### Installing via Smithery\n\nTo install SearXNG MCP Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@kevinwatt/mcp-server-searxng):\n\n```bash\nnpx -y @smithery/cli install @kevinwatt/mcp-server-searxng --client claude\n```\n\n### Manual Installation\n```bash\nnpm install -g @kevinwatt/mcp-server-searxng\n```\n\n## Usage\n\n### Direct Run\n\n```bash\nmcp-server-searxng\n```\n\n### With [Dive Desktop](https://github.com/OpenAgentPlatform/Dive)\n\n1. Click \"+ Add MCP Server\" in Dive Desktop\n2. Copy and paste this configuration:\n\n```json\n{\n  \"mcpServers\": {\n    \"searxng\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"@kevinwatt/mcp-server-searxng\"\n      ]\n    }\n  }\n}\n```\n\n3. Click \"Save\" to install the MCP server\n\n## Tool Documentation\n\n- **web_search**\n  - Execute meta searches across multiple engines\n  - Inputs:\n    - `query` (string): Search terms\n    - `page` (number, optional): Page number (default: 1)\n    - `language` (string, optional): Language code (e.g., 'en', 'all', default: 'all')\n    - `categories` (array, optional): Search categories (default: ['general'])\n      - Available: \"general\", \"news\", \"science\", \"files\", \"images\", \"videos\", \"music\", \"social media\", \"it\"\n    - `time_range` (string, optional): Time filter (day/week/month/year)\n    - `safesearch` (number, optional): Safe search level (0: None, 1: Moderate, 2: Strict, default: 1)\n\n## Development\n\n```bash\ngit clone https://github.com/kevinwatt/mcp-server-searxng.git\ncd mcp-server-searxng\nnpm install\nnpm run build\nnpm start\n```\n\n## License\n\nThis MCP server is licensed under the MIT License. See the LICENSE file for details.\n\n## Prerequisites\n\nYou need a local SearXNG instance running. To set it up:\n\n# Run SearXNG with Docker\n\n## Quick Start\n\n```bash\n# Create config directory\nmkdir -p searxng\n\n# Create config file\ntee searxng/settings.yml << EOF\nuse_default_settings: true\n\nserver:\n  bind_address: \"0.0.0.0\"\n  secret_key: \"CHANGE_THIS_TO_SOMETHING_SECURE\"  # Generate a random key\n  port: 8080\n\nsearch:\n  safe_search: 0\n  formats:\n    - html\n    - json\n\nengines:\n  - name: google\n    engine: google\n    shortcut: g\n\n  - name: duckduckgo\n    engine: duckduckgo\n    shortcut: d\n\n  - name: bing\n    engine: bing\n    shortcut: b\n\nserver.limiter: false\nEOF\n\n# Start container\ndocker run -d \\\n  --name searxng \\\n  -p 8080:8080 \\\n  -v \"$(pwd)/searxng:/etc/searxng\" \\\n  searxng/searxng\n```\n\n## Test Search Function\n\n```bash\n# Test JSON API with curl\ncurl -v 'http://localhost:8080/search?q=test&format=json'\n\n# Or visit in browser\nhttp://localhost:8080/search?q=test\n```\n\n## Container Management\n\n```bash\n# Stop container\ndocker stop searxng\n\n# Remove container\ndocker rm searxng\n\n# View container logs\ndocker logs searxng\n\n# Enable auto-start on boot\ndocker update --restart always searxng\n```\n\nThe `--restart always` flag ensures that:\n- Container starts automatically when Docker daemon starts\n- Container restarts automatically if it crashes\n- Container restarts automatically if it is stopped unless explicitly stopped by user\n\n## Custom Configuration\n\nEdit `searxng/settings.yml` to:\n- Modify search engine list\n- Adjust security settings\n- Configure UI language\n- Change API limits\n\nFor detailed configuration options, see [SearXNG Documentation](https://docs.searxng.org/)\n\n## Environment Variables\n\n- `SEARXNG_INSTANCES`: Comma-separated list of SearXNG instances URLs\n  Default: `http://localhost:8080`\n\n- `SEARXNG_USER_AGENT`: Custom User-Agent header for requests\n  Default: `MCP-SearXNG/1.0`\n\n- `NODE_TLS_REJECT_UNAUTHORIZED`: Set to '0' to bypass SSL certificate verification (for development with self-signed certificates)\n  Default: undefined (SSL verification enabled)\n\nExample configuration with all options:\n```json\n{\n  \"mcpServers\": {\n    \"searxng\": {\n      \"name\": \"searxng\",\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"@kevinwatt/mcp-server-searxng\"\n      ],\n      \"env\": {\n        \"SEARXNG_INSTANCES\": \"http://localhost:8080,https://searx.example.com\",\n        \"SEARXNG_USER_AGENT\": \"CustomBot/1.0\",\n        \"NODE_TLS_REJECT_UNAUTHORIZED\": \"0\"\n      }\n    }\n  }\n}\n```\n\n> ⚠️ Warning: Disabling SSL certificate verification is not recommended in production environments.\n",
      "npm_url": "https://www.npmjs.com/package/@kevinwatt/mcp-server-searxng",
      "npm_downloads": 0,
      "keywords": [
        "searxng",
        "search",
        "web",
        "server searxng",
        "searxng provide",
        "integrates searxng"
      ],
      "category": "web-search"
    },
    "kevinwatt--yt-dlp-mcp": {
      "owner": "kevinwatt",
      "name": "yt-dlp-mcp",
      "url": "https://github.com/kevinwatt/yt-dlp-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/kevinwatt.webp",
      "description": "Integrates with yt-dlp to download video and audio content from various platforms, and fetch subtitles in SRT format for processing by LLMs. Provides a privacy-focused direct download experience without tracking.",
      "stars": 84,
      "forks": 30,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-10-01T05:42:41Z",
      "readme_content": "# yt-dlp-mcp\n\nAn MCP server implementation that integrates with yt-dlp, providing video and audio content download capabilities (e.g. YouTube, Facebook, Tiktok, etc.) for LLMs.\n\n## Features\n\n* **Video Metadata**: Extract comprehensive video information without downloading content\n* **Subtitles**: Download subtitles in SRT format for LLMs to read\n* **Video Download**: Save videos to your Downloads folder with resolution control\n* **Audio Download**: Save audios to your Downloads folder\n* **Video Search**: Search for videos on YouTube using keywords\n* **Privacy-Focused**: Direct download without tracking\n* **MCP Integration**: Works with Dive and other MCP-compatible LLMs\n\n## Installation\n\n### Prerequisites\n\nInstall `yt-dlp` based on your operating system:\n\n```bash\n# Windows\nwinget install yt-dlp\n\n# macOS\nbrew install yt-dlp\n\n# Linux\npip install yt-dlp\n```\n\n### With [Dive Desktop](https://github.com/OpenAgentPlatform/Dive)\n\n1. Click \"+ Add MCP Server\" in Dive Desktop\n2. Copy and paste this configuration:\n\n```json\n{\n  \"mcpServers\": {\n    \"yt-dlp\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"@kevinwatt/yt-dlp-mcp\"\n      ]\n    }\n  }\n}\n```\n3. Click \"Save\" to install the MCP server\n\n## Tool Documentation\n\n* **search_videos**\n  * Search for videos on YouTube using keywords\n  * Inputs:\n    * `query` (string, required): Search keywords or phrase\n    * `maxResults` (number, optional): Maximum number of results to return (1-50, default: 10)\n\n* **list_subtitle_languages**\n  * List all available subtitle languages and their formats for a video (including auto-generated captions)\n  * Inputs:\n    * `url` (string, required): URL of the video\n\n* **download_video_subtitles**\n  * Download video subtitles in any available format. Supports both regular and auto-generated subtitles\n  * Inputs:\n    * `url` (string, required): URL of the video\n    * `language` (string, optional): Language code (e.g., 'en', 'zh-Hant', 'ja'). Defaults to 'en'\n\n* **download_video**\n  * Download video to user's Downloads folder\n  * Inputs:\n    * `url` (string, required): URL of the video\n    * `resolution` (string, optional): Video resolution ('480p', '720p', '1080p', 'best'). Defaults to '720p'\n    * `startTime` (string, optional): Start time for trimming (format: HH:MM:SS[.ms]) - e.g., '00:01:30' or '00:01:30.500'\n    * `endTime` (string, optional): End time for trimming (format: HH:MM:SS[.ms]) - e.g., '00:02:45' or '00:02:45.500'\n\n* **download_audio**\n  * Download audio in best available quality (usually m4a/mp3 format) to user's Downloads folder\n  * Inputs:\n    * `url` (string, required): URL of the video\n\n* **download_transcript**\n  * Download and clean video subtitles to produce a plain text transcript without timestamps or formatting\n  * Inputs:\n    * `url` (string, required): URL of the video\n    * `language` (string, optional): Language code (e.g., 'en', 'zh-Hant', 'ja'). Defaults to 'en'\n\n* **get_video_metadata**\n  * Extract comprehensive video metadata without downloading the content\n  * Returns detailed information including title, description, channel, timestamps, view counts, and more\n  * Inputs:\n    * `url` (string, required): URL of the video\n    * `fields` (array, optional): Specific metadata fields to extract (e.g., ['id', 'title', 'description', 'channel']). If not provided, returns all available metadata\n\n* **get_video_metadata_summary**\n  * Get a human-readable summary of key video metadata\n  * Returns formatted text with title, channel, duration, views, upload date, and description preview\n  * Inputs:\n    * `url` (string, required): URL of the video\n\n## Usage Examples\n\nAsk your LLM to:\n```\n\"Search for Python tutorial videos\"\n\"Find JavaScript courses and show me the top 5 results\"\n\"Search for machine learning tutorials with 15 results\"\n\"List available subtitles for this video: https://youtube.com/watch?v=...\"\n\"Download a video from facebook: https://facebook.com/...\"\n\"Download Chinese subtitles from this video: https://youtube.com/watch?v=...\"\n\"Download this video in 1080p: https://youtube.com/watch?v=...\"\n\"Download audio from this YouTube video: https://youtube.com/watch?v=...\"\n\"Get a clean transcript of this video: https://youtube.com/watch?v=...\"\n\"Download Spanish transcript from this video: https://youtube.com/watch?v=...\"\n\"Get metadata for this video: https://youtube.com/watch?v=...\"\n\"Show me the title, description, and channel info for this video: https://youtube.com/watch?v=...\"\n\"Get a summary of this video's metadata: https://youtube.com/watch?v=...\"\n\"Extract just the id, title, and view count from this video: https://youtube.com/watch?v=...\"\n```\n\n## Manual Start\n\nIf needed, start the server manually:\n```bash\nnpx @kevinwatt/yt-dlp-mcp\n```\n\n## Requirements\n\n* Node.js 20+\n* `yt-dlp` in system PATH\n* MCP-compatible LLM service\n\n\n## Documentation\n\n- [API Reference](./docs/api.md)\n- [Configuration](./docs/configuration.md)\n- [Error Handling](./docs/error-handling.md)\n- [Contributing](./docs/contributing.md)\n\n\n## License\n\nMIT\n\n## Author\n\nDewei Yen\n\n\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "srt",
        "yt",
        "dlp",
        "yt dlp",
        "subtitles srt",
        "srt format"
      ],
      "category": "web-search"
    },
    "kimtth--mcp-aoai-web-browsing": {
      "owner": "kimtth",
      "name": "mcp-aoai-web-browsing",
      "url": "https://github.com/kimtth/mcp-aoai-web-browsing",
      "imageUrl": "/freedevtools/mcp/pfp/kimtth.webp",
      "description": "An MCP server for web browsing automation that integrates Azure OpenAI to facilitate interactions with web applications through an automated interface. It utilizes Playwright for end-to-end testing and customizes responses to fit the OpenAI function calling format.",
      "stars": 30,
      "forks": 11,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-08-19T23:49:01Z",
      "readme_content": "## MCP Server & Client implementation for using Azure OpenAI\n\n<!-- [![smithery badge](https://smithery.ai/badge/mcp-web-auto)](https://smithery.ai/server/mcp-web-auto) -->\n\n- A minimal server/client application implementation utilizing the Model Context Protocol (MCP) and Azure OpenAI.\n\n    1. The MCP server is built with `FastMCP`.  \n    2. `Playwright` is an an open source, end to end testing framework by Microsoft for testing your modern web applications. \n    3. The MCP response about tools will be converted to the OpenAI function calling format.  \n    4. The bridge that converts the MCP server response to the OpenAI function calling format customises the `MCP-LLM Bridge` implementation.\n    5. To ensure a stable connection, the server object is passed directly into the bridge. \n\n## Model Context Protocol (MCP)\n\n**Model Context Protocol (MCP)** MCP (Model Context Protocol) is an open protocol that enables secure, controlled interactions between AI applications and local or remote resources. \n\n### Official Repositories\n\n- [MCP Python SDK](https://github.com/modelcontextprotocol/python-sdk)  \n- [Create Python Server](https://github.com/modelcontextprotocol/create-python-server)  \n- [MCP Servers](https://github.com/modelcontextprotocol/servers)  \n\n### Community Resources\n\n- [Awesome MCP Servers](https://github.com/punkpeye/awesome-mcp-servers)  \n- [MCP on Reddit](https://www.reddit.com/r/mcp/)  \n\n### Related Projects\n\n- [FastMCP](https://github.com/jlowin/fastmcp): The fast, Pythonic way to build MCP servers.\n- [Chat MCP](https://github.com/daodao97/chatmcp): MCP client\n- [MCP-LLM Bridge](https://github.com/bartolli/mcp-llm-bridge): MCP implementation that enables communication between MCP servers and OpenAI-compatible LLMs\n\n### MCP Playwright\n\n- [MCP Playwright server](https://github.com/executeautomation/mcp-playwright)  \n- [Microsoft Playwright for Python](https://github.com/microsoft/playwright-python)  \n\n### Configuration\n\nDuring the development phase in December 2024, the Python project should be initiated with 'uv'. Other dependency management libraries, such as 'pip' and 'poetry', are not yet fully supported by the MCP CLI.\n\n1. Rename `.env.template` to `.env`, then fill in the values in `.env` for Azure OpenAI:\n\n    ```bash\n    AZURE_OPEN_AI_ENDPOINT=\n    AZURE_OPEN_AI_API_KEY=\n    AZURE_OPEN_AI_DEPLOYMENT_MODEL=\n    AZURE_OPEN_AI_API_VERSION=\n    ```\n\n1. Install `uv` for python library management\n\n    ```bash\n    pip install uv\n    uv sync\n    ```\n\n1. Execute `python chatgui.py`\n\n    - The sample screen shows the client launching a browser to navigate to the URL.\n\n    \n\n### w.r.t. 'stdio'\n\n`stdio` is a **transport layer** (raw data flow), while **JSON-RPC** is an **application protocol** (structured communication). They are distinct but often used interchangeably, e.g., \"JSON-RPC over stdio\" in protocols.\n\n### Tool description\n\n```cmd\n@self.mcp.tool()\nasync def playwright_navigate(url: str, timeout=30000, wait_until=\"load\"):\n    \"\"\"Navigate to a URL.\"\"\" -> This comment provides a description, which may be used in a mechanism similar to function calling in LLMs.\n\n# Output\nTool(name='playwright_navigate', description='Navigate to a URL.', inputSchema={'properties': {'url': {'title': 'Url', 'type': 'string'}, 'timeout': {'default': 30000, 'title': 'timeout', 'type': 'string'}\n```\n\n### Tip: uv\n\n- [features](https://docs.astral.sh/uv/getting-started/features)\n\n```\nuv run: Run a script.\nuv venv: Create a new virtual environment. By default, '.venv'.\nuv add: Add a dependency to a script\nuv remove: Remove a dependency from a script\nuv sync: Sync (Install) the project's dependencies with the environment.\n```\n\n### Tip\n\n- taskkill command for python.exe\n\n```cmd\ntaskkill /IM python.exe /F\n```\n- Visual Code: Python Debugger: Debugging with launch.json will start the debugger using the configuration from .vscode/launch.json.\n\n<!-- ### Sample query\n\nNavigate to website http://eaapp.somee.com and click the login link. In the login page, enter the username and password as \"admin\" and \"password\" respectively and perform login. Then click the Employee List page and click \"Create New\" button and enter realistic employee details to create for Name, Salary, DurationWorked, Select dropdown for Grade as CLevel and Email. -->",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "openai",
        "aoai",
        "mcp",
        "aoai web",
        "azure openai",
        "openai facilitate"
      ],
      "category": "web-search"
    },
    "kirayhz007--GitHub-Chinese-Top-Charts": {
      "owner": "kirayhz007",
      "name": "GitHub-Chinese-Top-Charts",
      "url": "https://github.com/kirayhz007/GitHub-Chinese-Top-Charts",
      "imageUrl": "/freedevtools/mcp/pfp/kirayhz007.webp",
      "description": "A platform for discovering popular Chinese open-source projects on GitHub, facilitating the absorption of valuable experiences and results without language barriers. It provides a ranking of the most favored Chinese repositories to enhance development efficiency.",
      "stars": 0,
      "forks": 0,
      "license": "Other",
      "language": "",
      "updated_at": "2023-01-01T00:58:36Z",
      "readme_content": "<p align=\"center\">\n    \n</p>\n\n<h1 align=\"center\">GitHub中文排行榜</h1>\n\n<div align=\"center\">\n    「帮助你发现优秀中文项目，可以无语言障碍地、更高效地吸收优秀经验成果」\n</div>\n\n<br />\n\n<div align=\"center\">\n    <p><sub>↓ -- 项目说明 -- ↓</sub></p>\n    <a href=\"content/docs/features.md\">特色亮点</a> •\n    <a href=\"content/docs/definition_of_Chinese_repo.md\">中文定义</a> •\n    <a href=\"content/docs/inclusion_rules.md\">收录规则</a> •\n    <a href=\"content/docs/milestone.md\">重要更新</a> •\n    <a href=\"content/docs/feedback.md\">问题反馈</a> •\n    <a href=\"LICENSE.md\">许可协议</a>\n</div>\n\n<br />\n\n<div align=\"center\">\n    <p><sub>↓ -- 进阶之路 -- ↓</sub></p>\n    中文项目已通关？通往更广阔世界的路：<a href=\"https://github.com/kon9chunkit/GitHub-English-Top-Charts\">GitHub英文排行榜</a>\n</div>\n\n<br />\n\n<div align=\"center\">\n    <p><sub>↓ -- 内容目录 -- ↓</sub></p>\n    <table>\n        <tr>\n            <td colspan=\"2\" align=\"center\">中文总榜</td>\n            <td colspan=\"2\" align=\"center\">中文增速榜</td>  \n            <td colspan=\"2\" align=\"center\">中文新秀榜</td>\n        </tr>\n        <tr>\n            <td align=\"center\">软件类</td>\n            <td align=\"center\">资料类</td>\n            <td align=\"center\">软件类</td>\n            <td align=\"center\">资料类</td>\n            <td align=\"center\">软件类</td> \n            <td align=\"center\">资料类</td> \n        </tr>\n        <tr>\n            <td align=\"center\"><a href=\"content/charts/overall/software/All-Language.md\">All Language</a></td> \n            <td align=\"center\"><a href=\"content/charts/overall/knowledge/All-Language.md\">All Language</a></td> \n            <td align=\"center\"><a href=\"content/charts/growth/software/All-Language.md\">All Language</a></td> \n            <td align=\"center\"><a href=\"content/charts/growth/knowledge/All-Language.md\">All Language</a></td> \n            <td align=\"center\"><a href=\"content/charts/new_repo/software/All-Language.md\">All Language</a></td> \n            <td align=\"center\"><a href=\"content/charts/new_repo/knowledge/All-Language.md\">All Language</a></td> \n        </tr>\n        <tr>\n            <td align=\"center\"><a href=\"content/charts/overall/software/JavaScript.md\">JavaScript</a></td> \n            <td align=\"center\"><a href=\"content/charts/overall/knowledge/JavaScript.md\">JavaScript</a></td> \n            <td align=\"center\"><a href=\"content/charts/growth/software/JavaScript.md\">JavaScript</a></td> \n            <td align=\"center\"><a href=\"content/charts/growth/knowledge/JavaScript.md\">JavaScript</a></td> \n            <td align=\"center\"><a href=\"content/charts/new_repo/software/JavaScript.md\">JavaScript</a></td> \n            <td align=\"center\"><a href=\"content/charts/new_repo/knowledge/JavaScript.md\">JavaScript</a></td> \n        </tr>\n        <tr>\n            <td align=\"center\"><a href=\"content/charts/overall/software/Java.md\">Java</a></td> \n            <td align=\"center\"><a href=\"content/charts/overall/knowledge/Java.md\">Java</a></td> \n            <td align=\"center\"><a href=\"content/charts/growth/software/Java.md\">Java</a></td> \n            <td align=\"center\"><a href=\"content/charts/growth/knowledge/Java.md\">Java</a></td> \n            <td align=\"center\"><a href=\"content/charts/new_repo/software/Java.md\">Java</a></td> \n            <td align=\"center\"><a href=\"content/charts/new_repo/knowledge/Java.md\">Java</a></td> \n        </tr>\n        <tr>\n            <td align=\"center\"><a href=\"content/charts/overall/software/Python.md\">Python</a></td> \n            <td align=\"center\"><a href=\"content/charts/overall/knowledge/Python.md\">Python</a></td> \n            <td align=\"center\"><a href=\"content/charts/growth/software/Python.md\">Python</a></td> \n            <td align=\"center\"><a href=\"content/charts/growth/knowledge/Python.md\">Python</a></td> \n            <td align=\"center\"><a href=\"content/charts/new_repo/software/Python.md\">Python</a></td> \n            <td align=\"center\"><a href=\"content/charts/new_repo/knowledge/Python.md\">Python</a></td> \n        </tr>\n        <tr>\n            <td align=\"center\"><a href=\"content/charts/overall/software/Go.md\">Go</a></td> \n            <td align=\"center\"><a href=\"content/charts/overall/knowledge/Go.md\">Go</a></td> \n            <td align=\"center\"><a href=\"content/charts/growth/software/Go.md\">Go</a></td> \n            <td align=\"center\"><a href=\"content/charts/growth/knowledge/Go.md\">Go</a></td> \n            <td align=\"center\"><a href=\"content/charts/new_repo/software/Go.md\">Go</a></td> \n            <td align=\"center\"><a href=\"content/charts/new_repo/knowledge/Go.md\">Go</a></td> \n        </tr>\n        <tr>\n            <td align=\"center\"><a href=\"content/charts/overall/software/TypeScript.md\">TypeScript</a></td> \n            <td align=\"center\"><a href=\"content/charts/overall/knowledge/TypeScript.md\">TypeScript</a></td> \n            <td align=\"center\"><a href=\"content/charts/growth/software/TypeScript.md\">TypeScript</a></td> \n            <td align=\"center\"><a href=\"content/charts/growth/knowledge/TypeScript.md\">TypeScript</a></td> \n            <td align=\"center\"><a href=\"content/charts/new_repo/software/TypeScript.md\">TypeScript</a></td> \n            <td align=\"center\"><a href=\"content/charts/new_repo/knowledge/TypeScript.md\">TypeScript</a></td> \n        </tr>\n        <tr>\n            <td align=\"center\"><a href=\"content/charts/overall/software/Vue.md\">Vue</a></td> \n            <td align=\"center\"><a href=\"content/charts/overall/knowledge/Vue.md\">Vue</a></td> \n            <td align=\"center\"><a href=\"content/charts/growth/software/Vue.md\">Vue</a></td> \n            <td align=\"center\"><a href=\"content/charts/growth/knowledge/Vue.md\">Vue</a></td> \n            <td align=\"center\"><a href=\"content/charts/new_repo/software/Vue.md\">Vue</a></td> \n            <td align=\"center\"><a href=\"content/charts/new_repo/knowledge/Vue.md\">Vue</a></td> \n        </tr>\n        <tr>\n            <td align=\"center\"><a href=\"content/charts/overall/software/CPP.md\">C++</a></td> \n            <td align=\"center\"><a href=\"content/charts/overall/knowledge/CPP.md\">C++</a></td> \n            <td align=\"center\"><a href=\"content/charts/growth/software/CPP.md\">C++</a></td> \n            <td align=\"center\"><a href=\"content/charts/growth/knowledge/CPP.md\">C++</a></td> \n            <td align=\"center\"><a href=\"content/charts/new_repo/software/CPP.md\">C++</a></td> \n            <td align=\"center\"><a href=\"content/charts/new_repo/knowledge/CPP.md\">C++</a></td> \n        </tr>\n        <tr>\n            <td align=\"center\"><a href=\"content/charts/overall/software/C.md\">C</a></td> \n            <td align=\"center\"><a href=\"content/charts/overall/knowledge/C.md\">C</a></td> \n            <td align=\"center\"><a href=\"content/charts/growth/software/C.md\">C</a></td> \n            <td align=\"center\"><a href=\"content/charts/growth/knowledge/C.md\">C</a></td> \n            <td align=\"center\"><a href=\"content/charts/new_repo/software/C.md\">C</a></td> \n            <td align=\"center\"><a href=\"content/charts/new_repo/knowledge/C.md\">C</a></td> \n        </tr>\n        <tr>\n            <td align=\"center\"><a href=\"content/charts/overall/software/CSHARP.md\">C#</a></td> \n            <td align=\"center\"><a href=\"content/charts/overall/knowledge/CSHARP.md\">C#</a></td> \n            <td align=\"center\"><a href=\"content/charts/growth/software/CSHARP.md\">C#</a></td> \n            <td align=\"center\"><a href=\"content/charts/growth/knowledge/CSHARP.md\">C#</a></td> \n            <td align=\"center\"><a href=\"content/charts/new_repo/software/CSHARP.md\">C#</a></td> \n            <td align=\"center\"><a href=\"content/charts/new_repo/knowledge/CSHARP.md\">C#</a></td> \n        </tr>\n        <tr>\n            <td align=\"center\"><a href=\"content/charts/overall/software/PHP.md\">PHP</a></td> \n            <td align=\"center\"><a href=\"content/charts/overall/knowledge/PHP.md\">PHP</a></td> \n            <td align=\"center\"><a href=\"content/charts/growth/software/PHP.md\">PHP</a></td> \n            <td align=\"center\"><a href=\"content/charts/growth/knowledge/PHP.md\">PHP</a></td> \n            <td align=\"center\"><a href=\"content/charts/new_repo/software/PHP.md\">PHP</a></td> \n            <td align=\"center\"><a href=\"content/charts/new_repo/knowledge/PHP.md\">PHP</a></td> \n        </tr>\n        <tr>\n            <td align=\"center\"><a href=\"content/charts/overall/software/HTML.md\">HTML</a></td> \n            <td align=\"center\"><a href=\"content/charts/overall/knowledge/HTML.md\">HTML</a></td> \n            <td align=\"center\"><a href=\"content/charts/growth/software/HTML.md\">HTML</a></td> \n            <td align=\"center\"><a href=\"content/charts/growth/knowledge/HTML.md\">HTML</a></td> \n            <td align=\"center\"><a href=\"content/charts/new_repo/software/HTML.md\">HTML</a></td> \n            <td align=\"center\"><a href=\"content/charts/new_repo/knowledge/HTML.md\">HTML</a></td> \n        </tr>\n        <tr>\n            <td align=\"center\"><a href=\"content/charts/overall/software/Swift.md\">Swift</a></td> \n            <td align=\"center\"><a href=\"content/charts/overall/knowledge/Swift.md\">Swift</a></td> \n            <td align=\"center\"><a href=\"content/charts/growth/software/Swift.md\">Swift</a></td> \n            <td align=\"center\"><a href=\"content/charts/growth/knowledge/Swift.md\">Swift</a></td> \n            <td align=\"center\"><a href=\"content/charts/new_repo/software/Swift.md\">Swift</a></td> \n            <td align=\"center\"><a href=\"content/charts/new_repo/knowledge/Swift.md\">Swift</a></td> \n        </tr>\n        <tr>\n            <td align=\"center\"><a href=\"content/charts/overall/software/Kotlin.md\">Kotlin</a></td> \n            <td align=\"center\"><a href=\"content/charts/overall/knowledge/Kotlin.md\">Kotlin</a></td> \n            <td align=\"center\"><a href=\"content/charts/growth/software/Kotlin.md\">Kotlin</a></td> \n            <td align=\"center\"><a href=\"content/charts/growth/knowledge/Kotlin.md\">Kotlin</a></td> \n            <td align=\"center\"><a href=\"content/charts/new_repo/software/Kotlin.md\">Kotlin</a></td> \n            <td align=\"center\"><a href=\"content/charts/new_repo/knowledge/Kotlin.md\">Kotlin</a></td> \n        </tr>\n        <tr>\n            <td align=\"center\"><a href=\"content/charts/overall/software/Objective-C.md\">Objective-C</a></td> \n            <td align=\"center\"><a href=\"content/charts/overall/knowledge/Objective-C.md\">Objective-C</a></td> \n            <td align=\"center\"><a href=\"content/charts/growth/software/Objective-C.md\">Objective-C</a></td> \n            <td align=\"center\"><a href=\"content/charts/growth/knowledge/Objective-C.md\">Objective-C</a></td> \n            <td align=\"center\"><a href=\"content/charts/new_repo/software/Objective-C.md\">Objective-C</a></td> \n            <td align=\"center\"><a href=\"content/charts/new_repo/knowledge/Objective-C.md\">Objective-C</a></td> \n        </tr>\n        <tr>\n            <td align=\"center\"><a href=\"content/charts/overall/software/CSS.md\">CSS</a></td> \n            <td align=\"center\"><a href=\"content/charts/overall/knowledge/CSS.md\">CSS</a></td> \n            <td align=\"center\"><a href=\"content/charts/growth/software/CSS.md\">CSS</a></td> \n            <td align=\"center\"><a href=\"content/charts/growth/knowledge/CSS.md\">CSS</a></td> \n            <td align=\"center\"><a href=\"content/charts/new_repo/software/CSS.md\">CSS</a></td> \n            <td align=\"center\"><a href=\"content/charts/new_repo/knowledge/CSS.md\">CSS</a></td> \n        </tr>\n        <tr>\n            <td align=\"center\"><a href=\"content/charts/overall/software/Shell.md\">Shell</a></td> \n            <td align=\"center\"><a href=\"content/charts/overall/knowledge/Shell.md\">Shell</a></td> \n            <td align=\"center\"><a href=\"content/charts/growth/software/Shell.md\">Shell</a></td> \n            <td align=\"center\"><a href=\"content/charts/growth/knowledge/Shell.md\">Shell</a></td> \n            <td align=\"center\"><a href=\"content/charts/new_repo/software/Shell.md\">Shell</a></td> \n            <td align=\"center\"><a href=\"content/charts/new_repo/knowledge/Shell.md\">Shell</a></td> \n        </tr>\n        <tr>\n            <td align=\"center\"><a href=\"content/charts/overall/software/Dart.md\">Dart</a></td> \n            <td align=\"center\"><a href=\"content/charts/overall/knowledge/Dart.md\">Dart</a></td> \n            <td align=\"center\"><a href=\"content/charts/growth/software/Dart.md\">Dart</a></td> \n            <td align=\"center\"><a href=\"content/charts/growth/knowledge/Dart.md\">Dart</a></td> \n            <td align=\"center\"><a href=\"content/charts/new_repo/software/Dart.md\">Dart</a></td> \n            <td align=\"center\"><a href=\"content/charts/new_repo/knowledge/Dart.md\">Dart</a></td> \n        </tr>\n        <tr>\n            <td align=\"center\"><a href=\"content/charts/overall/software/Rust.md\">Rust</a></td> \n            <td align=\"center\"><a href=\"content/charts/overall/knowledge/Rust.md\">Rust</a></td> \n            <td align=\"center\"><a href=\"content/charts/growth/software/Rust.md\">Rust</a></td> \n            <td align=\"center\"><a href=\"content/charts/growth/knowledge/Rust.md\">Rust</a></td> \n            <td align=\"center\"><a href=\"content/charts/new_repo/software/Rust.md\">Rust</a></td> \n            <td align=\"center\"><a href=\"content/charts/new_repo/knowledge/Rust.md\">Rust</a></td> \n        </tr>\n        <tr>\n            <td align=\"center\"><a href=\"content/charts/overall/software/Jupyter-Notebook.md\">Jupyter Notebook</a></td> \n            <td align=\"center\"><a href=\"content/charts/overall/knowledge/Jupyter-Notebook.md\">Jupyter Notebook</a></td> \n            <td align=\"center\"><a href=\"content/charts/growth/software/Jupyter-Notebook.md\">Jupyter Notebook</a></td> \n            <td align=\"center\"><a href=\"content/charts/growth/knowledge/Jupyter-Notebook.md\">Jupyter Notebook</a></td> \n            <td align=\"center\"><a href=\"content/charts/new_repo/software/Jupyter-Notebook.md\">Jupyter Notebook</a></td> \n            <td align=\"center\"><a href=\"content/charts/new_repo/knowledge/Jupyter-Notebook.md\">Jupyter Notebook</a></td> \n        </tr>\n        <tr>\n            <td align=\"center\"><a href=\"content/charts/overall/software/Assembly.md\">Assembly</a></td> \n            <td align=\"center\"><a href=\"content/charts/overall/knowledge/Assembly.md\">Assembly</a></td> \n            <td align=\"center\"><a href=\"content/charts/growth/software/Assembly.md\">Assembly</a></td> \n            <td align=\"center\"><a href=\"content/charts/growth/knowledge/Assembly.md\">Assembly</a></td> \n            <td align=\"center\"><a href=\"content/charts/new_repo/software/Assembly.md\">Assembly</a></td> \n            <td align=\"center\"><a href=\"content/charts/new_repo/knowledge/Assembly.md\">Assembly</a></td> \n        </tr>\n        <tr>\n            <td align=\"center\"><a href=\"content/charts/overall/software/Vim-script.md\">Vim script</a></td> \n            <td align=\"center\"><a href=\"content/charts/overall/knowledge/Vim-script.md\">Vim script</a></td> \n            <td align=\"center\"><a href=\"content/charts/growth/software/Vim-script.md\">Vim script</a></td> \n            <td align=\"center\"><a href=\"content/charts/growth/knowledge/Vim-script.md\">Vim script</a></td> \n            <td align=\"center\"><a href=\"content/charts/new_repo/software/Vim-script.md\">Vim script</a></td> \n            <td align=\"center\"><a href=\"content/charts/new_repo/knowledge/Vim-script.md\">Vim script</a></td> \n        </tr>\n        <tr>\n            <td align=\"center\"><a href=\"content/charts/overall/software/Ruby.md\">Ruby</a></td> \n            <td align=\"center\"><a href=\"content/charts/overall/knowledge/Ruby.md\">Ruby</a></td> \n            <td align=\"center\"><a href=\"content/charts/growth/software/Ruby.md\">Ruby</a></td> \n            <td align=\"center\"><a href=\"content/charts/growth/knowledge/Ruby.md\">Ruby</a></td> \n            <td align=\"center\"><a href=\"content/charts/new_repo/software/Ruby.md\">Ruby</a></td> \n            <td align=\"center\"><a href=\"content/charts/new_repo/knowledge/Ruby.md\">Ruby</a></td> \n        </tr>\n        <tr>\n            <td align=\"center\"><a href=\"content/charts/overall/software/Lua.md\">Lua</a></td> \n            <td align=\"center\"><a href=\"content/charts/overall/knowledge/Lua.md\">Lua</a></td> \n            <td align=\"center\"><a href=\"content/charts/growth/software/Lua.md\">Lua</a></td> \n            <td align=\"center\"><a href=\"content/charts/growth/knowledge/Lua.md\">Lua</a></td> \n            <td align=\"center\"><a href=\"content/charts/new_repo/software/Lua.md\">Lua</a></td> \n            <td align=\"center\"><a href=\"content/charts/new_repo/knowledge/Lua.md\">Lua</a></td> \n        </tr>\n        <tr>\n            <td align=\"center\"><a href=\"content/charts/overall/software/TeX.md\">TeX</a></td> \n            <td align=\"center\"><a href=\"content/charts/overall/knowledge/TeX.md\">TeX</a></td> \n            <td align=\"center\"><a href=\"content/charts/growth/software/TeX.md\">TeX</a></td> \n            <td align=\"center\"><a href=\"content/charts/growth/knowledge/TeX.md\">TeX</a></td> \n            <td align=\"center\"><a href=\"content/charts/new_repo/software/TeX.md\">TeX</a></td> \n            <td align=\"center\"><a href=\"content/charts/new_repo/knowledge/TeX.md\">TeX</a></td> \n        </tr>\n        <tr>\n            <td align=\"center\"><a href=\"content/charts/overall/software/Groovy.md\">Groovy</a></td> \n            <td align=\"center\"><a href=\"content/charts/overall/knowledge/Groovy.md\">Groovy</a></td> \n            <td align=\"center\"><a href=\"content/charts/growth/software/Groovy.md\">Groovy</a></td> \n            <td align=\"center\"><a href=\"content/charts/growth/knowledge/Groovy.md\">Groovy</a></td> \n            <td align=\"center\"><a href=\"content/charts/new_repo/software/Groovy.md\">Groovy</a></td> \n            <td align=\"center\"><a href=\"content/charts/new_repo/knowledge/Groovy.md\">Groovy</a></td> \n        </tr>\n        <tr>\n            <td align=\"center\"><a href=\"content/charts/overall/software/Perl.md\">Perl</a></td> \n            <td align=\"center\"><a href=\"content/charts/overall/knowledge/Perl.md\">Perl</a></td> \n            <td align=\"center\"><a href=\"content/charts/growth/software/Perl.md\">Perl</a></td> \n            <td align=\"center\"><a href=\"content/charts/growth/knowledge/Perl.md\">Perl</a></td> \n            <td align=\"center\"><a href=\"content/charts/new_repo/software/Perl.md\">Perl</a></td> \n            <td align=\"center\"><a href=\"content/charts/new_repo/knowledge/Perl.md\">Perl</a></td> \n        </tr>\n        <tr>\n            <td align=\"center\"><a href=\"content/charts/overall/software/MATLAB.md\">MATLAB</a></td> \n            <td align=\"center\"><a href=\"content/charts/overall/knowledge/MATLAB.md\">MATLAB</a></td> \n            <td align=\"center\"><a href=\"content/charts/growth/software/MATLAB.md\">MATLAB</a></td> \n            <td align=\"center\"><a href=\"content/charts/growth/knowledge/MATLAB.md\">MATLAB</a></td> \n            <td align=\"center\"><a href=\"content/charts/new_repo/software/MATLAB.md\">MATLAB</a></td> \n            <td align=\"center\"><a href=\"content/charts/new_repo/knowledge/MATLAB.md\">MATLAB</a></td>\n        </tr>\n        <tr>\n            <td align=\"center\"><a href=\"content/charts/overall/software/Pascal.md\">Pascal</a></td> \n            <td align=\"center\"><a href=\"content/charts/overall/knowledge/Pascal.md\">Pascal</a></td> \n            <td align=\"center\"><a href=\"content/charts/growth/software/Pascal.md\">Pascal</a></td> \n            <td align=\"center\"><a href=\"content/charts/growth/knowledge/Pascal.md\">Pascal</a></td> \n            <td align=\"center\"><a href=\"content/charts/new_repo/software/Pascal.md\">Pascal</a></td> \n            <td align=\"center\"><a href=\"content/charts/new_repo/knowledge/Pascal.md\">Pascal</a></td> \n        </tr>\n        <tr>\n            <td align=\"center\"><a href=\"content/charts/overall/software/R.md\">R</a></td> \n            <td align=\"center\"><a href=\"content/charts/overall/knowledge/R.md\">R</a></td> \n            <td align=\"center\"><a href=\"content/charts/growth/software/R.md\">R</a></td> \n            <td align=\"center\"><a href=\"content/charts/growth/knowledge/R.md\">R</a></td> \n            <td align=\"center\"><a href=\"content/charts/new_repo/software/R.md\">R</a></td> \n            <td align=\"center\"><a href=\"content/charts/new_repo/knowledge/R.md\">R</a></td> \n        </tr>\n    </table>\n</div>\n\n<div align=\"center\">\n    <p><sub>↓ -- 感谢读者 -- ↓</sub></p>\n    榜单持续更新，如有帮助请加星收藏，方便后续浏览，感谢你的支持！\n</div>",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "github",
        "repositories",
        "search",
        "github chinese",
        "chinese repositories",
        "projects github"
      ],
      "category": "web-search"
    },
    "kouui--web-search-duckduckgo": {
      "owner": "kouui",
      "name": "web-search-duckduckgo",
      "url": "https://github.com/kouui/web-search-duckduckgo",
      "imageUrl": "/freedevtools/mcp/pfp/kouui.webp",
      "description": "Search the web using DuckDuckGo to retrieve relevant search results, including titles, URLs, and snippets. Optionally fetch and convert webpage content into markdown format with support for concurrent URL fetching and error handling.",
      "stars": 3,
      "forks": 9,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-07-29T04:23:05Z",
      "readme_content": "# DuckDuckGo Web Search MCP Server\n\nThis project provides an MCP (Model Context Protocol) server that allows you to search the web using the DuckDuckGo search engine and optionally fetch and summarize the content of the found URLs.\n\n## Features\n\n*   **Web Search:** Search the web using DuckDuckGo.\n*   **Result Extraction:** Extracts titles, URLs, and snippets from search results.\n*   **Content Fetching (Optional):** Fetches the content of the URLs found in the search results and converts it to markdown format using jina api.\n*   **Parallel Fetching:** Fetches multiple URLs concurrently for faster processing.\n*   **Error Handling:** Gracefully handles timeouts and other potential errors during search and fetching.\n*   **Configurable:** Allows you to set the maximum number of search results to return.\n* **Jina API**: using jina api to convert html to markdown.\n* **MCP Compliant**: This server is designed to be used with any MCP-compatible client.\n\n## Usage\n\n1.  **Prerequisites:**\n    *   `uvx` package manager\n\n\n2. **Claude Desktop Configuration**\n    * If you are using Claude Desktop, you can add the server to the `claude_desktop_config.json` file.\n    ```json\n    {\n        \"mcpServers\": {\n            \"web-search-duckduckgo\": {\n                \"command\": \"uvx\",\n                \"args\": [\n                    \"--from\",\n                    \"git+https://github.com/kouui/web-search-duckduckgo.git@main\",\n                    \"main.py\"\n                ]\n            }\n        }\n    }\n    ```\n    \n    **the above configuration is not working, you might need to clone the repository to local pc and use the following configuration**\n\n    ```json\n    {\n        \"mcpServers\": {\n            \"web-search-duckduckgo\": {\n                \"command\": \"uv\",\n                \"args\": [\n                    \"--directory\",\n                    \"/path/to/web-search-duckduckgo\",\n                    \"run\",\n                    \"main.py\"\n                ]\n            }\n        }\n    }\n    ```\n\n3. **Tool**\n    *   In your MCP client (e.g., Claude), you can now use the following tools:\n\n    *   **`search_and_fetch`:** Search the web and fetch the content of the URLs.\n\n        *   `query`: The search query string.\n        *   `limit`: The maximum number of results to return (default: 3, maximum: 10).\n\n\n    *   **`fetch`:** Fetch the content of a specific URL.\n\n        *   `url`: The URL to fetch.\n\n\n## License\n\nThis project is licensed under the MIT License. (Add a license file if you want to specify a license).\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "duckduckgo",
        "kouui",
        "search",
        "duckduckgo search",
        "kouui web",
        "search duckduckgo"
      ],
      "category": "web-search"
    },
    "kukapay--crypto-sentiment-mcp": {
      "owner": "kukapay",
      "name": "crypto-sentiment-mcp",
      "url": "https://github.com/kukapay/crypto-sentiment-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/kukapay.webp",
      "description": "Provides cryptocurrency sentiment analysis by aggregating social media and news data to assess market mood and detect trends. It offers insights into sentiment balance, social media mentions, and trending discussions surrounding various cryptocurrencies.",
      "stars": 36,
      "forks": 12,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-24T19:44:06Z",
      "readme_content": "# Crypto Sentiment MCP Server\n\nAn MCP server that delivers cryptocurrency sentiment analysis to AI agents, leveraging Santiment's aggregated social media and news data to track market mood and detect emerging trends.\n\n![GitHub License](https://img.shields.io/github/license/kukapay/crypto-sentiment-mcp)\n![Python Version](https://img.shields.io/badge/python-3.10+-blue)\n![Status](https://img.shields.io/badge/status-active-brightgreen.svg)\n\n\n## Features\n\n- **Sentiment Analysis**: Retrieve sentiment balance (positive vs. negative) for specific cryptocurrencies.\n- **Social Volume Tracking**: Monitor total social media mentions and detect significant shifts (spikes or drops).\n- **Social Dominance**: Measure the share of discussions an asset occupies in crypto media.\n- **Trending Words**: Identify the most popular terms trending in cryptocurrency discussions.\n\n## Tools\n\n| Tool Name               | Description                                                                                   | Parameters                                  |\n|-------------------------|-----------------------------------------------------------------------------------------------|---------------------------------------------|\n| `get_sentiment_balance` | Get the average sentiment balance for an asset over a specified period.                      | `asset: str`, `days: int = 7`              |\n| `get_social_volume`     | Fetch the total number of social media mentions for an asset.                                | `asset: str`, `days: int = 7`              |\n| `alert_social_shift`    | Detect significant spikes or drops in social volume compared to the previous average.        | `asset: str`, `threshold: float = 50.0`, `days: int = 7` |\n| `get_trending_words`    | Retrieve the top trending words in crypto discussions, ranked by score over a period.        | `days: int = 7`, `top_n: int = 5`          |\n| `get_social_dominance`  | Measure the percentage of crypto media discussions dominated by an asset.                    | `asset: str`, `days: int = 7`              |\n\n## Prerequisites\n\n- **Python**: 3.10 or higher\n- **Santiment API Key**: Obtain a free or paid key from [Santiment](https://app.santiment.net/). \n\n## Installation\n\n1. **Clone the Repository**:\n   ```bash\n   git clone https://github.com/kukapay/crypto-sentiment-mcp.git\n   cd crypto-sentiment-mcp\n   ```\n\n2. **Configure Client**:\n    ```\n    {\n      \"mcpServers\": {\n        \"crypto-sentiment-mcp\": {\n          \"command\": \"uv\",\n          \"args\": [\"--directory\", \"path/to/crypto-sentiment-mcp\", \"run\", \"main.py\"],\n          \"env\": {\n            \"SANTIMENT_API_KEY\": \"your_api_key_here\"\n          }\n        }\n      }\n    }\n    ```  \n\n## Examples\n\nBelow are examples of natural language inputs and their corresponding outputs when interacting with the server via an MCP-compatible client:\n\n- **Input**: \"What's the sentiment balance for Bitcoin over the last week?\"\n  - **Output**: \"Bitcoin's sentiment balance over the past 7 days is 12.5.\"\n\n- **Input**: \"How many times has Ethereum been mentioned on social media in the past 5 days?\"\n  - **Output**: \"Ethereum's social volume over the past 5 days is 8,432 mentions.\"\n\n- **Input**: \"Tell me if there's been a big change in Bitcoin's social volume recently, with a 30% threshold.\"\n  - **Output**: \"Bitcoin's social volume spiked by 75.0% in the last 24 hours, from an average of 1,000 to 1,750.\"\n\n- **Input**: \"What are the top 3 trending words in crypto over the past 3 days?\"\n  - **Output**: \"Top 3 trending words over the past 3 days: 'halving', 'bullrun', 'defi'.\"\n\n- **Input**: \"How dominant is Ethereum in social media discussions this week?\"\n  - **Output**: \"Ethereum's social dominance over the past 7 days is 18.7%.\"\n\n\n## License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "cryptocurrencies",
        "cryptocurrency",
        "crypto",
        "cryptocurrency sentiment",
        "crypto sentiment",
        "kukapay crypto"
      ],
      "category": "web-search"
    },
    "kukapay--crypto-whitepapers-mcp": {
      "owner": "kukapay",
      "name": "crypto-whitepapers-mcp",
      "url": "https://github.com/kukapay/crypto-whitepapers-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/kukapay.webp",
      "description": "Provides structured access to cryptocurrency whitepapers for searching, loading, and querying detailed content, enabling efficient analysis of tokenomics and project-specific insights.",
      "stars": 4,
      "forks": 6,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-24T02:53:19Z",
      "readme_content": "# Crypto Whitepapers MCP Server\n\nAn MCP server serving as a structured knowledge base of crypto whitepapers for AI agents to access, analyze, and learn from.\n\n[![Discord](https://img.shields.io/discord/1353556181251133481?cacheSeconds=3600)](https://discord.gg/aRnuu2eJ)\n![GitHub License](https://img.shields.io/github/license/kukapay/crypto-whitepapers-mcp)\n![Python Version](https://img.shields.io/badge/python-3.10+-blue)\n![Status](https://img.shields.io/badge/status-active-brightgreen.svg)\n\n\n## Features\n\n- **Search Whitepapers**: Use DuckDuckGo to find whitepaper PDFs for cryptocurrency projects.\n- **Load Whitepapers**: Download and index whitepaper PDFs into the knowledge base.\n- **Query Knowledge Base**: Query whitepaper content with optional project filtering.\n- **List Projects**: View all projects available in the knowledge base.\n- **Claude Desktop Integration**: Access tools and prompts via MCP in Claude Desktop.\n\n## Prerequisites\n\n- Python 3.10+\n- [uv](https://github.com/astral-sh/uv) for dependency management and running\n- Internet access.\n- [Claude Desktop](https://claude.ai/download) for MCP integration (optional)\n\n## Setup\n\n1. **Clone the Repository**:\n   ```bash\n   git clone https://github.com/kukapay/crypto-whitepapers-mcp.git\n   cd crypto-whitepapers-mcp\n   ```\n\n2. **Install Dependencies with uv**:\n   ```bash\n   uv sync\n   ```\n\n5. **Integrate with Claude Desktop** (Optional):\n   - Edit the Claude Desktop configuration file:\n     - **MacOS**: `~/Library/Application Support/Claude/claude_desktop_config.json`\n     - **Windows**: `%APPDATA%\\Claude\\claude_desktop_config.json`\n   - Add the following configuration:\n     ```json\n     {\n         \"mcpServers\": {\n             \"crypto-whitepapers\": {\n                 \"command\": \"uv\",\n                 \"args\": [\n                     \"--directory\",\n                     \"/absolute/path/to/crypto-whitepapers-mcp\"   \n                     \"run\",\n                     \"crypto-whitepapers-mcp\"\n                 ]\n             }\n         }\n     }\n     ```\n     Replace `/absolute/path/to/crypto-whitepapers-mcp` with the absolute path to the repository.\n   - Restart Claude Desktop and verify the server is loaded (look for the hammer icon in the input box).\n\n## Usage\n\n### Tools\nThe following tools are available via the MCP server:\n\n- **`list_available_projects()`**: Lists all projects in the knowledge base (derived from PDF filenames).\n  - Example: `list_available_projects()`\n  - Returns: JSON list of project names.\n  \n- **`search_whitepaper(project_name: str)`**: Searches for a project's whitepaper PDF using DuckDuckGo.\n  - Example: `search_whitepaper(\"bitcoin\")`\n  - Returns: JSON list of up to 5 results with title, URL, and snippet.\n\n- **`load_whitepaper(project_name: str, url: str)`**: Downloads a whitepaper PDF from a URL and loads it into the knowledge base.\n  - Example: `load_whitepaper(\"bitcoin\", \"https://bitcoin.org/bitcoin.pdf\")`\n  - Returns: Success or error message.\n\n- **`ask_whitepapers(query: str, project_name: str = None)`**: Searches the knowledge base for a query, optionally filtered by project.\n  - Example: `ask_whitepapers(\"blockchain technology\", \"bitcoin\")`\n  - Returns: Up to 5 matching text snippets.\n\n\n\n### Prompts\n- **`analyze_tokenomics(project_name: str)`**: Analyzes tokenomics (distribution, supply, incentives) in a project's whitepaper using the `ask_whitepapers` tool.\n  - Example: In Claude Desktop, run \"Analyze the tokenomics of Ethereum.\"\n\n### Examples\n1. List available projects:\n   ```\n   List all available projects.\n   ```\n2. Search for a whitepaper:\n   ```\n   Search for the Bitcoin whitepaper PDF.\n   ```\n3. Load a whitepaper:\n   ```\n   Load the Bitcoin whitepaper from https://bitcoin.org/bitcoin.pdf.\n   ```\n4. Query the knowledge base:\n   ```\n   Ask the knowledge base about blockchain technology in the Bitcoin whitepaper.\n   ```\n   \n## License\n\nThis project is licensed under the MIT License. See the [LICENSE](LICENSE) file for details.\n\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "crypto",
        "tokenomics",
        "cryptocurrency",
        "cryptocurrency whitepapers",
        "crypto whitepapers",
        "kukapay crypto"
      ],
      "category": "web-search"
    },
    "kukapay--hf-trending-mcp": {
      "owner": "kukapay",
      "name": "hf-trending-mcp",
      "url": "https://github.com/kukapay/hf-trending-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/kukapay.webp",
      "description": "Tracks and analyzes trending models, datasets, and spaces on Hugging Face, providing tools to fetch and search for content. Enables trend analysis to help users stay informed about the latest developments in AI resources.",
      "stars": 2,
      "forks": 2,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-08-12T18:44:48Z",
      "readme_content": "# HF Trending MCP Server\n\nAn MCP server that tracks trending models, datasets, and spaces on [Hugging Face](https://huggingface.co). \n\nIt provides tools and prompts to fetch trending content, search for specific items, and analyze current trends, making it a valuable tool for developers, researchers, and AI enthusiasts.\n\n![License](https://img.shields.io/badge/license-MIT-green.svg) \n![Python](https://img.shields.io/badge/python-3.10+-blue.svg) \n![Status](https://img.shields.io/badge/status-active-brightgreen.svg)\n\n\n## Features\n\n- **Trending Tools**: Retrieve the top trending models, datasets, and spaces with customizable limits.\n- **Search Functionality**: Search trending items by query across models, datasets, or spaces.\n- **Trend Analysis Prompt**: A built-in prompt to analyze and summarize current trends on Hugging Face.\n\n\n## Installation\n\n### Setup\n\n1. **Clone the Repository**:\n   ```bash\n   git clone https://github.com/kukapay/hf-trending-mcp.git\n   cd hf-trending-mcp\n   ```\n\n2. **Install Dependencies**:\n   ```bash\n   pip install mcp[cli] httpx\n   ```\n\n3. **Client Configuration**:\n    ```\n    \"mcpServers\": { \n      \"hf-trending-mcp\": { \n        \"command\": \"python\", \n        \"args\": [ \"path/to/hf-trending-mcp/main.py\" ]\n      } \n    }\n    ```\n\n## Usage\n\nOnce the server is running or installed, you can interact with it using an MCP client (e.g., Claude Desktop). Below are some example prompts:\n\n### Fetch Trending Content\n**Input**: \n> \"Show me the top 5 trending models on Hugging Face.\"\n\n**Output**:\n```\ngoogle/vit-base-patch16-224 (Downloads: 12345, Likes: 678)\nTags: vision, transformer\n\nfacebook/detr-resnet-50 (Downloads: 9876, Likes: 543)\nTags: object-detection, transformer\n...\n```\n\n**Input**: \n> \"What are the top 10 trending datasets right now?\"\n\n**Output**:\n```\nimdb (Downloads: 45678, Likes: 2345)\nTags: nlp, sentiment-analysis\n\nsquad (Downloads: 34567, Likes: 1987)\nTags: nlp, question-answering\n...\n```\n\n**Input**: \n> \"List the top 3 trending spaces.\"\n\n**Output**:\n```\ngradio/image-classifier (Likes: 890, SDK: Gradio)\nTags: demo, vision\n\nstreamlit/text-generator (Likes: 765, SDK: Streamlit)\nTags: nlp, demo\n...\n```\n\n### Search Trending Items\n\n**Input**: \n> \"Search for trending NLP models, limit to 5.\"\n\n**Output**:\n```\nbert-base-uncased (Likes: 456)\nTags: nlp, transformer\n\ndistilbert-base-uncased (Likes: 321)\nTags: nlp, transformer\n...\n```\n\n**Input**: \n> \"Find trending datasets about computer vision.\"\n\n**Output**:\n  ```\n  coco (Likes: 1234)\n  Tags: computer-vision, object-detection\n\n  imagenet-1k (Likes: 987)\n  Tags: computer-vision, classification\n  ...\n  ```\n\n### Analyze Trends\n\n**Input**: \n> \"Analyze the current trends on Hugging Face.\"\n \n**Output**: \n\nA detailed response combining results from all trending tools, followed by an analysis (generated by the client’s AI):\n```\nTop Trending Models:\ngoogle/vit-base-patch16-224 (Downloads: 12345, Likes: 678)\nTags: vision, transformer\n...\n\nTop Trending Datasets:\nimdb (Downloads: 45678, Likes: 2345)\nTags: nlp, sentiment-analysis\n...\n\nTop Trending Spaces:\ngradio/image-classifier (Likes: 890, SDK: Gradio)\nTags: demo, vision\n...\n\nSummary: Vision models like ViT are trending, likely due to recent advancements in image processing. NLP datasets remain popular for text-based AI research, and Gradio spaces are gaining traction for interactive demos.\n```\n\n\n## API Details\n\n### Tools\n- **`get_trending_models(limit: int = 10)`**: Fetches trending models with downloads, likes, and tags.\n- **`get_trending_datasets(limit: int = 10)`**: Fetches trending datasets with downloads, likes, and tags.\n- **`get_trending_spaces(limit: int = 10)`**: Fetches trending spaces with likes, SDK info, and tags.\n- **`search_trending(query: str, type: str = \"models\", limit: int = 10)`**: Searches trending items by query and type.\n\n### Prompt\n- **`analyze_trends()`**: Guides the analysis of trending items with a structured prompt.\n\n## License\n\nThis project is licensed under the MIT License. See the [LICENSE](LICENSE) file for details.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "trending",
        "trend",
        "kukapay",
        "trending models",
        "analyzes trending",
        "hf trending"
      ],
      "category": "web-search"
    },
    "kukapay--nearby-search-mcp": {
      "owner": "kukapay",
      "name": "nearby-search-mcp",
      "url": "https://github.com/kukapay/nearby-search-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/kukapay.webp",
      "description": "Search for nearby places using IP-based location detection and Google Places integration. Provides a customizable interface to filter results based on keywords and radius.",
      "stars": 17,
      "forks": 6,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-29T11:38:51Z",
      "readme_content": "# NearbySearch MCP Server\n\nAn MCP server for nearby place searches with IP-based location detection.\n\n![GitHub License](https://img.shields.io/github/license/kukapay/nearby-search-mcp) \n![GitHub Last Commit](https://img.shields.io/github/last-commit/kukapay/nearby-search-mcp) \n![Python Version](https://img.shields.io/badge/python-3.10%2B-blue)\n\n## Features\n\n- **IP-based Location Detection**: Uses ipapi.co to determine your current location\n- **Google Places Integration**: Searches for nearby places based on keywords and optional type filters\n- **Simple Interface**: Single tool endpoint with customizable radius\n\n## Requirements\n\n- Python 3.10+\n- Google Cloud Platform API Key with Places API enabled\n- Internet connection\n\n## Installation\n\n1. Clone the repository:\n```bash\ngit clone https://github.com/kukapay/nearby-search-mcp.git\ncd nearby-search-mcp\n```\n\n2. Install dependencies:\n```bash\n# Using uv (recommended)\nuv add \"mcp[cli]\" httpx python-dotenv\n\n# Or using pip\npip install mcp httpx python-dotenv\n```\n\n3. Client Configuration\n\n```json\n{\n  \"mcpServers\": {\n    \"nearby-search\": {\n      \"command\": \"uv\",\n      \"args\": [\"--directory\", \"path/to/nearby-search-mcp\", \"run\", \"main.py\"],\n      \"env\": {\n        \"GOOGLE_API_KEY\": \"your google api key\"\n      }\n    }\n  }\n}\n````\n\n## Usage\n\n### Running the Server\n\n- **Development Mode** (with MCP Inspector):\n```bash\nmcp dev main.py\n```\n\n- **Install in Claude Desktop**:\n```bash\nmcp install main.py --name \"NearbySearch\"\n```\n\n- **Direct Execution**:\n```bash\npython main.py\n```\n\n### Available Endpoints\n\n**Tool: `search_nearby`**\n - Searches for places near your current location\n - Parameters:\n   - `keyword` (str): What to search for (e.g., \"coffee shop\")\n   - `radius` (int, optional): Search radius in meters (default: 1500)\n   - `type` (str, optional): Place type (e.g., \"restaurant\", \"cafe\")\n\n\n## License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "kukapay",
        "nearby",
        "location",
        "search nearby",
        "google places",
        "nearby search"
      ],
      "category": "web-search"
    },
    "kxkaloo--mcp": {
      "owner": "kxkaloo",
      "name": "mcp",
      "url": "https://github.com/kxkaloo/mcp",
      "imageUrl": "/freedevtools/mcp/pfp/null.webp",
      "description": "Extract structured data from websites by providing a URL and a clear prompt describing the data to extract. Retrieve detailed product information, pricing, descriptions, and more from web pages seamlessly.",
      "stars": 0,
      "forks": 0,
      "license": "Unknown",
      "language": "Unknown",
      "updated_at": "",
      "readme_content": "",
      "npm_url": "https://www.npmjs.com/package/mcp",
      "npm_downloads": 0,
      "keywords": [
        "kxkaloo",
        "search",
        "extract",
        "search kxkaloo",
        "kxkaloo mcp",
        "web search"
      ],
      "category": "web-search"
    },
    "laosu888--poe": {
      "owner": "laosu888",
      "name": "poe",
      "url": "https://github.com/laosu888/poe",
      "imageUrl": "/freedevtools/mcp/pfp/laosu888.webp",
      "description": "Facilitates automated interactions with Poe.com by managing message sending, bot operations, and file attachments. Provides easy access to various AI models and chat functionalities for enhanced application integration.",
      "stars": 1,
      "forks": 0,
      "license": "GNU General Public License v3.0",
      "language": "Python",
      "updated_at": "2025-07-23T15:25:19Z",
      "readme_content": "<div align=\"center\">\n<a href=\"https://github.com/snowby666\">\n<img alt=\"logo_nobg_png_name_1_owner_1_pattern_Charlie_Brown_pulls_1_stargazers_1_theme_Auto\" src=\"https://socialify.git.ci/snowby666/poe-api-wrapper/image?font=Raleway&forks=1&issues=1&language=1&logo=https://i.ibb.co/xHrZxFY/logo-nobg.png&name=1&owner=1&pattern=Charlie%20Brown&pulls=1&stargazers=1&theme=Auto\" width=\"700\" height=\"350\"></a>\n\n<h1>Poe API Wrapper <img alt=\"favicon\" src=\"https://psc2.cf2.poecdn.net/favicon.svg\" height=\"35\"></h1>\n\n<p><em>A simple, lightweight and efficient API wrapper for Poe.com</em></p>\n</div>\n\n<p align=\"center\">\n<a href=\"https://pypi.org/project/poe-api-wrapper/\"><img alt=\"poe_api_wrapper\" src=\"https://img.shields.io/pypi/v/poe-api-wrapper\"></a>\n<img alt=\"Python Version\" src=\"https://img.shields.io/badge/python-3.7+-blue.svg\" alt=\"python\">\n<a href=\"https://www.pepy.tech/projects/poe-api-wrapper\">\n<img alt=\"PyPI - Downloads\" src=\"https://static.pepy.tech/badge/poe-api-wrapper\"></a>\n<a href=\"https://discord.gg/apUUqbxCBQ\">\n<img alt=\"Support Server\" src=\"https://dcbadge.limes.pink/api/server/https://discord.com/invite/apUUqbxCBQ?style=flat\"></a>\n<br>\n</p>\n\n## 📚 Table of Contents\n- [📚 Table of Contents](#-table-of-contents)\n- [✨ Highlights](#-highlights)\n- [🔧 Installation](#-installation)\n- [🦄 Documentation](#-documentation)\n  - [Available Default Bots](#available-default-bots)\n  - [How to get your Token](#how-to-get-your-token)\n    - [Getting p-b and p-lat cookies (*required*)](#getting-p-b-and-p-lat-cookies-required)\n    - [Getting formkey (*optional*)](#getting-formkey-optional)\n  - [OpenAI](#openai)\n    - [Available Routes](#available-routes)\n    - [Quick Setup](#quick-setup)\n    - [Built-in completion (WIP)](#built-in-completion-wip)\n    - [OpenAI Proxy Server](#openai-proxy-server)\n      - [Chat](#chat)\n      - [Images](#images)\n      - [Models](#models)\n  - [Basic Usage](#basic-usage)\n  - [Bots Group Chat](#bots-group-chat)\n  - [Misc](#misc)\n    - [Text files](#text-files)\n    - [Media files](#media-files)\n- [🙌 Contributing](#-contributing)\n  - [Run debug](#run-debug)\n  - [Ways to contribute](#ways-to-contribute)\n  - [Contributors](#contributors)\n- [🤝 Copyright](#-copyright)\n  - [Copyright Notice](#copyright-notice)\n\n## ✨ Highlights\n<details close>\n<summary>Support both <b>Sync</b> and <b>Async</b></summary>\n</details>\n<details close>\n<summary>Authentication</summary><br>\n<ul>\n<li>Log in with your Poe tokens</li>\n<li>Auto Proxy requests</li>\n<li>Specify Proxy context</li>\n</ul>\n</details>\n<details close>\n<summary>Message Automation</summary><br>\n<ul>\n<li>Create new chat thread</li>\n<li>Send messages</li>\n<li>Stream bot responses</li>\n<li>Send concurrent messages</li>\n<li>Retry the last message</li>\n<li>Support file attachments</li>\n<li>Retrieve suggested replies</li>\n<li>Stop message generation</li>\n<li>Delete chat threads</li>\n<li>Clear conversation context</li>\n<li>Purge messages of 1 bot</li>\n<li>Purge all messages of user</li>\n<li>Fetch previous messages</li>\n<li>Share and import messages</li>\n<li>Get citations</li>\n</ul>\n</details>\n<details close>\n<summary>Chat Management</summary><br>\n<ul>\n<li>Get Chat Ids & Chat Codes of bot(s)</li>\n<li>Get subscription info and remaining points</li>\n</ul>\n</details>\n<details close>\n<summary>Bot Management</summary><br>\n<ul>\n<li>Get bot info</li>\n<li>Get available creation models</li>\n<li>Create custom bot</li>\n<li>Edit custom bot</li>\n<li>Delete a custom bot</li>\n</ul>\n</details>\n<details close>\n<summary>Knowledge Base Customization</summary><br>\n<ul>\n<li>Get available knowledge bases</li>\n<li>Upload knowledge bases for custom bots</li>\n<li>Edit knowledge bases for custom bots</li>\n</ul>\n</details>\n<details close>\n<summary>Discovery</summary><br>\n<ul>\n<li>Get available bots</li>\n<li>Get a user's bots</li>\n<li>Get available categories</li>\n<li>Explore 3rd party bots and users</li>\n</ul>\n</details>\n<details close>\n<summary>Bots Group Chat <b>(Beta)</b></summary><br>\n<ul>\n<li>Create a group chat</li>\n<li>Delete a group chat</li>\n<li>Get created groups</li>\n<li>Get group data</li>\n<li>Save group chat history</li>\n<li>Load group chat history</li>\n</ul>\n</details>\n\n## 🔧 Installation\n- First, install this library with the following command:\n```ShellSession\npip install -U poe-api-wrapper\n```\nOr you can install auto-proxy version of this library for **Python 3.9+**\n```ShellSession\npip install -U 'poe-api-wrapper[proxy]'\n```\nQuick setup for Async Client:\n```py\nfrom poe_api_wrapper import AsyncPoeApi\nimport asyncio\ntokens = {\n    'p-b': ..., \n    'p-lat': ...,\n}\n\nasync def main():\n    client = await AsyncPoeApi(tokens=tokens).create()\n    message = \"Explain quantum computing in simple terms\"\n    async for chunk in client.send_message(bot=\"gpt3_5\", message=message):\n        print(chunk[\"response\"], end='', flush=True)\n        \nasyncio.run(main())\n```\n- You can run an example of this library:\n```py\nfrom poe_api_wrapper import PoeExample\ntokens = {\n    'p-b': ..., \n    'p-lat': ...,\n}\nPoeExample(tokens=tokens).chat_with_bot()\n```\n- This library also supports command-line interface:\n```ShellSession\npoe -b P-B_HERE -lat P-LAT_HERE -f FORMKEY_HERE\n```\n> [!TIP]\n> Type `poe -h` for more info\n\n<img src=\"https://i.imgur.com/oAkTHfB.png\" width=\"100%\" height=\"auto\">\n\n## 🦄 Documentation\n### Available Default Bots\n| Display Name            | Model                     | Token Limit | Words | Access Type                                                     |\n| ----------------------- | ------------------------- | ----------- | ----- | --------------------------------------------------------------- |\n| Assistant               | capybara                  | 4K          | 3K    | ![Free](https://img.shields.io/badge/free-2feb7a)               |\n| Claude-3.5-Sonnet       | claude_3_igloo            | 4K          | 3K    | ![Free](https://img.shields.io/badge/free-2feb7a)               |\n| Claude-3-Opus           | claude_2_1_cedar          | 4K          | 3K    | ![Subscriber](https://img.shields.io/badge/subscriber-fc4747)   |\n| Claude-3-Sonnet         | claude_2_1_bamboo         | 4K          | 3K    | ![Free](https://img.shields.io/badge/free-2feb7a)               |\n| Claude-3-Haiku          | claude_3_haiku            | 4K          | 3K    | ![Free](https://img.shields.io/badge/free-2feb7a)               |\n| Claude-3.5-Sonnet-200k  | claude_3_igloo_200k       | 200K        | 150K  | ![Free](https://img.shields.io/badge/free-2feb7a)               |\n| Claude-3-Opus-200k      | claude_3_opus_200k        | 200K        | 150K  | ![Subscriber](https://img.shields.io/badge/subscriber-fc4747)   |\n| Claude-3-Sonnet-200k    | claude_3_sonnet_200k      | 200K        | 150K  | ![Subscriber](https://img.shields.io/badge/subscriber-fc4747)   |\n| Claude-3-Haiku-200k     | claude_3_haiku_200k       | 200K        | 150K  | ![Free](https://img.shields.io/badge/free-2feb7a)               |\n| Claude-2                | claude_2_short            | 4K          | 3K    | ![Subscriber](https://img.shields.io/badge/subscriber-fc4747)   |\n| Claude-2-100k           | a2_2                      | 100K        | 75K   | ![Subscriber](https://img.shields.io/badge/subscriber-fc4747)   |\n| Claude-instant          | a2                        | 9K          | 7K    | ![Free](https://img.shields.io/badge/free-2feb7a)               |\n| Claude-instant-100k     | a2_100k                   | 100K        | 75K   | ![Free](https://img.shields.io/badge/free-2feb7a)               |\n| GPT-3.5-Turbo           | chinchilla                | 4K          | 3K    | ![Free](https://img.shields.io/badge/free-2feb7a)               |\n| GPT-3.5-Turbo-Raw       | gpt3_5                    | 2k          | 1.5K  | ![Free](https://img.shields.io/badge/free-2feb7a)               |\n| GPT-3.5-Turbo-Instruct  | chinchilla_instruct       | 2K          | 1.5K  | ![Free](https://img.shields.io/badge/free-2feb7a)               |\n| ChatGPT-16k             | agouti                    | 16K         | 12K   | ![Subscriber](https://img.shields.io/badge/subscriber-fc4747)   |\n| GPT-4-Classic           | gpt4_classic              | 2K          | 1.5K  | ![Subscriber](https://img.shields.io/badge/subscriber-fc4747)   |\n| GPT-4-Turbo             | beaver                    | 4K          | 3K    | ![Subscriber](https://img.shields.io/badge/subscriber-fc4747)   |\n| GPT-4-Turbo-128k        | vizcacha                  | 128K        | 96K   | ![Subscriber](https://img.shields.io/badge/subscriber-fc4747)   |\n| GPT-4o                  | gpt4_o                    | 4k          | 3K    | ![Free](https://img.shields.io/badge/free-2feb7a)               |\n| GPT-4o-128k             | gpt4_o_128k               | 128K        | 96K   | ![Subscriber](https://img.shields.io/badge/subscriber-fc4747)   |\n| GPT-4o-Mini             | gpt4_o_mini               | 4K          | 3K    | ![Free](https://img.shields.io/badge/free-2feb7a)               |\n| GPT-4o-Mini-128k        | gpt4_o_mini_128k          | 128K        | 96K    | ![Free](https://img.shields.io/badge/free-2feb7a)              |\n| Google-PaLM             | acouchy                   | 8K          | 6K    | ![Free](https://img.shields.io/badge/free-2feb7a)               |\n| Code-Llama-13b          | code_llama_13b_instruct   | 4K          | 3K    | ![Free](https://img.shields.io/badge/free-2feb7a)               |\n| Code-Llama-34b          | code_llama_34b_instruct   | 4K          | 3K    | ![Free](https://img.shields.io/badge/free-2feb7a)               |\n| Solar-Mini              | upstage_solar_0_70b_16bit | 2K          | 1.5K  | ![Free](https://img.shields.io/badge/free-2feb7a)               |\n| Gemini-1.5-Flash-Search | gemini_pro_search         | 4K          | 3K    | ![Free](https://img.shields.io/badge/free-2feb7a)               |\n| Gemini-1.5-Pro-2M       | gemini_1_5_pro_1m         | 2M          | 1.5M  | ![Subscriber](https://img.shields.io/badge/subscriber-fc4747)   |\n> [!IMPORTANT]  \n> The data on token limits and word counts listed above are approximate and may not be entirely accurate, as the pre-prompt engineering process of poe.com is private and not publicly disclosed. \n>\n> The table above only shows bots with different display names from their models. Other bots on poe.com have the same display name as model.\n\n### How to get your Token\n\n#### Getting p-b and p-lat cookies (*required*)\nSign in at https://poe.com/\n\nF12 for Devtools (Right-click + Inspect)\n- Chromium: Devtools > Application > Cookies > poe.com\n- Firefox: Devtools > Storage > Cookies\n- Safari: Devtools > Storage > Cookies\n\nCopy the values of `p-b` and `p-lat` cookies\n\n#### Getting formkey (*optional*)\n> [!IMPORTANT] \n> By default, **poe-api-wrapper** will automatically retrieve formkey for you. If it doesn't work, please pass this token manually by following these steps:\n\nThere are two ways to get formkey:\n\nF12 for Devtools (Right-click + Inspect)\n\n- 1st Method: Devtools > Network > gql_POST > Headers > Poe-Formkey\n\n    Copy the value of `Poe-Formkey`\n\n- 2nd Method: Devtools > Console > Type: `allow pasting` > Paste this script: `window.ereNdsRqhp2Rd3LEW()`\n\n    Copy the result\n\n### OpenAI\n<details close>\n<summary>Read Docs</summary>\n\n#### Available Routes\n\n- /models\n- /chat/completions\n- /images/generations\n- /images/edits\n- /v1/models\n- /v1/chat/completions\n- /v1/images/generations\n- /v1/images/edits\n\n#### Quick Setup\n- First, install the additional packages:\n```ShellSession\npip install -U 'poe-api-wrapper[llm]'\n```\n- Clone the repo or use the same setup in `openai` folder:\n```ShellSession\ngit clone https://github.com/snowby666/poe-api-wrapper.git\ncd poe-api-wrapper\\poe_api_wrapper\\openai\n```\n- Modify secrets.json with your own tokens\n  \n- Run the FastAPI server:\n```ShellSession\npython api.py\n```\n- Run the examples:\n```ShellSession\npython example.py\n```\n\n#### Built-in completion (WIP)\n\n#### OpenAI Proxy Server\n- Start the server\n```py\nfrom poe_api_wrapper import PoeServer\ntokens = [\n    {\"p-b\": \"XXXXXXXX\", \"p-lat\": \"XXXXXXXX\"},\n    {\"p-b\": \"XXXXXXXX\", \"p-lat\": \"XXXXXXXX\"},\n    {\"p-b\": \"XXXXXXXX\", \"p-lat\": \"XXXXXXXX\"}\n]\nPoeServer(tokens=tokens)\n\n# You can also specify address and port (default is 127.0.0.1:8000)\nPoeServer(tokens=tokens, address=\"0.0.0.0\", port=\"8080\")\n```\n\n##### Chat\n- Non-streamed example:\n```py\nimport openai \nclient = openai.OpenAI(api_key=\"anything\", base_url=\"http://127.0.0.1:8000/v1/\", default_headers={\"Authorization\": \"Bearer anything\"})\n\nresponse = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\", \n    messages = [\n                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n                {\"role\": \"user\", \"content\": \"Hello!\"}\n            ]\n)\n\nprint(response.choices[0].message.content)\n```\n- Streaming example:\n```py\nimport openai \nclient = openai.OpenAI(api_key=\"anything\", base_url=\"http://127.0.0.1:8000/v1/\", default_headers={\"Authorization\": \"Bearer anything\"})\n\nstream = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\", \n    messages = [\n                {\"role\": \"user\", \"content\": \"this is a test request, write a short poem\"}\n            ],\n    stream=True\n)\n\nfor chunk in stream:\n    print(chunk.choices[0].delta.content or \"\", end=\"\", flush=True)\n\n# Set max_tokens\nstream_2 = client.chat.completions.create(\n    model=\"claude-instant\", \n    messages = [\n                {\"role\": \"user\", \"content\": \"Can you tell me about the creation of blackholes?\"}\n            ],\n    stream=True,\n    max_tokens=20, # if max_tokens reached, finish_reason will be 'length'\n)\n\nfor chunk in stream_2:\n    print(chunk.choices[0].delta.content or \"\", end=\"\", flush=True)\n\n# Include usage \nstream_3 = client.chat.completions.create(\n    model=\"claude-instant\", \n    messages = [\n                {\"role\": \"user\", \"content\": \"Write a 100-character meta description for my blog post about llamas\"}\n            ],\n    stream=True,\n    max_tokens=4096,\n    stream_options={\n\t\t\"include_usage\": True # last chunk contains prompts_tokens, completion_tokens and total_tokens\n\t}\n)\n\nfor chunk in stream_3:\n    print(chunk, end=\"\\n\\n\", flush=True)\n```\n- Image input example:\n```py\nimport openai \nclient = openai.OpenAI(api_key=\"anything\", base_url=\"http://127.0.0.1:8000/v1/\", default_headers={\"Authorization\": \"Bearer anything\"})\n\n# Legacy style (https://platform.openai.com/docs/api-reference/chat/create)\nresponse = client.chat.completions.create(\n    model=\"claude-3.5-sonnet\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\"type\": \"text\", \"text\": \"What's in this image?\"},\n                {\n                    \"type\": \"image_url\",\n                    \"image_url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\",\n                }\n            ],\n        }\n    ]\n)\n\n# New style (https://platform.openai.com/docs/guides/vision)\nresponse = client.chat.completions.create(\n    model=\"claude-3.5-sonnet\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\"type\": \"text\", \"text\": \"What's in this image?\"},\n                {\n                    \"type\": \"image_url\",\n                    \"image_url\": {\n                        \"url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\"\n                    }\n                }\n            ],\n        }\n    ]\n)\n\n# Multiple images\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[\n    {\n      \"role\": \"user\",\n      \"content\": [\n        {\n          \"type\": \"text\",\n          \"text\": \"What are in these images? Is there any difference between them?\",\n        },\n        {\n          \"type\": \"image_url\",\n          \"image_url\": {\n            \"url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\",\n          },\n        },\n        {\n          \"type\": \"image_url\",\n          \"image_url\": {\n            \"url\": \"https://imgcdn.stablediffusionweb.com/2024/4/29/0b0b8798-1965-4e3d-b0a8-d153728320d4.jpg\",\n          }\n        }\n      ]\n    }\n  ]\n)\n\n# Base64 image\nimport base64\n\n# Function to encode the image\ndef encode_image(image_path):\n  with open(image_path, \"rb\") as image_file:\n    return base64.b64encode(image_file.read()).decode('utf-8')\n\n# Path to your image\nimage_path = \"path_to_your_image.jpg\"\n\n# Getting the base64 string\nbase64_image = encode_image(image_path)\n\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[\n    {\n      \"role\": \"user\",\n      \"content\": [\n        {\n          \"type\": \"text\",\n          \"text\": \"What’s in this image?\"\n        },\n        {\n          \"type\": \"image_url\",\n          \"image_url\": {\n            \"url\": f\"data:image/jpeg;base64,{base64_image}\"\n          }\n        }\n      ]\n    }\n  ]\n)\n\nprint(response.choices[0].message.content)\n```\n- Function calling example:\n```py\nimport openai, json\nclient = openai.OpenAI(api_key=\"anything\", base_url=\"http://127.0.0.1:8000/v1/\", default_headers={\"Authorization\": \"Bearer anything\"})\n\nTEST_MODEL = \"gpt-4o-mini\"\n\n# Example dummy function hard coded to return the same weather\n# In production, this could be your backend API or an external API\ndef get_current_temperature(location, unit=\"fahrenheit\"):\n    \"\"\"Get the current weather in a given location\"\"\"\n    if \"tokyo\" in location.lower():\n        return json.dumps({\"location\": \"Tokyo\", \"temperature\": \"10\", \"unit\": unit})\n    elif \"san francisco\" in location.lower():\n        return json.dumps({\"location\": \"San Francisco\", \"temperature\": \"72\", \"unit\": unit})\n    elif \"paris\" in location.lower():\n        return json.dumps({\"location\": \"Paris\", \"temperature\": \"22\", \"unit\": unit})\n    else:\n        return json.dumps({\"location\": location, \"temperature\": \"unknown\"})\n    \ndef get_rain_probability(location):\n    \"\"\"Get the probability of rain in a given location\"\"\"\n    if \"tokyo\" in location.lower():\n        return json.dumps({\"location\": \"Tokyo\", \"rain_probability\": \"10%\"})\n    elif \"san francisco\" in location.lower():\n        return json.dumps({\"location\": \"San Francisco\", \"rain_probability\": \"20%\"})\n    elif \"paris\" in location.lower():\n        return json.dumps({\"location\": \"Paris\", \"rain_probability\": \"30%\"})\n    else:\n        return json.dumps({\"location\": location, \"rain_probability\": \"unknown\"})\n    \ndef run_conversation():\n    # Step 1: send the conversation and available functions to the model\n    messages = [\n        {'role': 'user', 'content': \"Hello there. What the weather like in Tokyo?\"},\n        {'role': 'assistant', 'content': \"Let me check the weather for you.\"},\n        {'role': 'user', 'content': \"What is the chance of raining in paris? Can you also tell me the temperature in Tokyo and LA?\"},\n                ]\n    tools = [\n    {\n      \"type\": \"function\",\n      \"function\": {\n        \"name\": \"get_current_temperature\",\n        \"description\": \"Get the current temperature for a specific location\",\n        \"parameters\": {\n          \"type\": \"object\",\n          \"properties\": {\n            \"location\": {\n              \"type\": \"string\",\n              \"description\": \"The city and state, e.g., San Francisco, CA\"\n            },\n            \"unit\": {\n              \"type\": \"string\",\n              \"enum\": [\"Celsius\", \"Fahrenheit\"],\n              \"description\": \"The temperature unit to use. Infer this from the user's location.\"\n            }\n          },\n          \"required\": [\"location\", \"unit\"]\n        }\n      }\n    },\n    {\n      \"type\": \"function\",\n      \"function\": {\n        \"name\": \"get_rain_probability\",\n        \"description\": \"Get the probability of rain for a specific location\",\n        \"parameters\": {\n          \"type\": \"object\",\n          \"properties\": {\n            \"location\": {\n              \"type\": \"string\",\n              \"description\": \"The city and state, e.g., San Francisco, CA\"\n            }\n          },\n          \"required\": [\"location\"]\n        }\n      }\n    }\n  ]\n    response = client.chat.completions.create(\n        model=TEST_MODEL,\n        messages=messages,\n        tools=tools,\n        tool_choice={\"type\": \"function\", \"function\": {\"name\": \"get_current_temperature\"}},\n    )\n    response_message = response.choices[0].message\n    print(\"\\n\", response_message, \"\\n\")\n        \n    tool_calls = response_message.tool_calls\n    # Step 2: check if the model wanted to call a function\n    if tool_calls:\n        # Step 3: call the function\n        # Note: the JSON response may not always be valid; be sure to handle errors\n        available_functions = {\n            \"get_current_temperature\": get_current_temperature,\n            \"get_rain_probability\": get_rain_probability\n        }  # only two functions in this example, but you can have multiple\n        messages.append(response_message)  # extend conversation with assistant's reply\n        # Step 4: send the info for each function call and function response to the model\n        for tool_call in tool_calls:\n            print(tool_call, \"\\n\")\n            function_name = tool_call.function.name\n            function_to_call = available_functions[function_name]\n            function_args = json.loads(tool_call.function.arguments)\n            function_response = function_to_call(**function_args)\n            messages.append(\n                {\n                    \"tool_call_id\": tool_call.id,\n                    \"role\": \"tool\",\n                    \"name\": function_name,\n                    \"content\": function_response,\n                }\n            )  # extend conversation with function response\n    second_response = client.chat.completions.create(\n        model=TEST_MODEL,\n        messages=messages,\n    )  # get a new response from the model where it can see the function response\n    return second_response.choices[0].message.content\n\nprint(run_conversation())\n```\n\n##### Images\n- Create image example:\n```py\nimport openai\nclient = openai.OpenAI(api_key=\"anything\", base_url=\"http://127.0.0.1:8000/v1/\", default_headers={\"Authorization\": \"Bearer anything\"})\n\nimages_url = client.images.generate(\n  model=\"playground-v2.5\",\n  prompt=\"A cute baby sea otter\",\n  n=2, # The number of images to generate\n  size=\"1792x1024\" # The size of image (view models.json for available sizes)\n)\n\nprint(images_url)\n```\n- Edit image example:\n```py\nimport openai\nclient = openai.OpenAI(api_key=\"anything\", base_url=\"http://127.0.0.1:8000/v1/\", default_headers={\"Authorization\": \"Bearer anything\"})\n\nimages_url = client.images.edit(\n  image=\"https://imgcdn.stablediffusionweb.com/2024/4/29/0b0b8798-1965-4e3d-b0a8-d153728320d4.jpg\",\n  model=\"sdxl\",\n  prompt=\"A cute baby sea otter wearing a raincoat\",\n  n=1, # The number of images to generate\n  size=\"1024x1024\" # The size of image (view models.json for available sizes)\n)\n\nprint(images_url)\n```\n\n##### Models\n- List models example:\n```py\nimport openai\nclient = openai.OpenAI(api_key=\"anything\", base_url=\"http://127.0.0.1:8000/v1/\", default_headers={\"Authorization\": \"Bearer anything\"})\n\nmodels = client.models.list()\n\nprint(models)\n```\n- Retrieve model example:\n```py\nimport openai\nclient = openai.OpenAI(api_key=\"anything\", base_url=\"http://127.0.0.1:8000/v1/\", default_headers={\"Authorization\": \"Bearer anything\"})\n\nmodel = client.models.retrieve(\"gpt-3.5-turbo-instruct\")\n\nprint(model)\n```\n</details>\n\n### Basic Usage\n<details close>\n<summary>Read Docs</summary>\n\n- Connecting to the API\n```py\ntokens = {\n    'p-b': 'p-b cookie here',\n    'p-lat': 'p-lat cookie here',\n}\n\n# Default setup\nfrom poe_api_wrapper import PoeApi\nclient = PoeApi(tokens=tokens)\n\n# Using Client with auto_proxy (default is False)\nclient = PoeApi(tokens=tokens, auto_proxy=True)\n\n# Passing proxies manually\nproxy_context = [\n    {\"https://\":X1, \"http://\":X1},\n    {\"https://\":X2, \"http://\":X2},\n    ...\n]\n\nclient = PoeApi(tokens=tokens, proxy=proxy_context) \n\n# Add formkey and cloudflare cookies to pass challenges\ntokens = {\n    'p-b': 'p-b cookie here',\n    'p-lat': 'p-lat cookie here',\n    'formkey': 'formkey here',\n    '__cf_bm': '__cf_bm cookie here', \n    'cf_clearance': 'cf_clearance cookie here'\n}\n```\n- Getting Chat Ids & Chat Codes\n```py\n# Get chat data of all bots (this will fetch all available threads)\nprint(client.get_chat_history()['data'])\n>> Output:\n{'chinchilla': [{'chatId': 74397929, 'chatCode': '2ith0h11zfyvsta1u3z', 'id': 'Q2hhdDo3NDM5NzkyOQ==', 'title': 'Comparison'}], 'code_llama_7b_instruct': [{'chatId': 74397392, 'chatCode': '2ithbduzsysy3g178hb', 'id': 'Q2hhdDo3NDM5NzM5Mg==', 'title': 'Decent Programmers'}], 'a2': [{'chatId': 74396838, 'chatCode': '2ith9nikybn4ksn51l8', 'id': 'Q2hhdDo3NDM5NjgzOA==', 'title': 'Reverse Engineering'}, {'chatId': 74396452, 'chatCode': '2ith79n4x0p0p8w5yue', 'id': 'Q2hhdDo3NDM5NjQ1Mg==', 'title': 'Clean Code'}], 'leocooks': [{'chatId': 74396246, 'chatCode': '2ith82wj0tjrggj46no', 'id': 'Q2hhdDo3NDM5NjI0Ng==', 'title': 'Pizza perfection'}], 'capybara': [{'chatId': 74396020, 'chatCode': '2ith5o3p8c5ajkdwd3k', 'id': 'Q2hhdDo3NDM5NjAyMA==', 'title': 'Greeting'}]}\n\n# Get chat data of a bot (this will fetch all available threads)\nprint(client.get_chat_history(\"a2\")['data'])\n>> Output:\n{'a2': [{'chatId': 74396838, 'chatCode': '2ith9nikybn4ksn51l8', 'id': 'Q2hhdDo3NDM5NjgzOA==', 'title': 'Reverse Engineering'}, {'chatId': 74396452, 'chatCode': '2ith79n4x0p0p8w5yue', 'id': 'Q2hhdDo3NDM5NjQ1Mg==', 'title': 'Clean Code'}]}\n\n# Get a defined number of most recent chat threads (using count param will ignore interval param)\n# Fetching all bots\nprint(client.get_chat_history(count=20)['data'])\n# Fetching 1 bot\nprint(client.get_chat_history(bot=\"a2\", count=20)['data'])\n\n# You can pass the number of bots fetched for each interval to both functions. (default is 50)\n# Fetching 200 chat threads of all bots each interval\nprint(client.get_chat_history(interval=200)['data'])\n# Fetching 200 chat threads of a bot each interval\nprint(client.get_chat_history(bot=\"a2\", interval=200)['data'])\n\n# Pagination Example:\n# Fetch the first 20 chat threads\nhistory = client.get_chat_history(count=20)\npages = [history['data']]\nnew_cursor = history['cursor']\n\n# Set a while loop with a condition of your choice\nwhile new_cursor != None:\n    # Fetch the next 20 chat threads with new_cursor\n    new_history = client.get_chat_history(count=20, cursor=new_cursor)\n    # Append the next 20 chat threads \n    new_cursor = new_history['cursor']\n    pages.append(new_history['data'])\n\n# Print the pages (20 chat threads each page)\nfor page in range(len(pages)):\n    print(f'This is page {page+1}')\n    for bot, value in pages[page].items():\n        for thread in value:\n            print({bot: thread})\n```\n- Getting subscription info and remaining points\n```py\ndata = client.get_settings()\nprint(data)\n```\n- Sending messages & Streaming responses \n```py\nbot = \"a2\"\nmessage = \"What is reverse engineering?\"\n\n# Create new chat thread\n# Streamed example:\nfor chunk in client.send_message(bot, message):\n    print(chunk[\"response\"], end=\"\", flush=True)\nprint(\"\\n\")\n\n# Non-streamed example:\nfor chunk in client.send_message(bot, message):\n    pass\nprint(chunk[\"text\"])\n\n# You can get chatCode and chatId of created thread to continue the conversation\nchatCode = chunk[\"chatCode\"]\nchatId = chunk[\"chatId\"]\n# You can also retrieve msgPrice\nmsgPrice = chunk[\"msgPrice\"]\n\n# Send message to an existing chat thread\n# 1. Using chatCode\nfor chunk in client.send_message(bot, message, chatCode=\"2i58ciex72dom7im83r\"):\n    print(chunk[\"response\"], end=\"\", flush=True)\n# 2. Using chatId\nfor chunk in client.send_message(bot, message, chatId=59726162):\n    print(chunk[\"response\"], end=\"\", flush=True)\n# 3. Specify msgPrice manually (the wrapper automatically gets this, but you can also pass the param for less resources consumed)\nfor chunk in client.send_message(bot, message, chatId=59726162, msgPrice=msgPrice):\n    print(chunk[\"response\"], end=\"\", flush=True)\n```\n> [!NOTE]\n> Display names are the same as the codenames for custom bots, you can simply pass the bot's display name into `client.send_message(bot, message)`\n- Sending concurrent messages\n```py\n# Use at your own risk, increase timeout to avoid ratelimit (default is 20)\n\nimport time, threading\nthread_count = 0\n\ndef message_thread(prompt, counter):\n    global thread_count\n    try:\n        for chunk in client.send_message(\"gpt3_5\", prompt):\n            pass\n        print(prompt+\"\\n\"+chunk[\"text\"]+\"\\n\"*3)\n        thread_count -= 1\n    except Exception as e:\n        pass\n\nprompts = [\n  \"Write a paragraph about the impact of social media on mental health.\",\n  \"Write a paragraph about the history and significance of the Olympic Games.\",\n  \"Write a paragraph about the effects of climate change on the world's oceans.\",\n  \"Write a paragraph about the benefits and drawbacks of remote work for employees and companies.\",\n  \"Write a paragraph about the role of technology in modern education.\",\n  \"Write a paragraph about the history and impact of the Civil Rights Movement in America.\",\n  \"Write a paragraph about the impact of COVID-19 on global economies.\",\n  \"Write a paragraph about the rise and fall of the Roman Empire.\",\n  \"Write a paragraph about the benefits and drawbacks of genetically modified organisms (GMOs).\",\n  \"Write a paragraph about the impact of globalization on cultural identity.\",\n  \"Write a paragraph about the history and significance of the Mona Lisa painting.\",\n  \"Write a paragraph about the benefits and drawbacks of renewable energy sources.\",\n  \"Write a paragraph about the impact of social media on political discourse.\",\n  \"Write a paragraph about the history and impact of the Industrial Revolution.\",\n  \"Write a paragraph about the benefits and drawbacks of online shopping for consumers and businesses.\",\n  \"Write a paragraph about the impact of artificial intelligence on the job market.\",\n  \"Write a paragraph about the history and significance of the Great Wall of China.\",\n  \"Write a paragraph about the benefits and drawbacks of standardized testing in schools.\",\n  \"Write a paragraph about the impact of the feminist movement on women's rights.\",\n  \"Write a paragraph about the history and impact of the American Revolution.\"\n]\n\n   \nfor i in range(len(prompts)):\n    t = threading.Thread(target=message_thread, args=(prompts[i], i), daemon=True)\n    t.start()\n    thread_count += 1\n    time.sleep(1)\n\nwhile thread_count:\n    time.sleep(0.01)\n```\n- Retrying the last message\n```py\nfor chunk in client.retry_message(chatCode):\n    print(chunk['response'], end='', flush=True)\n```\n- Adding file attachments\n```py\n# Web urls example:\nfile_urls = [\"https://elinux.org/images/c/c5/IntroductionToReverseEngineering_Anderson.pdf\", \n            \"https://www.kcl.ac.uk/warstudies/assets/automation-and-artificial-intelligence.pdf\"]\nfor chunk in client.send_message(bot, \"Compare 2 files and describe them in 300 words\", file_path=file_urls):\n    print(chunk[\"response\"], end=\"\", flush=True)\n    \n# Local paths example:\nlocal_paths = [\"c:\\\\users\\\\snowby666\\\\hello_world.py\"]\nfor chunk in client.send_message(bot, \"What is this file about?\", file_path=local_paths):\n    print(chunk[\"response\"], end=\"\", flush=True)\n```\n> [!NOTE]\n> The files size limit is different for each model.\n- Retrieving suggested replies \n```py\nfor chunk in client.send_message(bot, \"Introduce 5 books about clean code\", suggest_replies=True):\n    print(chunk[\"response\"], end=\"\", flush=True)\nprint(\"\\n\")\n\nfor reply in chunk[\"suggestedReplies\"]:\n    print(reply)\n```\n- Stopping message generation\n```py\n# You can use an event to trigger this function\n# Example:\n# Note that keyboard library may not be compatible with MacOS, Linux, Ubuntu\nimport keyboard\nfor chunk in client.send_message(bot, message):\n    print(chunk[\"response\"], end=\"\", flush=True)\n    # Press Q key to stop the generation\n    if keyboard.is_pressed('q'):\n        client.cancel_message(chunk)\n        print(\"\\nMessage is now cancelled\")\n        break \n```\n- Deleting chat threads\n```py\n# Delete 1 chat\n# Using chatCode\nclient.delete_chat(bot, chatCode=\"2i58ciex72dom7im83r\")\n# Using chatId\nclient.delete_chat(bot, chatId=59726162)\n\n# Delete n chats\n# Using chatCode\nclient.delete_chat(bot, chatCode=[\"LIST_OF_CHAT_CODES\"])\n# Using chatId\nclient.delete_chat(bot, chatId=[\"LIST_OF_CHAT_IDS\"])\n\n# Delete all chats of a bot\nclient.delete_chat(bot, del_all=True)\n```\n- Clearing conversation context\n```py\n# 1. Using chatCode\nclient.chat_break(bot, chatCode=\"2i58ciex72dom7im83r\")\n# 2. Using chatId\nclient.chat_break(bot, chatId=59726162)\n```\n- Purging messages of 1 bot\n  \n```py\n# Purge a defined number of messages (default is 50)\n# 1. Using chatCode\nclient.purge_conversation(bot, chatCode=\"2i58ciex72dom7im83r\", count=10)\n# 2. Using chatId\nclient.purge_conversation(bot, chatId=59726162, count=10)\n\n# Purge all messsages of the thread\n# 1. Using chatCode\nclient.purge_conversation(bot, chatCode=\"2i58ciex72dom7im83r\", del_all=True)\n# 2. Using chatId\nclient.purge_conversation(bot, chatId=59726162,  del_all=True)\n```\n- Purging all messages of user\n```py\nclient.purge_all_conversations()\n```\n- Fetching previous messsages\n```py\n# Get a defined number of messages (default is 50)\n# Using chatCode\nprevious_messages = client.get_previous_messages('code_llama_34b_instruct', chatCode='2itg2a7muygs42v1u0k', count=2)\n# Using chatId\nprevious_messages = client.get_previous_messages('code_llama_34b_instruct', chatId=74411139, count=2)\nfor message in previous_messages:\n    print(message)\n>> Output:\n{'author': 'human', 'text': 'nice to meet you', 'messageId': 2861709279}\n{'author': 'code_llama_34b_instruct', 'text': \" Nice to meet you too! How are you doing today? Is there anything on your mind that you'd like to talk about? I'm here to listen and help\", 'messageId': 2861873125}\n\n# Get all previous messages\n# Using chatCode\nprevious_messages = client.get_previous_messages('code_llama_34b_instruct', chatCode='2itg2a7muygs42v1u0k', get_all=True)\n# Using chatId\nprevious_messages = client.get_previous_messages('code_llama_34b_instruct', chatId=74411139, get_all=True)\nfor message in previous_messages:\n    print(message)\n>> Output:\n{'author': 'human', 'text': 'hi there', 'messageId': 2861363514}\n{'author': 'code_llama_34b_instruct', 'text': \" Hello! It's nice to meet you. Is there something I can help you with or would you like to chat?\", 'messageId': 2861363530}\n{'author': 'chat_break', 'text': \"\", 'messageId': 2872383991}\n{'author': 'human', 'text': 'nice to meet you', 'messageId': 2861709279}\n{'author': 'code_llama_34b_instruct', 'text': \" Nice to meet you too! How are you doing today? Is there anything on your mind that you'd like to talk about? I'm here to listen and help\", 'messageId': 2861873125}\n```\n> [!NOTE]\n> It will fetch messages from the latest to the oldest, but the order to be displayed is reversed.\n- Getting available knowledge bases\n```py\n# Get a defined number of sources (default is 10)\nprint(client.get_available_knowledge(botName=\"BOT_NAME\", count=2))\n>> Output:\n{'What is Quora?': [86698], 'Founders of Quora': [86705]}\n# Get all available sources\nprint(client.get_available_knowledge(botName=\"BOT_NAME\", get_all=True))\n```\n- Uploading knowledge bases\n```py\n# Web urls example:\nfile_urls = [\"https://elinux.org/images/c/c5/IntroductionToReverseEngineering_Anderson.pdf\", \n            \"https://www.kcl.ac.uk/warstudies/assets/automation-and-artificial-intelligence.pdf\"]\nsource_ids = client.upload_knowledge(file_path=file_urls)\nprint(source_ids)\n>> Output:\n{'er-1-intro_to_re.pdf': [86344], 'automation-and-artificial-intelligence.pdf': [86345]}\n\n# Local paths example:\nlocal_paths = [\"c:\\\\users\\\\snowby666\\\\hello_world.py\"]\nsource_ids = client.upload_knowledge(file_path=local_paths)\nprint(source_ids)\n>> Output:\n{'hello_world.py': [86523]}\n\n# Plain texts example:\nknowledges = [\n    {\n        \"title\": \"What is Quora?\",\n        \"content\": \"Quora is a popular online platform that enables users to ask questions on various topics and receive answers from a diverse community. It covers a wide range of subjects, from academic and professional queries to personal experiences and opinions, fostering knowledge-sharing and meaningful discussions among its users worldwide.\"\n    },\n    {\n        \"title\": \"Founders of Quora\",\n        \"content\": \"Quora was founded by two individuals, Adam D'Angelo and Charlie Cheever. Adam D'Angelo, who previously served as the Chief Technology Officer (CTO) at Facebook, and Charlie Cheever, a former Facebook employee as well, launched Quora in June 2009. They aimed to create a platform that would enable users to ask questions and receive high-quality answers from knowledgeable individuals. Since its inception, Quora has grown into a widely used question-and-answer platform with a large user base and a diverse range of topics covered.\"\n    },\n]\nsource_ids = client.upload_knowledge(text_knowledge=knowledges)\nprint(source_ids)\n>> Output:\n{'What is Quora?': [86368], 'Founders of Quora': [86369]}\n\n# Hybrid example:\nsource_ids = client.upload_knowledge(file_path=file_urls, text_knowledge=knowledges)\nprint(source_ids)\n>> Output:\n{'What is Quora?': [86381], 'Founders of Quora': [86383], 'er-1-intro_to_re.pdf': [86395], 'automation-and-artificial-intelligence.pdf': [86396]}\n```\n- Editing knowledge bases (Only for plain texts)\n```py\nclient.edit_knowledge(knowledgeSourceId=86381, title='What is Quora?', content='Quora is a question-and-answer platform where users can ask questions, provide answers, and engage in discussions on various topics.')\n```\n- Getting bot info\n```py\nbot = 'gpt-4'\nprint(client.get_botInfo(handle=bot))\n>> Output:\n{'handle': 'GPT-4', 'model': 'beaver', 'supportsFileUpload': True, 'messageTimeoutSecs': 15, 'displayMessagePointPrice': 350, 'numRemainingMessages': 20, 'viewerIsCreator': False, 'id': 'Qm90OjMwMDc='}\n```\n- Getting available creation models\n```py\nprint(client.get_available_creation_models())\n>> Output:\n{'text': ['claude_3_igloo', 'gpt4_o_mini', 'gpt4_o', 'gemini_1_5_flash', 'gemini_1_5_pro', 'claude_2_1_bamboo', 'claude_3_haiku', 'claude_2_1_cedar', 'gemini_1_5_flash_128k', 'gemini_1_5_pro_128k', 'gemini_1_5_flash_1m', 'gemini_1_5_pro_1m', 'gpt4_o_mini_128k', 'gpt4_o_128k', 'beaver', 'gemini_pro', 'chinchilla', 'vizcacha', 'claude_3_igloo_200k', 'claude_3_sonnet_200k', 'claude_3_haiku_200k', 'claude_3_opus_200k', 'mixtral8x7bchat', 'claude_2_short', 'a2_2', 'mythomaxl213b', 'a2', 'a2_100k'], 'image': ['playgroundv25', 'ideogram', 'dalle3', 'stablediffusion3', 'sd3turbo', 'stablediffusionxl'], 'video': ['pika']}\n```\n- Creating a new Bot\n```py\nclient.create_bot(handle=\"BOT_NAME\", prompt=\"PROMPT_HERE\", base_model=\"a2\")\n\n# Using knowledge bases (you can use source_ids from uploaded knowledge bases for your custom bot)\nclient.create_bot(handle=\"BOT_NAME\", prompt=\"PROMPT_HERE\", base_model=\"a2\", knowledgeSourceIds=source_ids, shouldCiteSources=True)\n```\n- Editing a Bot\n```py\nclient.edit_bot(handle=\"BOT_NAME\", prompt=\"PROMPT_HERE\", new_handle=\"NEW_BOT_NAME\", base_model='chinchilla')\n\n# Adding knowledge bases \nclient.edit_bot(handle=\"BOT_NAME\", prompt=\"PROMPT_HERE\", new_handle=\"NEW_BOT_NAME\", base_model='chinchilla', knowledgeSourceIdsToAdd=source_ids, shouldCiteSources=True)\n\n# Removing knowledge bases\nclient.edit_bot(handle=\"BOT_NAME\", prompt=\"PROMPT_HERE\", new_handle=\"NEW_BOT_NAME\", base_model='chinchilla', knowledgeSourceIdsToRemove=source_ids, shouldCiteSources=True)\n```\n> [!TIP]\n> You can also use both `knowledgeSourceIdsToAdd` and `knowledgeSourceIdsToRemove` at the same time.\n- Deleting a Bot\n```py\nclient.delete_bot(handle=\"BOT_NAME\")\n```\n- Getting available bots (your bots section)\n```py\n# Get a defined number of bots (default is 25)\nprint(client.get_available_bots(count=10))\n# Get all available bots\nprint(client.get_available_bots(get_all=True))\n```\n- Getting a user's bots\n```py\nhandle = 'poe'\nprint(client.get_user_bots(user=handle))\n```\n- Getting available categories\n```py\nprint(client.get_available_categories())\n>> Output:\n['Official', 'Popular', 'New', 'ImageGen', 'AI', 'Professional', 'Funny', 'History', 'Cooking', 'Advice', 'Mind', 'Programming', 'Travel', 'Writing', 'Games', 'Learning', 'Roleplay', 'Utilities', 'Sports', 'Music']\n```\n- Exploring 3rd party bots and users\n```py\n# Explore section example:\n# Get a defined number of bots (default is 50)\nprint(client.explore(count=10))\n# Get all available bots\nprint(client.explore(explore_all=True))\n\n# Search for bots by query example:\n# Get a defined number of bots (default is 50)\nprint(client.explore(search=\"Midjourney\", count=30))\n# Get all available bots\nprint(client.explore(search=\"Midjourney\", explore_all=True))\n\n# Search for bots by category example (default is defaultCategory):\n# Get a defined number of bots (default is 50)\nprint(client.explore(categoryName=\"Popular\", count=30))\n# Get all available bots\nprint(client.explore(categoryName=\"AI\", explore_all=True))\n\n# Search for people example:\n# Get a defined number of people (default is 50)\nprint(client.explore(search=\"Poe\", entity_type='user', count=30))\n# Get all available people\nprint(client.explore(search=\"Poe\", entity_type='user', explore_all=True))\n```\n- Sharing & Importing messages\n```py\n# Share a defined number of messages (from the lastest to the oldest)\n# Using chatCode\nshareCode = client.share_chat(\"a2\", chatCode=\"2roap5g8nd7s28ul836\",count=10)\n# Using chatId\nshareCode = client.share_chat(\"a2\", chatId=204052028,count=10)\n\n# Share all messages\n# Using chatCode\nshareCode = client.share_chat(\"a2\", chatCode=\"2roap5g8nd7s28ul836\")\n# Using chatId\nshareCode = client.share_chat(\"a2\", chatId=204052028)\n\n# Set up the 2nd Client and import messages from the shareCode\nclient2 = PoeApi(\"2nd_TOKEN_HERE\")\nprint(client2.import_chat(bot, shareCode))\n>> Output:\n{'chatId': 72929127, 'chatCode': '2iw0xcem7a18wy1avd3'}\n```\n- Getting citations\n```py\nprint(client.get_citations(messageId=141597902621))\n```\n</details>\n\n### Bots Group Chat\n<details close>\n<summary>Read Docs</summary>\n\n- Creating a group chat\n```py\nbots = [\n    {'bot': 'yayayayaeclaude', 'name': 'Yae'}, \n    {'bot': 'gepardL', 'name': 'gepard'}, \n    {'bot': 'SayukiTokihara', 'name': 'Sayuki'}\n]\n\nclient.create_group(group_name='Hangout', bots=bots) \n```\n> [!NOTE]\n> `bot` arg is the model/displayName.\n> `name` arg is the one you'd mention them in group chat.\n- Sending messages and Streaming responses in group chat\n```py\n# User engagement example:\nwhile True: \n    message = str(input('\\n\\033[38;5;121mYou : \\033[0m'))\n    prev_bot = \"\"\n    for chunk in client.send_message_to_group(group_name='Hangout', message=message):\n        if chunk['bot'] != prev_bot:\n            print(f\"\\n\\033[38;5;121m{chunk['bot']} : \\033[0m\", end='', flush=True)\n            prev_bot = chunk['bot']\n        print(chunk['response'], end='', flush=True)\n    print('\\n')\n\n# Auto-play example:\nwhile True:\n    prev_bot = \"\"\n    for chunk in client.send_message_to_group(group_name='Hangout', autoplay=True):\n        if chunk['bot'] != prev_bot:\n            print(f\"\\n\\033[38;5;121m{chunk['bot']} : \\033[0m\", end='', flush=True)\n            prev_bot = chunk['bot']\n        print(chunk['response'], end='', flush=True)\n    print('\\n')\n\n# Preset history example:\npreset_path = \"c:\\\\users\\\\snowby666\\\\preset.json\"\nprev_bot = \"\"\nfor chunk in client.send_message_to_group(group_name='Hangout', autoplay=True, preset_history=preset_path):\n    if chunk['bot'] != prev_bot:\n        print(f\"\\n\\033[38;5;121m{chunk['bot']} : \\033[0m\", end='', flush=True)\n        prev_bot = chunk['bot']\n    print(chunk['response'], end='', flush=True)\nprint('\\n')\nwhile True:\n    for chunk in client.send_message_to_group(group_name='Hangout', autoplay=True):\n        if chunk['bot'] != prev_bot:\n            print(f\"\\n\\033[38;5;121m{chunk['bot']} : \\033[0m\", end='', flush=True)\n            prev_bot = chunk['bot']\n        print(chunk['response'], end='', flush=True)\n    print('\\n')\n```\n> [!NOTE]\n> You can also change your name in group chat by passing a new one to the above function: `client.send_message_to_group('Hangout', message=message, user='Danny')`\n> If you want to auto save the conversation_log, just simply set this to true: `client.send_message_to_group('Hangout', message=message, autosave=True)`\n- Deleting a group chat\n```py\nclient.delete_group(group_name='Hangout')\n```\n- Getting created groups\n```py\nprint(client.get_available_groups())\n```\n- Getting group data\n```py\nprint(client.get_group(group_name='Hangout'))\n```\n- Saving group chat history\n```py\n# Save as json in the same directory\nclient.save_group_history(group_name='Hangout')\n# Save with a local path (json only)\nlocal_path = \"c:\\\\users\\\\snowby666\\\\log.json\"\nclient.save_group_history(group_name='Hangout', file_path=local_path)\n```\n- Loading group chat history\n```py\nprint(client.load_group_history(file_path=local_path))\n```\n</details>\n\n### Misc\n<details close>\n<summary>Read Docs</summary>\n\n- How to find chatCode manually?\n\nHere is an example, the chatCode is 23o1gxjhb9cfnlacdcd\n\n![](https://i.imgur.com/m1zDP36.png)\n\n- What are the file types that poe-api-wrapper support?\n\nCurrently, this API only supports these file types for adding attachments\n\n#### Text files\n| .pdf | .docx | .txt | .md | .py | .js | .ts | .html | .css | .csv | .c | .cs | .cpp | .lua | .rs | .rb | .go | .java |\n| - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - |\n|                                                                       |\n#### Media files\n| .png | .jpg | .jpeg | .gif | .mp4 | .mov | .mp3 | .wav |\n| - | - | - | - | - | - | - | - |\n|                               |\n</details>\n\n## 🙌 Contributing\nWe would love to develop poe-api-wrapper together with our community! 💕\n### Run debug\nFirst, clone this repo:\n```ShellSession\ngit clone https://github.com/snowby666/poe-api-wrapper.git\ncd poe-api-wrapper\n```\nThen run the test cases:\n```ShellSession\npython -m pip install -e .[tests]\ntox\n```\n### Ways to contribute\n- Try poe-api-wrapper and give feedback\n- Add new integrations with open [PR](https://github.com/snowby666/poe-api-wrapper/pulls)\n- Help with open [issues](https://github.com/snowby666/poe-api-wrapper/issues) or [create your own](https://github.com/snowby666/poe-api-wrapper/issues/new/choose)\n- Share your thoughts and suggestions with us\n- Request a feature by submitting a proposal\n- Report a bug\n- **Improve documentation:** fix incomplete or missing docs, bad wording, examples or explanations.\n\n### Contributors\n<a href=\"https://github.com/snowby666/poe-api-wrapper/graphs/contributors\">\n  <img alt=\"poe_api_wrapper\" src=\"https://contrib.rocks/image?repo=snowby666/poe-api-wrapper\" />\n</a>\n\n<br>\n\n<img src=\"https://repobeats.axiom.co/api/embed/cba15fced158acd258575d31fc14d7e5c59b07a3.svg\" alt=\"Repobeats analytics image\">\n\n## 🤝 Copyright\nThis program is licensed under the [GNU GPL v3](https://github.com/snowby666/poe-api-wrapper/blob/main/LICENSE). Most code has been written by me, [snowby666](https://github.com/snowby666).\n\n### Copyright Notice\n```\nsnowby666/poe-api-wrapper: A simple, lightweight and efficient API wrapper for Poe.com\nCopyright (C) 2023 snowby666\n\nThis program is free software: you can redistribute it and/or modify\nit under the terms of the GNU General Public License as published by\nthe Free Software Foundation, either version 3 of the License, or\n(at your option) any later version.\n\nThis program is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\nGNU General Public License for more details.\n\nYou should have received a copy of the GNU General Public License\nalong with this program.  If not, see <https://www.gnu.org/licenses/>.\n```\n",
      "npm_url": "https://www.npmjs.com/package/poe",
      "npm_downloads": 2047,
      "keywords": [
        "poe",
        "chat",
        "ai",
        "laosu888 poe",
        "poe com",
        "poe facilitates"
      ],
      "category": "web-search"
    },
    "laulauland--bluesky-context-server": {
      "owner": "laulauland",
      "name": "bluesky-context-server",
      "url": "https://github.com/laulauland/bluesky-context-server",
      "imageUrl": "/freedevtools/mcp/pfp/laulauland.webp",
      "description": "Enables querying of Bluesky instances through an MCP protocol, facilitating data interaction and retrieval.",
      "stars": 27,
      "forks": 7,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-09-03T01:09:00Z",
      "readme_content": "# Bluesky Context Server\n[![smithery badge](https://smithery.ai/badge/bluesky-context-server)](https://smithery.ai/server/bluesky-context-server)\n\nA Model Context Protocol (MCP) server that enables MCP clients like Claude Desktop to interact with Bluesky. Query your profile, search posts, get your timeline, and more directly from your AI assistant.\n\n## Prerequisites\n\n- **Runtime**: Either [Bun](https://bun.sh/) or [Node.js](https://nodejs.org/) v22.6.0+\n- A Bluesky account\n\n## Setup\n\n### 1. Get Your Bluesky Credentials\n\nYou'll need two pieces of information from your Bluesky account:\n\n#### BLUESKY_IDENTIFIER\nThis is your Bluesky handle (username). It can be in either format:\n- `username.bsky.social` (e.g., `alice.bsky.social`)\n- `@username.bsky.social` (e.g., `@alice.bsky.social`)\n\n#### BLUESKY_APP_KEY\nThis is an App Password, which is different from your regular Bluesky password. To create one:\n\n1. Go to [Bluesky Settings](https://bsky.app/settings)\n2. Navigate to \"Privacy and Security\" → \"App Passwords\"\n3. Click \"Add App Password\"\n4. Give it a name (e.g., \"MCP Server\")\n5. Copy the generated password (it looks like `xxxx-xxxx-xxxx-xxxx`)\n\n⚠️ **Important**: Use the App Password, not your regular account password!\n\n### 2. Installation\n\n#### Option A: Installing via Smithery (Recommended)\n\nTo install Bluesky Context Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@laulauland/bluesky-context-server):\n\n```bash\nnpx -y @smithery/cli install @laulauland/bluesky-context-server --client claude\n```\n\nThen add your credentials to the generated configuration.\n\n#### Option B: Manual Installation\n\n1. Clone or download this repository\n2. Configure your Claude Desktop app to use the MCP server:\n\n**Using Bun:**\n```json\n// ~/Library/Application Support/Claude/config.json (macOS)\n// %APPDATA%/Claude/config.json (Windows)\n{\n\t\"mcpServers\": {\n\t\t\"bluesky\": {\n\t\t\t\"command\": \"/Users/your-username/.bun/bin/bun\",\n\t\t\t\"args\": [\n\t\t\t\t\"/path/to/bluesky-context-server/packages/server/bin/index.ts\"\n\t\t\t],\n\t\t\t\"env\": {\n\t\t\t\t\"BLUESKY_APP_KEY\": \"your-app-password-here\",\n\t\t\t\t\"BLUESKY_IDENTIFIER\": \"your-handle.bsky.social\"\n\t\t\t}\n\t\t}\n\t}\n}\n```\n\n**Using Node.js:**\n```json\n// ~/Library/Application Support/Claude/config.json (macOS)\n// %APPDATA%/Claude/config.json (Windows)\n{\n\t\"mcpServers\": {\n\t\t\"bluesky\": {\n\t\t\t\"command\": \"node\",\n\t\t\t\"args\": [\n\t\t\t\t\"--experimental-strip-types\",\n\t\t\t\t\"/path/to/bluesky-context-server/packages/server/bin/index.ts\"\n\t\t\t],\n\t\t\t\"env\": {\n\t\t\t\t\"BLUESKY_APP_KEY\": \"your-app-password-here\",\n\t\t\t\t\"BLUESKY_IDENTIFIER\": \"your-handle.bsky.social\"\n\t\t\t}\n\t\t}\n\t}\n}\n```\n\n3. Restart Claude Desktop\n\n### 3. Testing the Connection\n\nAfter setup, you can test the connection by asking Claude something like:\n- \"Can you get my Bluesky profile?\"\n- \"Show me my recent posts on Bluesky\"\n- \"Search for posts about AI on Bluesky\"\n\n## Available MCP Tools\n\nThis server provides the following tools that Claude can use:\n\n### Profile & Account Tools\n\n#### `bluesky_get_profile`\nGet your Bluesky profile information including display name, bio, follower count, etc.\n- **Parameters**: None\n- **Returns**: Complete profile data\n\n#### `bluesky_get_follows`\nGet a list of accounts you follow.\n- **Parameters**: \n  - `limit` (optional): Max items to return (default 50, max 100)\n  - `cursor` (optional): Pagination cursor for next page\n- **Returns**: List of followed accounts with profile info\n\n#### `bluesky_get_followers`\nGet a list of accounts following you.\n- **Parameters**: \n  - `limit` (optional): Max items to return (default 50, max 100)\n  - `cursor` (optional): Pagination cursor for next page\n- **Returns**: List of followers with profile info\n\n### Post & Feed Tools\n\n#### `bluesky_get_posts`\nGet your recent posts.\n- **Parameters**: \n  - `limit` (optional): Max items to return (default 50, max 100)\n  - `cursor` (optional): Pagination cursor for next page\n- **Returns**: Your recent posts with engagement data\n\n#### `bluesky_get_personal_feed`\nGet your personalized Bluesky timeline/feed.\n- **Parameters**: \n  - `limit` (optional): Max items to return (default 50, max 100)\n  - `cursor` (optional): Pagination cursor for next page\n- **Returns**: Posts from your personalized feed\n\n#### `bluesky_get_liked_posts`\nGet posts you've liked.\n- **Parameters**: \n  - `limit` (optional): Max items to return (default 50, max 100)\n  - `cursor` (optional): Pagination cursor for next page\n- **Returns**: Posts you've liked\n\n### Search Tools\n\n#### `bluesky_search_posts`\nSearch for posts across Bluesky.\n- **Parameters**: \n  - `query` (required): Search query string\n  - `limit` (optional): Max items to return (default 50, max 100)\n  - `cursor` (optional): Pagination cursor for next page\n- **Returns**: Posts matching your search query\n\n#### `bluesky_search_profiles`\nSearch for Bluesky user profiles.\n- **Parameters**: \n  - `query` (required): Search query string\n  - `limit` (optional): Max items to return (default 50, max 100)\n  - `cursor` (optional): Pagination cursor for next page\n- **Returns**: User profiles matching your search query\n\n## Example Usage\n\nOnce configured, you can ask Claude things like:\n\n- \"What's in my Bluesky feed today?\"\n- \"Search for posts about TypeScript on Bluesky\"\n- \"Who are my most recent followers?\"\n- \"Show me posts I've liked recently\"\n- \"Find Bluesky users interested in AI\"\n\n## Troubleshooting\n\n### Common Issues\n\n1. **\"Authentication failed\"**: Double-check your `BLUESKY_APP_KEY` and `BLUESKY_IDENTIFIER`\n2. **\"Server not responding\"**: Ensure Bun is installed and the path to the server is correct\n3. **\"Permission denied\"**: Make sure the server file has execute permissions\n\n### Getting Help\n\nIf you encounter issues:\n1. Check that your Bluesky credentials are correct\n2. Verify Bun is installed: `bun --version`\n3. Test the server manually: `cd packages/server && bun start`\n4. Check Claude Desktop's logs for error messages\n",
      "npm_url": "https://www.npmjs.com/package/@laulauland/bluesky-context-server",
      "npm_downloads": 76,
      "keywords": [
        "bluesky",
        "retrieval",
        "search",
        "querying bluesky",
        "bluesky instances",
        "context server"
      ],
      "category": "web-search"
    },
    "leehanchung--bing-search-mcp": {
      "owner": "leehanchung",
      "name": "bing-search-mcp",
      "url": "https://github.com/leehanchung/bing-search-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/leehanchung.webp",
      "description": "Connect to the Microsoft Bing Search API to perform comprehensive web, news, and image searches. Retrieve timely information and visual content efficiently, enhancing AI applications with search capabilities.",
      "stars": 56,
      "forks": 5,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-29T10:07:53Z",
      "readme_content": "# Bing Search MCP Server\n\nA Model Context Protocol (MCP) server for Microsoft Bing Search API integration, allowing AI assistants to perform web, news, and image searches.\n\n[![smithery badge](https://smithery.ai/badge/@leehanchung/bing-search-mcp)](https://smithery.ai/server/@leehanchung/bing-search-mcp)\n\n\n\n\n## Features\n\n- Web search for general information\n- News search for recent events and timely information\n- Image search for visual content\n- Rate limiting to prevent API abuse\n- Comprehensive error handling\n\n## Requirements\n\n- Python 3.10 or higher\n- Microsoft Bing Search API key\n- MCP-compatible client (e.g., Claude Desktop, Cursor)\n\n## Installation\n\n1. Clone this repository\n2. Install dependencies:\n   ```\n   uv venv\n   source .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\n   uv pip install -e .\n   ```\n\n## Configuration\n\nSet the required environment variables:\n\n```bash\nexport BING_API_KEY=\"your-bing-api-key\"\nexport BING_API_URL=\"https://api.bing.microsoft.com/\"  # Optional\n```\n\nFor Windows:\n```cmd\nset BING_API_KEY=your-bing-api-key\nset BING_API_URL=https://api.bing.microsoft.com/\n```\n\n## Usage\n\n### Running the server\n\n```\nuvx bing-search-mcp\n```\n\n### Configuring with Claude for Desktop\n\nAdd the following to your Claude Desktop configuration file (`~/Library/Application Support/Claude/claude_desktop_config.json` on macOS or `%APPDATA%\\Claude\\claude_desktop_config.json` on Windows):\n\n```json\n{\n  \"mcpServers\": {\n    \"bing-search\": {\n      \"command\": \"uvx\",\n      \"args\": [\n        \"/path/to/your/bing-search-mcp\"\n      ],\n      \"env\": {\n        \"BING_API_KEY\": \"your-bing-api-key\"\n      }\n    }\n  }\n}\n```\n\n\n## Available Tools\n\n### 1. bing_web_search\nGeneral web search for information, websites, and content.\n\n```python\nbing_web_search(query: str, count: int = 10, offset: int = 0, market: str = \"en-US\")\n```\n\n### 2. bing_news_search\nSearch for news articles and current events.\n\n```python\nbing_news_search(query: str, count: int = 10, market: str = \"en-US\", freshness: str = \"Day\")\n```\n\n### 3. bing_image_search\nSearch for images.\n\n```python\nbing_image_search(query: str, count: int = 10, market: str = \"en-US\")\n```\n\n## Getting a Bing API Key\n\n1. Visit [Microsoft Azure Portal](https://portal.azure.com/)\n2. Create or sign in to your Azure account\n3. Create a new Bing Search resource\n4. Go to the resource and find your API key in the \"Keys and Endpoint\" section\n\n## License\n\n[MIT License](LICENSE)",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "bing",
        "searches",
        "search",
        "bing search",
        "microsoft bing",
        "leehanchung bing"
      ],
      "category": "web-search"
    },
    "letsbuildagent--perplexity-tool": {
      "owner": "letsbuildagent",
      "name": "perplexity-tool",
      "url": "https://github.com/letsbuildagent/perplexity-tool",
      "imageUrl": "/freedevtools/mcp/pfp/letsbuildagent.webp",
      "description": "Integrate web-based research capabilities using Perplexity AI's API to provide accurate answers with citations, enhancing the information retrieval process.",
      "stars": 4,
      "forks": 3,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-05-30T12:01:23Z",
      "readme_content": "# Perplexity Tool for Claude Desktop\n\nA custom MCP tool that integrates Perplexity AI's API with Claude Desktop, allowing Claude to perform web-based research and provide answers with citations.\n\n## Prerequisites Installation\n\n1. Install Git:\n   - For Mac: \n     - Install [Homebrew](https://brew.sh/) first by pasting this in Terminal:\n     ```bash\n     /bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n     ```\n     - Then install Git:\n     ```bash\n     brew install git\n     ```\n   - For Windows:\n     - Download Git from [git-scm.com](https://git-scm.com/downloads)\n     - Run the installer\n\n2. Install Node.js:\n   - For Mac: \n     ```bash\n     brew install node\n     ```\n   - For Windows:\n     - Download from [nodejs.org](https://nodejs.org/)\n     - Run the installer\n\n3. Verify installations by running:\n```bash\ngit --version\nnode --version\n```\n\n## Tool Installation\n\n1. Clone the repository\n```bash\ngit clone https://github.com/letsbuildagent/perplexity-tool\ncd perplexity-tool\n```\n\n2. Install dependencies\n```bash\nnpm install\n```\n\n3. Set up your API Key\n\nYou have two options:\n\nOption 1 (Quick setup):\n- Open `server.js`\n- Find this line:\n```javascript\nconst PERPLEXITY_API_KEY = \"YOUR-API-KEY-HERE\";\n```\n- Replace with your Perplexity API key\n\nOption 2 (Best practice):\n- Create a .env file:\n  ```bash\n  # On Mac/Linux:\n  touch .env\n  open .env\n  \n  # On Windows:\n  notepad .env\n  ```\n  Or simply create a new file named `.env` in your text editor\n- Add your API key to the .env file:\n  ```\n  PERPLEXITY_API_KEY=your-api-key-here\n  ```\n- Install dotenv:\n  ```bash\n  npm install dotenv\n  ```\n- Update server.js:\n  ```javascript\n  import 'dotenv/config'\n  const PERPLEXITY_API_KEY = process.env.PERPLEXITY_API_KEY;\n  ```\n\n4. Configure Claude Desktop\n- Open `~/Library/Application Support/Claude/claude_desktop_config.json`\n- Add this configuration:\n```json\n{\n  \"mcpServers\": {\n    \"perplexity-tool\": {\n      \"command\": \"node\",\n      \"args\": [\n        \"/full/path/to/perplexity-tool/server.js\"\n      ]\n    }\n  }\n}\n```\nReplace `/full/path/to` with the actual path where you cloned the repository.\n\n5. Restart Claude Desktop\n\n## Usage\n\nOnce installed, you can use the tool through Claude with commands like:\n\n- \"Ask Perplexity about recent developments in AI\"\n- \"Use Perplexity to research the history of quantum computing\"\n- \"Search Perplexity for information about climate change, focusing on the last month\"\n\n### Advanced Options\n\nYou can specify additional parameters:\n- `temperature`: Controls response randomness (0-2)\n- `max_tokens`: Limits response length\n- `search_domain_filter`: Restricts search to specific domains\n- `search_recency_filter`: Filters by time period (day/week/month/year)\n\n## Troubleshooting\n\n1. Git not found:\n   - Make sure you've installed Git correctly\n   - Try restarting your terminal\n   - On Mac, make sure Homebrew is in your PATH\n\n2. Node.js errors:\n   - Verify Node.js installation with `node --version`\n   - Try reinstalling Node.js\n\n3. API Key issues:\n   - Make sure you've correctly copied your API key\n   - Check that there are no extra spaces in your .env file\n   - If using Option 2, verify dotenv is installed\n\n4. Tool not appearing in Claude:\n   - Check the path in claude_desktop_config.json\n   - Make sure the path points to your server.js file\n   - Restart Claude Desktop\n   - Check the console for any error messages\n\n## License\n\nMIT\n\n## Security Note\n\nIf you're planning to share your code or make it public:\n- Don't commit your API key to Git\n- Use the .env method (Option 2)\n- Add .env to your .gitignore file\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "citations",
        "search",
        "research",
        "perplexity tool",
        "perplexity ai",
        "web search"
      ],
      "category": "web-search"
    },
    "lieyanqzu--ygocdb-mcp": {
      "owner": "lieyanqzu",
      "name": "ygocdb-mcp",
      "url": "https://github.com/lieyanqzu/ygocdb-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/lieyanqzu.webp",
      "description": "Provides access to Chinese Yu-Gi-Oh! card information and images via keyword searches and ID queries, allowing for detailed retrieval of card data and visuals from the game.",
      "stars": 0,
      "forks": 2,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-09-09T05:51:51Z",
      "readme_content": "# 百鸽(ygocdb.com) MCP Server\n\n[English](README/README.en.md) | 中文\n\n一个基于 [Model Context Protocol (MCP)](https://modelcontextprotocol.io/) 的服务端，用于与 [百鸽(ygocdb.com)](https://ygocdb.com/)等 API 交互。提供了一系列工具来查询游戏王中文卡牌信息。\n\n[![smithery badge](https://smithery.ai/badge/@lieyanqzu/ygocdb-mcp)](https://smithery.ai/server/@lieyanqzu/ygocdb-mcp)\n\n<a href=\"https://glama.ai/mcp/servers/@lieyanqzu/ygocdb-mcp\">\n  <img alt=\"badge\" width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@lieyanqzu/ygocdb-mcp/badge\" />\n</a>\n\n## API 文档\n\n本服务端基于游戏王卡牌数据库的公开 API。\n\n- 卡牌搜索: `https://ygocdb.com/api/v0/?search=关键字`\n- 卡牌图片: `https://cdn.233.momobako.com/ygopro/pics/<id>.jpg`\n\n## 使用示例\n\n\n\n## 功能特性\n\n- **search_cards**  \n  通过关键字搜索游戏王卡牌，可以搜索卡牌名称、效果文本等。\n  \n- **get_card_by_id**  \n  通过卡牌ID获取单张游戏王卡牌的详细信息。\n  \n- **get_card_image**  \n  通过卡牌ID获取游戏王卡牌的图片。\n\n## 使用方法\n\n### 通过 NPM 包使用\n\n```bash\n# 全局安装\nnpm install -g ygocdb-mcp-server\n\n# 或直接运行（推荐）\nnpx ygocdb-mcp-server\n```\n\n### 本地开发\n\n```bash\n# 克隆项目\ngit clone <repository-url>\ncd ygocdb-mcp\n\n# 安装依赖\nnpm install\n\n# 构建项目\nnpm run build\n\n# 运行 STDIO 模式\nnpm run start:stdio\n\n# 运行 HTTP 模式\nnpm run start:http\n```\n\n### 运行模式\n\n服务端支持两种运行模式：\n\n#### STDIO 模式（默认）\n用于与 Claude Desktop 等 MCP 客户端直接集成：\n\n```bash\nnpm run start:stdio\n```\n\n#### HTTP 模式\n用于容器部署或 HTTP 客户端访问：\n\n```bash\nnpm run start:http\n```\n\nHTTP 服务器将在端口 8081 上启动，端点为 `http://localhost:8081/mcp`\n\n### 在 Claude Desktop 中集成\n\n在 `claude_desktop_config.json` 中添加配置：\n\n#### 使用 NPX（推荐）\n```json\n{\n  \"mcpServers\": {\n    \"ygocdb\": {\n      \"command\": \"npx\",\n      \"args\": [\"ygocdb-mcp-server\"]\n    }\n  }\n}\n```\n\n#### 使用本地构建\n```json\n{\n  \"mcpServers\": {\n    \"ygocdb\": {\n      \"command\": \"node\",\n      \"args\": [\"path/to/ygocdb-mcp/dist/index.js\"],\n      \"cwd\": \"path/to/ygocdb-mcp\"\n    }\n  }\n}\n```\n\n### Docker 部署\n\n```bash\n# 构建镜像\ndocker build -t ygocdb-mcp .\n\n# 运行 STDIO 模式（用于集成）\ndocker run -i --rm ygocdb-mcp\n\n# 运行 HTTP 模式（用于服务）\ndocker run -p 8081:8081 ygocdb-mcp\n```\n\n### 跨平台支持\n\n项目使用 `cross-env` 确保在所有平台上正确设置环境变量：\n\n- **Windows**: `npm run start:http` 或 `npm run start:stdio`\n- **macOS/Linux**: `npm run start:http` 或 `npm run start:stdio`\n- **Docker**: 自动使用 HTTP 模式",
      "npm_url": "https://www.npmjs.com/package/ygocdb-mcp-server",
      "npm_downloads": 282,
      "keywords": [
        "card",
        "ygocdb",
        "searches",
        "retrieval card",
        "card information",
        "ygocdb mcp"
      ],
      "category": "web-search"
    },
    "limklister--mcp-google-custom-search-server": {
      "owner": "limklister",
      "name": "mcp-google-custom-search-server",
      "url": "https://github.com/limklister/mcp-google-custom-search-server",
      "imageUrl": "/freedevtools/mcp/pfp/limklister.webp",
      "description": "Enables language models to perform web searches using Google's Custom Search API, providing formatted results such as titles, URLs, and descriptions. Supports real-time information retrieval while adhering to the Model Context Protocol (MCP).",
      "stars": 33,
      "forks": 17,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-09-23T14:50:30Z",
      "readme_content": "\n# MCP Google Custom Search Server\n\nA Model Context Protocol (MCP) server that provides web search capabilities through Google's Custom Search API. This server enables Language Learning Models (LLMs) to perform web searches using a standardized interface.\n\n\n\n## 🌟 Features\n\n- Seamless integration with Google Custom Search API\n- Model Context Protocol (MCP) compliant server implementation\n- Type-safe implementation using TypeScript\n- Environment variable configuration\n- Input validation using Zod\n- Configurable search results (up to 10 per query)\n- Formatted search results including titles, URLs, and descriptions\n- Error handling and validation\n- Compatible with Claude Desktop and other MCP clients\n\n## 📋 Prerequisites\n\nBefore you begin, ensure you have:\n\n1. A Google Cloud Project with Custom Search API enabled\n\n   - Visit [Google Cloud Console](https://console.cloud.google.com)\n   - Enable the Custom Search API\n   - Create API credentials\n\n2. A Custom Search Engine ID\n\n   - Visit [Programmable Search Engine](https://programmablesearchengine.google.com/)\n   - Create a new search engine\n   - Get your Search Engine ID\n\n3. Local development requirements:\n   - Node.js (v18 or higher)\n   - npm (comes with Node.js)\n\n## 🚀 Quick Start\n\n1. Clone the repository:\n\n   ```bash\n   git clone https://github.com/limklister/mcp-google-custom-search-server.git\n   cd mcp-google-custom-search-server\n   ```\n\n2. Install dependencies:\n\n   ```bash\n   npm install\n   ```\n\n3. Create a .env file:\n\n   ```bash\n   GOOGLE_API_KEY=your-api-key\n   GOOGLE_SEARCH_ENGINE_ID=your-search-engine-id\n   ```\n\n4. Build the server:\n\n   ```bash\n   npm run build\n   ```\n\n5. Start the server:\n   ```bash\n   npm start\n   ```\n\n## 🔧 Configuration\n\n### Environment Variables\n\n| Variable                | Description                       | Required |\n| ----------------------- | --------------------------------- | -------- |\n| GOOGLE_API_KEY          | Your Google Custom Search API key | Yes      |\n| GOOGLE_SEARCH_ENGINE_ID | Your Custom Search Engine ID      | Yes      |\n\n### Claude Desktop Integration\n\nAdd this configuration to your Claude Desktop config file (typically located at `~/Library/Application Support/Claude/claude_desktop_config.json`):\n\n```json\n{\n  \"mcpServers\": {\n    \"google-search\": {\n      \"command\": \"node\",\n      \"args\": [\n        \"/absolute/path/to/mcp-google-custom-search-server/build/index.js\"\n      ],\n      \"env\": {\n        \"GOOGLE_API_KEY\": \"your-api-key\",\n        \"GOOGLE_SEARCH_ENGINE_ID\": \"your-search-engine-id\"\n      }\n    }\n  }\n}\n```\n\n## 📖 API Reference\n\n### Available Tools\n\n#### search\n\nPerforms a web search using Google Custom Search API.\n\n**Parameters:**\n\n- `query` (string, required): The search query to execute\n- `numResults` (number, optional): Number of results to return\n  - Default: 5\n  - Maximum: 10\n\n**Example Response:**\n\n```\nResult 1:\nTitle: Example Search Result\nURL: https://example.com\nDescription: This is an example search result description\n---\n\nResult 2:\n...\n```\n\n## 🛠️ Development\n\n### Project Structure\n\n```\nmcp-google-custom-search-server/\n├── src/\n│   └── index.ts          # Main server implementation\n├── build/                # Compiled JavaScript output\n├── .env                  # Environment variables\n├── package.json          # Project dependencies and scripts\n├── tsconfig.json         # TypeScript configuration\n└── README.md            # Project documentation\n```\n\n### Available Scripts\n\n- `npm run build`: Compile TypeScript to JavaScript\n- `npm start`: Start the MCP server\n- `npm run dev`: Watch mode for development\n\n### Testing\n\n1. Using MCP Inspector:\n\n   ```bash\n   npx @modelcontextprotocol/inspector node build/index.js\n   ```\n\n2. Manual testing with example queries:\n   ```bash\n   # After starting the server\n   {\"jsonrpc\":\"2.0\",\"id\":1,\"method\":\"callTool\",\"params\":{\"name\":\"search\",\"arguments\":{\"query\":\"example search\"}}}\n   ```\n\n<a href=\"https://glama.ai/mcp/servers/y1s99uqqq6\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/y1s99uqqq6/badge\" alt=\"Google Custom Search Server MCP server\" />\n</a>\n\n[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/limklister-mcp-google-custom-search-server-badge.png)](https://mseep.ai/app/limklister-mcp-google-custom-search-server)\n\n\n## 📝 License\n\nThis project is licensed under the MIT License - see the LICENSE file for details.\n\n## 🙏 Acknowledgments\n\n- Built with [Model Context Protocol (MCP)](https://github.com/anthropics/model-context-protocol)\n- Uses Google's Custom Search API\n- Inspired by the need for better search capabilities in LLM applications\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "searches",
        "google",
        "limklister",
        "mcp google",
        "search limklister",
        "web search"
      ],
      "category": "web-search"
    },
    "liusheding--DeepSeek-R1": {
      "owner": "liusheding",
      "name": "DeepSeek-R1",
      "url": "https://github.com/liusheding/DeepSeek-R1",
      "imageUrl": "/freedevtools/mcp/pfp/liusheding.webp",
      "description": "Provides advanced reasoning capabilities utilizing distilled models for enhanced performance in mathematical, coding, and reasoning tasks. Designed to assist in a variety of applications requiring sophisticated model interactions.",
      "stars": 0,
      "forks": 0,
      "license": "MIT License",
      "language": "",
      "updated_at": "2025-01-28T01:16:04Z",
      "readme_content": "# DeepSeek-R1\n<!-- markdownlint-disable first-line-h1 -->\n<!-- markdownlint-disable html -->\n<!-- markdownlint-disable no-duplicate-header -->\n\n<div align=\"center\">\n  <img src=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true\" width=\"60%\" alt=\"DeepSeek-V3\" />\n</div>\n<hr>\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href=\"https://www.deepseek.com/\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Homepage\" src=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/badge.svg?raw=true\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://chat.deepseek.com/\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Chat\" src=\"https://img.shields.io/badge/🤖%20Chat-DeepSeek%20R1-536af5?color=536af5&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://huggingface.co/deepseek-ai\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Hugging Face\" src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DeepSeek%20AI-ffc107?color=ffc107&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n</div>\n\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href=\"https://discord.gg/Tc7c45Zzu5\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Discord\" src=\"https://img.shields.io/badge/Discord-DeepSeek%20AI-7289da?logo=discord&logoColor=white&color=7289da\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/qr.jpeg?raw=true\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Wechat\" src=\"https://img.shields.io/badge/WeChat-DeepSeek%20AI-brightgreen?logo=wechat&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://twitter.com/deepseek_ai\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Twitter Follow\" src=\"https://img.shields.io/badge/Twitter-deepseek_ai-white?logo=x&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n</div>\n\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href=\"https://github.com/deepseek-ai/DeepSeek-R1/blob/main/LICENSE\" style=\"margin: 2px;\">\n    <img alt=\"License\" src=\"https://img.shields.io/badge/License-MIT-f5de53?&color=f5de53\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n</div>\n\n\n<p align=\"center\">\n  <a href=\"https://github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek_R1.pdf\"><b>Paper Link</b>👁️</a>\n</p>\n\n\n## 1. Introduction\n\nWe introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. \nDeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrated remarkable performance on reasoning.\nWith RL, DeepSeek-R1-Zero naturally emerged with numerous powerful and interesting reasoning behaviors.\nHowever, DeepSeek-R1-Zero encounters challenges such as endless repetition, poor readability, and language mixing. To address these issues and further enhance reasoning performance,\nwe introduce DeepSeek-R1, which incorporates cold-start data before RL.\nDeepSeek-R1 achieves performance comparable to OpenAI-o1 across math, code, and reasoning tasks. \nTo support the research community, we have open-sourced DeepSeek-R1-Zero, DeepSeek-R1, and six dense models distilled from DeepSeek-R1 based on Llama and Qwen. DeepSeek-R1-Distill-Qwen-32B outperforms OpenAI-o1-mini across various benchmarks, achieving new state-of-the-art results for dense models.\n\n**NOTE: Before running DeepSeek-R1 series models locally, we kindly recommend reviewing the [Usage Recommendation](#usage-recommendations) section.**\n\n<p align=\"center\">\n  \n</p>\n\n## 2. Model Summary\n\n---\n\n**Post-Training: Large-Scale Reinforcement Learning on the Base Model**\n\n-  We directly apply reinforcement learning (RL) to the base model without relying on supervised fine-tuning (SFT) as a preliminary step. This approach allows the model to explore chain-of-thought (CoT) for solving complex problems, resulting in the development of DeepSeek-R1-Zero. DeepSeek-R1-Zero demonstrates capabilities such as self-verification, reflection, and generating long CoTs, marking a significant milestone for the research community. Notably, it is the first open research to validate that reasoning capabilities of LLMs can be incentivized purely through RL, without the need for SFT. This breakthrough paves the way for future advancements in this area.\n\n-   We introduce our pipeline to develop DeepSeek-R1. The pipeline incorporates two RL stages aimed at discovering improved reasoning patterns and aligning with human preferences, as well as two SFT stages that serve as the seed for the model's reasoning and non-reasoning capabilities.\n    We believe the pipeline will benefit the industry by creating better models. \n\n---\n\n**Distillation: Smaller Models Can Be Powerful Too**\n\n-  We demonstrate that the reasoning patterns of larger models can be distilled into smaller models, resulting in better performance compared to the reasoning patterns discovered through RL on small models. The open source DeepSeek-R1, as well as its API, will benefit the research community to distill better smaller models in the future. \n- Using the reasoning data generated by DeepSeek-R1, we fine-tuned several dense models that are widely used in the research community. The evaluation results demonstrate that the distilled smaller dense models perform exceptionally well on benchmarks. We open-source distilled 1.5B, 7B, 8B, 14B, 32B, and 70B checkpoints based on Qwen2.5 and Llama3 series to the community.\n\n## 3. Model Downloads\n\n### DeepSeek-R1 Models\n\n<div align=\"center\">\n\n| **Model** | **#Total Params** | **#Activated Params** | **Context Length** | **Download** |\n| :------------: | :------------: | :------------: | :------------: | :------------: |\n| DeepSeek-R1-Zero | 671B | 37B | 128K   | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Zero)   |\n| DeepSeek-R1   | 671B | 37B |  128K   | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1)   |\n\n</div>\n\nDeepSeek-R1-Zero & DeepSeek-R1 are trained based on DeepSeek-V3-Base. \nFor more details regarding the model architecture, please refer to [DeepSeek-V3](https://github.com/deepseek-ai/DeepSeek-V3) repository.\n\n### DeepSeek-R1-Distill Models\n\n<div align=\"center\">\n\n| **Model** | **Base Model** | **Download** |\n| :------------: | :------------: | :------------: |\n| DeepSeek-R1-Distill-Qwen-1.5B  | [Qwen2.5-Math-1.5B](https://huggingface.co/Qwen/Qwen2.5-Math-1.5B) | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B)   |\n| DeepSeek-R1-Distill-Qwen-7B  | [Qwen2.5-Math-7B](https://huggingface.co/Qwen/Qwen2.5-Math-7B) | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B)   |\n| DeepSeek-R1-Distill-Llama-8B  | [Llama-3.1-8B](https://huggingface.co/meta-llama/Llama-3.1-8B) | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-8B)   |\n| DeepSeek-R1-Distill-Qwen-14B   | [Qwen2.5-14B](https://huggingface.co/Qwen/Qwen2.5-14B) | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B)   |\n|DeepSeek-R1-Distill-Qwen-32B  | [Qwen2.5-32B](https://huggingface.co/Qwen/Qwen2.5-32B) | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B)   |\n| DeepSeek-R1-Distill-Llama-70B  | [Llama-3.3-70B-Instruct](https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct) | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-70B)   |\n\n</div>\n\nDeepSeek-R1-Distill models are fine-tuned based on open-source models, using samples generated by DeepSeek-R1.\nWe slightly change their configs and tokenizers. Please use our setting to run these models.\n\n## 4. Evaluation Results\n\n### DeepSeek-R1-Evaluation\n For all our models, the maximum generation length is set to 32,768 tokens. For benchmarks requiring sampling, we use a temperature of $0.6$, a top-p value of $0.95$, and generate 64 responses per query to estimate pass@1.\n<div align=\"center\">\n\n\n| Category | Benchmark (Metric) | Claude-3.5-Sonnet-1022 | GPT-4o 0513 | DeepSeek V3 | OpenAI o1-mini | OpenAI o1-1217 | DeepSeek R1 |\n|----------|-------------------|----------------------|------------|--------------|----------------|------------|--------------|\n| | Architecture | - | - | MoE | - | - | MoE |\n| | # Activated Params | - | - | 37B | - | - | 37B |\n| | # Total Params | - | - | 671B | - | - | 671B |\n| English | MMLU (Pass@1) | 88.3 | 87.2 | 88.5 | 85.2 | **91.8** | 90.8 |\n| | MMLU-Redux (EM) | 88.9 | 88.0 | 89.1 | 86.7 | - | **92.9** |\n| | MMLU-Pro (EM) | 78.0 | 72.6 | 75.9 | 80.3 | - | **84.0** |\n| | DROP (3-shot F1) | 88.3 | 83.7 | 91.6 | 83.9 | 90.2 | **92.2** |\n| | IF-Eval (Prompt Strict) | **86.5** | 84.3 | 86.1 | 84.8 | - | 83.3 |\n| | GPQA-Diamond (Pass@1) | 65.0 | 49.9 | 59.1 | 60.0 | **75.7** | 71.5 |\n| | SimpleQA (Correct) | 28.4 | 38.2 | 24.9 | 7.0 | **47.0** | 30.1 |\n| | FRAMES (Acc.) | 72.5 | 80.5 | 73.3 | 76.9 | - | **82.5** |\n| | AlpacaEval2.0 (LC-winrate) | 52.0 | 51.1 | 70.0 | 57.8 | - | **87.6** |\n| | ArenaHard (GPT-4-1106) | 85.2 | 80.4 | 85.5 | 92.0 | - | **92.3** |\n| Code | LiveCodeBench (Pass@1-COT) | 33.8 | 34.2 | - | 53.8 | 63.4 | **65.9** |\n| | Codeforces (Percentile) | 20.3 | 23.6 | 58.7 | 93.4 | **96.6** | 96.3 |\n| | Codeforces (Rating) | 717 | 759 | 1134 | 1820 | **2061** | 2029 |\n| | SWE Verified (Resolved) | **50.8** | 38.8 | 42.0 | 41.6 | 48.9 | 49.2 |\n| | Aider-Polyglot (Acc.) | 45.3 | 16.0 | 49.6 | 32.9 | **61.7** | 53.3 |\n| Math | AIME 2024 (Pass@1) | 16.0 | 9.3 | 39.2 | 63.6 | 79.2 | **79.8** |\n| | MATH-500 (Pass@1) | 78.3 | 74.6 | 90.2 | 90.0 | 96.4 | **97.3** |\n| | CNMO 2024 (Pass@1) | 13.1 | 10.8 | 43.2 | 67.6 | - | **78.8** |\n| Chinese | CLUEWSC (EM) | 85.4 | 87.9 | 90.9 | 89.9 | - | **92.8** |\n| | C-Eval (EM) | 76.7 | 76.0 | 86.5 | 68.9 | - | **91.8** |\n| | C-SimpleQA (Correct) | 55.4 | 58.7 | **68.0** | 40.3 | - | 63.7 |\n\n</div>\n\n\n### Distilled Model Evaluation\n\n\n<div align=\"center\">\n\n| Model                                    | AIME 2024 pass@1 | AIME 2024 cons@64 | MATH-500 pass@1 | GPQA Diamond pass@1 | LiveCodeBench pass@1 | CodeForces rating |\n|------------------------------------------|------------------|-------------------|-----------------|----------------------|----------------------|-------------------|\n| GPT-4o-0513                          | 9.3              | 13.4              | 74.6            | 49.9                 | 32.9                 | 759               |\n| Claude-3.5-Sonnet-1022             | 16.0             | 26.7                 | 78.3            | 65.0                 | 38.9                 | 717               |\n| o1-mini                              | 63.6             | 80.0              | 90.0            | 60.0                 | 53.8                 | **1820**          |\n| QwQ-32B-Preview                              | 44.0             | 60.0                 | 90.6            | 54.5               | 41.9                 | 1316              |\n| DeepSeek-R1-Distill-Qwen-1.5B       | 28.9             | 52.7              | 83.9            | 33.8                 | 16.9                 | 954               |\n| DeepSeek-R1-Distill-Qwen-7B          | 55.5             | 83.3              | 92.8            | 49.1                 | 37.6                 | 1189              |\n| DeepSeek-R1-Distill-Qwen-14B         | 69.7             | 80.0              | 93.9            | 59.1                 | 53.1                 | 1481              |\n| DeepSeek-R1-Distill-Qwen-32B        | **72.6**         | 83.3              | 94.3            | 62.1                 | 57.2                 | 1691              |\n| DeepSeek-R1-Distill-Llama-8B         | 50.4             | 80.0              | 89.1            | 49.0                 | 39.6                 | 1205              |\n| DeepSeek-R1-Distill-Llama-70B        | 70.0             | **86.7**          | **94.5**        | **65.2**             | **57.5**             | 1633              |\n\n</div>\n\n\n## 5. Chat Website & API Platform\nYou can chat with DeepSeek-R1 on DeepSeek's official website: [chat.deepseek.com](https://chat.deepseek.com), and switch on the button \"DeepThink\"\n\nWe also provide OpenAI-Compatible API at DeepSeek Platform: [platform.deepseek.com](https://platform.deepseek.com/)\n\n## 6. How to Run Locally\n\n### DeepSeek-R1 Models\n\nPlease visit [DeepSeek-V3](https://github.com/deepseek-ai/DeepSeek-V3) repo for more information about running DeepSeek-R1 locally.\n\n**NOTE: Hugging Face's Transformers has not been directly supported yet.**\n\n### DeepSeek-R1-Distill Models\n\nDeepSeek-R1-Distill models can be utilized in the same manner as Qwen or Llama models.\n\nFor instance, you can easily start a service using [vLLM](https://github.com/vllm-project/vllm):\n\n```shell\nvllm serve deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --tensor-parallel-size 2 --max-model-len 32768 --enforce-eager\n```\n\nYou can also easily start a service using [SGLang](https://github.com/sgl-project/sglang)\n\n```bash\npython3 -m sglang.launch_server --model deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --trust-remote-code --tp 2\n```\n\n### Usage Recommendations\n\n**We recommend adhering to the following configurations when utilizing the DeepSeek-R1 series models, including benchmarking, to achieve the expected performance:**\n\n1. Set the temperature within the range of 0.5-0.7 (0.6 is recommended) to prevent endless repetitions or incoherent outputs.\n2. **Avoid adding a system prompt; all instructions should be contained within the user prompt.**\n3. For mathematical problems, it is advisable to include a directive in your prompt such as: \"Please reason step by step, and put your final answer within \\boxed{}.\"\n4. When evaluating model performance, it is recommended to conduct multiple tests and average the results.\n\n## 7. License\nThis code repository and the model weights are licensed under the [MIT License](https://github.com/deepseek-ai/DeepSeek-R1/blob/main/LICENSE).\nDeepSeek-R1 series support commercial use, allow for any modifications and derivative works, including, but not limited to, distillation for training other LLMs. Please note that:\n- DeepSeek-R1-Distill-Qwen-1.5B, DeepSeek-R1-Distill-Qwen-7B, DeepSeek-R1-Distill-Qwen-14B and DeepSeek-R1-Distill-Qwen-32B are derived from [Qwen-2.5 series](https://github.com/QwenLM/Qwen2.5), which are originally licensed under [Apache 2.0 License](https://huggingface.co/Qwen/Qwen2.5-1.5B/blob/main/LICENSE), and now finetuned with 800k samples curated with DeepSeek-R1.\n- DeepSeek-R1-Distill-Llama-8B is derived from Llama3.1-8B-Base and is originally licensed under [llama3.1 license](https://huggingface.co/meta-llama/Llama-3.1-8B/blob/main/LICENSE).\n- DeepSeek-R1-Distill-Llama-70B is derived from Llama3.3-70B-Instruct and is originally licensed under [llama3.3 license](https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct/blob/main/LICENSE).\n\n## 8. Citation\n```\n@misc{deepseekai2025deepseekr1incentivizingreasoningcapability,\n      title={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning}, \n      author={DeepSeek-AI and Daya Guo and Dejian Yang and Haowei Zhang and Junxiao Song and Ruoyu Zhang and Runxin Xu and Qihao Zhu and Shirong Ma and Peiyi Wang and Xiao Bi and Xiaokang Zhang and Xingkai Yu and Yu Wu and Z. F. Wu and Zhibin Gou and Zhihong Shao and Zhuoshu Li and Ziyi Gao and Aixin Liu and Bing Xue and Bingxuan Wang and Bochao Wu and Bei Feng and Chengda Lu and Chenggang Zhao and Chengqi Deng and Chenyu Zhang and Chong Ruan and Damai Dai and Deli Chen and Dongjie Ji and Erhang Li and Fangyun Lin and Fucong Dai and Fuli Luo and Guangbo Hao and Guanting Chen and Guowei Li and H. Zhang and Han Bao and Hanwei Xu and Haocheng Wang and Honghui Ding and Huajian Xin and Huazuo Gao and Hui Qu and Hui Li and Jianzhong Guo and Jiashi Li and Jiawei Wang and Jingchang Chen and Jingyang Yuan and Junjie Qiu and Junlong Li and J. L. Cai and Jiaqi Ni and Jian Liang and Jin Chen and Kai Dong and Kai Hu and Kaige Gao and Kang Guan and Kexin Huang and Kuai Yu and Lean Wang and Lecong Zhang and Liang Zhao and Litong Wang and Liyue Zhang and Lei Xu and Leyi Xia and Mingchuan Zhang and Minghua Zhang and Minghui Tang and Meng Li and Miaojun Wang and Mingming Li and Ning Tian and Panpan Huang and Peng Zhang and Qiancheng Wang and Qinyu Chen and Qiushi Du and Ruiqi Ge and Ruisong Zhang and Ruizhe Pan and Runji Wang and R. J. Chen and R. L. Jin and Ruyi Chen and Shanghao Lu and Shangyan Zhou and Shanhuang Chen and Shengfeng Ye and Shiyu Wang and Shuiping Yu and Shunfeng Zhou and Shuting Pan and S. S. Li and Shuang Zhou and Shaoqing Wu and Shengfeng Ye and Tao Yun and Tian Pei and Tianyu Sun and T. Wang and Wangding Zeng and Wanjia Zhao and Wen Liu and Wenfeng Liang and Wenjun Gao and Wenqin Yu and Wentao Zhang and W. L. Xiao and Wei An and Xiaodong Liu and Xiaohan Wang and Xiaokang Chen and Xiaotao Nie and Xin Cheng and Xin Liu and Xin Xie and Xingchao Liu and Xinyu Yang and Xinyuan Li and Xuecheng Su and Xuheng Lin and X. Q. Li and Xiangyue Jin and Xiaojin Shen and Xiaosha Chen and Xiaowen Sun and Xiaoxiang Wang and Xinnan Song and Xinyi Zhou and Xianzu Wang and Xinxia Shan and Y. K. Li and Y. Q. Wang and Y. X. Wei and Yang Zhang and Yanhong Xu and Yao Li and Yao Zhao and Yaofeng Sun and Yaohui Wang and Yi Yu and Yichao Zhang and Yifan Shi and Yiliang Xiong and Ying He and Yishi Piao and Yisong Wang and Yixuan Tan and Yiyang Ma and Yiyuan Liu and Yongqiang Guo and Yuan Ou and Yuduan Wang and Yue Gong and Yuheng Zou and Yujia He and Yunfan Xiong and Yuxiang Luo and Yuxiang You and Yuxuan Liu and Yuyang Zhou and Y. X. Zhu and Yanhong Xu and Yanping Huang and Yaohui Li and Yi Zheng and Yuchen Zhu and Yunxian Ma and Ying Tang and Yukun Zha and Yuting Yan and Z. Z. Ren and Zehui Ren and Zhangli Sha and Zhe Fu and Zhean Xu and Zhenda Xie and Zhengyan Zhang and Zhewen Hao and Zhicheng Ma and Zhigang Yan and Zhiyu Wu and Zihui Gu and Zijia Zhu and Zijun Liu and Zilin Li and Ziwei Xie and Ziyang Song and Zizheng Pan and Zhen Huang and Zhipeng Xu and Zhongyu Zhang and Zhen Zhang},\n      year={2025},\n      eprint={2501.12948},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2501.12948}, \n}\n\n```\n\n## 9. Contact\nIf you have any questions, please raise an issue or contact us at [service@deepseek.com](service@deepseek.com).",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "deepseek",
        "models",
        "reasoning",
        "deepseek r1",
        "reasoning capabilities",
        "coding reasoning"
      ],
      "category": "web-search"
    },
    "lixiaotiancai--exa-mcp-server": {
      "owner": "lixiaotiancai",
      "name": "exa-mcp-server",
      "url": "https://github.com/lixiaotiancai/exa-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/null.webp",
      "description": "Enables AI assistants to perform web searches using Exa's search API, providing real-time web information with structured search results. It also caches recent searches for future access in a safe and controlled manner.",
      "stars": 0,
      "forks": 0,
      "license": "Unknown",
      "language": "Unknown",
      "updated_at": "",
      "readme_content": "",
      "npm_url": "https://www.npmjs.com/package/exa-mcp-server",
      "npm_downloads": 97542,
      "keywords": [
        "searches",
        "search",
        "exa",
        "exa search",
        "search lixiaotiancai",
        "web search"
      ],
      "category": "web-search"
    },
    "lmcc-dev--mult-fetch-mcp-server": {
      "owner": "lmcc-dev",
      "name": "mult-fetch-mcp-server",
      "url": "https://github.com/lmcc-dev/mult-fetch-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/lmcc-dev.webp",
      "description": "Fetch web content in various formats such as HTML and JSON using intelligent scraping techniques while providing bilingual support for English and Chinese.",
      "stars": 13,
      "forks": 6,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-09-10T08:03:43Z",
      "readme_content": "[![MseeP.ai Security Assessment Badge](https://mseep.net/mseep-audited.png)](https://mseep.ai/app/lmcc-dev-mult-fetch-mcp-server)\n\n# mult-fetch-mcp-server\n\n[![npm version](https://img.shields.io/npm/v/@lmcc-dev/mult-fetch-mcp-server.svg)](https://www.npmjs.com/package/@lmcc-dev/mult-fetch-mcp-server)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![Node.js Version](https://img.shields.io/node/v/@lmcc-dev/mult-fetch-mcp-server)](https://nodejs.org/)\n[![TypeScript](https://img.shields.io/badge/TypeScript-5.7-blue.svg)](https://www.typescriptlang.org/)\n[![MCP SDK](https://img.shields.io/badge/MCP%20SDK-1.7.0-brightgreen.svg)](https://github.com/modelcontextprotocol/typescript-sdk)\n[![GitHub Stars](https://img.shields.io/github/stars/lmcc-dev/mult-fetch-mcp-server)](https://github.com/lmcc-dev/mult-fetch-mcp-server/stargazers)\n[![GitHub Forks](https://img.shields.io/github/forks/lmcc-dev/mult-fetch-mcp-server)](https://github.com/lmcc-dev/mult-fetch-mcp-server/network/members)\n[![GitHub Issues](https://img.shields.io/github/issues/lmcc-dev/mult-fetch-mcp-server)](https://github.com/lmcc-dev/mult-fetch-mcp-server/issues)\n[![GitHub Pull Requests](https://img.shields.io/github/issues-pr/lmcc-dev/mult-fetch-mcp-server)](https://github.com/lmcc-dev/mult-fetch-mcp-server/pulls)\n[![npm downloads](https://img.shields.io/badge/downloads-coming%20soon-lightgrey)](https://www.npmjs.com/package/@lmcc-dev/mult-fetch-mcp-server)\n[![GitHub last commit](https://img.shields.io/github/last-commit/lmcc-dev/mult-fetch-mcp-server)](https://github.com/lmcc-dev/mult-fetch-mcp-server/commits/main)\n[![GitHub contributors](https://img.shields.io/github/contributors/lmcc-dev/mult-fetch-mcp-server)](https://github.com/lmcc-dev/mult-fetch-mcp-server/graphs/contributors)\n[![smithery badge](https://smithery.ai/badge/@lmcc-dev/mult-fetch-mcp-server)](https://smithery.ai/server/@lmcc-dev/mult-fetch-mcp-server)\n[![codecov](https://codecov.io/gh/lmcc-dev/mult-fetch-mcp-server/branch/main/graph/badge.svg)](https://codecov.io/gh/lmcc-dev/mult-fetch-mcp-server)\n[![CodeFactor](https://www.codefactor.io/repository/github/lmcc-dev/mult-fetch-mcp-server/badge)](https://www.codefactor.io/repository/github/lmcc-dev/mult-fetch-mcp-server)\n\n\n\n<!-- Future badges to consider:\n[![CodeFactor](https://www.codefactor.io/repository/github/lmcc-dev/mult-fetch-mcp-server/badge)](https://www.codefactor.io/repository/github/lmcc-dev/mult-fetch-mcp-server)\n[![Maintainability](https://api.codeclimate.com/v1/badges/a99a88d28ad37a79dbf6/maintainability)](https://codeclimate.com/github/lmcc-dev/mult-fetch-mcp-server/maintainability)\n-->\n\nThis project implements an MCP-compliant client and server for communication between AI assistants and external tools.\n\n[English](./README.md) | [中文文档](./README.zh.md)\n\n## Project Structure\n\n```\nfetch-mcp/\n├── src/                         # Source code directory\n│   ├── lib/                     # Library files\n│   │   ├── fetchers/            # Web fetching implementation\n│   │   │   ├── browser/         # Browser-based fetching\n│   │   │   │   ├── BrowserFetcher.ts      # Browser fetcher implementation\n│   │   │   │   ├── BrowserInstance.ts     # Browser instance management\n│   │   │   │   └── PageOperations.ts      # Page interaction operations\n│   │   │   ├── node/            # Node.js-based fetching\n│   │   │   └── common/          # Shared fetching utilities\n│   │   ├── utils/               # Utility modules\n│   │   │   ├── ChunkManager.ts        # Content chunking\n│   │   │   ├── ContentProcessor.ts    # HTML to text conversion\n│   │   │   ├── ContentExtractor.ts    # Intelligent content extraction\n│   │   │   ├── ContentSizeManager.ts  # Content size limiting\n│   │   │   └── ErrorHandler.ts        # Error handling\n│   │   ├── server/              # Server-related modules\n│   │   │   ├── index.ts         # Server entry\n│   │   │   ├── browser.ts       # Browser management\n│   │   │   ├── fetcher.ts       # Web fetching logic\n│   │   │   ├── tools.ts         # Tool registration and handling\n│   │   │   ├── resources.ts     # Resource handling\n│   │   │   ├── prompts.ts       # Prompt templates\n│   │   │   └── types.ts         # Server type definitions\n│   │   ├── i18n/                # Internationalization support\n│   │   └── types.ts             # Common type definitions\n│   ├── client.ts                # MCP client implementation\n│   └── mcp-server.ts            # MCP server main entry\n├── index.ts                     # Server entry point\n├── tests/                       # Test files\n└── dist/                        # Compiled files\n```\n\n## MCP Specification\n\nThe Model Context Protocol (MCP) defines two main transport methods:\n\n1. **Standard Input/Output (Stdio)**: The client starts the MCP server as a child process, and they communicate through standard input (stdin) and standard output (stdout).\n2. **Server-Sent Events (SSE)**: Used to pass messages between client and server.\n\nThis project implements the Standard Input/Output (Stdio) transport method.\n\n## Features\n\n- Implementation based on the official MCP SDK\n- Support for Standard Input/Output (Stdio) transport\n- Multiple web scraping methods (HTML, JSON, text, Markdown, plain text conversion)\n- Intelligent mode switching: automatic switching between standard requests and browser mode\n- Content size management: automatically splits large content into manageable chunks to solve AI model context size limitations\n- Chunked content retrieval: ability to request specific chunks of large content while maintaining context continuity\n- Detailed debug logging to stderr\n- Bilingual internationalization (English and Chinese)\n- Modular design for easy maintenance and extension\n- **Intelligent Content Extraction**: Based on Mozilla's Readability library, capable of extracting meaningful content from web pages while filtering out advertisements and navigation elements\n- **Metadata Support**: Ability to extract webpage metadata such as title, author, publication date, and site information\n- **Smart Content Detection**: Automatically detects if a page contains meaningful content, filtering out login pages, error pages, and other pages without substantial content\n- **Browser Automation Enhancements**: Support for page scrolling, cookie management, selector waiting, and other advanced browser interactions\n\n## Installation\n\n### Installing via Smithery\n\nTo install Mult Fetch MCP Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@lmcc-dev/mult-fetch-mcp-server):\n\n```bash\nnpx -y @smithery/cli install @lmcc-dev/mult-fetch-mcp-server --client claude\n```\n\n### Local Installation\n\n```bash\npnpm install\n```\n\n### Global Installation\n\n```bash\npnpm add -g @lmcc-dev/mult-fetch-mcp-server\n```\n\nOr run directly with npx (no installation required):\n\n```bash\nnpx @lmcc-dev/mult-fetch-mcp-server\n```\n\n## Integration with Claude\n\nTo integrate this tool with Claude desktop, you need to add server configuration:\n\n### Configuration File Location\n\n- **MacOS**: `~/Library/Application Support/Claude/claude_desktop_config.json`\n- **Windows**: `%APPDATA%/Claude/claude_desktop_config.json`\n\n### Configuration Examples\n\n#### Method 1: Using npx (Recommended)\n\nThis method is the simplest, doesn't require specifying the full path, and is suitable for global installation or direct use with npx:\n\n```json\n{\n  \"mcpServers\": {\n    \"mult-fetch-mcp-server\": {\n      \"command\": \"npx\",\n      \"args\": [\"@lmcc-dev/mult-fetch-mcp-server\"],\n      \"env\": {\n        \"MCP_LANG\": \"en\"  // Set language to English, options: \"zh\" or \"en\"\n      }\n    }\n  }\n}\n```\n\n#### Method 2: Specifying Full Path\n\nIf you need to use a specific installation location, you can specify the full path:\n\n```json\n{\n  \"mcpServers\": {\n    \"mult-fetch-mcp-server\": {\n      \"command\": \"path-to/bin/node\",\n      \"args\": [\"path-to/@lmcc-dev/mult-fetch-mcp-server/dist/index.js\"],\n      \"env\": {\n        \"MCP_LANG\": \"en\"  // Set language to English, options: \"zh\" or \"en\"\n      }\n    }\n  }\n}\n```\n\nPlease replace `path-to/bin/node` with the path to the Node.js executable on your system, and replace `path-to/@lmcc-dev/mult-fetch-mcp-server` with the actual path to this project.\n\n### Usage Examples\n\nBelow is an example of using this tool in Claude desktop client:\n\n\n\nThe image shows how Claude can use the fetch tools to retrieve web content and process it according to your instructions.\n\n### Usage\n\nAfter configuration, restart Claude desktop, and you can use the following tools in your conversation:\n\n- `fetch_html`: Get HTML content of a webpage\n- `fetch_json`: Get JSON data\n- `fetch_txt`: Get plain text content\n- `fetch_markdown`: Get Markdown formatted content\n- `fetch_plaintext`: Get plain text content converted from HTML (strips HTML tags)\n\n## Build\n\n```bash\npnpm run build\n```\n\n## Run Server\n\n```bash\npnpm run server\n# or\nnode dist/index.js\n# if globally installed, you can run directly\n@lmcc-dev/mult-fetch-mcp-server\n# or use npx\nnpx @lmcc-dev/mult-fetch-mcp-server\n```\n\n## Client Demo Tools\n\n> **Note**: The following client.js functionality is provided for demonstration and testing purposes only. When used with Claude or other AI assistants, the MCP server is driven by the AI, which manages the chunking process automatically.\n\n### Command Line Client\n\nThe project includes a command-line client for testing and development purposes:\n\n```bash\npnpm run client <method> <params_json>\n# example\npnpm run client fetch_html '{\"url\": \"https://example.com\", \"debug\": true}'\n```\n\n### Demo Client Chunk Control Parameters\n\nWhen testing with the command-line client, you can use these parameters to demonstrate content chunking capabilities:\n\n- `--all-chunks`: Command line flag to automatically fetch all chunks in sequence (demonstration purpose only)\n- `--max-chunks`: Command line flag to limit the maximum number of chunks to fetch (optional, default is 10)\n\n#### Real-time Output Demo\n\nThe client.js demo tool provides real-time output capabilities:\n\n```bash\nnode dist/src/client.js fetch_html '{\"url\":\"https://example.com\", \"startCursor\": 0, \"contentSizeLimit\": 500}' --all-chunks --debug\n```\n\nThe demo client will automatically fetch all chunks in sequence and display them immediately, showcasing how large content can be processed in real-time.\n\n## Run Tests\n\n```bash\n# Run MCP functionality tests\nnpm run test:mcp\n\n# Run mini4k.com website tests\nnpm run test:mini4k\n\n# Run direct client call tests\nnpm run test:direct\n```\n\n## Language Settings\n\nThis project supports Chinese and English bilingual internationalization. You can set the language using environment variables:\n\n### Using Environment Variables\n\nSet the `MCP_LANG` environment variable to control the language:\n\n```bash\n# Set to English\nexport MCP_LANG=en\nnpm run server\n\n# Set to Chinese\nexport MCP_LANG=zh\nnpm run server\n\n# Windows system\nset MCP_LANG=zh\nnpm run server\n```\n\nUsing environment variables ensures that all related processes (including the MCP server) use the same language settings.\n\n### Default Language\n\nBy default, the system will choose a language according to the following priority:\n1. `MCP_LANG` environment variable\n2. Operating system language (if it starts with \"zh\", use Chinese)\n3. English (as the final fallback option)\n\n## Debugging\n\nThis project follows the MCP protocol specification and does not output any logs by default to avoid interfering with JSON-RPC communication. Debug information is controlled through call parameters:\n\n### Using the debug Parameter\n\nSet the `debug: true` parameter when calling a tool:\n\n```json\n{\n  \"url\": \"https://example.com\",\n  \"debug\": true\n}\n```\n\nDebug messages are sent to the standard error stream (stderr) using the following format:\n\n```\n[MCP-SERVER] MCP server starting...\n[CLIENT] Fetching URL: https://example.com\n```\n\n### Debug Log File\n\nWhen debug mode is enabled, all debug messages are also written to a log file located at:\n\n```\n~/.mult-fetch-mcp-server/debug.log\n```\n\nThis log file can be accessed through the MCP resources API:\n\n```typescript\n// Access the debug log file\nconst result = await client.readResource({ uri: \"file:///logs/debug\" });\nconsole.log(result.contents[0].text);\n\n// Clear the debug log file\nconst clearResult = await client.readResource({ uri: \"file:///logs/clear\" });\nconsole.log(clearResult.contents[0].text);\n```\n\n## Proxy Settings\n\nThis tool supports various methods to configure proxy settings:\n\n### 1. Using the `proxy` Parameter\n\nThe most direct way is to specify the proxy in the request parameters:\n\n```json\n{\n  \"url\": \"https://example.com\",\n  \"proxy\": \"http://your-proxy-server:port\",\n  \"debug\": true\n}\n```\n\n### 2. Using Environment Variables\n\nThe tool will automatically detect and use proxy settings from standard environment variables:\n\n```bash\n# Set proxy environment variables\nexport HTTP_PROXY=http://your-proxy-server:port\nexport HTTPS_PROXY=http://your-proxy-server:port\n\n# Run the server\nnpm run server\n```\n\n### 3. System Proxy Detection\n\nThe tool attempts to detect system proxy settings based on your operating system:\n\n- **Windows**: Reads proxy settings from environment variables using the `set` command\n- **macOS/Linux**: Reads proxy settings from environment variables using the `env` command\n\n### 4. Proxy Troubleshooting\n\nIf you're having issues with proxy detection:\n\n1. Use the `debug: true` parameter to see detailed logs about proxy detection\n2. Explicitly specify the proxy using the `proxy` parameter\n3. Ensure your proxy URL is in the correct format: `http://host:port` or `https://host:port`\n4. For websites that require browser capabilities, set `useBrowser: true` to use browser mode\n\n### 5. Browser Mode and Proxies\n\nWhen using browser mode (`useBrowser: true`), the tool will:\n\n1. First try to use the explicitly specified proxy (if provided)\n2. Then try to use system proxy settings\n3. Finally, proceed without a proxy if none is found\n\nBrowser mode is particularly useful for websites that implement anti-scraping measures or require JavaScript execution.\n\n## Parameter Handling\n\nThis project handles parameters in the following ways:\n\n- **debug**: Passed through call parameters, each request can individually control whether to enable debug output\n- **MCP_LANG**: Retrieved from environment variables, controls the language settings of the entire server\n\n## Usage\n\n### Creating a Client\n\n```typescript\nimport { Client } from '@modelcontextprotocol/sdk/client/index.js';\nimport { StdioClientTransport } from '@modelcontextprotocol/sdk/client/stdio.js';\nimport path from 'path';\nimport { fileURLToPath } from 'url';\n\n// Get the directory path of the current file\nconst __filename = fileURLToPath(import.meta.url);\nconst __dirname = path.dirname(__filename);\n\n// Create client transport layer\nconst transport = new StdioClientTransport({\n  command: 'node',\n  args: [path.resolve(__dirname, 'dist/index.js')],\n  stderr: 'inherit',\n  env: {\n    ...process.env  // Pass all environment variables, including MCP_LANG\n  }\n});\n\n// Create client\nconst client = new Client({\n  name: \"example-client\",\n  version: \"1.0.0\"\n});\n\n// Connect to transport layer\nawait client.connect(transport);\n\n// Use client\nconst result = await client.callTool({\n  name: 'fetch_html',\n  arguments: {\n    url: 'https://example.com',\n    debug: true  // Control debug output through parameters\n  }\n});\n\nif (result.isError) {\n  console.error('Fetch failed:', result.content[0].text);\n} else {\n  console.log('Fetch successful!');\n  console.log('Content preview:', result.content[0].text.substring(0, 500));\n}\n```\n\n### Supported Tools\n\n- `fetch_html`: Get HTML content of a webpage\n- `fetch_json`: Get JSON data\n- `fetch_txt`: Get plain text content\n- `fetch_markdown`: Get Markdown formatted content\n- `fetch_plaintext`: Get plain text content converted from HTML (strips HTML tags)\n\n### Resources Support\n\nThe server includes support for the resources/list and resources/read methods, but currently no resources are defined in the implementation. The resource system is designed to provide access to project files and documentation, but this feature is not fully implemented yet.\n\n#### Resource Usage Example\n\n```typescript\n// Example: List available resources\nconst resourcesResult = await client.listResources({});\nconsole.log('Available resources:', resourcesResult);\n\n// Note: Currently this will return empty lists for resources and resourceTemplates\n```\n\n### Supported Prompt Templates\n\nThe server provides the following prompt templates:\n\n- `fetch-website`: Get website content, supporting different formats and browser mode\n- `extract-content`: Extract specific content from a website, supporting CSS selectors and data type specification\n- `debug-fetch`: Debug website fetching issues, analyze possible causes and provide solutions\n\n#### Prompt Template Usage\n\n1. Use `prompts/list` to get a list of available prompt templates\n2. Use `prompts/get` to get specific prompt template content\n\n```typescript\n// Example: List available prompt templates\nconst promptsResult = await client.listPrompts({});\nconsole.log('Available prompts:', promptsResult);\n\n// Example: Get website content prompt\nconst fetchPrompt = await client.getPrompt({\n  name: \"fetch-website\",\n  arguments: {\n    url: \"https://example.com\",\n    format: \"html\",\n    useBrowser: \"false\"\n  }\n});\nconsole.log('Fetch website prompt:', fetchPrompt);\n\n// Example: Debug website fetching issues\nconst debugPrompt = await client.getPrompt({\n  name: \"debug-fetch\",\n  arguments: {\n    url: \"https://example.com\",\n    error: \"Connection timeout\"\n  }\n});\nconsole.log('Debug fetch prompt:', debugPrompt);\n```\n\n### Parameter Options\n\nEach tool supports the following parameters:\n\n#### Basic Parameters\n- `url`: URL to fetch (required)\n- `headers`: Custom request headers (optional, default {})\n- `proxy`: Proxy server URL in the format http://host:port or https://host:port (optional)\n\n#### Network Control Parameters\n- `timeout`: Timeout in milliseconds (optional, default is 30000)\n- `maxRedirects`: Maximum number of redirects to follow (optional, default is 10)\n- `noDelay`: Whether to disable random delay between requests (optional, default is false)\n- `useSystemProxy`: Whether to use system proxy (optional, default is true)\n\n#### Content Size Control Parameters\n- `enableContentSplitting`: Whether to split large content into chunks (optional, default is true)\n- `contentSizeLimit`: Maximum content size in bytes before splitting (optional, default is 50000)\n- `startCursor`: Starting cursor position in bytes for retrieving content from a specific position (optional, default is 0)\n\nThese parameters help manage large content that would exceed AI model context size limits, allowing you to retrieve web content in manageable chunks while maintaining the ability to process the complete information.\n\n#### Chunk Management\n- `chunkId`: Unique identifier for a chunk set when content is split (used for requesting subsequent chunks)\n\nWhen content is split into chunks, the response includes metadata that allows the AI to request subsequent chunks using the `chunkId` and `startCursor` parameters. The system uses byte-level chunk management to provide precise control over content retrieval, enabling seamless processing of content from any position.\n\n#### Mode Control Parameters\n- `useBrowser`: Whether to use browser mode (optional, default is false)\n- `useNodeFetch`: Whether to force using Node.js mode (optional, default is false, mutually exclusive with `useBrowser`)\n- `autoDetectMode`: Whether to automatically detect and switch to browser mode if standard mode fails with 403/Forbidden errors (optional, default is true). Set to false to strictly use the specified mode without automatic switching.\n\n#### Browser Mode Specific Parameters\n- `waitForSelector`: Selector to wait for in browser mode (optional, default is 'body')\n- `waitForTimeout`: Timeout to wait in browser mode in milliseconds (optional, default is 5000)\n- `scrollToBottom`: Whether to scroll to the bottom of the page in browser mode (optional, default is false)\n- `saveCookies`: Whether to save cookies in browser mode (optional, default is true)\n- `closeBrowser`: Whether to close the browser instance (optional, default is false)\n\n#### Content Extraction Parameters\n- `extractContent`: Whether to use the Readability algorithm to extract main content (optional, default false)\n- `includeMetadata`: Whether to include metadata in the extracted content (optional, default false, only works when `extractContent` is true)\n- `fallbackToOriginal`: Whether to fall back to the original content when extraction fails (optional, default true, only works when `extractContent` is true)\n\n#### Debug Parameters\n- `debug`: Whether to enable debug output (optional, default false)\n\n### Content Extraction Feature\n\nUse the content extraction feature to get the core content of a webpage, filtering out navigation bars, advertisements, sidebars, and other distracting elements:\n\n```json\n{\n  \"url\": \"https://example.com/article\",\n  \"extractContent\": true,\n  \"includeMetadata\": true\n}\n```\n\nThe extracted content will include the following metadata (if available):\n- Title\n- Byline (author)\n- Site name\n- Excerpt\n- Content length\n- Readability flag (isReaderable)\n\n### Special Usage\n\n#### Content Extraction Examples\n\nTo extract only the meaningful content from an article webpage:\n\n```json\n{\n  \"url\": \"https://example.com/news/article\",\n  \"extractContent\": true,\n  \"includeMetadata\": true\n}\n```\n\nFor websites where content extraction might fail, you can use `fallbackToOriginal` to ensure you get some content:\n\n```json\n{\n  \"url\": \"https://example.com/complex-layout\",\n  \"extractContent\": true,\n  \"fallbackToOriginal\": true\n}\n```\n\n#### Closing Browser Without Fetching\nTo close the browser instance without performing any fetch operation:\n```json\n{\n  \"url\": \"about:blank\",\n  \"closeBrowser\": true\n}\n```\n\n#### Proxy Priority\nThe proxy is determined in the following order:\n1. Command line specified proxy\n2. `proxy` parameter in the request\n3. Environment variables (if `useSystemProxy` is true)\n4. Git configuration (if `useSystemProxy` is true)\n\nIf `proxy` is set, `useSystemProxy` will be automatically set to false.\n\n### Debug Output\n\nWhen `debug: true` is set, logs will be output to stderr with the following prefixes:\n- `[MCP-SERVER]`: Logs from the MCP server\n- `[NODE-FETCH]`: Logs from the Node.js fetcher\n- `[BROWSER-FETCH]`: Logs from the browser fetcher\n- `[CLIENT]`: Logs from the client\n- `[TOOLS]`: Logs from the tool implementation\n- `[FETCHER]`: Logs from the main fetcher interface\n- `[CONTENT]`: Logs related to content handling\n- `[CONTENT-PROCESSOR]`: Logs from the HTML content processor\n- `[CONTENT-SIZE]`: Logs related to content size management\n- `[CHUNK-MANAGER]`: Logs related to content chunking operations\n- `[ERROR-HANDLER]`: Logs related to error handling\n- `[BROWSER-MANAGER]`: Logs from the browser instance manager\n- `[CONTENT-EXTRACTOR]`: Logs from the content extractor\n\n\n## License\n\nMIT\n\n---\n\nUpdated by lmcc-dev",
      "npm_url": "https://www.npmjs.com/package/mult-fetch-mcp-server",
      "npm_downloads": 347,
      "keywords": [
        "scraping",
        "fetch",
        "web",
        "scraping techniques",
        "search lmcc",
        "intelligent scraping"
      ],
      "category": "web-search"
    },
    "logos-42--ANPtest": {
      "owner": "logos-42",
      "name": "ANPtest",
      "url": "https://github.com/logos-42/ANPtest",
      "imageUrl": "/freedevtools/mcp/pfp/logos-42.webp",
      "description": "Connects to AI agents using self-compressed decentralized identifiers (DIDs). Facilitates interactive conversations with an AI assistant in a talk show style, featuring functionalities for DID generation and QR code display.",
      "stars": 0,
      "forks": 0,
      "license": "No License",
      "language": "JavaScript",
      "updated_at": "2025-04-01T07:36:48Z",
      "readme_content": "# 脱口秀AI智能体\r\n\r\n基于自压缩DID技术，使用硅基流动API实现的脱口秀AI助手。\r\n\r\n## 项目特点\r\n\r\n- 实现自压缩DID，无需额外查询即可连接智能体\r\n- 使用硅基流动的DeepSeek-R1-Distill-Qwen-32B大模型\r\n- 提供生成DID、二维码展示和聊天功能\r\n- 支持智能体间的通信与连接\r\n\r\n## 技术栈\r\n\r\n- Next.js: React框架\r\n- 自压缩DID: 去中心化身份识别\r\n- 硅基流动API: AI大模型服务\r\n- Vercel: 部署服务\r\n\r\n## 快速开始\r\n\r\n### 本地开发\r\n\r\n1. 克隆项目并安装依赖:\r\n\r\n```bash\r\ngit clone <repository-url>\r\ncd comedyagent\r\nnpm install\r\n```\r\n\r\n2. 配置API密钥:\r\n\r\n如需使用不同的API密钥，请修改`lib/ai.js`文件中的`API_KEY`变量。\r\n\r\n3. 启动开发服务器:\r\n\r\n```bash\r\nnpm run dev\r\n```\r\n\r\n4. 访问 http://localhost:3000 查看应用。\r\n\r\n### Vercel部署\r\n\r\n1. Fork此仓库到您的GitHub账户。\r\n\r\n2. 在Vercel上创建新项目，并连接您的GitHub仓库。\r\n\r\n3. 部署完成后，即可通过Vercel提供的URL访问应用。\r\n\r\n## 使用说明\r\n\r\n### 生成DID\r\n\r\n1. 在首页点击\"生成DID\"按钮。\r\n2. 系统会生成一个自包含DID，并以文本和二维码形式显示。\r\n3. 您可以复制DID或分享二维码。\r\n\r\n### 测试智能体\r\n\r\n1. 在首页的聊天框中输入消息。\r\n2. 点击\"发送\"按钮或按Enter键。\r\n3. 智能体会以脱口秀演员的风格回复您的消息。\r\n\r\n### 连接其他智能体\r\n\r\n1. 前往\"/connect\"页面。\r\n2. 输入其他智能体的DID。\r\n3. 点击\"连接\"按钮。\r\n4. 连接成功后，您可以向该智能体发送消息。\r\n\r\n您还可以通过以下方式连接智能体:\r\n\r\n- 在浏览器中打开`{您的域名}/connect?did={DID字符串}`\r\n- 或者创建一个`did://`协议链接: `did://{DID字符串}`\r\n\r\n## 项目结构\r\n\r\n```\r\ncomedyagent/\r\n├── api/                  # API路由\r\n│   ├── generate-did.js   # DID生成API\r\n│   └── message.js        # 消息处理API\r\n├── components/           # React组件\r\n├── lib/                  # 工具库\r\n│   ├── ai.js             # AI服务\r\n│   └── did.js            # DID功能\r\n├── pages/                # 页面\r\n│   ├── index.js          # 首页\r\n│   └── connect.js        # 连接页面\r\n├── public/               # 静态资源\r\n├── styles/               # 样式文件\r\n│   ├── Home.module.css   # 首页样式\r\n│   └── Connect.module.css# 连接页面样式\r\n├── package.json          # 项目配置\r\n└── vercel.json           # Vercel配置\r\n```\r\n\r\n## 自压缩DID详解\r\n\r\n本项目中的自压缩DID是一种创新的数字身份表示方式，包含了以下信息:\r\n\r\n- 身份标识\r\n- 公钥\r\n- 服务端点\r\n- 元数据\r\n- 数字签名\r\n\r\n与传统DID不同，自压缩DID将所有必要信息编码在一个字符串中，无需查询额外服务器即可获取身份信息和通信方式。\r\n\r\n## 许可证\r\n\r\nMIT ",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "ai",
        "agents",
        "code",
        "ai agents",
        "connects ai",
        "ai assistant"
      ],
      "category": "web-search"
    },
    "longyi1207--glean-mcp-server": {
      "owner": "longyi1207",
      "name": "glean-mcp-server",
      "url": "https://github.com/longyi1207/glean-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/longyi1207.webp",
      "description": "Integrates the Glean API to provide functionalities for searching and interacting with a chatbot. Offers capabilities for retrieving search results based on queries and conducting question-and-answer sessions with a chatbot interface.",
      "stars": 8,
      "forks": 5,
      "license": "No License",
      "language": "JavaScript",
      "updated_at": "2025-07-08T00:46:09Z",
      "readme_content": "# Glean\n\nAn MCP server implementation that integrates the Glean API, providing the Search and Chat functions.\n\n## Tools\n- **Search**: List of search results given a query\n- **Chat**: Q&A with Chatbot\n\n### Usage with Claude Desktop\nBuild the docker image:\n```bash\ndocker build -t glean-server:latest -f src/glean/Dockerfile .\n```\n\nThen add this to your `claude_desktop_config.json`:\n```\n{\n  \"mcpServers\": {\n    \"glean-server\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"-i\",\n        \"--rm\",\n        \"-e\",\n        \"GLEAN_API_KEY\",\n        \"-e\",\n        \"GLEAN_DOMAIN\",\n        \"glean-server\"\n      ],\n      \"env\": {\n        \"GLEAN_API_KEY\": \"YOUR_API_KEY_HERE\",\n        \"GLEAN_DOMAIN\": \"YOUR_DOMAIN_HERE\"\n      }\n    }\n  }\n}\n```\n\n## License\n\nThis MCP server is licensed under the MIT License. This means you are free to use, modify, and distribute the software, subject to the terms and conditions of the MIT License. For more details, please see the LICENSE file in the project repository.\n",
      "npm_url": "https://www.npmjs.com/package/glean-mcp-server",
      "npm_downloads": 5,
      "keywords": [
        "chatbot",
        "searching",
        "search",
        "chatbot interface",
        "interacting chatbot",
        "glean api"
      ],
      "category": "web-search"
    },
    "lroolle--openai-agents-mcp-server": {
      "owner": "lroolle",
      "name": "openai-agents-mcp-server",
      "url": "https://github.com/lroolle/openai-agents-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/lroolle.webp",
      "description": "Connects OpenAI agents to various tools and data sources, enabling specialized tasks such as web searching, file analysis, and computer actions. Supports coordination through a multi-agent orchestrator to enhance AI functionalities.",
      "stars": 9,
      "forks": 2,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-07-27T09:22:42Z",
      "readme_content": "# OpenAI Agents MCP Server\n[![smithery badge](https://smithery.ai/badge/@lroolle/openai-agents-mcp-server)](https://smithery.ai/server/@lroolle/openai-agents-mcp-server)\n\nA Model Context Protocol (MCP) server that exposes OpenAI agents through the MCP protocol.\n\n## Features\n\nThis server exposes both individual agents and a multi-agent orchestrator using the OpenAI Agents SDK:\n\n### Individual Specialized Agents\n\n- **Web Search Agent**: A specialized agent for searching the web for real-time information\n- **File Search Agent**: A specialized agent for searching and analyzing files in OpenAI's vector store\n- **Computer Action Agent**: A specialized agent for performing actions on your computer safely\n\n### Multi-Agent Orchestrator\n\n- **Orchestrator Agent**: A powerful agent that can coordinate between the specialized agents, choosing the right one(s) for each task\n\nEach agent is accessed through the MCP protocol, making them available to any MCP client, including the Claude desktop app.\n\n## Installation\n\n### Prerequisites\n\n- Python 3.11 or higher\n- [uv](https://github.com/astral-sh/uv) package manager (recommended)\n- OpenAI API key\n\n\n### Installing via Smithery\n\nTo install openai-agents-mcp-server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@lroolle/openai-agents-mcp-server):\n\n```bash\nnpx -y @smithery/cli install @lroolle/openai-agents-mcp-server --client claude\n```\n\n### Claude Desktop\n\n```\n\"mcpServers\": {\n  \"openai-agents-mcp-server\": {\n    \"command\": \"uvx\",\n    \"args\": [\"openai-agents-mcp-server\"],\n    \"env\": {\n        \"OPENAI_API_KEY\": \"your-api-key-here\"\n    }\n  }\n}\n\n```\n\n\n## Implementation Details\n\n### Tool Requirements\n\n- **WebSearchTool**: No required parameters, but can accept optional location context\n- **FileSearchTool**: Requires vector_store_ids (IDs from your OpenAI vector stores)\n- **ComputerTool**: Requires an AsyncComputer implementation (currently simulated)\n\n### Customization\n\nYou can customize this server by:\n\n1. Implementing a full AsyncComputer interface to enable real computer interactions\n2. Adding additional specialized agents for other OpenAI tools\n3. Enhancing the orchestrator agent to handle more complex workflows\n\n## Configuration\n\nYou can configure the server using environment variables:\n\n- `OPENAI_API_KEY`: Your OpenAI API key (required)\n- `MCP_TRANSPORT`: Transport protocol to use (default: \"stdio\", can be \"sse\")\n\n## Development\n\n### Setup development environment\n\n```bash\n# Clone the repository\ngit clone https://github.com/lroolle/openai-agents-mcp-server.git\ncd openai-agents-mcp-server\n\n# Create a virtual environment\nuv venv\nsource .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\n\n# Install dependencies\nuv sync --dev\n```\n\n### Testing with MCP Inspector\n\nYou can test the server using the MCP Inspector:\n\n```bash\n# In one terminal, run the server with SSE transport\nexport OPENAI_API_KEY=your-api-key\nexport MCP_TRANSPORT=sse\n\nuv run mcp dev src/agents_mcp_server/server.py\n```\n\nThen open a web browser and navigate to http://localhost:5173.\n\n## License\n\nMIT\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "openai",
        "agents",
        "agent",
        "openai agents",
        "connects openai",
        "multi agent"
      ],
      "category": "web-search"
    },
    "lsd-so--lsd-mcp": {
      "owner": "lsd-so",
      "name": "lsd-mcp",
      "url": "https://github.com/lsd-so/lsd-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/lsd-so.webp",
      "description": "Connects Claude to the internet to gather high-quality information from websites using LSD SQL. Enables complex queries and actions while allowing Claude to self-correct SQL commands and interact with cloud browsers.",
      "stars": 64,
      "forks": 10,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-06-15T20:04:07Z",
      "readme_content": "# LSD MCP server\n\n\n\nImmediately gather an aggregation of high quality info directly from a website just by giving LSD the link via Claude MCP.\n\nYou will see Claude connect to the internet and:\n* Write LSD SQL\n* Self-correct LSD SQL\n* Run LSD SQL that's connected to cloud browsers\n\n## Demo\n\nHere's a demo of what that looks like in action:\n\n\n\nWe treated Claude to psychedelic therapy on LSD and now it can just do things. [Here's a longer video on YouTube](https://youtu.be/s97G-E46-Yo)\n\n## Contents\n\n* [Quickstart](#quickstart)\n  * [Dependencies](#dependencies)\n  * [Giving Claude LSD](#giving-claude-lsd)\n  * [Claude on LSD](#claude-on-lsd)\n  * [Failed to start MCP server](#failed-to-start-mcp-server)\n\t* [First time running an MCP server](#first-time-running-an-mcp-server)\n\t* [Missing executable](#missing-executable)\n\t* [Incomplete path](#incomplete-path)\n* [What is MCP?](#what-is-mcp)\n* [What is LSD?](#what-is-lsd)\n  * [Contact](#contact)\n* [Smithery](#smithery)\n  * [Installing via Smithery](#installing-via-smithery)\n\n## Quickstart\n\n### Dependencies\n\nTo run the MCP server, you'll need both [Python](https://www.python.org/) and [uv](https://docs.astral.sh/uv/) installed. To use the MCP server, you'll need to download either the [Claude desktop app](https://claude.ai/download) or [another MCP client](https://modelcontextprotocol.io/clients).\n\nTo use LSD, you'll need to sign up and [create an API key](https://lsd.so/profile) so your queries are privately associated to only your account. You can do so [for free with a Google account](https://lsd.so/connect).\n\n### Giving Claude LSD\n\n1. Clone this repository onto your computer\n\n```\n$ git clone https://github.com/lsd-so/lsd-mcp.git\n$ cd lsd-mcp\n```\n\n2. Update the values in the `.env` file with `LSD_USER` containing the email you have an account on LSD with and `LSD_API_KEY` containing the API key you obtained from the profile page.\n\n```\nLSD_USER=<your_email_here>\nLSD_API_KEY=<api_key_from_your_profile_page>\n```\n\n3. Give LSD to Claude\n\n```\n$ uv run mcp install app.py\n```\n\n**Note:** Every time you run `mcp install`, if you needed to update `claude_desktop_config.json` [the first time](#first-time-running-an-mcp-server), you will need to remember to update the path to `uv` each time you install the MCP server.\n\n4. Restart the Claude desktop app and, now, Claude should be able to do trippy things on LSD.\n\n### Claude on LSD\n\nIf it's the first time in a chat session where you'd like to have Claude use LSD, because we're not popular enough to get caught in Anthropic's crawls, you'll need to first leverage our custom prompt which feeds in our documentation as part of the assistance.\n\n\n\nSee the [`write_lsd_sql` function](https://github.com/lsd-so/lsd-mcp/blob/main/app.py#L48) if you're interested in how it works but it just boils down to a [convenient rule we added to our SCAN keyword](https://lsd.so/docs/database/language/keywords/scan#example) enabling a developer or LLM to retrieve the documentation for our language in markdown ([if you'd like to run it yourself](https://lsd.so/app?query=SCAN%20https%3A%2F%2Flsd.so%2Fdocs)).\n\n```\nSCAN https://lsd.so/docs/database/language\n```\n\n### Failed to start MCP server\n\n\n\nIf you encounter error messages when starting Claude desktop along the lines of the following message:\n\n```\nFailed to start MCP server: Could not start MCP server LSD: Error: spawn uv ENOENT\n```\n\n#### First time running an MCP server\n\nIf this is your first time using an MCP server on your computer than, to remedy the error shown above, follow the instructions [under the **Add the Filesystem MCP Server** step](https://modelcontextprotocol.io/quickstart/user#2-add-the-filesystem-mcp-server) to create a `claude_desktop_config.json` file that Claude desktop can know to refer to.\n\n#### Missing executable\n\nAdditionally, if you've never done anything relating to [Postgres](https://www.postgresql.org/) on your computer, then you may encounter an error message containing something like the following:\n\n```\nError: pg_config executable not found.\n```\n\nTo fix, simply install `postgres` to your machine using an available package manager. If you're on a Mac, you can do so [using brew](https://wiki.postgresql.org/wiki/Homebrew).\n\n```\n$ brew install postgres\n```\n\n#### Incomplete path\n\nOtherwise and maybe in addition to the issue shown above, in the location [where `claude_desktop_config.json` is stored](https://modelcontextprotocol.io/quickstart/user#2-add-the-filesystem-mcp-server) (is `~/Library/Application Support/Claude/claude_desktop_config.json` if you're running on a Mac), modify the value of the `command` key under `mcpServers -> LSD` to contain the full path to running `uv` (run `which uv` in your terminal if you don't already know what it is).\n\n```diff\n{\n  \"mcpServers\": {\n    \"LSD\": {\n-      \"command\": \"uv\",\n+      \"command\": \"/Users/your_mac_name/.local/bin/uv\",\n      \"args\": [\n        \"run\",\n        \"--with\",\n        \"mcp[cli]\",\n        \"--with\",\n        \"psycopg2-binary\",\n        \"mcp\",\n        \"run\",\n        \"/Users/y/testing-mcp/lsd-mcp/app.py\"\n      ]\n    }\n  }\n}\n```\n\nOnce you've done that, restart Claude desktop and the problem should be resolved. If not, please [file an issue](https://github.com/lsd-so/lsd-mcp/issues/new?template=Blank+issue).\n\n## What is MCP?\n\nMCP, short for [model context protocol](https://modelcontextprotocol.io/introduction), provides a communication layer between [Claude](https://claude.ai) and computer-accessible interfaces such as [the filesystem](https://github.com/modelcontextprotocol/servers/tree/main/src/filesystem) or [web APIs](https://github.com/modelcontextprotocol/servers/tree/main/src/slack). If a limiting factor of LLMs was its detachment from the \"real world\" since it's just a text generating model, MCP allows users and developers to bring Claude to life.\n\n## What is LSD?\n\nLSD SQL, a [DSL](https://en.wikipedia.org/wiki/Domain-specific_language) for the web, enables developers to connect the internet to your applications as though it were a [postgres compatible database](https://lsd.so/docs/database/postgres). Rather than present [a new semantic web ontology](https://xkcd.com/927/) or [make a new internet](https://urbit.org/), it provides a dynamic declarative language that sits atop the existing one.\n\nDesigned to target browsers instead of [an architecture](https://llvm.org/), LSD allows for [powerful parallelization](https://lsd.so/docs/database/language/keywords/dive#example) while preserving simplicity with just-in-time tables meaning you can just get data without running a CREATE TABLE beforehand. [Sign up for free with a Google account](https://lsd.so/connect) to start querying the internet! \n\n[Here's an example of something you can do with LSD, takes ~30 sec if first run](https://lsd.so/app?query=calculators%20%3C%7C%20https%3A%2F%2Fwww.smooth-on.com%2Fsupport%2Fcalculators%2F%20%7C%0Apour_on_mold%20%3C%7C%20div%5Bdata-calcid%3D%22pour-mold%22%5D%20%7C%0Aproduct_dropdown%20%3C%7C%20%23pour-prod%20%7C%0Adropdown_value%20%3C%7C%20%2224.7%22%20%7C%0Amodel_volume_input%20%3C%7C%20%23pour-model-volume%20%7C%0Amodel_volume%20%3C%7C%20%2212%22%20%7C%0Abox_volume_input%20%3C%7C%20%23pour-box-volume%20%7C%0Abox_volume%20%3C%7C%20%2220%22%20%7C%0Acalculate_button%20%3C%7C%20%23pour-calculate%20%7C%0Aestimate%20%3C%7C%20%23pour-results%20%7C%0A%0AFROM%20calculators%0A%7C%3E%20CLICK%20ON%20pour_on_mold%0A%7C%3E%20CHOOSE%20IN%20product_dropdown%20dropdown_value%0A%7C%3E%20ENTER%20INTO%20model_volume_input%20model_volume%0A%7C%3E%20ENTER%20INTO%20box_volume_input%20box_volume%0A%7C%3E%20CLICK%20ON%20calculate_button%0A%7C%3E%20SELECT%20estimate)\n\n### Contact\n\nReach out to pranav at lsd dot so if you have any questions.\n\n## Smithery\n\n[![smithery badge](https://smithery.ai/badge/@lsd-so/lsd-mcp)](https://smithery.ai/server/@lsd-so/lsd-mcp)\n\n### Installing via Smithery\n\nTo install LSD MCP Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@lsd-so/lsd-mcp):\n\n```bash\nnpx -y @smithery/cli install @lsd-so/lsd-mcp --client claude\n```",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "lsd",
        "queries",
        "web",
        "lsd sql",
        "search lsd",
        "lsd mcp"
      ],
      "category": "web-search"
    },
    "luebken--playlist-mcp": {
      "owner": "luebken",
      "name": "playlist-mcp",
      "url": "https://github.com/luebken/playlist-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/luebken.webp",
      "description": "Provides access to transcripts of YouTube playlists, enabling retrieval and querying of video transcripts for enhanced application functionalities. It can preload specific playlists or be customized by specifying a different playlist URL.",
      "stars": 1,
      "forks": 1,
      "license": "Apache License 2.0",
      "language": "Python",
      "updated_at": "2025-04-16T10:31:43Z",
      "readme_content": "# Playlist-MCP\n\nThis is an experimental MCP server, which makes the transcripts of a Youtube Playlist available.\n\nIt currently is preloaded with the KubeCon London 2025 transcripts. But you can change this by changing the URL.\n\n\n\n## Install\n\n```sh\n# Clone this repo\ngit clone git@github.com:luebken/playlist-mcp.git; cd playlist-mcp\n\n# Install python dependencies\nuv venv\nsource .venv/bin/activate\nuv pip install -e .\n\n# Fill the transcript cache and vector db.\nuv run server.py https://www.youtube.com/playlist?list=PLj6h78yzYM2MP0QhYFK8HOb8UqgbIkLMc\n```\n\n## Setup for Claude Desktop\n\n```sh\n# Configure for Claude > Settings > Developer > Edit Config\n# /Users/YOUR_USERNAME/Library/Application Support/Claude/claude_desktop_config.json\n{\n  \"mcpServers\": {\n      \"playlist-mcp\": {\n          \"command\": \"uv\",\n          \"args\": [\n              \"--directory\",\n              \"/PATH/TO/PARENT/playlist-mcp/\",\n              \"run\",\n              \"server.py\",\n              \"https://www.youtube.com/playlist?list=PLj6h78yzYM2MP0QhYFK8HOb8UqgbIkLMc\"\n          ]\n      }\n  }\n}\n```",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "playlists",
        "playlist",
        "youtube",
        "luebken playlist",
        "youtube playlists",
        "transcripts youtube"
      ],
      "category": "web-search"
    },
    "maccam912--searxng-mcp-server": {
      "owner": "maccam912",
      "name": "searxng-mcp-server",
      "url": "https://github.com/maccam912/searxng-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/maccam912.webp",
      "description": "Interact with SearXNG instances to perform search operations across multiple engines and sources.",
      "stars": 1,
      "forks": 4,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-05-19T19:19:25Z",
      "readme_content": "# SearXNG MCP Server\n\nA [Model Control Protocol (MCP)](https://github.com/microsoft/modelcontrol) server for [SearXNG](https://github.com/searxng/searxng), allowing AI assistants to search the web through a SearXNG instance.\n\n## Features\n\n- Search the web using SearXNG via simple API calls\n- Get information about available search engines\n- Configure search parameters including categories, languages, and safe search settings\n\n## Usage in MCP Configuration\n\n```json\n \"searxng-mcp-server\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"run\",\n        \"https://raw.githubusercontent.com/maccam912/searxng-mcp-server/refs/heads/main/server.py\",\n        \"--url\",\n        \"https://searxng.example.com\"\n      ]\n    }\n```\n\n## Docker Usage\n\nYou can run this MCP server using Docker:\n\n```bash\n# Build the Docker image\ndocker build -t searxng-mcp-server .\n\n# Run the container\n# Replace https://searxng.example.com with your actual SearXNG instance URL\ndocker run -p 8080:8080 searxng-mcp-server\n```\n\n## API Tools\n\n### Search\n\n```python\ndef search(query: str, categories: Optional[str] = None, engines: Optional[str] = None, \n           language: Optional[str] = None, page: int = 1, time_range: Optional[str] = None,\n           safe_search: int = 1)\n```\n\n### Get Available Engines\n\n```python\ndef get_available_engines()\n```\n\n## Local Development\n\nRequirements:\n- Python 3.11+\n- uv (Python package manager)\n\n```bash\n# Install dependencies\nuv sync\n\n# Run the server\nuv run server.py --url https://searxng.example.com\n```\r\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "searxng",
        "maccam912",
        "search",
        "searxng instances",
        "searxng mcp",
        "maccam912 searxng"
      ],
      "category": "web-search"
    },
    "madarco--ragrabbit": {
      "owner": "madarco",
      "name": "ragrabbit",
      "url": "https://github.com/madarco/ragrabbit",
      "imageUrl": "/freedevtools/mcp/pfp/madarco.webp",
      "description": "Crawl websites to create AI-powered search capabilities that enhance content discoverability and interaction with AI language models.",
      "stars": 124,
      "forks": 17,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-10-01T07:17:57Z",
      "readme_content": "<h1 style=\"font-weight:normal\">\n  <a href=\"https://ragrabbit.com\">\n    \n  </a>\n  &nbsp;RagRabbit&nbsp;\n  <a href=\"https://vercel.com/new/clone?repository-url=https%3A%2F%2Fgithub.com%2Fmadarco%2Fragrabbit&env=OPENAI_API_KEY,AUTH_USERNAME,AUTH_PASSWORD,AUTH_SECRET&envDescription=Get%20an%20OpenAI%20Api%20Key%20and%20set%20AUTH_USERNAME%20and%20AUTH_PASSWORD%20to%20the%20desired%20credentials%20to%20secure%20the%20admin%20section.%20Also%20be%20sure%20to%20enable%20the%20Postgres%20database%20integration&envLink=https%3A%2F%2Fplatform.openai.com%2Fapi-keys&demo-title=RagRabbit%20-%20AI%20Site%20Search%20and%20LLM.txt&demo-description=Site%20AI%20Search%20and%20LLM.txt%20in%20Minutes%2C%20Open%20Source%20with%201%20Click%20Deploy%20on%20Vercel.&demo-url=https%3A%2F%2Fragrabbit.vercel.app%2F&demo-image=https%3A%2F%2Fragrabbit.vercel.app%2Fopengraph-image.png&stores=%5B%7B%22type%22%3A%22postgres%22%7D%5D&root-directory=apps/saas\"><img alt=\"deploy_on_vercel_black\" src=\"https://img.shields.io/badge/deploy%20on-vercel-black.svg\"></a>\n  <a href=\"https://github.com/madarco/ragrabbit/blob/master/license.md\"><img src=https://img.shields.io/github/license/madarco/ragrabbit.svg?colorB=ff0000></a>\n  <a href=\"https://www.npmjs.com/package/@ragrabbit/mcp\"><img alt=\"ragrabbit_mcp_label_npm\" src=\"https://img.shields.io/npm/d18m/%40ragrabbit%2Fmcp?label=npm\" /></a>\n  <img alt=\"ragrabbit\" src=\"https://img.shields.io/github/stars/madarco/ragrabbit\" />\n</h1>\n\nSelf Hosted Site AI Search, LLMs.txt, MCP Server that crawls your content. 1-Click Deploy on Vercel.\n<br>\n\n<p align=\"center\">\n  \n\n</p>\n\n[![Deploy with Vercel](https://vercel.com/button)](https://vercel.com/new/clone?repository-url=https%3A%2F%2Fgithub.com%2Fmadarco%2Fragrabbit&env=OPENAI_API_KEY,AUTH_USERNAME,AUTH_PASSWORD,AUTH_SECRET&envDescription=Get%20an%20OpenAI%20Api%20Key%20and%20set%20AUTH_USERNAME%20and%20AUTH_PASSWORD%20to%20the%20desired%20credentials%20to%20secure%20the%20admin%20section.%20Also%20be%20sure%20to%20enable%20the%20Postgres%20database%20integration&envLink=https%3A%2F%2Fplatform.openai.com%2Fapi-keys&demo-title=RagRabbit%20-%20AI%20Site%20Search%20and%20LLM.txt&demo-description=Site%20AI%20Search%20and%20LLM.txt%20in%20Minutes%2C%20Open%20Source%20with%201%20Click%20Deploy%20on%20Vercel.&demo-url=https%3A%2F%2Fragrabbit.vercel.app%2F&demo-image=https%3A%2F%2Fragrabbit.vercel.app%2Fopengraph-image.png&stores=%5B%7B%22type%22%3A%22postgres%22%7D%5D&root-directory=apps/saas)\n\n## How it works\n\n[RagRabbit](https://github.com/madarco/ragrabbit) is a [Next.js](https://nextjs.org/) [Turborepo](https://turbo.build/repo) app that uses [Llamaindex](https://github.com/run-llama/LlamaIndexTS) with [pgVector](https://github.com/pgvector/pgvector).\n\nFeatures\n\n- 💬 Chat Widget: Embeddable AI Chat agent and instant Search\n- 🕸️ Website Crawler: scrapes and index pages with pgVector and PostgreSQL\n- 📄 LLMs.txt Generation: fully customizable wiht ToC reorder\n- 🔌 [MCP Server](./packages//mcp-server/README.md): `npx @ragrabbit/mcp` to access your docs from Claude Desktop and Cursor IDE\n- 🛠️ Flexible: Authentication, Open Source, API Keys access\n- 🚀 Easy Deployment: One-click setup on Vercel\n\nIntegrations:\n\n- [Fumadocs](#fumadocs)\n\n### Demo\n\nView [RagRabbit Demo Page](https://ragrabbit.vercel.app/widget/demo)\n\n\n\n## Install\n\nTo install on Vercel:\n\n[![Deploy with Vercel](https://vercel.com/button)](https://vercel.com/new/clone?repository-url=https%3A%2F%2Fgithub.com%2Fmadarco%2Fragrabbit&env=OPENAI_API_KEY,AUTH_USERNAME,AUTH_PASSWORD,AUTH_SECRET&envDescription=Get%20an%20OpenAI%20Api%20Key%20and%20set%20AUTH_USERNAME%20and%20AUTH_PASSWORD%20to%20the%20desired%20credentials%20to%20secure%20the%20admin%20section.%20Also%20be%20sure%20to%20enable%20the%20Postgres%20database%20integration&envLink=https%3A%2F%2Fplatform.openai.com%2Fapi-keys&demo-title=RagRabbit%20-%20AI%20Site%20Search%20and%20LLM.txt&demo-description=Site%20AI%20Search%20and%20LLM.txt%20in%20Minutes%2C%20Open%20Source%20with%201%20Click%20Deploy%20on%20Vercel.&demo-url=https%3A%2F%2Fragrabbit.vercel.app%2F&demo-image=https%3A%2F%2Fragrabbit.vercel.app%2Fopengraph-image.png&stores=%5B%7B%22type%22%3A%22postgres%22%7D%5D&root-directory=apps/saas)\n\nRequirements:\n\n- Node.js 20.x\n- PostgreSQL w/ pgVector\n- OpenAI API Key\n- (Optional) Trigger.dev API Key\n\n### Configuration\n\nSet the following environment variables:\n\n- OPENAI_API_KEY\n\nFor username/password login:\n\n- ADMIN_USER\n- ADMIN_PASSWORD\n\nFor email login:\n\n- RESEND_AUTH=true\n- To restrict access to those emails: RESEND_ALLOWED_EMAILS=\"test@test.com,foo@bar.com\"\n- To not send emails but logs the login link instead (in Vercel logs): SIMULATE_EMAILS=true\n\nSee [.env.example](./apps/saas/.env.example) for the complete list.\n\n## How to use\n\nUse the Indexing section to add a new url/website to index, either a single url or a website to crawl recursively:\n\n\n\n\nThen start the Job Runner (keep the tab open until it finish)\n\n\n\nIn the LLM.txt section you can preview the generated LLM.txt file:\n\n\n\nYou can then embed the widget in your site with the following snippet:\n\n### Chat Button\n\nEmbed a button at the bottom of your page:\n\n```\n<script src=\"https://<your deployed app>/widget.js\"></script>\n```\n\n\n\n### Chat Widget\n\nInsert a search input anwhere in your page:\n\n\n\n```\n<script src=\"https://ragrabbit.com/widget.js?type=search\"></script>\n<ragrabbit-search></ragrabbit-search>\n```\n\n### To use with React.js\n\n```typescript\n\"use client\";\n\nimport Script from \"next/script\";\n\nexport function RagRabbitSearch() {\n  return (\n    <>\n      <Script src=\"/widget.js?type=search\" strategy=\"lazyOnload\" />\n      <style>{`\n        ragrabbit-search .ragrabbit-search-input {\n            padding: 6px 12px;\n        }\n      `}</style>\n      <div className=\"ml-auto min-w-[300px] flex-1 sm:flex-initial\">\n        {/* @ts-ignore - Custom element will be mounted by external script */}\n        <ragrabbit-search></ragrabbit-search>\n      </div>\n    </>\n  );\n}\n```\n\n### MPC Server\n\nThe MCP Server allows any supported AI Clients to retrieve pages from your documentation using semantic search.\n\n### Claude Desktop\n\nAdd a custom mcp server with the name of your product, so that Claude AI can use it when looking for info about it.\n\nin `claude_desktop_config.json` (Claude -> Settings -> Developer -> Edit Config)\n\n```\n{\n  \"mcpServers\": {\n    \"<name_of_your_documentation_no_spaces>\": {\n      \"command\": \"npx\",\n      \"args\": [\"@ragrabbit/mcp\", \"http://<RagRabbit install>/\", \"<name of your documentation>\"]\n    }\n  }\n}\n```\n\n### In Cursor IDE\n\nGo to Cursor -> Settings -> Cursor Settings -> MCP\n\nAnd add a new MCP of type `command` with the command:\n\n```\nnpx @ragrabbit/mcp\", \"http://<RagRabbit install>/\", \"<name of your documentation>\"\n```\n\nArguments:\n\n- `ragrabbit-url`: (Required) The base URL of your RagRabbit instance, eg https://my-ragrabbit.vercel.com/\n- `name`: (Required) Custom name for the documentation search service (defaults to \"RagRabbit\") so that AI will know to use it when looking for info\n\n## Configuration Options\n\n### Chat button\n\nYou can configure the chat button by adding the following parameters to the widget.js script tag:\n\n#### buttonText\n\n```\n<script src=\"https://ragrabbit.com/widget.js?buttonText=Ask%20AI\"></script>\n```\n\n### Search widget\n\nYou can configure the search widget by adding the following parameters and use the mountSearch call:\n\n#### searchPlaceholder\n\n```\n<div id=\"search-container\"></div>\n<script>\n  window.mountSearch(\"search-container\", { searchPlaceholder: \"Search documentation...\" });\n</script>\n```\n\n## Integrations\n\n### Fumadocs\n\nCreate a component to replace the Search Dialog:\n\n```bash\npnpm add @ragrabbit/search-react\n```\n\n```typescript\n\"use client\";\nimport type { SharedProps } from \"fumadocs-ui/components/dialog/search\";\nimport { RagRabbitModal } from \"@ragrabbit/search-react\";\n\nexport default function SearchDialog({ open, onOpenChange }: SharedProps) {\n  return <RagRabbitModal\n    domain=\"http://localhost:3000/\"\n    open={open}\n    onOpenChange={onOpenChange}\n    />;\n}\n```\n\nThen set it in the `layout.tsx`:\n\n```tsx\n<RootProvider\n  search={{\n    SearchDialog,\n  }}\n>\n  ...\n</RootProvider>\n```\n\nOptionally add the Floating Chat button:\n\n```typescript\n\"use client\";\nimport { RagRabbitChatButton } from \"@ragrabbit/search-react\";\n\nexport default function ChatButton() {\n  return <RagRabbitChatButton domain=\"http://localhost:3000/\" />;\n}\n```\n\nAnd add it to the `layout.tsx`:\n\n```tsx\n<body className=\"flex flex-col min-h-screen\">\n  <ChatButton />\n  ...\n```\n\n## Development\n\n```bash\n# Start the db (Docker needed)\npnpm dev:utils # Starts postgresql with pgvector, Storybook and Drizzle ORM Studio\n\n# Start the app\ncd apps/saas\npnpm dev\n```\n\n### Directory structure:\n\nRagRabbit is a monorepo with Turborepo a Next.js app and a modular design with separate packages.\n\n```\napps/\n├── docs -> the documentation site\n├── saas -> the main application\n└── web -> the web site\npackages/\n├── db -> the database with Drizzle ORM\n├── auth -> the authentication with Auth.js\n├── core -> shared utils\n├── design -> the design system\n├── rag -> the LLM and RAG package with LlamaIndexTS\n├── jobs -> job runner with Trigger.dev\n└── storybook -> a Next.js Storybook app\n.cursorrules -> Fine tuned Cursor rules with all the locations to work with the monorepo\n```\n\n# Author\n\n[Marco D'Alia](https://www.madarco.net) - [@madarco](https://x.com/madarco) - [Linkedin](https://www.linkedin.com/in/marcodalia/)\n\n# License\n\nMIT",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "search",
        "ai",
        "crawl",
        "search capabilities",
        "powered search",
        "web search"
      ],
      "category": "web-search"
    },
    "magicuidesign--mcp": {
      "owner": "magicuidesign",
      "name": "mcp",
      "url": "https://github.com/magicuidesign/mcp",
      "imageUrl": "/freedevtools/mcp/pfp/magicuidesign.webp",
      "description": "Provides Magic UI components, device mocks, special effects, animations, and backgrounds to facilitate UI development. Allows integration with various MCP clients for accessing comprehensive implementation details of numerous UI elements.",
      "stars": 149,
      "forks": 20,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-10-02T09:34:18Z",
      "readme_content": "# @magicuidesign/mcp\n\n[![npm version](https://badge.fury.io/js/@magicuidesign%2Fmcp.svg?icon=si%3Anpm)](https://badge.fury.io/js/@magicuidesign%2Fmcp)\n\nOfficial ModelContextProtocol (MCP) server for [Magic UI](https://magicui.design/).\n\n<div align=\"center\">\n  <img src=\"https://github.com/magicuidesign/mcp/blob/main/public/mcp.png\" alt=\"MCP\" />\n</div>\n\n## Install MCP configuration\n\n```bash\nnpx @magicuidesign/cli@latest install <client>\n```\n\n### Supported Clients\n\n- [x] cursor\n- [x] windsurf\n- [x] claude\n- [x] cline\n- [x] roo-cline\n\n## Manual Installation\n\nAdd to your IDE's MCP config:\n\n```json\n{\n  \"mcpServers\": {\n    \"@magicuidesign/mcp\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@magicuidesign/mcp@latest\"]\n    }\n  }\n}\n```\n\n## Example Usage\n\nOnce configured, you can questions like:\n\n> \"Make a marquee of logos\"\n\n> \"Add a blur fade text animation\"\n\n> \"Add a grid background\"\n\n## Available Tools\n\nThe server provides the following tools callable via MCP:\n\n| Tool Name       | Description                                                                                                                                                                                                                                                                                                                                                                                             |\n|-----------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| `getUIComponents` | Provides a comprehensive list of all Magic UI components.                                                                                                                                                                                                                                                                                                                                                     |\n| `getLayout`       | Provides implementation details for [bento-grid](https://magicui.design/docs/components/bento-grid), [dock](https://magicui.design/docs/components/dock), [file-tree](https://magicui.design/docs/components/file-tree), [grid-pattern](https://magicui.design/docs/components/grid-pattern), [interactive-grid-pattern](https://magicui.design/docs/components/interactive-grid-pattern), [dot-pattern](https://magicui.design/docs/components/dot-pattern) components.         |\n| `getMedia`        | Provides implementation details for [hero-video-dialog](https://magicui.design/docs/components/hero-video-dialog), [terminal](https://magicui.design/docs/components/terminal), [marquee](https://magicui.design/docs/components/marquee), [script-copy-btn](https://magicui.design/docs/components/script-copy-btn), [code-comparison](https://magicui.design/docs/components/code-comparison) components.                                                               |\n| `getMotion`       | Provides implementation details for [blur-fade](https://magicui.design/docs/components/blur-fade), [scroll-progress](https://magicui.design/docs/components/scroll-progress), [scroll-based-velocity](https://magicui.design/docs/components/scroll-based-velocity), [orbiting-circles](https://magicui.design/docs/components/orbiting-circles), [animated-circular-progress-bar](https://magicui.design/docs/components/animated-circular-progress-bar) components. |\n| `getTextReveal`   | Provides implementation details for [text-animate](https://magicui.design/docs/components/text-animate), [line-shadow-text](https://magicui.design/docs/components/line-shadow-text), [aurora-text](https://magicui.design/docs/components/aurora-text), [animated-shiny-text](https://magicui.design/docs/components/animated-shiny-text), [animated-gradient-text](https://magicui.design/docs/components/animated-gradient-text), [text-reveal](https://magicui.design/docs/components/text-reveal), [typing-animation](https://magicui.design/docs/components/typing-animation), [box-reveal](https://magicui.design/docs/components/box-reveal), [number-ticker](https://magicui.design/docs/components/number-ticker) components. |\n| `getTextEffects`  | Provides implementation details for [word-rotate](https://magicui.design/docs/components/word-rotate), [flip-text](https://magicui.design/docs/components/flip-text), [hyper-text](https://magicui.design/docs/components/hyper-text), [morphing-text](https://magicui.design/docs/components/morphing-text), [spinning-text](https://magicui.design/docs/components/spinning-text), [sparkles-text](https://magicui.design/docs/components/sparkles-text) components.       |\n| `getButtons`      | Provides implementation details for [rainbow-button](https://magicui.design/docs/components/rainbow-button), [shimmer-button](https://magicui.design/docs/components/shimmer-button), [shiny-button](https://magicui.design/docs/components/shiny-button), [interactive-hover-button](https://magicui.design/docs/components/interactive-hover-button), [animated-subscribe-button](https://magicui.design/docs/components/animated-subscribe-button), [pulsating-button](https://magicui.design/docs/components/pulsating-button), [ripple-button](https://magicui.design/docs/components/ripple-button) components. |\n| `getEffects`      | Provides implementation details for [animated-beam](https://magicui.design/docs/components/animated-beam), [border-beam](https://magicui.design/docs/components/border-beam), [shine-border](https://magicui.design/docs/components/shine-border), [magic-card](https://magicui.design/docs/components/magic-card), [meteors](https://magicui.design/docs/components/meteors), [neon-gradient-card](https://magicui.design/docs/components/neon-gradient-card), [confetti](https://magicui.design/docs/components/confetti), [particles](https://magicui.design/docs/components/particles), [cool-mode](https://magicui.design/docs/components/cool-mode), [scratch-to-reveal](https://magicui.design/docs/components/scratch-to-reveal) components. |\n| `getWidgets`      | Provides implementation details for [animated-list](https://magicui.design/docs/components/animated-list), [tweet-card](https://magicui.design/docs/components/tweet-card), [client-tweet-card](https://magicui.design/docs/components/client-tweet-card), [lens](https://magicui.design/docs/components/lens), [pointer](https://magicui.design/docs/components/pointer), [avatar-circles](https://magicui.design/docs/components/avatar-circles), [icon-cloud](https://magicui.design/docs/components/icon-cloud), [globe](https://magicui.design/docs/components/globe) components.                                |\n| `getBackgrounds`  | Provides implementation details for [warp-background](https://magicui.design/docs/components/warp-background), [flickering-grid](https://magicui.design/docs/components/flickering-grid), [animated-grid-pattern](https://magicui.design/docs/components/animated-grid-pattern), [retro-grid](https://magicui.design/docs/components/retro-grid), [ripple](https://magicui.design/docs/components/ripple) components.                                                               |\n| `getDevices`      | Provides implementation details for [safari](https://magicui.design/docs/components/safari), [iphone-15-pro](https://magicui.design/docs/components/iphone-15-pro), [android](https://magicui.design/docs/components/android) components.                                                                                                                                                            |\n\n## MCP Limitations\n\nSome clients have a [limit](https://docs.cursor.com/context/model-context-protocol#limitations) on the number of tools they can call. This is why we opted to group the tools into categories. Note: For more specific context on each component, run the MCP locally and modify the logic that groups the components.\n\n## Credits\n\nBig thanks to [@beaubhp](https://github.com/beaubhp) for creating the MCP server 🙏\n\n[MIT](https://github.com/magicuidesign/mcp/blob/main/LICENSE.md)",
      "npm_url": "https://www.npmjs.com/package/mcp",
      "npm_downloads": 17624,
      "keywords": [
        "magicuidesign",
        "mcp",
        "ui",
        "magicuidesign mcp",
        "search magicuidesign",
        "magic ui"
      ],
      "category": "web-search"
    },
    "manimohans--farcaster-mcp": {
      "owner": "manimohans",
      "name": "farcaster-mcp",
      "url": "https://github.com/manimohans/farcaster-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/manimohans.webp",
      "description": "Interact with the Farcaster Network to fetch casts, search channels, and analyze content related to users and channels. Retrieve user-specific casts through FID or username, as well as casts from specific channels.",
      "stars": 2,
      "forks": 5,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-05-09T22:50:39Z",
      "readme_content": "# Farcaster MCP Server\n\n[![smithery badge](https://smithery.ai/badge/@manimohans/farcaster-mcp)](https://smithery.ai/server/@manimohans/farcaster-mcp)\n\nAn MCP server that provides tools to interact with the Farcaster network ([farcaster.xyz](https://www.farcaster.xyz)), allowing AI models to fetch casts, search channels, and analyze content.\n\n<a href=\"https://glama.ai/mcp/servers/koo5epnlc7\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/koo5epnlc7/badge\" alt=\"Farcaster Server MCP server\" />\n</a>\n\n## Features\n\n- **Get User Casts**: Retrieve casts from a specific Farcaster user by FID\n- **Get Username Casts**: Retrieve casts from a specific Farcaster user by username\n- **Get Channel Casts**: Retrieve casts from a specific Farcaster channel\n\n## Installation\n\n```bash\n# Clone the repository\ngit clone https://github.com/manimohans/farcaster-mcp.git\ncd farcaster-mcp\n\n# Install dependencies\nnpm install\n\n# Build the project\nnpm run build\n```\n\n## Usage\n\n### Running the server\n\n```bash\nnpm start\n```\n\n### Using with MCP Inspector\n\n```bash\nnpx @modelcontextprotocol/inspector node ./build/index.js\n```\n\n### Using with Claude for Desktop\n\n1. Install [Claude for Desktop](https://claude.ai/download)\n2. Open your Claude for Desktop App configuration at:\n   - macOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\n   - Windows: `%APPDATA%\\Claude\\claude_desktop_config.json`\n\n3. Add the following configuration:\n\n```json\n{\n  \"mcpServers\": {\n    \"farcaster\": {\n      \"command\": \"node\",\n      \"args\": [\"/absolute/path/to/farcaster-mcp/build/index.js\"]\n    }\n  }\n}\n```\n\n4. Restart Claude for Desktop\n\n### Using with Smithery\n\nThis project includes Smithery configuration files for easy deployment:\n\n```bash\n# Install Smithery CLI\nnpm install -g @smithery/cli\n\n# Deploy to Smithery (specify the client, e.g., claude, cline, windsurf, etc.)\nnpx @smithery/cli install @manimohans/farcaster-mcp --client claude\n```\n\nAvailable client options: claude, cline, windsurf, roo-cline, witsy, enconvo\n\n### Available Tools\n\n#### get-user-casts\n\nRetrieves casts from a specific Farcaster user by their FID (Farcaster ID).\n\nParameters:\n- `fid`: Farcaster user ID (number)\n- `limit` (optional): Maximum number of casts to return (default: 10)\n\nExample query: \"Show me the latest casts from FID 6846.\"\n\n#### get-username-casts\n\nRetrieves casts from a specific Farcaster user by their username.\n\nParameters:\n- `username`: Farcaster username (string)\n- `limit` (optional): Maximum number of casts to return (default: 10)\n\nExample query: \"Show me the latest casts from username 'mani'.\"\n\n#### get-channel-casts\n\nRetrieves casts from a specific Farcaster channel.\n\nParameters:\n- `channel`: Channel name or URL (string)\n- `limit` (optional): Maximum number of casts to return (default: 10)\n\nExample query: \"Show me the latest casts from the 'aichannel' channel.\"\n\n## Smithery Configuration\n\nThis repository includes the necessary configuration files for Smithery:\n\n- `smithery.yaml`: YAML configuration for Smithery deployment\n- `smithery.json`: JSON configuration for Smithery capabilities\n- `Dockerfile`: Container configuration for Smithery deployment\n\n## API Details\n\nThis implementation uses the Farcaster Hubble API to fetch data.\n\n## Development\n\n```bash\n# Run in development mode\nnpm run dev\n```\n\n## License\n\nMIT",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "farcaster",
        "search",
        "web",
        "farcaster mcp",
        "farcaster network",
        "search channels"
      ],
      "category": "web-search"
    },
    "marcopesani--mcp-server-serper": {
      "owner": "marcopesani",
      "name": "mcp-server-serper",
      "url": "https://github.com/marcopesani/mcp-server-serper",
      "imageUrl": "/freedevtools/mcp/pfp/marcopesani.webp",
      "description": "Provides web search capabilities and webpage scraping functions utilizing the Serper API, enabling content extraction and rich search result retrieval.",
      "stars": 117,
      "forks": 19,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-10-01T06:28:07Z",
      "readme_content": "# Serper Search and Scrape MCP Server\n[![smithery badge](https://smithery.ai/badge/@marcopesani/mcp-server-serper)](https://smithery.ai/server/@marcopesani/mcp-server-serper)\n\nA TypeScript-based MCP server that provides web search and webpage scraping capabilities using the Serper API. This server integrates with Claude Desktop to enable powerful web search and content extraction features.\n\n<a href=\"https://glama.ai/mcp/servers/5zk327i0pj\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/5zk327i0pj/badge\" alt=\"serper-search-scrape-mcp-server MCP server\" />\n</a>\n\n## Features\n\n### Tools\n\n- `google_search` - Perform web searches via Serper API\n  - Rich search results including organic results, knowledge graph, \"people also ask\", and related searches\n  - Supports region and language targeting\n  - Optional parameters for location, pagination, time filters, and autocorrection\n  - Supports advanced search operators:\n    - `site`: Limit results to specific domain\n    - `filetype`: Limit to specific file types (e.g., 'pdf', 'doc')\n    - `inurl`: Search for pages with word in URL\n    - `intitle`: Search for pages with word in title\n    - `related`: Find similar websites\n    - `cache`: View Google's cached version of a specific URL\n    - `before`: Date before in YYYY-MM-DD format\n    - `after`: Date after in YYYY-MM-DD format\n    - `exact`: Exact phrase match\n    - `exclude`: Terms to exclude from search results\n    - `or`: Alternative terms (OR operator)\n  \n- `scrape` - Extract content from web pages\n  - Get plain text and optional markdown content\n  - Includes JSON-LD and head metadata\n  - Preserves document structure\n\n## Requirements\n\n- Node.js >= 18\n- Serper API key (set as `SERPER_API_KEY` environment variable)\n\n## Development\n\nInstall dependencies:\n```bash\nnpm install\n```\n\nBuild the server:\n```bash\nnpm run build\n```\n\nFor development with auto-rebuild:\n```bash\nnpm run watch\n```\n\nRun tests:\n```bash\nnpm test                  # Run all tests\nnpm run test:watch       # Run tests in watch mode\nnpm run test:coverage    # Run tests with coverage\nnpm run test:integration # Run integration tests\n```\n\n### Environment Variables\n\nCreate a `.env` file in the root directory:\n\n```\nSERPER_API_KEY=your_api_key_here\n```\n\n### Debugging\n\nSince MCP servers communicate over stdio, debugging can be challenging. We recommend using the [MCP Inspector](https://github.com/modelcontextprotocol/inspector), which is available as a package script:\n\n```bash\nnpm run inspector\n```\n\nThe Inspector will provide a URL to access debugging tools in your browser.\n\n## Installation\n\n### Installing via Smithery\n\nTo install Serper Search and Scrape for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@marcopesani/mcp-server-serper):\n\n```bash\nnpx -y @smithery/cli install @marcopesani/mcp-server-serper --client claude\n```\n\n### Claude Desktop\n\nAdd the server config at:\n- MacOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\n- Windows: `%APPDATA%/Claude/claude_desktop_config.json`\n\n```json\n{\n  \"mcpServers\": {\n    \"serper-search\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"serper-search-scrape-mcp-server\"],\n      \"env\": {\n        \"SERPER_API_KEY\": \"your_api_key_here\"\n      }\n    }\n  }\n}\n```\n\n### Cline\n\n1. Open the Cline extension settings\n2. Open \"MCP Servers\" tab\n3. Click on \"Configure MCP Servers\"\n4. Add the server config:\n\n```json\n{\n  \"mcpServers\": {\n    \"github.com/marcopesani/mcp-server-serper\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"serper-search-scrape-mcp-server\"],\n      \"env\": {\n        \"SERPER_API_KEY\": \"your_api_key_here\"\n      },\n      \"disabled\": false,\n      \"autoApprove\": [\"google_search\", \"scrape\"]\n    }\n  }\n}\n```\n\nAdditional Cline configuration options:\n- `disabled`: Set to `false` to enable the server\n- `autoApprove`: List of tools that don't require explicit approval for each use\n\n### Cursor\n\n1. Open the Cursor settings\n2. Open \"Features\" settings\n3. In the \"MCP Servers\" section, click on \"Add new MCP Server\"\n4. Choose a name, and select \"command\" as \"Type\"\n5. In the \"Command\" field, enter the following:\n\n```\nenv SERPER_API_KEY=your_api_key_here npx -y serper-search-scrape-mcp-server\n```\n\n### Docker\n\nYou can also run the server using Docker. First, build the image:\n\n```bash\ndocker build -t mcp-server-serper .\n```\n\nThen run the container with your Serper API key:\n\n```bash\ndocker run -e SERPER_API_KEY=your_api_key_here mcp-server-serper\n```\n\nAlternatively, if you have your environment variables in a `.env` file:\n\n```bash\ndocker run --env-file .env mcp-server-serper\n```\n\nFor development, you might want to mount your source code as a volume:\n\n```bash\ndocker run -v $(pwd):/app --env-file .env mcp-server-serper\n```\n\nNote: Make sure to replace `your_api_key_here` with your actual Serper API key.",
      "npm_url": "https://www.npmjs.com/package/mcp-server-serper",
      "npm_downloads": 487,
      "keywords": [
        "scraping",
        "serper",
        "search",
        "webpage scraping",
        "serper api",
        "serper provides"
      ],
      "category": "web-search"
    },
    "mcollina--mcp-node-fetch": {
      "owner": "mcollina",
      "name": "mcp-node-fetch",
      "url": "https://github.com/mcollina/mcp-node-fetch",
      "imageUrl": "/freedevtools/mcp/pfp/mcollina.webp",
      "description": "Fetch web content from any URL using various HTTP methods, handle headers and request bodies, and support multiple response formats. Configurable timeout and redirect settings streamline reliable content retrieval.",
      "stars": 11,
      "forks": 5,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-06-02T12:44:21Z",
      "readme_content": "# MCP Node Fetch\n\nAn MCP server that enables fetching web content using the Node.js [undici](https://github.com/nodejs/undici) library.\n\n## Features\n\n- Fetch content from any URL using various HTTP methods\n- Support for headers and request body\n- Return content in various formats (text, JSON, binary, HTML fragments)\n- Handle errors gracefully\n- Configure timeout and redirect behavior\n\n## MCP Tools\n\nThis server provides the following MCP tools:\n\n### `fetch-url`\n\nFetches content from a URL and returns it.\n\nParameters:\n- `url` (string, required): The URL to fetch\n- `method` (string, optional): HTTP method (default: \"GET\")\n- `headers` (object, optional): HTTP headers to include\n- `body` (string, optional): Request body for POST/PUT requests\n- `timeout` (number, optional): Request timeout in milliseconds\n- `responseType` (string, optional): How to parse the response (\"text\", \"json\", \"binary\", \"html-fragment\")\n- `fragmentSelector` (string, optional): CSS selector to extract specific HTML fragments (when responseType is \"html-fragment\")\n- `followRedirects` (boolean, optional): Whether to follow redirects (default: true)\n\n### `extract-html-fragment`\n\nExtracts specific HTML content from a webpage using CSS selectors and optionally navigates to anchor points.\n\nParameters:\n- `url` (string, required): The URL to fetch\n- `selector` (string, required): CSS selector for the HTML fragment to extract\n- `anchorId` (string, optional): Optional anchor ID to locate a specific fragment\n- `method` (string, optional): HTTP method (default: \"GET\")\n- `headers` (object, optional): HTTP headers to include\n- `body` (string, optional): Request body for POST requests\n- `timeout` (number, optional): Request timeout in milliseconds\n- `followRedirects` (boolean, optional): Whether to follow redirects (default: true)\n\n### `check-status`\n\nChecks if a URL is accessible without downloading the full content.\n\nParameters:\n- `url` (string, required): The URL to check\n- `timeout` (number, optional): Request timeout in milliseconds\n\n\n## Claude for Desktop Configuration\n\nTo use with Claude for Desktop, add this to your `claude_desktop_config.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"node-fetch\": {\n      \"command\": \"node\",\n      \"args\": [\"dist/index.js\"]\n    }\n  }\n}\n```\n\n## License\n\nMIT\n",
      "npm_url": "https://www.npmjs.com/package/mcp-node-fetch",
      "npm_downloads": 3116,
      "keywords": [
        "mcollina",
        "fetch",
        "http",
        "search mcollina",
        "fetch web",
        "web search"
      ],
      "category": "web-search"
    },
    "mcp2everything--mcp2brave": {
      "owner": "mcp2everything",
      "name": "mcp2brave",
      "url": "https://github.com/mcp2everything/mcp2brave",
      "imageUrl": "/freedevtools/mcp/pfp/mcp2everything.webp",
      "description": "Connects to the Brave API to perform web searches, enabling access to search results within the MCP framework.",
      "stars": 1,
      "forks": 5,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-03-29T18:54:20Z",
      "readme_content": "# mcp2brave\n\n一个基于MCP协议的服务器，使用Brave API实现网络搜索功能。\n\n## 系统要求\n\n- Python 3.11+\n- UV包管理器\n- Brave API密钥\n\n## 安装步骤\n\n1. 克隆项目\n```bash\ngit clone <仓库地址>\ncd mcp2brave\n```\n\n2. 创建并编辑环境变量文件\n```bash\n# 添加Brave API密钥到.env文件\n.env\n```\n\n3. 使用UV创建虚拟环境\n```bash\n# 创建并激活虚拟环境\nuv venv\n# Windows系统使用:\n.venv\\Scripts\\activate\n# Linux/Mac系统使用:\nsource .venv/bin/activate\n```\n\n4. 安装依赖\n```bash\nuv sync\n```\n\n## 使用方法\n\n### 安装为Claude扩展\n```bash\nfastmcp install mcp2brave.py\n```\n\n### 开发模式与MCP检查器\n要使用MCP检查器测试功能：\n\n```bash\nfastmcp dev mcp2brave.py\n```\n\n运行后，可以在浏览器访问MCP检查器：http://localhost:5173\n\n## 可用工具\n\n- `search_web(query: str)`: 使用Brave API搜索网络\n- `search_web_info(query: str)`: 同上，带中文描述\n\n## 环境变量\n\n- `Brave_API_KEY`: 你的Brave API密钥（必需）\n\n## 注意事项\n\n- 确保在使用前已正确设置API密钥\n- 虚拟环境激活后才能运行相关命令\n- 如遇到编码问题，请确保系统使用UTF-8编码\n\n## 手动添加Cline Continue Claude\n打开Cline Continue Claude的MCP服务器配置文件，加入以下信息\n```json\n\n\"mcp2brave\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"run\",\n        \"--with\",\n        \"fastmcp\",\n        \"--with\",\n        \"python-dotenv\",\n        \"--with\",\n        \"beautifulsoup4\",\n        \"--with\",\n        \"requests\",\n        \"fastmcp\",\n        \"run\",\n        \"C:\\\\Users\\\\你的真实路径\\\\mcp2brave.py\"\n      ],\n      \"env\": {\n        \"BRAVE_API_KEY\": \"API密钥\"\n      }\n    }\n```\n\n\n## 详细操作步骤\n### Cline集成\n\n\n### Cline集成\n\n### Cline集成\n\n\n### 使用示例",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "mcp2brave",
        "mcp2everything",
        "searches",
        "search mcp2everything",
        "mcp2brave connects",
        "mcp2everything mcp2brave"
      ],
      "category": "web-search"
    },
    "mcp2everything--mcp2tavily": {
      "owner": "mcp2everything",
      "name": "mcp2tavily",
      "url": "https://github.com/mcp2everything/mcp2tavily",
      "imageUrl": "/freedevtools/mcp/pfp/mcp2everything.webp",
      "description": "This server implements web search functionality using the Tavily API, providing real-time search capabilities for applications. It allows users to retrieve information from the web quickly and easily.",
      "stars": 4,
      "forks": 6,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-09-25T06:51:45Z",
      "readme_content": "# MCP2Tavily\n\nA MCP protocol server that implements web search functionality using the Tavily API.\n\n## Prerequisites\n\n- Python 3.11+\n- UV package manager\n- Tavily API key\n\n## Installation\n\n1. Clone the repository\n```bash\ngit clone <repository-url>\ncd mcp2tavily\n```\n\n2. Create and edit the `.env` file\n```bash\n# Create .env file\ntouch .env\n# Add your Tavily API key to .env\necho \"TAVILY_API_KEY=your_api_key_here\" > .env\n```\n\n3. Set up virtual environment with UV\n```bash\n# Create and activate virtual environment\nuv venv\nsource .venv/bin/activate  # On Windows use: .venv\\Scripts\\activate\n```\n\n4. Install dependencies\n```bash\nuv sync\n```\n\n## Usage\n\n### Install as Claude extension\n```bash\nfastmcp install mcp2tavily.py\n```\n\n### Development mode with MCP Inspector\nTo test the functionality using MCP Inspector:\n\n```bash\nfastmcp dev mcp2tavily.py\n```\n\nOnce running, you can access the MCP Inspector at: http://localhost:5173\n\n## Available Tools\n\n- `search_web(query: str)`: Search the web using Tavily API\n- `search_web_info(query: str)`: Same as above, with Chinese description\n\n## Environment Variables\n\n- `TAVILY_API_KEY`: Your Tavily API key (required)\n\n## Step-by-Step Guide\n\n## 手动添加Cline Continue Claude\nCline Continue Claude的MCP JSON FILE\n```json\n\n\"mcp2tavily\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"run\",\n        \"--with\",\n        \"fastmcp\",\n        \"--with\",\n        \"python-dotenv\",\n        \"--with\",\n        \"tavily-python\",\n        \"fastmcp\",\n        \"run\",\n        \"C:\\\\Users\\\\你的真实路径\\\\mcp2tavily.py\"\n      ],\n      \"env\": {\n        \"TAVILY_API_KEY\": \"API密钥\"\n      }\n    }\n```\n\n\n\n### Cline\n\n\n### Cline\n\n### Cline\n\n\n### EXAMPLE",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "mcp2tavily",
        "mcp2everything",
        "search",
        "search mcp2everything",
        "mcp2tavily server",
        "web search"
      ],
      "category": "web-search"
    },
    "meicanhong--exa-mcp-server": {
      "owner": "meicanhong",
      "name": "exa-mcp-server",
      "url": "https://github.com/meicanhong/exa-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/meicanhong.webp",
      "description": "Enables AI assistants to perform real-time web searches using Exa's search API, providing structured results while managing caching and rate limits. Facilitates access to fresh web content in a secure manner.",
      "stars": 0,
      "forks": 0,
      "license": "MIT License",
      "language": "",
      "updated_at": "2025-04-07T11:00:11Z",
      "readme_content": "# Exa MCP Server 🔍\n[![npm version](https://badge.fury.io/js/exa-mcp-server.svg)](https://www.npmjs.com/package/exa-mcp-server)\n[![smithery badge](https://smithery.ai/badge/exa)](https://smithery.ai/server/exa)\n\nA Model Context Protocol (MCP) server lets AI assistants like Claude use the Exa AI Search API for web searches. This setup allows AI models to get real-time web information in a safe and controlled way.\n\nDemo video https://www.loom.com/share/ac676f29664e4c6cb33a2f0a63772038?sid=0e72619f-5bfc-415d-a705-63d326373f60\n\n\n## What is MCP? 🤔\n\nThe Model Context Protocol (MCP) is a system that lets AI apps, like Claude Desktop, connect to external tools and data sources. It gives a clear and safe way for AI assistants to work with local services and APIs while keeping the user in control.\n\n## What does this server do? 🚀\n\nThe Exa MCP server:\n- Enables AI assistants to perform web searches using Exa's powerful search API\n- Provides structured search results including titles, URLs, and content snippets\n- Caches recent searches as resources for reference\n- Handles rate limiting and error cases gracefully\n- Supports real-time web crawling for fresh content\n\n\n## Prerequisites 📋\n\nBefore you begin, ensure you have:\n\n- [Node.js](https://nodejs.org/) (v18 or higher)\n- [Claude Desktop](https://claude.ai/download) installed\n- An [Exa API key](https://dashboard.exa.ai/api-keys)\n- Git installed\n\nYou can verify your Node.js installation by running:\n```bash\nnode --version  # Should show v18.0.0 or higher\n```\n\n## Installation 🛠️\n\n### NPM Installation\n\n```bash\nnpm install -g exa-mcp-server\n```\n\n### Using Smithery\n\nTo install the Exa MCP server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/exa):\n\n```bash\nnpx -y @smithery/cli install exa --client claude\n```\n\n### Manual Installation\n\n1. Clone the repository:\n\n```bash\ngit clone https://github.com/exa-labs/exa-mcp-server.git\ncd exa-mcp-server\n```\n\n2. Install dependencies:\n\n```bash\nnpm install\n```\n\n3. Build the project:\n\n```bash\nnpm run build\n```\n\n4. Create a global link (this makes the server executable from anywhere):\n\n```bash\nnpm link\n```\n\n## Configuration ⚙️\n\n### 1. Configure Claude Desktop to recognize the Exa MCP server\n\nYou can find claude_desktop_config.json inside the settings of Claude Desktop app:\n\nOpen the Claude Desktop app and enable Developer Mode from the top-left menu bar. \n\nOnce enabled, open Settings (also from the top-left menu bar) and navigate to the Developer Option, where you'll find the Edit Config button. Clicking it will open the claude_desktop_config.json file, allowing you to make the necessary edits. \n\nOR (if you want to open claude_desktop_config.json from terminal)\n\n#### For macOS:\n\n1. Open your Claude Desktop configuration:\n\n```bash\ncode ~/Library/Application\\ Support/Claude/claude_desktop_config.json\n```\n\n#### For Windows:\n\n1. Open your Claude Desktop configuration:\n\n```powershell\ncode %APPDATA%\\Claude\\claude_desktop_config.json\n```\n\n### 2. Add the Exa server configuration:\n\n```json\n{\n  \"mcpServers\": {\n    \"exa\": {\n      \"command\": \"npx\",\n      \"args\": [\"/path/to/exa-mcp-server/build/index.js\"],\n      \"env\": {\n        \"EXA_API_KEY\": \"your-api-key-here\"\n      }\n    }\n  }\n}\n```\n\nReplace `your-api-key-here` with your actual Exa API key from [dashboard.exa.ai/api-keys](https://dashboard.exa.ai/api-keys).\n\n### 3. Available Tools & Tool Selection\n\nThe Exa MCP server includes the following tools:\n\n- **web_search**: Performs real-time web searches with optimized results and content extraction.\n- **research_paper_search**: Specialized search focused on academic papers and research content.\n- **twitter_search**: Dedicated Twitter/X.com search that finds tweets, profiles, and conversations.\n\nYou can choose which tools to enable by adding the `--tools` parameter to your Claude Desktop configuration:\n\n#### Specify which tools to enable:\n\n```json\n{\n  \"mcpServers\": {\n    \"exa\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"/path/to/exa-mcp-server/build/index.js\",\n        \"--tools=twitter_search\"\n      ],\n      \"env\": {\n        \"EXA_API_KEY\": \"your-api-key-here\"\n      }\n    }\n  }\n}\n```\n\nFor enabling multiple tools, use a comma-separated list:\n\n```json\n{\n  \"mcpServers\": {\n    \"exa\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"/path/to/exa-mcp-server/build/index.js\",\n        \"--tools=web_search,research_paper_search,twitter_search\"\n      ],\n      \"env\": {\n        \"EXA_API_KEY\": \"your-api-key-here\"\n      }\n    }\n  }\n}\n```\n\nIf you don't specify any tools, all tools enabled by default will be used.\n\n### 4. Restart Claude Desktop\n\nFor the changes to take effect:\n\n1. Completely quit Claude Desktop (not just close the window)\n2. Start Claude Desktop again\n3. Look for the 🔌 icon to verify the Exa server is connected\n\n## Using via NPX\n\nIf you prefer to run the server directly, you can use npx:\n\n```bash\n# Run with all tools enabled by default\nnpx exa-mcp-server\n\n# Enable specific tools only\nnpx exa-mcp-server --tools=web_search\n\n# Enable multiple tools\nnpx exa-mcp-server --tools=web_search,research_paper_search\n\n# List all available tools\nnpx exa-mcp-server --list-tools\n```\n\n## Usage 🎯\n\nOnce configured, you can ask Claude to perform web searches. Here are some example prompts:\n\n```\nCan you search for recent developments in quantum computing?\n```\n\n```\nSearch for and summarize the latest news about artificial intelligence startups in new york.\n```\n\n```\nFind and analyze recent research papers about climate change solutions.\n```\n\n```\nSearch Twitter for posts from @elonmusk about SpaceX.\n```\n\n```\nFind tweets from @samaltman that were published in the last week about AI safety.\n```\n\nThe server will:\n\n1. Process the search request\n2. Query the Exa API with optimal settings (including live crawling)\n3. Return formatted results to Claude\n4. Cache the search for future reference\n\n\n## Testing with MCP Inspector 🔍\n\nYou can test the server directly using the MCP Inspector:\n\n```bash\nnpx @modelcontextprotocol/inspector node ./build/index.js\n```\n\nThis opens an interactive interface where you can explore the server's capabilities, execute search queries, and view cached search results.\n\n## Troubleshooting 🔧\n\n### Common Issues\n\n1. **Server Not Found**\n   * Verify the npm link is correctly set up\n   * Check Claude Desktop configuration syntax\n   * Ensure Node.js is properly installed\n\n2. **API Key Issues**\n   * Confirm your EXA_API_KEY is valid\n   * Check the EXA_API_KEY is correctly set in the Claude Desktop config\n   * Verify no spaces or quotes around the API key\n\n3. **Connection Issues**\n   * Restart Claude Desktop completely\n   * Check Claude Desktop logs:\n   \n   ```bash\n   # macOS\n   tail -n 20 -f ~/Library/Logs/Claude/mcp*.log\n   \n   # Windows\n   type \"%APPDATA%\\Claude\\logs\\mcp*.log\"\n   ```\n\n## Acknowledgments 🙏\n\n* [Exa AI](https://exa.ai) for their powerful search API\n* [Model Context Protocol](https://modelcontextprotocol.io) for the MCP specification\n* [Anthropic](https://anthropic.com) for Claude Desktop\n",
      "npm_url": "https://www.npmjs.com/package/exa-mcp-server",
      "npm_downloads": 97542,
      "keywords": [
        "searches",
        "search",
        "exa",
        "exa search",
        "web search",
        "web searches"
      ],
      "category": "web-search"
    },
    "meicanhong--feapder": {
      "owner": "meicanhong",
      "name": "feapder",
      "url": "https://github.com/meicanhong/feapder",
      "imageUrl": "/freedevtools/mcp/pfp/meicanhong.webp",
      "description": "A Python framework designed for building web scrapers, featuring tools for breakpoint resuming, monitoring alerts, and browser rendering. It incorporates a management system for deploying and scheduling scrapers efficiently.",
      "stars": 0,
      "forks": 0,
      "license": "Other",
      "language": "Python",
      "updated_at": "2025-02-14T09:41:41Z",
      "readme_content": "# FEAPDER\n\n![python_3_6_brightgreen](https://img.shields.io/badge/python-3.6-brightgreen)\n![feapder_style_social](https://img.shields.io/github/watchers/Boris-code/feapder?style=social)\n![feapder_style_social](https://img.shields.io/github/stars/Boris-code/feapder?style=social)\n![feapder_style_social](https://img.shields.io/github/forks/Boris-code/feapder?style=social)\n[![Downloads](https://pepy.tech/badge/feapder)](https://pepy.tech/project/feapder)\n[![Downloads](https://pepy.tech/badge/feapder/month)](https://pepy.tech/project/feapder)\n[![Downloads](https://pepy.tech/badge/feapder/week)](https://pepy.tech/project/feapder)\n\n## 简介\n\n1. feapder是一款上手简单，功能强大的Python爬虫框架，内置AirSpider、Spider、TaskSpider、BatchSpider四种爬虫解决不同场景的需求。\n2. 支持断点续爬、监控报警、浏览器渲染、海量数据去重等功能。\n3. 更有功能强大的爬虫管理系统feaplat为其提供方便的部署及调度\n\n读音: `[ˈfiːpdə]`\n\n![feapder](http://markdown-media.oss-cn-beijing.aliyuncs.com/2023/09/04/feapder.jpg)\n\n\n## 文档地址\n\n- 官方文档：https://feapder.com\n- github：https://github.com/Boris-code/feapder\n- 更新日志：https://github.com/Boris-code/feapder/releases\n- 爬虫管理系统：http://feapder.com/#/feapder_platform/feaplat\n\n\n## 环境要求：\n\n- Python 3.6.0+\n- Works on Linux, Windows, macOS\n\n## 安装\n\nFrom PyPi:\n\n精简版\n\n```shell\npip install feapder\n```\n\n浏览器渲染版：\n```shell\npip install \"feapder[render]\"\n```\n\n完整版：\n\n```shell\npip install \"feapder[all]\"\n```\n\n三个版本区别：\n\n1. 精简版：不支持浏览器渲染、不支持基于内存去重、不支持入库mongo\n2. 浏览器渲染版：不支持基于内存去重、不支持入库mongo\n3. 完整版：支持所有功能\n\n完整版可能会安装出错，若安装出错，请参考[安装问题](docs/question/安装问题.md)\n\n## 小试一下\n\n创建爬虫\n\n```shell\nfeapder create -s first_spider\n```\n\n创建后的爬虫代码如下：\n\n```python\nimport feapder\n\n\nclass FirstSpider(feapder.AirSpider):\n    def start_requests(self):\n        yield feapder.Request(\"https://www.baidu.com\")\n\n    def parse(self, request, response):\n        print(response)\n\n\nif __name__ == \"__main__\":\n    FirstSpider().start()\n        \n```\n\n直接运行，打印如下：\n\n```shell\nThread-2|2021-02-09 14:55:11,373|request.py|get_response|line:283|DEBUG|\n                -------------- FirstSpider.parse request for ----------------\n                url  = https://www.baidu.com\n                method = GET\n                body = {'timeout': 22, 'stream': True, 'verify': False, 'headers': {'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/27.0.1453.93 Safari/537.36'}}\n\n<Response [200]>\nThread-2|2021-02-09 14:55:11,610|parser_control.py|run|line:415|DEBUG| parser 等待任务...\nFirstSpider|2021-02-09 14:55:14,620|air_spider.py|run|line:80|INFO| 无任务，爬虫结束\n```\n\n代码解释如下：\n\n1. start_requests： 生产任务\n2. parse： 解析数据\n\n## 参与贡献\n\n贡献之前请先阅读 [贡献指南](./CONTRIBUTING.md)\n\n感谢所有做过贡献的人!\n\n<a href=\"https://github.com/Boris-code/feapder/graphs/contributors\">\n  <img alt=\"feapder\" src=\"https://contrib.rocks/image?repo=Boris-code/feapder\" />\n</a>\n\n## 爬虫工具推荐\n\n1. 爬虫在线工具库：http://www.spidertools.cn\n2. 爬虫管理系统：http://feapder.com/#/feapder_platform/feaplat\n3. 验证码识别库：https://github.com/sml2h3/ddddocr\n\n## 微信赞赏\n\n如果您觉得这个项目帮助到了您，您可以帮作者买一杯咖啡表示鼓励 🍹\n\n也可和作者交个朋友，解决您在使用过程中遇到的问题\n\n\n![赞赏码](http://markdown-media.oss-cn-beijing.aliyuncs.com/2021/03/16/zan-shang-ma.png)\n\n## 学习交流\n\n<table border=\"0\"> \n    <tr> \n     <td> 知识星球：17321694 </td> \n     <td> 作者微信： boris_tm </td> \n     <td> QQ群号：521494615</td>\n    </tr> \n    <tr> \n    <td> <img alt=\"zhi_shi_xing_qiu\" src=\"http://markdown-media.oss-cn-beijing.aliyuncs.com/2020/02/16/zhi-shi-xing-qiu.jpeg\" width=250px>\n </td> \n     <td> <img alt=\"markdown_media\" src=\"http://markdown-media.oss-cn-beijing.aliyuncs.com/2021/07/12/er-wei-ma.jpeg?x-oss-process=style/markdown-media\" width=\"250px\" /> </td> \n     <td> <img alt=\"17142933285892\" src=\"http://markdown-media.oss-cn-beijing.aliyuncs.com/2024/04/28/17142933285892.jpg\" width=\"250px\" /> </td> \n    </tr> \n  </table> \n\n\n\n  加好友备注：feapder\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "scrapers",
        "feapder",
        "python",
        "web scrapers",
        "feapder python",
        "scrapers efficiently"
      ],
      "category": "web-search"
    },
    "meicanhong--meicanhong": {
      "owner": "meicanhong",
      "name": "meicanhong",
      "url": "https://github.com/meicanhong/meicanhong",
      "imageUrl": "/freedevtools/mcp/pfp/meicanhong.webp",
      "description": "Access a collection of articles and personal reflections across various topics, facilitating community engagement through comments and discussions. It serves as a platform for discovering diverse perspectives and ideas.",
      "stars": 0,
      "forks": 0,
      "license": "No License",
      "language": "",
      "updated_at": "2023-03-23T15:39:30Z",
      "readme_content": "\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "meicanhong",
        "search",
        "articles",
        "search meicanhong",
        "meicanhong meicanhong",
        "meicanhong access"
      ],
      "category": "web-search"
    },
    "meicanhong--video-search": {
      "owner": "meicanhong",
      "name": "video-search",
      "url": "https://github.com/meicanhong/video-search",
      "imageUrl": "/freedevtools/mcp/pfp/meicanhong.webp",
      "description": "Search for YouTube videos and analyze their content by generating summaries, retrieving subtitles, and pinpointing specific video segments. The tool supports keyword-based searches and provides detailed video metadata, intelligent subtitle selection, and content analysis powered by GPT.",
      "stars": 0,
      "forks": 0,
      "license": "No License",
      "language": "TypeScript",
      "updated_at": "2025-01-01T04:48:12Z",
      "readme_content": "# YouTube 视频搜索与分析工具\n\n基于 FastAPI 和 React 的 YouTube 视频搜索和内容分析工具，支持视频搜索、字幕分析和内容总结。\n\n## 功能特点\n\n- **视频搜索**：\n  - 基于关键词搜索 YouTube 视频\n  - 支持视频元数据获取（标题、时长、观看次数等）\n  - 自动生成搜索结果总结\n\n- **字幕处理**：\n  - 自动获取视频字幕\n  - 支持多语言字幕\n  - 智能选择最佳字幕源\n\n- **内容分析**：\n  - 基于 GPT 的内容理解和总结\n  - 精确定位相关视频片段\n  - 提供带时间戳的直达链接\n\n- **用户界面**：\n  - 现代化的 React 前端界面\n  - 响应式设计\n  - 实时内容分析\n  - 优雅的过渡动画\n\n## 技术栈\n\n### 后端\n- Python 3.8+\n- FastAPI\n- YouTube Data API v3\n- OpenAI GPT API\n- structlog 日志系统\n\n### 前端\n- React 18\n- TypeScript\n- Vite\n- Ant Design\n- TailwindCSS\n- React Query\n\n### 开发工具\n- Docker & Docker Compose\n- Rye 包管理\n- Pre-commit hooks\n- ESLint & Prettier\n\n## 快速开始\n\n1. 克隆项目\n```bash\ngit clone https://github.com/yourusername/video-search.git\ncd video-search\n```\n\n2. 配置环境变量\n```bash\n# 创建 .env 文件\ncp .env.example .env\n\n# 编辑 .env 文件，添加必要的 API 密钥\nYOUTUBE_API_KEY=your_youtube_api_key\nOPENAI_API_KEY=your_openai_api_key\n```\n\n3. 使用 Docker 启动服务\n```bash\n# 构建并启动服务\ndocker-compose up --build\n```\n\n服务将在以下地址启动：\n- 前端：http://localhost:3000\n- 后端：http://localhost:8000\n\n## 项目结构\n\n```\nvideo-search/\n├── src/                      # 后端源代码\n│   └── youtube_search/\n│       ├── client.py         # YouTube API 客户端\n│       ├── models.py         # 数据模型\n│       ├── service.py        # 业务逻辑\n│       ├── web.py           # Web API\n│       ├── subtitle.py      # 字幕处理\n│       ├── session.py       # 会话管理\n│       ├── openai_client.py # OpenAI API 客户端\n│       └── utils.py         # 工具函数\n├── frontend/                 # 前端源代码\n│   ├── src/\n│   │   ├── components/      # React 组件\n│   │   ├── hooks/          # 自定义 Hooks\n│   │   ├── pages/          # 页面组件\n│   │   ├── services/       # API 服务\n│   │   ├── types/          # TypeScript 类型\n│   │   └── utils/          # 工具函数\n│   ├── vite.config.ts      # Vite 配置\n│   └── tailwind.config.js  # Tailwind 配置\n├── docs/                    # 项目文档\n├── docker-compose.yml       # Docker 编排配置\n└── Dockerfile              # 后端 Docker 配置\n```\n\n## API 接口\n\n### 搜索视频\n```http\nPOST /search\nContent-Type: application/json\n\n{\n    \"keyword\": \"搜索关键词\",\n    \"max_results\": 5\n}\n```\n\n### 分析内容\n```http\nPOST /sessions/{session_id}/analyze\nContent-Type: application/json\n\n{\n    \"query\": \"用户问题\"\n}\n```\n\n详细的 API 文档请参考 `docs/api_reference.md`。\n\n## 开发指南\n\n### 后端开发\n```bash\n# 安装依赖\njust install\n\n# 启动开发服务器\njust dev\n\n# 格式化代码\njust format\n```\n\n### 前端开发\n```bash\ncd frontend\n\n# 安装依赖\nnpm install\n\n# 启动开发服务器\nnpm run dev\n\n# 构建生产版本\nnpm run build\n```\n\n## 注意事项\n\n1. API 密钥安全\n   - 请妥善保管 API 密钥\n   - 不要将 .env 文件提交到版本控制\n   - 建议设置 API 密钥使用限制\n\n2. 资源限制\n   - YouTube API 有每日配额限制\n   - OpenAI API 按使用量计费\n   - 建议实现缓存机制\n\n3. 开发建议\n   - 遵循代码规范和提交规范\n   - 编写单元测试\n   - 使用 TypeScript 类型检查\n\n## 贡献指南\n\n1. Fork 项目\n2. 创建功能分支 (`git checkout -b feature/AmazingFeature`)\n3. 提交更改 (`git commit -m 'Add some AmazingFeature'`)\n4. 推送到分支 (`git push origin feature/AmazingFeature`)\n5. 提交 Pull Request\n\n## 许可证\n\nMIT License",
      "npm_url": "https://www.npmjs.com/package/video-search",
      "npm_downloads": 92,
      "keywords": [
        "searches",
        "search",
        "youtube",
        "video search",
        "meicanhong video",
        "search youtube"
      ],
      "category": "web-search"
    },
    "metrosir--mcp-twikit": {
      "owner": "metrosir",
      "name": "mcp-twikit",
      "url": "https://github.com/metrosir/mcp-twikit",
      "imageUrl": "/freedevtools/mcp/pfp/metrosir.webp",
      "description": "Interact with Twitter to analyze sentiments and retrieve tweets from your timeline, offering insights into public opinions and trends on various topics.",
      "stars": 0,
      "forks": 0,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-04-02T07:54:39Z",
      "readme_content": "# MCP-Twikit\n\n[![smithery badge](https://smithery.ai/badge/mcp-twikit)](https://smithery.ai/server/mcp-twikit)\nA Model Context Protocol (MCP) server for interacting with Twitter.\n\n<a href=\"https://glama.ai/mcp/servers/49i9dd08w8\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/49i9dd08w8/badge\" alt=\"mcp-twikit MCP server\" /></a>\n\n## Installation\n\n### Installing via Smithery\n\nTo install Twikit Twitter Search for Claude Desktop automatically via [Smithery](https://smithery.ai/server/mcp-twikit):\n\n```bash\nnpx -y @smithery/cli install mcp-twikit --client claude\n```\n\n### Manual Installation\n```json\n{\n    \"mcpServer\": {\n        \"command\": \"uvx\",\n        \"args\": [\"--from\", \"git+https://github.com/adhikasp/mcp-twikit\", \"mcp-twikit\"],\n        \"env\": {\n            \"TWITTER_USERNAME\": \"@example\",\n            \"TWITTER_EMAIL\": \"me@example.com\",\n            \"TWITTER_PASSWORD\": \"secret\",\n        }\n    }\n}\n```\n\n## Example usage\n\nUsing [mcp-client-cli](https://github.com/adhikasp/mcp-client-cli)\n\n\n### Comparing sentiments across different twitter accounts\n\n```\n$ llm compare 20 latest tweet directed @IndiHomeCare, @di_cbn, @BiznetHome, @ID_MyRepublic. What are people sentiment to the product? Do 1 search for each account\n\nI'll help you analyze the latest tweets for these internet service providers in Indonesia to understand the public sentiment. I'll use the search_twitter function for each account.\n\nTool Calls:\n  search_twitter\n  Args:\n    query: to:IndiHomeCare\n    count: 20\n    sort_by: Latest\n\nTool Calls:\n  search_twitter\n  Args:\n    query: to:di_cbn\n    count: 20\n    sort_by: Latest\n\nTool Calls:\n  search_twitter\n  Args:\n    query: to:BiznetHome\n    count: 20\n    sort_by: Latest\n\nTool Calls:\n  search_twitter\n  Args:\n    query: to:ID_MyRepublic\n    count: 20\n    sort_by: Latest\n\nSentiment Analysis Summary:\n\n1. IndiHomeCare:\n- Predominantly Negative Sentiment\n- Key Issues:\n  - Unstable internet connection\n  - Slow response times\n  - Intermittent service disruptions\n  - Long waiting times for customer support\n- Specific Complaints:\n  - Network instability\n  - Lagging internet\n  - Unresolved technical issues\n  - Lack of timely communication\n\n2. CBN (di_cbn):\n- Mostly Negative Sentiment\n- Key Issues:\n  - Prolonged internet outages (some reporting 1-2 days of no service)\n  - Slow customer support response\n  - Lack of clear communication about service disruptions\n- Specific Complaints:\n  - LOS (Line of Sight) issues\n  - No timely resolution of technical problems\n  - Delayed email and support responses\n\n3. BiznetHome:\n- Strongly Negative Sentiment\n- Key Issues:\n  - Frequent network interruptions\n  - Slow internet speeds\n  - Poor customer support responsiveness\n- Specific Complaints:\n  - Packet loss\n  - Inconsistent connection\n  - Long download times\n  - Unresolved technical issues\n\n4. MyRepublic (ID_MyRepublic):\n- Extremely Negative Sentiment\n- Key Issues:\n  - Severe and prolonged service disruptions\n  - Complete internet outages\n  - Inability to access online services and games\n  - Extremely poor customer support\n- Specific Complaints:\n  - Multi-day service interruptions\n  - Inability to access websites and online games\n  - Missed technician appointments\n  - Lack of compensation or clear communication\n\nOverall Sentiment Ranking (from Bad to Worst):\n1. IndiHomeCare (Bad)\n2. CBN (Worse)\n3. BiznetHome (Very Bad)\n4. MyRepublic (Extremely Bad)\n\nCommon Themes Across Providers:\n- Poor network stability\n- Slow customer support response\n- Lack of transparent communication\n- Prolonged service interruptions\n- Unresolved technical issues\n\nRecommendation: These Indonesian internet service providers are experiencing significant customer satisfaction issues. Customers are frustrated with unreliable services, poor technical support, and lack of timely problem resolution.\n```\n\n### Getting tweets from your home timeline\n\n```\n$ llm what is happening on my twitter timeline?\n\nI'll help you check your Twitter timeline. I'll retrieve the latest tweets from your timeline:\n\nTool Calls:\n  get_timeline\n  Args:\n\nI'll summarize the key highlights from your Twitter timeline:\n\n1. Professional/Tech Highlights:\n- @ibamarief is preparing to onboard their first intern, noting it's a significant responsibility\n- @papanberjalan shared a critique about web developers not understanding computer performance\n- @pathofexile is running a Twitch Drops promotion for their game\n\n2. Notable Tweets:\n- @elonmusk posted a cryptic \"Hmm\"\n- @realDonaldTrump discussed eliminating Daylight Saving Time\n- @blinkist shared an article about billionaires reading up to 50 books per year\n\n3. Miscellaneous Interesting Tweets:\n- @nealtaparia shared a thread about 11 companies built around a single product\n- @Rixhabh__ posted about creative and iconic ads\n- Several tweets in Indonesian covering various topics from personal stories to social issues\n\nWould you like me to elaborate on any of these tweets or provide more context about any specific post?\n```\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "twikit",
        "twitter",
        "tweets",
        "mcp twikit",
        "twitter analyze",
        "interact twitter"
      ],
      "category": "web-search"
    },
    "mfengzhishang--mcp": {
      "owner": "mfengzhishang",
      "name": "mcp",
      "url": "https://github.com/mfengzhishang/mcp",
      "imageUrl": "/freedevtools/mcp/pfp/mfengzhishang.webp",
      "description": "Provides web scraping, structured data extraction, and web crawling capabilities. Integrates browser automation agents for tasks related to data retrieval from webpages.",
      "stars": 1,
      "forks": 0,
      "license": "MIT License",
      "language": "",
      "updated_at": "2025-08-10T22:25:34Z",
      "readme_content": "# Hyperbrowser MCP Server\n[![smithery badge](https://smithery.ai/badge/@hyperbrowserai/mcp)](https://smithery.ai/server/@hyperbrowserai/mcp)\n\n![Frame 5](https://github.com/user-attachments/assets/3309a367-e94b-418a-a047-1bf1ad549c0a)\n\nThis is Hyperbrowser's Model Context Protocol (MCP) Server. It provides various tools to scrape, extract structured data, and crawl webpages. It also provides easy access to general purpose browser agents like OpenAI's CUA, Anthropic's Claude Computer Use, and Browser Use.\n\nMore information about the Hyperbrowser can be found [here](https://docs.hyperbrowser.ai/). The hyperbrowser API supports a superset of features present in the mcp server.\n\nMore information about the Model Context Protocol can be found [here](https://modelcontextprotocol.io/introduction).\n\n## Table of Contents\n\n- [Installation](#installation)\n- [Usage](#usage)\n- [Tools](#tools)\n- [Configuration](#configuration)\n- [License](#license)\n\n## Installation\n\n### Manual Installation\nTo install the server, run:\n\n```bash\nnpx hyperbrowser-mcp <YOUR-HYPERBROWSER-API-KEY>\n```\n\n## Running on Cursor\nAdd to `~/.cursor/mcp.json` like this:\n```json\n{\n  \"mcpServers\": {\n    \"hyperbrowser\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"hyperbrowser-mcp\"],\n      \"env\": {\n        \"HYPERBROWSER_API_KEY\": \"YOUR-API-KEY\"\n      }\n    }\n  }\n}\n```\n\n## Running on Windsurf\nAdd to your `./codeium/windsurf/model_config.json` like this:\n```json\n{\n  \"mcpServers\": {\n    \"hyperbrowser\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"hyperbrowser-mcp\"],\n      \"env\": {\n        \"HYPERBROWSER_API_KEY\": \"YOUR-API-KEY\"\n      }\n    }\n  }\n}\n```\n\n### Development\n\nFor development purposes, you can run the server directly from the source code.\n\n1. Clone the repository:\n\n   ```sh\n   git clone git@github.com:hyperbrowserai/mcp.git hyperbrowser-mcp\n   cd hyperbrowser-mcp\n   ```\n\n2. Install dependencies:\n\n   ```sh\n   npm install # or yarn install\n   npm run build\n   ```\n\n3. Run the server:\n\n   ```sh\n   node dist/server.js\n   ```\n\n## Claude Desktop app\nThis is an example config for the Hyperbrowser MCP server for the Claude Desktop client.\n\n```json\n{\n  \"mcpServers\": {\n    \"hyperbrowser\": {\n      \"command\": \"npx\",\n      \"args\": [\"--yes\", \"hyperbrowser-mcp\"],\n      \"env\": {\n        \"HYPERBROWSER_API_KEY\": \"your-api-key\"\n      }\n    }\n  }\n}\n```\n\n\n## Tools\n* `scrape_webpage` - Extract formatted (markdown, screenshot etc) content from any webpage \n* `crawl_webpages` - Navigate through multiple linked pages and extract LLM-friendly formatted content\n* `extract_structured_data` - Convert messy HTML into structured JSON\n* `search_with_bing` - Query the web and get results with Bing search\n* `browser_use_agent` - Fast, lightweight browser automation with the Browser Use agent\n* `openai_computer_use_agent` - General-purpose automation using OpenAI’s CUA model\n* `claude_computer_use_agent` - Complex browser tasks using Claude computer use\n* `create_profile` - Creates a new persistent Hyperbrowser profile.\n* `delete_profile` - Deletes an existing persistent Hyperbrowser profile.\n* `list_profiles` - Lists existing persistent Hyperbrowser profiles.\n\n### Installing via Smithery\n\nTo install Hyperbrowser MCP Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@hyperbrowserai/mcp):\n\n```bash\nnpx -y @smithery/cli install @hyperbrowserai/mcp --client claude\n```\n\n## Resources\n\nThe server provides the documentation about hyperbrowser through the `resources` methods. Any client which can do discovery over resources has access to it.\n\n## License\n\nThis project is licensed under the MIT License.\n",
      "npm_url": "https://www.npmjs.com/package/mcp",
      "npm_downloads": 17624,
      "keywords": [
        "scraping",
        "webpages",
        "search",
        "web scraping",
        "extraction web",
        "web crawling"
      ],
      "category": "web-search"
    },
    "mfukushim--map-traveler-mcp": {
      "owner": "mfukushim",
      "name": "map-traveler-mcp",
      "url": "https://github.com/mfukushim/map-traveler-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/mfukushim.webp",
      "description": "Create immersive travel experiences by navigating Google Maps with an avatar, providing real-time updates and photos of the journey. Integrate unique travel narratives and social media interactions during the exploration.",
      "stars": 23,
      "forks": 10,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-10-02T03:51:13Z",
      "readme_content": "# Virtual Traveling bot environment for MCP\n\n[![Verified on MseeP](https://mseep.ai/badge.svg)](https://mseep.ai/app/073d88cc-277d-40b6-8c20-bcabf6c275e9)\n[![smithery badge](https://smithery.ai/badge/@mfukushim/map-traveler-mcp)](https://smithery.ai/server/@mfukushim/map-traveler-mcp)\n\nEnglish / [Japanese](./README_jp.md)\n\nThis is an MCP server that creates an environment for an avatar to virtually travel on Google Maps.\n\nFrom an MCP client such as Claude Desktop, you can give instructions to the avatar and report on the progress of its journey with photos.\n\n<img alt=\"img_5.png\" src=\"https://raw.githubusercontent.com/mfukushim/map-traveler-mcp/for_image/tools/img_5.png\" width=\"400\"/>\n\n> Preparing for MCP Registry Support https://blog.modelcontextprotocol.io/posts/2025-09-08-mcp-registry-preview/  \n\n> Added gemini-2.5-flash-image-preview (nano-banana) to travel image generation  \n\nSupport for nano-banana has been added. Nano-banana's semantic mask allows you to generate composite travel images in a short time without setting remBg.  \nAlthough conventional image synthesis is still possible, we recommend using Gemini nano-banana.  \n\n> Supports both Streamable-HTTP and stdio (compliant with Smithery.ai's config interface)  \n\nIt can be used as a stdio-type MCP as before, or as Streamable-HTTP.  \nAlthough it supports multiple users, the database API must be specified per session using the Smithery.ai config interface.  \nSince it supports both Streamable-HTTP and stdio, it is expected to work as is with the previous MCP client, but if you use the previous stdio version, please use v0.0.x (v0.0.81).  \n``` npx -y @mfukushim/map-traveler-mcp@0.0.81 ```  \n\n> Now supports librechat https://www.librechat.ai/.\n\n> Now supports Smithery https://smithery.ai/server/@mfukushim/map-traveler-mcp (images are excluded because they are heavy to run).\n\n> Now verified MseeP https://mseep.ai/app/mfukushim-map-traveler-mcp \n\n## Functions\n\n#### MCP server tools function\n\nThe following functions can be used as an MCP server. The available functions vary depending on the settings and execution state.\n\nYou can specify the function name directly, but Claude LLM will automatically recognize it, so you can specify the operation in general terms.\n\nExample:\n\"Where are you now?\" \"Let's leave for Tokyo Station.\"\n\n- get_traveler_view_info(includePhoto:boolean,includeNearbyFacilities:boolean)  \n  Gets information about the current travel avatar's location.  \n  - includePhoto: Gets nearby Google Street View photos. If you have set up an image generation AI, it will synthesize the avatar.\n  - includeNearbyFacilities: Gets information about nearby facilities.\n- get_traveler_location()  \n  Gets information about the current travel avatar's address and nearby facilities.\n- reach_a_percentage_of_destination()\n  Reach a specified percentage of the destination (moveMode=skip only)\n  timeElapsedPercentage: Percent progress towards destination(0~100)\n- set_traveler_location(address: string)  \n  Sets the current travel avatar's location.\n  - address: Address information (exact address, or general name that Google Maps or Claude can recognize, etc.)\n- get_traveler_destination_address  \n  Get the destination of the travel avatar you set\n- set_traveler_destination_address(address: string)  \n  Set the destination of the travel avatar\n   - address: Address information (exact address, or general name that Google Maps or Claude can recognize, etc.)\n- start_traveler_journey  \n  Start the journey at the destination.(moveMode=realtime only)\n- stop_traveler_journey  \n  Stop the journey.(moveMode=realtime only)\n- set_traveler_info(settings:string)  \n  Set the traveler's attributes. Set the traveler's personality that you want to change dynamically, such as name and personality. However, if you use a role script, the script is more stable.\n  - settings: Setting information such as name and personality.\n- get_traveler_info  \n  Get the traveler's attributes. Get the traveler's personality.\n- set_avatar_prompt(prompt:string)  \n  Set the prompt when generating the travel avatar image. The default is an anime-style woman. The anime style is enforced to prevent fake images.\n  - prompt\n- reset_avatar_prompt  \n  Reset avatar generation prompts to default.\n- get_sns_feeds  \n  Gets Bluesky SNS articles for the specified custom feed (feeds containing a specific tag).\n- get_sns_mentions  \n  Gets recent mentions (likes, replies) to Bluesky SNS posts that you made yourself.\n- post_sns_writer(message:string)  \n  Posts an article to Bluesky SNS with the specified custom feed. Set a specific tag so that it can be determined that the post was generated by the travel bot.\n  - message: article\n- reply_sns_writer(message:string,id:string)  \n  Reply to the article with the specified id. Set a specific tag so that it can be determined that the post was generated by the travel bot.\n  - message: reply\n  - id: The ID of the post to reply to\n- add_like(id:string)  \n  Add a like to the specified post.\n  - id: The ID of the post to like\n- tips  \n  Guides you on how to set up features that have not yet been set.\n- get_setting  \n  Get environment and image settings.\n\n#### MCP resources\n\nHas five custom prompt samples.\nWhen you import a prompt with Claude Desktop, Claude will act as a traveler.\nThe SNS-compatible version controls SNS input and output while having a travel conversation.\n\n- role.txt  \n  Claude will act as a traveler.\n\n- roleWithSns.txt  \n  Claude will act as a traveler. It also controls reading and posting to SNS.\n- carBattle.txt  \n  This is a small novel game about a story of transporting secret documents from Yokohama to Tokyo. Scenes are automatically generated. Set moveMode=skip to play.\n- japanMapChallenge.txt,japanMapChallenge2.txt  \n  Two AIs communicate with each other via SNS and play a challenge game using landscape images.  \n  To play, you need two Bluesky accounts and two Claude Desktops. Also set moveMode=skip. (However, the operation is somewhat unstable.)  \n  japanMapChallenge2 has a challenge reflection rule.\n\n## Setting\n\nYou will need to obtain and set access keys for multiple APIs, such as for accessing multiple Google maps and generating images.\nUse of the API may incur charges.\n\n#### Settings for using with Claude Desktop \n\n- claude_desktop_config.json (stdio type)\n```json\n{\n  \"mcpServers\": {\n    \"traveler\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@mfukushim/map-traveler-mcp\"],\n      \"env\":{\n        \"MT_GOOGLE_MAP_KEY\":\"(Google Map API key)\",\n        \"MT_GEMINI_IMAGE_KEY\": \"(Gemini Image Api key)\",\n        \"MT_MAX_RETRY_GEMINI\": \"(Number of retries when generating Gemini images Default: 0)\",\n        \"MT_AVATAR_IMAGE_URI\": \"(Character reference image uri (file:// or https://) when generating Gemini image. Multiple settings can be made by separating them with the '|'. When multiple settings are made, they will be selected randomly.)\",\n        \"MT_MAP_API_URL\": \"(Optional: Map API custom endpoint. Example: direction=https://xxxx,places=https://yyyy )\",\n        \"MT_TIME_SCALE\": \"(Optional:Scale of travel time on real roads duration. default 4)\",\n        \"MT_SQLITE_PATH\":\"(db save path: e.g. %USERPROFILE%/Desktop/traveler.sqlite ,$HOME/traveler.sqlite )\",\n        \"MT_TURSO_URL\":\"(Turso sqlite API URL)\",\n        \"MT_TURSO_TOKEN\":\"(Turso sqlite API access token)\",\n        \"MT_REMBG_PATH\": \"(absolute path of the installed rembg cli)\",\n        \"MT_REMBG_URL\": \"(rembg API URL)\",\n        \"MT_REMBG_WO_KEY\": \"(withoutbg.com rembg API key)\",\n        \"MT_PIXAI_KEY\":\"(pixAi API key)\",\n        \"MT_SD_KEY\":\"(or Stability.ai image generation API key\",\n        \"MT_PIXAI_MODEL_ID\": \"(Optional: pixAi ModelId, if not set use default model 1648918127446573124 \",\n        \"MT_COMFY_URL\": \"(Option: Generate image using ComfyUI API at specified URL. Example: http://192.168.1.100:8188)\",\n        \"MT_COMFY_WORKFLOW_T2I\": \"(Optional: Path to API workflow file when using text to image with ComfyUI. If not specified: assets/comfy/t2i_sample.json)\",\n        \"MT_COMFY_WORKFLOW_I2I\": \"(Optional: Path of API workflow file when image to image in ComfyUI. If not specified: assets/comfy/i2i_sample.json)\",\n        \"MT_COMFY_PARAMS\": \"(Optional: Variable values to send to the workflow via comfyUI API)\",\n        \"MT_FIXED_MODEL_PROMPT\": \"(Optional: Fixed avatar generation prompt. You will no longer be able to change your avatar during conversations.)\",\n        \"MT_BODY_AREA_RATIO\": \"(Optional: Acceptable avatar image area ratio. default 0.042)\",\n        \"MT_BODY_HW_RATIO\": \"(Optional: Acceptable avatar image aspect ratios. default 1.5~2.3)\",\n        \"MT_BODY_WINDOW_RATIO_W\": \"(Optional: Avatar composite window horizontal ratio. default 0.5)\",\n        \"MT_BODY_WINDOW_RATIO_H\": \"(Optional: Avatar composite window aspect ratio. default 0.75)\",\n        \"MT_BS_ID\":\"(Bluesky sns registration address)\",\n        \"MT_BS_PASS\":\"(bluesky sns password)\",\n        \"MT_BS_HANDLE\":\"(bluesky sns handle name: e.g. xxxxxxxx.bsky.social )\",\n        \"MT_FILTER_TOOLS\": \"(Optional: Directly filter the tools to be used. All are available if not specified. e.g. tips,set_traveler_location)\",\n        \"MT_MOVE_MODE\": \"(Option: Specify whether the movement mode is realtime or skip. default realtime)\",\n        \"MT_IMAGE_WIDTH\": \"(Option: Output image width (pixels) Default is 512)\",\n        \"MT_NO_IMAGE\": \"(Options: true = do not output image, not specified = output image if possible, default is not specified)\",\n        \"MT_NO_AVATAR\": \"(Option: true = Output StreetView image as is without avatar superimposition. Not specified = Superimpose avatar image. Default is not specified.)\",\n        \"MT_FEED_TAG\": \"(Optional: Specify the feed tag when posting to SNS (#required, 15 characters or more) Default is #geo_less_traveler)\",\n        \"MT_MAX_SESSIONS\": \"(Maximum number of sessions when using Streamable-http)\",\n        \"MT_SESSION_TTL_MS\": \"(Session TTL when using Streamable-http)\",\n        \"MT_SERVICE_TTL_MS\": \"(Service TTL when using Streamable-http)\"\n      }\n    }\n  }\n}\n```  \n\n- claude_desktop_config.json (streamable-http type)  \nThe above MT_ environment variables should be set as environment variables for the server that runs the map-traveler-mcp web service.  \n```json\n{\n  \"mcpServers\": {\n    \"traveler\": {\n      \"type\": \"streamable-http\",\n      \"url\": \"https://(mcp server address)/mcp?config=(base64 config json)\"\n    }\n  }\n}\n```  \n\nbase64 config json (Smithery.ai Expansion)  \nBy concatenating the json in the following format into a single line of string, converting it to base64, and setting it as (base64 setting json), you can overwrite different APIs and settings for each user session.  \nIf the database is not set base64 config json, it will be shared across the entire service (the location of the traveler will be shared across the database and counted for one person).  \nWe plan to reconsider the operation of assigning an individual UserId for each session once the MCP authentication mechanism has become a little clearer.  \n```json\n{\n  \"MT_GOOGLE_MAP_KEY\": \"xxxyyyzzz\",\n  \"MT_GEMINI_IMAGE_KEY\": \"xxyyzz\",\n  \"MT_MAX_RETRY_GEMINI\": \"1\",\n  \"MT_AVATAR_IMAGE_URI\": \"file:///C:/Users/xxxx/Desktop/avatar.png\",\n  \"MT_TURSO_URL\": \"libsql://xxxyyyzzz\",\n  \"MT_TURSO_TOKEN\": \"abcdabcd\",\n  \"MT_BS_ID\": \"xyxyxyxyx\",\n  \"MT_BS_PASS\": \"1234xyz\",\n  \"MT_BS_HANDLE\": \"aabbccdd\",\n  \"MT_FILTER_TOOLS\": \"tips,set_traveler_location\",\n  \"MT_MOVE_MODE\": \"direct\",\n  \"MT_FEED_TAG\": \"#abcdefgabcdefgabcdefg\"\n}\n```  \n(All json values can be omitted)  \n↓ (json text concatenation)  \n```text\n{\"MT_GOOGLE_MAP_KEY\": \"xxxyyyzzz\", \"MT_GEMINI_IMAGE_KEY\": \"xxyyzz\", \"MT_MAX_RETRY_GEMINI\": \"1\", \"MT_TURSO_URL\": \"libsql://xxxyyyzzz\", \"MT_TURSO_TOKEN\": \"abcdabcd\", \"MT_BS_ID\": \"xyxyxyxyx\", \"MT_BS_PASS\": \"1234xyz\", \"MT_BS_HANDLE\": \"aabbccdd\", \"MT_FILTER_TOOLS\": \"tips,set_traveler_location\", \"MT_MOVE_MODE\": \"direct\", \"MT_FEED_TAG\": \"#abcdefgabcdefgabcdefg\"}\n```\n↓ (Set the base64 version to config=)  \n```text\neyJNVF9HT09HTEVfTUFQX0tFWSI6ICJ4eHh5eXl6enoiLCAiTVRfR0VNSU5JX0lNQUdFX0tFWSI6ICJ4eHl5enoiLCAiTVRfTUFYX1JFVFJZX0dFTUlOSSI6ICIxIiwgIk1UX1RVUlNPX1VSTCI6ICJsaWJzcWw6Ly94eHh5eXl6enoiLCAiTVRfVFVSU09fVE9LRU4iOiAiYWJjZGFiY2QiLCAiTVRfQlNfSUQiOiAieHl4eXh5eHl4IiwgIk1UX0JTX1BBU1MiOiAiMTIzNHh5eiIsICJNVF9CU19IQU5ETEUiOiAiYWFiYmNjZGQiLCAiTVRfRklMVEVSX1RPT0xTIjogInRpcHMsc2V0X3RyYXZlbGVyX2xvY2F0aW9uIiwgIk1UX01PVkVfTU9ERSI6ICJkaXJlY3QiLCAiTVRfRkVFRF9UQUciOiAiI2FiY2RlZmdhYmNkZWZnYWJjZGVmZyJ9\n```\n\n\n> NOTE: The environment variables have been renamed to standard snake case. The MT_ prefix is added because they may be used in conjunction with other environment variables, such as in librechat. The old names can still be used for backward compatibility.  \n\nPlease set the following three Credentials for Google Map API.  \n- Street View Static API\n- Places API (New)\n- Time Zone API\n- Directions API\n\nhttps://developers.google.com/maps/documentation/streetview/get-api-key\n\nIf you want to use the image generation AI, set either pixAi_key or sd_key. You also need to have python3.7~3.11 installed on your PC and rembg cli installed (virtual environment recommended).\n\nhttps://platform.pixai.art/docs  \nhttps://platform.stability.ai/docs/api-reference#tag/SDXL-1.0-and-SD1.6/operation/textToImage\n\nThe bluesky SNS address/password are optional. It is recommended that you create a dedicated account as it will post automatically.\n\nhttps://bsky.app/\n\nYou can also run it in practice mode, which does not require an API key for verification.\n\n#### Practice mode settings  \nclaude_desktop_config.json\n```json\n{\n  \"mcpServers\": {\n    \"traveler\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@mfukushim/map-traveler-mcp\"]\n    }\n  }\n}\n```\n\n## How to use\n\n#### Use the practice mode\n\n1. Install nodejs 22.\n\n2. Set up Claude Desktop for use.\n\n3. Reflect one of the above settings in claude_desktop_config.json.\n\n4. Restart Claude Desktop. It may take some time to set up (if an error occurs, try restarting Claude Desktop again. If it doesn't work, see the notes below). Make sure the following mark appears in the bottom right of the screen.\n\n  <img alt=\"img_1.png\" src=\"https://raw.githubusercontent.com/mfukushim/map-traveler-mcp/for_image/tools/img_1.png\" width=\"150\"/>\n\n5. Ask \"Where are you now?\" and \"Go on a journey.\" A conversation will begin. When using the API, a confirmation screen will appear, so select Allow.\n\n<img alt=\"img_4.png\" src=\"https://raw.githubusercontent.com/mfukushim/map-traveler-mcp/for_image/tools/img_4.png\" width=\"200\"/>\n\n6. Select Attach from MCP and select role.txt.\n\n<img alt=\"img_2.png\" src=\"https://raw.githubusercontent.com/mfukushim/map-traveler-mcp/for_image/tools/img_2.png\" width=\"200\"/>\n\n<img alt=\"img_3.png\" src=\"https://raw.githubusercontent.com/mfukushim/map-traveler-mcp/for_image/tools/img_3.png\" width=\"200\"/>\n\n7. A travel prompt has been built in, so feel free to talk to it.\n\n#### Use the full feature\n\n1. Get a Google Map API access key and set the permissions for Street View Static API, Places API (New), Time Zone API, and Directions API. Set this in the env of claude_desktop_config.json and restart.\n   At this point, the travel log will be based on the real map. Travel images will also be output if they are not superimposed.\n2. Decide on a path that will not interfere with the disk and set it in the sqlite_path of the env of claude_desktop_config.json. (Example: %USERPROFILE%/Desktop/traveler.sqlite $HOME/Documents/traveler.sqlite, etc.)\n   At this point, your travel log will be saved and you can continue your journey even if you close Claude Desktop.\n3. Install python 3.7 to 3.11 and install rembg with cli. We recommend using a virtual environment such as venv.\n  ```bash\n  python3 -m venv venv\n  . venv/bin/activate or .\\venv\\Scripts\\activate\n  pip install \"rembg[cpu,cli]\"\n  ```\n  Check if rembg cli works properly using a sample image file. Input an image with a person in it, and if the person is cut out in the output file, it's OK.  \n  ```bash\n  rembg i source_image_file dest_image_file\n  ```\n4. rembg cli will be installed in the python exe location, so get the path. The file location varies depending on the OS and python installation status, but in the case of venv, it is (virtual environment name)\\Scripts\\rembg.exe or (virtual environment name)/bin/rembg above the directory you set. If you can't find it, search for the path with a file search software. Set that path to rembg_path of env in claude_desktop_config.json. (Example: \"rembg_path\": \"C:\\\\Users\\\\xxxx\\\\Documents\\\\rembg_venv\\\\venv\\\\Scripts\\\\rembg.exe\")\n5. Get an image generation API key from the pixAI or Stability.ai site. Set the key to pixAi_key or sd_key in env of claude_desktop_config.json.\n   The avatar will now be overlaid on the travel image.\n6. Get the bluesky SNS address/password and handle name. Set these in bs_id, bs_pass, and bs_handle in env of claude_desktop_config.json, respectively.\n   Import the travel knowledge prompt roleWithSns.txt to report travel actions to SNS (it will automatically post as a bot, so we recommend allocating a dedicated account)\n\nInstead of preparing rembg with the cli, we have added a setting that allows you to handle rembg as a service API.  \nIf you configure the following rembg service, you can use rembg by setting the URL in remBgUrl.  \n\nhttps://github.com/danielgatis/rembg?tab=readme-ov-file#rembg-s  \n\nSetup is simple if you use the Docker version to launch a container and access it.  \n\nhttps://github.com/danielgatis/rembg?tab=readme-ov-file#usage-as-a-docker  \n\n#### Use Turso libsql API for configuration database\n\nIf you want to use the cloud API Turso libsql (https://turso.tech/libsql) without having a local sqlite file, sign up for Turso and allocate a sqlite database (paid, free tier available).   \nThis add-in will automatically configure (migrate) the database.  \nMT_TURSO_URL = turso db URL  \nMT_TURSO_TOKEN = turso db access token  \n\n\n#### Use Cloud API for rembg\n\nLocal settings around rembg are complicated no matter what method you use, but we have added settings for the paid cloud rembg (https://withoutbg.com/).  \n> Note: There is a small free trial available, but please be aware that this is a commercial API and is quite expensive (about 0.1 euros per image).\n\nMT_REMBG_WO_KEY = withoutbg access token\n\n\n#### When using external ComfyUI (for more advanced users)\n\nYou can also use a local ComfyUI as an image generation server. You can configure the image generation characteristics yourself in detail to reduce API costs.\n\nHowever, the configuration will be quite complicated and image generation may take longer.\n\n1. Configure ComfyUI to run in API mode.\n2. Set the server URL to comfy_url in env.\n3. Set detailed configuration values such as the model to be used in env in the form of a json string.\nexample.\n```json\n{\n  \"env\": {\n    \"comfy_url\": \"http://192.168.1.100:8188\",\n    \"comfy_workflow_t2i\": \"C:\\\\Documents\\\\t2itest.json\",\n    \"comfy_workflow_i2i\":\"C:\\\\Documents\\\\i2itest.json\",\n    \"comfy_params\":\"ckpt_name='animagineXL40_v40.safetensors',denoise=0.65\"\n  }\n}\n```\n4. The default workflow can use assets/comfy/t2i_sample.json and assets/comfy/i2i_sample.json in the package. You can specify variables using % and specify the variables in comfy_params.\n\n## Using libreChat\n\nIt has been adapted to work with libreChat. This makes it easier to use, but some additional settings are required.  \nAlso, it seems that it will not be stable unless the PC you use has a decent level of performance, such as one that can stably run Docker.\n\n#### Install libreChat  \n\nPlease make sure it works as described on the official website.  \nIn this case, we recommend using Docker configuration due to additional settings.\n\nhttps://www.librechat.ai/docs/local/docker  \n\nConfigure librechat.yaml using the official procedure.  \nI think you will need to add a local or API LLM service.  \n\nhttps://www.librechat.ai/docs/configuration/librechat_yaml  \n\nAdd a user for login.  \n\nhttps://www.librechat.ai/docs/configuration/authentication#create-user-script  \n\nPlease set it so that you can have general chat conversations.  \n\n#### Add a rembg container with additional settings  \n\nTo use rembg with Docker, add pulling and running the rembg Docker container.  \n\ndocker-compose.override.yml\n```yml\n services:\n   api:\n     volumes:\n       - type: bind\n         source: ./librechat.yaml\n         target: /app/librechat.yaml\n\n   rembg:\n     image: danielgatis/rembg:latest\n     restart: always\n     command: \"s --host 0.0.0.0 --port 7000 --log_level info\"\n\n```\n\n#### Add map-traveler-mcp to the MCP service  \n\nAdd librechat.yaml\n```yaml\nmcpServers:\n  traveler:\n    type: stdio\n    command: npx\n    args:\n      - -y\n      - \"@mfukushim/map-traveler-mcp\"\n```\n\nAdd .env (Same as env in claude_desktop_config.json)\n\n```env\n# map-traveler-mcp\nGoogleMapApi_key=(Google Map API key)\nsqlite_path=/home/run_test.sqlite (e.g. librechat in an unobtrusive location inside the container, or in an external directory that you don't want to mount.)\nremBgUrl=http://rembg:7000 (rembg Service API URL, container URL)\n(Other settings such as image generation AI settings, PixAI key, stability.ai API key, ComfyUI settings, etc.)\n\n```\n\nAfter setting, restart the container.  \nOn slow PCs, mcp initialization may fail. Multiple restarts may work, but this may be difficult to run...\n\n#### llibreChat settings\n\nTo use the MCP function in libreChat, use the Agents function.  \n\n1. On the conversation screen, select Agents.  \n   <img alt=\"libre1.png\" src=\"https://raw.githubusercontent.com/mfukushim/map-traveler-mcp/for_image/tools/libre1.png\" width=\"200\"/>\n2. Select Agent Builder from the panel on the right side of the screen and configure your agent.  \n   <img alt=\"libre2.png\" src=\"https://raw.githubusercontent.com/mfukushim/map-traveler-mcp/for_image/tools/libre2.png\" width=\"200\"/>\n3. Select Add Tools to use map-traveler.  \n   <img alt=\"libre3.png\" src=\"https://raw.githubusercontent.com/mfukushim/map-traveler-mcp/for_image/tools/libre3.png\" width=\"200\"/>\n4. The agent tools screen will appear, so select and add all the map-traveler-mcp tools (if the map-traveler-mcp tools are not listed, MCP initialization has failed, so please restart the container or review the settings by checking the logs, etc.)  \n   <img alt=\"libre4.png\" src=\"https://raw.githubusercontent.com/mfukushim/map-traveler-mcp/for_image/tools/libre4.png\" width=\"200\"/>  \n   <img alt=\"libre5.png\" src=\"https://raw.githubusercontent.com/mfukushim/map-traveler-mcp/for_image/tools/libre5.png\" width=\"200\"/>  \n5. Enter additional script in the instruction area.  \n   Since libreChat does not have the MCP resource function, enter the content text of the following URL into the instruction area instead.   \n   https://github.com/mfukushim/map-traveler-mcp/blob/main/assets/scenario/role.txt  \n   <img alt=\"libre7.png\" src=\"https://raw.githubusercontent.com/mfukushim/map-traveler-mcp/for_image/tools/libre7.png\" width=\"200\"/>  \n6. Click the Create button to save the agent.  \n   <img alt=\"libre6.png\" src=\"https://raw.githubusercontent.com/mfukushim/map-traveler-mcp/for_image/tools/libre6.png\" width=\"200\"/>\n7. Start a new chat.\n\n## Smithery\n\nPlease refer to https://smithery.ai/server/@mfukushim/map-traveler-mcp.  \nRemote MCP (Streamable-http mode) is supported. Image generation is only available on nano-banana.  \nDatabase settings can now be recorded with Turso sqlite, so if you configure Turso, your travel progress will also be saved.  \n\n\n\n\n## Install guide (Japanese, but lots of photos)\n\n1. introduction and Practice mode  \n   https://note.com/marble_walkers/n/n7a8f79e4fb30\n2. DB, Google Map API, Image gen API  \n   https://note.com/marble_walkers/n/n765257c27f3b\n3. Avatar prompt  \n   https://note.com/marble_walkers/n/nc7273724faea\n4. SNS integration  \n   https://note.com/marble_walkers/n/na7c956befe7b\n5. Application 1  \n   https://note.com/marble_walkers/n/n3c86edd8e817\n6. ComfyUI API  \n   https://note.com/marble_walkers/n/ncefc7c05d102  \n7. Application 2  \n   https://note.com/marble_walkers/n/ne7584ed231c8\n8. LibreChat setting  \n   https://note.com/marble_walkers/n/n339bf7905324\n9. AI Agent SNS Battle Map Challenge  \n   https://note.com/marble_walkers/n/n6db937573eaa\n10. Support Smithery, Turso libSQL, and rembg API   \n   https://note.com/marble_walkers/n/ne3b3c0f99707\n11. Streamable-HTTP support  \n    https://note.com/marble_walkers/n/n030063f22dc0\n12. Nano-Banana support  \n    https://note.com/marble_walkers/n/n5d49514dddec  \n\n\n#### Additional about the source code\n\nI use Effect.ts to simplify error management & for my own learning.  \nWe also use the Effect Service, but due to the way MCP calls work, we believe that consolidating it using the Service was not optimal.  \nI think it would be simpler to handle the MCP calls directly in the Effect.  \nAddendum: I'm aware that I will be able to reconsider how to use the Effect Service and rewrite it neatly, but I'm still considering whether to rewrite it.  \n\n#### Notes on the latest updates\n\n- Added image_width to env. The default is 512. Setting it smaller may reduce the cost of LLM API.  \n- Added an env setting that does not output images for MCP clients that do not have image input/output.  \n\"MT_NO_IMAGE\": \"true\" will not generate or output any images. Other image-related settings can be omitted.  \n```\n{\n  \n  \"env\": {\n    \"MT_NO_IMAGE\": \"true\"\n  }\n  \n}\nor\n{\n  \n  \"env\": {\n    \"GoogleMapApi_key\": \"xxxx\",\n    \"MT_NO_IMAGE\": \"true\"\n  }\n  \n}\n\n```  \n- You can now specify the tag name to be added when posting to SNS (Bluesky). #Required and must be at least 15 characters. If not specified, it will become \"#geo_less_traveler\".  \n- The information obtained from SNS has been slightly changed. The information posted to SNS has been slightly changed.  \n- A script has been added that allows multiple travel bots to converse and play via SNS.  \n\n- Supports remote use from Smithery.  \n  If you do not want to configure detailed settings, start the app in practice mode.\n  You can also run the app at full speed by configuring each cloud API, but please be aware of charges as it uses many paid APIs such as rembg API.\n  If you do not want to synthesize avatars, you can run the app with the minimum settings of Google Map API and Turso sqlite API.\n\n- Added the MT_NO_AVATAR option.  \n  If set, an avatar image will not be composited onto the landscape image. Since there will be no retry processing for avatar composition, the time it takes to obtain a response will be significantly shorter.  \n  Set this option if image composition is slow or fails unavoidably.\n\n- Partially applied MCP version 2025-06-18.  \n  I added title to the schema. I plan to apply outputSchema and structured response in the future, but I haven't implemented them this time. Since the output of Travel Bot is simple text, I don't think structuring is necessary yet.  \n  https://modelcontextprotocol.io/specification/2025-06-18/server/tools  \n- Fixed an issue where some functions, such as SNS functions, could not be called regardless of the env settings due to an initialization error.  \n\n- Added support for Streamable-http. This was done in a hurry, so if you experience any issues, please consider using version 0.0.81 or similar.  \n\n- Support for nano-banana (gemini-2.5-flash-image-preview) image generation has been added. When using nano-banana, no rembg settings are required. The characteristics of the avatar prompt have changed, so image generation may fail with the previous avatar prompt. In this case, you will need to adjust the avatar appearance prompt to one that is acceptable for nano-banana.\n\n- When generating images for nano-banana, you can now reference the original character image with MT_AVATAR_IMAGE_URI. Please use it in a way that does not infringe on copyrights.\n\n[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/mfukushim-map-traveler-mcp-badge.png)](https://mseep.ai/app/mfukushim-map-traveler-mcp)",
      "npm_url": "https://www.npmjs.com/package/@mfukushim/map-traveler-mcp",
      "npm_downloads": 7662,
      "keywords": [
        "maps",
        "traveler",
        "map",
        "map traveler",
        "traveler mcp",
        "immersive travel"
      ],
      "category": "web-search"
    },
    "mgsrevolver--seo-inspector-mcp": {
      "owner": "mgsrevolver",
      "name": "seo-inspector-mcp",
      "url": "https://github.com/mgsrevolver/seo-inspector-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/mgsrevolver.webp",
      "description": "Analyzes HTML files and web pages to identify SEO issues and validate structured data schemas. Provides actionable recommendations for improving SEO quality directly through integrated tools without the need for a browser extension.",
      "stars": 4,
      "forks": 5,
      "license": "No License",
      "language": "JavaScript",
      "updated_at": "2025-09-14T17:34:26Z",
      "readme_content": "// README.md - Instructions for setup and use\n\n# SEO Inspector & Schema Validator MCP\n\nA Model Context Protocol (MCP) server for Cursor that analyzes web pages for SEO issues and validates structured data schemas.\n\n## Features\n\n- Analyze HTML files in a codebase for SEO issues\n- Validate JSON-LD structured data\n- Get recommendations to improve SEO\n- No browser extension required - works directly with your codebase\n\n## Installation\n\n1. Clone this repository:\n\n   ```\n   git clone https://github.com/yourusername/seo-inspector-mcp.git\n   cd seo-inspector-mcp\n   ```\n\n2. Install dependencies:\n\n   ```\n   npm install\n   ```\n\n3. Configure Cursor to use this MCP server:\n   - Go to Settings > Features > MCP in Cursor\n   - Add a new MCP server with:\n     - Name: SEO Inspector\n     - Type: sse\n     - URL: http://localhost:8767/sse\n\n## Usage\n\n1. Start the MCP server:\n\n   ```\n   ./run-mcp.sh\n   ```\n\n2. In Cursor, you can now use the SEO Inspector tools:\n   - `seo.analyze-codebase` - Analyze HTML files in a directory\n   - `seo.analyze-html` - Analyze a specific HTML string\n\n## Prioritized SEO Components\n\nThe tool checks for these key SEO elements (in order of importance):\n\n### Critical\n\n- Page Title\n- Meta Description\n- H1 Heading\n- Canonical URL\n\n### Important\n\n- Heading Structure (H2-H6)\n- Image Alt Text\n- Structured Data (JSON-LD)\n- Robots Directives\n\n### Recommended\n\n- Open Graph Tags\n- Twitter Cards\n- Internal Linking\n- URL Structure\n- Mobile Friendliness\n\n## Schema Validation\n\nThe tool validates the following schema types:\n\n- Organization\n- LocalBusiness\n- Product\n- Article\n- WebPage\n- FAQPage\n- Event\n- Recipe\n- Review\n\n## License\n\nMIT\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "seo",
        "mgsrevolver",
        "html",
        "mgsrevolver seo",
        "seo inspector",
        "search mgsrevolver"
      ],
      "category": "web-search"
    },
    "micahman33--VonageAICodeAssist": {
      "owner": "micahman33",
      "name": "VonageAICodeAssist",
      "url": "https://github.com/micahman33/VonageAICodeAssist",
      "imageUrl": "/freedevtools/mcp/pfp/micahman33.webp",
      "description": "Provides AI-assisted access to Vonage API documentation for targeted search and content extraction, facilitating quick access to relevant information about communication APIs. Integrates with external web search APIs to enhance documentation retrieval.",
      "stars": 0,
      "forks": 2,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-03-31T13:18:11Z",
      "readme_content": "# Vonage AI Code Assist MCP Server\n\n## Overview\n\nVonage AI Code Assist is a Model Context Protocol (MCP) server designed to help developers integrate Vonage API capabilities into their applications. The server leverages FastMCP to provide AI-assisted access to Vonage documentation, enabling developers to quickly find relevant information about Vonage's communication APIs.\n\n## How It Works\n\nThe Vonage Assist MCP server operates as follows:\n\n1. **Documentation Search**: The server provides a specialized tool called \"Vonage-Assist\" that searches through Vonage's official documentation.\n\n2. **Web Search Integration**: Using the Google Serper API, the tool performs targeted searches within the Vonage developer documentation domain (`developer.vonage.com/en/documentation`).\n\n3. **Content Extraction**: When a search query is submitted, the server:\n   - Formulates a site-specific search query\n   - Sends the query to Google Serper API\n   - Receives search results with relevant documentation links\n   - Fetches the content from these links\n   - Returns the extracted text content to the user\n\n4. **MCP Tool Integration**: The server is compatible with Claude and other AI assistants that support the MCP protocol, allowing these AI systems to directly utilize Vonage documentation in their responses.\n\n## Setup & Requirements\n\nTo run the Vonage Assist MCP server:\n\n1. Ensure Python 3.13+ is installed.\n\n2. Set up the required environment variables:\n   - `SERPER_API_KEY`: API key for Google Serper (required for web searches)\n\n3. Install dependencies:\n   ```bash\n   uv install\n   ```\n\n4. Run the server:\n   ```bash\n   python main.py\n   ```\n\n## Usage\n\nOnce running, the MCP server exposes the `Vonage-Assist` tool with the following parameters:\n\n- `query`: The search query (e.g., \"number verification\", \"SMS API\")\n- `library`: The documentation library to search (\"vonage\" is currently the only supported option)\n\nExample tool usage (via an MCP-compatible AI):\n```\nUse the Vonage-Assist tool to find information about implementing two-factor authentication with Vonage APIs.\n```\n\n## Technical Implementation\n\nThe server is built using:\n- FastMCP for the MCP server framework\n- httpx for asynchronous HTTP requests\n- BeautifulSoup for HTML parsing and text extraction\n- python-dotenv for environment variable management\n\nThe core functionality is implemented through several key functions:\n- `search_web()`: Performs API requests to Google Serper\n- `fetch_url()`: Retrieves and extracts content from web pages\n- `vonage_docs()`: The main tool function that orchestrates the search and content retrieval process\n\n## Future Considerations\n\nTop potential enhancements for the Vonage Assist MCP server:\n\n1. **Code Generation Tool**: Add capabilities to generate sample code snippets for common Vonage API integrations (SMS, Voice, Verify, Video) in multiple programming languages, helping developers quickly implement Vonage features with proper syntax and best practices.\n\n2. **API Parameter Helper**: Develop a tool that helps developers construct valid API requests by suggesting parameters, validating inputs, and explaining required vs. optional fields for different Vonage API endpoints.\n\n3. **Troubleshooting Assistant**: Implement functionality to diagnose common integration issues by analyzing error codes and providing actionable solutions based on KB articles and documentation - significantly reducing debugging time.\n\n4. **Webhook Configuration Helper**: Create a tool to assist with setting up and testing webhook endpoints for Vonage services, guiding developers through the process of handling callbacks and events.\n\n5. **Best Practices Advisor**: Add a capability to provide context-specific best practices for performance, security, and resilience when implementing Vonage APIs, helping developers build more robust applications.\n\n6. **Rate Limit & Pricing Estimator**: Help developers estimate costs and understand rate limits for their specific use cases.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "vonageaicodeassist",
        "vonage",
        "api",
        "vonageaicodeassist provides",
        "micahman33 vonageaicodeassist",
        "vonage api"
      ],
      "category": "web-search"
    },
    "michalnaka--mcp-substack": {
      "owner": "michalnaka",
      "name": "mcp-substack",
      "url": "https://github.com/michalnaka/mcp-substack",
      "imageUrl": "/freedevtools/mcp/pfp/michalnaka.webp",
      "description": "Download and summarize public Substack posts by extracting titles, authors, subtitles, and content directly within workflows.",
      "stars": 10,
      "forks": 7,
      "license": "No License",
      "language": "JavaScript",
      "updated_at": "2025-09-09T22:20:27Z",
      "readme_content": "# MCP Substack Server\n\nA Model Context Protocol (MCP) server for downloading and parsing Substack posts. Works with Claude.ai desktop app.\n\n## Installation\n\n1. Install dependencies:\n```bash\nnpm install\n```\n\n2. Configure Claude desktop app:\n   \nAdd to `~/Library/Application Support/Claude/claude_desktop_config.json`:\n```json\n{\n  \"mcpServers\": {\n    \"mcp-substack\": {\n      \"command\": \"/opt/homebrew/bin/node\",\n      \"args\": [\"/path/to/mcp-substack/lib/index.mjs\"],\n      \"methods\": {\n        \"download_substack\": {\n          \"description\": \"Download and parse content from a Substack post\"\n        }\n      }\n    }\n  }\n}\n```\n\n3. Start the server:\n```bash\nnpm start\n```\n\n## Usage\n\nIn Claude desktop app, use:\n```\nCould you download and summarize this Substack post: [URL]\n```\n\n## Features\n\n- Downloads and parses Substack posts\n- Extracts title, author, subtitle, and content\n- Works with public Substack posts\n- Integrates with Claude.ai desktop app\n\n## Requirements\n\n- Node.js v18+\n- Claude desktop app\n\n## License\n\nMIT\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "substack",
        "subtitles",
        "michalnaka",
        "substack posts",
        "substack download",
        "mcp substack"
      ],
      "category": "web-search"
    },
    "mikechao--artic-mcp": {
      "owner": "mikechao",
      "name": "artic-mcp",
      "url": "https://github.com/mikechao/artic-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/mikechao.webp",
      "description": "Connects to the Art Institute of Chicago's art collection, enabling searches of artworks by title, full text, or artist, and retrieves detailed information including images.",
      "stars": 2,
      "forks": 3,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-07-26T08:24:57Z",
      "readme_content": "[![artic-logo](https://raw.githubusercontent.com/Art-Institute-of-Chicago/template/main/aic-logo.gif)](https://www.artic.edu/)\n<br/>\n[![smithery badge](https://smithery.ai/badge/@mikechao/artic-mcp)](https://smithery.ai/server/@mikechao/artic-mcp)\n<br/>\n[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/mikechao-artic-mcp-badge.png)](https://mseep.ai/app/mikechao-artic-mcp)\n[![Verified on MseeP](https://mseep.ai/badge.svg)](https://mseep.ai/app/d10632ad-6dcb-4f97-a3ca-eae970713956)\n<br/>\n<a href=\"https://glama.ai/mcp/servers/@mikechao/artic-mcp\">\n<img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@mikechao/artic-mcp/badge\" alt=\"artic-mcp MCP server\" />\n</a>\n\n# Art Institute of Chicago MCP Server\n\nA Model Context Protocol (MCP) server that provides access to the [Art Institute of Chicago Collection](https://www.artic.edu/collection) through natural language interactions. This server allows AI models to search the art collection at the Art Institute of Chicago and have artworks available as a Resource.\n\n## Features\n\nThis server provides AI models with the following tools for interacting with the art collection.\nThe server also includes a prompt, **art-gallery** that will generate an interactive art gallery for a specific artist.\n\n### 1. Search By Title (search-by-title)\n\nSearch for artworks by title in the Art Institute of Chicago\n\n- Inputs:\n  - `title` (string) The title of the artwork to search for.\n  - `limit` (number, optional, default 10) The number of resources to return per page.\n  - `page` (number, optional, default 1) The page of results to return. Used for pagination.\n- Outputs:\n\n  ```\n  Title: Nighthawks\n  Artwork ID: 111628\n  Thumbnail alt text: Scene in a diner, viewed through wrap-around glass windows, at night on an empty urban street. A light-skinned man and woman, he in a suit and she in a red dress, sit together at a triangular wood bar, eyes downcast. At left sits another man, his back to the viewer. Behind the counter is a light-skinned man in a white uniform. The interior lights cast a yellow glow that spills onto the street in pale green. Above the diner a sign reads, \"Phillies.\"\n  Score: 1055.3839\n\n  -----\n  Title: Nighthawks\n  Artwork ID: 118165\n  Thumbnail alt text: A work made of chromogenic print.\n  Score: 57.28689\n\n  Pagination Info\n  Total: 2\n  Total Pages: 1\n  Current Page: 1\n  ```\n\n### 2. Get a specific artwork (get-artwork-by-id)\n\nGets additional information, including the image if available, for a piece of art based on it's id.\n\n- Inputs:\n  - `id` (number) The ID of the artwork to retrieve.\n  - `includeImage` (boolean, optional, default true) Whether to include the artwork image in the response.\n- Outputs:\n\n  ```\n  Title: Nighthawks\n  Artist: Edward Hopper (American, 1882–1967)\n  Artist ID: 34996\n  Description: <p>About <em>Nighthawks</em> Edward Hopper recollected, “unconsciously, probably, I was painting the loneliness of a large city.” In an all-night diner, three customers sit at the counter opposite a server, each appear to be lost in thought and disengaged from one another. The composition is tightly organized and spare in details: there is no entrance to the establishment, no debris on the streets. Through harmonious geometric forms and the glow of the diner’s electric lighting, Hopper created a serene, beautiful, yet enigmatic scene. Although inspired by a restaurant Hopper had seen on Greenwich Avenue in New York, the painting is not a realistic transcription of an actual place. As viewers, we are left to wonder about the figures, their relationships, and this imagined world.</p>\n\n  Image ID: 831a05de-d3f6-f4fa-a460-23008dd58dda\n  Place of Origin: United States\n  Dimensions: 84.1 × 152.4 cm (33 1/8 × 60 in.)\n  Medium: Oil on canvas\n  Credit Line: Friends of American Art Collection\n  Department: Arts of the Americas\n  Is On View: Yes\n  Main Reference Number: 1942.51\n  Has not been viewed much: No\n  Date Start: 1942\n  Date End: 1942\n  Date: 1942\n  Fiscal Year: 1942\n  Is Public Domain: No\n  Gallery: Gallery 262\n  Artwork Type: Painting\n  Artist Title: Edward Hopper\n  Artist Titles: Edward Hopper\n  Style Title: Modernism\n\n  ```\n\n  ```\n  **image encoded in base64 if available and includeImage is true\n  ```\n\n### 3. Full text search (full-text-search)\n\nPerforms a full text search of artworks whose metadata contains the search query.\n\n- Inputs:\n  - `query` (string) The term to search the metadata for.\n  - `limit` (number, optional, default 10) The number of resources to return per page.\n  - `page` (number, optional, default 1) The page of results to return. Used for pagination.\n- Outputs:\n\n  ```\n  Title: Untitled\n  Artwork ID: 62290\n  Thumbnail alt text: A work made of oil and enamel on paper, mounted on composition board.\n  Score: 108.70728\n\n  -----\n  ...\n  ...\n  -----\n  Title: Homage to the Square: Light Passage\n  Artwork ID: 5569\n  Thumbnail alt text: Painting of overlapping squares in grey, yellow, gold, and orange.\n  Score: 104.18398\n\n  Pagination Info\n  Total: 8399\n  Total Pages: 840\n  Current Page: 1\n  ```\n\n### 4. Artist search (search-for-artist)\n\nSearch for a specific artist\n\n- Inputs:\n  - `name` (string) The name of the artist to search for.\n  - `limit` (number, optional, default 10) The number of resources to return per page.\n  - `page` (number, optional, default 1) The page of results to return. Used for pagination.\n- Outputs:\n\n  ```\n  Title: Vincent van Gogh\n  Artist ID: 40610\n  Score: 55.865852\n\n  -----\n  Title: Imitator of Vincent van Gogh\n  Artist ID: 47301\n  Score: 48.782307\n\n  Pagination Info\n  Total: 2\n  Total Pages: 1\n  Current Page: 1\n  ```\n\n### 5. Find artwork by artist (get-artwork-by-artist)\n\nFind works of art by an artist\n\n- Inputs:\n  - `id` (number) The id of the artist to search for artworks. Should be the Artist ID of the `search-for-artist` tool.\n  - `limit` (number, optional, default 10) The number of resources to return per page.\n  - `page` (number, optional, default 1) The page of results to return. Used for pagination.\n- Outputs:\n\n  ```\n  Title: The Bedroom\n  Artwork ID: 28560\n  Thumbnail alt text: Painting of bedroom, blue walls, green window, tan bed, red bedding.\n  Score: 11473.843\n\n  -----\n  .\n  .\n  .\n  -----\n  Title: Weeping Tree\n  Artwork ID: 52733\n  Thumbnail alt text: A work made of reed pen and black-brown ink, with black chalk on off-white wove paper.\n  Score: 11.8061\n\n  Pagination Info\n  Total: 18\n  Total Pages: 2\n  Current Page: 1\n  ```\n\n### 6. Find artwork by medium (search-by-medium)\n\nSearch for artworks by medium in the Art Institute of Chicago\n\n- Inputs\n  - `medium` (string) The medium to search for (e.g., \"oil on canvas\", \"acrylic on panel\", \"watercolor on paper\").\n  - `limit` (number, optional, default 10) The number of resources to return per page.\n  - `page` (number, optional, default 1) The page of results to return. Used for pagination.\n- Outputs:\n\n  ```\n  Title: The Bedroom\n  Artwork ID: 28560\n  Thumbnail alt text: Painting of bedroom, blue walls, green window, tan bed, red bedding.\n  Score: 56644.137\n\n  -----\n  Title: Starry Night and the Astronauts\n  Artwork ID: 129884\n  Thumbnail alt text: Abstract painting composed of small vertical dabs of multiple shades of blue with a small area of similar strokes of red, orange, and yellow in the upper right.\n  Score: 44797.906\n\n  -----\n  ```\n\n### 7. Art Gallery Prompt (art-gallery)\n\nIn the Claude Desktop app, use this prompt—along with your chosen artist’s name—to generate an interactive HTML art gallery showcasing their work.\n\nImages are not displayed in the Claude Desktop app due to security reasons. You can download the HTML from Claude Desktop and view the images in the browser.\n\nClaude Desktop Example\n\n<a href=\"https://mikechao.github.io/images/gallery-prompt.webp\" target=\"_blank\" rel=\"noopener noreferrer\">\n<img width=\"380\" height=\"200\" src=\"https://mikechao.github.io/images/gallery-prompt.webp\" alt=\"claude desktop example\" />\n</a>\n\nThe Generated Gallery in a browser\n\n<a href=\"https://mikechao.github.io/images/gallery-prompt-browser.webp\" target=\"_blank\" rel=\"noopener noreferrer\">\n<img width=\"380\" height=\"200\" src=\"https://mikechao.github.io/images/gallery-prompt-browser.webp\" alt=\"claude desktop example\" />\n</a>\n\n## Usage\n\n### Usage with Claude Desktop\n\nAdd this to your `claude_desktop_config.json`:\n\n```json\n{\n  \"mcp-servers\": {\n    \"artic-museum\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"artic-mcp\"\n      ]\n    }\n  }\n}\n```\n\n## Installing via Smithery\n\nTo install artic-mcp for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@mikechao/artic-mcp):\n\n```bash\nnpx -y @smithery/cli install @mikechao/artic-mcp --client claude\n```\n\n## Example queries\n\nHere some questions you can ask the AI model when this server in connected:\n\n```\nCan you show me the painting titled \"Nighthawks\"?\nCan you find art done by Vincent van Gogh in 1890 that is on display?\nCan you find art by the artist Jackson Pollock?\n```\n\n## Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n\n## License\n\nThis MCP server is licensed under the MIT License. This means you are free to use, modify, and distribute the software, subject to the terms and conditions of the MIT License. For more details, please see the LICENSE file in the project repository.\n\n## Disclaimer\n\nThis MCP Server is not officially associated with The Art Institute of Chicago. It is a third-party implementation of the [The Art Institute of Chicago's API](https://api.artic.edu/docs/) with a MCP Server.\n",
      "npm_url": "https://www.npmjs.com/package/artic-mcp",
      "npm_downloads": 382,
      "keywords": [
        "mikechao",
        "mcp",
        "searches",
        "search mikechao",
        "searches artworks",
        "mikechao artic"
      ],
      "category": "web-search"
    },
    "mikechao--brave-search-mcp": {
      "owner": "mikechao",
      "name": "brave-search-mcp",
      "url": "https://github.com/mikechao/brave-search-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/mikechao.webp",
      "description": "Integrates the Brave Search API to provide comprehensive search functionalities, including web, image, news, video, and local points of interest searches. Enables retrieval of relevant results for various search queries.",
      "stars": 80,
      "forks": 15,
      "license": "GNU General Public License v3.0",
      "language": "TypeScript",
      "updated_at": "2025-09-18T13:13:13Z",
      "readme_content": "# Brave Search MCP Server\n\nAn MCP Server implementation that integrates the [Brave Search API](https://brave.com/search/api/), providing, Web Search, Local Points of Interest Search, Video Search, Image Search and News Search capabilities\n\n<a href=\"https://glama.ai/mcp/servers/@mikechao/brave-search-mcp\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@mikechao/brave-search-mcp/badge\" alt=\"Brave Search MCP server\" />\n</a>\n\n## Features\n\n- **Web Search**: Perform a regular search on the web\n- **Image Search**: Search the web for images. Image search results will be available as a Resource\n- **News Search**: Search the web for news\n- **Video Search**: Search the web for videos\n- **Local Points of Interest Search**: Search for local physical locations, businesses, restaurants, services, etc\n\n## Tools\n\n- **brave_web_search**\n\n  - Execute web searches using Brave's API\n  - Inputs:\n    - `query` (string): The term to search the internet for\n    - `count` (number, optional): The number of results to return (max 20, default 10)\n    - `offset` (number, optional, default 0): The offset for pagination\n    - `freshness` (enum, optional): Filters search results by when they were discovered\n      - The following values are supported\n        - pd: Discovered within the last 24 hours.\n        - pw: Discovered within the last 7 Days.\n        - pm: Discovered within the last 31 Days.\n        - py: Discovered within the last 365 Days\n        - YYYY-MM-DDtoYYYY-MM-DD: Custom date range (e.g., 2022-04-01to2022-07-30)\n\n- **brave_image_search**\n\n  - Get images from the web relevant to the query\n  - Inputs:\n    - `query` (string): The term to search the internet for images of\n    - `count` (number, optional): The number of images to return (max 3, default 1)\n\n- **brave_news_search**\n\n  - Searches the web for news\n  - Inputs:\n    - `query` (string): The term to search the internet for news articles, trending topics, or recent events\n    - `count` (number, optional): The number of results to return (max 20, default 10)\n    - `freshness` (enum, optional): Filters search results by when they were discovered\n      - The following values are supported\n        - pd: Discovered within the last 24 hours.\n        - pw: Discovered within the last 7 Days.\n        - pm: Discovered within the last 31 Days.\n        - py: Discovered within the last 365 Days\n        - YYYY-MM-DDtoYYYY-MM-DD: Custom date range (e.g., 2022-04-01to2022-07-30)\n\n- **brave_local_search**\n\n  - Search for local businesses, services and points of interest\n  - **REQUIRES** subscription to the Pro api plan for location results\n  - Falls back to brave_web_search if no location results are found\n  - Inputs:\n    - `query` (string): Local search term\n    - `count` (number, optional): The number of results to return (max 20, default 5)\n\n- **brave_video_search**\n\n  - Search the web for videos\n  - Inputs:\n    - `query`: (string): The term to search for videos\n    - `count`: (number, optional): The number of videos to return (max 20, default 10)\n    - `freshness` (enum, optional): Filters search results by when they were discovered\n      - The following values are supported\n        - pd: Discovered within the last 24 hours.\n        - pw: Discovered within the last 7 Days.\n        - pm: Discovered within the last 31 Days.\n        - py: Discovered within the last 365 Days\n        - YYYY-MM-DDtoYYYY-MM-DD: Custom date range (e.g., 2022-04-01to2022-07-30)\n\n## Configuration\n\n### Getting an API Key\n\n1. Sign up for a [Brave Search API account](https://brave.com/search/api/)\n2. Choose a plan (Free tier available with 2,000 queries/month)\n3. Generate your API key [from the developer dashboard](https://api.search.brave.com/app/keys)\n\n### Usage with Claude Code\n\nFor [Claude Code](https://claude.ai/code) users, run this command:\n\n**Windows:**\n\n```bash\nclaude mcp add-json brave-search '{\"command\":\"cmd\",\"args\":[\"/c\",\"npx\",\"-y\",\"brave-search-mcp\"],\"env\":{\"BRAVE_API_KEY\":\"YOUR_API_KEY_HERE\"}}'\n```\n\n**Linux/macOS:**\n\n```bash\nclaude mcp add-json brave-search '{\"command\":\"npx\",\"args\":[\"-y\",\"brave-search-mcp\"],\"env\":{\"BRAVE_API_KEY\":\"YOUR_API_KEY_HERE\"}}'\n```\n\nReplace `YOUR_API_KEY_HERE` with your actual Brave Search API key.\n\n### Usage with Claude Desktop\n\n## Desktop Extension (DXT)\n\n1. Download the `dxt` file from the [Releases](https://github.com/mikechao/brave-search-mcp/releases)\n2. Open it with Claude Desktop\n   or\n   Go to File -> Settings -> Extensions and drag the .DXT file to the window to install it\n\n## Docker\n\n1. Clone the repo\n2. Docker build\n\n```bash\ndocker build -t brave-search-mcp:latest -f ./Dockerfile .\n```\n\n3. Add this to your `claude_desktop_config.json`:\n\n```json\n{\n  \"mcp-servers\": {\n    \"brave-search\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"-i\",\n        \"--rm\",\n        \"-e\",\n        \"BRAVE_API_KEY\",\n        \"brave-search-mcp\"\n      ],\n      \"env\": {\n        \"BRAVE_API_KEY\": \"YOUR API KEY HERE\"\n      }\n    }\n  }\n}\n```\n\n### NPX\n\nAdd this to your `claude_desktop_config.json`:\n\n```json\n{\n  \"mcp-servers\": {\n    \"brave-search\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"brave-search-mcp\"\n      ],\n      \"env\": {\n        \"BRAVE_API_KEY\": \"YOUR API KEY HERE\"\n      }\n    }\n  }\n}\n```\n\n### Usage with LibreChat\n\nAdd this to librechat.yaml\n\n```yaml\nbrave-search:\n  command: sh\n  args:\n    - -c\n    - BRAVE_API_KEY=API KEY npx -y brave-search-mcp\n```\n\n## Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n\n## Desktop Extensions (DXT)\n\nAnthropic recently released [Desktop Extensions](https://github.com/anthropics/dxt) allowing installation of local MCP Servers with one click.\n\nInstall the CLI tool to help generate both `manifest.json` and final `.dxt` file.\n\n```sh\nnpm install -g @anthropic-ai/dxt\n```\n\n### Creating the manifest.json file\n\n1. In this folder/directory which contains the local MCP Server, run `dxt init`. The command will start an interactive CLI to help create the `manifest.json`.\n\n### Creating the `dxt` file\n\n1. First install dev dependencies and build\n\n```sh\nnpm install\nnpm run build\n```\n\n2. Then install only the production dependencies, generate a smaller nodule_modules directory\n\n```sh\nnpm install --omit=dev\n```\n\n3. Run `dxt pack` to create a `dxt` file. This will also validate the manifest.json that was created. The `dxt` is essentially a zip file and will contain everything in this directory.\n\n## Disclaimer\n\nThis library is not officially associated with Brave Software. It is a third-party implementation of the Brave Search API with a MCP Server.\n\n## License\n\nThis project is licensed under the GNU General Public License v3.0 - see the [LICENSE](LICENSE) file for details.\n",
      "npm_url": "https://www.npmjs.com/package/brave-search-mcp",
      "npm_downloads": 12525,
      "keywords": [
        "searches",
        "search",
        "retrieval",
        "brave search",
        "search api",
        "web search"
      ],
      "category": "web-search"
    },
    "mikechao--metmuseum-mcp": {
      "owner": "mikechao",
      "name": "metmuseum-mcp",
      "url": "https://github.com/mikechao/metmuseum-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/mikechao.webp",
      "description": "Access the Metropolitan Museum of Art's collection through natural language queries to search for artworks and retrieve detailed information on various departments.",
      "stars": 13,
      "forks": 5,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-09-29T15:45:19Z",
      "readme_content": "[![themet logo](https://upload.wikimedia.org/wikipedia/commons/thumb/7/73/The_Metropolitan_Museum_of_Art_Logo.svg/250px-The_Metropolitan_Museum_of_Art_Logo.svg.png)](https://www.metmuseum.org/)\n\n# Met Museum MCP Server\n\nA Model Context Protocol (MCP) server that provides access to the Metropolitan Museum of Art Collection through natural language interactions. This server allows AI models to search The Met's art collection and have art works available as a Resource.\n\n<a href=\"https://glama.ai/mcp/servers/@mikechao/metmuseum-mcp\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@mikechao/metmuseum-mcp/badge\" alt=\"Met Museum MCP Server\" /></a>\n\n[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/mikechao-metmuseum-mcp-badge.png)](https://mseep.ai/app/mikechao-metmuseum-mcp)\n\n[![Verified on MseeP](https://mseep.ai/badge.svg)](https://mseep.ai/app/ccc75a48-9b33-4a9a-8ef7-8dc3848db263)\n\n## Features\n\nThis server provides AI models the following tools to interact with the art collection of The Met\n\n### 1. List Departments (list-departments)\n\nLists all the valid departments at The Met\n\n- Inputs:\n  - None\n- Output:\n  ```\n  Department ID: 1, Display Name: American Decorative Arts\n  Department ID: 3, Display Name: Ancient Near Eastern Art\n  ...\n  ```\n\n### 2. Search Museum Objects (search-museum-objects)\n\nSearch for various objects in The Met based on the inputs.\n\n- Inputs:\n  - `q` (string): The search term e.g. sunflowers\n  - `hasImages` (boolean, optional, default: false): Only search for objects with images\n  - `title` (boolean, optional, default: false): Returns objects that match the query, specifically searching against the title field for objects.\n  - `departmentId` (number, optional): Returns objects that are a part of a specific department.\n- Outputs:\n\n  ```\n  Total objects found: 54\n  Object IDs: 436532, 789578, 436840, 438722,...\n  ```\n\n### 3. Get Museum Objects (get-museum-object)\n\nGet a specific object from The Met containing all open access data about that object, including its image (if the image is available under Open Access).\n\nIf there is an image it is added to the Resource of the server via the title of the object.\n\n- Inputs:\n  - `objectId` (number): The id of the object to retrieve\n  - `returnImage` (boolean, optional, default: true): Whether to return the image (if available) of the object and add it to the server resources\n- Outputs:\n  ```\n  Title: Self-Portrait with a Straw Hat (obverse: The Potato Peeler)\n  Artist: Vincent van Gogh\n  Artist Bio: Dutch, Zundert 1853–1890 Auvers-sur-Oise\n  Department: European Paintings\n  Credit Line: Bequest of Miss Adelaide Milton de Groot (1876-1967), 1967\n  Medium: Oil on canvas\n  Dimensions: 16 x 12 1/2 in. (40.6 x 31.8 cm)\n  Primary Image URL: https://images.metmuseum.org/CRDImages/ep/original/DT1502_cropped2.jpg\n  Tags: Men, Self-portraits\n  ```\n  If returnImage is true\n  ```\n  **base64 encoding of jpeg image**\n  ```\n\n### Usage with Claude Desktop\n\n## Via Desktop Extension (DXT)\n\n1. Download the `dxt` file from the [Releases](https://github.com/mikechao/metmuseum-mcp/releases)\n2. Open it with Claude Desktop\n   or\n   Go to File -> Settings -> Extensions and drag the .DXT file to the window to install it\n\n## Via npx\n\nAdd this to your `claude_desktop_config.json`:\n\n```json\n{\n  \"mcp-servers\": {\n    \"met-museum\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"metmuseum-mcp\"\n      ]\n    }\n  }\n}\n```\n\n### Usage with LibreChat\n\nAdd the following in your `librechat.yaml`\n\n```yaml\nmcpServers:\n  metmuseum:\n    command: npx\n    args:\n      - -y\n      - metmuseum-mcp\n```\n\n## Example queries\n\nHere some questions you can ask the AI model when this server in connected:\n\n```\nCan you show me a few painting from the Asian Art department?\nCan you find the painting titled \"Corridor in the Asylum\"?\nCan you find any art that has \"cat\" in the title or features \"cats\"?\n```\n\n## Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n\n## License\n\nThis MCP server is licensed under the MIT License. This means you are free to use, modify, and distribute the software, subject to the terms and conditions of the MIT License. For more details, please see the LICENSE file in the project repository.\n\n## Disclaimer\n\nThis library is not officially associated with The Metropolitan Museum of Art in New York. It is a third-party implementation of the [The Metropolitan Museum of Art Collection API](https://metmuseum.github.io/) with a MCP Server.\n",
      "npm_url": "https://www.npmjs.com/package/metmuseum-mcp",
      "npm_downloads": 23255,
      "keywords": [
        "metmuseum",
        "mikechao",
        "museum",
        "mikechao metmuseum",
        "metmuseum mcp",
        "search mikechao"
      ],
      "category": "web-search"
    },
    "mingdedi--InternetSearch-mcp-server": {
      "owner": "mingdedi",
      "name": "InternetSearch-mcp-server",
      "url": "https://github.com/mingdedi/InternetSearch-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/mingdedi.webp",
      "description": "Facilitates real-time internet searches using the Bocha AI search API, enabling applications to access and retrieve web-based information efficiently. Requires an API key for integration and supports setup through a specific configuration format.",
      "stars": 0,
      "forks": 1,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-06-12T02:46:13Z",
      "readme_content": "# Internetsearch-mcp-server\n\n- [English README](README.en.md)\n\n一个用于联网搜索的MCP服务器\n基于博查搜索API的MCP服务器  \n需要使用博查AI的搜索服务密钥，具体文档请查阅[博查开发文档](https://bocha-ai.feishu.cn/wiki/HmtOw1z6vik14Fkdu5uc9VaInBb).\n\n**如何使用**  \n1、将仓库git clone\n```bash\ngit clone https://github.com/mingdedi/Internetsearch-mcp-server.git\n```\n2、使用uv重建环境\n```bash\ncd Internetsearch-mcp-server\npip install uv\nuv venv\n./.venv/Scripts/activate.bat\nuv sync\n```\n3、在配置文件中添加，类似格式如下\n```json\n{\n  \"mcpServers\": {\n    \"Internetsearch-mcp-server\": {\n      \"description\": \"Internetsearch-mcp-server\",\n      \"command\": \"uv\",\n      \"args\": [\n        \"--directory\",\n        \"/path/Internetsearch-mcp-server\",\n        \"run\",\n        \"Internet_search.py\"\n      ],\n      \"env\": {\n        \"BOCHAAI_API_KEY\": \"sk-123456789412345678312323456789e\"\n      }\n    }\n  }\n}\n```\n\n**注意**  \n这里的BOCHAAI_API_KEY中的密钥仅仅是一个示例。  \n如果想要获取一个实际可用的密钥请访问[博查AI](https://bochaai.com/)\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "internetsearch",
        "searches",
        "search",
        "internetsearch mcp",
        "mingdedi internetsearch",
        "web search"
      ],
      "category": "web-search"
    },
    "misanthropic-ai--ddg-mcp": {
      "owner": "misanthropic-ai",
      "name": "ddg-mcp",
      "url": "https://github.com/misanthropic-ai/ddg-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/misanthropic-ai.webp",
      "description": "Leverage DuckDuckGo's capabilities to perform text and image searches while ensuring user privacy. The server provides an interface to obtain summaries of search results and conduct web searches for various media types.",
      "stars": 9,
      "forks": 8,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-09-18T10:40:07Z",
      "readme_content": "# ddg-mcp MCP server\n\nDuckDuckGo search API MCP - A server that provides DuckDuckGo search capabilities through the Model Context Protocol.\n\n## Components\n\n### Prompts\n\nThe server provides the following prompts:\n- **search-results-summary**: Creates a summary of DuckDuckGo search results\n  - Required \"query\" argument for the search term\n  - Optional \"style\" argument to control detail level (brief/detailed)\n\n### Tools\n\nThe server implements the following DuckDuckGo search tools:\n\n- **ddg-text-search**: Search the web for text results using DuckDuckGo\n  - Required: \"keywords\" - Search query keywords\n  - Optional: \"region\", \"safesearch\", \"timelimit\", \"max_results\"\n  \n- **ddg-image-search**: Search the web for images using DuckDuckGo\n  - Required: \"keywords\" - Search query keywords\n  - Optional: \"region\", \"safesearch\", \"timelimit\", \"size\", \"color\", \"type_image\", \"layout\", \"license_image\", \"max_results\"\n  \n- **ddg-news-search**: Search for news articles using DuckDuckGo\n  - Required: \"keywords\" - Search query keywords\n  - Optional: \"region\", \"safesearch\", \"timelimit\", \"max_results\"\n  \n- **ddg-video-search**: Search for videos using DuckDuckGo\n  - Required: \"keywords\" - Search query keywords\n  - Optional: \"region\", \"safesearch\", \"timelimit\", \"resolution\", \"duration\", \"license_videos\", \"max_results\"\n  \n- **ddg-ai-chat**: Chat with DuckDuckGo AI\n  - Required: \"keywords\" - Message or question to send to the AI\n  - Optional: \"model\" - AI model to use (options: \"gpt-4o-mini\", \"llama-3.3-70b\", \"claude-3-haiku\", \"o3-mini\", \"mistral-small-3\")\n\n## Installation\n\n### Prerequisites\n\n- Python 3.9 or higher\n- [uv](https://github.com/astral-sh/uv) (recommended) or pip\n\n### Install from PyPI\n\n```bash\n# Using uv\nuv install ddg-mcp\n\n# Using pip\npip install ddg-mcp\n```\n\n### Install from Source\n\n1. Clone the repository:\n```bash\ngit clone https://github.com/misanthropic-ai/ddg-mcp.git\ncd ddg-mcp\n```\n\n2. Install the package:\n```bash\n# Using uv\nuv install -e .\n\n# Using pip\npip install -e .\n```\n\n## Configuration\n\n### Required Dependencies\n\nThe server requires the `duckduckgo-search` package, which will be installed automatically when you install `ddg-mcp`.\n\nIf you need to install it manually:\n```bash\nuv install duckduckgo-search\n# or\npip install duckduckgo-search\n```\n\n## DuckDuckGo Search Parameters\n\n### Common Parameters\n\nThese parameters are available for most search types:\n\n- **region**: Region code for localized results (default: \"wt-wt\")\n  - Examples: \"us-en\" (US English), \"uk-en\" (UK English), \"ru-ru\" (Russian)\n  - See [DuckDuckGo regions](https://duckduckgo.com/params) for more options\n\n- **safesearch**: Content filtering level (default: \"moderate\")\n  - \"on\": Strict filtering\n  - \"moderate\": Moderate filtering\n  - \"off\": No filtering\n\n- **timelimit**: Time range for results\n  - \"d\": Last day\n  - \"w\": Last week\n  - \"m\": Last month\n  - \"y\": Last year (not available for news/videos)\n\n- **max_results**: Maximum number of results to return (default: 10)\n\n### Search Operators\n\nYou can use these operators in your search keywords:\n\n- `cats dogs`: Results about cats or dogs\n- `\"cats and dogs\"`: Results for exact term \"cats and dogs\"\n- `cats -dogs`: Fewer dogs in results\n- `cats +dogs`: More dogs in results\n- `cats filetype:pdf`: PDFs about cats (supported: pdf, doc(x), xls(x), ppt(x), html)\n- `dogs site:example.com`: Pages about dogs from example.com\n- `cats -site:example.com`: Pages about cats, excluding example.com\n- `intitle:dogs`: Page title includes the word \"dogs\"\n- `inurl:cats`: Page URL includes the word \"cats\"\n\n### Image Search Specific Parameters\n\n- **size**: \"Small\", \"Medium\", \"Large\", \"Wallpaper\"\n- **color**: \"color\", \"Monochrome\", \"Red\", \"Orange\", \"Yellow\", \"Green\", \"Blue\", \"Purple\", \"Pink\", \"Brown\", \"Black\", \"Gray\", \"Teal\", \"White\"\n- **type_image**: \"photo\", \"clipart\", \"gif\", \"transparent\", \"line\"\n- **layout**: \"Square\", \"Tall\", \"Wide\"\n- **license_image**: \"any\", \"Public\", \"Share\", \"ShareCommercially\", \"Modify\", \"ModifyCommercially\"\n\n### Video Search Specific Parameters\n\n- **resolution**: \"high\", \"standard\"\n- **duration**: \"short\", \"medium\", \"long\"\n- **license_videos**: \"creativeCommon\", \"youtube\"\n\n### AI Chat Models\n\n- **gpt-4o-mini**: OpenAI's GPT-4o mini model\n- **llama-3.3-70b**: Meta's Llama 3.3 70B model\n- **claude-3-haiku**: Anthropic's Claude 3 Haiku model\n- **o3-mini**: OpenAI's O3 mini model\n- **mistral-small-3**: Mistral AI's small model\n\n## Quickstart\n\n### Install\n\n#### Claude Desktop\n\nOn MacOS: `~/Library/Application\\ Support/Claude/claude_desktop_config.json`\nOn Windows: `%APPDATA%/Claude/claude_desktop_config.json`\n\n<details>\n  <summary>Development/Unpublished Servers Configuration</summary>\n  ```\n  \"mcpServers\": {\n    \"ddg-mcp\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"--directory\",\n        \"/Users/shannon/Workspace/artivus/ddg-mcp\",\n        \"run\",\n        \"ddg-mcp\"\n      ]\n    }\n  }\n  ```\n</details>\n\n<details>\n  <summary>Published Servers Configuration</summary>\n  ```\n  \"mcpServers\": {\n    \"ddg-mcp\": {\n      \"command\": \"uvx\",\n      \"args\": [\n        \"ddg-mcp\"\n      ]\n    }\n  }\n  ```\n</details>\n\n## Usage Examples\n\n### Text Search\n\n```\nUse the ddg-text-search tool to search for \"climate change solutions\"\n```\n\nAdvanced example:\n```\nUse the ddg-text-search tool to search for \"renewable energy filetype:pdf site:edu\" with region \"us-en\", safesearch \"off\", timelimit \"y\", and max_results 20\n```\n\n### Image Search\n\n```\nUse the ddg-image-search tool to find images of \"renewable energy\" with color set to \"Green\"\n```\n\nAdvanced example:\n```\nUse the ddg-image-search tool to find images of \"mountain landscape\" with size \"Large\", color \"Blue\", type_image \"photo\", layout \"Wide\", and license_image \"Public\"\n```\n\n### News Search\n\n```\nUse the ddg-news-search tool to find recent news about \"artificial intelligence\" from the last day\n```\n\nAdvanced example:\n```\nUse the ddg-news-search tool to search for \"space exploration\" with region \"uk-en\", timelimit \"w\", and max_results 15\n```\n\n### Video Search\n\n```\nUse the ddg-video-search tool to find videos about \"machine learning tutorials\" with duration set to \"medium\"\n```\n\nAdvanced example:\n```\nUse the ddg-video-search tool to search for \"cooking recipes\" with resolution \"high\", duration \"short\", license_videos \"creativeCommon\", and max_results 10\n```\n\n### AI Chat\n\n```\nUse the ddg-ai-chat tool to ask \"What are the latest developments in quantum computing?\" using the claude-3-haiku model\n```\n\n### Search Results Summary\n\n```\nUse the search-results-summary prompt with query \"space exploration\" and style \"detailed\"\n```\n\n## Claude config\n\"ddg-mcp\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"--directory\",\n        \"/PATH/TO/YOUR/INSTALLATION/ddg-mcp\",\n        \"run\",\n        \"ddg-mcp\"\n      ]\n  },\n\n## Development\n\n### Building and Publishing\n\nTo prepare the package for distribution:\n\n1. Sync dependencies and update lockfile:\n```bash\nuv sync\n```\n\n2. Build package distributions:\n```bash\nuv build\n```\n\nThis will create source and wheel distributions in the `dist/` directory.\n\n3. Publish to PyPI:\n```bash\nuv publish\n```\n\nNote: You'll need to set PyPI credentials via environment variables or command flags:\n- Token: `--token` or `UV_PUBLISH_TOKEN`\n- Or username/password: `--username`/`UV_PUBLISH_USERNAME` and `--password`/`UV_PUBLISH_PASSWORD`\n\n### Automated Publishing with GitHub Actions\n\nThis repository includes a GitHub Actions workflow for automated publishing to PyPI. The workflow is triggered when:\n\n1. A new GitHub Release is created\n2. The workflow is manually triggered via the GitHub Actions interface\n\nTo set up automated publishing:\n\n1. Generate a PyPI API token:\n   - Go to https://pypi.org/manage/account/token/\n   - Create a new token with scope limited to the `ddg-mcp` project\n   - Copy the token value (you'll only see it once)\n\n2. Add the token to your GitHub repository secrets:\n   - Go to your repository on GitHub\n   - Navigate to Settings > Secrets and variables > Actions\n   - Click \"New repository secret\"\n   - Name: `PYPI_API_TOKEN`\n   - Value: Paste your PyPI token\n   - Click \"Add secret\"\n\n3. To publish a new version:\n   - Update the version number in `pyproject.toml`\n   - Create a new release on GitHub or manually trigger the workflow\n\n### Debugging\n\nSince MCP servers run over stdio, debugging can be challenging. For the best debugging\nexperience, we strongly recommend using the [MCP Inspector](https://github.com/modelcontextprotocol/inspector).\n\n\nYou can launch the MCP Inspector via [`npm`](https://docs.npmjs.com/downloading-and-installing-node-js-and-npm) with this command:\n\n```bash\nnpx @modelcontextprotocol/inspector uv --directory /path/to/your/ddg-mcp run ddg-mcp\n```\n\n\nUpon launching, the Inspector will display a URL that you can access in your browser to begin debugging.\n",
      "npm_url": "https://www.npmjs.com/package/ddg-mcp-server",
      "npm_downloads": 0,
      "keywords": [
        "duckduckgo",
        "searches",
        "search",
        "duckduckgo capabilities",
        "image searches",
        "web search"
      ],
      "category": "web-search"
    },
    "mixelpixx--Google-Search-MCP-Server": {
      "owner": "mixelpixx",
      "name": "Google-Search-MCP-Server",
      "url": "https://github.com/mixelpixx/Google-Search-MCP-Server",
      "imageUrl": "/freedevtools/mcp/pfp/mixelpixx.webp",
      "description": "Integrates Google Custom Search to retrieve search results and perform detailed analysis of webpage content. Supports multiple output formats and provides features for categorization, summarization, and optimized responses.",
      "stars": 147,
      "forks": 46,
      "license": "Other",
      "language": "TypeScript",
      "updated_at": "2025-10-03T07:23:23Z",
      "readme_content": "# Version 2.0 is here\r\n\r\n# Google Search MCP Server\r\nAn MCP (Model Context Protocol) server that provides Google search capabilities and webpage content analysis tools. This server enables AI models to perform Google searches and analyze webpage content programmatically.\r\n\r\n## Features\r\n\r\n- Google Custom Search integration\r\n- Advanced search features (filters, sorting, pagination, categorization)\r\n- Webpage content analysis in multiple formats (markdown, HTML, plain text)\r\n- Batch webpage analysis\r\n- Result categorization and classification\r\n- Content summarization\r\n- Optimized, human-readable responses\r\n- MCP-compliant interface\r\n\r\n## Prerequisites\r\n\r\n- Node.js (v16 or higher)\r\n- Google Cloud Platform account\r\n- Custom Search Engine ID\r\n- Google API Key\r\n\r\n## Installation\r\n\r\n1. Clone the repository\r\n2. Install Node.js dependencies:\r\n```bash\r\nnpm install\r\n```\r\n3. Build the TypeScript code:\r\n```bash\r\nnpm run build\r\n```\r\n\r\n## Configuration\r\n\r\n1. Set up environment variables for your Google API credentials:\r\n\r\nYou can either set these as system environment variables or configure them in your MCP settings file.\r\n\r\nRequired environment variables:\r\n- `GOOGLE_API_KEY`: Your Google API key\r\n- `GOOGLE_SEARCH_ENGINE_ID`: Your Custom Search Engine ID\r\n\r\n2. Add the server configuration to your MCP settings file (typically located at `%APPDATA%/Code/User/globalStorage/saoudrizwan.claude-dev/settings/cline_mcp_settings.json`):\r\n```json\r\n{\r\n  \"mcpServers\": {\r\n    \"google-search\": {\r\n      \"autoApprove\": [\r\n        \"google_search\",\r\n        \"extract_webpage_content\",\r\n        \"extract_multiple_webpages\"\r\n      ],\r\n      \"disabled\": false,\r\n      \"timeout\": 60,\r\n      \"command\": \"node\",\r\n      \"args\": [\r\n        \"/path/to/google-search-mcp-server/dist/google-search.js\"\r\n      ],\r\n      \"env\": {\r\n        \"GOOGLE_API_KEY\": \"your-google-api-key\",\r\n        \"GOOGLE_SEARCH_ENGINE_ID\": \"your-custom-search-engine-id\"\r\n      },\r\n      \"transportType\": \"stdio\"\r\n    }\r\n  }\r\n}\r\n```\r\n\r\n## Running\r\n\r\nStart the MCP server:\r\n```bash\r\nnpm run start\r\n```\r\n\r\n## Available Tools\r\n\r\n### 1. google_search\r\nSearch Google and return relevant results from the web. This tool finds web pages, articles, and information on specific topics using Google's search engine.\r\n\r\n```typescript\r\n{\r\n  \"name\": \"google_search\",\r\n  \"arguments\": {\r\n    \"query\": \"your search query\",\r\n    \"num_results\": 5, // optional, default: 5\r\n    \"site\": \"example.com\", // optional, limit results to specific website\r\n    \"language\": \"en\", // optional, filter by language (ISO 639-1 code)\r\n    \"dateRestrict\": \"m6\", // optional, filter by date (e.g., \"m6\" for last 6 months)\r\n    \"exactTerms\": \"exact phrase\", // optional, search for exact phrase\r\n    \"resultType\": \"news\", // optional, specify type (news, images, videos)\r\n    \"page\": 2, // optional, page number for pagination (starts at 1)\r\n    \"resultsPerPage\": 10, // optional, results per page (max: 10)\r\n    \"sort\": \"date\" // optional, sort by \"date\" or \"relevance\" (default)\r\n  }\r\n}\r\n```\r\n\r\nResponse includes:\r\n- Search results with title, link, snippet in a readable format\r\n- Pagination information (current page, total results, etc.)\r\n- Categories of results (automatically detected)\r\n- Navigation hints for pagination\r\n\r\n### 2. extract_webpage_content\r\nExtract and analyze content from a webpage, converting it to readable text. This tool fetches the main content while removing ads, navigation elements, and other clutter.\r\n\r\n```typescript\r\n{\r\n  \"name\": \"extract_webpage_content\",\r\n  \"arguments\": {\r\n    \"url\": \"https://example.com\",\r\n    \"format\": \"markdown\" // optional, format options: \"markdown\" (default), \"html\", or \"text\"\r\n  }\r\n}\r\n```\r\n\r\nResponse includes:\r\n- Title and description of the webpage\r\n- Content statistics (word count, character count)\r\n- Content summary\r\n- Content preview (first 500 characters)\r\n\r\n### 3. extract_multiple_webpages\r\nExtract and analyze content from multiple webpages in a single request. Ideal for comparing information across different sources or gathering comprehensive information on a topic.\r\n\r\n```typescript\r\n{\r\n  \"name\": \"extract_multiple_webpages\",\r\n  \"arguments\": {\r\n    \"urls\": [\r\n      \"https://example1.com\",\r\n      \"https://example2.com\"\r\n    ],\r\n    \"format\": \"html\" // optional, format options: \"markdown\" (default), \"html\", or \"text\"\r\n  }\r\n}\r\n```\r\n\r\nResponse includes:\r\n- Title and description of each webpage\r\n- Content statistics for each webpage\r\n- Content summary for each webpage\r\n- Content preview for each webpage (first 150 characters)\r\n\r\n## Getting Google API Credentials\r\n\r\n1. Go to the [Google Cloud Console](https://console.cloud.google.com/)\r\n2. Create a new project or select an existing one\r\n3. Enable the Custom Search API\r\n4. Create API credentials (API Key)\r\n5. Go to the [Custom Search Engine](https://programmablesearchengine.google.com/about/) page\r\n6. Create a new search engine and get your Search Engine ID\r\n7. Add these credentials to your MCP settings file or set them as environment variables\r\n\r\n## Error Handling\r\n\r\nThe server provides detailed error messages for:\r\n- Missing or invalid API credentials\r\n- Failed search requests\r\n- Invalid webpage URLs\r\n- Network connectivity issues\r\n\r\n## Architecture\r\n\r\nThe server is built with TypeScript and uses the MCP SDK to provide a standardized interface for AI models to interact with Google Search and webpage content analysis tools. It consists of two main services:\r\n\r\n1. **GoogleSearchService**: Handles Google API interactions for search functionality\r\n2. **ContentExtractor**: Manages webpage content analysis and extraction\r\n\r\nThe server uses caching mechanisms to improve performance and reduce API calls.\r\n\r\n## Distributing the Built Version\r\n\r\nIf you prefer to distribute only the built version of this tool rather than the source code, you can follow these steps:\r\n\r\n1. Build the TypeScript code:\r\n```bash\r\nnpm run build\r\n```\r\n\r\n2. Create a distribution package with only the necessary files:\r\n```bash\r\n# Create a distribution directory\r\nmkdir -p dist-package\r\n\r\n# Copy the compiled JavaScript files\r\ncp -r dist dist-package/\r\n\r\n# Copy package files (without dev dependencies)\r\ncp package.json dist-package/\r\ncp README.md dist-package/\r\n\r\n# Create a simplified package.json for distribution\r\nnode -e \"const pkg = require('./package.json'); delete pkg.devDependencies; delete pkg.scripts.build; delete pkg.scripts.dev; pkg.scripts.start = 'node dist/google-search.js'; require('fs').writeFileSync('dist-package/package.json', JSON.stringify(pkg, null, 2));\"\r\n```\r\n\r\n3. Users can then install and run the built version:\r\n```bash\r\n# Install production dependencies only\r\nnpm install --production\r\n\r\n# Start the server\r\nnpm start\r\n```\r\n\r\nThis approach allows you to distribute the compiled JavaScript files without exposing the TypeScript source code. Users will still need to:\r\n\r\n1. Configure their Google API credentials as environment variables\r\n2. Add the server configuration to their MCP settings file\r\n3. Install the production dependencies\r\n\r\nNote that the package.json in the distribution will only include production dependencies and a simplified set of scripts.\r\n\r\n## License\r\n\r\nMIT\r\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "google",
        "search",
        "webpage",
        "web search",
        "search mixelpixx",
        "search results"
      ],
      "category": "web-search"
    },
    "mkusaka--mcp-server-perplexity": {
      "owner": "mkusaka",
      "name": "mcp-server-perplexity",
      "url": "https://github.com/mkusaka/mcp-server-perplexity",
      "imageUrl": "/freedevtools/mcp/pfp/mkusaka.webp",
      "description": "Integrates Perplexity AI's API for advanced search capabilities, supporting multiple models with configurable result counts and detailed error handling.",
      "stars": 1,
      "forks": 5,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-02-11T07:12:56Z",
      "readme_content": "# Perplexity AI MCP Server\n\nAn MCP server implementation that integrates Perplexity AI's API, providing advanced search capabilities with multiple model options.\n\n## Features\n\n- Search using Perplexity AI's models\n- Support for all official Sonar models\n- Configurable result count\n- Detailed error handling and logging\n- MCP Inspector compatible\n\n## Available Models\n\n- sonar-reasoning-pro (127k context)\n- sonar-reasoning (127k context)\n- sonar-pro (200k context)\n- sonar (127k context)\n\n## Installation\n\n```bash\npnpm install\npnpm build\n```\n\n## Configuration\n\n### API Key\n1. Sign up for a [Perplexity AI account](https://www.perplexity.ai)\n2. Get your API key from the dashboard\n3. Set the environment variable:\n```bash\nexport PERPLEXITY_API_KEY=your_api_key_here\n```\n\n## Usage\n\n### Direct Execution\n```bash\nnode dist/index.js\n# or if you made it executable\n./dist/index.js\n```\n\n### Development\n```bash\npnpm dev\n```\n\n### Testing with MCP Inspector\n```bash\npnpm inspect\n```\n\n## Tool Reference\n\n### perplexity_search\n\nPerforms a search using Perplexity AI's models.\n\nParameters:\n- `query` (string, required): The search query\n- `model` (string, optional): Model to use (default: \"sonar\")\n  - Available options: sonar-reasoning-pro, sonar-reasoning, sonar-pro, sonar\n- `count` (number, optional): Number of results (1-10, default: 5)\n\n## Development\n\n### Project Structure\n```\nsrc/perplexity/\n├── index.ts        # Main server implementation\n├── lib/\n│   └── logger.ts   # Logging configuration\n└── adr.md         # Architectural decisions\n```\n\n### Logging\nLogs are written to `perplexity-mcp.log` in the project root directory.\n\n## License\n\nMIT\n\n## Contributing\n\n1. Fork the repository\n2. Create your feature branch\n3. Commit your changes\n4. Push to the branch\n5. Create a new Pull Request\n",
      "npm_url": "https://www.npmjs.com/package/mcp-server-perplexity",
      "npm_downloads": 934,
      "keywords": [
        "search",
        "perplexity",
        "ai",
        "perplexity ai",
        "search capabilities",
        "ai api"
      ],
      "category": "web-search"
    },
    "mmmaaatttttt--mcp-live-events": {
      "owner": "mmmaaatttttt",
      "name": "mcp-live-events",
      "url": "https://github.com/mmmaaatttttt/mcp-live-events",
      "imageUrl": "/freedevtools/mcp/pfp/mmmaaatttttt.webp",
      "description": "Fetch real-time event data from Ticketmaster's API and format responses for easy interpretation by AI language models. Provides details on concerts and events based on user queries.",
      "stars": 2,
      "forks": 6,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-07-23T21:08:29Z",
      "readme_content": "# MCP Live Events Server\n\n`mcp-live-events` is a Model Context Protocol (MCP) server that integrates with\nthe Ticketmaster API to provide real-time event data. It allows AI agents to\nfetch concert and event details dynamically.\n\n## Features\n\n- 🎟️ Integrates with the Ticketmaster API to search for events\n- 🗣️ Formats API responses for ease of LLM interpretation\n\n## Setup\n\n### Prerequisites\n\nEnsure you have the following installed:\n\n- [uv](https://github.com/astral-sh/uv) (used for package management)\n- Python 3.13+\n- A [Ticketmaster](https://developer.ticketmaster.com/explore/) API key (free to\n  use, but rate limited)\n\n### Installation\n\n1. Clone the repository:\n\n    ```sh\n    git clone https://github.com/mmmaaatttttt/mcp-live-events.git\n    cd mcp-live-events\n    ```\n\n2. Install dependencies:\n\n    ```sh\n    uv venv\n    uv sync\n    ```\n\n3. Set up your environment variables, i.e. the Ticketmaster API key. This can\n   either be placed in a `.env` file in this repository, following the pattern\n   of the `.env.example` file, or it can be placed in an \"env\" section of this\n   server's configuration in your MCP client.\n\n   Note that on the Ticketmaster developer portal, the API key is named\n   \"Consumer Key.\"\n\n### Running the server\n\n```sh\nuv run mcp-live-events\n```\n\nIf it's successful, you should see `MCP Live Event server is running!` print to\nyour terminal.\n\n## Resources\n\n- [Introducing the Model Context Protocol](https://www.anthropic.com/news/model-context-protocol)\n- [Model Context Protocol Documentation](https://modelcontextprotocol.io/introduction)\n- [MCP Server Demo Quickstart](https://modelcontextprotocol.io/quickstart/server)\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "ticketmaster",
        "event",
        "events",
        "live events",
        "ticketmaster api",
        "events fetch"
      ],
      "category": "web-search"
    },
    "mnhlt--WebSearch-MCP": {
      "owner": "mnhlt",
      "name": "WebSearch-MCP",
      "url": "https://github.com/mnhlt/WebSearch-MCP",
      "imageUrl": "/freedevtools/mcp/pfp/mnhlt.webp",
      "description": "Provides real-time web search capabilities to retrieve up-to-date information from the internet using the WebSearch Crawler API, enabling AI assistants to access the latest web content.",
      "stars": 17,
      "forks": 6,
      "license": "No License",
      "language": "JavaScript",
      "updated_at": "2025-09-08T19:26:30Z",
      "readme_content": "# WebSearch-MCP\n\n[![smithery badge](https://smithery.ai/badge/@mnhlt/WebSearch-MCP)](https://smithery.ai/server/@mnhlt/WebSearch-MCP)\n\nA Model Context Protocol (MCP) server implementation that provides a web search capability over stdio transport. This server integrates with a WebSearch Crawler API to retrieve search results.\n\n## Table of Contents\n\n- [About](#about)\n- [Installation](#installation)\n- [Configuration](#configuration)\n- [Setup & Integration](#setup--integration)\n  - [Setting Up the Crawler Service](#setting-up-the-crawler-service)\n    - [Prerequisites](#prerequisites)\n    - [Starting the Crawler Service](#starting-the-crawler-service)\n    - [Testing the Crawler API](#testing-the-crawler-api)\n    - [Custom Configuration](#custom-configuration)\n  - [Integrating with MCP Clients](#integrating-with-mcp-clients)\n    - [Quick Reference: MCP Configuration](#quick-reference-mcp-configuration)\n    - [Claude Desktop](#claude-desktop)\n    - [Cursor IDE](#cursor-ide)\n    - [Cline](#cline-command-line-interface-for-claude)\n- [Usage](#usage)\n  - [Parameters](#parameters)\n  - [Example Search Response](#example-search-response)\n  - [Testing Locally](#testing-locally)\n  - [As a Library](#as-a-library)\n- [Troubleshooting](#troubleshooting)\n  - [Crawler Service Issues](#crawler-service-issues)\n  - [MCP Server Issues](#mcp-server-issues)\n- [Development](#development)\n  - [Project Structure](#project-structure)\n  - [Publishing to npm](#publishing-to-npm)\n- [Contributing](#contributing)\n- [License](#license)\n\n## About\n\nWebSearch-MCP is a Model Context Protocol server that provides web search capabilities to AI assistants that support MCP. It allows AI models like Claude to search the web in real-time, retrieving up-to-date information about any topic.\n\nThe server integrates with a Crawler API service that handles the actual web searches, and communicates with AI assistants using the standardized Model Context Protocol.\n\n## Installation\n\n### Installing via Smithery\n\nTo install WebSearch for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@mnhlt/WebSearch-MCP):\n\n```bash\nnpx -y @smithery/cli install @mnhlt/WebSearch-MCP --client claude\n```\n\n### Manual Installation\n```bash\nnpm install -g websearch-mcp\n```\n\nOr use without installing:\n\n```bash\nnpx websearch-mcp\n```\n\n## Configuration\n\nThe WebSearch MCP server can be configured using environment variables:\n\n- `API_URL`: The URL of the WebSearch Crawler API (default: `http://localhost:3001`)\n- `MAX_SEARCH_RESULT`: Maximum number of search results to return when not specified in the request (default: `5`)\n\nExamples:\n```bash\n# Configure API URL\nAPI_URL=https://crawler.example.com npx websearch-mcp\n\n# Configure maximum search results\nMAX_SEARCH_RESULT=10 npx websearch-mcp\n\n# Configure both\nAPI_URL=https://crawler.example.com MAX_SEARCH_RESULT=10 npx websearch-mcp\n```\n\n## Setup & Integration\n\nSetting up WebSearch-MCP involves two main parts: configuring the crawler service that performs the actual web searches, and integrating the MCP server with your AI client applications.\n\n### Setting Up the Crawler Service\n\nThe WebSearch MCP server requires a crawler service to perform the actual web searches. You can easily set up the crawler service using Docker Compose.\n\n### Prerequisites\n\n- [Docker](https://www.docker.com/get-started) and [Docker Compose](https://docs.docker.com/compose/install/)\n\n### Starting the Crawler Service\n\n1. Create a file named `docker-compose.yml` with the following content:\n\n```yaml\nversion: '3.8'\n\nservices:\n  crawler:\n    image: laituanmanh/websearch-crawler:latest\n    container_name: websearch-api\n    restart: unless-stopped\n    ports:\n      - \"3001:3001\"\n    environment:\n      - NODE_ENV=production\n      - PORT=3001\n      - LOG_LEVEL=info\n      - FLARESOLVERR_URL=http://flaresolverr:8191/v1\n    depends_on:\n      - flaresolverr\n    volumes:\n      - crawler_storage:/app/storage\n\n  flaresolverr:\n    image: 21hsmw/flaresolverr:nodriver\n    container_name: flaresolverr\n    restart: unless-stopped\n    environment:\n      - LOG_LEVEL=info\n      - TZ=UTC\n\nvolumes:\n  crawler_storage:\n```\nworkaround for Mac Apple Silicon\n```\nversion: '3.8'\n\nservices:\n  crawler:\n    image: laituanmanh/websearch-crawler:latest\n    container_name: websearch-api\n    platform: \"linux/amd64\"\n    restart: unless-stopped\n    ports:\n      - \"3001:3001\"\n    environment:\n      - NODE_ENV=production\n      - PORT=3001\n      - LOG_LEVEL=info\n      - FLARESOLVERR_URL=http://flaresolverr:8191/v1\n    depends_on:\n      - flaresolverr\n    volumes:\n      - crawler_storage:/app/storage\n\n  flaresolverr:\n    image: 21hsmw/flaresolverr:nodriver\n    platform: \"linux/arm64\"\n    container_name: flaresolverr\n    restart: unless-stopped\n    environment:\n      - LOG_LEVEL=info\n      - TZ=UTC\n\nvolumes:\n  crawler_storage:\n```\n\n2. Start the services:\n\n```bash\ndocker-compose up -d\n```\n\n3. Verify that the services are running:\n\n```bash\ndocker-compose ps\n```\n\n4. Test the crawler API health endpoint:\n\n```bash\ncurl http://localhost:3001/health\n```\n\nExpected response:\n```json\n{\n  \"status\": \"ok\",\n  \"details\": {\n    \"status\": \"ok\",\n    \"flaresolverr\": true,\n    \"google\": true,\n    \"message\": null\n  }\n}\n```\n\nThe crawler API will be available at `http://localhost:3001`.\n\n### Testing the Crawler API\n\nYou can test the crawler API directly using curl:\n\n```bash\ncurl -X POST http://localhost:3001/crawl \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"query\": \"typescript best practices\",\n    \"numResults\": 2,\n    \"language\": \"en\",\n    \"filters\": {\n      \"excludeDomains\": [\"youtube.com\"],\n      \"resultType\": \"all\" \n    }\n  }'\n```\n\n### Custom Configuration\n\nYou can customize the crawler service by modifying the environment variables in the `docker-compose.yml` file:\n\n- `PORT`: The port on which the crawler API listens (default: 3001)\n- `LOG_LEVEL`: Logging level (options: debug, info, warn, error)\n- `FLARESOLVERR_URL`: URL of the FlareSolverr service (for bypassing Cloudflare protection)\n\n## Integrating with MCP Clients\n\n### Quick Reference: MCP Configuration\n\nHere's a quick reference for MCP configuration across different clients:\n\n```json\n{\n    \"mcpServers\": {\n        \"websearch\": {\n            \"command\": \"npx\",\n            \"args\": [\n                \"websearch-mcp\"\n            ],\n            \"environment\": {\n                \"API_URL\": \"http://localhost:3001\",\n                \"MAX_SEARCH_RESULT\": \"5\" // reduce to save your tokens, increase for wider information gain\n            }\n        }\n    }\n}\n```\n\nWorkaround for Windows, due to [Issue](https://github.com/smithery-ai/mcp-obsidian/issues/19)\n```\n{\n\t\"mcpServers\": {\n\t  \"websearch\": {\n            \"command\": \"cmd\",\n            \"args\": [\n\t\t\t\t\"/c\",\n\t\t\t\t\"npx\",\n                \"websearch-mcp\"\n            ],\n            \"environment\": {\n                \"API_URL\": \"http://localhost:3001\",\n                \"MAX_SEARCH_RESULT\": \"1\"\n            }\n        }\n\t}\n  }\n```\n\n## Usage\n\nThis package implements an MCP server using stdio transport that exposes a `web_search` tool with the following parameters:\n\n### Parameters\n\n- `query` (required): The search query to look up\n- `numResults` (optional): Number of results to return (default: 5)\n- `language` (optional): Language code for search results (e.g., 'en')\n- `region` (optional): Region code for search results (e.g., 'us')\n- `excludeDomains` (optional): Domains to exclude from results\n- `includeDomains` (optional): Only include these domains in results\n- `excludeTerms` (optional): Terms to exclude from results\n- `resultType` (optional): Type of results to return ('all', 'news', or 'blogs')\n\n### Example Search Response\n\nHere's an example of a search response:\n\n```json\n{\n  \"query\": \"machine learning trends\",\n  \"results\": [\n    {\n      \"title\": \"Top Machine Learning Trends in 2025\",\n      \"snippet\": \"The key machine learning trends for 2025 include multimodal AI, generative models, and quantum machine learning applications in enterprise...\",\n      \"url\": \"https://example.com/machine-learning-trends-2025\",\n      \"siteName\": \"AI Research Today\",\n      \"byline\": \"Dr. Jane Smith\"\n    },\n    {\n      \"title\": \"The Evolution of Machine Learning: 2020-2025\",\n      \"snippet\": \"Over the past five years, machine learning has evolved from primarily supervised learning approaches to more sophisticated self-supervised and reinforcement learning paradigms...\",\n      \"url\": \"https://example.com/ml-evolution\",\n      \"siteName\": \"Tech Insights\",\n      \"byline\": \"John Doe\"\n    }\n  ]\n}\n```\n\n### Testing Locally\n\nTo test the WebSearch MCP server locally, you can use the included test client:\n\n```bash\nnpm run test-client\n```\n\nThis will start the MCP server and a simple command-line interface that allows you to enter search queries and see the results.\n\nYou can also configure the API_URL for the test client:\n\n```bash\nAPI_URL=https://crawler.example.com npm run test-client\n```\n\n### As a Library\n\nYou can use this package programmatically:\n\n```typescript\nimport { createMCPClient } from '@modelcontextprotocol/sdk';\n\n// Create an MCP client\nconst client = createMCPClient({\n  transport: { type: 'subprocess', command: 'npx websearch-mcp' }\n});\n\n// Execute a web search\nconst response = await client.request({\n  method: 'call_tool',\n  params: {\n    name: 'web_search',\n    arguments: {\n      query: 'your search query',\n      numResults: 5,\n      language: 'en'\n    }\n  }\n});\n\nconsole.log(response.result);\n```\n\n## Troubleshooting\n\n### Crawler Service Issues\n\n- **API Unreachable**: Ensure that the crawler service is running and accessible at the configured API_URL.\n- **Search Results Not Available**: Check the logs of the crawler service to see if there are any errors:\n  ```bash\n  docker-compose logs crawler\n  ```\n- **FlareSolverr Issues**: Some websites use Cloudflare protection. If you see errors related to this, check if FlareSolverr is working:\n  ```bash\n  docker-compose logs flaresolverr\n  ```\n\n### MCP Server Issues\n\n- **Import Errors**: Ensure you have the latest version of the MCP SDK:\n  ```bash\n  npm install -g @modelcontextprotocol/sdk@latest\n  ```\n- **Connection Issues**: Make sure the stdio transport is properly configured for your client.\n\n## Development\n\nTo work on this project:\n\n1. Clone the repository\n2. Install dependencies: `npm install`\n3. Build the project: `npm run build`\n4. Run in development mode: `npm run dev`\n\nThe server expects a WebSearch Crawler API as defined in the included swagger.json file. Make sure the API is running at the configured API_URL.\n\n### Project Structure\n\n- `.gitignore`: Specifies files that Git should ignore (node_modules, dist, logs, etc.)\n- `.npmignore`: Specifies files that shouldn't be included when publishing to npm\n- `package.json`: Project metadata and dependencies\n- `src/`: Source TypeScript files\n- `dist/`: Compiled JavaScript files (generated when building)\n\n### Publishing to npm\n\nTo publish this package to npm:\n\n1. Make sure you have an npm account and are logged in (`npm login`)\n2. Update the version in package.json (`npm version patch|minor|major`)\n3. Run `npm publish`\n\nThe `.npmignore` file ensures that only the necessary files are included in the published package:\n- The compiled code in `dist/`\n- README.md and LICENSE files\n- package.json\n\n## Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n\n## License\n\nISC\n",
      "npm_url": "https://www.npmjs.com/package/websearch-mcp",
      "npm_downloads": 5853,
      "keywords": [
        "websearch",
        "search",
        "web",
        "websearch mcp",
        "mnhlt websearch",
        "web search"
      ],
      "category": "web-search"
    },
    "modelcontextprotocol--servers": {
      "owner": "modelcontextprotocol",
      "name": "servers",
      "url": "https://github.com/modelcontextprotocol/servers",
      "imageUrl": "/freedevtools/mcp/pfp/modelcontextprotocol.webp",
      "description": "Enables retrieval and processing of web content by fetching web pages and converting HTML to markdown for easier manipulation. Supports content extraction in chunks from specified starting indices to facilitate information retrieval from larger documents.",
      "stars": 69425,
      "forks": 8223,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-10-03T09:28:41Z",
      "readme_content": "# Model Context Protocol servers\n\nThis repository is a collection of *reference implementations* for the [Model Context Protocol](https://modelcontextprotocol.io/) (MCP), as well as references to community-built servers and additional resources.\n\nThe servers in this repository showcase the versatility and extensibility of MCP, demonstrating how it can be used to give Large Language Models (LLMs) secure, controlled access to tools and data sources.\nTypically, each MCP server is implemented with an MCP SDK:\n\n- [C# MCP SDK](https://github.com/modelcontextprotocol/csharp-sdk)\n- [Go MCP SDK](https://github.com/modelcontextprotocol/go-sdk)\n- [Java MCP SDK](https://github.com/modelcontextprotocol/java-sdk)\n- [Kotlin MCP SDK](https://github.com/modelcontextprotocol/kotlin-sdk)\n- [PHP MCP SDK](https://github.com/modelcontextprotocol/php-sdk)\n- [Python MCP SDK](https://github.com/modelcontextprotocol/python-sdk)\n- [Ruby MCP SDK](https://github.com/modelcontextprotocol/ruby-sdk)\n- [Rust MCP SDK](https://github.com/modelcontextprotocol/rust-sdk)\n- [Swift MCP SDK](https://github.com/modelcontextprotocol/swift-sdk)\n- [TypeScript MCP SDK](https://github.com/modelcontextprotocol/typescript-sdk)\n\n> [!NOTE]\n> Lists in this README are maintained in alphabetical order to minimize merge conflicts when adding new items.\n\n## 🌟 Reference Servers\n\nThese servers aim to demonstrate MCP features and the official SDKs.\n\n- **[Everything](src/everything)** - Reference / test server with prompts, resources, and tools.\n- **[Fetch](src/fetch)** - Web content fetching and conversion for efficient LLM usage.\n- **[Filesystem](src/filesystem)** - Secure file operations with configurable access controls.\n- **[Git](src/git)** - Tools to read, search, and manipulate Git repositories.\n- **[Memory](src/memory)** - Knowledge graph-based persistent memory system.\n- **[Sequential Thinking](src/sequentialthinking)** - Dynamic and reflective problem-solving through thought sequences.\n- **[Time](src/time)** - Time and timezone conversion capabilities.\n\n### Archived\n\nThe following reference servers are now archived and can be found at [servers-archived](https://github.com/modelcontextprotocol/servers-archived).\n\n- **[AWS KB Retrieval](https://github.com/modelcontextprotocol/servers-archived/tree/main/src/aws-kb-retrieval-server)** - Retrieval from AWS Knowledge Base using Bedrock Agent Runtime.\n- **[Brave Search](https://github.com/modelcontextprotocol/servers-archived/tree/main/src/brave-search)** - Web and local search using Brave's Search API.  Has been replaced by the [official server](https://github.com/brave/brave-search-mcp-server).\n- **[EverArt](https://github.com/modelcontextprotocol/servers-archived/tree/main/src/everart)** - AI image generation using various models.\n- **[GitHub](https://github.com/modelcontextprotocol/servers-archived/tree/main/src/github)** - Repository management, file operations, and GitHub API integration.\n- **[GitLab](https://github.com/modelcontextprotocol/servers-archived/tree/main/src/gitlab)** - GitLab API, enabling project management.\n- **[Google Drive](https://github.com/modelcontextprotocol/servers-archived/tree/main/src/gdrive)** - File access and search capabilities for Google Drive.\n- **[Google Maps](https://github.com/modelcontextprotocol/servers-archived/tree/main/src/google-maps)** - Location services, directions, and place details.\n- **[PostgreSQL](https://github.com/modelcontextprotocol/servers-archived/tree/main/src/postgres)** - Read-only database access with schema inspection.\n- **[Puppeteer](https://github.com/modelcontextprotocol/servers-archived/tree/main/src/puppeteer)** - Browser automation and web scraping.\n- **[Redis](https://github.com/modelcontextprotocol/servers-archived/tree/main/src/redis)** - Interact with Redis key-value stores.\n- **[Sentry](https://github.com/modelcontextprotocol/servers-archived/tree/main/src/sentry)** - Retrieving and analyzing issues from Sentry.io.\n- **[Slack](https://github.com/modelcontextprotocol/servers-archived/tree/main/src/slack)** - Channel management and messaging capabilities. Now maintained by [Zencoder](https://github.com/zencoderai/slack-mcp-server)\n- **[SQLite](https://github.com/modelcontextprotocol/servers-archived/tree/main/src/sqlite)** - Database interaction and business intelligence capabilities.\n\n## 🤝 Third-Party Servers\n\n### 🎖️ Official Integrations\n\nOfficial integrations are maintained by companies building production ready MCP servers for their platforms.\n\n- <img height=\"12\" width=\"12\" src=\"https://www.21st.dev/favicon.ico\" alt=\"21st.dev Logo\" /> **[21st.dev Magic](https://github.com/21st-dev/magic-mcp)** - Create crafted UI components inspired by the best 21st.dev design engineers.\n- <img height=\"12\" width=\"12\" src=\"https://framerusercontent.com/images/LpSK1tSZweomrAHOMAj9Gea96lA.svg\" alt=\"Paragon Logo\" /> **[ActionKit by Paragon](https://github.com/useparagon/paragon-mcp)** - Connect to 130+ SaaS integrations (e.g. Slack, Salesforce, Gmail) with Paragon’s [ActionKit](https://www.useparagon.com/actionkit) API.\n- <img height=\"12\" width=\"12\" src=\"https://invoxx-public-bucket.s3.eu-central-1.amazonaws.com/frontend-resources/adfin-logo-small.svg\" alt=\"Adfin Logo\" /> **[Adfin](https://github.com/Adfin-Engineering/mcp-server-adfin)** - The only platform you need to get paid - all payments in one place, invoicing and accounting reconciliations with [Adfin](https://www.adfin.com/).\n- <img height=\"12\" width=\"12\" src=\"https://github.com/AgentOps-AI/agentops/blob/main/docs/favicon.png\" alt=\"AgentOps Logo\" /> **[AgentOps](https://github.com/AgentOps-AI/agentops-mcp)** - Provide observability and tracing for debugging AI agents with [AgentOps](https://www.agentops.ai/) API.\n- <img height=\"12\" width=\"12\" src=\"https://www.agentql.com/favicon/favicon.png\" alt=\"AgentQL Logo\" /> **[AgentQL](https://github.com/tinyfish-io/agentql-mcp)** - Enable AI agents to get structured data from unstructured web with [AgentQL](https://www.agentql.com/).\n- <img height=\"12\" width=\"12\" src=\"https://agentrpc.com/favicon.ico\" alt=\"AgentRPC Logo\" /> **[AgentRPC](https://github.com/agentrpc/agentrpc)** - Connect to any function, any language, across network boundaries using [AgentRPC](https://www.agentrpc.com/).\n- **[Agentset](https://github.com/agentset-ai/mcp-server)** - RAG for your knowledge base connected to [Agentset](https://agentset.ai).\n- <img height=\"12\" width=\"12\" src=\"https://aiven.io/favicon.ico\" alt=\"Aiven Logo\" /> **[Aiven](https://github.com/Aiven-Open/mcp-aiven)** - Navigate your [Aiven projects](https://go.aiven.io/mcp-server) and interact with the PostgreSQL®, Apache Kafka®, ClickHouse® and OpenSearch® services\n- <img height=\"12\" width=\"12\" src=\"https://www.alation.com/resource-center/download/7p3vnbbznfiw/34FMtBTex5ppvs2hNYa9Fc/c877c37e88e5339878658697c46d2d58/Alation-Logo-Bug-Primary.svg\" alt=\"Alation Logo\" /> **[Alation](https://github.com/Alation/alation-ai-agent-sdk)** - Unlock the power of the enterprise Data Catalog by harnessing tools provided by the Alation MCP server.\n- <img height=\"12\" width=\"12\" src=\"https://i.postimg.cc/5NYw9qjS/alby-icon-head-yellow-500x500.png\" alt=\"Alby Logo\" /> **[Alby Bitcoin Payments](https://github.com/getAlby/mcp)** - Connect any bitcoin lightning wallet to your agent to send and receive instant payments globally with your agent.\n- **[Algolia](https://github.com/algolia/mcp)** - Use AI agents to provision, configure, and query your [Algolia](https://algolia.com) search indices.\n- <img height=\"12\" width=\"12\" src=\"https://img.alicdn.com/imgextra/i4/O1CN01epkXwH1WLAXkZfV6N_!!6000000002771-2-tps-200-200.png\" alt=\"Alibaba Cloud AnalyticDB for MySQL Logo\" /> **[Alibaba Cloud AnalyticDB for MySQL](https://github.com/aliyun/alibabacloud-adb-mysql-mcp-server)** - Connect to an [AnalyticDB for MySQL](https://www.alibabacloud.com/en/product/analyticdb-for-mysql) cluster for getting database or table metadata, querying and analyzing data. It will be supported to add the OpenAPI for cluster operation in the future.\n- <img height=\"12\" width=\"12\" src=\"https://github.com/aliyun/alibabacloud-adbpg-mcp-server/blob/master/images/AnalyticDB.png\" alt=\"Alibaba Cloud AnalyticDB for PostgreSQL Logo\" /> **[Alibaba Cloud AnalyticDB for PostgreSQL](https://github.com/aliyun/alibabacloud-adbpg-mcp-server)** - An MCP server to connect to [AnalyticDB for PostgreSQL](https://github.com/aliyun/alibabacloud-adbpg-mcp-server) instances, query and analyze data.\n- <img height=\"12\" width=\"12\" src=\"https://img.alicdn.com/imgextra/i3/O1CN0101UWWF1UYn3rAe3HU_!!6000000002530-2-tps-32-32.png\" alt=\"DataWorks Logo\" /> **[Alibaba Cloud DataWorks](https://github.com/aliyun/alibabacloud-dataworks-mcp-server)** - A Model Context Protocol (MCP) server that provides tools for AI, allowing it to interact with the [DataWorks](https://www.alibabacloud.com/help/en/dataworks/) Open API through a standardized interface. This implementation is based on the Alibaba Cloud Open API and enables AI agents to perform cloud resources operations seamlessly.\n- <img height=\"12\" width=\"12\" src=\"https://opensearch-shanghai.oss-cn-shanghai.aliyuncs.com/ouhuang/aliyun-icon.png\" alt=\"Alibaba Cloud OpenSearch Logo\" /> **[Alibaba Cloud OpenSearch](https://github.com/aliyun/alibabacloud-opensearch-mcp-server)** - This MCP server equips AI Agents with tools to interact with [OpenSearch](https://help.aliyun.com/zh/open-search/?spm=5176.7946605.J_5253785160.6.28098651AaYZXC) through a standardized and extensible interface.\n- <img height=\"12\" width=\"12\" src=\"https://github.com/aliyun/alibaba-cloud-ops-mcp-server/blob/master/image/alibaba-cloud.png\" alt=\"Alibaba Cloud OPS Logo\" /> **[Alibaba Cloud OPS](https://github.com/aliyun/alibaba-cloud-ops-mcp-server)** - Manage the lifecycle of your Alibaba Cloud resources with [CloudOps Orchestration Service](https://www.alibabacloud.com/en/product/oos) and Alibaba Cloud OpenAPI.\n- <img height=\"12\" width=\"12\" src=\"https://github.com/aliyun/alibabacloud-rds-openapi-mcp-server/blob/main/assets/alibabacloudrds.png\" alt=\"Alibaba Cloud RDS MySQL Logo\" /> **[Alibaba Cloud RDS](https://github.com/aliyun/alibabacloud-rds-openapi-mcp-server)** - An MCP server designed to interact with the Alibaba Cloud RDS OpenAPI, enabling programmatic management of RDS resources via an LLM.\n- <img height=\"12\" width=\"12\" src=\"https://www.alipayplus.com/favicon.ico\" alt=\"AlipayPlus Logo\" /> **[AlipayPlus](https://github.com/alipay/global-alipayplus-mcp)** - Connect your AI Agents to AlipayPlus Checkout Payment.\n- <img height=\"12\" width=\"12\" src=\"https://cdn.allvoicelab.com/resources/workbench/dist/icon-dark.ico\" alt=\"AllVoiceLab Logo\" /> **[AllVoiceLab](https://www.allvoicelab.com/mcp)** - An AI voice toolkit with TTS, voice cloning, and video translation, now available as an MCP server for smarter agent integration.\n- <img height=\"12\" width=\"12\" src=\"https://files.alpaca.markets/webassets/favicon-32x32.png\" alt=\"Alpaca Logo\" /> **[Alpaca](https://github.com/alpacahq/alpaca-mcp-server)** – Alpaca's MCP server lets you trade stocks and options, analyze market data, and build strategies through [Alpaca's Trading API](https://alpaca.markets/)\n- <img height=\"12\" width=\"12\" src=\"https://www.alphavantage.co/logo.png/\" alt=\"AlphaVantage Logo\" /> **[AlphaVantage](https://mcp.alphavantage.co/)** - Connect to 100+ APIs for financial market data, including stock prices, fundamentals, and more from [AlphaVantage](https://www.alphavantage.co)\n- <img height=\"12\" width=\"12\" src=\"https://alttester.com/app/themes/alttester-sage-theme/public/images/logo-alttester.038ec8.png\" alt=\"AltTester Logo\" /> **[AltTester®](https://alttester.com/docs/desktop/latest/pages/ai-extension.html)** - Use AltTester® capabilities to connect and test your Unity or Unreal game. Write game test automation faster and smarter, using [AltTester](https://alttester.com) and the AltTester® MCP server. \n- <img height=\"12\" width=\"12\" src=\"https://www.antom.com/favicon.ico\" alt=\"Antom Logo\" /> **[Antom](https://github.com/alipay/global-antom-mcp)** - Connect your AI Agents to Antom Checkout Payment.\n- <img height=\"12\" width=\"12\" src=\"https://developers.anytype.io/img/favicon.ico\" alt=\"Anytype Logo\" /> **[Anytype](https://github.com/anyproto/anytype-mcp)** - An MCP server enabling AI assistants to interact with [Anytype](https://anytype.io) - a local and collaborative wiki - to organize objects, lists, and more through natural language.\n- <img height=\"12\" width=\"12\" src=\"https://doris.apache.org/images/favicon.ico\" alt=\"Apache Doris Logo\" /> **[Apache Doris](https://github.com/apache/doris-mcp-server)** - MCP Server For [Apache Doris](https://doris.apache.org/), an MPP-based real-time data warehouse.\n- <img height=\"12\" width=\"12\" src=\"https://iotdb.apache.org/img/logo.svg\" alt=\"Apache IoTDB Logo\" /> **[Apache IoTDB](https://github.com/apache/iotdb-mcp-server)** - MCP Server for [Apache IoTDB](https://github.com/apache/iotdb) database and its tools\n- **[Apache Pinot](https://github.com/startreedata/mcp-pinot)** – MCP server for running real - time analytics queries on Apache Pinot, an open-source OLAP database built for high-throughput, low-latency powering real-time applications.\n- <img height=\"12\" width=\"12\" src=\"https://apify.com/favicon.ico\" alt=\"Apify Logo\" /> **[Apify](https://github.com/apify/apify-mcp-server)** - Use 6,000+ pre-built cloud tools to extract data from websites, e-commerce, social media, search engines, maps, and more\n- <img height=\"12\" width=\"12\" src=\"https://2052727.fs1.hubspotusercontent-na1.net/hubfs/2052727/cropped-cropped-apimaticio-favicon-1-32x32.png\" alt=\"APIMatic Logo\" /> **[APIMatic MCP](https://github.com/apimatic/apimatic-validator-mcp)** - APIMatic MCP Server is used to validate OpenAPI specifications using [APIMatic](https://www.apimatic.io/). The server processes OpenAPI files and returns validation summaries by leveraging APIMatic's API.\n- <img height=\"12\" width=\"12\" src=\"https://apollo-server-landing-page.cdn.apollographql.com/_latest/assets/favicon.png\" alt=\"Apollo Graph Logo\" /> **[Apollo MCP Server](https://github.com/apollographql/apollo-mcp-server/)** - Connect your GraphQL APIs to AI agents\n- <img height=\"12\" width=\"12\" src=\"https://developer.aqara.com/favicon.ico\" alt=\"Aqara Logo\" /> **[Aqara MCP Server](https://github.com/aqara/aqara-mcp-server/)** - Control  [Aqara](https://www.aqara.com/) smart home devices, query status, execute scenes, and much more using natural language.\n- <img height=\"12\" width=\"12\" src=\"https://media.licdn.com/dms/image/v2/C4D0BAQEeD7Dxbpadkw/company-logo_200_200/company-logo_200_200/0/1644692667545/archbee_logo?e=2147483647&v=beta&t=lTi9GRIoqzG6jN3kJC26uZWh0q3uiQelsH6mGoq_Wfw\" alt=\"Archbee Logo\" /> **[Archbee](https://www.npmjs.com/package/@archbee/mcp)** - Write and publish documentation that becomes the trusted source for instant answers with AI. Stop cobbling tools and use [Archbee](https://www.archbee.com/) — the first complete documentation platform.\n- <img height=\"12\" width=\"12\" src=\"https://phoenix.arize.com/wp-content/uploads/2023/04/cropped-Favicon-32x32.png\" alt=\"Arize-Phoenix Logo\" /> **[Arize Phoenix](https://github.com/Arize-ai/phoenix/tree/main/js/packages/phoenix-mcp)** - Inspect traces, manage prompts, curate datasets, and run experiments using [Arize Phoenix](https://github.com/Arize-ai/phoenix), an open-source AI and LLM observability tool.\n- <img height=\"12\" width=\"12\" src=\"https://731523176-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FaVUBXRZbpAgtjYf5HsvO%2Fuploads%2FaRRrVVocXCTr6GkepfCx%2Flogo_color.svg?alt=media&token=3ba24089-0ab2-421f-a9d9-41f2f94f954a\" alt=\"Armor Logo\" /> **[Armor Crypto MCP](https://github.com/armorwallet/armor-crypto-mcp)** - MCP to interface with multiple blockchains, staking, DeFi, swap, bridging, wallet management, DCA, Limit Orders, Coin Lookup, Tracking and more.\n- <img height=\"12\" width=\"12\" src=\"https://console.asgardeo.io/app/libs/themes/wso2is/assets/images/branding/favicon.ico\" alt=\"Asgardeo Logo\" /> **[Asgardeo](https://github.com/asgardeo/asgardeo-mcp-server)** - MCP server to interact with your [Asgardeo](https://wso2.com/asgardeo) organization through LLM tools.\n- <img height=\"12\" width=\"12\" src=\"https://www.datastax.com/favicon-32x32.png\" alt=\"DataStax logo\" /> **[Astra DB](https://github.com/datastax/astra-db-mcp)** - Comprehensive tools for managing collections and documents in a [DataStax Astra DB](https://www.datastax.com/products/datastax-astra) NoSQL database with a full range of operations such as create, update, delete, find, and associated bulk actions.\n- <img height=\"12\" width=\"12\" src=\"https://cdn.prod.website-files.com/66598898fd13d51606c3215d/66ccbfef13bd8bc19d587578_favicon-32x32.png\" alt=\"Atla Logo\" /> **[Atla](https://github.com/atla-ai/atla-mcp-server)** - Enable AI agents to interact with the [Atla API](https://docs.atla-ai.com/) for state-of-the-art LLMJ evaluation.\n- <img height=\"12\" width=\"12\" src=\"https://assets.atlan.com/assets/atlan-a-logo-blue-background.png\" alt=\"Atlan Logo\" /> **[Atlan](https://github.com/atlanhq/agent-toolkit/tree/main/modelcontextprotocol)** - The Atlan Model Context Protocol server allows you to interact with the [Atlan](https://www.atlan.com/) services through multiple tools.\n- <img height=\"12\" width=\"12\" src=\"https://www.atlassian.com/favicon.ico\" alt=\"Atlassian Logo\" /> **[Atlassian](https://www.atlassian.com/platform/remote-mcp-server)** - Securely interact with Jira work items and Confluence pages, and search across both.\n- <img height=\"12\" width=\"12\" src=\"https://res.oafimg.cn/-/737b3b3ffed9b19e/logo.png\" alt=\"AtomGit Logo\" /> **[AtomGit](https://atomgit.com/atomgit-open-source-ecosystem/atomgit-mcp-server)** - Official AtomGit server for integration with repository management, PRs, issues, branches, labels, and more.\n- <img height=\"12\" width=\"12\" src=\"https://resources.audiense.com/hubfs/favicon-1.png\" alt=\"Audiense Logo\" /> **[Audiense Insights](https://github.com/AudienseCo/mcp-audiense-insights)** - Marketing insights and audience analysis from [Audiense](https://www.audiense.com/products/audiense-insights) reports, covering demographic, cultural, influencer, and content engagement analysis.\n- <img height=\"12\" width=\"12\" src=\"https://cdn.auth0.com/website/website/favicons/auth0-favicon.svg\" alt=\"Auth0 Logo\" /> **[Auth0](https://github.com/auth0/auth0-mcp-server)** - MCP server for interacting with your Auth0 tenant, supporting creating and modifying actions, applications, forms, logs, resource servers, and more.\n- <img height=\"12\" width=\"12\" src=\"https://firstorder.ai/favicon_auth.ico\" alt=\"Authenticator App Logo\" /> **[Authenticator App · 2FA](https://github.com/firstorderai/authenticator_mcp)** - A secure MCP (Model Context Protocol) server that enables AI agents to interact with the Authenticator App.\n- <img height=\"12\" width=\"12\" src=\"https://a0.awsstatic.com/libra-css/images/site/fav/favicon.ico\" alt=\"AWS Logo\" /> **[AWS](https://github.com/awslabs/mcp)** -  Specialized MCP servers that bring AWS best practices directly to your development workflow.\n- <img height=\"12\" width=\"12\" src=\"https://axiom.co/favicon.ico\" alt=\"Axiom Logo\" /> **[Axiom](https://github.com/axiomhq/mcp-server-axiom)** - Query and analyze your Axiom logs, traces, and all other event data in natural language\n- <img height=\"12\" width=\"12\" src=\"https://cdn-dynmedia-1.microsoft.com/is/content/microsoftcorp/acom_social_icon_azure\" alt=\"Microsoft Azure Logo\" /> **[Azure](https://github.com/microsoft/mcp/tree/main/servers/Azure.Mcp.Server)** - The Azure MCP Server gives MCP Clients access to key Azure services and tools like Azure Storage, Cosmos DB, the Azure CLI, and more.\n- <img height=\"12\" width=\"12\" src=\"https://cdn-dynmedia-1.microsoft.com/is/content/microsoftcorp/1062064-Products-1.2-24x24\" alt=\"Microsoft Azure DevOps Logo\" /> **[Azure DevOps](https://github.com/microsoft/azure-devops-mcp)** - Interact with Azure DevOps services like repositories, work items, builds, releases, test plans, and code search.\n- <img height=\"12\" width=\"12\" src=\"https://application.backdocket.com/favicon.ico\" alt=\"Backdocket Logo\" /> **[Backdocket](https://ai.backdocket.com)** - Search, Retrieve, and Update your **[Backdocket](https://backdocket.com)** data. This currently includes Claims, Matters, Contacts, Tasks and Advanced Searches. To easily use the Remote Mcp Server utilize the following url: **[https://ai.backdocket.com/mcp]([https://backdocket.com](https://ai.backdocket.com/mcp))**\n- <img height=\"12\" width=\"12\" src=\"https://mapopen-website-wiki.cdn.bcebos.com/LOGO/lbsyunlogo_icon.ico\" alt=\"Baidu Map Logo\" /> **[Baidu Map](https://github.com/baidu-maps/mcp)** - [Baidu Map MCP Server](https://lbsyun.baidu.com/faq/api?title=mcpserver/base) provides tools for AI agents to interact with Baidu Maps APIs, enabling location-based services and geospatial data analysis.\n- <img height=\"12\" width=\"12\" src=\"https://www.bankless.com/favicon.ico\" alt=\"Bankless Logo\" /> **[Bankless Onchain](https://github.com/bankless/onchain-mcp)** - Query Onchain data, like ERC20 tokens, transaction history, smart contract state.\n- <img height=\"12\" width=\"12\" src=\"https://baserow.io/img/logo_baserow_square_large.png\" alt=\"Baserow Logo\" /> **[Baserow](https://gitlab.com/baserow/baserow/-/tree/develop/backend/src/baserow/api/mcp)** - Query data from Baserow self-hosted or SaaS databases using MCP integration.\n- <img height=\"12\" width=\"12\" src=\"https://bicscan.io/favicon.png\" alt=\"BICScan Logo\" /> **[BICScan](https://github.com/ahnlabio/bicscan-mcp)** - Risk score / asset holdings of EVM blockchain address (EOA, CA, ENS) and even domain names.\n- <img height=\"12\" width=\"12\" src=\"https://web-cdn.bitrise.io/favicon.ico\" alt=\"Bitrise Logo\" /> **[Bitrise](https://github.com/bitrise-io/bitrise-mcp)** - Chat with your builds, CI, and [more](https://bitrise.io/blog/post/chat-with-your-builds-ci-and-more-introducing-the-bitrise-mcp-server).\n- <img height=\"12\" width=\"12\" src=\"https://boikot.xyz/assets/favicon.svg\" alt=\"boikot Logo\" /> **[Boikot](https://github.com/boikot-xyz/boikot)** - Learn about the ethical and unethical actions of major companies with [boikot.xyz](https://boikot.xyz/).\n- <img height=\"12\" width=\"12\" src=\"https://boldsign.com/favicon.ico\" alt=\"BoldSign Logo\" /> **[BoldSign](https://github.com/boldsign/boldsign-mcp)** - Search, request, and manage e-signature contracts effortlessly with [BoldSign](https://boldsign.com/).\n- <img height=\"12\" width=\"12\" src=\"https://boost.space/favicon.ico\" alt=\"Boost.space Logo\" /> **[Boost.space](https://github.com/boostspace/boostspace-mcp-server)** - An MCP server integrating with [Boost.space](https://boost.space) for centralized, automated business data from 2000+ sources.\n- <img height=\"12\" width=\"12\" src=\"https://www.box.com/favicon.ico\" alt=\"Box Logo\" /> **[Box](https://github.com/box-community/mcp-server-box)** - Interact with the Intelligent Content Management platform through Box AI.\n- <img height=\"12\" width=\"12\" src=\"https://www.brightdata.com/favicon.ico\" alt=\"BrightData Logo\" /> **[BrightData](https://github.com/luminati-io/brightdata-mcp)** - Discover, extract, and interact with the web - one interface powering automated access across the public internet.\n- <img height=\"12\" width=\"12\" src=\"https://browserbase.com/favicon.ico\" alt=\"Browserbase Logo\" /> **[Browserbase](https://github.com/browserbase/mcp-server-browserbase)** - Automate browser interactions in the cloud (e.g. web navigation, data extraction, form filling, and more)\n- <img height=\"12\" width=\"12\" src=\"https://browserstack.wpenginepowered.com/wp-content/themes/browserstack/img/favicons/favicon.ico\" alt=\"BrowserStack Logo\" /> **[BrowserStack](https://github.com/browserstack/mcp-server)** - Access BrowserStack's [Test Platform](https://www.browserstack.com/test-platform) to debug, write and fix tests, do accessibility testing and more.\n- <img height=\"12\" width=\"12\" src=\"https://www.google.com/s2/favicons?domain=buildkite.com&sz=24\" alt=\"Buildkite Logo\" /> **[Buildkite](https://github.com/buildkite/buildkite-mcp-server)** - Exposing Buildkite data (pipelines, builds, jobs, tests) to AI tooling and editors.\n- <img height=\"12\" width=\"12\" src=\"https://bldbl.dev/favico.png\" alt=\"Buildable Logo\" />**[Buildable](https://github.com/chunkydotdev/bldbl-mcp)** (TypeScript) - Official MCP server for Buildable AI-powered development platform. Enables AI assistants to manage tasks, track progress, get project context, and collaborate with humans on software projects.\n- <img height=\"12\" width=\"12\" src=\"https://builtwith.com/favicon.ico\" alt=\"BuiltWith Logo\" /> **[BuiltWith](https://github.com/builtwith/mcp)** - Identify the technology stack behind any website.\n- <img height=\"12\" width=\"12\" src=\"https://portswigger.net/favicon.ico\" alt=\"PortSwigger Logo\" /> **[Burp Suite](https://github.com/PortSwigger/mcp-server)** - MCP Server extension allowing AI clients to connect to [Burp Suite](https://portswigger.net)\n- <img src=\"https://app.cal.com/favicon.ico\" alt=\"Cal.com\" width=\"12\" height=\"12\"> **[Cal.com](https://www.npmjs.com/package/@calcom/cal-mcp?activeTab=readme)** - Connect to the Cal.com API to schedule and manage bookings and appointments.\n- <img height=\"12\" width=\"12\" src=\"https://campertunity.com/assets/icon/favicon.ico\" alt=\"Campertunity Logo\" /> **[Campertunity](https://github.com/campertunity/mcp-server)** - Search campgrounds around the world on campertunity, check availability, and provide booking links.\n- <img height=\"12\" width=\"12\" src=\"https://static.canva.com/static/images/favicon.ico\" alt=\"Canva logo\" /> **[Canva](https://www.canva.dev/docs/apps/mcp-server/)** — Provide AI - powered development assistance for [Canva](https://canva.com) apps and integrations.\n- <img height=\"12\" width=\"12\" src=\"https://carbonvoice.app/favicon.ico\" alt=\"Carbon Voice Logo\" /> **[Carbon Voice](https://github.com/PhononX/cv-mcp-server)** - MCP Server that connects AI Agents to [Carbon Voice](https://getcarbon.app). Create, manage, and interact with voice messages, conversations, direct messages, folders, voice memos, AI actions and more in [Carbon Voice](https://getcarbon.app).\n-  **[Cartesia](https://github.com/cartesia-ai/cartesia-mcp)** - Connect to the [Cartesia](https://cartesia.ai/) voice platform to perform text-to-speech, voice cloning etc.\n- <img height=\"12\" width=\"12\" src=\"https://www.cashfree.com/favicon.ico\" alt=\"Cashfree logo\" /> **[Cashfree](https://github.com/cashfree/cashfree-mcp)** - [Cashfree Payments](https://www.cashfree.com/) official MCP server.\n- **[CB Insights](https://github.com/cbinsights/cbi-mcp-server)** - Use the [CB Insights](https://www.cbinsights.com) MCP Server to connect to [ChatCBI](https://www.cbinsights.com/chatcbi/)\n- <img height=\"12\" width=\"12\" src=\"https://cleanupcrew.ai/favicon-light.png\" alt=\"Cleanup Crew logo\" /> **[Cleanup Crew](https://cleanupcrew.ai/install)** - Real-time human support service for non-technical founders using AI coding tools. When AI hits a wall, request instant human help directly from your IDE.\n- <img height=\"12\" width=\"12\" src=\"https://www.chargebee.com/static/resources/brand/favicon.png\" alt=\"Chargebee Logo\" /> **[Chargebee](https://github.com/chargebee/agentkit/tree/main/modelcontextprotocol)** - MCP Server that connects AI agents to [Chargebee platform](https://www.chargebee.com).\n- <img height=\"12\" width=\"12\" src=\"https://cheqd.io/wp-content/uploads/2023/03/logo_cheqd_favicon.png\" alt=\"Cheqd Logo\" /> **[Cheqd](https://github.com/cheqd/mcp-toolkit)** - Enable AI Agents to be trusted, verified, prevent fraud, protect your reputation, and more through [cheqd's](https://cheqd.io) Trust Registries and Credentials.\n- <img height=\"12\" width=\"12\" src=\"https://cdn.chiki.studio/brand/logo.png\" alt=\"Chiki StudIO Logo\" /> **[Chiki StudIO](https://chiki.studio/galimybes/mcp/)** - Create your own configurable MCP servers purely via configuration (no code), with instructions, prompts, and tools support.\n- <img height=\"12\" width=\"12\" src=\"https://trychroma.com/_next/static/media/chroma-logo.ae2d6e4b.svg\" alt=\"Chroma Logo\" /> **[Chroma](https://github.com/chroma-core/chroma-mcp)** - Embeddings, vector search, document storage, and full-text search with the open-source AI application database\n- <img height=\"12\" width=\"12\" src=\"https://www.chronulus.com/favicon/chronulus-logo-blue-on-alpha-square-128x128.ico\" alt=\"Chronulus AI Logo\" /> **[Chronulus AI](https://github.com/ChronulusAI/chronulus-mcp)** - Predict anything with Chronulus AI forecasting and prediction agents.\n- <img height=\"12\" width=\"12\" src=\"https://circleci.com/favicon.ico\" alt=\"CircleCI Logo\" /> **[CircleCI](https://github.com/CircleCI-Public/mcp-server-circleci)** - Enable AI Agents to fix build failures from CircleCI.\n- <img height=\"12\" width=\"12\" src=\"https://assets.zilliz.com/Zilliz_Logo_Mark_White_20230223_041013_86057436cc.png\" alt=\"Claude Context Logo\" /> **[Claude Context](https://github.com/zilliztech/claude-context)** - Bring your codebase as context to Claude Code\n- <img height=\"12\" width=\"12\" src=\"https://clickhouse.com/favicon.ico\" alt=\"ClickHouse Logo\" /> **[ClickHouse](https://github.com/ClickHouse/mcp-clickhouse)** - Query your [ClickHouse](https://clickhouse.com/) database server.\n- <img height=\"12\" width=\"12\" src=\"https://brand.clicksend.com/_ipx/s_794x608/img/clicksend_icon_only.svg\" alt=\"ClickSend Logo\" /> **[ClickSend](https://github.com/ClickSend/clicksend-mcp-server/)** - This is the official ClickSend MCP Server developed by ClickSend team.\n- <img height=\"12\" width=\"12\" src=\"https://7463-tcb-advanced-a656fc-1257967285.tcb.qcloud.la/mcp/cloudbase-logo.svg\" alt=\"CloudBase Logo\" /> **[CloudBase](https://github.com/TencentCloudBase/CloudBase-AI-ToolKit)** - One-stop backend services for WeChat Mini-Programs and full-stack apps with serverless cloud functions and databases by [Tencent CloudBase](https://tcb.cloud.tencent.com/)\n- <img height=\"12\" width=\"12\" src=\"https://www.cloudbet.com/favicon.ico\" alt=\"Cloudbet Logo\" /> **[Cloudbet](https://github.com/cloudbet/sports-mcp-server)** - Structured sports and esports data via Cloudbet API: fixtures, live odds, stake limits, and markets.\n- <img height=\"12\" width=\"12\" src=\"https://www.cloudbees.com/favicon.ico\" alt=\"CloudBees Logo\" /> **[CloudBees](https://docs.cloudbees.com/docs/cloudbees-mcp/latest/)** - Enable AI access to your [CloudBees Unify](https://www.cloudbees.com/unify) environment.\n- <img src=\"http://www.google.com/s2/favicons?domain=www.cloudera.com\" alt=\"Cloudera Iceberg\" width=\"12\" height=\"12\"> **[Cloudera Iceberg](https://github.com/cloudera/iceberg-mcp-server)** - enabling AI on the [Open Data Lakehouse](https://www.cloudera.com/products/open-data-lakehouse.html).\n- <img alt=\"cloudflare\" height=\"12\" width=\"12\" src=\"https://cdn.simpleicons.org/cloudflare\" /> **[Cloudflare](https://github.com/cloudflare/mcp-server-cloudflare)** - Deploy, configure & interrogate your resources on the Cloudflare developer platform (e.g. Workers/KV/R2/D1)\n- <img src=\"https://cdn.prod.website-files.com/64d41aab8183c7c3324ddb29/67c0f1e272e51cf3c511c17c_Gyph.svg\" alt=\"Cloudinary\" width=\"12\" height=\"12\"> **[Cloudinary](https://github.com/cloudinary/mcp-servers)** - Exposes Cloudinary's media upload, transformation, AI analysis, management, optimization and delivery as tools usable by AI agents\n- <img height=\"12\" width=\"12\" src=\"https://raw.githubusercontent.com/Cloudsway-AI/smartsearch/refs/heads/main/plugin_cloudsway.ico\" alt=\"Cloudsway Logo\" /> **[Cloudsway SmartSearch](https://github.com/Cloudsway-AI/smartsearch)** - Web search MCP server powered by Cloudsway, supporting keyword search, language, and safety options. Returns structured JSON results.\n-  **[Codacy](https://github.com/codacy/codacy-mcp-server/)** - Interact with [Codacy](https://www.codacy.com) API to query code quality issues, vulnerabilities, and coverage insights about your code.\n-  **[CodeLogic](https://github.com/CodeLogicIncEngineering/codelogic-mcp-server)** - Interact with [CodeLogic](https://codelogic.com), a Software Intelligence platform that graphs complex code and data architecture dependencies, to boost AI accuracy and insight.\n- <img height=\"12\" width=\"12\" src=\"https://www.coingecko.com/favicon.ico\" alt=\"CoinGecko Logo\" /> **[CoinGecko](https://github.com/coingecko/coingecko-typescript/tree/main/packages/mcp-server)** - Official [CoinGecko API](https://www.coingecko.com/en/api) MCP Server for Crypto Price & Market Data, across 200+ Blockchain Networks and 8M+ Tokens.\n- <img height=\"12\" width=\"12\" src=\"https://www.comet.com/favicon.ico\" alt=\"Comet Logo\" /> **[Comet Opik](https://github.com/comet-ml/opik-mcp)** - Query and analyze your [Opik](https://github.com/comet-ml/opik) logs, traces, prompts and all other telemetry data from your LLMs in natural language.\n- <img height=\"12\" width=\"12\" src=\"https://cdn.prod.website-files.com/6572bd8c27ee5db3eb91f4b3/6572bd8d27ee5db3eb91f55e_favicon-dashflow-webflow-template.svg\" alt=\"OSS Conductor Logo\" /> <img height=\"12\" width=\"12\" src=\"https://orkes.io/icons/icon-48x48.png\" alt=\"Orkes Conductor Logo\" />**[Conductor](https://github.com/conductor-oss/conductor-mcp)** - Interact with Conductor (OSS and Orkes) REST APIs.\n- <img height=\"12\" width=\"12\" src=\"https://platform.composio.dev/favicon.ico\" alt=\"Composio Logo\" /> **[Composio](https://docs.composio.dev/docs/mcp-overview#-getting-started)** – Use [Composio](https://composio.dev) to connect 100+ tools. Zero setup. Auth built-in. Made for agents, works for humans.\n- <img height=\"12\" width=\"12\" src=\"https://www.confluent.io/favicon.ico\" alt=\"Confluent Logo\" /> **[Confluent](https://github.com/confluentinc/mcp-confluent)** - Interact with Confluent Kafka and Confluent Cloud REST APIs.\n- <img src=\"https://contrastsecurity.com/favicon.ico\" alt=\"Contrast Security\" width=\"12\" height=\"12\"> **[Contrast Security](https://github.com/Contrast-Security-OSS/mcp-contrast)** - Brings Contrast's vulnerability and SCA data into your coding agent to quickly remediate vulnerabilities.\n- <img height=\"12\" width=\"12\" src=\"https://www.convex.dev/favicon.ico\" alt=\"Convex Logo\" /> **[Convex](https://stack.convex.dev/convex-mcp-server)** - Introspect and query your apps deployed to Convex.\n- <img height=\"12\" width=\"12\" src=\"https://www.cortex.io/favicon.ico\" alt=\"Cortex Logo\" /> **[Cortex](https://github.com/cortexapps/cortex-mcp)** - Official MCP server for [Cortex](https://www.cortex.io).\n- <img height=\"12\" width=\"12\" src=\"https://avatars.githubusercontent.com/u/605755?s=200&v=4\" alt=\"Couchbase Logo\" /> **[Couchbase](https://github.com/Couchbase-Ecosystem/mcp-server-couchbase)** - Interact with the data stored in Couchbase clusters.\n- <img height=\"12\" width=\"12\" src=\"https://github.com/user-attachments/assets/b256f9fa-2020-4b37-9644-c77229ef182b\" alt=\"CRIC 克而瑞 LOGO\"> **[CRIC Wuye AI](https://github.com/wuye-ai/mcp-server-wuye-ai)** - Interact with capabilities of the CRIC Wuye AI platform, an intelligent assistant specifically for the property management industry.\n- <img height=\"12\" width=\"12\" src=\"https://www.crowdstrike.com/etc.clientlibs/crowdstrike/clientlibs/crowdstrike-common/resources/favicon.ico\" alt=\"CrowdStrike Logo\" /> **[CrowdStrike Falcon](https://github.com/CrowdStrike/falcon-mcp)** - Connects AI agents with the CrowdStrike Falcon platform for intelligent security analysis, providing programmatic access to detections, incidents, behaviors, threat intelligence, hosts, vulnerabilities, and identity protection capabilities.\n- <img height=\"12\" width=\"12\" src=\"https://avatars.githubusercontent.com/u/58433296\" alt=\"CTERA Edge Filer\" /> **[CTERA Edge Filer](https://github.com/ctera/mcp-ctera-edge)** - CTERA Edge Filer delivers intelligent edge caching and multiprotocol file access, enabling fast, secure access to files across core and remote sites.\n- <img height=\"12\" width=\"12\" src=\"https://avatars.githubusercontent.com/u/58433296\" alt=\"CTERA Portal\" /> **[CTERA Portal](https://github.com/ctera/mcp-ctera-core)** - CTERA Portal is a multi-tenant, multi-cloud platform that delivers a global namespace and unified management across petabytes of distributed content.\n- <img height=\"12\" width=\"12\" src=\"https://app.cycode.com/img/favicon.ico\" alt=\"Cycode Logo\" /> **[Cycode](https://github.com/cycodehq/cycode-cli#mcp-command-experiment)** - Boost security in your dev lifecycle via SAST, SCA, Secrets & IaC scanning with [Cycode](https://cycode.com/).\n- <img height=\"12\" width=\"12\" src=\"http://app.itsdart.com/static/img/favicon.png\" alt=\"Dart Logo\" /> **[Dart](https://github.com/its-dart/dart-mcp-server)** - Interact with task, doc, and project data in [Dart](https://itsdart.com), an AI-native project management tool\n- <img height=\"12\" width=\"12\" src=\"https://cdn.bfldr.com/9AYANS2F/at/k8bgnnxhb4bggjk88r4x9snf/databricks-symbol-color.svg?auto=webp&format=png&width=12&height=13\" alt=\"Databricks Logo\" /> **[Databricks](https://docs.databricks.com/aws/en/generative-ai/mcp/)** - Connect to data, AI tools & agents, and the rest of the Databricks platform using turnkey managed MCP servers. Or, host your own custom MCP servers within the Databricks security and data governance boundary.\n- <img height=\"12\" width=\"12\" src=\"https://datahub.com/wp-content/uploads/2025/04/cropped-Artboard-1-32x32.png\" alt=\"DataHub Logo\" /> **[DataHub](https://github.com/acryldata/mcp-server-datahub)** - Search your data assets, traverse data lineage, write SQL queries, and more using [DataHub](https://datahub.com/) metadata.\n- <img height=\"12\" width=\"12\" src=\"https://www.daytona.io/brand/social-daytona-icon.png\" alt=\"Daytona Logo\" /> **[Daytona](https://github.com/daytonaio/daytona/tree/main/apps/cli/mcp)** - Fast and secure execution of your AI generated code with [Daytona](https://daytona.io) sandboxes\n- <img height=\"12\" width=\"12\" src=\"https://debugg.ai/favicon.svg\" alt=\"Debugg AI Logo\" /> **[Debugg.AI](https://github.com/debugg-ai/debugg-ai-mcp)** - Zero-Config, Fully AI-Managed End-to-End Testing for any code gen platform via [Debugg.AI](https://debugg.ai) remote browsing test agents.\n- <img height=\"12\" width=\"12\" src=\"https://www.deepl.com/img/logo/deepl-logo-blue.svg\" alt=\"DeepL Logo\" /> **[DeepL](https://github.com/DeepLcom/deepl-mcp-server)** - Translate or rewrite text with [DeepL](https://deepl.com)'s very own AI models using [the DeepL API](https://developers.deepl.com/docs)\n- <img height=\"12\" width=\"12\" src=\"https://defang.io/_next/static/media/defang-icon-dark-colour.25f95b77.svg\" alt=\"Defang Logo\" /> **[Defang](https://github.com/DefangLabs/defang/blob/main/src/pkg/mcp/README.md)** - Deploy your project to the cloud seamlessly with the [Defang](https://www.defang.io) platform without leaving your integrated development environment\n- <img height=\"12\" width=\"12\" src=\"https://detailer.ginylil.com/favicon.ico\" alt=\"Detailer Logo\" /> **[Detailer](https://detailer.ginylil.com/)** – Instantly generate rich, AI-powered documentation for your GitHub repositories. Designed for AI agents to gain deep project context before taking action.\n- <img height=\"12\" width=\"12\" src=\"https://devcycle.com/_next/image?url=%2Fassets%2Fbrand%2FColor-logo-mark.png&w=384&q=75\" alt=\"DevCycle Logo\" /> **[DevCycle](https://docs.devcycle.com/cli-mcp/mcp-getting-started)** - Create and monitor feature flags using natural language in your AI coding assistant.\n- <img height=\"12\" width=\"12\" src=\"https://www.devhub.com/img/upload/favicon-196x196-dh.png\" alt=\"DevHub Logo\" /> **[DevHub](https://github.com/devhub/devhub-cms-mcp)** - Manage and utilize website content within the [DevHub](https://www.devhub.com) CMS platform\n- <img height=\"12\" width=\"12\" src=\"https://devrev.ai/favicon.ico\" alt=\"DevRev Logo\" /> **[DevRev](https://github.com/devrev/mcp-server)** - An MCP server to integrate with DevRev APIs to search through your DevRev Knowledge Graph where objects can be imported from diff. Sources listed [here](https://devrev.ai/docs/import#available-sources).\n- <img height=\"12\" width=\"12\" src=\"https://dexpaprika.com/favicon.ico\" alt=\"DexPaprika Logo\" /> **[DexPaprika (CoinPaprika)](https://github.com/coinpaprika/dexpaprika-mcp)** - Access real-time DEX data, liquidity pools, token information, and trading analytics across multiple blockchain networks with [DexPaprika](https://dexpaprika.com) by CoinPaprika.\n- <img height=\"12\" width=\"12\" src=\"https://github.com/dolthub/dolt/raw/main/images/Dolt-Logo@3x.svg\" alt=\"Dolt Logo\" /> **[Dolt](https://github.com/dolthub/dolt-mcp)** - The official MCP server for version-controlled [Dolt](https://doltdb.com/) databases.\n- <img height=\"12\" width=\"12\" src=\"https://eu.getdot.ai/favicon.ico\" alt=\"GetDot.ai Logo\" /> **[Dot (GetDot.ai)](https://docs.getdot.ai/dot/integrations/mcp)** - Fetch, analyze or visualize data from your favorite database or data warehouse (Snowflake, BigQuery, Redshift, Databricks, Clickhouse, ...) with [Dot](https://getdot.ai), your AI Data Analyst. This remote MCP server is a one-click integration for user that have setup Dot.\n- <img height=\"12\" width=\"12\" src=\"https://avatars.githubusercontent.com/u/65421071?s=200&v=4\" alt=\"Drata Logo\" /> **[Drata](https://drata.com/mcp)** - Get hands-on with our experimental MCP server—bringing real-time compliance intelligence into your AI workflows.\n- <img height=\"12\" width=\"12\" src=\"https://avatars.githubusercontent.com/u/204530939?s=200&v=4\" alt=\"Dumpling AI Logo\" /> **[Dumpling AI](https://github.com/Dumpling-AI/mcp-server-dumplingai)** - Access data, web scraping, and document conversion APIs by [Dumpling AI](https://www.dumplingai.com/)\n- <img height=\"12\" width=\"12\" src=\"https://avatars.githubusercontent.com/u/58178984\" alt=\"Dynatrace Logo\" /> **[Dynatrace](https://github.com/dynatrace-oss/dynatrace-mcp)** - Manage and interact with the [Dynatrace Platform ](https://www.dynatrace.com/platform) for real-time observability and monitoring.\n- <img height=\"12\" width=\"12\" src=\"https://e2b.dev/favicon.ico\" alt=\"E2B Logo\" /> **[E2B](https://github.com/e2b-dev/mcp-server)** - Run code in secure sandboxes hosted by [E2B](https://e2b.dev)\n- <img height=\"12\" width=\"12\" src=\"https://www.edgee.cloud/favicon.ico\" alt=\"Edgee Logo\" /> **[Edgee](https://github.com/edgee-cloud/mcp-server-edgee)** - Deploy and manage [Edgee](https://www.edgee.cloud) components and projects\n- <img height=\"12\" width=\"12\" src=\"https://static.edubase.net/media/brand/favicon/favicon-32x32.png\" alt=\"EduBase Logo\" /> **[EduBase](https://github.com/EduBase/MCP)** - Interact with [EduBase](https://www.edubase.net), a comprehensive e-learning platform with advanced quizzing, exam management, and content organization capabilities\n- <img height=\"12\" width=\"12\" src=\"https://www.elastic.co/favicon.ico\" alt=\"Elasticsearch Logo\" /> **[Elasticsearch](https://github.com/elastic/mcp-server-elasticsearch)** - Query your data in [Elasticsearch](https://www.elastic.co/elasticsearch)\n- <img height=\"12\" width=\"12\" src=\"https://github.com/EmberAGI/arbitrum-vibekit/blob/main/img/Ember%20Black.png?raw=true\" alt=\"Ember AI Logo\" /> **[Ember AI](https://docs.emberai.xyz/)** - A unified MCP server that enables AI agents to execute cross-chain DeFi strategies.\n- <img height=\"12\" width=\"12\" src=\"https://cdn.prod.website-files.com/656eaf5c6da3527caf362363/656ecc07555afac40df4c40e_Facicon.png\" alt=\"Endor Labs Logo\" /> **[Endor Labs](https://docs.endorlabs.com/deployment/ide/mcp/)** - Find and fix security risks in you code. Integrate [Endor Labs](https://endorlabs.com) to scan and secure your code from vulnerabilities and secret leaks.\n- <img height=\"12\" width=\"12\" src=\"https://esignatures.com/favicon.ico\" alt=\"eSignatures Logo\" /> **[eSignatures](https://github.com/esignaturescom/mcp-server-esignatures)** - Contract and template management for drafting, reviewing, and sending binding contracts.\n- <img height=\"12\" width=\"12\" src=\"https://rainmaker.espressif.com/favicon.ico\" alt=\"ESP RainMaker Logo\" /> **[ESP RainMaker](https://github.com/espressif/esp-rainmaker-mcp)** - Official Espressif MCP Server to Control and Manage ESP RainMaker Devices.\n- <img height=\"12\" width=\"12\" src=\"https://exa.ai/images/favicon-32x32.png\" alt=\"Exa Logo\" /> **[Exa](https://github.com/exa-labs/exa-mcp-server)** - Search Engine made for AIs by [Exa](https://exa.ai)\n- <img height=\"12\" width=\"12\" src=\"https://www.explorium.ai/wp-content/uploads/2025/04/Favicon-Purple-512x512-1-150x150.png\" alt=\"Explorium Logo\" /> **[Explorium](https://github.com/explorium-ai/mcp-explorium)** - B2B data and infrastructure for AI SDR & GTM Agents [Explorium](https://www.explorium.ai)\n- **[FalkorDB](https://github.com/FalkorDB/FalkorDB-MCPServer)** - FalkorDB graph database server get schema and read/write-cypher [FalkorDB](https://www.falkordb.com)\n- <img height=\"12\" width=\"12\" src=\"https://fetchserp.com/icon.png\" alt=\"fetchSERP Logo\" /> **[fetchSERP](https://github.com/fetchSERP/fetchserp-mcp-server-node)** - All-in-One SEO & Web Intelligence Toolkit API [fetchSERP](https://www.fetchserp.com/)\n- <img height=\"12\" width=\"12\" src=\"https://fewsats.com/favicon.svg\" alt=\"Fewsats Logo\" /> **[Fewsats](https://github.com/Fewsats/fewsats-mcp)** - Enable AI Agents to purchase anything in a secure way using [Fewsats](https://fewsats.com)\n- <img height=\"12\" width=\"12\" src=\"https://fibery.io/favicon.svg\" alt=\"Fibery Logo\" /> **[Fibery](https://github.com/Fibery-inc/fibery-mcp-server)** - Perform queries and entity operations in your [Fibery](https://fibery.io) workspace.\n- <img height=\"12\" width=\"12\" src=\"https://financialdatasets.ai/favicon.ico\" alt=\"Financial Datasets Logo\" /> **[Financial Datasets](https://github.com/financial-datasets/mcp-server)** - Stock market API made for AI agents\n- <img height=\"12\" width=\"12\" src=\"https://www.gstatic.com/devrel-devsite/prod/v7aeef7f1393bb1d75a4489145c511cdd5aeaa8e13ad0a83ec1b5b03612e66330/firebase/images/favicon.png\" alt=\"Firebase Logo\" /> **[Firebase](https://github.com/firebase/firebase-tools/blob/master/src/mcp)** - Firebase's experimental [MCP Server](https://firebase.google.com/docs/cli/mcp-server) to power your AI Tools\n- <img height=\"12\" width=\"12\" src=\"https://firecrawl.dev/favicon.ico\" alt=\"Firecrawl Logo\" /> **[Firecrawl](https://github.com/firecrawl/firecrawl-mcp-server)** - Extract web data with [Firecrawl](https://firecrawl.dev)\n- <img height=\"12\" width=\"12\" src=\"https://avatars.githubusercontent.com/u/100200663?s=200&v=4\" alt=\"Firefly Logo\" /> **[Firefly](https://github.com/gofireflyio/firefly-mcp)** - Integrates, discovers, manages, and codifies cloud resources with [Firefly](https://firefly.ai).\n- <img height=\"12\" width=\"12\" src=\"https://fireproof.storage/favicon.ico\" alt=\"Fireproof Logo\" /> **[Fireproof](https://github.com/fireproof-storage/mcp-database-server)** - Immutable ledger database with live synchronization\n- <img height=\"12\" width=\"12\" src=\"https://fixparser.dev/favicon.ico\" alt=\"FIXParser Logo\" /> **[FIXParser](https://gitlab.com/logotype/fixparser/-/tree/main/packages/fixparser-plugin-mcp)** - A modern FIX Protocol engine for AI-powered trading agents\n- <img height=\"12\" width=\"12\" src=\"https://avatars.githubusercontent.com/u/52471808\" alt=\"Fluid Attacks Logo\" /> **[Fluid Attacks](https://github.com/fluidattacks/mcp)** - Interact with the [Fluid Attacks](https://fluidattacks.com/) API, enabling vulnerability management, organization insights, and GraphQL query execution.\n- <img height=\"12\" width=\"12\" src=\"https://forevervm.com/icon.png\" alt=\"ForeverVM Logo\" /> **[ForeverVM](https://github.com/jamsocket/forevervm/tree/main/javascript/mcp-server)** - Run Python in a code sandbox.\n- <img height=\"12\" width=\"12\" src=\"https://flutterwave.com/favicon.ico\" alt=\"Flutterwave Logo\" /> **[Flutterwave](https://github.com/bajoski34/mcp-flutterwave/tree/main)** - Interact with Flutterwave payment solutions API, to manage transactions, payment links and more.\n- <img height=\"12\" width=\"12\" src=\"https://app.gibsonai.com/favicon.ico\" alt=\"GibsonAI Logo\" /> **[GibsonAI](https://github.com/GibsonAI/mcp)** - AI-Powered Cloud databases: Build, migrate, and deploy database instances with AI\n- <img height=\"12\" width=\"12\" src=\"https://gcore.com/assets/favicon/favicon-16x16.png\" alt=\"Gcore Logo\" /> **[Gcore](https://github.com/G-Core/gcore-mcp-server)** - Interact with Gcore platform services via LLM assistants, providing unified access to CDN, GPU Cloud & AI Inference, Video Streaming, WAAP, and cloud resources including instances and networks.\n- <img height=\"12\" width=\"12\" src=\"https://gitea.com/assets/img/favicon.svg\" alt=\"Gitea Logo\" /> **[Gitea](https://gitea.com/gitea/gitea-mcp)** - Interact with Gitea instances with MCP.\n- <img height=\"12\" width=\"12\" src=\"https://gitee.com/favicon.ico\" alt=\"Gitee Logo\" /> **[Gitee](https://github.com/oschina/mcp-gitee)** - Gitee API integration, repository, issue, and pull request management, and more.\n- <img height=\"12\" width=\"12\" src=\"https://cdn.prod.website-files.com/5ee25cbe47310017adf964da/6323888a9b9f4e22a7bc766b_GG%20Favicon.svg\" alt=\"GitGuardian Logo\" /> **[GitGuardian](https://github.com/GitGuardian/gg-mcp)** - GitGuardian official MCP server - Scan projects using GitGuardian's industry-leading API, which features over 500 secret detectors to prevent credential leaks before they reach public repositories. Resolve security incidents directly with rich contextual data for rapid, automated remediation.\n- <img height=\"12\" width=\"12\" src=\"https://gitlab.com/favicon.ico\" alt=\"GitLab Logo\" /> **[GitLab](https://docs.gitlab.com/user/gitlab_duo/model_context_protocol/mcp_server/)** - GitLab's official MCP server enabling AI tools to securely access GitLab project data, manage issues, and perform repository operations via OAuth 2.0.\n- <img height=\"12\" width=\"12\" src=\"https://github.githubassets.com/assets/GitHub-Mark-ea2971cee799.png\" alt=\"GitHub Logo\" /> **[GitHub](https://github.com/github/github-mcp-server)** - GitHub's official MCP Server.\n- <img height=\"12\" width=\"12\" src=\"https://www.gitkraken.com/wp-content/uploads/2021/03/android-chrome-144x144-1.png\" alt=\"GitKraken Logo\" /> **[GitKraken](https://github.com/gitkraken/gk-cli?tab=readme-ov-file#mcp-server)** - A CLI for interacting with GitKraken APIs. Includes an MCP server via `gk mcp` that not only wraps GitKraken APIs, but also Jira, GitHub, GitLab, and more.\n- <img height=\"12\" width=\"12\" src=\"https://app.glean.com/images/favicon3-196x196.png\" alt=\"Glean Logo\" /> **[Glean](https://github.com/gleanwork/mcp-server)** - Enterprise search and chat using Glean's API.\n- <img height=\"12\" width=\"12\" src=\"https://cdn.jsdelivr.net/gh/jsdelivr/globalping-media@refs/heads/master/icons/android-chrome-192x192.png\" alt=\"Globalping Logo\" /> **[Globalping](https://github.com/jsdelivr/globalping-mcp-server)** - Access a network of thousands of probes to run network commands like ping, traceroute, mtr, http and DNS resolve.\n- <img height=\"12\" width=\"12\" src=\"https://gnucleus.ai/favicon.ico\" alt=\"gNucleus Logo\" /> **[gNucleus Text-To-CAD](https://github.com/gNucleus/text-to-cad-mcp)** - Generate CAD parts and assemblies from text using gNucleus AI models.\n- <img height=\"12\" width=\"12\" src=\"https://www.gstatic.com/cgc/favicon.ico\" alt=\"Google Cloud Logo\" /> **[Google Cloud Run](https://github.com/GoogleCloudPlatform/cloud-run-mcp)** - Deploy code to Google Cloud Run\n- <img height=\"12\" width=\"12\" src=\"https://api.gologin.com/favicon.ico\" alt=\"GoLogin Logo\" /> **[GoLogin MCP server](https://github.com/gologinapp/gologin-mcp)** - Manage your GoLogin browser profiles and automation directly through AI conversations!\n- <img height=\"12\" width=\"12\" src=\"https://avatars.githubusercontent.com/u/3717923?s=200&v=4\" alt=\"Google Maps Platform Logo\" /> **[Google Maps Platform Code Assist](https://github.com/googlemaps/platform-ai/tree/main/packages/code-assist)** - Ground agents on fresh, official documentation and code samples for optimal geo-related guidance and code..\n- <img height=\"12\" width=\"12\" src=\"https://cdn.prod.website-files.com/6605a2979ff17b2cd1939cd4/6605a460de47e7596ed84f06_icon256.png\" alt=\"gotoHuman Logo\" /> **[gotoHuman](https://github.com/gotohuman/gotohuman-mcp-server)** - Human-in-the-loop platform - Allow AI agents and automations to send requests for approval to your [gotoHuman](https://www.gotohuman.com) inbox.\n- <img height=\"12\" width=\"12\" src=\"https://grafana.com/favicon.ico\" alt=\"Grafana Logo\" /> **[Grafana](https://github.com/grafana/mcp-grafana)** - Search dashboards, investigate incidents and query datasources in your Grafana instance\n- <img height=\"12\" width=\"12\" src=\"https://grafbase.com/favicon.ico\" alt=\"Grafbase Logo\" /> **[Grafbase](https://github.com/grafbase/grafbase/tree/main/crates/mcp)** - Turn your GraphQL API into an efficient MCP server with schema intelligence in a single command.\n- <img height=\"12\" width=\"12\" src=\"https://cdn.prod.website-files.com/5f5e90c17e7c9eb95c7acb17/61d3457a519242f2c75c725c_favicon.png\" alt=\"Grain Logo\" /> **[Grain](https://grain.com/release-note/06-18-2025)** - Access your Grain meetings notes & transcripts directly in claude and generate reports with native Claude Prompts.\n- <img height=\"12\" width=\"12\" src=\"https://framerusercontent.com/images/KCOWBYLKunDff1Dr452y6EfjiU.png\" alt=\"Graphlit Logo\" /> **[Graphlit](https://github.com/graphlit/graphlit-mcp-server)** - Ingest anything from Slack to Gmail to podcast feeds, in addition to web crawling, into a searchable [Graphlit](https://www.graphlit.com) project.\n- <img height=\"12\" width=\"12\" src=\"https://cdn.prod.website-files.com/64a5291e7847ac04fe1531ad/64a529af2f1fc7debc26f2a6_favicon-32x32.avif\" alt=\"Gremlin favicon\" /> **[Gremlin](https://github.com/gremlin/mcp)** - The official [Gremlin](https://www.gremlin.com) MCP server. Analyze your reliability posture, review recent tests and chaos engineering experiments, and create detailed reports.\n- <img height=\"12\" width=\"12\" src=\"https://greptime.com/favicon.ico\" alt=\"Greptime Logo\" /> **[GreptimeDB](https://github.com/GreptimeTeam/greptimedb-mcp-server)** - Provides AI assistants with a secure and structured way to explore and analyze data in [GreptimeDB](https://github.com/GreptimeTeam/greptimedb).\n- <img height=\"12\" width=\"12\" src=\"https://growi.org/assets/images/favicon.ico\" alt=\"GROWI Logo\" /> **[GROWI](https://github.com/growilabs/growi-mcp-server)** - Official MCP Server to integrate with GROWI APIs.\n- <img height=\"12\" width=\"12\" src=\"https://gyazo.com/favicon.ico\" alt=\"Gyazo Logo\" /> **[Gyazo](https://github.com/nota/gyazo-mcp-server)** - Search, fetch, upload, and interact with Gyazo images, including metadata and OCR data.\n- <img height=\"12\" width=\"12\" src=\"https://cdn.prod.website-files.com/6374050260446c42f94dc90f/63d828be3e13d32ee6973f35_favicon-32x32.png\" alt=\"Harper Logo\" /> **[Harper](https://github.com/HarperDB/mcp-server)** - An MCP server providing an interface for MCP clients to access data within [Harper](https://www.harpersystems.dev/).\n- <img height=\"12\" width=\"12\" src=\"https://www.herokucdn.com/favicons/favicon.ico\" alt=\"Heroku Logo\" /> **[Heroku](https://github.com/heroku/heroku-mcp-server)** - Interact with the Heroku Platform through LLM-driven tools for managing apps, add-ons, dynos, databases, and more.\n- <img height=\"12\" width=\"12\" src=\"https://heyoncall.com/favicon.ico\" alt=\"HeyOnCall Logo\" /> **[HeyOnCall](https://heyoncall.com/blog/mcp-server-for-paging-a-human)** - Page a human, sending critical or non-critical alerts to the free [HeyOnCall](https://heyoncall.com/) iOS or Android apps.\n- <img height=\"12\" width=\"12\" src=\"https://www.hiveflow.ai/favicon.ico\" alt=\"Hiveflow Logo\" /> **[Hiveflow](https://github.com/hiveflowai/hiveflow-mcp-server)** - Create, manage, and execute agentic AI workflows directly from your assistant.\n- <img height=\"12\" width=\"12\" src=\"https://hiveintelligence.xyz/favicon.ico\" alt=\"Hive Intelligence Logo\" /> **[Hive Intelligence](https://github.com/hive-intel/hive-crypto-mcp)** - Ultimate cryptocurrency MCP for AI assistants with unified access to crypto, DeFi, and Web3 analytics\n- <img height=\"12\" width=\"12\" src=\"https://img.alicdn.com/imgextra/i3/O1CN01d9qrry1i6lTNa2BRa_!!6000000004364-2-tps-218-200.png\" alt=\"Hologres Logo\" /> **[Hologres](https://github.com/aliyun/alibabacloud-hologres-mcp-server)** - Connect to a [Hologres](https://www.alibabacloud.com/en/product/hologres) instance, get table metadata, query and analyze data.\n- <img height=\"12\" width=\"12\" src=\"https://brew.sh/assets/img/favicon.ico\" alt=\"Homebrew Logo\" /> **[Homebrew](https://docs.brew.sh/MCP-Server)** Allows [Homebrew](https://brew.sh) users to run Homebrew commands locally.\n- <img height=\"12\" width=\"12\" src=\"https://www.honeycomb.io/favicon.ico\" alt=\"Honeycomb Logo\" /> **[Honeycomb](https://github.com/honeycombio/honeycomb-mcp)** Allows [Honeycomb](https://www.honeycomb.io/) Enterprise customers to query and analyze their data, alerts, dashboards, and more; and cross-reference production behavior with the codebase.\n- <img height=\"12\" width=\"12\" src=\"https://static.hsinfrastatic.net/StyleGuideUI/static-3.438/img/sprocket/favicon-32x32.png\" alt=\"HubSpot Logo\" /> **[HubSpot](https://developer.hubspot.com/mcp)** - Connect, manage, and interact with [HubSpot](https://www.hubspot.com/) CRM data\n- <img height=\"12\" width=\"12\" src=\"https://huggingface.co/datasets/huggingface/brand-assets/resolve/main/hf-logo.svg\" alt=\"HuggingFace Logo\" /> **[Hugging Face](https://huggingface.co/settings/mcp)** - Connect to the Hugging Face Hub APIs programmatically: semantic search for spaces and papers, exploration of datasets and models, and access to all compatible MCP Gradio tool spaces!\n- <img height=\"12\" width=\"12\" src=\"https://hunter.io/favicon.ico\" alt=\"Hunter Logo\" /> **[Hunter](https://github.com/hunter-io/hunter-mcp)** - Interact with the [Hunter API](https://hunter.io) to get B2B data using natural language.\n- <img height=\"12\" width=\"12\" src=\"https://app.hyperbolic.xyz/hyperbolic-logo.svg\" alt=\"Hyperbolic Labs Logo\" /> **[Hyperbolic](https://github.com/HyperbolicLabs/hyperbolic-mcp)** - Interact with Hyperbolic's GPU cloud, enabling agents and LLMs to view and rent available GPUs, SSH into them, and run GPU-powered workloads for you.\n- <img height=\"12\" width=\"12\" src=\"https://hyperbrowser-assets-bucket.s3.us-east-1.amazonaws.com/Hyperbrowser-logo.png\" alt=\"Hyperbrowsers23 Logo\" /> **[Hyperbrowser](https://github.com/hyperbrowserai/mcp)** - [Hyperbrowser](https://www.hyperbrowser.ai/) is the next-generation platform empowering AI agents and enabling effortless, scalable browser automation.\n- **[IBM wxflows](https://github.com/IBM/wxflows/tree/main/examples/mcp/javascript)** - Tool platform by IBM to build, test and deploy tools for any data source\n- <img height=\"12\" width=\"12\" src=\"https://www.getinboxzero.com/icon.png\" alt=\"Inbox Zero Logo\" /> **[Inbox Zero](https://github.com/elie222/inbox-zero/tree/main/apps/mcp-server)** - AI personal assistant for email [Inbox Zero](https://www.getinboxzero.com)\n- <img height=\"12\" width=\"12\" src=\"https://www.inflectra.com/Favicon.ico\" alt=\"Inflectra Logo\" /> **[Inflectra Spira](https://github.com/Inflectra/mcp-server-spira)** - Connect to your instance of the SpiraTest, SpiraTeam or SpiraPlan application lifecycle management platform by [Inflectra](https://www.inflectra.com)\n-  **[Inkeep](https://github.com/inkeep/mcp-server-python)** - RAG Search over your content powered by [Inkeep](https://inkeep.com)\n- <img height=\"12\" width=\"12\" src=\"https://integration.app/favicon.ico\" alt=\"Integration App Icon\" /> **[Integration App](https://github.com/integration-app/mcp-server)** - Interact with any other SaaS applications on behalf of your customers.\n- <img height=\"12\" width=\"12\" src=\"https://www.ip2location.io/favicon.ico\" alt=\"IP2Location.io Icon\" /> **[IP2Location.io](https://github.com/ip2location/mcp-ip2location-io)** - Interact with IP2Location.io API to retrieve the geolocation information for an IP address.\n- <img height=\"12\" width=\"12\" src=\"https://static.iplocate.io/custom/logo-square-rounded.png\" alt=\"IPLocate Icon\" /> **[IPLocate](https://github.com/iplocate/mcp-server-iplocate)** - Look up IP address geolocation, network information, detect proxies and VPNs, and find abuse contact details using [IPLocate.io](https://www.iplocate.io)\n- <img height=\"12\" width=\"12\" src=\"https://jellyfish.co/favicon.ico\" alt=\"Jellyfish Logo\" /> **[Jellyfish](https://github.com/Jellyfish-AI/jellyfish-mcp)** – Give your AI agent context about your team's software engineering allocations and workflow via the [Jellyfish](https://jellyfish.co) platform\n- <img alt=\"jetbrains\" height=\"12\" width=\"12\" src=\"https://cdn.simpleicons.org/jetbrains\" /> **[JetBrains](https://www.jetbrains.com/help/idea/mcp-server.html)** – Work on your code with JetBrains IDEs: IntelliJ IDEA, PhpStorm, etc.\n- <img height=\"12\" width=\"12\" src=\"https://speedmedia.jfrog.com/08612fe1-9391-4cf3-ac1a-6dd49c36b276/media.jfrog.com/wp-content/uploads/2019/04/20131046/Jfrog16-1.png\" alt=\"JFrog Logo\" /> **[JFrog](https://github.com/jfrog/mcp-jfrog)** - Model Context Protocol (MCP) Server for the [JFrog](https://jfrog.com/) Platform API, enabling repository management, build tracking, release lifecycle management, and more.\n- <img height=\"12\" width=\"12\" src=\"https://jenkins.io/images/logos/jenkins/jenkins.svg\" alt=\"Jenkins Logo\" /> **[Jenkins](https://plugins.jenkins.io/mcp-server/)** - Official Jenkins MCP Server plugin enabling AI assistants to manage builds, check job statuses, retrieve logs, and integrate with CI/CD pipelines through standardized MCP interface.\n- <img height=\"12\" width=\"12\" src=\"https://kagi.com/favicon.ico\" alt=\"Kagi Logo\" /> **[Kagi Search](https://github.com/kagisearch/kagimcp)** - Search the web using Kagi's search API\n- <img height=\"12\" width=\"12\" src=\"https://connection.keboola.com/favicon.ico\" alt=\"Keboola Logo\" /> **[Keboola](https://github.com/keboola/keboola-mcp-server)** - Build robust data workflows, integrations, and analytics on a single intuitive platform.\n- <img height=\"12\" width=\"12\" src=\"https://mcp.onkernel.com/favicon.svg\" alt=\"Kernel Logo\" /> **[Kernel](https://github.com/onkernel/kernel-mcp-server)** – Access Kernel's cloud‑based browsers via MCP.\n- <img height=\"12\" width=\"12\" src=\"https://keywordseverywhere.com/favicon.ico\" alt=\"Keywords Everywhere Logo\" /> **[Keywords Everywhere](https://api.keywordseverywhere.com/docs/#/mcp_integration)** – Access SEO data through the official Keywords Everywhere API MCP server.\n- <img height=\"12\" width=\"12\" src=\"https://keywordspeopleuse.com/favicon.ico\" alt=\"KeywordsPeopleUse Logo\" /> **[KeywordsPeopleUse.com](https://github.com/data-skunks/kpu-mcp)** - Find questions people ask online with [KeywordsPeopleUse](https://keywordspeopleuse.com).\n- <img height=\"12\" width=\"12\" src=\"https://avatars.githubusercontent.com/u/4815054\" alt=\"Kintone Logo\" /> **[Kintone](https://github.com/kintone/mcp-server)** - The official local MCP server for [Kintone](https://kintone.com).\n- <img height=\"12\" width=\"12\" src=\"https://kirokuforms.com/favicon.svg\" alt=\"KirokuForms Logo\" /> **[KirokuForms](https://www.kirokuforms.com/ai/mcp)** - [KirokuForms](https://www.kirokuforms.com) is an AI-powered form platform combining professional form building with Human-in-the-Loop (HITL) capabilities. Create custom forms, collect submissions, and integrate human oversight into AI workflows through [MCP integration](https://kirokuforms.com/ai/mcp).\n- <img height=\"12\" width=\"12\" src=\"https://raw.githubusercontent.com/klavis-ai/klavis/main/static/klavis-ai.png\" alt=\"Klavis Logo\" /> **[Klavis ReportGen](https://github.com/Klavis-AI/klavis/tree/main/mcp_servers/report_generation)** - Create professional reports from a simple user query.\n- <img height=\"12\" width=\"12\" src=\"https://www.klaviyo.com/media/Favicon-16by16.png\" alt=\"Klaviyo Logo\" /> **[Klaviyo](https://developers.klaviyo.com/en/docs/klaviyo_mcp_server)** - Interact with your [Klaviyo](https://www.klaviyo.com/) marketing data.\n- <img height=\"12\" width=\"12\" src=\"https://platform.kluster.ai/logo-light.svg\" alt=\"kluster.ai Logo\" /> **[kluster.ai](https://docs.kluster.ai/get-started/mcp/overview/)** - kluster.ai provides MCP servers that bring AI services directly into your development workflow, including guardrails like hallucination detection.\n- <img height=\"12\" width=\"12\" src=\"https://cdn.prod.website-files.com/6347ea26001f0287c592ff91/649953ef7a9ffe1f3e492b5a_Knit%20Logo.svg\" alt=\"Knit Logo\" /> **[Knit MCP Server](https://developers.getknit.dev/docs/knit-mcp-server-getting-started)** - Production-ready remote MCP servers that enable you to connect with 10000+ tools across CRM, HRIS, Payroll, Accounting, ERP, Calendar, Expense Management, and Chat categories.\n- <img height=\"12\" width=\"12\" src=\"https://knock.app/favicon/favicon-dark.svg\" alt=\"Knock Logo\" /> **[Knock MCP Server](https://github.com/knocklabs/agent-toolkit#model-context-protocol-mcp)** - Send product and customer messaging across email, in-app, push, SMS, Slack, MS Teams.\n- <img height=\"12\" width=\"12\" src=\"https://kumo-sdk-public.s3.us-west-2.amazonaws.com/rfm-colabs/kumo_ai_logo.jpeg\" alt=\"Kumo Logo\" /> **[Kumo](https://github.com/kumo-ai/kumo-rfm-mcp)** - MCP Server to interact with KumoRFM, a foundation model for generating predictions from your relational data.\n- <img height=\"12\" width=\"12\" src=\"https://www.kurrent.io/favicon.ico\" alt=\"Kurrent Logo\" /> **[KurrentDB](https://github.com/kurrent-io/mcp-server)** - This is a simple MCP server to help you explore data and prototype projections faster on top of KurrentDB.\n- <img height=\"12\" width=\"12\" src=\"https://kuzudb.com/favicon.ico\" alt=\"Kuzu Logo\" /> **[Kuzu](https://github.com/kuzudb/kuzu-mcp-server)** - This server enables LLMs to inspect database schemas and execute queries on the provided Kuzu graph database. See [blog](https://blog.kuzudb.com/post/2025-03-23-kuzu-mcp-server/)) for a debugging use case.\n- <img height=\"12\" width=\"12\" src=\"https://avatars.githubusercontent.com/u/187484914\" alt=\"KWDB Logo\" /> **[KWDB](https://github.com/KWDB/kwdb-mcp-server)** - Reading, writing, querying, modifying data, and performing DDL operations with data in your KWDB Database.\n- <img height=\"12\" width=\"12\" src=\"https://labelstud.io/favicon-16x16.png\" alt=\"Label Studio Logo\" /> **[Label Studio](https://github.com/HumanSignal/label-studio-mcp-server)** - Open Source data labeling platform.\n- <img src=\"https://avatars.githubusercontent.com/u/188884511?s=48&v=4\" alt=\"Lambda Capture\" width=\"12\" height=\"12\"> **[Lambda Capture](https://github.com/lambda-capture/mcp-server)** - Macroeconomic Forecasts & Semantic Context from Federal Reserve, Bank of England, ECB.\n- <img src=\"https://www.lambdatest.com/resources/images/header/professional-service.svg\" alt=\"LambdaTest MCP server\" width=\"12\" height=\"12\"> **[LambdaTest](https://www.lambdatest.com/mcp)** - LambdaTest MCP Servers ranging from Accessibility, SmartUI, Automation, and HyperExecute allows you to connect AI assistants with your testing workflow, streamlining setup, analyzing failures, and generating fixes to speed up testing and improve efficiency.\n- <img height=\"12\" width=\"12\" src=\"https://langfuse.com/favicon.ico\" alt=\"Langfuse Logo\" /> **[Langfuse Prompt Management](https://github.com/langfuse/mcp-server-langfuse)** - Open-source tool for collaborative editing, versioning, evaluating, and releasing prompts.\n- <img height=\"12\" width=\"12\" src=\"https://laratranslate.com/favicon.ico\" alt=\"Lara Translate Logo\" /> **[Lara Translate](https://github.com/translated/lara-mcp)** - MCP Server for Lara Translate API, enabling powerful translation capabilities with support for language detection and context-aware translations.\n- <img height=\"12\" width=\"12\" src=\"https://last9.io/favicon.png\" alt=\"Last9 Logo\" /> **[Last9](https://github.com/last9/last9-mcp-server)** - Seamlessly bring real-time production context—logs, metrics, and traces—into your local environment to auto-fix code faster.\n- <img height=\"12\" width=\"12\" src=\"https://www.launchdarkly.com/favicon.ico\" alt=\"LaunchDarkly Logo\" /> **[LaunchDarkly](https://github.com/launchdarkly/mcp-server)** - LaunchDarkly is a continuous delivery platform that provides feature flags as a service and allows developers to iterate quickly and safely.\n- <img height=\"12\" width=\"12\" src=\"https://www.line.me/favicon-32x32.png\" alt=\"LINE Logo\" /> **[LINE](https://github.com/line/line-bot-mcp-server)** - Integrates the LINE Messaging API to connect an AI Agent to the LINE Official Account.\n- <img height=\"12\" width=\"12\" src=\"https://linear.app/favicon.ico\" alt=\"Linear Logo\" /> **[Linear](https://linear.app/docs/mcp)** - Search, create, and update Linear issues, projects, and comments.\n- <img height=\"12\" width=\"12\" src=\"https://lingo.dev/favicon.ico\" alt=\"Lingo.dev Logo\" /> **[Lingo.dev](https://github.com/lingodotdev/lingo.dev/blob/main/mcp.md)** - Make your AI agent speak every language on the planet, using [Lingo.dev](https://lingo.dev) Localization Engine.\n- <img height=\"12\" width=\"12\" src=\"https://ligo.ertiqah.com/favicon.avif\" alt=\"LiGo Logo\" /> **[LinkedIn MCP Runner](https://github.com/ertiqah/linkedin-mcp-runner)** - Write, edit, and schedule LinkedIn posts right from ChatGPT and Claude with [LiGo](https://ligo.ertiqah.com/).\n- <img height=\"12\" width=\"12\" src=\"https://avatars.githubusercontent.com/u/175112039?s=200&v=4\" alt=\"Linkup Logo\" /> **[Linkup](https://github.com/LinkupPlatform/js-mcp-server)** - (JS version) MCP server that provides web search capabilities through Linkup's advanced search API. This server enables AI assistants and development tools to perform intelligent web searches with natural language queries.\n- <img height=\"12\" width=\"12\" src=\"https://avatars.githubusercontent.com/u/175112039?s=200&v=4\" alt=\"Linkup Logo\" /> **[Linkup](https://github.com/LinkupPlatform/python-mcp-server)** - (Python version) MCP server that provides web search capabilities through Linkup's advanced search API. This server enables AI assistants and development tools to perform intelligent web searches with natural language queries.\n- <img src=\"https://avatars.githubusercontent.com/u/149083471\" alt=\"Lippia.io\" width=\"12\" height=\"12\"> **[Lippia](https://github.com/Lippia-io/Lippia-MCP-Server/blob/main/getting-started.md)** - MCP Server to accelerate Test Automation using Lippia Framework.\n- <img src=\"https://gornschool.com/gorn.png\" alt=\"Lisply\" width=\"12\" height=\"12\"> **[Lisply](https://github.com/gornskew/lisply-mcp)** - Flexible frontend for compliant Lisp-speaking backends.\n- <img height=\"12\" width=\"12\" src=\"https://litmus.io/favicon.ico\" alt=\"Litmus.io Logo\" /> **[Litmus.io](https://github.com/litmusautomation/litmus-mcp-server)** - Official MCP server for configuring [Litmus](https://litmus.io) Edge for Industrial Data Collection, Edge Analytics & Industrial AI.\n- <img height=\"12\" width=\"12\" src=\"https://liveblocks.io/favicon.ico\" alt=\"Liveblocks Logo\" /> **[Liveblocks](https://github.com/liveblocks/liveblocks-mcp-server)** - Ready‑made features for AI & human collaboration—use this to develop your [Liveblocks](https://liveblocks.io) app quicker.\n- <img height=\"12\" width=\"12\" src=\"https://logfire.pydantic.dev/favicon.ico\" alt=\"Logfire Logo\" /> **[Logfire](https://github.com/pydantic/logfire-mcp)** - Provides access to OpenTelemetry traces and metrics through Logfire.\n- <img height=\"12\" width=\"12\" src=\"https://make.magicmealkits.com/favicon.ico\" alt=\"Magic Meal Kits Logo\" /> **[Magic Meal Kits](https://github.com/pureugong/mmk-mcp)** - Unleash Make's Full Potential by [Magic Meal Kits](https://make.magicmealkits.com/)\n- <img height=\"12\" width=\"12\" src=\"https://www.mailgun.com/favicon.ico\" alt=\"Mailgun Logo\" /> **[Mailgun](https://github.com/mailgun/mailgun-mcp-server)** - Interact with Mailgun API.\n- <img height=\"12\" width=\"12\" src=\"https://www.mailjet.com/favicon.ico\" alt=\"Mailjet Logo\" /> **[Mailjet](https://github.com/mailgun/mailjet-mcp-server)** - Official MCP server which allows AI agents to interact with contact, campaign, segmentation, statistics, workflow (and more) APIs from [Sinch Mailjet](https://www.mailjet.com).\n- <img height=\"12\" width=\"12\" src=\"https://www.make.com/favicon.ico\" alt=\"Make Logo\" /> **[Make](https://github.com/integromat/make-mcp-server)** - Turn your [Make](https://www.make.com/) scenarios into callable tools for AI assistants.\n- <img height=\"12\" width=\"12\" src=\"https://static-assets.mapbox.com/branding/favicon/v1/favicon.ico\" alt=\"Mapbox Logo\" /> **[Mapbox](https://github.com/mapbox/mcp-server)** - Unlock geospatial intelligence through Mapbox APIs like geocoding, POI search, directions, isochrones and more.\n- <img height=\"12\" width=\"12\" src=\"https://www.mariadb.com/favicon.ico\" alt=\"MariaDB Logo\" /> **[MariaDB](https://github.com/mariadb/mcp)** - A standard interface for managing and querying MariaDB databases, supporting both standard SQL operations and advanced vector/embedding-based search.\n- <img height=\"14\" width=\"14\" src=\"https://raw.githubusercontent.com/rust-mcp-stack/mcp-discovery/refs/heads/main/docs/_media/mcp-discovery-logo.png\" alt=\"mcp-discovery logo\" /> **[MCP Discovery](https://github.com/rust-mcp-stack/mcp-discovery)** - A lightweight CLI tool built in Rust for discovering MCP server capabilities.\n- <img height=\"12\" width=\"12\" src=\"https://googleapis.github.io/genai-toolbox/favicons/favicon.ico\" alt=\"MCP Toolbox for Databases Logo\" /> **[MCP Toolbox for Databases](https://github.com/googleapis/genai-toolbox)** - Open source MCP server specializing in easy, fast, and secure tools for Databases. Supports  AlloyDB, BigQuery, Bigtable, Cloud SQL, Dgraph, Looker, MySQL, Neo4j, Postgres, Spanner, and more.\n- <img height=\"12\" width=\"12\" src=\"https://www.meilisearch.com/favicon.ico\" alt=\"Meilisearch Logo\" /> **[Meilisearch](https://github.com/meilisearch/meilisearch-mcp)** - Interact & query with Meilisearch (Full-text & semantic search API)\n- <img height=\"12\" width=\"12\" src=\"https://memgraph.com/favicon.png\" alt=\"Memgraph Logo\" /> **[Memgraph](https://github.com/memgraph/ai-toolkit/tree/main/integrations/mcp-memgraph)** - Query your data in [Memgraph](https://memgraph.com/) graph database.\n- <img height=\"12\" width=\"12\" src=\"https://www.mercadolibre.com.ar/favicon.ico\" alt=\"MercadoLibre Logo\" /> **[Mercado Libre](https://mcp.mercadolibre.com/)** - Mercado Libre's official MCP server.\n- <img height=\"12\" width=\"12\" src=\"https://www.mercadopago.com/favicon.ico\" alt=\"MercadoPago Logo\" /> **[Mercado Pago](https://mcp.mercadopago.com/)** - Mercado Pago's official MCP server.\n- <img height=\"12\" width=\"12\" src=\"https://metoro.io/static/images/logos/MetoroLogo.png\" alt=\"Metoro Logo\" /> **[Metoro](https://github.com/metoro-io/metoro-mcp-server)** - Query and interact with kubernetes environments monitored by Metoro\n- <img height=\"12\" width=\"12\" src=\"https://claritystatic.azureedge.net/images/logo.ico\" alt=\"Microsoft Clarity Logo\"/> **[Microsoft Clarity](https://github.com/microsoft/clarity-mcp-server)** - Official MCP Server to get your behavioral analytics data and insights from [Clarity](https://clarity.microsoft.com)\n- <img height=\"12\" width=\"12\" src=\"https://conn-afd-prod-endpoint-bmc9bqahasf3grgk.b01.azurefd.net/releases/v1.0.1735/1.0.1735.4099/commondataserviceforapps/icon.png\" alt=\"Microsoft Dataverse Logo\" /> **[Microsoft Dataverse](https://go.microsoft.com/fwlink/?linkid=2320176)** - Chat over your business data using NL - Discover tables, run queries, retrieve data, insert or update records, and execute custom prompts grounded in business knowledge and context.\n- <img height=\"12\" width=\"12\" src=\"https://learn.microsoft.com/favicon.ico\" alt=\"Microsoft Learn Logo\" /> **[Microsoft Learn Docs](https://github.com/microsoftdocs/mcp)** - An MCP server that provides structured access to Microsoft's official documentation. Retrieves accurate, authoritative, and context-aware technical content for code generation, question answering, and workflow grounding.\n- <img height=\"12\" width=\"12\" src=\"https://statics.teams.microsoft.com/hashedassets/favicon/prod/favicon-9f45b466.ico\" alt=\"Microsoft Teams Logo\" /> **[Microsoft Teams](https://devblogs.microsoft.com/microsoft365dev/announcing-the-updated-teams-ai-library-and-mcp-support/)** - Official Microsoft Teams AI Library with MCP support enabling advanced agent orchestration, multi-agent collaboration, and seamless integration with Teams messaging and collaboration features.\n- <img alt=\"favicon_32x32\" height=\"12\" width=\"12\" src=\"https://milvus.io/favicon-32x32.png\" /> **[Milvus](https://github.com/zilliztech/mcp-server-milvus)** - Search, Query and interact with data in your Milvus Vector Database.\n- <img src=\"https://www.mimilabs.ai/logos/mimilabsSquare.svg\" alt=\"mimilabs\" width=\"12\" height=\"12\"> **[mimilabs](https://www.mimilabs.ai/mcp)** - A US healthcare data discovery guide for 50+ gov sources and thousands of publicly available US healthcare datasets regarding gov-funded programs, policies, drug pricings, clinical trials, etc.\n- <img src=\"https://avatars.githubusercontent.com/u/94089762?s=48&v=4\" alt=\"Mobb\" width=\"12\" height=\"12\"> **[Mobb](https://github.com/mobb-dev/bugsy?tab=readme-ov-file#model-context-protocol-mcp-server)** - The [Mobb Vibe Shield](https://vibe.mobb.ai/) MCP server identifies and remediates vulnerabilities in both human and AI-written code, ensuring your applications remain secure without slowing development.\n- <img alt=\"favicon\" height=\"12\" width=\"12\" src=\"https://console.gomomento.com/favicon.ico\" /> **[Momento](https://github.com/momentohq/mcp-momento)** - Momento Cache lets you quickly improve your performance, reduce costs, and handle load at any scale.\n- <img height=\"12\" width=\"12\" src=\"https://www.monday.com/favicon.ico\" alt=\"Monday.com Logo\" /> **[Monday.com](https://github.com/mondaycom/mcp)** - Interact with Monday.com boards, items, accounts and work forms.\n- <img alt=\"favicon\" height=\"12\" width=\"12\" src=\"https://www.mongodb.com/favicon.ico\" /> **[MongoDB](https://github.com/mongodb-js/mongodb-mcp-server)** - Both MongoDB Community Server and MongoDB Atlas are supported.\n- <img height=\"12\" width=\"12\" src=\"https://moorcheh.ai/Moorcheh-mcp.ico\" alt=\"Moorcheh Logo\" /> **[Moorcheh](https://github.com/moorcheh-ai/moorcheh-mcp)** - Embed, store, and search your documents, and build secure chatbots and RAG systems with Moorcheh's information-theoretic semantic search engine\n- <img height=\"12\" width=\"12\" src=\"https://www.motherduck.com/favicon.ico\" alt=\"MotherDuck Logo\" /> **[MotherDuck](https://github.com/motherduckdb/mcp-server-motherduck)** - Query and analyze data with MotherDuck and local DuckDB\n- <img height=\"12\" width=\"12\" src=\"https://docs.mulesoft.com/_/img/favicon.ico\" alt=\"Mulesoft Logo\" /> **[Mulesoft](https://www.npmjs.com/package/@mulesoft/mcp-server)** - Build, deploy, and manage MuleSoft applications with natural language, directly inside any compatible IDE.\n- <img height=\"12\" width=\"12\" src=\"https://www.multiplayer.app/favicon-32x32.png\" alt=\"Multiplayer Logo\" /> **[Multiplayer](https://www.multiplayer.app/docs/ai/mcp-server)** - Analyze your full stack session recordings easily. Record a bug with Multiplayer, analyze and fix it with LLM\n-  **[Nango](https://docs.nango.dev/guides/use-cases/mcp-server)** - Integrate your AI agent with 500+ APIs: Auth, custom tools, and observability. Open-source.\n- <img height=\"12\" width=\"12\" src=\"https://avatars.githubusercontent.com/u/38020270\" alt=\"NanoVMs Logo\" /> **[NanoVMs](https://github.com/nanovms/ops-mcp)** - Easily Build and Deploy unikernels to any cloud.\n- <img height=\"12\" width=\"12\" src=\"https://needle-ai.com/images/needle-logo-orange-2-rounded.png\" alt=\"Needle AI Logo\" /> **[Needle](https://github.com/needle-ai/needle-mcp)** - Production-ready RAG out of the box to search and retrieve data from your own documents.\n- <img height=\"12\" width=\"12\" src=\"https://neo4j.com/favicon.ico\" alt=\"Neo4j Logo\" /> **[Neo4j](https://github.com/neo4j-contrib/mcp-neo4j/)** - Neo4j graph database server (schema + read/write-cypher) and separate graph database backed memory\n- <img height=\"12\" width=\"12\" src=\"https://knowall.ai/favicon.ico\" alt=\"Neo4j Agent Memory Logo\" /> **[Neo4j Agent Memory](https://github.com/knowall-ai/mcp-neo4j-agent-memory)** - Memory management for AI agents using Neo4j knowledge graphs\n- <img height=\"12\" width=\"12\" src=\"https://neo4j.com/favicon.ico\" alt=\"Neo4j Logo\" /> **[Neo4j GDS](https://github.com/neo4j-contrib/gds-agent)** - Neo4j graph data science server with comprehensive graph algorithms that enables complex graph reasoning and Q&A.\n- <img height=\"12\" width=\"12\" src=\"https://avatars.githubusercontent.com/u/183852044?s=48&v=4\" alt=\"Neon Logo\" /> **[Neon](https://github.com/neondatabase/mcp-server-neon)** - Interact with the Neon serverless Postgres platform\n- <img height=\"12\" width=\"12\" src=\"https://app.usenerve.com/favicon.ico\" alt=\"Nerve Logo\" /> **[Nerve](https://github.com/nerve-hq/nerve-mcp-server)** - Search and Act on all your company data across all your SaaS apps via [Nerve](https://www.usenerve.com/)\n- <img height=\"12\" width=\"12\" src=\"https://www.netdata.cloud/favicon-32x32.png\" alt=\"Netdata Logo\" /> **[Netdata](https://github.com/netdata/netdata/blob/master/src/web/mcp/README.md)** - Discovery, exploration, reporting and root cause analysis using all observability data, including metrics, logs, systems, containers, processes, and network connections\n- <img height=\"12\" width=\"12\" src=\"https://www.netlify.com/favicon/icon.svg\" alt=\"Netlify Logo\" /> **[Netlify](https://docs.netlify.com/welcome/build-with-ai/netlify-mcp-server/)** - Create, build, deploy, and manage your websites with Netlify web platform.\n- <img height=\"12\" width=\"12\" src=\"https://www.thenile.dev/favicon.ico\" alt=\"Nile Logo\" /> **[Nile](https://github.com/niledatabase/nile-mcp-server)** - An MCP server that talks to Nile - Postgres re-engineered for B2B apps. Manage and query databases, tenants, users, auth using LLMs\n- <img height=\"12\" width=\"12\" src=\"https://avatars.githubusercontent.com/u/208441832?s=400&v=4\" alt=\"Nodit Logo\" /> **[Nodit](https://github.com/noditlabs/nodit-mcp-server)** - Official Nodit MCP Server enabling access to multi-chain RPC Nodes and Data APIs for blockchain data.\n- <img height=\"12\" width=\"12\" src=\"https://app.norman.finance/favicons/favicon-32x32.png\" alt=\"Norman Logo\" /> **[Norman Finance](https://github.com/norman-finance/norman-mcp-server)** - MCP server for managing accounting and taxes with Norman Finance.\n- <img height=\"12\" width=\"12\" src=\"https://avatars.githubusercontent.com/u/4792552?s=200&v=4\" alt=\"Notion Logo\" /> **[Notion](https://github.com/makenotion/notion-mcp-server#readme)** - This project implements an MCP server for the Notion API.\n-  **[Nutrient](https://github.com/PSPDFKit/nutrient-dws-mcp-server)** - Create, Edit, Sign, Extract Documents using Natural Language\n- <img height=\"12\" width=\"12\" src=\"https://nx.dev/favicon/favicon.svg\" alt=\"Nx Logo\" /> **[Nx](https://github.com/nrwl/nx-console/blob/master/apps/nx-mcp)** - Makes [Nx's understanding](https://nx.dev/features/enhance-AI) of your codebase accessible to LLMs, providing insights into the codebase architecture, project relationships and runnable tasks thus allowing AI to make precise code suggestions.\n- <img height=\"12\" width=\"12\" src=\"https://avatars.githubusercontent.com/u/82347605?s=48&v=4\" alt=\"OceanBase Logo\" /> **[OceanBase](https://github.com/oceanbase/mcp-oceanbase)** - MCP Server for OceanBase database and its tools\n- <img height=\"12\" width=\"12\" src=\"https://docs.octagonagents.com/logo.svg\" alt=\"Octagon Logo\" /> **[Octagon](https://github.com/OctagonAI/octagon-mcp-server)** - Deliver real-time investment research with extensive private and public market data.\n- <img height=\"12\" width=\"12\" src=\"https://octoeverywhere.com/img/logo.png\" alt=\"OctoEverywhere Logo\" /> **[OctoEverywhere](https://github.com/OctoEverywhere/mcp)** - A 3D Printing MCP server that allows for querying for live state, webcam snapshots, and 3D printer control.\n- <img height=\"12\" width=\"12\" src=\"https://avatars.githubusercontent.com/u/211697972\" alt=\"Offorte Logo\" /> **[Offorte](https://github.com/offorte/offorte-mcp-server#readme)** - Offorte Proposal Software official MCP server enables creation and sending of business proposals.\n-  **[OlaMaps](https://pypi.org/project/ola-maps-mcp-server)** - Official Ola Maps MCP Server for services like geocode, directions, place details and many more.\n- <img height=\"12\" width=\"12\" src=\"https://www.olostep.com/favicon.ico\" alt=\"Olostep\" /> **[Olostep](https://github.com/olostep/olostep-mcp-server)** - Search, scrape and crawl content from web. Real-time results in clean markdown.\n- <img height=\"12\" width=\"12\" src=\"https://static.onlyoffice.com/images/favicon.ico\" alt=\"ONLYOFFICE DocSpace\" /> **[ONLYOFFICE DocSpace](https://github.com/ONLYOFFICE/docspace-mcp)** - Interact with [ONLYOFFICE DocSpace](https://www.onlyoffice.com/docspace.aspx) API to create rooms, manage files and folders.\n- **[OMOP MCP](https://github.com/OHNLP/omop_mcp)** - Map clinical terminology to OMOP concepts using LLMs for healthcare data standardization.\n- <img height=\"12\" width=\"12\" src=\"https://op.gg/favicon.ico\" alt=\"OP.GG Logo\" /> **[OP.GG](https://github.com/opgginc/opgg-mcp)** - Access real-time gaming data across popular titles like League of Legends, TFT, and Valorant, offering champion analytics, esports schedules, meta compositions, and character statistics.\n- <img height=\"12\" width=\"12\" src=\"https://www.openfort.io/img/icon.svg\" alt=\"Openfort\" /> **[Openfort](https://github.com/openfort-xyz/mcp)** - Connect your AI to Openfort's smart wallet, auth, and project infrastructure.\n- <img height=\"12\" width=\"12\" src=\"https://open-metadata.org/favicon.ico\" alt=\"OpenMetadata\" /> **[OpenMetadata](https://open-metadata.org/mcp)** - The first Enterprise-grade MCP server for metadata\n- <img height=\"12\" width=\"12\" src=\"https://opensearch.org/wp-content/uploads/2025/01/opensearch_mark_default.svg\" alt=\"OpenSearch Logo\" /> **[OpenSearch](https://github.com/opensearch-project/opensearch-mcp-server-py)** -  MCP server that enables AI agents to perform search and analytics use cases on data stored in [OpenSearch](https://opensearch.org/).\n- <img height=\"12\" width=\"12\" src=\"https://app.opslevel.com/favicon.ico\" alt=\"OpsLevel\" /> **[OpsLevel](https://github.com/opslevel/opslevel-mcp)** - Official MCP Server for [OpsLevel](https://www.opslevel.com).\n- <img height=\"12\" width=\"12\" src=\"https://optuna.org/assets/img/favicon.ico\" alt=\"Optuna Logo\" /> **[Optuna](https://github.com/optuna/optuna-mcp)** - Official MCP server enabling seamless orchestration of hyperparameter search and other optimization tasks with [Optuna](https://optuna.org/).\n- <img height=\"12\" width=\"12\" src=\"https://raw.githubusercontent.com/oracle/mcp/refs/heads/main/oracle.svg\" alt=\"Oracle Logo\" /> **[Oracle](https://docs.oracle.com/en/database/oracle/sql-developer-command-line/25.2/sqcug/starting-and-managing-sqlcl-mcp-server.html#GUID-5F916B5D-8670-42BD-9F8B-D3D2424EC47E)** - Official [Oracle Database: SQLcl ](https://www.oracle.com/database/sqldeveloper/technologies/sqlcl/download/) MCP server enabling all access to any Oracle Database via native MCP support directly in SQLcl.\n- <img height=\"12\" width=\"12\" src=\"https://orshot.com/brand/favicon.svg\" alt=\"Orshot Logo\" /> **[Orshot](https://github.com/rishimohan/orshot-mcp-server)** - Official [Orshot](https://orshot.com) MCP server to dynamically generate images from custom design templates.\n- <img height=\"12\" width=\"12\" src=\"https://oxylabs.io/favicon.ico\" alt=\"Oxylabs Logo\" /> **[Oxylabs](https://github.com/oxylabs/oxylabs-mcp)** - Scrape websites with Oxylabs Web API, supporting dynamic rendering and parsing for structured data extraction.\n- <img height=\"12\" width=\"12\" src=\"https://developer.paddle.com/favicon.svg\" alt=\"Paddle Logo\" /> **[Paddle](https://github.com/PaddleHQ/paddle-mcp-server)** - Interact with the Paddle API. Manage product catalog, billing and subscriptions, and reports.\n- **[PaddleOCR](https://paddlepaddle.github.io/PaddleOCR/latest/en/version3.x/deployment/mcp_server.html)** - An MCP server that brings enterprise-grade OCR and document parsing capabilities to AI applications.\n- <img height=\"12\" width=\"12\" src=\"https://cdn.brandfolder.io/YX9ETPCP/at/266537g8kh6mmvt24jvsjb/P-GreenRGB.svg\" alt=\"PagerDuty Logo\" /> **[PagerDuty](https://github.com/PagerDuty/pagerduty-mcp-server)** - Interact with your PagerDuty account, allowing you to manage incidents, services, schedules, and more directly from your MCP-enabled client.\n- **[Pagos](https://github.com/pagos-ai/pagos-mcp)** - Interact with the Pagos API. Query Credit Card BIN Data with more to come.\n- <img height=\"12\" width=\"12\" src=\"https://paiml.com/favicon.ico\" alt=\"PAIML Logo\" /> **[PAIML MCP Agent Toolkit](https://github.com/paiml/paiml-mcp-agent-toolkit)** - Professional project scaffolding toolkit with zero-configuration AI context generation, template generation for Rust/Deno/Python projects, and hybrid neuro-symbolic code analysis.\n- <img height=\"12\" width=\"12\" src=\"https://app.paperinvest.io/favicon.svg\" alt=\"Paper Logo\" /> **[Paper](https://github.com/paperinvest/mcp-server)** - Realistic paper trading platform with market simulation, 22 broker emulations, and professional tools for risk-free trading practice. First trading platform with MCP integration.\n- **[Patronus AI](https://github.com/patronus-ai/patronus-mcp-server)** - Test, evaluate, and optimize AI agents and RAG apps\n- <img height=\"12\" width=\"12\" src=\"https://mcp.paubox.com/paubox.png\" alt=\"Paubox Logo\" />**[Paubox](https://mcp.paubox.com)** - Official MCP server which allows AI agents to interact with Paubox Email API. HITRUST certified.\n- <img height=\"12\" width=\"12\" src=\"https://www.paypalobjects.com/webstatic/icon/favicon.ico\" alt=\"PayPal Logo\" /> **[PayPal](https://mcp.paypal.com)** - PayPal's official MCP server.\n- <img height=\"12\" width=\"12\" src=\"https://ww2-secure.pearl.com/static/pearl/pearl-logo.svg\" alt=\"Pearl Logo\" /> **[Pearl](https://github.com/Pearl-com/pearl_mcp_server)** - Official MCP Server to interact with Pearl API. Connect your AI Agents with 12,000+ certified experts instantly.\n- <img height=\"12\" width=\"12\" src=\"https://www.perplexity.ai/favicon.ico\" alt=\"Perplexity Logo\" /> **[Perplexity](https://github.com/ppl-ai/modelcontextprotocol)** - An MCP server that connects to Perplexity's Sonar API, enabling real-time web-wide research in conversational AI.\n- <img height=\"12\" width=\"12\" src=\"https://www.foxit.com/favicon.ico\" alt=\"Foxit Logo\" /> **[PDFActionInspector](https://github.com/foxitsoftware/PDFActionInspector/tree/develop)** - A Model Context Protocol server for extracting and analyzing JavaScript Actions from PDF files. Provides comprehensive security analysis to detect malicious PDF behaviors, hidden scripts, and potential security threats through AI-assisted risk assessment.\n- <img height=\"12\" width=\"12\" src=\"https://www.pga.com/favicon.ico\" alt=\"PGA Logo\" /> **[PGA (Golf)](https://mcp.pga.com)** - PGA's official MCP Server for all things golf-related. Find a coach, play golf, improve your game, and more.\n- <img alt=\"54333248\" height=\"12\" width=\"12\" src=\"https://avatars.githubusercontent.com/u/54333248\" /> **[Pinecone](https://github.com/pinecone-io/pinecone-mcp)** - [Pinecone](https://docs.pinecone.io/guides/operations/mcp-server)'s developer MCP Server assist developers in searching documentation and managing data within their development environment.\n- <img alt=\"54333248\" height=\"12\" width=\"12\" src=\"https://avatars.githubusercontent.com/u/54333248\" /> **[Pinecone Assistant](https://github.com/pinecone-io/assistant-mcp)** - Retrieves context from your [Pinecone Assistant](https://docs.pinecone.io/guides/assistant/mcp-server) knowledge base.\n- <img height=\"12\" width=\"12\" src=\"https://pipedream.com/favicon.ico\" alt=\"Pipedream Logo\" /> **[Pipedream](https://github.com/PipedreamHQ/pipedream/tree/master/modelcontextprotocol)** - Connect with 2,500 APIs with 8,000+ prebuilt tools.\n- <img height=\"12\" width=\"12\" src=\"https://playcanvas.com/static-assets/images/icons/favicon.png\" alt=\"PlayCanvas Logo\" /> **[PlayCanvas](https://github.com/playcanvas/editor-mcp-server)** - Create interactive 3D web apps with the PlayCanvas Editor.\n- <img height=\"12\" width=\"12\" src=\"https://playwright.dev/img/playwright-logo.ico\" alt=\"Playwright Logo\" /> **[Playwright](https://github.com/microsoft/playwright-mcp)** — Browser automation MCP server using Playwright to run tests, navigate pages, capture screenshots, scrape content, and automate web interactions reliably.\n- <img height=\"12\" width=\"12\" src=\"https://www.plugged.in/favicon.ico\" alt=\"Plugged.in Logo\" /> **[Plugged.in](https://github.com/VeriTeknik/pluggedin-mcp)** - A comprehensive proxy that combines multiple MCP servers into a single MCP. It provides discovery and management of tools, prompts, resources, and templates across servers, plus a playground for debugging when building MCP servers.\n- <img height=\"12\" width=\"12\" src=\"https://github.com/port-labs/port-mcp-server/blob/main/assets/port_symbol_white.svg\" alt=\"Port Logo\" /> **[Port IO](https://github.com/port-labs/port-mcp-server)** - Access and manage your software catalog to improve service quality and compliance.\n- **[PostHog](https://github.com/posthog/mcp)** - Interact with PostHog analytics, feature flags, error tracking and more with the official PostHog MCP server.\n- **[Postman API](https://github.com/postmanlabs/postman-api-mcp)** - Manage your Postman resources using the [Postman API](https://www.postman.com/postman/postman-public-workspace/collection/i2uqzpp/postman-api).\n- <img height=\"12\" width=\"12\" src=\"https://powerdrill.ai/_next/static/media/powerdrill.0fa27d00.webp\" alt=\"Powerdrill Logo\" /> **[Powerdrill](https://github.com/powerdrillai/powerdrill-mcp)** - An MCP server that provides tools to interact with Powerdrill datasets, enabling smart AI data analysis and insights.\n- <img height=\"12\" width=\"12\" src=\"https://www.prisma.io/images/favicon-32x32.png\" alt=\"Prisma Logo\" /> **[Prisma](https://www.prisma.io/docs/postgres/mcp-server)** - Create and manage Prisma Postgres databases\n- <img height=\"12\" width=\"12\" src=\"https://probe.dev/favicon.ico\" alt=\"Probe.dev Logo\" /> **[Probe.dev](https://docs.probe.dev/guides/mcp-integration)** - Comprehensive media analysis and validation powered by [Probe.dev](https://probe.dev). Hosted MCP server with FFprobe, MediaInfo, and Probe Report analysis capabilities.\n- <img height=\"12\" width=\"12\" src=\"https://framerusercontent.com/images/FGzpihs4MxmSJhyGZ6n7f2Xj0.png\" alt=\"Prode.ai Logo\" /> **[ProdE](https://github.com/CuriousBox-AI/ProdE-mcp)** - Your 24/7 production engineer that preserves context across multiple codebases.\n- <img height=\"12\" width=\"12\" src=\"https://programintegrity.org/wp-content/uploads/2024/07/PIA-Favicon.svg\" alt=\"Program Integrity Alliance (PIA) Logo\" /> **[Program Integrity Alliance (PIA)](https://github.com/Program-Integrity-Alliance/pia-mcp-local)** - Local and Hosted MCP servers providing AI-friendly access to U.S. Government Open Datasets. Also available on [Docker MCP Catalog](https://hub.docker.com/mcp/explore?search=PIA). See [our website](https://programintegrity.org) for more details.\n- <img height=\"12\" width=\"12\" src=\"https://github.com/newtype-01/prompthouse-mcp/raw/main/prompthouse-logo-12x12.png\" alt=\"PromptHouse Logo\" /> **[PromptHouse](https://github.com/newtype-01/prompthouse-mcp)** - Personal prompt library with MCP integration for AI clients.\n- <img height=\"12\" width=\"12\" src=\"https://docs.speedscale.com/img/favicon.ico\" alt=\"proxymock Logo\" /> **[proxymock](https://docs.speedscale.com/proxymock/reference/mcp/)** - An MCP server that automatically generates tests and mocks by recording a live app.\n- <img src=\"https://www.pubnub.com/favicon/favicon-32x32.png\" alt=\"PubNub\" width=\"12\" height=\"12\"> **[PubNub](https://github.com/pubnub/pubnub-mcp-server)** - Retrieves context for developing with PubNub SDKs and calling APIs.\n- <img height=\"12\" width=\"12\" src=\"https://www.pulumi.com/images/favicon.ico\" alt=\"Pulumi Logo\" /> **[Pulumi](https://github.com/pulumi/mcp-server)** - Deploy and manage cloud infrastructure using [Pulumi](https://pulumi.com).\n- <img height=\"12\" width=\"12\" src=\"https://pure.md/favicon.png\" alt=\"Pure.md Logo\" /> **[Pure.md](https://github.com/puremd/puremd-mcp)** - Reliably access web content in markdown format with [pure.md](https://pure.md) (bot detection avoidance, proxy rotation, and headless JS rendering built in).\n- <img height=\"12\" width=\"12\" src=\"https://put.io/images/favicon.ico\" alt=\"Put.io Logo\" /> **[Put.io](https://github.com/putdotio/putio-mcp-server)** - Interact with your Put.io account to download torrents.\n- <img alt=\"logomark\" height=\"12\" width=\"12\" src=\"https://qdrant.tech/img/brand-resources-logos/logomark.svg\" /> **[Qdrant](https://github.com/qdrant/mcp-server-qdrant/)** - Implement semantic memory layer on top of the Qdrant vector search engine\n- <img src=\"https://api.qoretechnologies.com/api/public/apps/Qorus/qorus-logo.svg\" alt=\"Qorus\" width=\"12\" height=\"12\"> **[Qorus](https://qoretechnologies.com/manual/qorus/current/qorus/sysarch.html#mcp_server)** - Connect to any application, system, or technology and automate your business processes without coding and with AI\n- <img src=\"https://avatars.githubusercontent.com/u/18053493?s=200&v=4\" alt=\"Qonto\" width=\"12\" height=\"12\"> **[Qonto](https://github.com/qonto/qonto-mcp-server)** - Access and interact your Qonto account through LLMs using MCP.\n- <img height=\"12\" width=\"12\" src=\"https://avatars.githubusercontent.com/u/3912814\" alt=\"QuantConnect Logo\" /> **[QuantConnect](https://github.com/QuantConnect/mcp-server)** - Interact with your [QuantConnect](https://www.quantconnect.com/) account to update projects, write strategies, run backtest, and deploying strategies to production live-trading.\n- **[Quickchat AI](https://github.com/incentivai/quickchat-ai-mcp)** - Launch your conversational [Quickchat AI](https://quickchat.ai) agent as an MCP to give AI apps real-time access to its Knowledge Base and conversational capabilities\n- <img height=\"12\" width=\"12\" src=\"https://avatars.githubusercontent.com/u/165178062\" alt=\"Ragie Logo\" /> **[Ragie](https://github.com/ragieai/ragie-mcp-server/)** - Retrieve context from your [Ragie](https://www.ragie.ai) (RAG) knowledge base connected to integrations like Google Drive, Notion, JIRA and more.\n- <img alt=\"favicon\" height=\"12\" width=\"12\" src=\"https://www.ramp.com/favicon.ico\" /> **[Ramp](https://github.com/ramp-public/ramp-mcp)** - Interact with [Ramp](https://ramp.com)'s Developer API to run analysis on your spend and gain insights leveraging LLMs\n- **[Raygun](https://github.com/MindscapeHQ/mcp-server-raygun)** - Interact with your crash reporting and real using monitoring data on your Raygun account\n- <img height=\"12\" width=\"12\" src=\"https://framerusercontent.com/images/CU1m0xFonUl76ZeaW0IdkQ0M.png\" alt=\"Razorpay Logo\" /> **[Razorpay](https://github.com/razorpay/razorpay-mcp-server)** - Razorpay's official MCP server\n- <img height=\"12\" width=\"12\" src=\"https://www.recraft.ai/favicons/icon.svg\" alt=\"Recraft Logo\" /> **[Recraft](https://github.com/recraft-ai/mcp-recraft-server)** - Generate raster and vector (SVG) images using [Recraft](https://recraft.ai). Also you can edit, upscale images, create your own styles, and vectorize raster images\n- <img height=\"12\" width=\"12\" src=\"https://www.redhat.com/favicon.ico\" alt=\"Red Hat Logo\" /> **[Red Hat Insights](https://github.com/RedHatInsights/insights-mcp)** - Interact with [Red Hat Insights](https://www.redhat.com/en/technologies/management/insights) - build images, manage vulnerabilities, or view targeted recommendations.\n- <img height=\"12\" width=\"12\" src=\"https://avatars.githubusercontent.com/u/1529926\" alt=\"Redis Logo\" /> **[Redis](https://github.com/redis/mcp-redis/)** - The Redis official MCP Server offers an interface to manage and search data in Redis.\n- <img height=\"12\" width=\"12\" src=\"https://avatars.githubusercontent.com/u/1529926\" alt=\"Redis Logo\" /> **[Redis Cloud API](https://github.com/redis/mcp-redis-cloud/)** - The Redis Cloud API MCP Server allows you to manage your Redis Cloud resources using natural language.\n- <img src=\"https://avatars.githubusercontent.com/u/149024635\" alt=\"Reexpress\" width=\"12\" height=\"12\"> **[Reexpress](https://github.com/ReexpressAI/reexpress_mcp_server)** - Enable Similarity-Distance-Magnitude statistical verification for your search, software, and data science workflows\n- <img height=\"12\" width=\"12\" src=\"https://cdn.prod.website-files.com/68a872edf3df6064de547670/68b7f089c45a6083ce25acb1_reflag-favicon-32.png\" alt=\"Reflag\" /> **[Reflag](https://github.com/reflagcom/javascript/tree/main/packages/cli#model-context-protocol)** - Create and manage feature flags using [Reflag](https://reflag.com)\n- <img height=\"12\" width=\"12\" src=\"https://www.reltio.com/wp-content/uploads/2024/03/cropped-cropped-Reltio_Light_Mode_Dark_Mode_Favicon-270x270.png\" alt=\"Reltio Logo\" /> **[Reltio](https://github.com/reltio-ai/reltio-mcp-server)** - A lightweight, plugin-based MCP server designed to perform advanced entity matching with language models in Reltio environments.\n- <img height=\"12\" width=\"12\" src=\"https://www.rember.com/favicon.ico\" alt=\"Rember Logo\" /> **[Rember](https://github.com/rember/rember-mcp)** - Create spaced repetition flashcards in [Rember](https://rember.com) to remember anything you learn in your chats\n- <img height=\"12\" width=\"12\" src=\"https://avatars.githubusercontent.com/u/114033652\" alt=\"Render Logo\" /> **[Render](https://render.com/docs/mcp-server)** - The official Render MCP server: spin up new services, run queries against your databases, and debug rapidly with direct access to service metrics and logs.\n- <img height=\"12\" width=\"12\" src=\"https://reportportal.io/favicon.ico\" alt=\"ReportPortal Logo\" /> **[ReportPortal](https://github.com/reportportal/reportportal-mcp-server)** - explore and analyze automated test results from [ReportPortal](https://reportportal.io) using your favourite LLM.\n- <img height=\"12\" width=\"12\" src=\"http://nonica.io/Nonica-logo.ico\" alt=\"Nonica Logo\" /> **[Revit](https://github.com/NonicaTeam/AI-Connector-for-Revit)** - Connect and interact with your Revit models live.\n- <img height=\"12\" width=\"12\" src=\"https://ui.rilldata.com/favicon.png\" alt=\"Rill Data Logo\" /> **[Rill Data](https://docs.rilldata.com/explore/mcp)** - Interact with Rill Data to query and analyze your data.\n- <img height=\"12\" width=\"12\" src=\"https://riza.io/favicon.ico\" alt=\"Riza logo\" /> **[Riza](https://github.com/riza-io/riza-mcp)** - Arbitrary code execution and tool-use platform for LLMs by [Riza](https://riza.io)\n- <img height=\"12\" width=\"12\" src=\"https://cdn.foundation.roblox.com/current/RobloxStudio.ico\" alt=\"Roblox Studio\" /> **[Roblox Studio](https://github.com/Roblox/studio-rust-mcp-server)** - Roblox Studio MCP Server, create and manipulate scenes, scripts in Roblox Studio\n- <img src=\"https://hyper3d.ai/favicon.ico\" alt=\"Rodin\" width=\"12\" height=\"12\"> **[Rodin](https://github.com/DeemosTech/rodin-api-mcp)** - Generate 3D Models with [Hyper3D Rodin](https://hyper3d.ai)\n- <img height=\"12\" width=\"12\" src=\"https://cdn.prod.website-files.com/66b7de6a233c04f4dac200a6/66bed52680d689629483c18b_faviconV2%20(2).png\" alt=\"Root Signals Logo\" /> **[Root Signals](https://github.com/root-signals/root-signals-mcp)** - Improve and quality control your outputs with evaluations using LLM-as-Judge\n- **[Routine](https://github.com/routineco/mcp-server)** - MCP server to interact with [Routine](https://routine.co/): calendars, tasks, notes, etc.\n- <img height=\"12\" width=\"12\" src=\"https://platform.composio.dev/favicon.ico\" alt=\"Composio Logo\"> **[Rube](https://github.com/ComposioHQ/Rube)** - Rube is a Model Context Protocol (MCP) server that connects your AI tools to 500+ apps like Gmail, Slack, GitHub, and Notion. Simply install it in your AI client, authenticate once with your apps, and start asking your AI to perform real actions like \"Send an email\" or \"Create a task.\"\n- <img height=\"12\" width=\"12\" src=\"https://raw.githubusercontent.com/safedep/.github/refs/heads/main/assets/logo/1.png\" alt=\"SafeDep Logo\" /> **[SafeDep](https://github.com/safedep/vet/blob/main/docs/mcp.md)** - SafeDep `vet-mcp` helps in  vetting open source packages for security risks—such as vulnerabilities and malicious code—before they're used in your project, especially with AI-generated code suggestions.\n- <img height=\"12\" width=\"12\" src=\"https://waf-ce.chaitin.cn/favicon.ico\" alt=\"SafeLine Logo\" /> **[SafeLine](https://github.com/chaitin/SafeLine/tree/main/mcp_server)** - [SafeLine](https://safepoint.cloud/landing/safeline) is a self-hosted WAF(Web Application Firewall) to protect your web apps from attacks and exploits.\n- <img height=\"12\" width=\"12\" src=\"https://scrapi.tech/favicon.ico\" alt=\"ScrAPI Logo\" /> **[ScrAPI](https://github.com/DevEnterpriseSoftware/scrapi-mcp)** - Web scraping using [ScrAPI](https://scrapi.tech). Extract website content that is difficult to access because of bot detection, captchas or even geolocation restrictions.\n- <img height=\"12\" width=\"12\" src=\"https://upnorthmedia.co/favicon.ico\" alt=\"Up North Media Logo\" /> **[ScreenshotMCP](https://github.com/upnorthmedia/ScreenshotMCP/)** - A Model Context Protocol MCP server for capturing website screenshots with full page, element, and device size features.\n- <img height=\"12\" width=\"12\" src=\"https://screenshotone.com/favicon.ico\" alt=\"ScreenshotOne Logo\" /> **[ScreenshotOne](https://github.com/screenshotone/mcp/)** - Render website screenshots with [ScreenshotOne](https://screenshotone.com/)\n- <img height=\"12\" width=\"12\" src=\"https://pics.fatwang2.com/56912e614b35093426c515860f9f2234.svg\" alt=\"Search1API Logo\" /> **[Search1API](https://github.com/fatwang2/search1api-mcp)** - One API for Search, Crawling, and Sitemaps\n- <img height=\"12\" width=\"12\" src=\"https://www.searchunify.com/favicon.ico\" alt=\"SearchUnify Logo\" /> **[SearchUnify](https://github.com/searchunify/su-mcp/)** - SearchUnify MCP Server (su-mcp) enables seamless integration of SearchUnify with Claude Desktop\n- <img height=\"12\" width=\"12\" src=\"https://secureframe.com/favicon.ico\" alt=\"Secureframe Logo\" /> **[Secureframe](https://github.com/secureframe/secureframe-mcp-server)** - Query security controls, monitor compliance tests, and access audit data across SOC 2, ISO 27001, CMMC, FedRAMP, and other frameworks from [Secureframe](https://secureframe.com).\n- <img height=\"12\" width=\"12\" src=\"https://semgrep.dev/favicon.ico\" alt=\"Semgrep Logo\" /> **[Semgrep](https://github.com/semgrep/mcp)** - Enable AI agents to secure code with [Semgrep](https://semgrep.dev/).\n- <img height=\"12\" width=\"12\" src=\"https://avatars.githubusercontent.com/u/187640573?s=48&v=4\" alt=\"Sequa Logo\" /> **[Sequa.AI](https://github.com/sequa-ai/sequa-mcp)** - Stop stitching context for Copilot and Cursor. With [Sequa MCP](https://github.com/sequa-ai/sequa-mcp), your AI tools know all your codebases and docs out of the box.\n- <img height=\"12\" width=\"12\" src=\"https://cdn.prod.website-files.com/6372338e5477e047032b37a5/64f85e6388a2a5c8c9525b4d_favLogo.png\" alt=\"Shortcut Logo\" /> **[Shortcut](https://github.com/useshortcut/mcp-server-shortcut)** - Access and implement all of your projects and tasks (Stories) from [Shortcut](https://shortcut.com/).\n- <img alt=\"favicon_32x32_png_v_277b9cbbe31e8bc416504cf3b902d430\" height=\"12\" width=\"12\" src=\"https://www.singlestore.com/favicon-32x32.png?v=277b9cbbe31e8bc416504cf3b902d430\"/> **[SingleStore](https://github.com/singlestore-labs/mcp-server-singlestore)** - Interact with the SingleStore database platform\n- <img height=\"12\" width=\"12\" src=\"https://smartbear.com/smartbear/assets/img/favicon.png\" alt=\"SmartBear Logo\" /> **[SmartBear](https://github.com/SmartBear/smartbear-mcp)** - Provides access to multiple capabilities across SmartBear's API Hub, Test Hub, and Insight Hub, all through [dedicated tools and resources](https://developer.smartbear.com/smartbear-mcp/docs/mcp-server).\n- <img src=\"https://smooth-operator.online/logo48.png\" alt=\"Smooth Operator\" width=\"12\" height=\"12\"> **[Smooth Operator](https://smooth-operator.online/agent-tools-api-docs/toolserverdocs)** - Tools to automate Windows via AI Vision, Mouse, Keyboard, Automation Trees, Webbrowser\n- <img height=\"12\" width=\"12\" src=\"https://app.snyk.io/bundle/favicon-faj49uD9.png\" alt=\"Snyk Logo\" /> **[Snyk](https://github.com/snyk/snyk-ls/blob/main/mcp_extension/README.md)** - Enhance security posture by embedding [Snyk](https://snyk.io/) vulnerability scanning directly into agentic workflows.\n- <img height=\"12\" width=\"12\" src=\"https://www.sonarsource.com/favicon.ico\" alt=\"SonarQube Logo\" /> **[SonarQube](https://github.com/SonarSource/sonarqube-mcp-server)** - Enables seamless integration with [SonarQube](https://www.sonarsource.com/) Server or Cloud and allows for code snippet analysis within the agent context.\n- <img src=\"https://sophtron.com/favicon.ico\" alt=\"Sophtron\" width=\"12\" height=\"12\"> **[Sophtron](https://github.com/sophtron/Sophtron-Integration/tree/main/modelcontextprotocol)** - Connect to your bank, credit card, utilities accounts to retrieve account balances and transactions with [Sophtron Bank Integration](https://sophtron.com).\n- <img height=\"12\" width=\"12\" src=\"https://www.stackhawk.com/wp-content/uploads/2025/03/icon-512x512-2-150x150.png\" alt=\"StackHawk Logo\" /> **[StackHawk](https://github.com/stackhawk/stackhawk-mcp)** - Use [StackHawk](https://www.stackhawk.com/) to test for and FIX security problems in your code or vibe coded app.\n- <img height=\"12\" width=\"12\" src=\"https://www.starrocks.io/favicon.ico\" alt=\"StarRocks Logo\" /> **[StarRocks](https://github.com/StarRocks/mcp-server-starrocks)** - Interact with [StarRocks](https://www.starrocks.io/)\n- <img height=\"12\" width=\"12\" src=\"https://downloads.steadybit.com/logomark.svg\" alt=\"Steadybit Logo\" /> **[Steadybit](https://github.com/steadybit/mcp)** - Interact with [Steadybit](https://www.steadybit.com/)\n- <img height=\"12\" width=\"12\" src=\"https://steuerboard.net/favicon.ico\" alt=\"Steuerboard Logo\" /> **[Steuerboard](https://github.com/steuerboard/steuerboard-mcp-typescript)** - Interact with the accounting data in your business using our official MCP server\n- <img height=\"12\" width=\"12\" src=\"https://avatars.githubusercontent.com/u/22632046?s=200&v=4\" alt=\"Storybook Logo\" /> **[Storybook](https://github.com/storybookjs/addon-mcp)** - Interact with [Storybook](https://storybook.js.org/) to automate UI component testing and documentation\n- <img height=\"12\" width=\"12\" src=\"https://stripe.com/favicon.ico\" alt=\"Stripe Logo\" /> **[Stripe](https://github.com/stripe/agent-toolkit)** - Interact with Stripe API\n- <img height=\"12\" width=\"12\" src=\"https://sunra.ai/favicon.ico\" alt=\"Sunra AI Logo\" /> **[Sunra AI](https://github.com/sunra-ai/sunra-clients/tree/main/mcp-server)** - Search for and run AI models on [Sunra.ai](https://sunra.ai). Discover models, create video, image, and 3D model content, track their status, and manage the generated media.\n- <img height=\"12\" width=\"12\" src=\"https://supabase.com/favicon/favicon.ico\" alt=\"Supabase Logo\" /> **[Supabase](https://github.com/supabase-community/supabase-mcp)** - Interact with Supabase: Create tables, query data, deploy edge functions, and more.\n- <img height=\"12\" width=\"12\" src=\"https://supadata.ai/favicon.ico\" alt=\"Supadata Logo\" /> **[Supadata](https://github.com/supadata-ai/mcp)** - Official MCP server for [Supadata](https://supadata.ai) - YouTube, TikTok, X and Web data for makers.\n- <img height=\"12\" width=\"12\" src=\"https://d12w4pyrrczi5e.cloudfront.net/archive/50eb154ab859c63a8f1c850f9fe094e25d35e929/images/favicon.ico\" alt=\"Tako Logo\" /> **[Tako](https://github.com/TakoData/tako-mcp)** - Use natural language to search [Tako](https://trytako.com) for real-time financial, sports, weather, and public data with visualization\n- <img height=\"12\" width=\"12\" src=\"https://tavily.com/favicon.ico\" alt=\"Tavily Logo\" /> **[Tavily](https://github.com/tavily-ai/tavily-mcp)** - Search engine for AI agents (search + extract) powered by [Tavily](https://tavily.com/)\n- <img height=\"12\" width=\"12\" src=\"https://avatars.githubusercontent.com/u/10522416?s=200&v=4\" alt=\"Telnyx Logo\" /> **[Telnyx](https://github.com/team-telnyx/telnyx-mcp-server)** - Official MCP server for building AI-powered communication apps. Create voice assistants, send SMS campaigns, manage phone numbers, and integrate real-time messaging with enterprise-grade reliability. Includes remote [streamable-http](https://api.telnyx.com/v2/mcp) and [sse](https://api.telnyx.com/mcp/sse) servers.\n- <img height=\"12\" width=\"12\" src=\"https://avatars.githubusercontent.com/u/1615979?s=200&v=4\" alt=\"Teradata Logo\" /> **[Teradata](https://github.com/Teradata/teradata-mcp-server)** - This MCP Server support tools and prompts for multi task data analytics on a [Teradata](https://teradata.com) platform.\n- <img height=\"12\" width=\"12\" src=\"https://raw.githubusercontent.com/hashicorp/terraform-mcp-server/main/public/images/Terraform-LogoMark_onDark.svg\" alt=\"Terraform Logo\" /> **[Terraform](https://github.com/hashicorp/terraform-mcp-server)** - Seamlessly integrate with Terraform ecosystem, enabling advanced automation and interaction capabilities for Infrastructure as Code (IaC) development powered by [Terraform](https://www.hashicorp.com/en/products/terraform)\n- <img height=\"12\" width=\"12\" src=\"https://www.textin.com/favicon.png\" alt=\"TextIn Logo\" /> **[TextIn](https://github.com/intsig-textin/textin-mcp)** - An MCP server for the [TextIn](https://www.textin.com/?from=github_mcp) API, is a tool for extracting text and performing OCR on documents, it also supports converting documents into Markdown\n- <img height=\"12\" width=\"12\" src=\"https://avatars.githubusercontent.com/u/106156665?s=200\" alt=\"Thena Logo\" /> **[Thena](https://mcp.thena.ai)** - Thena's MCP server for enabling users and AI agents to interact with Thena's services and manage customers across different channels such as Slack, Email, Web, Discord etc.\n- <img height=\"12\" width=\"12\" src=\"https://avatars.githubusercontent.com/u/24291394?v=4\" alt=\"ThingsBoard\" /> **[ThingsBoard](https://github.com/thingsboard/thingsboard-mcp)** - The ThingsBoard MCP Server provides a natural language interface for LLMs and AI agents to interact with your ThingsBoard IoT platform.\n- <img height=\"12\" width=\"12\" src=\"https://www.lg.com/favicon.ico\" alt=\"ThinQ Logo\" /> **[ThinQ Connect](https://github.com/thinq-connect/thinqconnect-mcp)** - Interact with LG ThinQ smart home devices and appliances through the ThinQ Connect MCP server.\n- <img height=\"12\" width=\"12\" src=\"https://thirdweb.com/favicon.ico\" alt=\"Thirdweb Logo\" /> **[Thirdweb](https://github.com/thirdweb-dev/ai/tree/main/python/thirdweb-mcp)** - Read/write to over 2k blockchains, enabling data querying, contract analysis/deployment, and transaction execution, powered by [Thirdweb](https://thirdweb.com/)\n- <img height=\"12\" width=\"12\" src=\"https://www.thoughtspot.com/favicon-16x16.png\" alt=\"ThoughtSpot Logo\" /> **[ThoughtSpot](https://github.com/thoughtspot/mcp-server)** - AI is the new BI. A dedicated data analyst for everyone on your team. Bring [ThoughtSpot](https://thoughtspot.com) powers into Claude or any MCP host.\n- <img height=\"12\" width=\"12\" src=\"https://tianji.msgbyte.com/img/dark-brand.svg\" alt=\"Tianji Logo\" /> **[Tianji](https://github.com/msgbyte/tianji/tree/master/apps/mcp-server)** - Interact with Tianji platform whatever selfhosted or cloud platform, powered by [Tianji](https://tianji.msgbyte.com/).\n- <img height=\"12\" width=\"12\" src=\"https://www.pingcap.com/favicon.ico\" alt=\"TiDB Logo\" /> **[TiDB](https://github.com/pingcap/pytidb)** - MCP Server to interact with TiDB database platform.\n- <img height=\"12\" width=\"12\" src=\"https://www.tinybird.co/favicon.ico\" alt=\"Tinybird Logo\" /> **[Tinybird](https://github.com/tinybirdco/mcp-tinybird)** - Interact with Tinybird serverless ClickHouse platform\n- <img height=\"12\" width=\"12\" src=\"https://b2729162.smushcdn.com/2729162/wp-content/uploads/2023/10/cropped-Favicon-1-192x192.png?lossy=1&strip=1&webp=1\" alt=\"Tldv Logo\" /> **[Tldv](https://gitlab.com/tldv/tldv-mcp-server)** - Connect your AI agents to Google-Meet, Zoom & Microsoft Teams through [tl;dv](https://tldv.io)\n- <img height=\"12\" width=\"12\" src=\"https://www.todoist.com/static/favicon-32x32.png\" alt=\"Todoist Logo\" /> **[Todoist](https://github.com/doist/todoist-ai)** - Search, add, and update [Todoist](https://todoist.com) tasks, projects, sections, comments, and more.\n- <img height=\"12\" width=\"12\" src=\"https://cdn.tokenmetrics.com/logo.svg\" alt=\"Token Metrics Logo\" /> **[Token Metrics](https://github.com/token-metrics/mcp)** - [Token Metrics](https://www.tokenmetrics.com/) integration for fetching real-time crypto market data, trading signals, price predictions, and advanced analytics.\n- <img height=\"12\" width=\"12\" src=\"https://di8m9w6rqrh5d.cloudfront.net/2G3TRwfv1w3GTLfmT7Dmco1VddoFTI5P/1920_6b7e7ec2-d897-4cd7-94f3-46a8301212c3.png\" alt=\"TomTom Logo\" /> **[TomTom-MCP](https://github.com/tomtom-international/tomtom-mcp)** - The [TomTom](https://www.tomtom.com/) MCP Server simplifies geospatial development by providing seamless access to TomTom's location services, including search, routing, traffic and static maps data.\n- <img height=\"12\" width=\"12\" src=\"https://images.thetradeagent.ai/trade_agent/logo.svg\" alt=\"Trade Agent Logo\" /> **[Trade Agent](https://github.com/Trade-Agent/trade-agent-mcp)** - Execute stock and crypto trades on your brokerage via [Trade Agent](https://thetradeagent.ai)\n-  **[Twelve Data](https://github.com/twelvedata/mcp)** — Integrate your AI agents with real-time and historical financial market data through our official [Twelve Data](https://twelvedata.com) MCP server.\n- <img height=\"12\" width=\"12\" src=\"https://www.twilio.com/content/dam/twilio-com/core-assets/social/favicon-16x16.png\" alt=\"Twilio Logo\" /> **[Twilio](https://github.com/twilio-labs/mcp)** - Interact with [Twilio](https://www.twilio.com/en-us) APIs to send SMS messages, manage phone numbers, configure your account, and more.\n- <img height=\"12\" width=\"12\" src=\"https://avatars.githubusercontent.com/u/91520705?s=48&v=4\" alt=\"Tencent RTC Logo\" /> **[Tencent RTC](https://github.com/Tencent-RTC/mcp)** - The MCP Server enables AI IDEs to more effectively understand and use [Tencent's Real-Time Communication](https://trtc.io/) SDKs and APIs, which significantly streamlines the process for developers to build audio/video call applications.\n- <img height=\"12\" width=\"12\" src=\"https://uberall.com/media/favicon.svg\" alt=\"Uberall Logo\" /> **[Uberall](https://github.com/uberall/uberall-mcp-server)** – Manage multi - location presence, including listings, reviews, and social posting, via [uberall](https://uberall.com).\n- <img height=\"12\" width=\"12\" src=\"https://avatars.githubusercontent.com/u/91906527\" alt=\"Unblocked Logo\" /> **[Unblocked](https://docs.getunblocked.com/unblocked-mcp)** Help your AI-powered IDEs generate faster, more accurate code by giving them access to context from Slack, Confluence, Google Docs, JIRA, and more with [Unblocked](https://getunblocked.com).\n- <img height=\"12\" width=\"12\" src=\"https://unifai.network/favicon.ico\" alt=\"UnifAI Logo\" /> **[UnifAI](https://github.com/unifai-network/unifai-mcp-server)** - Dynamically search and call tools using [UnifAI Network](https://unifai.network)\n- <img height=\"12\" width=\"12\" src=\"https://framerusercontent.com/images/plcQevjrOYnyriuGw90NfQBPoQ.jpg\" alt=\"Unstructured Logo\" /> **[Unstructured](https://github.com/Unstructured-IO/UNS-MCP)** - Set up and interact with your unstructured data processing workflows in [Unstructured Platform](https://unstructured.io)\n- <img height=\"12\" width=\"12\" src=\"https://upstash.com/icons/favicon-32x32.png\" alt=\"Upstash Logo\" /> **[Upstash](https://github.com/upstash/mcp-server)** - Manage Redis databases and run Redis commands on [Upstash](https://upstash.com/) with natural language.\n-  **[Vantage](https://github.com/vantage-sh/vantage-mcp-server)** - Interact with your organization's cloud cost spend.\n- <img height=\"12\" width=\"12\" src=\"https://mcp.variflight.com/favicon.ico\" alt=\"VariFlight Logo\" /> **[VariFlight](https://github.com/variflight/variflight-mcp)** - VariFlight's official MCP server provides tools to query flight information, weather data, comfort metrics, the lowest available fares, and other civil aviation-related data.\n- <img height=\"12\" width=\"12\" src=\"https://docs.octagonagents.com/logo.svg\" alt=\"Octagon Logo\" /> **[VCAgents](https://github.com/OctagonAI/octagon-vc-agents)** - Interact with investor agents—think Wilson or Thiel—continuously updated with market intel.\n- **[Vectorize](https://github.com/vectorize-io/vectorize-mcp-server/)** - [Vectorize](https://vectorize.io) MCP server for advanced retrieval, Private Deep Research, Anything-to-Markdown file extraction and text chunking.\n- <img height=\"12\" width=\"12\" src=\"https://static.verbwire.com/favicon-16x16.png\" alt=\"Verbwire Logo\" /> **[Verbwire](https://github.com/verbwire/verbwire-mcp-server)** - Deploy smart contracts, mint NFTs, manage IPFS storage, and more through the Verbwire API\n- <img height=\"12\" width=\"12\" src=\"http://vercel.com/favicon.ico\" alt=\"Vercel Logo\" /> **[Vercel](https://vercel.com/docs/mcp/vercel-mcp)** - Access logs, search docs, and manage projects and deployments.\n- <img height=\"12\" width=\"12\" src=\"https://verodat.io/assets/favicon-16x16.png\" alt=\"Verodat Logo\" /> **[Verodat](https://github.com/Verodat/verodat-mcp-server)** - Interact with Verodat AI Ready Data platform\n- <img height=\"12\" width=\"12\" src=\"https://www.veyrax.com/favicon.ico\" alt=\"VeyraX Logo\" /> **[VeyraX](https://github.com/VeyraX/veyrax-mcp)** - Single tool to control all 100+ API integrations, and UI components\n- <img height=\"12\" width=\"12\" src=\"https://avatars.githubusercontent.com/u/174736222?s=200&v=4\" alt=\"VictoriaMetrics Logo\" /> **[VictoriaMetrics](https://github.com/VictoriaMetrics-Community/mcp-victoriametrics)** - Comprehensive integration with [VictoriaMetrics APIs](https://docs.victoriametrics.com/victoriametrics/url-examples/) and [documentation](https://docs.victoriametrics.com/) for monitoring, observability, and debugging tasks related to your VictoriaMetrics instances.\n- <img height=\"12\" width=\"12\" src=\"https://framerusercontent.com/images/ijlYG00LOcMD6zR1XLMxHbAwZkM.png\" alt=\"VideoDB Director\" /> **[VideoDB Director](https://github.com/video-db/agent-toolkit/tree/main/modelcontextprotocol)** - Create AI-powered video workflows including automatic editing, content moderation, voice cloning, highlight generation, and searchable video moments—all accessible via simple APIs and intuitive chat-based interfaces.\n- <img height=\"12\" width=\"12\" src=\"https://landing.ai/wp-content/uploads/2024/04/cropped-favicon-192x192.png\" alt=\"LandingAI VisionAgent\" /> **[VisionAgent MCP](https://github.com/landing-ai/vision-agent-mcp)** - A simple MCP server that enables your LLM to better reason over images, video and documents.\n- <img height=\"12\" width=\"12\" src=\"https://raw.githubusercontent.com/mckinsey/vizro/main/vizro-core/docs/assets/images/favicon.png\" alt=\"Vizro Logo\" /> **[Vizro](https://github.com/mckinsey/vizro/tree/main/vizro-mcp)** - Tools and templates to create validated and maintainable data charts and dashboards\n- <img height=\"12\" width=\"12\" src=\"https://wavespeed.ai/logo.webp\" alt=\"WaveSpeed Logo\" /> **[WaveSpeed](https://github.com/WaveSpeedAI/mcp-server)** - WaveSpeed MCP server providing AI agents with image and video generation capabilities.\n- <img height=\"12\" width=\"12\" src=\"https://waystation.ai/images/logo.svg\" alt=\"WayStation Logo\" /> **[WayStation](https://github.com/waystation-ai/mcp)** - Universal MCP server to connect to popular productivity tools such as Notion, Monday, AirTable, and many more\n- <img height=\"12\" width=\"12\" src=\"https://static.whatsapp.net/rsrc.php/v3/yz/r/ujTY9i_Jhs1.png\" alt=\"WhatsApp Business Logo\" /> **[WhatsApp Business](https://medium.com/@wassenger/introducing-whatsapp-mcp-ai-connector-3d393b52d1b0)** - WhatsApp Business MCP connector enabling AI agents to send messages, manage conversations, access templates, and integrate with WhatsApp Business API for automated customer communication.\n- <img height=\"12\" width=\"12\" src=\"https://www.webflow.com/favicon.ico\" alt=\"Webflow Logo\"> **[Webflow](https://github.com/webflow/mcp-server)** - Interact with Webflow sites, pages, and collections\n- <img height=\"12\" width=\"12\" src=\"https://webscraping.ai/favicon.ico\" alt=\"WebScraping.AI Logo\" /> **[WebScraping.AI](https://github.com/webscraping-ai/webscraping-ai-mcp-server)** - Interact with **[WebScraping.AI](https://WebScraping.AI)** for web data extraction and scraping\n- <img height=\"12\" width=\"12\" src=\"https://winston-app-production-public.s3.us-east-1.amazonaws.com/winston-ai-favicon-light.svg\" alt=\"Winston.AI Logo\" /> **[Winston AI](https://github.com/gowinston-ai/winston-ai-mcp-server)** - AI detector MCP server with industry leading accuracy rates in detecting use of AI in text and images. The [Winston AI](https://gowinston.ai) MCP server also offers a robust plagiarism checker to help maintain integrity.\n- <img height=\"12\" width=\"12\" src=\"https://www.xero.com/favicon.ico\" alt=\"Xero Logo\" /> **[Xero](https://github.com/XeroAPI/xero-mcp-server)** - Interact with the accounting data in your business using our official MCP server\n- <img height=\"12\" width=\"12\" src=\"https://storage.yandexcloud.net/ydb-www-prod-site-assets/favicon-202305/favicon.ico\" alt=\"YDB Logo\" /> **[YDB](https://github.com/ydb-platform/ydb-mcp)** - Query [YDB](https://ydb.tech/) databases\n- <img height=\"12\" width=\"12\" src=\"https://fe-resource.yeelight.com/logo-black.jpeg\" alt=\"Yeelight Logo\" /> **[Yeelight MCP Server](https://github.com/Yeelight/yeelight-iot-mcp)** - The official [Yeelight MCP Server](https://github.com/Yeelight/yeelight-iot-mcp) enables users to control and query their [Yeelight](https://en.yeelight.com/) smart devices using natural language, offering a seamless and efficient human-AI interaction experience.\n- <img height=\"12\" width=\"12\" src=\"https://cdn.prod.website-files.com/632cd328ed2b485519c3f689/6334977a5d1a542102d4b9b5_favicon-32x32.png\" alt=\"YepCode Logo\" /> **[YepCode](https://github.com/yepcode/mcp-server-js)** - Run code in a secure, scalable sandbox environment with full support for dependencies, secrets, logs, and access to APIs or databases. Powered by [YepCode](https://yepcode.io)\n- <img height=\"12\" width=\"12\" src=\"https://www.yugabyte.com/favicon-16x16.png\" alt=\"YugabyteDB Logo\" /> **[YugabyteDB](https://github.com/yugabyte/yugabytedb-mcp-server)** -  MCP Server to interact with your [YugabyteDB](https://www.yugabyte.com/) database\n- <img height=\"12\" width=\"12\" src=\"https://avatars.githubusercontent.com/u/14069894\" alt=\"Yunxin Logo\" /> **[Yunxin](https://github.com/netease-im/yunxin-mcp-server)** - An MCP server that connects to Yunxin's IM/RTC/DATA Open-API\n- <img height=\"12\" width=\"12\" src=\"https://cdn.zapier.com/zapier/images/favicon.ico\" alt=\"Zapier Logo\" /> **[Zapier](https://zapier.com/mcp)** - Connect your AI Agents to 8,000 apps instantly.\n- <img height=\"12\" width=\"12\" src=\"https://www.zenable.app/zenable_light.svg\" alt=\"Zenable Logo\" /> **[Zenable](https://docs.zenable.io/integrations/mcp/getting-started)** - Clean up sloppy AI code and prevent vulnerabilities\n- **[ZenML](https://github.com/zenml-io/mcp-zenml)** - Interact with your MLOps and LLMOps pipelines through your [ZenML](https://www.zenml.io) MCP server\n- <img height=\"12\" width=\"12\" src=\"https://www.zine.ai/images/zine-logo.png\" alt=\"Zine Logo\" /> **[Zine](https://www.zine.ai)** - Your memory, everywhere AI goes. Think iPhoto for your knowledge - upload and curate. Like ChatGPT but portable - context that travels with you.\n- <img height=\"12\" width=\"12\" src=\"https://zizai.work/images/logo.jpg\" alt=\"ZIZAI Logo\" /> **[ZIZAI Recruitment](https://github.com/zaiwork/mcp)** - Interact with the next-generation intelligent recruitment platform for employees and employers, powered by [ZIZAI Recruitment](https://zizai.work).\n\n### 🌎 Community Servers\n\nA growing set of community-developed and maintained servers demonstrates various applications of MCP across different domains.\n\n> [!NOTE]\n> Community servers are **untested** and should be used at **your own risk**. They are not affiliated with or endorsed by Anthropic.\n\n- **[1mcpserver](https://github.com/particlefuture/1mcpserver)** - MCP of MCPs. Automatically discover, configure, and add MCP servers on your local machine.\n- **[1Panel](https://github.com/1Panel-dev/mcp-1panel)** - MCP server implementation that provides 1Panel interaction.\n- **[A2A](https://github.com/GongRzhe/A2A-MCP-Server)** - An MCP server that bridges the Model Context Protocol (MCP) with the Agent-to-Agent (A2A) protocol, enabling MCP-compatible AI assistants (like Claude) to seamlessly interact with A2A agents.\n- **[Ableton Live](https://github.com/Simon-Kansara/ableton-live-mcp-server)** - an MCP server to control Ableton Live.\n- **[Ableton Live](https://github.com/ahujasid/ableton-mcp)** (by ahujasid) - Ableton integration allowing prompt enabled music creation.\n- **[Actor Critic Thinking](https://github.com/aquarius-wing/actor-critic-thinking-mcp)** - Actor-critic thinking for performance evaluation\n- **[Adobe Commerce](https://github.com/rafaelstz/adobe-commerce-dev-mcp)** — MCP to interact with Adobe Commerce GraphQL API, including orders, products, customers, etc.\n- **[ADR Analysis](https://github.com/tosin2013/mcp-adr-analysis-server)** - AI-powered Architectural Decision Records (ADR) analysis server that provides architectural insights, technology stack detection, security checks, and TDD workflow enhancement for software development projects.\n- **[AgentBay](https://github.com/Michael98671/agentbay)** - An MCP server for providing serverless cloud infrastructure for AI agents.\n- **[AgentMode](https://www.agentmode.app)** - Connect to dozens of databases, data warehouses, Github & more, from a single MCP server.  Run the Docker image locally, in the cloud, or on-premise.\n- **[AI Agent Marketplace Index](https://github.com/AI-Agent-Hub/ai-agent-marketplace-index-mcp)** - MCP server to search more than 5000+ AI agents and tools of various categories from [AI Agent Marketplace Index](http://www.deepnlp.org/store/ai-agent) and monitor traffic of AI Agents.\n- **[AI Tasks](https://github.com/jbrinkman/valkey-ai-tasks)** - Let the AI manage complex plans with integrated task management and tracking tools. Supports STDIO, SSE and Streamable HTTP transports.\n- **[ai-Bible](https://github.com/AdbC99/ai-bible)** - Search the bible reliably and repeatably [ai-Bible Labs](https://ai-bible.com)\n- **[Airbnb](https://github.com/openbnb-org/mcp-server-airbnb)** - Provides tools to search Airbnb and get listing details.\n- **[Airflow](https://github.com/yangkyeongmo/mcp-server-apache-airflow)** - An MCP Server that connects to [Apache Airflow](https://airflow.apache.org/) using official python client.\n- **[Airtable](https://github.com/domdomegg/airtable-mcp-server)** - Read and write access to [Airtable](https://airtable.com/) databases, with schema inspection.\n- **[Airtable](https://github.com/felores/airtable-mcp)** - Airtable Model Context Protocol Server.\n- **[Algorand](https://github.com/GoPlausible/algorand-mcp)** - A comprehensive MCP server for tooling interactions (40+) and resource accessibility (60+) plus many useful prompts for interacting with the Algorand blockchain.\n- **[Amadeus](https://github.com/donghyun-chae/mcp-amadeus)** (by donghyun-chae) - An MCP server to access, explore, and interact with Amadeus Flight Offers Search API for retrieving detailed flight options, including airline, times, duration, and pricing data.\n- **[Amazon Ads](https://github.com/MarketplaceAdPros/amazon-ads-mcp-server)** - MCP Server that provides interaction capabilities with Amazon Advertising through [MarketplaceAdPros](https://marketplaceadpros.com)/\n- **[AniList](https://github.com/yuna0x0/anilist-mcp)** (by yuna0x0) - An MCP server to interact with AniList API, allowing you to search for anime and manga, retrieve user data, and manage your watchlist.\n- **[Anki](https://github.com/scorzeth/anki-mcp-server)** - An MCP server for interacting with your [Anki](https://apps.ankiweb.net) decks and cards.\n- **[Anki](https://github.com/nietus/anki-mcp)** - MCP server to run locally with Anki and Ankiconnect. Supports creating, updating, searching and filtering cards and decks. Include mass update and other advanced tools.\n- **[AntV Chart](https://github.com/antvis/mcp-server-chart)** - A Model Context Protocol server for generating 15+ visual charts using [AntV](https://github.com/antvis).\n- **[Any Chat Completions](https://github.com/pyroprompts/any-chat-completions-mcp)** - Interact with any OpenAI SDK Compatible Chat Completions API like OpenAI, Perplexity, Groq, xAI and many more.\n- **[Apache Gravitino(incubating)](https://github.com/datastrato/mcp-server-gravitino)** - Allow LLMs to explore metadata of structured data and unstructured data with Gravitino, and perform data governance tasks including tagging/classification.\n- **[API Lab MCP](https://github.com/atototo/api-lab-mcp)** - Transform Claude into your AI-powered API testing laboratory. Test, debug, and document APIs through natural conversation with authentication support, response validation, and performance metrics.\n- **[APIWeaver](https://github.com/GongRzhe/APIWeaver)** - An MCP server that dynamically creates MCP  servers from web API configurations. This allows you to easily integrate any REST API, GraphQL endpoint, or web service into an MCP-compatible tool that can be used by AI assistants like Claude.\n- **[Apollo IO MCP Server](https://github.com/AgentX-ai/apollo-io-mcp-server)** - apollo.io mcp server. Get/enrich contact data for people and organizations agentically.\n- **[Apple Books](https://github.com/vgnshiyer/apple-books-mcp)** - Interact with your library on Apple Books, manage your book collection, summarize highlights, notes, and much more.\n- **[Apple Calendar](https://github.com/Omar-v2/mcp-ical)** - An MCP server that allows you to interact with your macOS Calendar through natural language, including features such as event creation, modification, schedule listing, finding free time slots etc.\n- **[Apple Docs](https://github.com/kimsungwhee/apple-docs-mcp)** - A powerful Model Context Protocol (MCP) server that provides seamless access to Apple Developer Documentation through natural language queries. Search, explore, and get detailed information about Apple frameworks, APIs, sample code, and more directly in your AI-powered development environment.\n- **[Apple Script](https://github.com/peakmojo/applescript-mcp)** - MCP server that lets LLM run AppleScript code to to fully control anything on Mac, no setup needed.\n- **[APT MCP](https://github.com/GdMacmillan/apt-mcp-server)** - MCP server which runs debian package manager (apt) commands for you using ai agents.\n- **[Aranet4](https://github.com/diegobit/aranet4-mcp-server)** - MCP Server to manage your Aranet4 CO2 sensor. Fetch data and store in a local SQLite. Ask questions about historical data.\n- **[ArangoDB](https://github.com/ravenwits/mcp-server-arangodb)** - MCP Server that provides database interaction capabilities through [ArangoDB](https://arangodb.com/).\n- **[ArangoDB Graph](https://github.com/PCfVW/mcp-arangodb-async)** - Async-first Python architecture, wrapping the official [python-arango driver](https://github.com/arangodb/python-arango) with graph management capabilities, content conversion utilities (JSON, Markdown, YAML and Table), backup/restore functionality, and graph analytics capabilities; the 33 MCP tools use strict [Pydantic](https://github.com/pydantic/pydantic) validation.\n- **[Arduino](https://github.com/vishalmysore/choturobo)** - MCP Server that enables AI-powered robotics using Claude AI and Arduino (ESP32) for real-world automation and interaction with robots.\n- **[arXiv API](https://github.com/prashalruchiranga/arxiv-mcp-server)** - An MCP server that enables interacting with the arXiv API using natural language.\n- **[arxiv-latex-mcp](https://github.com/takashiishida/arxiv-latex-mcp)** - MCP server that fetches and processes arXiv LaTeX sources for precise interpretation of mathematical expressions in papers.\n- **[Atlassian](https://github.com/sooperset/mcp-atlassian)** - Interact with Atlassian Cloud products (Confluence and Jira) including searching/reading Confluence spaces/pages, accessing Jira issues, and project metadata.\n- **[Atlassian Server (by phuc-nt)](https://github.com/phuc-nt/mcp-atlassian-server)** - An MCP server that connects AI agents (Cline, Claude Desktop, Cursor, etc.) to Atlassian Jira & Confluence, enabling data queries and actions through the Model Context Protocol.\n- **[Attestable MCP](https://github.com/co-browser/attestable-mcp-server)** - An MCP server running inside a trusted execution environment (TEE) via Gramine, showcasing remote attestation using [RA-TLS](https://gramine.readthedocs.io/en/stable/attestation.html). This allows an MCP client to verify the server before connecting.\n- **[Audius](https://github.com/glassBead-tc/audius-mcp-atris)** - Audius + AI = Atris. Interact with fans, stream music, tip your favorite artists, and more on Audius: all through Claude.\n- **[AutoML](https://github.com/emircansoftware/MCP_Server_DataScience)** – An MCP server for data analysis workflows including reading, preprocessing, feature engineering, model selection, visualization, and hyperparameter tuning.\n- **[AX-Platform](https://github.com/AX-MCP/PaxAI?tab=readme-ov-file#mcp-setup-guides)** - AI Agent collaboration platform. Collaborate on tasks, share context, and coordinate workflows.\n- **[AWS](https://github.com/rishikavikondala/mcp-server-aws)** - Perform operations on your AWS resources using an LLM.\n- **[AWS Athena](https://github.com/lishenxydlgzs/aws-athena-mcp)** - An MCP server for AWS Athena to run SQL queries on Glue Catalog.\n- **[AWS Cognito](https://github.com/gitCarrot/mcp-server-aws-cognito)** - An MCP server that connects to AWS Cognito for authentication and user management.\n- **[AWS Cost Explorer](https://github.com/aarora79/aws-cost-explorer-mcp-server)** - Optimize your AWS spend (including Amazon Bedrock spend) with this MCP server by examining spend across regions, services, instance types and foundation models ([demo video](https://www.youtube.com/watch?v=WuVOmYLRFmI&feature=youtu.be)).\n- **[AWS Resources Operations](https://github.com/baryhuang/mcp-server-aws-resources-python)** - Run generated python code to securely query or modify any AWS resources supported by boto3.\n- **[AWS S3](https://github.com/aws-samples/sample-mcp-server-s3)** - A sample MCP server for AWS S3 that flexibly fetches objects from S3 such as PDF documents.\n- **[AWS SES](https://github.com/aws-samples/sample-for-amazon-ses-mcp)** Sample MCP Server for Amazon SES (SESv2). See [AWS blog post](https://aws.amazon.com/blogs/messaging - and-targeting/use-ai-agents-and-the-model-context-protocol-with-amazon-ses/) for more details.\n- **[Azure ADX](https://github.com/pab1it0/adx-mcp-server)** - Query and analyze Azure Data Explorer databases.\n- **[Azure DevOps](https://github.com/Vortiago/mcp-azure-devops)** - An MCP server that provides a bridge to Azure DevOps services, enabling AI assistants to query and manage work items.\n- **[Azure MCP Hub](https://github.com/Azure-Samples/mcp)** - A curated list of all MCP servers and related resources for Azure developers by **[Arun Sekhar](https://github.com/achandmsft)**\n- **[Azure OpenAI DALL-E 3 MCP Server](https://github.com/jacwu/mcp-server-aoai-dalle3)** - An MCP server for Azure OpenAI DALL-E 3 service to generate image from text.\n- **[Azure Wiki Search](https://github.com/coder-linping/azure-wiki-search-server)** - An MCP that enables AI to query the wiki hosted on Azure Devops Wiki.\n- **[Baidu AI Search](https://github.com/baidubce/app-builder/tree/master/python/mcp_server/ai_search)** - Web search with Baidu Cloud's AI Search\n- **[BambooHR MCP](https://github.com/encoreshao/bamboohr-mcp)** - An MCP server that interfaces with the BambooHR APIs, providing access to employee data, time tracking, and HR management features.\n- **[Base Free USDC Transfer](https://github.com/magnetai/mcp-free-usdc-transfer)** - Send USDC on [Base](https://base.org) for free using Claude AI! Built with [Coinbase CDP](https://docs.cdp.coinbase.com/mpc-wallet/docs/welcome).\n- **[Basic Memory](https://github.com/basicmachines-co/basic-memory)** - Local-first knowledge management system that builds a semantic graph from Markdown files, enabling persistent memory across conversations with LLMs.\n- **[BGG MCP](https://github.com/kkjdaniel/bgg-mcp)** (by kkjdaniel) - MCP to enable interaction with the BoardGameGeek API via AI tooling.\n- **[Bible](https://github.com/trevato/bible-mcp)** - Add biblical context to your generative AI applications.\n- **[BigQuery](https://github.com/LucasHild/mcp-server-bigquery)** (by LucasHild) - This server enables LLMs to inspect database schemas and execute queries on BigQuery.\n- **[BigQuery](https://github.com/ergut/mcp-bigquery-server)** (by ergut) - Server implementation for Google BigQuery integration that enables direct BigQuery database access and querying capabilities\n- **[Bilibili](https://github.com/wangshunnn/bilibili-mcp-server)** - This MCP server provides tools to fetch Bilibili user profiles, video metadata, search videos, and more.\n- **[Binance](https://github.com/ethancod1ng/binance-mcp-server)** - Cryptocurrency trading and market data access through Binance API integration.\n- **[Binance](https://github.com/AnalyticAce/BinanceMCPServer)** (by dosseh shalom) - Unofficial tools and server implementation for Binance's Model Context Protocol (MCP). Designed to support developers building crypto trading AI Agents.\n- **[Bing Web Search API](https://github.com/leehanchung/bing-search-mcp)** (by hanchunglee) - Server implementation for Microsoft Bing Web Search API.\n- **[BioMCP](https://github.com/genomoncology/biomcp)** (by imaurer) - Biomedical research assistant server providing access to PubMed, ClinicalTrials.gov, and MyVariant.info.\n- **[bioRxiv](https://github.com/JackKuo666/bioRxiv-MCP-Server)** - 🔍 Enable AI assistants to search and access bioRxiv papers through a simple MCP interface.\n- **[Bitable MCP](https://github.com/lloydzhou/bitable-mcp)** (by lloydzhou) - MCP server provides access to Lark Bitable through the Model Context Protocol. It allows users to interact with Bitable tables using predefined tools.\n- **[Blender](https://github.com/ahujasid/blender-mcp)** (by ahujasid) - Blender integration allowing prompt enabled 3D scene creation, modeling and manipulation.\n- **[Blender MCP](https://github.com/pranav-deshmukh/blender-mcp)** - MCP server to create professional like 3d scenes on blender using natural language.\n- **[Blockbench MCP Plugin](https://github.com/jasonjgardner/blockbench-mcp-plugin)** (by jasonjgardner) - Blockbench plugin to connect AI agents to Blockbench's JavaScript API. Allows for creating and editing 3D models or pixel art textures with AI in Blockbench.\n- **[Blockchain MCP](https://github.com/tatumio/blockchain-mcp)** - MCP Server for Blockchain Data from **[Tatum](http://tatum.io/mcp)** that instantly unlocks blockchain access for your AI agents. This official Tatum MCP server connects to any LLM in seconds.\n- **[Bluesky](https://github.com/semioz/bluesky-mcp)** (by semioz) - An MCP server for Bluesky, a decentralized social network. It enables automated interactions with the AT Protocol, supporting features like posting, liking, reposting, timeline management, and profile operations.\n- **[Bluetooth MCP Server](https://github.com/Hypijump31/bluetooth-mcp-server)** - Control Bluetooth devices and manage connections through natural language commands, including device discovery, pairing, and audio controls.\n- **[BNBChain MCP](https://github.com/bnb-chain/bnbchain-mcp)** - An MCP server for interacting with BSC, opBNB, and the Greenfield blockchain.\n- **[Braintree](https://github.com/QuentinCody/braintree-mcp-server)** - Unofficial PayPal Braintree payment gateway MCP Server for AI agents to process payments, manage customers, and handle transactions securely.\n- **[Brazilian Law](https://github.com/pdmtt/brlaw_mcp_server/)** (by pdmtt) - Agent-driven research on Brazilian law using official sources.\n- **[BreakoutRoom](https://github.com/agree-able/room-mcp)** - Agents accomplishing goals together in p2p rooms\n- **[Browser MCP](https://github.com/bytedance/UI-TARS-desktop/tree/main/packages/agent-infra/mcp-servers/browser)** (by UI-TARS) - A fast, lightweight MCP server that empowers LLMs with browser automation via Puppeteer’s structured accessibility data, featuring optional vision mode for complex visual understanding and flexible, cross-platform configuration.\n- **[browser-use](https://github.com/co-browser/browser-use-mcp-server)** (by co-browser) - browser-use MCP server with dockerized playwright + chromium + vnc. supports stdio & resumable http.\n- **[BrowserLoop](https://github.com/mattiasw/browserloop)** - An MCP server for taking screenshots of web pages using Playwright. Supports high-quality capture with configurable formats, viewport sizes, cookie-based authentication, and both full page and element-specific screenshots.\n- **[Bsc-mcp](https://github.com/TermiX-official/bsc-mcp)** The first MCP server that serves as the bridge between AI and BNB Chain, enabling AI agents to execute complex on-chain operations through seamless integration with the BNB Chain, including transfer, swap, launch, security check on any token and even more.\n- **[BugBug MCP Server](https://github.com/simplypixi/bugbug-mcp-server)** - Unofficial MCP server for BugBug API.\n- **[BVG MCP Server - (Unofficial) ](https://github.com/svkaizoku/mcp-bvg)** - Unofficial MCP server for Berliner Verkehrsbetriebe Api.\n- **[Bybit](https://github.com/ethancod1ng/bybit-mcp-server)** - A Model Context Protocol (MCP) server for integrating AI assistants with Bybit cryptocurrency exchange APIs, enabling automated trading, market data access, and account management.\n- **[CAD-MCP](https://github.com/daobataotie/CAD-MCP#)** (by daobataotie) - Drawing CAD(Line,Circle,Text,Annotation...) through MCP server, supporting mainstream CAD software.\n- **[Calculator](https://github.com/githejie/mcp-server-calculator)** - This server enables LLMs to use calculator for precise numerical calculations.\n- **[CalDAV MCP](https://github.com/dominik1001/caldav-mcp)** - A CalDAV MCP server to expose calendar operations as tools for AI assistants.\n- **[Calendly-mcp-server](https://github.com/meAmitPatil/calendly-mcp-server)** - Open source calendly mcp server.\n- **[Catalysis Hub](https://github.com/QuentinCody/catalysishub-mcp-server)** - Unofficial MCP server for searching and retrieving scientific data from the Catalysis Hub database, providing access to computational catalysis research and surface reaction data.\n- **[CCTV VMS MCP](https://github.com/jyjune/mcp_vms)** - A Model Context Protocol (MCP) server designed to connect to a CCTV recording program (VMS) to retrieve recorded and live video streams. It also provides tools to control the VMS software, such as showing live or playback dialogs for specific channels at specified times.\n- **[CFBD API](https://github.com/lenwood/cfbd-mcp-server)** - An MCP server for the [College Football Data API](https://collegefootballdata.com/).\n- **[ChatMCP](https://github.com/AI-QL/chat-mcp)** – An Open Source Cross-platform GUI Desktop application compatible with Linux, macOS, and Windows, enabling seamless interaction with MCP servers across dynamically selectable LLMs, by **[AIQL](https://github.com/AI-QL)**\n- **[ChatSum](https://github.com/mcpso/mcp-server-chatsum)** - Query and Summarize chat messages with LLM. by [mcpso](https://mcp.so)\n- **[Chess.com](https://github.com/pab1it0/chess-mcp)** - Access Chess.com player data, game records, and other public information through standardized MCP interfaces, allowing AI assistants to search and analyze chess information.\n- **[ChessPal Chess Engine (stockfish)](https://github.com/wilson-urdaneta/chesspal-mcp-engine)** - A Stockfish-powered chess engine exposed as an MCP server. Calculates best moves and supports both HTTP/SSE and stdio transports.\n- **[Chroma](https://github.com/privetin/chroma)** - Vector database server for semantic document search and metadata filtering, built on Chroma\n- **[Chrome history](https://github.com/vincent-pli/chrome-history-mcp)** - Talk with AI about your browser history, get fun ^_^\n- **[CIViC](https://github.com/QuentinCody/civic-mcp-server)** - MCP server for the Clinical Interpretation of Variants in Cancer (CIViC) database, providing access to clinical variant interpretations and genomic evidence for cancer research.\n- **[Claude Thread Continuity](https://github.com/peless/claude-thread-continuity)** - Persistent memory system enabling Claude Desktop conversations to resume with full context across sessions. Maintains conversation history, project states, and user preferences for seamless multi-session workflows.\n- **[ClaudePost](https://github.com/ZilongXue/claude-post)** - ClaudePost enables seamless email management for Gmail, offering secure features like email search, reading, and sending.\n- **[CLDGeminiPDF Analyzer](https://github.com/tfll37/CLDGeminiPDF-Analyzer)** - MCP server tool enabling sharing large PDF files to Google LLMs via API for further/additional analysis and response retrieval to Claude Desktop.\n- **[ClearML MCP](https://github.com/prassanna-ravishankar/clearml-mcp)** - Get comprehensive ML experiment context and analysis directly from [ClearML](https://clear.ml) in your AI conversations.\n- **[ClickUp](https://github.com/TaazKareem/clickup-mcp-server)** - MCP server for ClickUp task management, supporting task creation, updates, bulk operations, and markdown descriptions.\n- **[Cloudinary](https://github.com/felores/cloudinary-mcp-server)** - Cloudinary Model Context Protocol Server to upload media to Cloudinary and get back the media link and details.\n- **[CockroachDB](https://github.com/amineelkouhen/mcp-cockroachdb)** - MCP server enabling AI agents and LLMs to manage, monitor, and query **[CockroachDB](https://www.cockroachlabs.com/)** using natural language.\n- **[CockroachDB MCP Server](https://github.com/viragtripathi/cockroachdb-mcp-server)** – Full - featured MCP implementation built with FastAPI and CockroachDB. Supports schema bootstrapping, JSONB storage, LLM-ready CLI, and optional `/debug` endpoints.\n- **[code-assistant](https://github.com/stippi/code-assistant)** - A coding assistant MCP server that allows to explore a code-base and make changes to code. Should be used with trusted repos only (insufficient protection against prompt injections).\n- **[code-context-provider-mcp](https://github.com/AB498/code-context-provider-mcp)** - MCP server that provides code context and analysis for AI assistants. Extracts directory structure and code symbols using WebAssembly Tree-sitter parsers without Native Dependencies.\n- **[code-executor](https://github.com/bazinga012/mcp_code_executor)** - An MCP server that allows LLMs to execute Python code within a specified Conda environment.\n- **[code-sandbox-mcp](https://github.com/Automata-Labs-team/code-sandbox-mcp)** - An MCP server to create secure code sandbox environment for executing code within Docker containers.\n- **[cognee-mcp](https://github.com/topoteretes/cognee/tree/main/cognee-mcp)** - GraphRAG memory server with customizable ingestion, data processing and search\n- **[coin_api_mcp](https://github.com/longmans/coin_api_mcp)** - Provides access to [coinmarketcap](https://coinmarketcap.com/) cryptocurrency data.\n- **[CoinMarketCap](https://github.com/shinzo-labs/coinmarketcap-mcp)** - Implements the complete [CoinMarketCap](https://coinmarketcap.com/) API for accessing cryptocurrency market data, exchange information, and other blockchain-related metrics.\n- **[commands](https://github.com/g0t4/mcp-server-commands)** - Run commands and scripts. Just like in a terminal.\n- **[Companies House MCP](https://github.com/stefanoamorelli/companies-house-mcp)** (by Stefano Amorelli) - MCP server to connect with the UK Companies House API.\n- **[computer-control-mcp](https://github.com/AB498/computer-control-mcp)** - MCP server that provides computer control capabilities, like mouse, keyboard, OCR, etc. using PyAutoGUI, RapidOCR, ONNXRuntime Without External Dependencies.\n- **[Computer-Use - Remote MacOS Use](https://github.com/baryhuang/mcp-remote-macos-use)** - Open-source out-of-the-box alternative to OpenAI Operator, providing a full desktop experience and optimized for using remote macOS machines as autonomous AI agents.\n- **[Congress.gov API](https://github.com/AshwinSundar/congress_gov_mcp)** - An MCP server to interact with real-time data from the Congress.gov API, which is the official API for the United States Congress.\n- **[consul-mcp](https://github.com/kocierik/consul-mcp-server)** - A consul MCP server for service management, health check and Key-Value Store\n- **[consult7](https://github.com/szeider/consult7)** - Analyze large codebases and document collections using high-context models via OpenRouter, OpenAI, or Google AI -- very useful, e.g., with Claude Code\n- **[Contentful-mcp](https://github.com/ivo-toby/contentful-mcp)** - Read, update, delete, publish content in your [Contentful](https://contentful.com) space(s) from this MCP Server.\n- **[Context Crystallizer](https://github.com/hubertciebiada/context-crystallizer)** - AI Context Engineering tool that transforms large repositories into crystallized, AI-consumable knowledge through systematic analysis and optimization.\n- **[MCP Context Provider](https://github.com/doobidoo/MCP-Context-Provider)** - Static server that provides AI models with persistent tool-specific context and rules, preventing context loss between chat sessions and enabling consistent behavior across interactions.\n- **[context-portal](https://github.com/GreatScottyMac/context-portal)** - Context Portal (ConPort) is a memory bank database system that effectively builds a project-specific knowledge graph, capturing entities like decisions, progress, and architecture, along with their relationships. This serves as a powerful backend for Retrieval Augmented Generation (RAG), enabling AI assistants to access precise, up-to-date project information.\n- **[cplusplus-mcp](https://github.com/kandrwmrtn/cplusplus_mcp)** - Semantic C++ code analysis using libclang. Enables Claude to understand C++ codebases through AST parsing rather than text search - find classes, navigate inheritance, trace function calls, and explore code relationships.\n- **[CreateveAI Nexus](https://github.com/spgoodman/createveai-nexus-server)** - Open-Source Bridge Between AI Agents and Enterprise Systems, with simple custom API plug-in capabilities (including close compatibility with ComfyUI nodes), support for Copilot Studio's MCP agent integations, and support for Azure deployment in secure environments with secrets stored in Azure Key Vault, as well as straightforward on-premises deployment.\n- **[CRASH](https://github.com/nikkoxgonzales/crash-mcp)** - MCP server for structured, iterative reasoning and thinking with flexible validation, confidence tracking, revision mechanisms, and branching support.\n- **[Creatify](https://github.com/TSavo/creatify-mcp)** - MCP Server that exposes Creatify AI API capabilities for AI video generation, including avatar videos, URL-to-video conversion, text-to-speech, and AI-powered editing tools.\n- **[Cronlytic](https://github.com/Cronlytic/cronlytic-mcp-server)** - Create CRUD operations for serverless cron jobs through [Cronlytic](https://cronlytic.com) MCP Server\n- **[crypto-feargreed-mcp](https://github.com/kukapay/crypto-feargreed-mcp)**  -  Providing real-time and historical Crypto Fear & Greed Index data.\n- **[crypto-indicators-mcp](https://github.com/kukapay/crypto-indicators-mcp)**  -  An MCP server providing a range of cryptocurrency technical analysis indicators and strategies.\n- **[crypto-sentiment-mcp](https://github.com/kukapay/crypto-sentiment-mcp)**  -  An MCP server that delivers cryptocurrency sentiment analysis to AI agents.\n- **[cryptopanic-mcp-server](https://github.com/kukapay/cryptopanic-mcp-server)** - Providing latest cryptocurrency news to AI agents, powered by CryptoPanic.\n- **[CSV Editor](https://github.com/santoshray02/csv-editor)** - Comprehensive CSV processing with 40+ operations for data manipulation, analysis, and validation. Features auto-save, undo/redo, and handles GB+ files. Built with FastMCP & Pandas.\n- **[Cursor MCP Installer](https://github.com/matthewdcage/cursor-mcp-installer)** - A tool to easily install and configure other MCP servers within Cursor IDE, with support for npm packages, local directories, and Git repositories.\n- **[CVE Intelligence Server](https://github.com/gnlds/mcp-cve-intelligence-server-lite)** – Provides vulnerability intelligence via multi - source CVE data, essential exploit discovery, and EPSS risk scoring through the MCP. Useful for security research, automation, and agent workflows.\n- **[D365FO](https://github.com/mafzaal/d365fo-client)** - A comprehensive MCP server for Microsoft Dynamics 365 Finance & Operations (D365 F&O) that provides easy access to OData endpoints, metadata operations, label management, and AI assistant integration.\n- **[Dagster](https://github.com/dagster-io/dagster/tree/master/python_modules/libraries/dagster-dg-cli)** - An MCP server to easily build data pipelines using [Dagster](https://dagster.io/).\n- **[Dappier](https://github.com/DappierAI/dappier-mcp)** - Connect LLMs to real-time, rights-cleared, proprietary data from trusted sources. Access specialized models for Real-Time Web Search, News, Sports, Financial Data, Crypto, and premium publisher content. Explore data models at [marketplace.dappier.com](https://marketplace.dappier.com/marketplace).\n- **[Data Exploration](https://github.com/reading-plus-ai/mcp-server-data-exploration)** - MCP server for autonomous data exploration on .csv-based datasets, providing intelligent insights with minimal effort. NOTE: Will execute arbitrary Python code on your machine, please use with caution!\n- **[Databricks](https://github.com/JordiNeil/mcp-databricks-server)** - Allows LLMs to run SQL queries, list and get details of jobs executions in a Databricks account.\n- **[Databricks Genie](https://github.com/yashshingvi/databricks-genie-MCP)** - A server that connects to the Databricks Genie, allowing LLMs to ask natural language questions, run SQL queries, and interact with Databricks conversational agents.\n- **[Databricks Smart SQL](https://github.com/RafaelCartenet/mcp-databricks-server)** - Leveraging Databricks Unity Catalog metadata, perform smart efficient SQL queries to solve Ad-hoc queries and explore data.\n- **[DataCite](https://github.com/QuentinCody/datacite-mcp-server)** - Unofficial MCP server for DataCite, providing access to research data and publication metadata through DataCite's REST API and GraphQL interface for scholarly research discovery.\n- **[Datadog](https://github.com/GeLi2001/datadog-mcp-server)** - Datadog MCP Server for application tracing, monitoring, dashboard, incidents queries built on official datadog api.\n- **[Dataset Viewer](https://github.com/privetin/dataset-viewer)** - Browse and analyze Hugging Face datasets with features like search, filtering, statistics, and data export\n- **[DataWorks](https://github.com/aliyun/alibabacloud-dataworks-mcp-server)** - A Model Context Protocol (MCP) server that provides tools for AI, allowing it to interact with the [DataWorks](https://www.alibabacloud.com/help/en/dataworks/) Open API through a standardized interface. This implementation is based on the Alibaba Cloud Open API and enables AI agents to perform cloud resources operations seamlessly.\n- **[Data4library](https://github.com/isnow890/data4library-mcp)** (by isnow890) - MCP server for Korea's Library Information Naru API, providing comprehensive access to public library data, book searches, loan status, reading statistics, and GPS-based nearby library discovery across South Korea.\n\n- **[DaVinci Resolve](https://github.com/samuelgursky/davinci-resolve-mcp)** - MCP server integration for DaVinci Resolve providing powerful tools for video editing, color grading, media management, and project control.\n- **[DBHub](https://github.com/bytebase/dbhub/)** - Universal database MCP server connecting to MySQL, MariaDB, PostgreSQL, and SQL Server.\n- **[Deebo](https://github.com/snagasuri/deebo-prototype)** – Agentic debugging MCP server that helps AI coding agents delegate and fix hard bugs through isolated multi-agent hypothesis testing.\n- **[Deep Research](https://github.com/reading-plus-ai/mcp-server-deep-research)** - Lightweight MCP server offering Grok/OpenAI/Gemini/Perplexity-style automated deep research exploration and structured reporting.\n- **[DeepSeek MCP Server](https://github.com/DMontgomery40/deepseek-mcp-server)** - Model Context Protocol server integrating DeepSeek's advanced language models, in addition to [other useful API endpoints](https://github.com/DMontgomery40/deepseek-mcp-server?tab=readme-ov-file#features)\n- **[deepseek-thinker-mcp](https://github.com/ruixingshi/deepseek-thinker-mcp)** - A MCP (Model Context Protocol) provider Deepseek reasoning content to MCP-enabled AI Clients, like Claude Desktop. Supports access to Deepseek's thought processes from the Deepseek API service or from a local Ollama server.\n- **[Deepseek_R1](https://github.com/66julienmartin/MCP-server-Deepseek_R1)** - A Model Context Protocol (MCP) server implementation connecting Claude Desktop with DeepSeek's language models (R1/V3)\n- **[Depyler](https://github.com/paiml/depyler/blob/main/docs/mcp-integration.md)** - Energy-efficient Python-to-Rust transpiler with progressive verification, enabling AI assistants to convert Python code to safe, performant Rust while reducing energy consumption by 75-85%.\n- **[deploy-mcp](https://github.com/alexpota/deploy-mcp)** - Universal deployment tracker for AI assistants with live status badges and deployment monitoring.\n- **[Descope](https://github.com/descope-sample-apps/descope-mcp-server)** - An MCP server to integrate with [Descope](https://descope.com) to search audit logs, manage users, and more.\n- **[DesktopCommander](https://github.com/wonderwhy-er/DesktopCommanderMCP)** - Let AI edit and manage files on your computer, run terminal commands, and connect to remote servers via SSH - all powered by one of the most popular local MCP servers.\n- **[Devcontainer](https://github.com/AI-QL/mcp-devcontainers)** - An MCP server for devcontainer to generate and configure development containers directly from devcontainer configuration files.\n- **[DevDb](https://github.com/damms005/devdb-vscode?tab=readme-ov-file#mcp-configuration)** - An MCP server that runs right inside the IDE, for connecting to MySQL, Postgres, SQLite, and MSSQL databases.\n- **[DevOps AI Toolkit](https://github.com/vfarcic/dot-ai)** - AI-powered development productivity platform that enhances software development workflows through intelligent automation and AI-driven assistance.\n- **[DevOps-MCP](https://github.com/wangkanai/devops-mcp)** - Dynamic Azure DevOps MCP server with directory-based authentication switching, supporting work items, repositories, builds, pipelines, and multi-project management with local configuration files.\n- **[DGIdb](https://github.com/QuentinCody/dgidb-mcp-server)** - MCP server for the Drug Gene Interaction Database (DGIdb), providing access to drug-gene interaction data, druggable genome information, and pharmacogenomics research.\n- **[Dicom](https://github.com/ChristianHinge/dicom-mcp)** - An MCP server to query and retrieve medical images and for parsing and reading dicom-encapsulated documents (pdf etc.).\n- **[Dify](https://github.com/YanxingLiu/dify-mcp-server)** - A simple implementation of an MCP server for dify workflows.\n- **[Discogs](https://github.com/cswkim/discogs-mcp-server)** - An MCP server that connects to the Discogs API for interacting with your music collection.\n- **[Discord](https://github.com/v-3/discordmcp)** - An MCP server to connect to Discord guilds through a bot and read and write messages in channels\n- **[Discord](https://github.com/SaseQ/discord-mcp)** - An MCP server, which connects to Discord through a bot, and provides comprehensive integration with Discord.\n- **[Discord](https://github.com/Klavis-AI/klavis/tree/main/mcp_servers/discord)** - For Discord API integration by Klavis AI\n- **[Discourse](https://github.com/AshDevFr/discourse-mcp-server)** - An MCP server to search Discourse posts on a Discourse forum.\n- **[DocBase](https://help.docbase.io/posts/3925317)** - Official MCP server for DocBase API integration, enabling post management, user collaboration, group administration, and more.\n- **[Docker](https://github.com/ckreiling/mcp-server-docker)** - Integrate with Docker to manage containers, images, volumes, and networks.\n- **[Docker](https://github.com/0xshariq/docker-mcp-server)** - Docker MCP Server provides advanced, unified Docker management via CLI and MCP workflows, supporting containers, images, volumes, networks, and orchestration.\n- **[Docs](https://github.com/da1z/docsmcp)** - Enable documentation access for the AI agent, supporting llms.txt and other remote or local files.\n- **[documcp](https://github.com/tosin2013/documcp)** - An MCP server for intelligent document processing and management, supporting multiple formats and document operations.\n- **[Docy](https://github.com/oborchers/mcp-server-docy)** - Docy gives your AI direct access to the technical documentation it needs, right when it needs it. No more outdated information, broken links, or rate limits - just accurate, real-time documentation access for more precise coding assistance.\n- **[Dodo Payments](https://github.com/dodopayments/dodopayments-node/tree/main/packages/mcp-server)** - Enables AI agents to securely perform payment operations via a lightweight, serverless-compatible interface to the [Dodo Payments](https://dodopayments.com) API.\n- **[Domain Tools](https://github.com/deshabhishek007/domain-tools-mcp-server)** - A Model Context Protocol (MCP) server for comprehensive domain analysis: WHOIS, DNS records, and DNS health checks.\n- **[DPLP](https://github.com/szeider/mcp-dblp)**  - Searches the [DBLP](https://dblp.org) computer science bibliography database.\n- **[Druid MCP Server](https://github.com/iunera/druid-mcp-server)** - STDIO/SEE MCP Server for Apache Druid by [iunera](https://www.iunera.com) that provides extensive tools, resources, and prompts for managing and analyzing Druid clusters.\n- **[Drupal](https://github.com/Omedia/mcp-server-drupal)** - Server for interacting with [Drupal](https://www.drupal.org/project/mcp) using STDIO transport layer.\n- **[dune-analytics-mcp](https://github.com/kukapay/dune-analytics-mcp)** -  A mcp server that bridges Dune Analytics data to AI agents.\n- **[DynamoDB-Toolbox](https://www.dynamodbtoolbox.com/docs/databases/actions/mcp-toolkit)** - Leverages your Schemas and Access Patterns to interact with your [DynamoDB](https://aws.amazon.com/dynamodb) Database using natural language.\n- **[eBook-mcp](https://github.com/onebirdrocks/ebook-mcp)** - A lightweight MCP server that allows LLMs to read and interact with your personal PDF and EPUB ebooks. Ideal for building AI reading assistants or chat-based ebook interfaces.\n- **[ECharts MCP Server](https://github.com/hustcc/mcp-echarts)** - Generate visual charts using ECharts with AI MCP dynamically, used for chart generation and data analysis.\n- **[EDA MCP Server](https://github.com/NellyW8/mcp-EDA)** - A comprehensive Model Context Protocol server for Electronic Design Automation tools, enabling AI assistants to synthesize Verilog with Yosys, simulate designs with Icarus Verilog, run complete ASIC flows with OpenLane, and view results with GTKWave and KLayout.\n- **[EdgeOne Pages MCP](https://github.com/TencentEdgeOne/edgeone-pages-mcp)** - An MCP service for deploying HTML content to EdgeOne Pages and obtaining a publicly accessible URL.\n- **[Edwin](https://github.com/edwin-finance/edwin/tree/main/examples/mcp-server)** - MCP server for edwin SDK - enabling AI agents to interact with DeFi protocols across EVM, Solana and other blockchains.\n- **[eechat](https://github.com/Lucassssss/eechat)** - An open-source, cross-platform desktop application that seamlessly connects with MCP servers, across Linux, macOS, and Windows.\n- **[Elasticsearch](https://github.com/cr7258/elasticsearch-mcp-server)** - MCP server implementation that provides Elasticsearch interaction.\n- **[ElevenLabs](https://github.com/mamertofabian/elevenlabs-mcp-server)** - A server that integrates with ElevenLabs text-to-speech API capable of generating full voiceovers with multiple voices.\n- **[Email](https://github.com/Shy2593666979/mcp-server-email)** - This server enables users to send emails through various email providers, including Gmail, Outlook, Yahoo, Sina, Sohu, 126, 163, and QQ Mail. It also supports attaching files from specified directories, making it easy to upload attachments along with the email content.\n- **[Email SMTP](https://github.com/egyptianego17/email-mcp-server)** - A simple MCP server that lets your AI agent send emails and attach files through SMTP.\n- **[Enhance Prompt](https://github.com/FelixFoster/mcp-enhance-prompt)** - An MCP service for enhance you prompt.\n- **[Entrez](https://github.com/QuentinCody/entrez-mcp-server)** - Unofficial MCP server for NCBI Entrez databases, providing access to PubMed articles, gene information, protein data, and other biomedical research resources through NCBI's E-utilities API.\n- **[Ergo Blockchain MCP](https://github.com/marctheshark3/ergo-mcp)** -An MCP server to integrate Ergo Blockchain Node and Explorer APIs for checking address balances, analyzing transactions, viewing transaction history, performing forensic analysis of addresses, searching for tokens, and monitoring network status.\n- **[ESP MCP Server](https://github.com/horw/esp-mcp)** - An MCP server that integrates ESP IDF commands like building and flashing code for ESP Microcontrollers using an LLM.\n- **[Eunomia](https://github.com/whataboutyou-ai/eunomia-MCP-server)** - Extension of the Eunomia framework that connects Eunomia instruments with MCP servers\n- **[Everything Search](https://github.com/mamertofabian/mcp-everything-search)** - Fast file searching capabilities across Windows (using [Everything SDK](https://www.voidtools.com/support/everything/sdk/)), macOS (using mdfind command), and Linux (using locate/plocate command).\n- **[EVM MCP Server](https://github.com/mcpdotdirect/evm-mcp-server)** - Comprehensive blockchain services for 30+ EVM networks, supporting native tokens, ERC20, NFTs, smart contracts, transactions, and ENS resolution.\n- **[Excel](https://github.com/haris-musa/excel-mcp-server)** - Excel manipulation including data reading/writing, worksheet management, formatting, charts, and pivot table.\n- **[Excel to JSON MCP by WTSolutions](https://github.com/he-yang/excel-to-json-mcp)** - MCP Server providing a standardized interface for converting (1) Excel or CSV data into JSON format ;(2) Excel(.xlsx) file into Structured JSON.\n- **[Extended Memory](https://github.com/ssmirnovpro/extended-memory-mcp)** - Persistent memory across Claude conversations with multi-project support, automatic importance scoring, and tag-based organization. Production-ready with 400+ tests.\n- **[F1](https://github.com/AbhiJ2706/f1-mcp/tree/main)** - Access to Formula 1 data including race results, driver information, lap times, telemetry, and circuit details.\n- **[Fabric MCP](https://github.com/aci-labs/ms-fabric-mcp)** - Microsoft Fabric MCP server to accelerate working in your Fabric Tenant with the help of your favorite LLM models.\n- **[Fabric Real-Time Intelligence MCP](https://github.com/Microsoft/fabric-rti-mcp)** - Official Microsoft Fabric RTI server to accelerate working with Eventhouse, Azure Data Explorer(Kusto), Eventstreams and other RTI items using your favorite LLM models.\n- **[fabric-mcp-server](https://github.com/adapoet/fabric-mcp-server)** - The fabric-mcp-server is an MCP server that integrates [Fabric](https://github.com/danielmiessler/fabric) patterns with [Cline](https://cline.bot/), exposing them as tools for AI-driven task execution and enhancing Cline's capabilities.\n- **[Fal MCP Server](https://github.com/raveenb/fal-mcp-server)** - Generate AI images, videos, and music using Fal.ai models (FLUX, Stable Diffusion, MusicGen) directly in Claude\n- **[Facebook Ads](https://github.com/gomarble-ai/facebook-ads-mcp-server)** - MCP server acting as an interface to the Facebook Ads, enabling programmatic access to Facebook Ads data and management features.\n- **[Facebook Ads 10xeR](https://github.com/fortytwode/10xer)** - Advanced Facebook Ads MCP server with enhanced creative insights, multi-dimensional breakdowns, and comprehensive ad performance analytics.\n- **[Facebook Ads Library](https://github.com/trypeggy/facebook-ads-library-mcp)** - Get any answer from the Facebook Ads Library, conduct deep research including messaging, creative testing and comparisons in seconds.\n- **[Fantasy PL](https://github.com/rishijatia/fantasy-pl-mcp)** - Give your coding agent direct access to up-to date Fantasy Premier League data\n- **[Fastmail MCP](https://github.com/MadLlama25/fastmail-mcp)** - Access Fastmail via JMAP: list/search emails, send and move mail, handle attachments/threads, plus contacts and calendar tools.\n- **[fastn.ai – Unified API MCP Server](https://github.com/fastnai/mcp-fastn)** - A remote, dynamic MCP server with a unified API that connects to 1,000+ tools, actions, and workflows, featuring built-in authentication and monitoring.\n- **[FDIC BankFind MCP Server - (Unofficial)](https://github.com/clafollett/fdic-bank-find-mcp-server)** - The is a MCPserver that brings the power of FDIC BankFind APIs straight to your AI tools and workflows. Structured U.S. banking data, delivered with maximum vibes. 😎📊\n- **[FPE Demo MCP](https://github.com/Horizon-Digital-Engineering/fpe-demo-mcp)** - FF3 Format Preserving Encryption with authentication patterns for secure data protection in LLM workflows.\n- **[Federal Reserve Economic Data (FRED)](https://github.com/stefanoamorelli/fred-mcp-server)** (by Stefano Amorelli) - Community developed MCP server to interact with the Federal Reserve Economic Data.\n- **[Fetch](https://github.com/zcaceres/fetch-mcp)** - A server that flexibly fetches HTML, JSON, Markdown, or plaintext.\n- **[Feyod](https://github.com/jeroenvdmeer/feyod-mcp)** - A server that answers questions about football matches, and specialised in the football club Feyenoord.\n- **[Fast Filesystem](https://github.com/efforthye/fast-filesystem-mcp)** - Advanced filesystem operations with large file handling capabilities and Claude-optimized features. Provides fast file reading/writing, sequential reading for large files, directory operations, file search, and streaming writes with backup & recovery.\n- **[FHIR](https://github.com/wso2/fhir-mcp-server)** - A Model Context Protocol server that provides seamless, standardized access to Fast Healthcare Interoperability Resources (FHIR) data from any compatible FHIR server. Designed for easy integration with AI tools, developer workflows, and healthcare applications, it enables natural language and programmatic search, retrieval, and analysis of clinical data.\n- **[Fibaro HC3](https://github.com/coding-sailor/mcp-server-hc3)** - MCP server for Fibaro Home Center 3 smart home systems.\n- **[Figma](https://github.com/GLips/Figma-Context-MCP)** - Give your coding agent direct access to Figma file data, helping it one-shot design implementation.\n- **[Figma](https://github.com/paulvandermeijs/figma-mcp)** - A blazingly fast MCP server to read and export your Figma design files.\n- **[Figma to Flutter](https://github.com/mhmzdev/figma-flutter-mcp)** - Write down clean and better Flutter code from Figma design tokens and enrich nodes data in Flutter terminology.\n- **[Files](https://github.com/flesler/mcp-files)** - Enables agents to quickly find and edit code in a codebase with surgical precision. Find symbols, edit them everywhere.\n- **[FileSystem Server](https://github.com/Oncorporation/filesystem_server)** - Local MCP server for Visual Studio 2022 that provides code-workspace functionality by giving AI agents selective access to project folders and files\n- **[finmap.org](https://github.com/finmap-org/mcp-server)** MCP server provides comprehensive historical data from the US, UK, Russian and Turkish stock exchanges. Access sectors, tickers, company profiles, market cap, volume, value, and trade counts, as well as treemap and histogram visualizations.\n- **[Firebase](https://github.com/gannonh/firebase-mcp)** - Server to interact with Firebase services including Firebase Authentication, Firestore, and Firebase Storage.\n- **[Fish Audio](https://github.com/da-okazaki/mcp-fish-audio-server)** - Text-to-Speech integration with Fish Audio's API, supporting multiple voices, streaming, and real-time playback\n- **[FitBit MCP Server](https://github.com/NitayRabi/fitbit-mcp)** - An MCP server that connects to FitBit API using a token obtained from OAuth flow.\n- **[FlightRadar24](https://github.com/sunsetcoder/flightradar24-mcp-server)** - A Claude Desktop MCP server that helps you track flights in real-time using Flightradar24 data.\n- **[Fluent-MCP](https://github.com/modesty/fluent-mcp)** - MCP server for Fluent (ServiceNow SDK) providing access to ServiceNow SDK CLI, API specifications, code snippets, and more.\n- **[Flyworks Avatar](https://github.com/Flyworks-AI/flyworks-mcp)** - Fast and free zeroshot lipsync MCP server.\n- **[fmp-mcp-server](https://github.com/vipbat/fmp-mcp-server)** - Enable your agent for M&A analysis and investment banking workflows. Access company profiles, financial statements, ratios, and perform sector analysis with the [Financial Modeling Prep APIs]\n- **[FoundationModels](https://github.com/phimage/mcp-foundation-models)** - An MCP server that integrates Apple's [FoundationModels](https://developer.apple.com/documentation/foundationmodels) for text generation.\n- **[Foursquare](https://github.com/foursquare/foursquare-places-mcp)** - Enable your agent to recommend places around the world with the [Foursquare Places API](https://location.foursquare.com/products/places-api/)\n- **[FrankfurterMCP](https://github.com/anirbanbasu/frankfurtermcp)** - MCP server acting as an interface to the [Frankfurter API](https://frankfurter.dev/) for currency exchange data.\n- **[freqtrade-mcp](https://github.com/kukapay/freqtrade-mcp)** - An MCP server that integrates with the Freqtrade cryptocurrency trading bot.\n- **[Geolocation](https://github.com/jackyang25/geolocation-mcp-server)** - WalkScore API integration for walkability, transit, and bike scores.\n- **[GDB](https://github.com/pansila/mcp_server_gdb)** - A GDB/MI protocol server based on the MCP protocol, providing remote application debugging capabilities with AI assistants.\n- **[ggRMCP](https://github.com/aalobaidi/ggRMCP)** - A Go gateway that converts gRPC services into MCP-compatible tools, allowing AI models like Claude to directly call your gRPC services.\n- **[Gemini Bridge](https://github.com/eLyiN/gemini-bridge)** - Lightweight MCP server that enables Claude to interact with Google's Gemini AI through the official CLI, offering zero API costs and stateless architecture.\n- **[Ghost](https://github.com/MFYDev/ghost-mcp)** - A Model Context Protocol (MCP) server for interacting with Ghost CMS through LLM interfaces like Claude.\n- **[Git](https://github.com/geropl/git-mcp-go)** - Allows LLM to interact with a local git repository, incl. optional push support.\n- **[Git Mob](https://github.com/Mubashwer/git-mob-mcp-server)** - MCP server that interfaces with the [git-mob](https://github.com/Mubashwer/git-mob) CLI app for managing co-authors in git commits during pair/mob programming.\n- **[Github](https://github.com/0xshariq/github-mcp-server)** - A Model Context Protocol (MCP) server that provides 29 Git operations + 11 workflow combinations for AI assistants and developers. This server exposes comprehensive Git repository management through a standardized interface, enabling AI models and developers to safely manage complex version control workflows.\n- **[GitHub Actions](https://github.com/ko1ynnky/github-actions-mcp-server)** - A Model Context Protocol (MCP) server for interacting with GitHub Actions.\n- **[GitHub Enterprise MCP](https://github.com/ddukbg/github-enterprise-mcp)** - A Model Context Protocol (MCP) server for interacting with GitHub Enterprise.\n- **[GitHub GraphQL](https://github.com/QuentinCody/github-graphql-mcp-server)** - Unofficial GitHub MCP server that provides access to GitHub's GraphQL API, enabling more powerful and flexible queries for repository data, issues, pull requests, and other GitHub resources.\n- **[GitHub Projects](https://github.com/redducklabs/github-projects-mcp)** — Manage GitHub Projects with full GraphQL API access including items, fields, and milestones.\n- **[GitHub Repos Manager MCP Server](https://github.com/kurdin/github-repos-manager-mcp)** - Token-based GitHub automation management. No Docker, Flexible configuration, 80+ tools with direct API integration.\n- **[GitMCP](https://github.com/idosal/git-mcp)** - gitmcp.io is a generic remote MCP server to connect to ANY GitHub repository or project documentation effortlessly\n- **[Glean](https://github.com/longyi1207/glean-mcp-server)** - A server that uses Glean API to search and chat.\n- **[Gmail](https://github.com/GongRzhe/Gmail-MCP-Server)** - A Model Context Protocol (MCP) server for Gmail integration in Claude Desktop with auto authentication support.\n- **[Gmail](https://github.com/Ayush-k-Shukla/gmail-mcp-server)** - A Simple MCP server for Gmail with support for all basic operations with oauth2.0.\n- **[Gmail Headless](https://github.com/baryhuang/mcp-headless-gmail)** - Remote hostable MCP server that can get and send Gmail messages without local credential or file system setup.\n- **[Gmail MCP](https://github.com/gangradeamitesh/mcp-google-email)** - A Gmail service implementation using MCP (Model Context Protocol) that provides functionality for sending, receiving, and managing emails through Gmail's API.\n- **[Gnuradio](https://github.com/yoelbassin/gnuradioMCP)** - An MCP server for GNU Radio that enables LLMs to autonomously create and modify RF .grc flowcharts.\n- **[Goal Story](https://github.com/hichana/goalstory-mcp)** - a Goal Tracker and Visualization Tool for personal and professional development.\n- **[GOAT](https://github.com/goat-sdk/goat/tree/main/typescript/examples/by-framework/model-context-protocol)** - Run more than +200 onchain actions on any blockchain including Ethereum, Solana and Base.\n- **[Godot](https://github.com/Coding-Solo/godot-mcp)** - An MCP server providing comprehensive Godot engine integration for project editing, debugging, and scene management.\n- **[Golang Filesystem Server](https://github.com/mark3labs/mcp-filesystem-server)** - Secure file operations with configurable access controls built with Go!\n- **[Goodnews](https://github.com/VectorInstitute/mcp-goodnews)** - A simple MCP server that delivers curated positive and uplifting news stories.\n- **[Gopher MCP](https://github.com/cameronrye/gopher-mcp)** - Modern, cross-platform MCP server that enables AI assistants to browse and interact with both Gopher protocol and Gemini protocol resources safely and efficiently.\n- **[Google Ads](https://github.com/gomarble-ai/google-ads-mcp-server)** - MCP server acting as an interface to the Google Ads, enabling programmatic access to Facebook Ads data and management features.\n- **[Google Analytics](https://github.com/surendranb/google-analytics-mcp)** - Google Analytics MCP Server to bring data across 200+ dimensions & metrics for LLMs to analyse.\n- **[Google Calendar](https://github.com/v-3/google-calendar)** - Integration with Google Calendar to check schedules, find time, and add/delete events\n- **[Google Calendar](https://github.com/nspady/google-calendar-mcp)** - Google Calendar MCP Server for managing Google calendar events. Also supports searching for events by attributes like title and location.\n- **[Google Custom Search](https://github.com/adenot/mcp-google-search)** - Provides Google Search results via the Google Custom Search API\n- **[Google Maps](https://github.com/Mastan1301/google_maps_mcp)** - Provides location results using Google Places API.\n- **[Google Sheets](https://github.com/xing5/mcp-google-sheets)** - Access and editing data to your Google Sheets.\n- **[Google Sheets](https://github.com/rohans2/mcp-google-sheets)** - An MCP Server written in TypeScript to access and edit data in your Google Sheets.\n- **[Google Tasks](https://github.com/zcaceres/gtasks-mcp)** - Google Tasks API Model Context Protocol Server.\n- **[Google Vertex AI Search](https://github.com/ubie-oss/mcp-vertexai-search)** - Provides Google Vertex AI Search results by grounding a Gemini model with your own private data\n- **[Google Workspace](https://github.com/taylorwilsdon/google_workspace_mcp)** - Comprehensive Google Workspace MCP with full support for Calendar, Drive, Gmail, and Docs using Streamable HTTP or SSE transport.\n- **[Google-Scholar](https://github.com/JackKuo666/Google-Scholar-MCP-Server)** - Enable AI assistants to search and access Google Scholar papers through a simple MCP interface.\n- **[Google-Scholar](https://github.com/mochow13/google-scholar-mcp)** - An MCP server for Google Scholar written in TypeScript with Streamable HTTP transport, along with a `client` implementations that integrates with the server and interacts with `gemini-2.5-flash`.\n- **[gx-mcp-server](https://github.com/davidf9999/gx-mcp-server)** - Expose Great Expectations data validation and quality checks as MCP tools for AI agents.\n- **[Gralio SaaS Database](https://github.com/tymonTe/gralio-mcp)** - Find and compare SaaS products, including data from G2 reviews, Trustpilot, Crunchbase, Linkedin, pricing, features and more, using [Gralio MCP](https://gralio.ai/mcp) server\n- **[GraphQL](https://github.com/drestrepom/mcp_graphql)** - Comprehensive GraphQL API integration that automatically exposes each GraphQL query as a separate tool.\n- **[GraphQL Schema](https://github.com/hannesj/mcp-graphql-schema)** - Allow LLMs to explore large GraphQL schemas without bloating the context.\n- **[HackMD](https://github.com/yuna0x0/hackmd-mcp)** (by yuna0x0) - An MCP server for HackMD, a collaborative markdown editor. It allows users to create, read, and update documents in HackMD using the Model Context Protocol.\n- **[HAProxy](https://github.com/tuannvm/haproxy-mcp-server)** - A Model Context Protocol (MCP) server for HAProxy implemented in Go, leveraging HAProxy Runtime API.\n- **[Hashing MCP Server](https://github.com/kanad13/MCP-Server-for-Hashing)** - MCP Server with cryptographic hashing functions e.g. SHA256, MD5, etc.\n- **[HDW LinkedIn](https://github.com/horizondatawave/hdw-mcp-server)** - Access to profile data and management of user account with [HorizonDataWave.ai](https://horizondatawave.ai/).\n- **[HeatPump](https://github.com/jiweiqi/heatpump-mcp-server)** — Residential heat - pump sizing & cost-estimation tools by **HeatPumpHQ**.\n- **[Helm Chart CLI](https://github.com/jeff-nasseri/helm-chart-cli-mcp)** - Helm MCP provides a bridge between AI assistants and the Helm package manager for Kubernetes. It allows AI assistants to interact with Helm through natural language requests, executing commands like installing charts, managing repositories, and more.\n- **[Heurist Mesh Agent](https://github.com/heurist-network/heurist-mesh-mcp-server)** - Access specialized web3 AI agents for blockchain analysis, smart contract security, token metrics, and blockchain interactions through the [Heurist Mesh network](https://github.com/heurist-network/heurist-agent-framework/tree/main/mesh).\n- **[HLedger MCP](https://github.com/iiAtlas/hledger-mcp)** - Double entry plain text accounting, right in your LLM! This MCP enables comprehensive read, and (optional) write access to your local [HLedger](https://hledger.org/) accounting journals.\n- **[Holaspirit](https://github.com/syucream/holaspirit-mcp-server)** - Interact with [Holaspirit](https://www.holaspirit.com/).\n- **[Home Assistant](https://github.com/tevonsb/homeassistant-mcp)** - Interact with [Home Assistant](https://www.home-assistant.io/) including viewing and controlling lights, switches, sensors, and all other Home Assistant entities.\n- **[Home Assistant](https://github.com/voska/hass-mcp)** - Docker-ready MCP server for Home Assistant with entity management, domain summaries, automation support, and guided conversations. Includes pre-built container images for easy installation.\n- **[HubSpot](https://github.com/buryhuang/mcp-hubspot)** - HubSpot CRM integration for managing contacts and companies. Create and retrieve CRM data directly through Claude chat.\n- **[HuggingFace Spaces](https://github.com/evalstate/mcp-hfspace)** - Server for using HuggingFace Spaces, supporting Open Source Image, Audio, Text Models and more. Claude Desktop mode for easy integration.\n- **[Human-In-the-Loop](https://github.com/GongRzhe/Human-In-the-Loop-MCP-Server)** - A powerful MCP Server that enables AI assistants like Claude to interact with humans through intuitive GUI dialogs. This server bridges the gap between automated AI processes and human decision-making by providing real-time user input tools, choices, confirmations, and feedback mechanisms.\n- **[Human-use](https://github.com/RapidataAI/human-use)** - Instant human feedback through an MCP, have your AI interact with humans around the world. Powered by [Rapidata](https://www.rapidata.ai/)\n- **[Hyperledger Fabric Agent Suite](https://github.com/padmarajkore/hlf-fabric-agent)** - Modular toolkit for managing Fabric test networks and chaincode lifecycle via MCP tools.\n- **[Hyperliquid](https://github.com/mektigboy/server-hyperliquid)** - An MCP server implementation that integrates the Hyperliquid SDK for exchange data.\n- **[Hypertool](https://github.com/toolprint/hypertool-mcp)** – MCP that let's you create hot - swappable, \"persona toolsets\" from multiple MCP servers to reduce tool overload and improve tool execution.\n- **[hyprmcp](https://github.com/stefanoamorelli/hyprmcp)** (by Stefano Amorelli) - Lightweight MCP server for `hyprland`.\n- **[iFlytek SparkAgent Platform](https://github.com/iflytek/ifly-spark-agent-mcp)** - This is a simple example of using MCP Server to invoke the task chain of the  iFlytek SparkAgent Platform.\n- **[iFlytek Workflow](https://github.com/iflytek/ifly-workflow-mcp-server)** - Connect to iFlytek Workflow via the MCP server and run your own Agent.\n- **[IIIF](https://github.com/code4history/IIIF_MCP)** - Comprehensive IIIF (International Image Interoperability Framework) protocol support for searching, navigating, and manipulating digital collections from museums, libraries, and archives worldwide.\n- **[Image Generation](https://github.com/GongRzhe/Image-Generation-MCP-Server)** - This MCP server provides image generation capabilities using the Replicate Flux model.\n- **[ImageSorcery MCP](https://github.com/sunriseapps/imagesorcery-mcp)** - ComputerVision-based 🪄 sorcery of image recognition and editing tools for AI assistants.\n- **[IMAP MCP](https://github.com/dominik1001/imap-mcp)** - 📧 An IMAP Model Context Protocol (MCP) server to expose IMAP operations as tools for AI assistants.\n- **[iMCP](https://github.com/loopwork-ai/iMCP)** - A macOS app that provides an MCP server for your iMessage, Reminders, and other Apple services.\n- **[InfluxDB](https://github.com/idoru/influxdb-mcp-server)** - Run queries against InfluxDB OSS API v2.\n- **[Intelligent Image Generator](https://github.com/shinpr/mcp-image)** - Turn casual prompts into professional-quality images with AI enhancement\n- **[Inner Monologue MCP](https://github.com/abhinav-mangla/inner-monologue-mcp)** - A cognitive reasoning tool that enables LLMs to engage in private, structured self-reflection and multi-step reasoning before generating responses, improving response quality and problem-solving capabilities.\n- **[Inoyu](https://github.com/sergehuber/inoyu-mcp-unomi-server)** - Interact with an Apache Unomi CDP customer data platform to retrieve and update customer profiles\n- **[Instagram DM](https://github.com/trypeggy/instagram_dm_mcp)** - Send DMs on Instagram via your LLM\n- **[interactive-mcp](https://github.com/ttommyth/interactive-mcp)** - Enables interactive LLM workflows by adding local user prompts and chat capabilities directly into the MCP loop.\n- **[Intercom](https://github.com/raoulbia-ai/mcp-server-for-intercom)** - An MCP-compliant server for retrieving customer support tickets from Intercom. This tool enables AI assistants like Claude Desktop and Cline to access and analyze your Intercom support tickets.\n- **[iOS Simulator](https://github.com/InditexTech/mcp-server-simulator-ios-idb)** - A Model Context Protocol (MCP) server that enables LLMs to interact with iOS simulators (iPhone, iPad, etc.) through natural language commands.\n- **[ipybox](https://github.com/gradion-ai/ipybox)** - Python code execution sandbox based on IPython and Docker. Stateful code execution, file transfer between host and container, configurable network access. See [ipybox MCP server](https://gradion-ai.github.io/ipybox/mcp-server/) for details.\n- **[it-tools-mcp](https://github.com/wrenchpilot/it-tools-mcp)** - A Model Context Protocol server that recreates [CorentinTh it-tools](https://github.com/CorentinTh/it-tools) utilities for AI agents, enabling access to a wide range of developer tools (encoding, decoding, conversions, and more) via MCP.\n- **[itemit MCP](https://github.com/umin-ai/itemit-mcp)** - itemit is Asset Tracking MCP that manage the inventory, monitoring and location tracking that powers over +300 organizations.\n- **[iTerm MCP](https://github.com/ferrislucas/iterm-mcp)** - Integration with iTerm2 terminal emulator for macOS, enabling LLMs to execute and monitor terminal commands.\n- **[iTerm MCP Server](https://github.com/rishabkoul/iTerm-MCP-Server)** - A Model Context Protocol (MCP) server implementation for iTerm2 terminal integration. Able to manage multiple iTerm Sessions.\n- **[Java Decompiler](https://github.com/idachev/mcp-javadc)** - Decompile Java bytecode into readable source code from .class files, package names, or JAR archives using CFR decompiler\n- **[JavaFX](https://github.com/quarkiverse/quarkus-mcp-servers/tree/main/jfx)** - Make drawings using a JavaFX canvas\n- **[JDBC](https://github.com/quarkiverse/quarkus-mcp-servers/tree/main/jdbc)** - Connect to any JDBC-compatible database and query, insert, update, delete, and more. Supports MySQL, PostgreSQL, Oracle, SQL Server, SQLite and [more](https://github.com/quarkiverse/quarkus-mcp-servers/tree/main/jdbc#supported-jdbc-variants).\n- **[Jenkins](https://github.com/jasonkylelol/jenkins-mcp-server)** - This MCP server allow you to create Jenkins tasks.\n- **[JMeter](https://github.com/QAInsights/jmeter-mcp-server)** - Run load testing using Apache JMeter via MCP-compliant tools.\n- **[Job Searcher](https://github.com/0xDAEF0F/job-searchoor)** - A FastMCP server that provides tools for retrieving and filtering job listings based on time period, keywords, and remote work preferences.\n- **[jobswithgpt](https://github.com/jobswithgpt/mcp)** - Job search MCP using jobswithgpt which indexes 500K+ public job listings and refreshed continously.\n- **[joinly](https://github.com/joinly-ai/joinly)** - MCP server to interact with browser-based meeting platforms (Zoom, Teams, Google Meet). Enables AI agents to send bots to online meetings, gather live transcripts, speak text, and send messages in the meeting chat.\n- **[JSON](https://github.com/GongRzhe/JSON-MCP-Server)** - JSON handling and processing server with advanced query capabilities using JSONPath syntax and support for array, string, numeric, and date operations.\n- **[JSON](https://github.com/kehvinbehvin/json-mcp-filter)** - JSON schema generation and filtering server with TypeScript type creation optimised for retrieving relevant context JSON data using quicktype-core and support for shape-based data extraction, nested object filtering, and array processing operations.\n- **[JSON to Excel by WTSolutions](https://github.com/he-yang/json-to-excel-mcp)** - Converting JSON into CSV format string from (1) JSON data, (2) URLs pointing to publiclly available .json files.\n- **[JSON2Video MCP](https://github.com/omergocmen/json2video-mcp-server)** - A Model Context Protocol (MCP) server implementation for programmatically generating videos using the json2video API. This server exposes powerful video generation and status-checking tools for use with LLMs, agents, or any MCP-compatible client.\n- **[jupiter-mcp](https://github.com/kukapay/jupiter-mcp)** - An MCP server for executing token swaps on the Solana blockchain using Jupiter's new Ultra API.\n- **[Jupyter MCP Server](https://github.com/datalayer/jupyter-mcp-server)** – Real-time interaction with Jupyter Notebooks, allowing AI to edit, document and execute code for data analysis, visualization etc. Compatible with any Jupyter deployment (local, JupyterHub, ...).\n- **[Jupyter Notebook](https://github.com/jjsantos01/jupyter-notebook-mcp)** - connects Jupyter Notebook to Claude AI, allowing Claude to directly interact with and control Jupyter Notebooks. This integration enables AI-assisted code execution, data analysis, visualization, and more.\n- **[k8s-multicluster-mcp](https://github.com/razvanmacovei/k8s-multicluster-mcp)** - An MCP server for interact with multiple Kubernetes clusters simultaneously using multiple kubeconfig files.\n- **[Kafka](https://github.com/tuannvm/kafka-mcp-server)** - A Model Context Protocol (MCP) server for Apache Kafka implemented in Go, leveraging [franz-go](https://github.com/twmb/franz-go).\n- **[Kafka Schema Registry MCP](https://github.com/aywengo/kafka-schema-reg-mcp)** \\ - A comprehensive MCP server for Kafka Schema Registry with 48 tools, multi-registry support, authentication, and production safety features. Enables AI-powered schema management with enterprise-grade capabilities including schema contexts, migration tools, and comprehensive export capabilities.\n- **[kafka-mcp](https://github.com/shivamxtech/kafka-mcp)** - An MCP Server for Kafka clusters to interact with kafka environment via tools on messages, topics, offsets, partitions for consumer and producers along with seamless integration with MCP clients.\n- **[Keycloak](https://github.com/idoyudha/mcp-keycloak)** - The Keycloak MCP Server designed for agentic applications to manage and search data in Keycloak efficiently.\n- **[Keycloak MCP](https://github.com/ChristophEnglisch/keycloak-model-context-protocol)** - This MCP server enables natural language interaction with Keycloak for user and realm management including creating, deleting, and listing users and realms.\n- **[Keycloak MCP Server](https://github.com/sshaaf/keycloak-mcp-server)** - designed to work with Keycloak for identity and access management, with about 40+ tools covering, Users, Realms, Clients, Roles, Groups, IDPs, Authentication. Native builds available.\n- **[Kibana MCP](https://github.com/TocharianOU/mcp-server-kibana.git)** (by TocharianOU) - A community-maintained MCP server implementation that allows any MCP-compatible client to access and manage Kibana instances through natural language or programmatic requests.\n- **[Kibela](https://github.com/kiwamizamurai/mcp-kibela-server)** (by kiwamizamurai) - Interact with Kibela API.\n- **[KiCad MCP](https://github.com/lamaalrajih/kicad-mcp)** - MCP server for KiCad on Mac, Windows, and Linux.\n- **[kill-process-mcp](https://github.com/misiektoja/kill-process-mcp)** - List and terminate OS processes via natural language queries\n- **[Kindred Offers & Discounts MCP](https://github.com/kindred-app/mcp-server-kindred-offers)** (by kindred.co) - This MCP server allows you to get live deals and offers/coupons from e-commerce merchant sites all over the world.\n- **[kintone](https://github.com/macrat/mcp-server-kintone)** - Manage records and apps in [kintone](https://kintone.com) through LLM tools.\n- **[Kokoro TTS](https://github.com/mberg/kokoro-tts-mcp)** - Use Kokoro text to speech to convert text to MP3s with optional autoupload to S3.\n- **[Kong Konnect](https://github.com/Kong/mcp-konnect)** - A Model Context Protocol (MCP) server for interacting with Kong Konnect APIs, allowing AI assistants to query and analyze Kong Gateway configurations, traffic, and analytics.\n- **[Korea Stock Analyzer](https://github.com/Mrbaeksang/korea-stock-analyzer-mcp)** - Analyze Korean stocks (KOSPI/KOSDAQ) with 6 legendary investment strategies including Buffett, Lynch, Graham, Greenblatt, Fisher, and Templeton.\n- **[Kubernetes](https://github.com/Flux159/mcp-server-kubernetes)** - Connect to Kubernetes cluster and manage pods, deployments, and services.\n- **[Kubernetes and OpenShift](https://github.com/manusa/kubernetes-mcp-server)** - A powerful Kubernetes MCP server with additional support for OpenShift. Besides providing CRUD operations for any Kubernetes resource, this server provides specialized tools to interact with your cluster.\n- **[KubeSphere](https://github.com/kubesphere/ks-mcp-server)** - The KubeSphere MCP Server is a Model Context Protocol(MCP) server that provides integration with KubeSphere APIs, enabling to get resources from KubeSphere. Divided into four tools modules: Workspace Management, Cluster Management, User and Roles, Extensions Center.\n- **[Kukapay MCP Servers](https://github.com/kukapay/kukapay-mcp-servers)** - A comprehensive suite of Model Context Protocol (MCP) servers dedicated to cryptocurrency, blockchain, and Web3 data aggregation, analysis, and services from Kukapay.\n- **[kwrds.ai](https://github.com/mkotsollaris/kwrds_ai_mcp)** - Keyword research, people also ask, SERP and other SEO tools for [kwrds.ai](https://www.kwrds.ai/)\n- **[KYC-mcp-server](https://github.com/vishnurudra-ai/KYC-mcp-server)** - Know Your Computer (KYC) - MCP Server compatible with Claude Desktop. Comprehensive system diagnostics for Windows, Mac OS and Linux operating system with AI-powered recommendations.\n- **[Langflow-DOC-QA-SERVER](https://github.com/GongRzhe/Langflow-DOC-QA-SERVER)** - A Model Context Protocol server for document Q&A powered by Langflow. It demonstrates core MCP concepts by providing a simple interface to query documents through a Langflow backend.\n- **[Language Server](https://github.com/isaacphi/mcp-language-server)** - MCP Language Server helps MCP enabled clients navigate codebases more easily by giving them access to semantic tools like get definition, references, rename, and diagnostics.\n- **[Lark(Feishu)](https://github.com/kone-net/mcp_server_lark)** - A Model Context Protocol(MCP) server for Lark(Feishu) sheet, message, doc and etc.\n- **[Lazy Toggl MCP](https://github.com/movstox/lazy-toggl-mcp)** - Simple unofficial MCP server to track time via Toggl API\n- **[lean-lsp-mcp](https://github.com/oOo0oOo/lean-lsp-mcp)** - Interact with the [Lean theorem prover](https://lean-lang.org/) via the Language Server Protocol.\n- **[librenms-mcp](https://github.com/mhajder/librenms-mcp)** - MCP server for [LibreNMS](https://www.librenms.org/) management\n- **[libvirt-mcp](https://github.com/MatiasVara/libvirt-mcp)** - Allows LLM to interact with libvirt thus enabling to create, destroy or list the Virtual Machines in a system.\n- **[Lightdash](https://github.com/syucream/lightdash-mcp-server)** - Interact with [Lightdash](https://www.lightdash.com/), a BI tool.\n- **[LINE](https://github.com/amornpan/py-mcp-line)** (by amornpan) - Implementation for LINE Bot integration that enables Language Models to read and analyze LINE conversations through a standardized interface. Features asynchronous operation, comprehensive logging, webhook event handling, and support for various message types.\n- **[Linear](https://github.com/tacticlaunch/mcp-linear)** - Interact with Linear project management system.\n- **[Linear](https://github.com/jerhadf/linear-mcp-server)** - Allows LLM to interact with Linear's API for project management, including searching, creating, and updating issues.\n- **[Linear (Go)](https://github.com/geropl/linear-mcp-go)** - Allows LLM to interact with Linear's API via a single static binary.\n- **[Linear MCP](https://github.com/anoncam/linear-mcp)** - Full blown implementation of the Linear SDK to support comprehensive Linear management of projects, initiatives, issues, users, teams and states.\n- **[Linked API MCP](https://github.com/Linked-API/linkedapi-mcp)** - MCP server that lets AI assistants control LinkedIn accounts and retrieve real-time data.\n- **[Listmonk MCP Server](https://github.com/rhnvrm/listmonk-mcp)** (by rhnvrm) - Full API coverage of [Listmonk](https://github.com/knadh/listmonk) email marketing FOSS.\n- **[LlamaCloud](https://github.com/run-llama/mcp-server-llamacloud)** (by marcusschiesser) - Integrate the data stored in a managed index on [LlamaCloud](https://cloud.llamaindex.ai/)\n- **[lldb-mcp](https://github.com/stass/lldb-mcp)** - A Model Context Protocol server for LLDB that provides LLM-driven debugging.\n- **[llm-context](https://github.com/cyberchitta/llm-context.py)** - Provides a repo-packing MCP tool with configurable profiles that specify file inclusion/exclusion patterns and optional prompts.\n- **[Local History](https://github.com/xxczaki/local-history-mcp)** – MCP server for accessing VS Code/Cursor's Local History.\n- **[Locust](https://github.com/QAInsights/locust-mcp-server)** - Allows running and analyzing Locust tests using MCP compatible clients.\n- **[Loki](https://github.com/scottlepp/loki-mcp)** - Golang based MCP Server to query logs from [Grafana Loki](https://github.com/grafana/loki).\n- **[Loki MCP Server](https://github.com/mo-silent/loki-mcp-server)** - Python based MCP Server for querying and analyzing logs from Grafana Loki with advanced filtering and authentication support.\n- **[LottieFiles](https://github.com/junmer/mcp-server-lottiefiles)** - Searching and retrieving Lottie animations from [LottieFiles](https://lottiefiles.com/)\n- **[lsp-mcp](https://github.com/Tritlo/lsp-mcp)** - Interact with Language Servers usint the Language Server Protocol to provide additional context information via hover, code actions and completions.\n- **[Lspace](https://github.com/Lspace-io/lspace-server)** - Turn scattered ChatGPT/Claude/Cursor conversations into persistent, searchable knowledge.\n- **[lucene-mcp-server](https://github.com/VivekKumarNeu/MCP-Lucene-Server)** - spring boot server using Lucene for fast document search and management.\n- **[lucid-mcp-server](https://github.com/smartzan63/lucid-mcp-server)** – An MCP server for Lucidchart and Lucidspark: connect, search, and obtain text representations of your Lucid documents and diagrams via LLM - driven AI Vision analysis. [npm](https://www.npmjs.com/package/lucid-mcp-server)\n- **[LunarCrush Remote MCP](https://github.com/lunarcrush/mcp-server)** - Get the latest social metrics and posts for both current live social context as well as historical metrics in LLM and token optimized outputs. Ideal for automated trading / financial advisory.\n- **[mac-messages-mcp](https://github.com/carterlasalle/mac_messages_mcp)** - An MCP server that securely interfaces with your iMessage database via the Model Context Protocol (MCP), allowing LLMs to query and analyze iMessage conversations. It includes robust phone number validation, attachment processing, contact management, group chat handling, and full support for sending and receiving messages.\n- **[Maestro MCP](https://github.com/maestro-org/maestro-mcp)** - An MCP server for interacting with Bitcoin via the Maestro RPC API.\n- **[Magg: The MCP Aggregator](https://github.com/sitbon/magg)** - A meta-MCP server that acts as a universal hub, allowing LLMs to autonomously discover, install, and orchestrate multiple MCP servers - essentially giving AI assistants the power to extend their own capabilities on-demand. Includes `mbro`, a powerful CLI MCP server browser with scripting capability.\n- **[Mailchimp MCP](https://github.com/AgentX-ai/mailchimp-mcp)** - Allows AI agents to interact with the Mailchimp API (read-only)\n- **[MalwareBazaar_MCP](https://github.com/mytechnotalent/MalwareBazaar_MCP)** (by Kevin Thomas) - An AI-driven MCP server that autonomously interfaces with MalwareBazaar, delivering real-time threat intel and sample metadata for authorized cybersecurity research workflows.\n- **[Mandoline](https://github.com/mandoline-ai/mandoline-mcp-server)** - Enable AI assistants to reflect on, critique, and continuously improve their own performance using Mandoline's evaluation framework.\n- **[Matrix](https://github.com/mjknowles/matrix-mcp-server)** - Interact with a Matrix homeserver.\n- **[man-mcp-server](https://github.com/guyru/man-mcp-server)** - MCP to search and access man pages on the local machine.\n- **[MariaDB](https://github.com/abel9851/mcp-server-mariadb)** - MariaDB database integration with configurable access controls in Python.\n- **[Markdown2doc](https://github.com/Klavis-AI/klavis/tree/main/mcp_servers/pandoc)** - Convert between various file formats using Pandoc\n- **[Markdownify](https://github.com/zcaceres/mcp-markdownify-server)** - MCP to convert almost anything to Markdown (PPTX, HTML, PDF, Youtube Transcripts and more)\n- **[market-fiyati](https://github.com/mtcnbzks/market-fiyati-mcp-server)** - The MCP server for marketfiyati.org.tr, offering grocery price search and comparison across Turkish markets.)\n- **[Markitdown](https://github.com/Klavis-AI/klavis/tree/main/mcp_servers/markitdown)** - Convert files to Markdown\n- **[Masquerade](https://github.com/postralai/masquerade)** - Redact sensitive information from your PDF documents before sending them to Claude. Masquerade serves as a privacy firewall for LLMs.\n- **[MasterGo](https://github.com/mastergo-design/mastergo-magic-mcp)** - The server designed to connect MasterGo design tools with AI models. It enables AI models to directly retrieve DSL data from MasterGo design files.\n- **[Matlab-MCP-Tools](https://github.com/neuromechanist/matlab-mcp-tools)** - An MCP to write and execute MATLAB scripts, maintain workspace context between MCP calls, visualize plots, and perform section-by-section analysis of MATLAB code with full access to MATLAB's computational capabilities.\n- **[Maton](https://github.com/maton-ai/agent-toolkit/tree/main/modelcontextprotocol)** - Connect to your SaaS tools like HubSpot, Salesforce, and more.\n- **[Maven Tools MCP](https://github.com/arvindand/maven-tools-mcp)** - Maven Central dependency intelligence for JVM build tools. Supports all build tools (Maven, Gradle, SBT, Mill) with Context7 integration for documentation support.\n- **[MCP-Airflow-API](https://github.com/call518/MCP-Airflow-API)** - Model Context Protocol (MCP) server for Apache Airflow API integration. Provides comprehensive tools for managing Airflow clusters including service operations, configuration management, status monitoring, and request tracking.\n- **[mcpcap](https://github.com/mcpcap/mcpcap)** - A modular Python MCP (Model Context Protocol) Server for analyzing PCAP files.\n- **[MCP Compass](https://github.com/liuyoshio/mcp-compass)** - Suggest the right MCP server for your needs\n- **[MCP Create](https://github.com/tesla0225/mcp-create)** - A dynamic MCP server management service that creates, runs, and manages Model Context Protocol servers on-the-fly.\n- **[MCP Documentation Server](https://github.com/andrea9293/mcp-documentation-server)** - Server that provides local-first document management and semantic search via embeddings or Gemini AI (recommended). Optimized for performance with disk persistence, an in-memory index, and caching.\n- **[MCP Installer](https://github.com/anaisbetts/mcp-installer)** - This server is a server that installs other MCP servers for you.\n- **[MCP ProjectManage OpenProject](https://github.com/boma086/mcp-projectmanage-openproject)** - This server provides the MCP service for project weekly reports, with project management information supplied by OpenProject.\n- **[MCP Proxy Server](https://github.com/TBXark/mcp-proxy)** - An MCP proxy server that aggregates and serves multiple MCP resource servers through a single HTTP server.\n- **[MCP Server Creator](https://github.com/GongRzhe/MCP-Server-Creator)** - A powerful Model Context Protocol (MCP) server that creates other MCP servers! This meta-server provides tools for dynamically generating FastMCP server configurations and Python code.\n- **[MCP Server Generator](https://github.com/SerhatUzbas/mcp-server-generator)** - An MCP server that creates and manages  MCP servers! Helps both non-technical users and developers build custom JavaScript MCP servers with AI guidance, automatic dependency management, and Claude Desktop integration.\n- **[MCP STDIO to Streamable HTTP Adapter](https://github.com/pyroprompts/mcp-stdio-to-streamable-http-adapter)** - Connect to Streamable HTTP MCP Servers even if the MCP Client only supports STDIO.\n- **[MCP-Ambari-API](https://github.com/call518/MCP-Ambari-API)** - Model Context Protocol (MCP) server for Apache Ambari API integration. This project provides tools for managing Hadoop clusters, including service operations, configuration management, status monitoring, and request tracking.\n- **[MCP-OpenStack-Ops](https://github.com/call518/MCP-OpenStack-Ops)** - Professional OpenStack operations automation via MCP server. Specialized tools for cluster monitoring, instance management, volume control & network analysis. FastMCP + OpenStack SDK + Bearer auth. Claude Desktop ready. Perfect for DevOps & cloud automation.\n- **[MCP-PostgreSQL-Ops](https://github.com/call518/MCP-PostgreSQL-Ops)** - Model Context Protocol (MCP) server for Apache Ambari API integration. This project provides tools for managing Hadoop clusters, including service operations, configuration management, status monitoring, and request tracking.\n- **[mcp-containerd](https://github.com/jokemanfire/mcp-containerd)** - The containerd MCP implemented by Rust supports the operation of the CRI interface.\n- **[MCP-Database-Server](https://github.com/executeautomation/mcp-database-server)** - Fastest way to interact with your Database such as SQL Server, SQLite and PostgreSQL\n- **[mcp-grep](https://github.com/erniebrodeur/mcp-grep)** - Python-based MCP server that brings grep functionality to LLMs. Supports common grep features including pattern searching, case-insensitive matching, context lines, and recursive directory searches.\n- **[mcp-k8s-go](https://github.com/strowk/mcp-k8s-go)** - Golang-based Kubernetes server for MCP to browse pods and their logs, events, namespaces and more. Built to be extensible.\n- **[mcp-local-rag](https://github.com/nkapila6/mcp-local-rag)** - \"primitive\" RAG-like web search model context protocol (MCP) server that runs locally using Google's MediaPipe Text Embedder and DuckDuckGo Search.\n- **[mcp-mcp](https://github.com/wojtyniak/mcp-mcp)** - Meta-MCP Server that acts as a tool discovery service for MCP clients.\n- **[mcp-meme-sticky](https://github.com/nkapila6/mcp-meme-sticky)** - Make memes or stickers using MCP server for WhatsApp or Telegram.\n- **[mcp-memory-service](https://github.com/doobidoo/mcp-memory-service)** - Universal MCP memory service providing semantic memory search, persistent storage, and autonomous memory consolidation for AI assistants across 13+ AI applications.\n- **[MCP-NixOS](https://github.com/utensils/mcp-nixos)** - A Model Context Protocol server that provides AI assistants with accurate, real-time information about NixOS packages, system options, Home Manager settings, and nix-darwin macOS configurations.\n- **[mcp-open-library](https://github.com/8enSmith/mcp-open-library)** - A Model Context Protocol (MCP) server for the Open Library API that enables AI assistants to search for book and author information.\n- **[mcp-proxy](https://github.com/sparfenyuk/mcp-proxy)** - Connect to MCP servers that run on SSE transport, or expose stdio servers as an SSE server.\n- **[mcp-read-website-fast](https://github.com/just-every/mcp-read-website-fast)** - Fast, token-efficient web content extraction that converts websites to clean Markdown. Features Mozilla Readability, smart caching, polite crawling with robots.txt support, and concurrent fetching with minimal dependencies.\n- **[mcp-salesforce](https://github.com/lciesielski/mcp-salesforce-example)** - MCP server with basic demonstration of interactions with your Salesforce instance\n- **[mcp-sanctions](https://github.com/madupay/mcp-sanctions)** - Screen individuals and organizations against global sanctions lists (OFAC, SDN, UN, etc). Query by prompt or document upload.\n- **[mcp-screenshot-website-fast](https://github.com/just-every/mcp-screenshot-website-fast)** - High-quality screenshot capture optimized for Claude Vision API. Automatically tiles full pages into 1072x1072 chunks (1.15 megapixels) with configurable viewports and wait strategies for dynamic content.\n- **[mcp-server-leetcode](https://github.com/doggybee/mcp-server-leetcode)** - Practice and retrieve problems from LeetCode. Automate problem retrieval, solutions, and insights for coding practice and competitions.\n- **[Mcp-Swagger-Server](https://github.com/zaizaizhao/mcp-swagger-server)** (by zaizaizhao) - This MCP server transforms OpenAPI specifications into MCP tools, enabling AI assistants to interact with REST APIs through standardized protocol\n- **[MCP Dynamic Tool Groups](https://github.com/ECF/MCPToolGroups)** - Example MCP servers that use [annotated](https://github.com/spring-ai-community/mcp-annotations) Java interfaces/classes as 'tool groups'.  Using standard MCP annotations, service implementations can then, at runtime, be used to generate tool specifications, and then dynamically added or removed from MCP servers.   The functionality is demonstrated in a sample tool group, but can be similarly used for any API or service.\n- **[mcp-vision](https://github.com/groundlight/mcp-vision)** - An MCP server exposing HuggingFace computer vision models such as zero-shot object detection as tools, enhancing the vision capabilities of large language or vision-language models.\n- **[mcp-weather](https://github.com/TimLukaHorstmann/mcp-weather)** - Accurate weather forecasts via the AccuWeather API (free tier available).\n- **[KnowAir Weather MCP](https://github.com/shuowang-ai/Weather-MCP)** - A comprehensive Model Context Protocol (MCP) server providing real-time weather data, air quality monitoring, forecasts, and astronomical information powered by Caiyun Weather API.\n- **[mcp-youtube-extract](https://github.com/sinjab/mcp_youtube_extract)** - A Model Context Protocol server for YouTube operations, extracting video information and transcripts with intelligent fallback logic. Features comprehensive logging, error handling, and support for both auto-generated and manual transcripts.\n- **[mcp_weather](https://github.com/isdaniel/mcp_weather_server)** - Get weather information from https://api.open-meteo.com API.\n- **[MCPfinder](https://github.com/mcpfinder/server)** - The AI Agent's \"App Store\": Discover, install, and monetize AI capabilities — all within the MCP ecosystem.\n- **[MCPIgnore Filesytem](https://github.com/CyberhavenInc/filesystem-mcpignore)** - A Data Security First filesystem MCP server that implements .mcpignore to prevent MCP clients from accessing sensitive data.\n- **[MCPJungle](https://github.com/mcpjungle/MCPJungle)** - Self-hosted MCP Registry and Gateway for enterprise AI Agents\n- **[Md2doc](https://github.com/Yorick-Ryu/md2doc-mcp)** - Convert Markdown text to DOCX format using an external conversion service\n- **[MeasureSpace MCP](https://github.com/MeasureSpace/measure-space-mcp-server)** - A free [Model Context Protocol (MCP) Server](https://smithery.ai/server/@MeasureSpace/measure-space-mcp-server) that provides global weather, climate, air quality forecast and geocoding services by [measurespace.io](https://measurespace.io).\n- **[MediaWiki](https://github.com/ProfessionalWiki/MediaWiki-MCP-Server)** - A Model Context Protocol (MCP) Server that interacts with any MediaWiki wiki\n- **[MediaWiki MCP adapter](https://github.com/lucamauri/MediaWiki-MCP-adapter)** - A custom Model Context Protocol adapter for MediaWiki and WikiBase APIs\n- **[medRxiv](https://github.com/JackKuo666/medRxiv-MCP-Server)** - Enable AI assistants to search and access medRxiv papers through a simple MCP interface.\n- **[mem0-mcp](https://github.com/mem0ai/mem0-mcp)** - A Model Context Protocol server for Mem0, which helps with managing coding preferences.\n- **[Membase](https://github.com/unibaseio/membase-mcp)** - Save and query your agent memory in distributed way by Membase.\n- **[Meme MCP](https://github.com/lidorshimoni/meme-mcp)** - Generate memes via AI using the Imgflip API through the Model Context Protocol.\n- **[memento-mcp](https://github.com/gannonh/memento-mcp)** - Knowledge graph memory system built on Neo4j with semantic search, temporal awareness.\n- **[Meta Ads Remote MCP](https://github.com/pipeboard-co/meta-ads-mcp)** - Remote MCP server to interact with Meta Ads API - access, analyze, and manage Facebook, Instagram, and other Meta platforms advertising campaigns.\n- **[MetaTrader MCP](https://github.com/ariadng/metatrader-mcp-server)** - Enable AI LLMs to execute trades using MetaTrader 5 platform.\n- **[Metricool MCP](https://github.com/metricool/mcp-metricool)** - A Model Context Protocol server that integrates with Metricool's social media analytics platform to retrieve performance metrics and schedule content across networks like Instagram, Facebook, Twitter, LinkedIn, TikTok and YouTube.\n- **[Microsoft 365](https://github.com/merill/lokka)** - (by Merill) A Model Context Protocol (MCP) server for Microsoft 365. Includes support for all services including Teams, SharePoint, Exchange, OneDrive, Entra, Intune and more. See [Lokka](https://lokka.dev/) for more details.\n- **[Microsoft 365](https://github.com/softeria/ms-365-mcp-server)** - MCP server that connects to Microsoft Office and the whole Microsoft 365 suite using Graph API (including Outlook/mail, files, Excel, calendar)\n- **[Microsoft 365](https://github.com/pnp/cli-microsoft365-mcp-server)** - Single MCP server that allows to manage many different areas of Microsoft 365, for example: Entra ID, OneDrive, OneNote, Outlook, Planner, Power Apps, Power Automate, Power Platform, SharePoint Embedded, SharePoint Online, Teams, Viva Engage, and many more.\n- **[Microsoft 365 Files (SharePoint/OneDrive)](https://github.com/godwin3737/mcp-server-microsoft365-filesearch)** (by godwin3737) - MCP server with tools to search and get file content from Microsoft 365 including Onedrive and SharePoint. Works with Documents (pdf/docx), Presentations, Spreadsheets and Images.\n- **[Microsoft Teams](https://github.com/InditexTech/mcp-teams-server)** - MCP server that integrates Microsoft Teams messaging (read, post, mention, list members and threads)\n- **[Mifos X](https://github.com/openMF/mcp-mifosx)** - An MCP server for the Mifos X Open Source Banking useful for managing clients, loans, savings, shares, financial transactions and generating financial reports.\n- **[Mikrotik](https://github.com/jeff-nasseri/mikrotik-mcp)** - Mikrotik MCP server which cover networking operations (IP, DHCP, Firewall, etc)\n- **[Mindmap](https://github.com/YuChenSSR/mindmap-mcp-server)** (by YuChenSSR) - A server that generates mindmaps from input containing markdown code.\n- **[Minima](https://github.com/dmayboroda/minima)** - MCP server for RAG on local files\n- **[Modao Proto MCP](https://github.com/modao-dev/modao-proto-mcp)** - AI-powered HTML prototype generation server that converts natural language descriptions into complete HTML code with modern design and responsive layouts. Supports design description expansion and seamless integration with Modao workspace.\n- **[Mobile MCP](https://github.com/mobile-next/mobile-mcp)** (by Mobile Next) - MCP server for Mobile(iOS/Android) automation, app scraping and development using physical devices or simulators/emulators.\n- **[Monday.com (unofficial)](https://github.com/sakce/mcp-server-monday)** - MCP Server to interact with Monday.com boards and items.\n- **[MongoDB](https://github.com/kiliczsh/mcp-mongo-server)** - A Model Context Protocol Server for MongoDB.\n- **[MongoDB & Mongoose](https://github.com/nabid-pf/mongo-mongoose-mcp)** - MongoDB MCP Server with Mongoose Schema and Validation.\n- **[MongoDB Lens](https://github.com/furey/mongodb-lens)** - Full Featured MCP Server for MongoDB Databases.\n- **[Monzo](https://github.com/BfdCampos/monzo-mcp-bfdcampos)** - Access and manage your Monzo bank accounts through natural language, including balance checking, pot management, transaction listing, and transaction annotation across multiple account types (personal, joint, flex).\n- **[Morningstar](https://github.com/Morningstar/morningstar-mcp-server)** - MCP Server to interact with Morningstar Research, Editorial and Datapoints\n- **[MSSQL](https://github.com/aekanun2020/mcp-server/)** - MSSQL database integration with configurable access controls and schema inspection\n- **[MSSQL](https://github.com/JexinSam/mssql_mcp_server)** (by jexin) - MCP Server for MSSQL database in Python\n- **[MSSQL-MCP](https://github.com/daobataotie/mssql-mcp)** (by daobataotie) - MSSQL MCP that refer to the official website's SQLite MCP for modifications to adapt to MSSQL\n- **[MSSQL-MCP-Node](https://github.com/mihai-dulgheru/mssql-mcp-node)** (by mihai - dulgheru) – Node.js MCP server for Microsoft SQL Server featuring auto-detected single / multi-database configs, execute-SQL and schema tools, robust Zod validation, and optional Express endpoints for local testing\n- **[MSSQL-Python](https://github.com/amornpan/py-mcp-mssql)** (by amornpan) - A read-only Python implementation for MSSQL database access with enhanced security features, configurable access controls, and schema inspection capabilities. Focuses on safe database interaction through Python ecosystem.\n- **[Multi-Model Advisor](https://github.com/YuChenSSR/multi-ai-advisor-mcp)** - A Model Context Protocol (MCP) server that orchestrates queries across multiple Ollama models, synthesizing their insights to deliver a comprehensive and multifaceted AI perspective on any given query.\n- **[Multicluster-MCP-Sever](https://github.com/yanmxa/multicluster-mcp-server)** - The gateway for GenAI systems to interact with multiple Kubernetes clusters.\n- **[MySQL](https://github.com/benborla/mcp-server-mysql)** (by benborla) - MySQL database integration in NodeJS with configurable access controls and schema inspection\n- **[MySQL](https://github.com/designcomputer/mysql_mcp_server)** (by DesignComputer) - MySQL database integration in Python with configurable access controls and schema inspection\n- **[MySQL-Server](https://github.com/tonycai/mcp-mysql-server)** (by TonyCai) - MySQL Database Integration using Python script with configurable access controls and schema inspection, usng stdio mode to suitable local deployment, you can run it in docker container.\n- **[n8n](https://github.com/leonardsellem/n8n-mcp-server)** - This MCP server provides tools and resources for AI assistants to manage n8n workflows and executions, including listing, creating, updating, and deleting workflows, as well as monitoring their execution status.\n- **[Nacos MCP Router](https://github.com/nacos-group/nacos-mcp-router)** - This MCP(Model Context Protocol) Server provides tools to search, install, proxy other MCP servers.\n- **[NASA](https://github.com/ProgramComputer/NASA-MCP-server)** (by ProgramComputer) - Access to a unified gateway of NASA's data sources including but not limited to APOD, NEO, EPIC, GIBS.\n- **[NASA Image MCP Server](https://github.com/adithya1012/NASA-MCP-Server/blob/main/README.md)** - MCP server providing access to NASA's visual data APIs including Mars Rover photos, Earth satellite imagery (EPIC/GIBS), and Astronomy picture of the day. Features built-in image analysis tools with automatic format detection, compression, and base64 conversion for LLM integration.\n- **[Nasdaq Data Link](https://github.com/stefanoamorelli/nasdaq-data-link-mcp)** (by stefanoamorelli) - An MCP server to access, explore, and interact with Nasdaq Data Link's extensive and valuable financial and economic datasets.\n- **[National Parks](https://github.com/KyrieTangSheng/mcp-server-nationalparks)** - The server provides latest information of park details, alerts, visitor centers, campgrounds, hiking trails, and events for U.S. National Parks.\n- **[NAVER](https://github.com/pfldy2850/py-mcp-naver)** (by pfldy2850) - This MCP server provides tools to interact with various Naver services, such as searching blogs, news, books, and more.\n- **[Naver](https://github.com/isnow890/naver-search-mcp)** (by isnow890) - MCP server for Naver Search API integration, supporting blog, news, shopping search and DataLab analytics features.\n- **[NBA](https://github.com/Taidgh-Robinson/nba-mcp-server)** - This MCP server provides tools to fetch recent and historical NBA games including basic and advanced statistics.\n- **[NCI GDC](https://github.com/QuentinCody/nci-gdc-mcp-server)** - Unofficial MCP server for the National Cancer Institute's Genomic Data Commons (GDC), providing access to harmonized cancer genomic and clinical data for oncology research.\n- **[Neo4j](https://github.com/da-okazaki/mcp-neo4j-server)** - A community built server that interacts with Neo4j Graph Database.\n- **[Neovim](https://github.com/bigcodegen/mcp-neovim-server)** - An MCP Server for your Neovim session.\n- **[Netbird](https://github.com/aantti/mcp-netbird)** - List and analyze Netbird network peers, groups, policies, and more.\n- **[NetMind ParsePro](https://github.com/protagolabs/Netmind-Parse-PDF-MCP)** - The PDF Parser AI service, built and customized by the [NetMind](https://www.netmind.ai/) team.\n- **[Nikto MCP](https://github.com/weldpua2008/nikto-mcp)** (by weldpua2008) - A secure MCP server that enables AI agents to interact with Nikto web server scanner](- use with npx or docker).\n- **[NocoDB](https://github.com/edwinbernadus/nocodb-mcp-server)** - Read and write access to NocoDB database.\n- **[Node Code Sandbox](https://github.com/alfonsograziano/node-code-sandbox-mcp)** – A Node.js MCP server that spins up isolated Docker - based sandboxes for executing JavaScript snippets with on-the-fly npm dependency installation\n- **[nomad-mcp](https://github.com/kocierik/mcp-nomad)** - A server that provides a set of tools for managing Nomad clusters through the MCP.\n- **[Notion](https://github.com/suekou/mcp-notion-server)** (by suekou) - Interact with Notion API.\n- **[Notion](https://github.com/v-3/notion-server)** (by v-3) - Notion MCP integration. Search, Read, Update, and Create pages through Claude chat.\n- **[NPM Plus](https://github.com/shacharsol/js-package-manager-mcp)** - AI-powered JavaScript package management with security scanning, bundle analysis, and intelligent dependency management for MCP-compatible editors.\n- **[NS Travel Information](https://github.com/r-huijts/ns-mcp-server)** - Access Dutch Railways (NS) real-time train travel information and disruptions through the official NS API.\n- **[ntfy-mcp](https://github.com/teddyzxcv/ntfy-mcp)** (by teddyzxcv) - The MCP server that keeps you informed by sending the notification on phone using ntfy\n- **[ntfy-me-mcp](https://github.com/gitmotion/ntfy-me-mcp)** (by gitmotion) - An ntfy MCP server for sending/fetching ntfy notifications to your self-hosted ntfy server from AI Agents 📤 (supports secure token auth & more - use with npx or docker!)\n- **[oatpp-mcp](https://github.com/oatpp/oatpp-mcp)** - C++ MCP integration for Oat++. Use [Oat++](https://oatpp.io) to build MCP servers.\n- **[Obsidian Markdown Notes](https://github.com/calclavia/mcp-obsidian)** - Read and search through your Obsidian vault or any directory containing Markdown notes\n- **[obsidian-mcp](https://github.com/StevenStavrakis/obsidian-mcp)** - (by Steven Stavrakis) An MCP server for Obsidian.md with tools for searching, reading, writing, and organizing notes.\n- **[OceanBase](https://github.com/yuanoOo/oceanbase_mcp_server)** - (by yuanoOo) A Model Context Protocol (MCP) server that enables secure interaction with OceanBase databases.\n- **[Octocode](https://github.com/bgauryy/octocode-mcp)** - (by Guy Bary) AI-powered developer assistant that enables advanced code research, analysis and discovery across GitHub and NPM realms in realtime\n- **[Odoo](https://github.com/ivnvxd/mcp-server-odoo)** - Connect AI assistants to Odoo ERP systems for business data access and workflow automation.\n- **[Office-PowerPoint-MCP-Server](https://github.com/GongRzhe/Office-PowerPoint-MCP-Server)** - A Model Context Protocol (MCP) server for creating, reading, and manipulating Microsoft PowerPoint documents.\n- **[Office-Visio-MCP-Server](https://github.com/GongRzhe/Office-Visio-MCP-Server)** - A Model Context Protocol (MCP) server for creating, reading, and manipulating Microsoft Visio documents.\n- **[Office-Word-MCP-Server](https://github.com/GongRzhe/Office-Word-MCP-Server)** - A Model Context Protocol (MCP) server for creating, reading, and manipulating Microsoft Word documents.\n- **[Okta](https://github.com/kapilduraphe/okta-mcp-server)** - Interact with Okta API.\n- **[OKX-MCP-Server](https://github.com/memetus/okx-mcp-playground)** - An MCP server provides various blockchain data and market price data via the OKX API. The server enables Claude to perform operations like retrieve assets prices, transaction data, account history data and trade instruction data.\n- **[OneNote](https://github.com/rajvirtual/MCP-Servers/tree/master/onenote)** - (by Rajesh Vijay) An MCP server that connects to Microsoft OneNote using the Microsoft Graph API. Reading notebooks, sections, and pages from OneNote,Creating new notebooks, sections, and pages in OneNote.\n- **[Onyx MCP Sandbox](https://github.com/avd1729/Onyx)** – (by Aravind) A secure MCP server that executes code in isolated Docker sandboxes. Supports Python, Java, C, C++, JavaScript, and Rust. Provides the `run_code` tool, enforces CPU/memory limits, includes comprehensive tests, and detailed setup instructions.\n- **[Open Strategy Partners Marketing Tools](https://github.com/open-strategy-partners/osp_marketing_tools)** - Content editing codes, value map, and positioning tools for product marketing.\n- **[OpenAI WebSearch MCP](https://github.com/ConechoAI/openai-websearch-mcp)** - This is a Python-based MCP server that provides OpenAI `web_search` built-in tool.\n- **[OpenAlex.org MCP](https://github.com/drAbreu/alex-mcp)** - Professional MCP server providing ML-powered author disambiguation and comprehensive researcher profiles using the OpenAlex database.\n- **[OpenAPI](https://github.com/snaggle-ai/openapi-mcp-server)** - Interact with [OpenAPI](https://www.openapis.org/) APIs.\n- **[OpenAPI AnyApi](https://github.com/baryhuang/mcp-server-any-openapi)** - Interact with large [OpenAPI](https://www.openapis.org/) docs using built-in semantic search for endpoints. Allows for customizing the MCP server prefix.\n- **[OpenAPI Schema](https://github.com/hannesj/mcp-openapi-schema)** - Allow LLMs to explore large [OpenAPI](https://www.openapis.org/) schemas without bloating the context.\n- **[OpenAPI Schema Explorer](https://github.com/kadykov/mcp-openapi-schema-explorer)** - Token-efficient access to local or remote OpenAPI/Swagger specs via MCP Resources.\n- **[OpenCTI](https://github.com/Spathodea-Network/opencti-mcp)** - Interact with OpenCTI platform to retrieve threat intelligence data including reports, indicators, malware and threat actors.\n- **[OpenCV](https://github.com/GongRzhe/opencv-mcp-server)** - An MCP server providing OpenCV computer vision capabilities. This allows AI assistants and language models to access powerful computer vision tools.\n- **[OpenDota](https://github.com/asusevski/opendota-mcp-server)** - Interact with OpenDota API to retrieve Dota 2 match data, player statistics, and more.\n- **[OpenLink Generic Java Database Connectivity](https://github.com/OpenLinkSoftware/mcp-jdbc-server)** - Generic Database Management System (DBMS) access via Open Database Connectivity (ODBC) Connectors (Drivers)\n- **[OpenLink Generic Open Database Connectivity](https://github.com/OpenLinkSoftware/mcp-odbc-server)** - Generic Database Management System (DBMS) access via Open Database Connectivity (ODBC) Connectors (Drivers)\n- **[OpenLink Generic Python Open Database Connectivity](https://github.com/OpenLinkSoftware/mcp-pyodbc-server)** - Generic Database Management System (DBMS) access via Open Database Connectivity (ODBC) Connectors (Drivers) for PyODBC\n- **[OpenLink Generic SQLAlchemy Object-Relational Database Connectivity for PyODBC](https://github.com/OpenLinkSoftware/mcp-sqlalchemy-server)** - Generic Database Management System (DBMS) access via SQLAlchemy (PyODBC) Connectors (Drivers)\n- **[OpenMetadata](https://github.com/yangkyeongmo/mcp-server-openmetadata)** - MCP Server for OpenMetadata, an open-source metadata management platform.\n- **[OpenNeuro](https://github.com/QuentinCody/open-neuro-mcp-server)** - Unofficial MCP server for OpenNeuro, providing access to open neuroimaging datasets, study metadata, and brain imaging data for neuroscience research and analysis.\n- **[OpenReview](https://github.com/anyakors/openreview-mcp-server)** - An MCP server for [OpenReview](https://openreview.net/) to fetch, read and save manuscripts from AI/ML conferences.\n- **[OpenRPC](https://github.com/shanejonas/openrpc-mpc-server)** - Interact with and discover JSON-RPC APIs via [OpenRPC](https://open-rpc.org).\n- **[OpenStack](https://github.com/wangsqly0407/openstack-mcp-server)** - MCP server implementation that provides OpenStack interaction.\n- **[Open Targets](https://github.com/QuentinCody/open-targets-mcp-server)** - Unofficial MCP server for the Open Targets Platform, providing access to target-disease associations, drug discovery data, and therapeutic hypothesis generation for biomedical research.\n- **[OpenWeather](https://github.com/mschneider82/mcp-openweather)** - Interact with the free openweathermap API to get the current and forecast weather for a location.\n- **[OpenZIM MCP](https://github.com/cameronrye/openzim-mcp)** - Modern, secure, and high-performance MCP server that enables AI models to access and search ZIM format knowledge bases offline, including Wikipedia and educational content archives.\n- **[Operative WebEvalAgent](https://github.com/Operative-Sh/web-eval-agent)** (by [Operative.sh](https://www.operative.sh)) - An MCP server to test, debug, and fix web applications autonomously.\n- **[OPNSense MCP](https://github.com/vespo92/OPNSenseMCP)** - MCP Server for OPNSense Firewall Management and API access\n- **[OpenAI GPT Image](https://github.com/SureScaleAI/openai-gpt-image-mcp)** - OpenAI GPT image generation/editing MCP server.\n- **[Optimade MCP](https://github.com/dianfengxiaobo/optimade-mcp-server)** - An MCP server conducts real-time material science data queries with the Optimade database (for example, elemental composition, crystal structure).\n- **[Oracle](https://github.com/marcelo-ochoa/servers)** (by marcelo-ochoa) - Oracle Database integration in NodeJS with configurable access controls, query explain, stats and schema inspection\n- **[Oracle Cloud Infrastructure (OCI)](https://github.com/karthiksuku/oci-mcp)** (by karthiksukumar) - Python MCP server for OCI infrastructure (Compute, Autonomous Database, Object Storage). Read-heavy by default with safe instance actions (start/stop/reset). Includes Claude Desktop config and `.env` compartment scoping.\n- **[Oura MCP server](https://github.com/tomekkorbak/oura-mcp-server)** - MCP server for Oura API to retrieve one's sleep data\n- **[Oura Ring](https://github.com/rajvirtual/oura-mcp-server)** (by Rajesh Vijay) - MCP Server to access and analyze your Oura Ring data. It provides a structured way to fetch and understand your health metrics.\n- **[Outline](https://github.com/Vortiago/mcp-outline)** - MCP Server to interact with [Outline](https://www.getoutline.com) knowledge base to search, read, create, and manage documents and their content, access collections, add comments, and manage document backlinks.\n- **[Outlook Mail + Calendar + OneDrive](https://github.com/Norcim133/OutlookMCPServer) - Virtual assistant with Outlook Mail, Calendar, and early OneDrive support (requires Azure admin).\n- **[Pacman](https://github.com/oborchers/mcp-server-pacman)** - An MCP server that provides package index querying capabilities. This server is able to search and retrieve information from package repositories like PyPI, npm, crates.io, Docker Hub, and Terraform Registry.\n- **[pancakeswap-poolspy-mcp](https://github.com/kukapay/pancakeswap-poolspy-mcp)** - An MCP server that tracks newly created liquidity pools on Pancake Swap.\n- **[Pandoc](https://github.com/vivekVells/mcp-pandoc)** - MCP server for seamless document format conversion using Pandoc, supporting Markdown, HTML, PDF, DOCX (.docx), csv and more.\n- **[Paradex MCP](https://github.com/sv/mcp-paradex-py)** - MCP native server for interacting with Paradex platform, including fully features trading.\n- **[Parliament MCP]([https://github.com/sv/mcp-paradex-py](https://github.com/i-dot-ai/parliament-mcp))** - MCP server for querying UK parliamentary data.\n- **[PDF reader MCP](https://github.com/gpetraroli/mcp_pdf_reader)** - MCP server to read and search text in a local PDF file.\n- **[PDF Tools MCP](https://github.com/Sohaib-2/pdf-mcp-server)** - Comprehensive PDF manipulation toolkit (merge, split, encrypt, optimize and much more)\n- **[PDMT](https://github.com/paiml/pdmt)** - Pragmatic Deterministic MCP Templating - High-performance deterministic templating library with comprehensive todo validation, quality enforcement, and 0.0 temperature generation for reproducible outputs.\n- **[Peacock for VS Code](https://github.com/johnpapa/peacock-mcp)** - MCP Server for the Peacock extension for VS Code, coloring your world, one Code editor at a time. The main goal of the project is to show how an MCP server can be used to interact with APIs.\n- **[persistproc](https://github.com/irskep/persistproc)** - MCP server + command line tool that allows agents to see & control long-running processes like web servers.\n- **[Pexels](https://github.com/garylab/pexels-mcp-server)** - A MCP server providing access to Pexels Free Image API, enabling seamless search, retrieval, and download of high-quality royalty-free images.\n- **[Pharos](https://github.com/QuentinCody/pharos-mcp-server)** - Unofficial MCP server for the Pharos database by the National Center for Advancing Translational Sciences (NCATS), providing access to target, drug, and disease information for drug discovery research.\n- **[Phone MCP](https://github.com/hao-cyber/phone-mcp)** - 📱 A powerful plugin that lets you control your Android phone. Enables AI agents to perform complex tasks like automatically playing music based on weather or making calls and sending texts.\n- **[PIF](https://github.com/hungryrobot1/MCP-PIF)** - A Personal Intelligence Framework (PIF), providing tools for file operations, structured reasoning, and journal-based documentation to support continuity and evolving human-AI collaboration across sessions.\n- **[Pinecone](https://github.com/sirmews/mcp-pinecone)** - MCP server for searching and uploading records to Pinecone. Allows for simple RAG features, leveraging Pinecone's Inference API.\n- **[Pinner MCP](https://github.com/safedep/pinner-mcp)** - An MCP server for pinning GitHub Actions and container base images to their immutable SHA hashes to prevent supply chain attacks.\n- **[Pixelle MCP](https://github.com/AIDC-AI/Pixelle-MCP)** - An omnimodal AIGC framework that seamlessly converts ComfyUI workflows into MCP tools with zero code, enabling full-modal support for Text, Image, Sound, and Video generation with Chainlit-based web interface.\n- **[Placid.app](https://github.com/felores/placid-mcp-server)** - Generate image and video creatives using Placid.app templates\n- **[Plane](https://github.com/kelvin6365/plane-mcp-server)** - This MCP Server will help you to manage projects and issues through Plane's API\n- **[Playwright](https://github.com/executeautomation/mcp-playwright)** - This MCP Server will help you run browser automation and webscraping using Playwright\n- **[Podbean](https://github.com/amurshak/podbeanMCP)** - MCP server for managing your podcasts, episodes, and analytics through the Podbean API. Allows for updating, adding, deleting podcasts, querying show description, notes, analytics, and more.\n- **[Polarsteps](https://github.com/remuzel/polarsteps-mcp)** - An MCP server to help you review your previous Trips and plan new ones!\n- **[PostgreSQL](https://github.com/ahmedmustahid/postgres-mcp-server)** - A PostgreSQL MCP server offering dual HTTP/Stdio transports for database schema inspection and read-only query execution with session management and Podman(or Docker) support.\n- **[Postman](https://github.com/shannonlal/mcp-postman)** - MCP server for running Postman Collections locally via Newman. Allows for simple execution of Postman Server and returns the results of whether the collection passed all the tests.\n- **[Powerdrill](https://github.com/powerdrillai/powerdrill-mcp)** - Interact with Powerdrill datasets, authenticated with [Powerdrill](https://powerdrill.ai) User ID and Project API Key.\n- **[Prefect](https://github.com/allen-munsch/mcp-prefect)** - MCP Server for workflow orchestration and ELT/ETL with Prefect Server, and Prefect Cloud [https://www.prefect.io/] using the `prefect` python client.\n- **[Productboard](https://github.com/kenjihikmatullah/productboard-mcp)** - Integrate the Productboard API into agentic workflows via MCP.\n- **[Prometheus](https://github.com/pab1it0/prometheus-mcp-server)** - Query and analyze Prometheus - open-source monitoring system.\n- **[Prometheus (TypeScript)](https://github.com/yanmxa/prometheus-mcp-server)** - Enable AI assistants to query Prometheus using natural language with TypeScript implementation.\n- **[Prometheus (Golang)](https://github.com/tjhop/prometheus-mcp-server/)** - A Prometheus MCP server with full API support for comprehensive management and deep interaction with Prometheus beyond basic query support. Written in go, it is a single binary install that is capable of STDIO, SSE, and HTTP transports for complex deployments. \n- **[PubChem](https://github.com/sssjiang/pubchem_mcp_server)** - extract drug information from pubchem API.\n- **[PubMed](https://github.com/JackKuo666/PubMed-MCP-Server)** - Enable AI assistants to search, access, and analyze PubMed articles through a simple MCP interface.\n- **[Pulumi](https://github.com/dogukanakkaya/pulumi-mcp-server)** - MCP Server to Interact with Pulumi API, creates and lists Stacks\n- **[Puppeteer vision](https://github.com/djannot/puppeteer-vision-mcp)** - Use Puppeteer to browse a webpage and return a high quality Markdown. Use AI vision capabilities to handle cookies, captchas, and other interactive elements automatically.\n- **[Pushover](https://github.com/ashiknesin/pushover-mcp)** - Send instant notifications to your devices using [Pushover.net](https://pushover.net/)\n- **[py-mcp-qdrant-rag](https://github.com/amornpan/py-mcp-qdrant-rag)** (by amornpan) - A Model Context Protocol server implementation that provides RAG capabilities through Qdrant vector database integration, enabling AI agents to perform semantic search and document retrieval with local or cloud-based embedding generation support across Mac, Linux, and Windows platforms.\n- **[pydantic/pydantic-ai/mcp-run-python](https://github.com/pydantic/pydantic-ai/tree/main/mcp-run-python)** - Run Python code in a secure sandbox via MCP tool calls, powered by Deno and Pyodide\n- **[Python CLI MCP](https://github.com/ofek/pycli-mcp)** - Interact with local Python command line applications.\n- **[QGIS](https://github.com/jjsantos01/qgis_mcp)** - connects QGIS to Claude AI through the MCP. This integration enables prompt-assisted project creation, layer loading, code execution, and more.\n- **[Qiniu MCP Server](https://github.com/qiniu/qiniu-mcp-server)** - The Model Context Protocol (MCP) Server built on Qiniu Cloud products supports users in accessing Qiniu Cloud Storage, intelligent multimedia services, and more through this MCP Server within the context of AI large model clients.\n- **[QuantConnect](https://github.com/taylorwilsdon/quantconnect-mcp)** - QuantConnect Algorithmic Trading Platform Orchestration MCP - Agentic LLM Driven Trading Strategy Design, Research & Implementation.\n- **[Quarkus](https://github.com/quarkiverse/quarkus-mcp-servers)** - MCP servers for the Quarkus Java framework.\n- **[QuickChart](https://github.com/GongRzhe/Quickchart-MCP-Server)** - A Model Context Protocol server for generating charts using QuickChart.io\n- **[Qwen_Max](https://github.com/66julienmartin/MCP-server-Qwen_Max)** - A Model Context Protocol (MCP) server implementation for the Qwen models.\n- **[RabbitMQ](https://github.com/kenliao94/mcp-server-rabbitmq)** - The MCP server that interacts with RabbitMQ to publish and consume messages.\n- **[RAE](https://github.com/rae-api-com/rae-mcp)** - MPC Server to connect your preferred model with rae-api.com, Roya Academy of Spanish Dictionary\n- **[RAG Local](https://github.com/renl/mcp-rag-local)** - This MCP server for storing and retrieving text passages locally based on their semantic meaning.\n- **[RAG Web Browser](https://github.com/apify/mcp-server-rag-web-browser)** An MCP server for Apify's open-source RAG Web Browser [Actor](https://apify.com/apify/rag-web-browser) to perform web searches, scrape URLs, and return content in Markdown.\n- **[Raindrop.io](https://github.com/hiromitsusasaki/raindrop-io-mcp-server)** - An integration that allows LLMs to interact with Raindrop.io bookmarks using the Model Context Protocol (MCP).\n- **[Random Number](https://github.com/zazencodes/random-number-mcp)** - Provides LLMs with essential random generation abilities, built entirely on Python's standard library.\n- **[RCSB PDB](https://github.com/QuentinCody/rcsb-pdb-mcp-server)** - Unofficial MCP server for the Research Collaboratory for Structural Bioinformatics Protein Data Bank (RCSB PDB), providing access to 3D protein structures, experimental data, and structural bioinformatics information.\n- **[Reaper](https://github.com/dschuler36/reaper-mcp-server)** - Interact with your [Reaper](https://www.reaper.fm/) (Digital Audio Workstation) projects.\n- **[Redbee](https://github.com/Tamsi/redbee-mcp)** - Redbee MCP server that provides support for interacting with Redbee API.\n- **[Redfish](https://github.com/nokia/mcp-redfish)** - Redfish MCP server that provides support for interacting with [DMTF Redfish API](https://www.dmtf.org/standards/redfish).\n- **[Redis](https://github.com/GongRzhe/REDIS-MCP-Server)** - Redis database operations and caching microservice server with support for key-value operations, expiration management, and pattern-based key listing.\n- **[Redis](https://github.com/prajwalnayak7/mcp-server-redis)** MCP server to interact with Redis Server, AWS Memory DB, etc for caching or other use-cases where in-memory and key-value based storage is appropriate\n- **[RedNote MCP](https://github.com/ifuryst/rednote-mcp)** - MCP server for accessing RedNote(XiaoHongShu, xhs) content\n- **[Reed Jobs](https://github.com/kld3v/reed_jobs_mcp)** - Search and retrieve job listings from Reed.co.uk.\n- **[Rememberizer AI](https://github.com/skydeckai/mcp-server-rememberizer)** - An MCP server designed for interacting with the Rememberizer data source, facilitating enhanced knowledge retrieval.\n- **[Replicate](https://github.com/deepfates/mcp-replicate)** - Search, run and manage machine learning models on Replicate through a simple tool-based interface. Browse models, create predictions, track their status, and handle generated images.\n- **[Resend](https://github.com/Klavis-AI/klavis/tree/main/mcp_servers/resend)** - Send email using Resend services\n- **[Revit MCP](https://github.com/revit-mcp)** - A service implementing the MCP protocol for Autodesk Revit.\n- **[Rijksmuseum](https://github.com/r-huijts/rijksmuseum-mcp)** - Interface with the Rijksmuseum API to search artworks, retrieve artwork details, access image tiles, and explore user collections.\n- **[Riot Games](https://github.com/jifrozen0110/mcp-riot)** - MCP server for League of Legends – fetch player info, ranks, champion stats, and match history via Riot API.\n- **[Rohlik](https://github.com/tomaspavlin/rohlik-mcp)** - Shop groceries across the Rohlik Group platforms (Rohlik.cz, Knuspr.de, Gurkerl.at, Kifli.hu, Sezamo.ro)\n- **[Rquest](https://github.com/xxxbrian/mcp-rquest)** - An MCP server providing realistic browser-like HTTP request capabilities with accurate TLS/JA3/JA4 fingerprints for bypassing anti-bot measures.\n- **[Rust MCP Filesystem](https://github.com/rust-mcp-stack/rust-mcp-filesystem)** - Fast, asynchronous MCP server for efficient handling of various filesystem operations built with the power of Rust.\n- **[SafetySearch](https://github.com/surabhya/SafetySearch)** - Real-time FDA food safety data: recalls, adverse events, analysis.\n- **[Salesforce MCP](https://github.com/smn2gnt/MCP-Salesforce)** - Interact with Salesforce Data and Metadata\n- **[Salesforce MCP (AiondaDotCom)](https://github.com/AiondaDotCom/mcp-salesforce)** - Universal Salesforce integration with OAuth authentication, smart learning system, comprehensive backup capabilities, and full CRUD operations for any Salesforce org including custom objects and fields.\n- **[Salesforce MCP Server](https://github.com/tsmztech/mcp-server-salesforce)** - Comprehensive Salesforce integration with tools for querying records, executing Apex, managing fields/objects, and handling debug logs\n- **[Scanova MCP Server](https://github.com/trycon/scanova-mcp)** - MCP server for creating and managing QR codes using the [Scanova](https://scanova.io) API. Provides tools for generating, managing, and downloading QR codes.\n- **[SchemaCrawler](https://github.com/schemacrawler/SchemaCrawler-MCP-Server-Usage)** - Connect to any relational database, and be able to get valid SQL, and ask questions like what does a certain column prefix mean.\n- **[SchemaFlow](https://github.com/CryptoRadi/schemaflow-mcp-server)** - Real-time PostgreSQL & Supabase database schema access for AI-IDEs via Model Context Protocol. Provides live database context through secure SSE connections with three powerful tools: get_schema, analyze_database, and check_schema_alignment. [SchemaFlow](https://schemaflow.dev)\n- **[Scholarly](https://github.com/adityak74/mcp-scholarly)** - An MCP server to search for scholarly and academic articles.\n- **[scrapling-fetch](https://github.com/cyberchitta/scrapling-fetch-mcp)** - Access text content from bot-protected websites. Fetches HTML/markdown from sites with anti-automation measures using Scrapling.\n- **[Screeny](https://github.com/rohanrav/screeny)** - Privacy-first macOS MCP server that provides visual context for AI agents through window screenshots\n- **[ScriptFlow](https://github.com/yanmxa/scriptflow-mcp)** - Transform complex, repetitive AI interactions into persistent, executable scripts with comprehensive script management (add, edit, remove, list, search, execute) and multi-language support (Bash, Python, Node.js, TypeScript).\n- **[SearXNG](https://github.com/ihor-sokoliuk/mcp-searxng)** - A Model Context Protocol Server for [SearXNG](https://docs.searxng.org)\n- **[SearXNG](https://github.com/erhwenkuo/mcp-searxng)** - An MCP server provide web searching via [SearXNG](https://docs.searxng.org) & retrieve url as makrdown.\n- **[SearXNG Public](https://github.com/pwilkin/mcp-searxng-public)** - A Model Context Protocol Server for retrieving data from public [SearXNG](https://docs.searxng.org) instances, with fallback support\n- **[SEC EDGAR](https://github.com/stefanoamorelli/sec-edgar-mcp)** - (by Stefano Amorelli) A community Model Context Protocol Server to access financial filings and data through the U.S. Securities and Exchange Commission ([SEC](https://www.sec.gov/)) `Electronic Data Gathering, Analysis, and Retrieval` ([EDGAR](https://www.sec.gov/submit-filings/about-edgar)) database\n- **[SEO MCP](https://github.com/cnych/seo-mcp)** - A free SEO tool MCP (Model Control Protocol) service based on Ahrefs data. Includes features such as backlinks, keyword ideas, and more. by [claudemcp](https://www.claudemcp.com/servers/seo-mcp).\n- **[Serper](https://github.com/garylab/serper-mcp-server)** - An MCP server that performs Google searches using [Serper](https://serper.dev).\n- **[ServiceNow](https://github.com/osomai/servicenow-mcp)** - An MCP server to interact with a ServiceNow instance\n- **[ShaderToy](https://github.com/wilsonchenghy/ShaderToy-MCP)** - This MCP server lets LLMs to interact with the ShaderToy API, allowing LLMs to learn from compute shaders examples and enabling them to create complex GLSL shaders that they are previously not capable of.\n- **[ShareSeer](https://github.com/shareseer/shareseer-mcp-server)** - MCP to Access SEC filings, financials & insider trading data in real time using [ShareSeer](https://shareseer.com)\n- **[Shell](https://github.com/sonirico/mcp-shell)** - Give hands to AI. MCP server to run shell commands securely, auditably, and on demand\n- **[Shodan MCP](https://github.com/Hexix23/shodan-mcp)** - MCP server to interact with [Shodan](https://www.shodan.io/)\n- **[Shopify](https://github.com/GeLi2001/shopify-mcp)** - MCP to interact with Shopify API including order, product, customers and so on.\n- **[Shopify Storefront](https://github.com/QuentinCody/shopify-storefront-mcp-server)** - Unofficial MCP server that allows AI agents to discover Shopify storefronts and interact with them to fetch products, collections, and other store data through the Storefront API.\n- **[Simple Loki MCP](https://github.com/ghrud92/simple-loki-mcp)** - A simple MCP server to query Loki logs using logcli.\n- **[Siri Shortcuts](https://github.com/dvcrn/mcp-server-siri-shortcuts)** - MCP to interact with Siri Shortcuts on macOS. Exposes all Shortcuts as MCP tools.\n- **[Skyvern](https://github.com/Skyvern-AI/skyvern/tree/main/integrations/mcp)** - MCP to let Claude / Windsurf / Cursor / your LLM control the browser\n- **[Slack](https://github.com/korotovsky/slack-mcp-server)** - The most powerful MCP server for Slack Workspaces. This integration supports both Stdio and SSE transports, proxy settings and does not require any permissions or bots being created or approved by Workspace admins 😏.\n- **[Slack](https://github.com/zencoderai/slack-mcp-server)** - Slack MCP server which supports both stdio and Streamable HTTP transports. Extended from the original Anthropic's implementation which is now [archived](https://github.com/modelcontextprotocol/servers-archived/tree/main/src/slack)\n- **[Slidespeak](https://github.com/SlideSpeak/slidespeak-mcp)** - Create PowerPoint presentations using the [Slidespeak](https://slidespeak.com/) API.\n- **[Smartlead](https://github.com/jean-technologies/smartlead-mcp-server-local)** - MCP to connect to Smartlead. Additional, tooling, functionality, and connection to workflow automation platforms also available.\n- **[Snowflake](https://github.com/Snowflake-Labs/mcp)** - Open-source MCP server for Snowflake from official Snowflake-Labs supports prompting Cortex Agents, querying structured & unstructured data, object management, SQL execution, semantic view querying, and more. RBAC, fine-grained CRUD controls, and all authentication methods supported.\n- **[Snowflake](https://github.com/isaacwasserman/mcp-snowflake-server)** - This MCP server enables LLMs to interact with Snowflake databases, allowing for secure and controlled data operations.\n- **[Snowflake Cortex MCP Server](https://github.com/thisisbhanuj/Snowflake-Cortex-MCP-Server)** -This Snowflake MCP server provides tooling for Snowflake Cortex AI features, bringing these capabilities to the MCP ecosystem. When connected to an MCP Client (e.g. Claude for Desktop, fast-agent, Agentic Orchestration Framework), users can leverage these Cortex AI features.\n- **[SoccerDataAPI](https://github.com/yeonupark/mcp-soccer-data)** - This MCP server provides real-time football match data based on the SoccerDataAPI.\n- **[Solana Agent Kit](https://github.com/sendaifun/solana-agent-kit/tree/main/examples/agent-kit-mcp-server)** - This MCP server enables LLMs to interact with the Solana blockchain with help of Solana Agent Kit by SendAI, allowing for 40+ protocol actions and growing\n- **[Solr MCP](https://github.com/mjochum64/mcp-solr-search)** - This MCP server offers a basic functionality to perform a search on Solr servers.\n- **[Solver](https://github.com/szeider/mcp-solver)** - Solves constraint satisfaction and optimization problems .\n- **[Solvitor](https://github.com/Adeptus-Innovatio/solvitor-mcp)** – Solvitor MCP server provides tools to access reverse engineering tools that help developers extract IDL files from closed - source Solana smart contracts and decompile them.\n- **[Sourcerer](https://github.com/st3v3nmw/sourcerer-mcp)** - MCP for semantic code search & navigation that reduces token waste.\n- **[Specbridge](https://github.com/TBosak/specbridge)** - Easily turn your OpenAPI specs into MCP Tools.\n- **[Splunk](https://github.com/jkosik/mcp-server-splunk)** - Golang MCP server for Splunk (lists saved searches, alerts, indexes, macros...). Supports SSE and STDIO.\n- **[Spotify](https://github.com/varunneal/spotify-mcp)** - This MCP allows an LLM to play and use Spotify.\n- **[Spring Initializr](https://github.com/hpalma/springinitializr-mcp)** - This MCP allows an LLM to create Spring Boot projects with custom configurations. Instead of manually visiting start.spring.io, you can now ask your AI assistant to generate projects with specific dependencies, Java versions, and project structures.\n- **[Squad AI](https://github.com/the-basilisk-ai/squad-mcp)** – Product‑discovery and strategy platform integration. Create, query and update opportunities, solutions, outcomes, requirements and feedback from any MCP‑aware LLM.\n- **[SSH](https://github.com/AiondaDotCom/mcp-ssh)** - Agent for managing and controlling SSH connections.\n- **[SSH](https://github.com/classfang/ssh-mcp-server)** - An MCP server that can execute SSH commands remotely, upload files, download files, and so on.\n- **[SSH MCP Server](https://github.com/sinjab/mcp_ssh)** - A production-ready Model Context Protocol server for SSH automation with background execution, file transfers, and comprehensive timeout protection. Features structured output, progress tracking, and enterprise-grade testing (87% coverage).\n- **[sslmon](https://github.com/firesh/sslmon-mcp)** - Domain/HTTPS/SSL domain registration information and SSL certificate monitoring capabilities. Query domain registration and expiration information, and SSL certificate information and validity status for any domain.\n- **[Standard Korean Dictionary](https://github.com/privetin/stdict)** - Search the dictionary using API\n- **[Star Wars](https://github.com/johnpapa/mcp-starwars)** -MCP Server for the SWAPI Star Wars API. The main goal of the project is to show how an MCP server can be used to interact with APIs.\n- **[Starknet MCP Server](https://github.com/mcpdotdirect/starknet-mcp-server)** - A comprehensive MCP server for interacting with the Starknet blockchain, providing tools for querying blockchain data, resolving StarknetIDs, and performing token transfers.\n- **[Starwind UI](https://github.com/Boston343/starwind-ui-mcp/)** - This MCP provides relevant commands, documentation, and other information to allow LLMs to take full advantage of Starwind UI's open source Astro components.\n- **[Stellar](https://github.com/syronlabs/stellar-mcp/)** - This MCP server enables LLMs to interact with the Stellar blockchain to create accounts, check address balances, analyze transactions, view transaction history, mint new assets, interact with smart contracts and much more.\n- **[Stitch AI](https://github.com/StitchAI/stitch-ai-mcp/)** - Knowledge management system for AI agents with memory space creation and retrieval capabilities.\n- **[Stockfish](https://github.com/sonirico/mcp-stockfish)** - MCP server connecting AI systems to Stockfish chess engine\n- **[Storybook](https://github.com/stefanoamorelli/storybook-mcp-server)** (by Stefano Amorelli) - Interact with Storybook component libraries, enabling component discovery, story management, prop inspection, and visual testing across different viewports.\n- **[Strava](https://github.com/r-huijts/strava-mcp)** - Connect to the Strava API to access activity data, athlete profiles, segments, and routes, enabling fitness tracking and analysis with Claude.\n- **[Strava API](https://github.com/tomekkorbak/strava-mcp-server)** - MCP server for Strava API to retrieve one's activities\n- **[Stripe](https://github.com/atharvagupta2003/mcp-stripe)** - This MCP allows integration with Stripe for handling payments, customers, and refunds.\n- **[Substack/Medium](https://github.com/jonathan-politzki/mcp-writer-substack)** - Connect Claude to your Substack/Medium writing, enabling semantic search and analysis of your published content.\n- **[System Health](https://github.com/thanhtung0201/mcp-remote-system-health)** - The MCP (Multi-Channel Protocol) System Health Monitoring is a robust, real-time monitoring solution designed to provide comprehensive health metrics and alerts for remote Linux servers.\n- **[SystemSage](https://github.com/Tarusharma1/SystemSage)** - A powerful, cross-platform system management and monitoring tool for Windows, Linux, and macOS.\n- **[Talk To Figma](https://github.com/sonnylazuardi/cursor-talk-to-figma-mcp)** - This MCP server enables LLMs to interact with Figma, allowing them to read and modify designs programmatically.\n- **[Talk To Figma via Claude](https://github.com/gaganmanku96/talk-with-figma-claude)** - TMCP server that provides seamless Figma integration specifically for Claude Desktop, enabling design creation, modification, and real-time collaboration through natural language commands.\n- **[TAM MCP Server](https://github.com/gvaibhav/TAM-MCP-Server)** - Market research and business intelligence with TAM/SAM calculations and integration across 8 economic data sources: Alpha Vantage, BLS, Census Bureau, FRED, IMF, Nasdaq Data Link, OECD, and World Bank.\n- **[Tasks](https://github.com/flesler/mcp-tasks)** - An efficient task manager. Designed to minimize tool confusion and maximize LLM budget efficiency while providing powerful search, filtering, and organization capabilities across multiple file formats (Markdown, JSON, YAML)\n- **[Tavily search](https://github.com/RamXX/mcp-tavily)** - An MCP server for Tavily's search & news API, with explicit site inclusions/exclusions\n- **[TcpSocketMCP](https://github.com/SpaceyKasey/TcpSocketMCP/)** - A Model Context Protocol (MCP) server that provides raw TCP socket access, enabling AI models to interact directly with network services using raw TCP Sockets. Supports multiple concurrent connections, buffering of response data and triggering automatic responses.\n- **[TeamRetro](https://github.com/adepanges/teamretro-mcp-server)** - This MCP server allows LLMs to interact with TeamRetro, allowing LLMs to manage user, team, team member, retrospective, health check, action, agreement and fetch the reports.\n- **[Telegram](https://github.com/chigwell/telegram-mcp)** - An MCP server that provides paginated chat reading, message retrieval, and message sending capabilities for Telegram through Telethon integration.\n- **[Telegram-Client](https://github.com/chaindead/telegram-mcp)** - A Telegram API bridge that manages user data, dialogs, messages, drafts, read status, and more for seamless interactions.\n- **[Telegram-mcp-server](https://github.com/DLHellMe/telegram-mcp-server)** - Access Telegram channels and groups directly in Claude. Features dual-mode operation with API access (100x faster) or web scraping, unlimited post retrieval, and search functionality.\n- **[Template MCP Server](https://github.com/mcpdotdirect/template-mcp-server)** - A CLI tool to create a new Model Context Protocol server project with TypeScript support, dual transport options, and an extensible structure\n- **[Tempo](https://github.com/scottlepp/tempo-mcp-server)** - An MCP server to query traces/spans from [Grafana Tempo](https://github.com/grafana/tempo).\n- **[Teradata](https://github.com/arturborycki/mcp-teradata)** - his MCP server enables LLMs to interact with Teradata databases. This MCP Server support tools and prompts for multi task data analytics\n- **[Terminal-Control](https://github.com/GongRzhe/terminal-controller-mcp)** - An MCP server that enables secure terminal command execution, directory navigation, and file system operations through a standardized interface.\n- **[Terraform-Cloud](https://github.com/severity1/terraform-cloud-mcp)** - An MCP server that integrates AI assistants with the Terraform Cloud API, allowing you to manage your infrastructure through natural conversation.\n- **[Tideways](https://github.com/abuhamza/tideways-mcp-server)** - A Model Context Protocol server that enables AI assistants to query Tideways performance monitoring data and provide conversational performance insights for PHP applications.\n- **[TFT-Match-Analyzer](https://github.com/GeLi2001/tft-mcp-server)** - MCP server for teamfight tactics match history & match details fetching, providing user the detailed context for every match.\n- **[Thales CDSP CAKM MCP Server](https://github.com/sanyambassi/thales-cdsp-cakm-mcp-server)** - An MCP server for the Thales CipherTrust Data Security Platform (CDSP) Cloud Key Management (CAKM) connector. This MCP server supports Ms SQL and Oracle databases.\n- **[Thales CDSP CRDP MCP Server](https://github.com/sanyambassi/thales-cdsp-crdp-mcp-server)** - A Model Context Protocol (MCP) server that allows interacting with the CipherTrust RestFul Data Protection (CRDP) data protection service.\n- **[Thales CipherTrust Manager MCP Server](https://github.com/sanyambassi/ciphertrust-manager-mcp-server)** - MCP server for Thales CipherTrust Manager integration, enabling secure key management and cryptographic operations.\n- **[thegraph-mcp](https://github.com/kukapay/thegraph-mcp)** - An MCP server that powers AI agents with indexed blockchain data from The Graph.\n- **[TheHive MCP Server](https://github.com/redwaysecurity/the-hive-mcp-server)** - An MCP server for [TheHive](https://strangebee.com/thehive/) Security Incident Response Platform.\n- **[Things3 MCP](https://github.com/urbanogardun/things3-mcp)** - Things3 task management integration for macOS with comprehensive TODO, project, and tag management.\n- **[Think MCP](https://github.com/Rai220/think-mcp)** - Enhances any agent's reasoning capabilities by integrating the think-tools, as described in [Anthropic's article](https://www.anthropic.com/engineering/claude-think-tool).\n- **[Think Node MCP](https://github.com/abhinav-mangla/think-tool-mcp)** - Enhances any agent's reasoning capabilities by integrating the think-tools, as described in [Anthropic's article](https://www.anthropic.com/engineering/claude-think-tool). (Works with Node)\n- **[Ticketmaster](https://github.com/delorenj/mcp-server-ticketmaster)** - Search for events, venues, and attractions through the Ticketmaster Discovery API\n- **[Ticketmaster MCP Server](https://github.com/mochow13/ticketmaster-mcp-server)** - A Model Context Protocol (MCP) server implemented in Streamable HTTP transport that allows AI models to interact with the Ticketmaster Discovery API, enabling searching events, venues, and attractions.\n- **[TickTick](https://github.com/alexarevalo9/ticktick-mcp-server)** - A Model Context Protocol (MCP) server designed to integrate with the TickTick task management platform, enabling intelligent context-aware task operations and automation.\n- **[TigerGraph](https://github.com/custom-discoveries/TigerGraph_MCP)** - A community built MCP server that interacts with TigerGraph Graph Database.\n- **[tip.md](https://github.com/tipdotmd#-mcp-server-for-ai-assistants)** - An MCP server that enables AI assistants to interact with tip.md's crypto tipping functionality, allowing agents or supporters to tip registered developers directly from AI chat interfaces.\n- **[TMD Earthquake](https://github.com/amornpan/tmd-earthquake-server-1.0)** - 🌍 Real-time earthquake monitoring from Thai Meteorological Department. Features magnitude filtering, location-based search (Thai/English), today's events tracking, dangerous earthquake alerts, and comprehensive statistics. Covers regional and global seismic activities.\n- **[TMDB](https://github.com/Laksh-star/mcp-server-tmdb)** - This MCP server integrates with The Movie Database (TMDB) API to provide movie information, search capabilities, and recommendations.\n- **[Todoist](https://github.com/abhiz123/todoist-mcp-server)** - Interact with Todoist to manage your tasks.\n- **[Todos](https://github.com/tomelliot/todos-mcp)** - A practical todo list manager to use with your favourite chatbot.\n- **[token-minter-mcp](https://github.com/kukapay/token-minter-mcp)** - An MCP server providing tools for AI agents to mint ERC-20 tokens across multiple blockchains.\n- **[token-revoke-mcp](https://github.com/kukapay/token-revoke-mcp)** - An MCP server for checking and revoking ERC-20 token allowances across multiple blockchains.\n- **[Ton Blockchain MCP](https://github.com/devonmojito/ton-blockchain-mcp)** - An MCP server for interacting with Ton Blockchain.\n- **[TouchDesigner](https://github.com/8beeeaaat/touchdesigner-mcp)** - An MCP server for TouchDesigner, enabling interaction with TouchDesigner projects, nodes, and parameters.\n- **[Transcribe](https://github.com/transcribe-app/mcp-transcribe)** - An MCP server provides fast and reliable transcriptions for audio/video files and voice memos. It allows LLMs to interact with the text content of audio/video file.\n- **[Travel Planner](https://github.com/GongRzhe/TRAVEL-PLANNER-MCP-Server)** - Travel planning and itinerary management server integrating with Google Maps API for location search, place details, and route calculations.\n- **[Trello MCP Server](https://github.com/lioarce01/trello-mcp-server)** - An MCP server that interact with user Trello boards, modifying them with prompting.\n- **[Trino](https://github.com/tuannvm/mcp-trino)** - A high-performance Model Context Protocol (MCP) server for Trino implemented in Go.\n- **[Tripadvisor](https://github.com/pab1it0/tripadvisor-mcp)** - An MCP server that enables LLMs to interact with Tripadvisor API, supporting location data, reviews, and photos through standardized MCP interfaces\n- **[Triplyfy MCP](https://github.com/helpful-AIs/triplyfy-mcp)** - An MCP server that lets LLMs plan and manage itineraries with interactive maps in Triplyfy; manage itineraries, places and notes, and search/save flights.\n- **[TrueNAS Core MCP](https://github.com/vespo92/TrueNasCoreMCP)** - An MCP server for interacting with TrueNAS Core.\n- **[TuriX Computer Automation MCP](https://github.com/TurixAI/TuriX-CUA/tree/mac_mcp)** - MCP server for helping automation control your computer complete your pre-setting task.\n- **[Tyk API Management](https://github.com/TykTechnologies/tyk-dashboard-mcp)** - Chat with all of your organization's managed APIs and perform other API lifecycle operations, managing tokens, users, analytics, and more.\n- **[Typesense](https://github.com/suhail-ak-s/mcp-typesense-server)** - A Model Context Protocol (MCP) server implementation that provides AI models with access to Typesense search capabilities. This server enables LLMs to discover, search, and analyze data stored in Typesense collections.\n- **[UniFi Dream Machine](https://github.com/sabler/mcp-unifi)** An MCP server that gets your network telemetry from the UniFi Site Manager and your local UniFi router.\n- **[UniProt](https://github.com/QuentinCody/uniprot-mcp-server)** - Unofficial MCP server for UniProt, providing access to protein sequence data, functional annotations, taxonomic information, and cross-references for proteomics and bioinformatics research.\n- **[uniswap-poolspy-mcp](https://github.com/kukapay/uniswap-poolspy-mcp)** - An MCP server that tracks newly created liquidity pools on Uniswap across nine blockchain networks.\n- **[uniswap-trader-mcp](https://github.com/kukapay/uniswap-trader-mcp)** -An MCP server for AI agents to automate token swaps on Uniswap DEX across multiple blockchains.\n- **[Unity Catalog](https://github.com/ognis1205/mcp-server-unitycatalog)** - An MCP server that enables LLMs to interact with Unity Catalog AI, supporting CRUD operations on Unity Catalog Functions and executing them as MCP tools.\n- **[Unity Integration (Advanced)](https://github.com/quazaai/UnityMCPIntegration)** - Advanced Unity3d Game Engine MCP which supports ,Execution of Any Editor Related Code Directly Inside of Unity, Fetch Logs, Get Editor State and Allow File Access of the Project making it much more useful in Script Editing or asset creation.\n- **[Unity3d Game Engine](https://github.com/CoderGamester/mcp-unity)** - An MCP server that enables LLMs to interact with Unity3d Game Engine, supporting access to a variety of the Unit's Editor engine tools (e.g. Console Logs, Test Runner logs, Editor functions, hierarchy state, etc) and executing them as MCP tools or gather them as resources.\n- **[Universal MCP Servers](https://github.com/universal-mcp)** - A collection of MCP servers created using the [AgentR Universal MCP SDK](https://github.com/universal-mcp/universal-mcp).\n- **[Unleash Integration (Feature Toggle)](https://github.com/cuongtl1992/unleash-mcp)** - A Model Context Protocol (MCP) server implementation that integrates with Unleash Feature Toggle system. Provide a bridge between LLM applications and Unleash feature flag system\n- **[Upbit MCP Server](https://github.com/solangii/upbit-mcp-server)** – An MCP server that enables real - time access to cryptocurrency prices, market summaries, and asset listings from the Upbit exchange.\n- **[use_aws_mcp](https://github.com/runjivu/use_aws_mcp)** - amazon-q-cli's use_aws tool extracted into independent mcp, for general aws api usage.\n- **[User Feedback](https://github.com/mrexodia/user-feedback-mcp)** - Simple MCP Server to enable a human-in-the-loop workflow in tools like Cline and Cursor.\n- **[USPTO](https://github.com/riemannzeta/patent_mcp_server)** - MCP server for accessing United States Patent & Trademark Office data through its Open Data Protocol (ODP) API.\n- **[Vectara](https://github.com/vectara/vectara-mcp)** - Query Vectara's trusted RAG-as-a-service platform.\n- **[Vega-Lite](https://github.com/isaacwasserman/mcp-vegalite-server)** - Generate visualizations from fetched data using the VegaLite format and renderer.\n- **[Vertica](https://github.com/nolleh/mcp-vertica)** - Vertica database integration in Python with configurable access controls and schema inspection\n- **[Vibe Check](https://github.com/PV-Bhat/vibe-check-mcp-server)** - An MCP server leveraging an external oversight layer to \"vibe check\" agents, and also self-improve accuracy & user alignment over time. Prevents scope creep, code bloat, misalignment, misinterpretation, tunnel vision, and overcomplication.\n- **[Video Editor](https://github.com/burningion/video-editing-mcp)** - A Model Context Protocol Server to add, edit, and search videos with [Video Jungle](https://www.video-jungle.com/).\n- **[Video Still Capture](https://github.com/13rac1/videocapture-mcp)** - 📷 Capture video stills from an OpenCV-compatible webcam or other video source.\n- **[Virtual location (Google Street View,etc.)](https://github.com/mfukushim/map-traveler-mcp)** - Integrates Google Map, Google Street View, PixAI, Stability.ai, ComfyUI API and Bluesky to provide a virtual location simulation in LLM (written in Effect.ts)\n- **[VMware Fusion](https://github.com/yeahdongcn/vmware-fusion-mcp-server)** - Manage VMware Fusion virtual machines via the Fusion REST API.\n- **[VoiceMode](https://github.com/mbailey/voicemode)** - Enable voice conversations with Claude using any OpenAI-compatible STT/TTS service [getvoicemode.com](https://getvoicemode.com/)\n- **[Voice Status Report](https://github.com/tomekkorbak/voice-status-report-mcp-server)** - An MCP server that provides voice status updates using OpenAI's text-to-speech API, to be used with Cursor or Claude Code.\n- **[VolcEngine TOS](https://github.com/dinghuazhou/sample-mcp-server-tos)** - A sample MCP server for VolcEngine TOS that flexibly get objects from TOS.\n- **[Voyp](https://github.com/paulotaylor/voyp-mcp)** - VOYP MCP server for making calls using Artificial Intelligence.\n- **[vulnicheck](https://github.com/andrasfe/vulnicheck)** - Real-time Python package vulnerability scanner that checks dependencies against OSV and NVD databases, providing comprehensive security analysis with CVE details, lock file support, and actionable upgrade recommendations.\n- **[Wanaku MCP Router](https://github.com/wanaku-ai/wanaku/)** - The Wanaku MCP Router is a SSE-based MCP server that provides an extensible routing engine that allows integrating your enterprise systems with AI agents.\n- **[weather-mcp-server](https://github.com/devilcoder01/weather-mcp-server)** - Get real-time weather data for any location using weatherapi.\n- **[Web Search MCP](https://github.com/mrkrsl/web-search-mcp)** - A server that provides full web search, summaries and page extration for use with Local LLMs.\n- **[Webex](https://github.com/Kashyap-AI-ML-Solutions/webex-messaging-mcp-server)** - A Model Context Protocol (MCP) server that provides AI assistants with comprehensive access to Cisco Webex messaging capabilities.\n- **[Webflow](https://github.com/kapilduraphe/webflow-mcp-server)** - Interact with the Webflow APIs\n- **[webhook-mcp](https://github.com/noobnooc/webhook-mcp)** (by Nooc) - A Model Context Protocol (MCP) server that sends webhook notifications when called.\n- **[whale-tracker-mcp](https://github.com/kukapay/whale-tracker-mcp)**  -  A mcp server for tracking cryptocurrency whale transactions.\n- **[WhatsApp MCP Server](https://github.com/lharries/whatsapp-mcp)** - MCP server for your personal WhatsApp handling individuals, groups, searching and sending.\n- **[Whois MCP](https://github.com/bharathvaj-ganesan/whois-mcp)** - MCP server that performs whois lookup against domain, IP, ASN and TLD.\n- **[Wikidata MCP](https://github.com/zzaebok/mcp-wikidata)** - Wikidata MCP server that interact with Wikidata, by searching identifiers, extracting metadata, and executing sparql query.\n- **[Wikidata SPARQL](https://github.com/QuentinCody/wikidata-sparql-mcp-server)** - Unofficial REMOTE MCP server for Wikidata's SPARQL endpoint, providing access to structured knowledge data, entity relationships, and semantic queries for research and data analysis.\n- **[Wikifunctions](https://github.com/Fredibau/wikifunctions-mcp-fredibau)** - Allowing AI models to discover and execute functions from the WikiFunctions library.\n- **[Wikipedia MCP](https://github.com/Rudra-ravi/wikipedia-mcp)** - Access and search Wikipedia articles via MCP for AI-powered information retrieval.\n- **[WildFly MCP](https://github.com/wildfly-extras/wildfly-mcp)** - WildFly MCP server that enables LLM to interact with running WildFly servers (retrieve metrics, logs, invoke operations, ...).\n- **[Windows CLI](https://github.com/SimonB97/win-cli-mcp-server)** - MCP server for secure command-line interactions on Windows systems, enabling controlled access to PowerShell, CMD, and Git Bash shells.\n- **[Windsor](https://github.com/windsor-ai/windsor_mcp)** - Windsor MCP (Model Context Protocol) enables your LLM to query, explore, and analyze your full-stack business data integrated into Windsor.ai with zero SQL writing or custom scripting.\n- **[Wordle MCP](https://github.com/cr2007/mcp-wordle-python)** - MCP Server that gets the Wordle Solution for a particular date.\n- **[WordPress MCP](https://github.com/Automattic/wordpress-mcp)** - Make your WordPress site into a simple MCP server, exposing functionality to LLMs and AI agents.\n- **[Workflowy](https://github.com/danield137/mcp-workflowy)** - A server that interacts with [workflowy](https://workflowy.com/).\n- **[World Bank data API](https://github.com/anshumax/world_bank_mcp_server)** - A server that fetches data indicators available with the World Bank as part of their data API\n- **[Wren Engine](https://github.com/Canner/wren-engine)** - The Semantic Engine for Model Context Protocol(MCP) Clients and AI Agents\n- **[X (Twitter)](https://github.com/EnesCinr/twitter-mcp)** (by EnesCinr) - Interact with twitter API. Post tweets and search for tweets by query.\n- **[X (Twitter)](https://github.com/vidhupv/x-mcp)** (by vidhupv) - Create, manage and publish X/Twitter posts directly through Claude chat.\n- **[Xcode](https://github.com/r-huijts/xcode-mcp-server)** - MCP server that brings AI to your Xcode projects, enabling intelligent code assistance, file operations, project management, and automated development tasks.\n- **[Xcode-mcp-server](https://github.com/drewster99/xcode-mcp-server)** (by drewster99) - Best Xcode integration - ClaudeCode and Cursor can build your project *with* Xcode and see the same errors you do. Fast easy setup.\n- **[xcodebuild](https://github.com/ShenghaiWang/xcodebuild)**  - 🍎 Build iOS Xcode workspace/project and feed back errors to llm.\n- **[Xero-mcp-server](https://github.com/john-zhang-dev/xero-mcp)** - Enabling clients to interact with Xero system for streamlined accounting, invoicing, and business operations.\n- **[XiYan](https://github.com/XGenerationLab/xiyan_mcp_server)** - 🗄️ An MCP server that supports fetching data from a database using natural language queries, powered by XiyanSQL as the text-to-SQL LLM.\n- **[XMind](https://github.com/apeyroux/mcp-xmind)** - Read and search through your XMind directory containing XMind files.\n- **[Yahoo Finance](https://github.com/AgentX-ai/yahoo-finance-server)** - 📈 Lets your AI interact with Yahoo Finance to get comprehensive stock market data, news, financials, and more. Proxy supported.\n- **[yfinance](https://github.com/Adity-star/mcp-yfinance-server)** -💹The MCP YFinance Stock Server provides real-time and historical stock data in a standard format, powering dashboards, AI agents,and research tools with seamless financial insights.\n- **[YNAB](https://github.com/ChuckBryan/ynabmcpserver)** - A Model Context Protocol (MCP) server for integrating with YNAB (You Need A Budget), allowing AI assistants to securely access and analyze your financial data.\n- **[YouTrack](https://github.com/tonyzorin/youtrack-mcp)** - A Model Context Protocol (MCP) server implementation for JetBrains YouTrack, allowing AI assistants to interact with YouTrack issue tracking system.\n- **[YouTube](https://github.com/Klavis-AI/klavis/tree/main/mcp_servers/youtube)** - Extract Youtube video information (with proxies support).\n- **[YouTube](https://github.com/ZubeidHendricks/youtube-mcp-server)** - Comprehensive YouTube API integration for video management, Shorts creation, and analytics.\n- **[YouTube DLP](https://github.com/AgentX-ai/youtube-dlp-server)** - Retrieve video information, subtitles, and top comments with proxies.\n- **[YouTube MCP](https://github.com/aardeshir/youtube-mcp)** - Create playlists from song lists with OAuth2. Search videos, manage playlists, let AI curate your YouTube collections.\n- **[Youtube Uploader MCP](https://github.com/anwerj/youtube-uploader-mcp)** - AI‑powered YouTube uploader—no CLI, no YouTube Studio.\n- **[YouTube Video Summarizer](https://github.com/nabid-pf/youtube-video-summarizer-mcp)** - Summarize lengthy youtube videos.\n- **[yutu](https://github.com/eat-pray-ai/yutu)** - A fully functional MCP server and CLI for YouTube to automate YouTube operation.\n- **[ZapCap](https://github.com/bogdan01m/zapcap-mcp-server)** - MCP server for ZapCap API providing video caption and B-roll generation via natural language\n- **[Zettelkasten](https://github.com/joshylchen/zettelkasten)**- Comprehensive AI-powered knowledge management system implementing the Zettelkasten method. Features atomic note creation, full-text search, AI-powered CEQRC workflows (Capture→Explain→Question→Refine→Connect), intelligent link discovery, and multi-interface access (CLI, API, Web UI, MCP). Perfect for researchers, students, and knowledge workers.\n- **[ZincBind](https://github.com/QuentinCody/zincbind-mcp-server)** - Unofficial MCP server for ZincBind, providing access to a comprehensive database of zinc binding sites in proteins, structural coordination data, and metalloproteomics research information.\n- **[Zoom](https://github.com/Prathamesh0901/zoom-mcp-server/tree/main)** - Create, update, read and delete your zoom meetings.\n## 📚 Frameworks\n\nThese are high-level frameworks that make it easier to build MCP servers or clients.\n\n### For servers\n\n* **[Anubis MCP](https://github.com/zoedsoupe/anubis-mcp)** (Elixir) - A high-performance and high-level Model Context Protocol (MCP) implementation in Elixir. Think like \"Live View\" for MCP.\n* **[ModelFetch](https://github.com/phuctm97/modelfetch/)** (TypeScript) - Runtime-agnostic SDK to create and deploy MCP servers anywhere TypeScript/JavaScript runs\n* **[EasyMCP](https://github.com/zcaceres/easy-mcp/)** (TypeScript)\n* **[FastAPI to MCP auto generator](https://github.com/tadata-org/fastapi_mcp)** – A zero-configuration tool for automatically exposing FastAPI endpoints as MCP tools by **[Tadata](https://tadata.com/)**\n* **[FastMCP](https://github.com/punkpeye/fastmcp)** (TypeScript)\n* **[Foobara MCP Connector](https://github.com/foobara/mcp-connector)** - Easily expose Foobara commands written in Ruby as tools via MCP\n* **[Foxy Contexts](https://github.com/strowk/foxy-contexts)** – A library to build MCP servers in Golang by **[strowk](https://github.com/strowk)**\n* **[Higress MCP Server Hosting](https://github.com/alibaba/higress/tree/main/plugins/wasm-go/mcp-servers)** - A solution for hosting MCP Servers by extending the API Gateway (based on Envoy) with wasm plugins.\n* **[MCP Declarative Java SDK](https://github.com/codeboyzhou/mcp-declarative-java-sdk)** Annotation-driven MCP servers development with Java, no Spring Framework Required, minimize dependencies as much as possible.\n* **[MCP-Framework](https://mcp-framework.com)** Build MCP servers with elegance and speed in TypeScript. Comes with a CLI to create your project with `mcp create app`. Get started with your first server in under 5 minutes by **[Alex Andru](https://github.com/QuantGeekDev)**\n* **[MCP Plexus](https://github.com/Super-I-Tech/mcp_plexus)**: A secure, **multi-tenant** and Multi-user MCP python server framework built to integrate easily with external services via OAuth 2.1, offering scalable and robust solutions for managing complex AI applications.\n* **[mcp_sse (Elixir)](https://github.com/kEND/mcp_sse)** An SSE implementation in Elixir for rapidly creating MCP servers.\n* **[mxcp](https://github.com/raw-labs/mxcp)** (Python) - Open-source framework for building enterprise-grade MCP servers using just YAML, SQL, and Python, with built-in auth, monitoring, ETL and policy enforcement.\n* **[Next.js MCP Server Template](https://github.com/vercel-labs/mcp-for-next.js)** (Typescript) - A starter Next.js project that uses the MCP Adapter to allow MCP clients to connect and access resources.\n* **[PayMCP](https://github.com/blustAI/paymcp)** (Python & TypeScript) - Lightweight payments layer for MCP servers: turn tools into paid endpoints with a two-line decorator. [PyPI](https://pypi.org/project/paymcp/) · [npm](https://www.npmjs.com/package/paymcp) · [TS repo](https://github.com/blustAI/paymcp-ts)\n* **[Perl SDK](https://github.com/mojolicious/mojo-mcp)** - An SDK for building MCP servers and clients with the Perl programming language.\n* **[Quarkus MCP Server SDK](https://github.com/quarkiverse/quarkus-mcp-server)** (Java)\n- **[R mcptools](https://github.com/posit-dev/mcptools)** - An R SDK for creating R-based MCP servers and retrieving functionality from third-party MCP servers as R functions.\n* **[SAP ABAP MCP Server SDK](https://github.com/abap-ai/mcp)** - Build SAP ABAP based MCP servers. ABAP 7.52 based with 7.02 downport; runs on R/3 & S/4HANA on-premises, currently not cloud-ready.\n* **[Spring AI MCP Server](https://docs.spring.io/spring-ai/reference/api/mcp/mcp-server-boot-starter-docs.html)** - Provides auto-configuration for setting up an MCP server in Spring Boot applications.\n* **[Template MCP Server](https://github.com/mcpdotdirect/template-mcp-server)** - A CLI tool to create a new Model Context Protocol server project with TypeScript support, dual transport options, and an extensible structure\n* **[AgentR Universal MCP SDK](https://github.com/universal-mcp/universal-mcp)** - A python SDK to build MCP Servers with inbuilt credential management by **[Agentr](https://agentr.dev/home)**\n* **[Vercel MCP Adapter](https://github.com/vercel/mcp-adapter)** (TypeScript) - A simple package to start serving an MCP server on most major JS meta-frameworks including Next, Nuxt, Svelte, and more.\n* **[PHP MCP Server](https://github.com/php-mcp/server)** (PHP) - Core PHP implementation for the Model Context Protocol (MCP) server\n\n### For clients\n\n* **[codemirror-mcp](https://github.com/marimo-team/codemirror-mcp)** - CodeMirror extension that implements the Model Context Protocol (MCP) for resource mentions and prompt commands\n* **[llm-analysis-assistant](https://github.com/xuzexin-hz/llm-analysis-assistant)** <img height=\"12\" width=\"12\" src=\"https://raw.githubusercontent.com/xuzexin-hz/llm-analysis-assistant/refs/heads/main/src/llm_analysis_assistant/pages/html/imgs/favicon.ico\" alt=\"Langfuse Logo\" /> - A very streamlined mcp client that supports calling and monitoring stdio/sse/streamableHttp, and can also view request responses through the /logs page. It also supports monitoring and simulation of ollama/openai interface.\n* **[MCP-Agent](https://github.com/lastmile-ai/mcp-agent)** - A simple, composable framework to build agents using Model Context Protocol by **[LastMile AI](https://www.lastmileai.dev)**\n* **[Spring AI MCP Client](https://docs.spring.io/spring-ai/reference/api/mcp/mcp-client-boot-starter-docs.html)** - Provides auto-configuration for MCP client functionality in Spring Boot applications.\n* **[MCP CLI Client](https://github.com/vincent-pli/mcp-cli-host)** - A CLI host application that enables Large Language Models (LLMs) to interact with external tools through the Model Context Protocol (MCP).\n* **[OpenMCP Client](https://github.com/LSTM-Kirigaya/openmcp-client/)** - An all-in-one vscode/trae/cursor plugin for MCP server debugging. [Document](https://kirigaya.cn/openmcp/) & [OpenMCP SDK](https://kirigaya.cn/openmcp/sdk-tutorial/).\n* **[PHP MCP Client](https://github.com/php-mcp/client)** - Core PHP implementation for the Model Context Protocol (MCP) Client\n\n\n## 📚 Resources\n\nAdditional resources on MCP.\n\n- **[A2A-MCP Java Bridge](https://github.com/vishalmysore/a2ajava)** - A2AJava brings powerful A2A-MCP integration directly into your Java applications. It enables developers to annotate standard Java methods and instantly expose them as MCP Server, A2A-discoverable actions — with no boilerplate or service registration overhead.\n- **[AiMCP](https://www.aimcp.info)** - A collection of MCP clients&servers to find the right mcp tools by **[Hekmon](https://github.com/hekmon8)**\n- **[Awesome Crypto MCP Servers by badkk](https://github.com/badkk/awesome-crypto-mcp-servers)** - A curated list of MCP servers by **[Luke Fan](https://github.com/badkk)**\n- **[Awesome MCP Servers by appcypher](https://github.com/appcypher/awesome-mcp-servers)** - A curated list of MCP servers by **[Stephen Akinyemi](https://github.com/appcypher)**\n- **[Awesome MCP Servers by punkpeye](https://github.com/punkpeye/awesome-mcp-servers)** (**[website](https://glama.ai/mcp/servers)**) - A curated list of MCP servers by **[Frank Fiegel](https://github.com/punkpeye)**\n- **[Awesome MCP Servers by wong2](https://github.com/wong2/awesome-mcp-servers)** (**[website](https://mcpservers.org)**) - A curated list of MCP servers by **[wong2](https://github.com/wong2)**\n- **[Awesome Remote MCP Servers by JAW9C](https://github.com/jaw9c/awesome-remote-mcp-servers)** - A curated list of **remote** MCP servers, including their authentication support by **[JAW9C](https://github.com/jaw9c)**\n- **[Discord Server](https://glama.ai/mcp/discord)** – A community discord server dedicated to MCP by **[Frank Fiegel](https://github.com/punkpeye)**\n- **[Discord Server (ModelContextProtocol)](https://discord.gg/jHEGxQu2a5)** – Connect with developers, share insights, and collaborate on projects in an active Discord community dedicated to the Model Context Protocol by **[Alex Andru](https://github.com/QuantGeekDev)**\n- <img height=\"12\" width=\"12\" src=\"https://raw.githubusercontent.com/klavis-ai/klavis/main/static/klavis-ai.png\" alt=\"Klavis Logo\" /> **[Klavis AI](https://www.klavis.ai)** - Open Source MCP Infra. Hosted MCP servers and MCP clients on Slack and Discord.\n- **[MCP Badges](https://github.com/mcpx-dev/mcp-badges)** – Quickly highlight your MCP project with clear, eye-catching badges, by **[Ironben](https://github.com/nanbingxyz)**\n- **[MCPRepository.com](https://mcprepository.com/)** - A repository that indexes and organizes all MCP servers for easy discovery.\n- **[mcp-cli](https://github.com/wong2/mcp-cli)** - A CLI inspector for the Model Context Protocol by **[wong2](https://github.com/wong2)**\n- **[mcp-dockmaster](https://mcp-dockmaster.com)** - An Open-Sourced UI to install and manage MCP servers for Windows, Linux and macOS.\n- **[mcp-get](https://mcp-get.com)** - Command line tool for installing and managing MCP servers by **[Michael Latman](https://github.com/michaellatman)**\n- **[mcp-guardian](https://github.com/eqtylab/mcp-guardian)** - GUI application + tools for proxying / managing control of MCP servers by **[EQTY Lab](https://eqtylab.io)**\n- **[MCP Linker](https://github.com/milisp/mcp-linker)** - A cross-platform Tauri GUI tool for one-click setup and management of MCP servers, supporting Claude Desktop, Cursor, Windsurf, VS Code, Cline, and Neovim.\n- **[mcp-manager](https://github.com/zueai/mcp-manager)** - Simple Web UI to install and manage MCP servers for Claude Desktop by **[Zue](https://github.com/zueai)**\n- **[MCP Marketplace Web Plugin](https://github.com/AI-Agent-Hub/mcp-marketplace)** MCP Marketplace is a small Web UX plugin to integrate with AI applications, Support various MCP Server API Endpoint (e.g pulsemcp.com/deepnlp.org and more). Allowing user to browse, paginate and select various MCP servers by different categories. [Pypi](https://pypi.org/project/mcp-marketplace) | [Maintainer](https://github.com/AI-Agent-Hub) | [Website](http://www.deepnlp.org/store/ai-agent/mcp-server)\n- **[mcp.natoma.ai](https://mcp.natoma.ai)** – A Hosted MCP Platform to discover, install, manage and deploy MCP servers by **[Natoma Labs](https://www.natoma.ai)**\n- **[mcp.run](https://mcp.run)** - A hosted registry and control plane to install & run secure + portable MCP Servers.\n- **[MCPHub](https://www.mcphub.com)** - Website to list high quality MCP servers and reviews by real users. Also provide online chatbot for popular LLM models with MCP server support.\n- **[MCP Router](https://mcp-router.net)** – Free Windows and macOS app that simplifies MCP management while providing seamless app authentication and powerful log visualization by **[MCP Router](https://github.com/mcp-router/mcp-router)**\n- **[MCP Servers Hub](https://github.com/apappascs/mcp-servers-hub)** (**[website](https://mcp-servers-hub-website.pages.dev/)**) - A curated list of MCP servers by **[apappascs](https://github.com/apappascs)**\n- **[MCPServers.com](https://mcpservers.com)** - A growing directory of high-quality MCP servers with clear setup guides for a variety of MCP clients. Built by the team behind the **[Highlight MCP client](https://highlightai.com/)**\n- **[MCP Servers Rating and User Reviews](http://www.deepnlp.org/store/ai-agent/mcp-server)** - Website to rate MCP servers, write authentic user reviews, and [search engine for agent & mcp](http://www.deepnlp.org/search/agent)\n- **[MCP Sky](https://bsky.app/profile/brianell.in/feed/mcp)** - Bluesky feed for MCP related news and discussion by **[@brianell.in](https://bsky.app/profile/brianell.in)**\n- **[MCP X Community](https://x.com/i/communities/1861891349609603310)** – A X community for MCP by **[Xiaoyi](https://x.com/chxy)**\n- **[MCPHub](https://github.com/Jeamee/MCPHub-Desktop)** – An Open Source macOS & Windows GUI Desktop app for discovering, installing and managing MCP servers by **[Jeamee](https://github.com/jeamee)**\n- **[mcpm](https://github.com/pathintegral-institute/mcpm.sh)** ([website](https://mcpm.sh)) - MCP Manager (MCPM) is a Homebrew-like service for managing Model Context Protocol (MCP) servers across clients by **[Pathintegral](https://github.com/pathintegral-institute)**\n- **[MCPVerse](https://mcpverse.dev)** - A portal for creating & hosting authenticated MCP servers and connecting to them securely.\n- **[MCP Servers Search](https://github.com/atonomus/mcp-servers-search)** - An MCP server that provides tools for querying and discovering available MCP servers from this list.\n- **[Search MCP Server](https://github.com/krzysztofkucmierz/search-mcp-server)** - Recommends the most relevant MCP servers based on the client's query by searching this README file.\n- **[MCPWatch](https://github.com/kapilduraphe/mcp-watch)** - A comprehensive security scanner for Model Context Protocol (MCP) servers that detects vulnerabilities and security issues in your MCP server implementations.\n- <img height=\"12\" width=\"12\" src=\"https://mkinf.io/favicon-lilac.png\" alt=\"mkinf Logo\" /> **[mkinf](https://mkinf.io)** - An Open Source registry of hosted MCP Servers to accelerate AI agent workflows.\n- **[Open-Sourced MCP Servers Directory](https://github.com/chatmcp/mcp-directory)** - A curated list of MCP servers by **[mcpso](https://mcp.so)**\n- <img height=\"12\" width=\"12\" src=\"https://opentools.com/favicon.ico\" alt=\"OpenTools Logo\" /> **[OpenTools](https://opentools.com)** - An open registry for finding, installing, and building with MCP servers by **[opentoolsteam](https://github.com/opentoolsteam)**\n- **[PulseMCP](https://www.pulsemcp.com)** ([API](https://www.pulsemcp.com/api)) - Community hub & weekly newsletter for discovering MCP servers, clients, articles, and news by **[Tadas Antanavicius](https://github.com/tadasant)**, **[Mike Coughlin](https://github.com/macoughl)**, and **[Ravina Patel](https://github.com/ravinahp)**\n- **[r/mcp](https://www.reddit.com/r/mcp)** – A Reddit community dedicated to MCP by **[Frank Fiegel](https://github.com/punkpeye)**\n- **[r/modelcontextprotocol](https://www.reddit.com/r/modelcontextprotocol)** – A Model Context Protocol community Reddit page - discuss ideas, get answers to your questions, network with like-minded people, and showcase your projects! by **[Alex Andru](https://github.com/QuantGeekDev)**\n- **[MCP.ing](https://mcp.ing/)** - A list of MCP services for discovering MCP servers in the community and providing a convenient search function for MCP services by **[iiiusky](https://github.com/iiiusky)**\n- **[MCP Hunt](https://mcp-hunt.com)** - Realtime platform for discovering trending MCP servers with momentum tracking, upvoting, and community discussions - like Product Hunt meets Reddit for MCP\n- **[Smithery](https://smithery.ai/)** - A registry of MCP servers to find the right tools for your LLM agents by **[Henry Mao](https://github.com/calclavia)**\n- **[Toolbase](https://gettoolbase.ai)** - Desktop application that manages tools and MCP servers with just a few clicks - no coding required by **[gching](https://github.com/gching)**\n- **[ToolHive](https://github.com/StacklokLabs/toolhive)** - A lightweight utility designed to simplify the deployment and management of MCP servers, ensuring ease of use, consistency, and security through containerization by **[StacklokLabs](https://github.com/StacklokLabs)**\n- **[NetMind](https://www.netmind.ai/AIServices)** - Access powerful AI services via simple APIs or MCP servers to supercharge your productivity.\n\n## 🚀 Getting Started\n\n### Using MCP Servers in this Repository\nTypeScript-based servers in this repository can be used directly with `npx`.\n\nFor example, this will start the [Memory](src/memory) server:\n```sh\nnpx -y @modelcontextprotocol/server-memory\n```\n\nPython-based servers in this repository can be used directly with [`uvx`](https://docs.astral.sh/uv/concepts/tools/) or [`pip`](https://pypi.org/project/pip/). `uvx` is recommended for ease of use and setup.\n\nFor example, this will start the [Git](src/git) server:\n```sh\n# With uvx\nuvx mcp-server-git\n\n# With pip\npip install mcp-server-git\npython -m mcp_server_git\n```\n\nFollow [these](https://docs.astral.sh/uv/getting-started/installation/) instructions to install `uv` / `uvx` and [these](https://pip.pypa.io/en/stable/installation/) to install `pip`.\n\n### Using an MCP Client\nHowever, running a server on its own isn't very useful, and should instead be configured into an MCP client. For example, here's the Claude Desktop configuration to use the above server:\n\n```json\n{\n  \"mcpServers\": {\n    \"memory\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@modelcontextprotocol/server-memory\"]\n    }\n  }\n}\n```\n\nAdditional examples of using the Claude Desktop as an MCP client might look like:\n\n```json\n{\n  \"mcpServers\": {\n    \"filesystem\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@modelcontextprotocol/server-filesystem\", \"/path/to/allowed/files\"]\n    },\n    \"git\": {\n      \"command\": \"uvx\",\n      \"args\": [\"mcp-server-git\", \"--repository\", \"path/to/git/repo\"]\n    },\n    \"github\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@modelcontextprotocol/server-github\"],\n      \"env\": {\n        \"GITHUB_PERSONAL_ACCESS_TOKEN\": \"<YOUR_TOKEN>\"\n      }\n    },\n    \"postgres\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@modelcontextprotocol/server-postgres\", \"postgresql://localhost/mydb\"]\n    }\n  }\n}\n```\n\n## 🛠️ Creating Your Own Server\n\nInterested in creating your own MCP server? Visit the official documentation at [modelcontextprotocol.io](https://modelcontextprotocol.io/introduction) for comprehensive guides, best practices, and technical details on implementing MCP servers.\n\n## 🤝 Contributing\n\nSee [CONTRIBUTING.md](CONTRIBUTING.md) for information about contributing to this repository.\n\n## 🔒 Security\n\nSee [SECURITY.md](SECURITY.md) for reporting security vulnerabilities.\n\n## 📜 License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n\n## 💬 Community\n\n- [GitHub Discussions](https://github.com/orgs/modelcontextprotocol/discussions)\n\n## ⭐ Support\n\nIf you find MCP servers useful, please consider starring the repository and contributing new servers or improvements!\n\n---\n\nManaged by Anthropic, but built together with the community. The Model Context Protocol is open source and we encourage everyone to contribute their own servers and improvements!",
      "npm_url": "https://www.npmjs.com/package/servers",
      "npm_downloads": 2441,
      "keywords": [
        "modelcontextprotocol",
        "retrieval",
        "html",
        "search modelcontextprotocol",
        "content extraction",
        "modelcontextprotocol servers"
      ],
      "category": "web-search"
    },
    "modelcontextprotocol-servers--google-search-mcp": {
      "owner": "modelcontextprotocol-servers",
      "name": "google-search-mcp",
      "url": "https://github.com/modelcontextprotocol-servers/google-search-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/modelcontextprotocol-servers.webp",
      "description": "Perform real-time Google searches while bypassing anti-bot mechanisms and handling CAPTCHAs automatically. Extract structured search results in various languages and regions to support AI assistants.",
      "stars": 2,
      "forks": 6,
      "license": "No License",
      "language": "TypeScript",
      "updated_at": "2025-09-21T19:34:05Z",
      "readme_content": "# Google Search MCP\n\n[Model Context Protocol](https://modelcontextprotocol.wiki) server for google search.\nA Playwright-based Model Context Protocol (MCP) tool that bypasses search engine anti-bot mechanisms, performs Google searches, and extracts results, providing real-time search capabilities for AI assistants like Claude and Cursor.\n\n## Features\n\n- **Anti-Bot Bypass**: Uses browser fingerprint spoofing and real user behavior simulation to avoid detection\n- **Automatic CAPTCHA Handling**: Switches to headed mode when encountering CAPTCHAs, allowing users to complete verification\n- **State Persistence**: Saves browser session state to reduce the need for repeated verification\n- **Adaptability**: Uses multiple selector combinations to adapt to changes in Google search pages\n- **MCP Integration**: Implements the Model Context Protocol for easy integration with AI assistants\n- **Multi-language Support**: Supports search results in different languages and regions\n\n\n## Using with Cursor\n\n**Installation - Globally**\n\nRun the MCP server using npx:\n\n```bash\nnpx -y @mcp-server/google-search-mcp@latest\n```\n\nIn your Cursor IDE\n\n1. Go to `Cursor Settings` > `MCP`\n2. Click `+ Add New MCP Server`\n3. Fill in the form:\n   - Name: `google-search` (or any name you prefer)\n   - Type: `command`\n   - Command: `npx -y @mcp-server/google-search-mcp@latest`\n\n\n**Installation - Project-specific**\n\nAdd an `.cursor/mcp.json` file to your project:\n\n```json\n{\n  \"mcpServers\": {\n    \"google-search\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"@mcp-server/google-search-mcp@latest\"\n      ]\n    }\n  }\n}\n```\n\n## Development\n\n```bash\nyarn install\n```\n\n## Build the project\n\n```bash\nyarn build\n```\n\n## Usage\n\n### Running as an MCP Server\n\n```bash\nyarn start\n```\n\n### Using with MCP Inspector\n\nTo debug the server, you can use the [MCP Inspector](https://github.com/modelcontextprotocol/inspector):\n\n```bash\n# First build the project\nyarn build\n\n# Start the MCP Inspector and server\nnpx @modelcontextprotocol/inspector node dist/index.js\n```\n\n## Parameters\n\nThe search tool accepts the following parameters:\n\n- `query` (required): Search query string\n- `limit` (optional): Number of search results to return, default is 10\n- `timeout` (optional): Search operation timeout in milliseconds, default is 60000\n- `language` (optional): Language for search results, e.g., zh-CN, en-US, default is zh-CN\n- `region` (optional): Region for search results, e.g., cn, com, co.jp, default is cn\n\n## How It Works\n\n1. The tool uses Playwright to control a Chromium browser to perform Google searches\n2. It avoids bot detection through browser fingerprint spoofing and real user behavior simulation\n3. When encountering CAPTCHA verification, it automatically switches to headed mode for user completion\n4. It extracts search results and returns them in a structured format\n5. It saves browser state for reuse in subsequent searches\n\n## Advanced Configuration\n\n### Browser State File\n\nBy default, the browser state is saved in the `.google-search-browser-state.json` file in the user's home directory. You can modify this path through parameters.\n\n### Language and Region Settings\n\nYou can specify the language and region for search results through parameters:\n\n```\n// English (US) search results\n\"language\": \"en-US\", \"region\": \"com\"\n\n// Japanese search results\n\"language\": \"ja-JP\", \"region\": \"co.jp\"\n\n// Chinese (Simplified) search results\n\"language\": \"zh-CN\", \"region\": \"cn\"\n```\n\n## Notes\n\n- On first use, if you encounter CAPTCHA verification, the system will automatically switch to headed mode for you to complete the verification\n- After verification, the system will save the state file, making subsequent searches smoother\n- Overly frequent search requests may trigger Google's rate limiting mechanisms\n- This tool is for learning and research purposes only, please comply with Google's terms of service\n\n## License\n\nMIT\n\n## Disclaimer\n\nThis tool is for learning and research purposes only. When using this tool to access Google or other search engines, please comply with relevant terms of service and legal regulations. The author is not responsible for any issues resulting from the use of this tool.",
      "npm_url": "https://www.npmjs.com/package/google-search-mcp",
      "npm_downloads": 1098,
      "keywords": [
        "searches",
        "search",
        "captchas",
        "ai assistants",
        "web search",
        "google searches"
      ],
      "category": "web-search"
    },
    "modelcontextprotocol-servers--whois-mcp": {
      "owner": "modelcontextprotocol-servers",
      "name": "whois-mcp",
      "url": "https://github.com/modelcontextprotocol-servers/whois-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/modelcontextprotocol-servers.webp",
      "description": "Retrieve detailed domain registration information, including ownership, registration dates, and domain status. Check domain availability and ownership details directly in an AI environment without needing to search in a browser.",
      "stars": 19,
      "forks": 4,
      "license": "No License",
      "language": "JavaScript",
      "updated_at": "2025-08-28T06:15:48Z",
      "readme_content": "# Whois MCP\n\n[Model Context Protocol](https://modelcontextprotocol.io) server for whois lookups.\n\n## Overview\n\nThis MCP server allows AI agents like Claude Desktop, Cursor, Windsurf,.. etc to perform WHOIS lookups and retrieve domain details. \n\n**Purpose**\nYou can directly ask the AI to check if a domain is available, who owns it, when it was registered, and other important details. No need to go to browser and search.\n\n**What is a WHOIS Lookup?**\nA WHOIS lookup is the process of querying a WHOIS database to retrieve registration details about a domain name, IP address, or autonomous system. It helps users find out who owns a domain, when it was registered, when it expires, and other important details.\n\n**What Information Can a WHOIS Lookup Provide?**\n\nWhen you perform a WHOIS lookup, you can retrieve details such as:\n\n- Domain Name – The specific domain queried\n- Registrar Name – The company managing the domain registration (e.g., GoDaddy, Namecheap)\n- Registrant Details – The name, organization, and contact details of the domain owner (unless protected by WHOIS privacy)\n- Registration & Expiry Date – When the domain was registered and when it will expire\n- Name Servers – The DNS servers the domain is using\n- Domain Status – Active, expired, locked, or pending deletion\n- Contact Information – Administrative, technical, and billing contacts (if not hidden)\n\n## Available Tools\n\n| Tool                  | Description                                |\n| --------------------- | ------------------------------------------ |\n| `whois_domain`        | Looksup whois information about the domain |\n| `whois_tld`           | Looksup whois information about the Top Level Domain (TLD)    |\n| `whois_ip`            | Looksup whois information about the IP     |\n| `whois_as`            | Looksup whois information about the Autonomous System Number (ASN)     |\n\n## Using with Cursor\n\n**Installation - Globally**\n\nRun the MCP server using npx:\n\n```bash\nnpx -y @mcp-server/whois-mcp@latest\n```\n\nIn your Cursor IDE\n\n1. Go to `Cursor Settings` > `MCP`\n2. Click `+ Add New MCP Server`\n3. Fill in the form:\n   - Name: `Whois Lookup` (or any name you prefer)\n   - Type: `command`\n   - Command: `npx -y @mcp-server/whois-mcp@latest`\n\n\n**Installation - Project-specific**\n\nAdd an `.cursor/mcp.json` file to your project:\n\n```json\n{\n  \"mcpServers\": {\n    \"whois\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"@mcp-server/whois-mcp@latest\"\n      ]\n    }\n  }\n}\n```\n\n**Usage**\n\nOnce configured, the whois tools will be automatically available to the Cursor AI Agent. You can:\n\n1. The tool will be listed under `Available Tools` in MCP settings\n2. Agent will automatically use it when relevant\n3. You can explicitly ask Agent to send notifications\n\n## Using with Roo Code\nAccess the MCP settings by clicking \"Edit MCP Settings\" in Roo Code settings or using the \"Roo Code: Open MCP Config\" command in VS Code's command palette.\n\n```json\n{\n  \"mcpServers\": {\n    \"whois\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"@mcp-server/whois-mcp@latest\"\n      ]\n    }\n  }\n}\n```\n3. The whois capabilities will be available to Roo Code's AI agents\n\n## Development\n\n```bash\n# Install dependencies\npnpm install\n\n# Build\npnpm build\n\n```\n\n## Debugging the Server\n\nTo debug your server, you can use the [MCP Inspector](https://github.com/modelcontextprotocol/inspector).\n\nFirst build the server\n\n```\npnpm build\n```\n\nRun the following command in your terminal:\n\n```\n# Start MCP Inspector and server with all tools\nnpx @modelcontextprotocol/inspector node dist/index.js\n```\n\n## License\n\nMIT\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "whois",
        "search",
        "modelcontextprotocol",
        "servers whois",
        "search modelcontextprotocol",
        "whois mcp"
      ],
      "category": "web-search"
    },
    "mondweep--youtube-music-mcp-server": {
      "owner": "mondweep",
      "name": "youtube-music-mcp-server",
      "url": "https://github.com/mondweep/youtube-music-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/mondweep.webp",
      "description": "Control YouTube Music playback by searching for and playing songs via voice commands or text input in Google Chrome. The server enables interaction between AI models and YouTube Music to manage music playback effectively.",
      "stars": 14,
      "forks": 4,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-10-02T04:22:42Z",
      "readme_content": "# YouTube Music MCP Server\n\n## Overview\nThis project implements a Model Context Protocol (MCP) server that enables AI models to control YouTube Music playback through Google Chrome. It bridges the gap between AI assistants and music playback, allowing AI to search for and play songs based on song names and artist names.\n\n## What is MCP?\nThe Model Context Protocol (MCP) is a standardized way for AI models to interact with external tools and services. It provides a structured communication protocol that allows AI assistants to:\n- Discover available tools\n- Understand tool capabilities\n- Execute actions through these tools\n- Handle responses and errors consistently\n\nLearn more about MCP:\n- [MCP Documentation](https://github.com/modelcontextprotocol/protocol)\n- [MCP SDK](https://github.com/modelcontextprotocol/sdk)\n\n## Features\n- Search YouTube Music for songs\n- Play songs directly in Google Chrome\n- Support for song name and artist name search\n- Error handling and logging\n- Cross-platform support (focused on macOS for Chrome automation)\n\n## Architecture\n\n### High-Level Overview\n\n# youtube-music-server MCP Server\n\nA Model Context Protocol server\n\nThis is a TypeScript-based MCP server that implements a simple notes system. It demonstrates core MCP concepts by providing:\n\n- Resources representing text notes with URIs and metadata\n- Tools for creating new notes\n- Prompts for generating summaries of notes\n\n## Features\n\n### Resources\n- List and access notes via `note://` URIs\n- Each note has a title, content and metadata\n- Plain text mime type for simple content access\n\n### Tools\n- `create_note` - Create new text notes\n  - Takes title and content as required parameters\n  - Stores note in server state\n\n### Prompts\n- `summarize_notes` - Generate a summary of all stored notes\n  - Includes all note contents as embedded resources\n  - Returns structured prompt for LLM summarization\n\n## Development\n\nInstall dependencies:\n```bash\nnpm install\n```\n\nBuild the server:\n```bash\nnpm run build\n```\n\nFor development with auto-rebuild:\n```bash\nnpm run watch\n```\n\n## Installation\n\nTo use with Claude Desktop, add the server config:\n\nOn MacOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\nOn Windows: `%APPDATA%/Claude/claude_desktop_config.json`\n\n```json\n{\n  \"mcpServers\": {\n    \"youtube-music-server\": {\n      \"command\": \"/path/to/youtube-music-server/build/index.js\"\n    }\n  }\n}\n```\n\n### Debugging\n\nSince MCP servers communicate over stdio, debugging can be challenging. We recommend using the [MCP Inspector](https://github.com/modelcontextprotocol/inspector), which is available as a package script:\n\n```bash\nnpm run inspector\n```\n\nThe Inspector will provide a URL to access debugging tools in your browser.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "playback",
        "youtube",
        "chrome",
        "mondweep youtube",
        "youtube music",
        "control youtube"
      ],
      "category": "web-search"
    },
    "moyu6027--deepseek-MCP-server": {
      "owner": "moyu6027",
      "name": "deepseek-MCP-server",
      "url": "https://github.com/moyu6027/deepseek-MCP-server",
      "imageUrl": "/freedevtools/mcp/pfp/moyu6027.webp",
      "description": "Integrates the DeepSeek R1 model to enhance reasoning capabilities by allowing advanced logical analysis and structured cognitive frameworks. Evaluates confidence and uncertainty while monitoring reasoning quality and detecting biases in user interactions.",
      "stars": 1,
      "forks": 4,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-10-01T05:06:26Z",
      "readme_content": "# 🧠  DeepSeek MCP Server\n\n\n\n\n## 🚀 Features\n\n**Enhance Claude's reasoning capabilities** with the integration of DeepSeek R1's advanced reasoning engine. This server enables Claude to tackle complex reasoning tasks by leveraging the reasoning capabilites of deepseek r1 model.\n\n- DeepSeek R1 (The Brain) acts as the advanced reasoning planner:\n\n   - Plans multi-step logical analysis strategies\n   - Structures cognitive frameworks\n   - Evaluates confidence and uncertainty\n   - Monitors reasoning quality\n   - Detects edge cases and biases\n\n- Claude (The Executor) implements the reasoning plans:\n\n   - Executes the structured analysis\n   - Implements planned strategies\n   - Delivers final responses\n   - Handles user interaction\n   - Manages system integrations\n\n---\n\n## 🚀 Features\n\n### **Advanced Reasoning Capabilities**\n- Supports intricate multi-step reasoning tasks.\n- Designed for precision and efficiency in generating thoughtful responses.\n- 使用无问芯穹的API\n\n\n\n\n\n\n---\n\n## Complete Setup guide\n\n\n### Prerequisites\n- Python 3.12 or higher\n- `uv` package manager\n- INFINI_API_KEY For DeepSeek (Sign up at [无问芯穹](https://cloud.infini-ai.com/genstudio/model))\n\n\n\n1. **Clone the Repository**\n   ```bash\n   git clone https://github.com/moyu6027/deepseek-MCP-server.git\n   cd deepseek-MCP-server\n   ```\n\n2. **Ensure UV is Set Up**\n   - **Windows**: Run the following in PowerShell:\n     ```powershell\n     powershell -ExecutionPolicy ByPass -c \"irm https://astral.sh/uv/install.ps1 | iex\"\n     ```\n   - **Mac**: Run the following:\n     ```bash\n     curl -LsSf https://astral.sh/uv/install.sh | sh\n     ```\n\n3. **Create Virtual Environment**\n   ```bash\n   uv venv\n   source .venv/bin/activate\n   ```\n\n4. **Install Dependencies**\n   ```bash\n   uv add \"mcp[cli]\" httpx\n   ```\n\n5. **Set Up API Key**\n   ```bash\n   echo \"INFINI_API_KEY=your_key_here\" > .env\n   ```\n\n6. **Install the Server**\n   ```bash\n   mcp install server.py -f .env\n   ```\n\n7. **Configure MCP Server**\n   Edit the `claude_desktop_config.json` file to include the following configuration:\n\n   ```json\n   {\n       \"mcpServers\": {\n           \"deepseek-mcp\": {\n               \"command\": \"uv\",\n               \"args\": [\n                   \"--directory\",\n                   \"PATH_TO_DEEPSEEK_MCP_SERVER\",\n                   \"run\",\n                   \"server.py\"\n               ]\n           }\n       }\n   }\n   ```\n\n8. **Run the Server**\n   ```bash\n   uv run server.py\n   ```\n\n---\n\n## 🛠 Usage\n\n### Starting the Server\nThe server automatically starts when used with Claude Desktop. Ensure Claude Desktop is configured to detect the MCP server.\n\n### Example Workflow\n1. Claude receives a query requiring advanced reasoning.\n2. The query is forwarded to DeepSeek R1 for processing.\n3. DeepSeek R1 returns structured reasoning wrapped in `<ant_thinking>` tags.\n4. Claude integrates the reasoning into its final response.\n\n---\n\n\n## 📄 License\n\nThis project is licensed under the MIT License. See the [LICENSE](LICENSE) file for details.\n\n---",
      "npm_url": "https://www.npmjs.com/package/deepseek-mcp-server",
      "npm_downloads": 4652,
      "keywords": [
        "deepseek",
        "reasoning",
        "cognitive",
        "reasoning capabilities",
        "monitoring reasoning",
        "reasoning quality"
      ],
      "category": "web-search"
    },
    "mrgoonie--searchapi-mcp-server": {
      "owner": "mrgoonie",
      "name": "searchapi-mcp-server",
      "url": "https://github.com/mrgoonie/searchapi-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/mrgoonie.webp",
      "description": "Connects AI assistants to external search data sources such as Google and Bing, enabling web, image, and YouTube searches through a standardized MCP interface.",
      "stars": 10,
      "forks": 3,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-09-13T05:57:04Z",
      "readme_content": "# SearchAPI.site - MCP Server\n\nThis project provides a Model Context Protocol (MCP) server that connects AI assistants to external data sources (Google, Bing, etc.) via [SearchAPI.site](https://searchapi.site). \n\n**Author:** Claude\n\n- [Glama](https://glama.ai/mcp/servers/@mrgoonie/searchapi-mcp-server)\n- [Github](https://github.com/mrgoonie/searchapi-mcp-server)\n- [NPM](https://www.npmjs.com/package/searchapi-mcp-server)\n\n<a href=\"https://glama.ai/mcp/servers/@mrgoonie/searchapi-mcp-server\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@mrgoonie/searchapi-mcp-server/badge\" alt=\"SearchAPI Server MCP server\" />\n</a>\n\n### Available platforms\n- [x] Google - Web Search\n- [x] Google - Image Search\n- [x] Google - YouTube Search\n- [ ] Google - Maps Search\n- [x] Bing - Web Search\n- [ ] Bing - Image Search\n- [ ] Reddit\n- [ ] X/Twitter\n- [ ] Facebook Search\n- [ ] Facebook Group Search\n- [ ] Instagram\n- [ ] TikTok\n\n## SearchAPI.site\n\n- [Website](https://searchapi.site)\n- [API Docs](https://searchapi.site/api-docs)\n- [Swagger UI Config](https://searchapi.site/api-docs/swagger-ui-init.js)\n- Create Search API key [here](https://searchapi.site/profile)\n- [GitHub](https://github.com/mrgoonie/searchapi)\n\n## Supported Transports\n\n- [x] [\"stdio\"](https://modelcontextprotocol.io/specification/2025-03-26/basic/transports#stdio) transport - Default transport for CLI usage\n- [x] [\"Streamable HTTP\"](https://modelcontextprotocol.io/specification/2025-03-26/basic/transports#streamable-http) transport - For web-based clients\n  - [ ] Implement auth (\"Authorization\" headers with `Bearer <token>`)\n- [x] ~~\"sse\" transport~~ **[(Deprecated)](https://modelcontextprotocol.io/specification/2025-03-26/basic/transports#backwards-compatibility)**\n- [ ] Write tests\n\n## How to use\n\n### CLI\n\n```bash\n# Google search via CLI\nnpm run dev:cli -- search-google --query \"your search query\" --api-key \"your-api-key\"\n\n# Google image search via CLI\nnpm run dev:cli -- search-google-images --query \"your search query\" --api-key \"your-api-key\"\n\n# YouTube search via CLI\nnpm run dev:cli -- search-youtube --query \"your search query\" --api-key \"your-api-key\" --max-results 5\n```\n\n### MCP Setup\n\n**For local configuration with stdio transport:**\n```json\n{\n  \"mcpServers\": {\n    \"searchapi\": {\n      \"command\": \"node\",\n      \"args\": [\"/path/to/searchapi-mcp-server/dist/index.js\"],\n      \"transportType\": \"stdio\"\n    }\n  }\n}\n```\n\n**For remote HTTP configuration:**\n```json\n{\n  \"mcpServers\": {\n    \"searchapi\": {\n      \"type\": \"http\",\n      \"url\": \"http://mcp.searchapi.site/mcp\"\n    }\n  }\n}\n```\n\n**Environment Variables for HTTP Transport:**\n\nYou can configure the HTTP server using these environment variables:\n\n- `MCP_HTTP_HOST`: The host to bind to (default: `127.0.0.1`)\n- `MCP_HTTP_PORT`: The port to listen on (default: `8080`)\n- `MCP_HTTP_PATH`: The endpoint path (default: `/mcp`)\n\n---\n\n# Source Code Overview\n\n## What is MCP?\n\nModel Context Protocol (MCP) is an open standard that allows AI systems to securely and contextually connect with external tools and data sources.\n\nThis boilerplate implements the MCP specification with a clean, layered architecture that can be extended to build custom MCP servers for any API or data source.\n\n## Why Use This Boilerplate?\n\n- **Production-Ready Architecture**: Follows the same pattern used in published MCP servers, with clear separation between CLI, tools, controllers, and services.\n\n- **Type Safety**: Built with TypeScript for improved developer experience, code quality, and maintainability.\n\n- **Working Example**: Includes a fully implemented IP lookup tool demonstrating the complete pattern from CLI to API integration.\n\n- **Testing Framework**: Comes with testing infrastructure for both unit and CLI integration tests, including coverage reporting.\n\n- **Development Tooling**: Includes ESLint, Prettier, TypeScript, and other quality tools preconfigured for MCP server development.\n\n---\n\n# Getting Started\n\n## Prerequisites\n\n- **Node.js** (>=18.x): [Download](https://nodejs.org/)\n- **Git**: For version control\n\n---\n\n## Step 1: Clone and Install\n\n```bash\n# Clone the repository\ngit clone https://github.com/mrgoonie/searchapi-mcp-server.git\ncd searchapi-mcp-server\n\n# Install dependencies\nnpm install\n```\n\n---\n\n## Step 2: Run Development Server\n\nStart the server in development mode with stdio transport (default):\n\n```bash\nnpm run dev:server\n```\n\nOr with the Streamable HTTP transport:\n\n```bash\nnpm run dev:server:http\n```\n\nThis starts the MCP server with hot-reloading and enables the MCP Inspector at http://localhost:5173.\n\n⚙️ Proxy server listening on port 6277\n🔍 MCP Inspector is up and running at http://127.0.0.1:6274\n\nWhen using HTTP transport, the server will be available at http://127.0.0.1:8080/mcp by default.\n\n---\n\n## Step 3: Test the Example Tool\n\nRun the example IP lookup tool from the CLI:\n\n```bash\n# Using CLI in development mode\nnpm run dev:cli -- search-google --query \"your search query\" --api-key \"your-api-key\"\n\n# Or with a specific IP\nnpm run dev:cli -- search-google --query \"your search query\" --api-key \"your-api-key\" --limit 10 --offset 0 --sort \"date:d\" --from_date \"2023-01-01\" --to_date \"2023-12-31\"\n```\n\n---\n\n# Architecture\n\nThis boilerplate follows a clean, layered architecture pattern that separates concerns and promotes maintainability.\n\n## Project Structure\n\n```\nsrc/\n├── cli/              # Command-line interfaces\n├── controllers/      # Business logic\n├── resources/        # MCP resources: expose data and content from your servers to LLMs\n├── services/         # External API interactions\n├── tools/            # MCP tool definitions\n├── types/            # Type definitions\n├── utils/            # Shared utilities\n└── index.ts          # Entry point\n```\n\n## Layers and Responsibilities\n\n### CLI Layer (`src/cli/*.cli.ts`)\n\n- **Purpose**: Define command-line interfaces that parse arguments and call controllers\n- **Naming**: Files should be named `<feature>.cli.ts`\n- **Testing**: CLI integration tests in `<feature>.cli.test.ts`\n\n### Tools Layer (`src/tools/*.tool.ts`)\n\n- **Purpose**: Define MCP tools with schemas and descriptions for AI assistants\n- **Naming**: Files should be named `<feature>.tool.ts` with types in `<feature>.types.ts`\n- **Pattern**: Each tool should use zod for argument validation\n\n### Controllers Layer (`src/controllers/*.controller.ts`)\n\n- **Purpose**: Implement business logic, handle errors, and format responses\n- **Naming**: Files should be named `<feature>.controller.ts`\n- **Pattern**: Should return standardized `ControllerResponse` objects\n\n### Services Layer (`src/services/*.service.ts`)\n\n- **Purpose**: Interact with external APIs or data sources\n- **Naming**: Files should be named `<feature>.service.ts`\n- **Pattern**: Pure API interactions with minimal logic\n\n### Utils Layer (`src/utils/*.util.ts`)\n\n- **Purpose**: Provide shared functionality across the application\n- **Key Utils**:\n    - `logger.util.ts`: Structured logging\n    - `error.util.ts`: Error handling and standardization\n    - `formatter.util.ts`: Markdown formatting helpers\n\n---\n\n# Development Guide\n\n## Development Scripts\n\n```bash\n# Start server in development mode (hot-reload & inspector)\nnpm run dev:server\n\n# Run CLI in development mode\nnpm run dev:cli -- [command] [args]\n\n# Build the project\nnpm run build\n\n# Start server in production mode\nnpm run start:server\n\n# Run CLI in production mode\nnpm run start:cli -- [command] [args]\n```\n\n## Testing\n\n```bash\n# Run all tests\nnpm test\n\n# Run specific tests\nnpm test -- src/path/to/test.ts\n\n# Generate test coverage report\nnpm run test:coverage\n```\n\n##  evals\n\nThe evals package loads an mcp client that then runs the index.ts file, so there is no need to rebuild between tests. You can load environment variables by prefixing the npx command. Full documentation can be found [here](https://www.mcpevals.io/docs).\n\n```bash\nOPENAI_API_KEY=your-key  npx mcp-eval src/evals/evals.ts src/tools/searchapi.tool.ts\n```\n\n## Code Quality\n\n```bash\n# Lint code\nnpm run lint\n\n# Format code with Prettier\nnpm run format\n\n# Check types\nnpm run typecheck\n```\n\n---\n\n# Building Custom Tools\n\nFollow these steps to add your own tools to the server:\n\n## 1. Define Service Layer\n\nCreate a new service in `src/services/` to interact with your external API:\n\n```typescript\n// src/services/example.service.ts\nimport { Logger } from '../utils/logger.util.js';\n\nconst logger = Logger.forContext('services/example.service.ts');\n\nexport async function getData(param: string): Promise<any> {\n\tlogger.debug('Getting data', { param });\n\t// API interaction code here\n\treturn { result: 'example data' };\n}\n```\n\n## 2. Create Controller\n\nAdd a controller in `src/controllers/` to handle business logic:\n\n```typescript\n// src/controllers/example.controller.ts\nimport { Logger } from '../utils/logger.util.js';\nimport * as exampleService from '../services/example.service.js';\nimport { formatMarkdown } from '../utils/formatter.util.js';\nimport { handleControllerError } from '../utils/error-handler.util.js';\nimport { ControllerResponse } from '../types/common.types.js';\n\nconst logger = Logger.forContext('controllers/example.controller.ts');\n\nexport interface GetDataOptions {\n\tparam?: string;\n}\n\nexport async function getData(\n\toptions: GetDataOptions = {},\n): Promise<ControllerResponse> {\n\ttry {\n\t\tlogger.debug('Getting data with options', options);\n\n\t\tconst data = await exampleService.getData(options.param || 'default');\n\n\t\tconst content = formatMarkdown(data);\n\n\t\treturn { content };\n\t} catch (error) {\n\t\tthrow handleControllerError(error, {\n\t\t\tentityType: 'ExampleData',\n\t\t\toperation: 'getData',\n\t\t\tsource: 'controllers/example.controller.ts',\n\t\t});\n\t}\n}\n```\n\n## 3. Implement MCP Tool\n\nCreate a tool definition in `src/tools/`:\n\n```typescript\n// src/tools/example.tool.ts\nimport { McpServer } from '@modelcontextprotocol/sdk/server/mcp.js';\nimport { z } from 'zod';\nimport { Logger } from '../utils/logger.util.js';\nimport { formatErrorForMcpTool } from '../utils/error.util.js';\nimport * as exampleController from '../controllers/example.controller.js';\n\nconst logger = Logger.forContext('tools/example.tool.ts');\n\nconst GetDataArgs = z.object({\n\tparam: z.string().optional().describe('Optional parameter'),\n});\n\ntype GetDataArgsType = z.infer<typeof GetDataArgs>;\n\nasync function handleGetData(args: GetDataArgsType) {\n\ttry {\n\t\tlogger.debug('Tool get_data called', args);\n\n\t\tconst result = await exampleController.getData({\n\t\t\tparam: args.param,\n\t\t});\n\n\t\treturn {\n\t\t\tcontent: [{ type: 'text' as const, text: result.content }],\n\t\t};\n\t} catch (error) {\n\t\tlogger.error('Tool get_data failed', error);\n\t\treturn formatErrorForMcpTool(error);\n\t}\n}\n\nexport function register(server: McpServer) {\n\tserver.tool(\n\t\t'get_data',\n\t\t`Gets data from the example API, optionally using \\`param\\`.\nUse this to fetch example data. Returns formatted data as Markdown.`,\n\t\tGetDataArgs.shape,\n\t\thandleGetData,\n\t);\n}\n```\n\n## 4. Add CLI Support\n\nCreate a CLI command in `src/cli/`:\n\n```typescript\n// src/cli/example.cli.ts\nimport { program } from 'commander';\nimport { Logger } from '../utils/logger.util.js';\nimport * as exampleController from '../controllers/example.controller.js';\nimport { handleCliError } from '../utils/error-handler.util.js';\n\nconst logger = Logger.forContext('cli/example.cli.ts');\n\nprogram\n\t.command('get-data')\n\t.description('Get example data')\n\t.option('--param <value>', 'Optional parameter')\n\t.action(async (options) => {\n\t\ttry {\n\t\t\tlogger.debug('CLI get-data called', options);\n\n\t\t\tconst result = await exampleController.getData({\n\t\t\t\tparam: options.param,\n\t\t\t});\n\n\t\t\tconsole.log(result.content);\n\t\t} catch (error) {\n\t\t\thandleCliError(error);\n\t\t}\n\t});\n```\n\n## 5. Register Components\n\nUpdate the entry points to register your new components:\n\n```typescript\n// In src/cli/index.ts\nimport '../cli/example.cli.js';\n\n// In src/index.ts (for the tool)\nimport exampleTool from './tools/example.tool.js';\n// Then in registerTools function:\nexampleTool.register(server);\n```\n\n---\n\n# Debugging Tools\n\n## MCP Inspector\n\nAccess the visual MCP Inspector to test your tools and view request/response details:\n\n1. Run `npm run dev:server`\n2. Open http://localhost:5173 in your browser\n3. Test your tools and view logs directly in the UI\n\n## Server Logs\n\nEnable debug logs for development:\n\n```bash\n# Set environment variable\nDEBUG=true npm run dev:server\n\n# Or configure in ~/.mcp/configs.json\n```\n\n---\n\n# Publishing Your MCP Server\n\nWhen ready to publish your custom MCP server:\n\n1. Update package.json with your details\n2. Update README.md with your tool documentation\n3. Build the project: `npm run build`\n4. Test the production build: `npm run start:server`\n5. Publish to npm: `npm publish`\n\n---\n\n# License\n\n[ISC License](https://opensource.org/licenses/ISC)\n\n```json\n{\n\t\"searchapi\": {\n\t\t\"environments\": {\n\t\t\t\"DEBUG\": \"true\",\n\t\t\t\"SEARCHAPI_API_KEY\": \"value\"\n\t\t}\n\t}\n}\n```\n\n**Note:** For backward compatibility, the server will also recognize configurations under the full package name (`searchapi-mcp-server`) or the unscoped package name (`searchapi-mcp-server`) if the `searchapi` key is not found. However, using the short `searchapi` key is recommended for new configurations.\n\n## Co-Authors\n\n- Claude Code (Claude AI Assistant)\n- Goon",
      "npm_url": "https://www.npmjs.com/package/searchapi-mcp-server",
      "npm_downloads": 154,
      "keywords": [
        "searchapi",
        "searches",
        "bing",
        "searchapi mcp",
        "mrgoonie searchapi",
        "ai assistants"
      ],
      "category": "web-search"
    },
    "mshk--mcp-rss-crawler": {
      "owner": "mshk",
      "name": "mcp-rss-crawler",
      "url": "https://github.com/mshk/mcp-rss-crawler",
      "imageUrl": "/freedevtools/mcp/pfp/mshk.webp",
      "description": "Fetch and manage RSS feeds with caching and filtering capabilities. Integrates seamlessly with LLMs to provide access to the latest articles from various sources.",
      "stars": 15,
      "forks": 4,
      "license": "No License",
      "language": "TypeScript",
      "updated_at": "2025-09-21T15:38:33Z",
      "readme_content": "# MCP-RSS-Crawler\n\nAn MCP (Message Chain Protocol) server that fetches RSS feeds and shares them with LLMs.\n\n## Features\n\n- Fetching and caching of RSS feeds (SQLite database)\n- MCP protocol implementation for seamless LLM integration\n- Support for filtering feeds by category, source, or keywords\n- Comprehensive API endpoints for feed management\n  - Add, update, and delete feeds\n- Support for fetching articles from Firecrawl\n\n## Requirements\n\n- Bun\n- Firecrawl API key\n- Claude Desktop or other MCP client\n\n## Setup as MCP Server\n\n1. Clone this repository\n2. Create a `claude_desktop_config.json` file based on `claude_desktop_config.json.example` with your configuration\n\n```json\n{\n  \"mcpServers\": {\n    \"rss-crawler\": {\n      \"command\": \"/path/to/bun\",\n      \"args\": [\"run\", \"/path/to/mcp-rss-crawler/apps/mcp-server/src/mcp-cli.ts\"],\n      \"cwd\": \"/path/to/mcp-rss-crawler\",\n      \"env\": {\n        \"PORT\": \"5556\",\n        \"DB_DIR\": \"/path/to/mcp-rss-crawler\",\n        \"FIRECRAWL_API_KEY\": \"fc-<YOUR_FIRECRAWL_API_KEY>\"\n      }\n    }\n  }\n}\n```\n\n3. Install dependencies:\n   ```\n   bun install\n   ```\n4. Start Claude Desktop:\n\n## MCP Protocol\n\nThe server implements the Message Chain Protocol (MCP) which allows LLMs to access your latest RSS feeds. The MCP endpoint accepts POST requests with a JSON body containing a messages array and returns a response with the latest feed items.\n\nExample request:\n```json\n{\n  \"messages\": [\n    {\n      \"role\": \"user\",\n      \"content\": \"What are the latest news from my RSS feeds?\"\n    }\n  ]\n}\n```\n\nExample response:\n```json\n{\n  \"messages\": [\n    {\n      \"role\": \"assistant\",\n      \"content\": \"Here are the latest articles from your RSS feeds:\",\n      \"name\": \"rss-mcp\"\n    },\n    {\n      \"role\": \"tool\",\n      \"content\": \"[{\\\"title\\\":\\\"Article Title\\\",\\\"summary\\\":\\\"Article summary...\\\",\\\"published\\\":\\\"2025-03-16T04:30:00.000Z\\\",\\\"origin\\\":\\\"Feed Name\\\",\\\"link\\\":\\\"https://example.com/article\\\"}]\",\n      \"name\": \"rss-feeds\"\n    }\n  ]\n}\n```\n\n## Configuration Options\n\nThe server can be configured through environment variables or a `.env` file:\n\n- `PORT` - Server port (default: 5556)\n- `FIRECRAWL_API_KEY` - Firecrawl API key\n- `DB_DIR` - Database directory (default: `~/.mcp-rss-crawler`)\n\n## Troubleshooting\n\n- For connection issues, check your network settings and firewall configuration\n- Logs are available in the console and can be used to diagnose problems\n- For more detailed logging, set the `DEBUG=mcp-rss:*` environment variable\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "rss",
        "feeds",
        "search",
        "rss crawler",
        "rss feeds",
        "manage rss"
      ],
      "category": "web-search"
    },
    "mshojaei77--ReActMCP": {
      "owner": "mshojaei77",
      "name": "ReActMCP",
      "url": "https://github.com/mshojaei77/ReActMCP",
      "imageUrl": "/freedevtools/mcp/pfp/mshojaei77.webp",
      "description": "Integrates real-time web search capabilities into AI assistants, performing basic and advanced searches and returning markdown-formatted results including titles, URLs, publication dates, and content summaries.",
      "stars": 141,
      "forks": 23,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-09-25T22:54:50Z",
      "readme_content": "# ReActMCP Web Search\n\nReActMCP Web Search is an MCP (Model Context Protocol) server that integrates web search capabilities into your AI assistant framework. It leverages the Exa API to perform both basic and advanced web searches, returning real-time, markdown-formatted results including titles, URLs, publication dates, and content summaries.\n\nThis repository is part of the broader ReActMCP project that connects various MCP tools and servers to empower your AI assistant with a wide range of capabilities.\n\n---\n\n## Table of Contents\n\n- [Features](#features)\n- [Requirements](#requirements)\n- [Installation](#installation)\n- [Configuration](#configuration)\n  - [Environment Variables](#environment-variables)\n  - [MCP Configuration](#mcp-configuration)\n  - [System Prompt](#system-prompt)\n- [Usage](#usage)\n  - [Running the Web Search Server](#running-the-web-search-server)\n  - [Testing the Tools](#testing-the-tools)\n- [Troubleshooting](#troubleshooting)\n- [License](#license)\n- [Contributing](#contributing)\n\n---\n\n## Features\n\n- **Basic Web Search**: Perform simple searches using the Exa API.\n- **Advanced Web Search**: Use additional filtering options such as domain restrictions, text inclusion requirements, and date filters.\n- **Markdown Output**: Format search results in Markdown to easily incorporate titles, URLs, and summaries.\n- **MCP Integration**: Easily add this tool into your MCP server ecosystem for multi-tool AI assistance.\n\n---\n## Requirements\n\n- **Python 3.8+**\n- [python-dotenv](https://pypi.org/project/python-dotenv/)\n- [exa_py](https://github.com/your-org/exa_py) (Exa API client)\n- Other dependencies that may be required by your MCP framework\n\n---\n\n## Installation\n\n1. **Clone the Repository**\n\n   ```bash\n   git clone https://github.com/mshojaei77/ReActMCP.git\n   cd ReActMCP\n   ```\n\n2. **Create a Virtual Environment (Optional but recommended)**\n\n   ```bash\n   python -m venv venv\n   source venv/bin/activate  # On Windows use: venv\\Scripts\\activate\n   ```\n\n3. **Install Dependencies**\n\n   ```bash\n   pip install -r requirements.txt\n   ```\n\n---\n\n## Configuration\n\n### Environment Variables\n\nCreate a `.env` file in the project root directory with at least the following variable:\n\n```env\nEXA_API_KEY=your_exa_api_key_here\nOPENAI_API_KEY=...\n```\n\nThis key is required by the Exa API for performing web searches.\n\n### MCP Configuration\n\nThe MCP configuration file `mcp_config.json` defines the settings and tools available to your MCP server. An example configuration is provided:\n\n```json\n{\n  \"websearch\": {\n    \"script\": \"web_search.py\",\n    \"encoding_error_handler\": \"ignore\",\n    \"description\": \"Web search capability using Exa API that provides real-time internet search results. Supports both basic and advanced search with filtering options including domain restrictions, text inclusion requirements, and date filtering. Returns formatted results with titles, URLs, publication dates, and content summaries.\",\n    \"required_env_vars\": [\"EXA_API_KEY\"],\n    \"active\": true\n  },\n  \"settings\": {\n    \"model\": \"gpt-4o\",\n    \"system_prompt_path\": \"system_prompt.txt\"\n  }\n}\n```\n\nYou can personalize or extend this configuration by modifying parameters such as default number of results or adding new MCP tools.\n\n### System Prompt\n\nThe `system_prompt.txt` file configures the behavior and tone of your AI assistant. It guides responses to be friendly, engaging, and informative, with the inclusion of emojis. An example prompt is provided:\n\n```text\nYou are a helpful, knowledgeable AI assistant with web search capabilities. Your goal is to provide accurate, comprehensive, and up-to-date information to users.\nUse lots of emojis and make your responses fun and engaging.\n\n## Available Search Tools\n\n- `search_web`: Basic web search that returns results based on a query\n- `advanced_search_web`: Advanced search with filtering options for domains, required text, and date ranges\n\n## Guidelines for Responding to Questions\n\n1. For current information or facts that might have changed since your training data, use the appropriate search tool to find the most recent and relevant information.\n\n2. Use `search_web` for general queries and `advanced_search_web` with appropriate filters for more specific needs.\n\n3. Formulate precise search queries to maximize result relevance.\n\n4. For recent information, use the `max_age_days` parameter in advanced search to limit results to recent publications.\n\n5. When targeting specific sources, use the `include_domains` parameter to focus your search.\n\n6. Cite sources by including URLs from search results.\n\n7. For insufficient or contradictory results, acknowledge limitations and explain findings.\n\n8. Break down complex topics into organized sections.\n\n9. Provide balanced perspectives on controversial topics.\n\n10. Be transparent about uncertainty rather than making up information.\n\n11. Maintain a helpful, informative, and conversational tone.\n\n## Response Quality Standards\n\nYour responses should be well-structured, factually accurate, and tailored to the user's level of understanding on the topic. Use the web search capabilities as your primary tools for accessing current information before responding to time-sensitive or factual queries.\n```\n\nFeel free to adjust the system prompt to align with your desired assistant behavior.\n\n---\n\n## Usage\n\n### Running the Web Search Server\n\nThe MCP servers is implemented in `servers` directory. To run a server, simply execute it :\n\n```bash\npython servers/web_search.py\n```\n\nThis command will start the MCP server which listens for requests and exposes the following tools:\n\n- **search_web**: Perform basic web searches.\n- **advanced_search_web**: Perform advanced web searches with filtering options.\n\n### Testing the Tools\n\nWithin `web_search.py`, a test function `test_search()` is provided (currently commented out) to demonstrate basic usage of the search capabilities. You can run this test by uncommenting the test execution block and using Python's asyncio runner:\n\n```python\nif __name__ == \"__main__\":\n    import asyncio\n    # Uncomment the following line to perform a test search\n    # asyncio.run(test_search())\n    mcp.run()\n```\n\nThis will print search results for sample queries and help you verify that the tool is functioning as expected.\n\n---\n## Claude Desktop Configuration:\nConfigure Claude Desktop to use this server by adding the following to your configuration:\n\n```json\n{\n  \"mcpServers\": {\n    \"websearch\": {\n         \"command\": \"python\",\n         \"args\": [\"path/to/servers/exa_web_search.py\"]\n       }\n  }\n}\n```\n\n## Troubleshooting\n\n- **Missing EXA_API_KEY:** Ensure that the `.env` file is properly set up with your valid Exa API key.\n- **Dependency Issues:** Verify that all necessary Python packages are installed (check your `requirements.txt` file). Reinstall packages if needed.\n- **API Errors:** If you encounter errors during web searches, check your network connection and verify the Exa API status.\n\n---\n\n## License\n\nThis project is licensed under the MIT License. See the [LICENSE](LICENSE) file for details.\n\n---\n\n## Contributing\n\nContributions are welcome! If you have suggestions, bug fixes, or improvements, please open an issue or submit a pull request.\n\nHappy coding and enjoy building your personalized, multi-tool AI assistant with ReActMCP Web Search! 🚀😊\n\n## Star History\n\n<a href=\"https://www.star-history.com/#mshojaei77/ReActMCP&Date\">\n <picture>\n   <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://api.star-history.com/svg?repos=mshojaei77/ReActMCP&type=Date&theme=dark\" />\n   <source media=\"(prefers-color-scheme: light)\" srcset=\"https://api.star-history.com/svg?repos=mshojaei77/ReActMCP&type=Date\" />\n   <img alt=\"Star History Chart\" src=\"https://api.star-history.com/svg?repos=mshojaei77/ReActMCP&type=Date\" />\n </picture>\n</a>\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "reactmcp",
        "searches",
        "search",
        "mshojaei77 reactmcp",
        "web search",
        "search capabilities"
      ],
      "category": "web-search"
    },
    "mugoosse--sitemap-mcp-server": {
      "owner": "mugoosse",
      "name": "sitemap-mcp-server",
      "url": "https://github.com/mugoosse/sitemap-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/mugoosse.webp",
      "description": "Fetch and analyze website sitemaps to provide insights into site structure, uncover hidden pages, and extract organized hierarchies. Supports standard sitemap formats and includes prompt templates for tasks like health checks and content gap identification.",
      "stars": 5,
      "forks": 1,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-08-30T06:28:14Z",
      "readme_content": "# Sitemap MCP Server\n\nDiscover website architecture and analyze site structure by fetching, parsing, and visualizing sitemaps from any URL. Uncover hidden pages and extract organized hierarchies without manual exploration.\n\nIncludes ready-to-use prompt templates for Claude Desktop that let you analyze websites, check sitemap health, extract URLs, find missing content, and create visualizations with just a URL input.\n\n![License](https://img.shields.io/github/license/mugoosse/sitemap-mcp-server)\n![PyPI](https://img.shields.io/pypi/v/sitemap-mcp-server)\n![Python Version](https://img.shields.io/badge/python-3.11+-blue)\n![Status](https://img.shields.io/badge/status-active-brightgreen.svg)\n[![smithery badge](https://smithery.ai/badge/@mugoosse/sitemap)](https://smithery.ai/server/@mugoosse/sitemap)\n\n<a href=\"https://glama.ai/mcp/servers/@mugoosse/sitemap-mcp-server\">\n  <img alt=\"badge\" width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@mugoosse/sitemap-mcp-server/badge\" />\n</a>\n\n## Demo\n\nGet answers to questions about any website leveraging the power of sitemaps.\n\n<details><summary>Cursor: how many pages does a modelcontextprotocol.io have?</summary>\n\n<br/>\n\n<img width=\"1541\" alt=\"image\" src=\"https://github.com/user-attachments/assets/f234b35f-ccb2-44c6-8ce5-71a6d2531e43\" />\n\n</details>\n\n<details><summary>Claude + prompt: visualize the sitemap in a diagram of windsurf.com</summary>\n\n<br/>\n\nClick on the \"attach\" button next to the tools button:\n\n![image](https://github.com/user-attachments/assets/e5b558c7-85fa-4b8f-b108-6d66d3b20719)\n\nThen select `visualize_sitemap`:\n\n<img width=\"558\" alt=\"image\" src=\"https://github.com/user-attachments/assets/2106e720-52c3-48a3-aa64-fd9c0a6ab075\" />\n\nNow we enter windsurf.com:\n\n![image](https://github.com/user-attachments/assets/ceddad25-3549-4a2f-a053-f54c1154912b)\n\nAnd we get a visualization of teh sitemap:\n\n<img width=\"1470\" alt=\"image\" src=\"https://github.com/user-attachments/assets/04464315-e619-4df5-8082-a981e6437da9\" />\n\n</details>\n\n## Installation\n\nMake sure [uv](https://docs.astral.sh/uv/getting-started/installation/) is installed.\n\n### Installing in Claude Desktop, Cursor or Windsurf\n\nAdd this entry to your [claude_desktop_config.json](https://modelcontextprotocol.io/quickstart/user#2-add-the-filesystem-mcp-server), Cursor settings, etc.:\n\n```json\n{\n  \"mcpServers\": {\n    \"sitemap\": {\n      \"command\": \"uvx\",\n      \"args\": [\"sitemap-mcp-server\"],\n      \"env\": { \"TRANSPORT\": \"stdio\" }\n    }\n  }\n}\n```\n\nRestart Claude if it's running. For Cursor simply press refresh and/or enable the MCP Server in the settings.\n\n### Installing via Smithery\n\nTo install sitemap for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@mugoosse/sitemap):\n\n```bash\nnpx -y @smithery/cli install @mugoosse/sitemap --client claude\n```\n\n### MCP Inspector\n\n<details><summary>uv + stdio transport</summary>\n\n```bash\nnpx @modelcontextprotocol/inspector env TRANSPORT=stdio uvx sitemap-mcp-server\n```\n\nOpen the MCP Inspector at http://127.0.0.1:6274, select `stdio` transport, and connect to the MCP server.\n\n</details>\n\n<details><summary>uv + sse transport</summary>\n\n```bash\n# Start the server\nuvx sitemap-mcp-server\n\n# Start the MCP Inspector in a separate terminal\nnpx @modelcontextprotocol/inspector connect http://127.0.0.1:8050\n```\n\nOpen the MCP Inspector at http://127.0.0.1:6274, select `sse` transport, and connect to the MCP server.\n\n</details>\n\n### SSE Transport\n\nIf you want to use the SSE transport, follow these steps:\n\n1. Start the server:\n\n```bash\nuvx sitemap-mcp-server\n```\n\n2. Configure your MCP Client, e.g. Cursor:\n\n```json\n{\n  \"mcpServers\": {\n    \"sitemap\": {\n      \"transport\": \"sse\",\n      \"url\": \"http://localhost:8050/sse\"\n    }\n  }\n}\n```\n\n### Local Development\n\nFor instructions on building and running the project from source, please refer to the [DEVELOPERS.md](DEVELOPERS.md) guide.\n\n## Usage\n\n### Tools\n\nThe following tools are available via the MCP server:\n\n- **get_sitemap_tree** - Fetch and parse the sitemap tree from a website URL\n\n  - Arguments: `url` (website URL), `include_pages` (optional, boolean)\n  - Returns: JSON representation of the sitemap tree structure\n\n- **get_sitemap_pages** - Get all pages from a website's sitemap with filtering options\n\n  - Arguments: `url` (website URL), `limit` (optional), `include_metadata` (optional), `route` (optional), `sitemap_url` (optional), `cursor` (optional)\n  - Returns: JSON list of pages with pagination metadata\n\n- **get_sitemap_stats** - Get statistics about a website's sitemap\n\n  - Arguments: `url` (website URL)\n  - Returns: JSON object with sitemap statistics including page counts, modification dates, and subsitemap details\n\n- **parse_sitemap_content** - Parse a sitemap directly from its XML or text content\n  - Arguments: `content` (sitemap XML content), `include_pages` (optional, boolean)\n  - Returns: JSON representation of the parsed sitemap\n\n### Prompts\n\nThe server includes ready-to-use prompts that appear as templates in Claude Desktop. After installing the server, you'll see these templates in the \"Templates\" menu (click the + icon next to the message input):\n\n- **Analyze Sitemap**: Provides comprehensive structure analysis of a website's sitemap\n- **Check Sitemap Health**: Evaluates SEO and health metrics of a sitemap\n- **Extract URLs from Sitemap**: Extracts and filters specific URLs from a sitemap\n- **Find Missing Content in Sitemap**: Identifies content gaps in a website's sitemap\n- **Visualize Sitemap Structure**: Creates a Mermaid.js diagram visualization of sitemap structure\n\nTo use these prompts:\n\n1. Click the + icon next to the message input in Claude Desktop\n2. Select the desired template from the list\n3. Fill in the website URL when prompted\n4. Claude will execute the appropriate sitemap analysis\n\n### Examples\n\n#### Fetch a Complete Sitemap\n\n```json\n{\n  \"name\": \"get_sitemap_tree\",\n  \"arguments\": {\n    \"url\": \"https://example.com\",\n    \"include_pages\": true\n  }\n}\n```\n\n#### Get Pages with Filtering and Pagination\n\n##### Filter by Route\n\n```json\n{\n  \"name\": \"get_sitemap_pages\",\n  \"arguments\": {\n    \"url\": \"https://example.com\",\n    \"limit\": 100,\n    \"include_metadata\": true,\n    \"route\": \"/blog/\"\n  }\n}\n```\n\n##### Filter by Specific Subsitemap\n\n```json\n{\n  \"name\": \"get_sitemap_pages\",\n  \"arguments\": {\n    \"url\": \"https://example.com\",\n    \"limit\": 100,\n    \"include_metadata\": true,\n    \"sitemap_url\": \"https://example.com/blog-sitemap.xml\"\n  }\n}\n```\n\n##### Cursor-Based Pagination\n\nThe server implements MCP cursor-based pagination to handle large sitemaps efficiently:\n\n**Initial Request:**\n\n```json\n{\n  \"name\": \"get_sitemap_pages\",\n  \"arguments\": {\n    \"url\": \"https://example.com\",\n    \"limit\": 50\n  }\n}\n```\n\n**Response with Pagination:**\n\n```json\n{\n  \"base_url\": \"https://example.com\",\n  \"pages\": [...],  // First batch of pages\n  \"limit\": 50,\n  \"nextCursor\": \"eyJwYWdlIjoxfQ==\"\n}\n```\n\n**Subsequent Request with Cursor:**\n\n```json\n{\n  \"name\": \"get_sitemap_pages\",\n  \"arguments\": {\n    \"url\": \"https://example.com\",\n    \"limit\": 50,\n    \"cursor\": \"eyJwYWdlIjoxfQ==\"\n  }\n}\n```\n\nWhen there are no more results, the `nextCursor` field will be absent from the response.\n\n#### Get Sitemap Statistics\n\n```json\n{\n  \"name\": \"get_sitemap_stats\",\n  \"arguments\": {\n    \"url\": \"https://example.com\"\n  }\n}\n```\n\nThe response includes both total statistics and detailed stats for each subsitemap:\n\n```json\n{\n  \"total\": {\n    \"url\": \"https://example.com\",\n    \"page_count\": 150,\n    \"sitemap_count\": 3,\n    \"sitemap_types\": [\"WebsiteSitemap\", \"NewsSitemap\"],\n    \"priority_stats\": {\n      \"min\": 0.1,\n      \"max\": 1.0,\n      \"avg\": 0.65\n    },\n    \"last_modified_count\": 120\n  },\n  \"subsitemaps\": [\n    {\n      \"url\": \"https://example.com/sitemap.xml\",\n      \"type\": \"WebsiteSitemap\",\n      \"page_count\": 100,\n      \"priority_stats\": {\n        \"min\": 0.3,\n        \"max\": 1.0,\n        \"avg\": 0.7\n      },\n      \"last_modified_count\": 80\n    },\n    {\n      \"url\": \"https://example.com/blog/sitemap.xml\",\n      \"type\": \"WebsiteSitemap\",\n      \"page_count\": 50,\n      \"priority_stats\": {\n        \"min\": 0.1,\n        \"max\": 0.9,\n        \"avg\": 0.5\n      },\n      \"last_modified_count\": 40\n    }\n  ]\n}\n```\n\nThis allows MCP clients to understand which subsitemaps might be of interest for further investigation. You can then use the `sitemap_url` parameter in `get_sitemap_pages` to filter pages from a specific subsitemap.\n\n#### Parse Sitemap Content Directly\n\n```json\n{\n  \"name\": \"parse_sitemap_content\",\n  \"arguments\": {\n    \"content\": \"<?xml version=\\\"1.0\\\" encoding=\\\"UTF-8\\\"?><urlset xmlns=\\\"http://www.sitemaps.org/schemas/sitemap/0.9\\\"><url><loc>https://example.com/</loc></url></urlset>\",\n    \"include_pages\": true\n  }\n}\n```\n\n## Acknowledgements\n\n- This MCP Server leverages the [ultimate-sitemap-parser](https://github.com/GateNLP/ultimate-sitemap-parser) library\n- Built using the [Model Context Protocol](https://modelcontextprotocol.io) Python SDK\n\n## License\n\nThis project is licensed under the MIT License. See the [LICENSE](LICENSE) file for details.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "sitemaps",
        "sitemap",
        "mugoosse",
        "mugoosse sitemap",
        "website sitemaps",
        "sitemaps provide"
      ],
      "category": "web-search"
    },
    "mukulkathayat--linkedin-mcp": {
      "owner": "mukulkathayat",
      "name": "linkedin-mcp",
      "url": "https://github.com/mukulkathayat/linkedin-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/mukulkathayat.webp",
      "description": "Integrates with LinkedIn to access profiles, posts, and interactions, providing real-time context for AI applications.",
      "stars": 1,
      "forks": 0,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-05-31T05:23:39Z",
      "readme_content": "a few details, I was actually using this server just for testing purpose, if you want to give it a shot\n\nJust create a Free Account on rapidAPI, Generate an API key\n\nhere is the URL:\n\nhttps://rapidapi.com/mgujjargamingm/api/linkedin-bulk-data-scraper/playground/apiendpoint_eee12490-a5e8-4557-b361-00e726fd3f03\n\nusername - \"usama\"\n\nhost - \"linkedin-bulk-data-scraper.p.rapidapi.co\"\n\napi_key - \"RAPID_API_KEY\"\n\n\nu'll get some free credits, if you want to test it out, i think they give 500 credits,\n",
      "npm_url": "https://www.npmjs.com/package/linkedin-mcp",
      "npm_downloads": 283,
      "keywords": [
        "linkedin",
        "search",
        "web",
        "linkedin mcp",
        "integrates linkedin",
        "mukulkathayat linkedin"
      ],
      "category": "web-search"
    },
    "mzxrai--mcp-webresearch": {
      "owner": "mzxrai",
      "name": "mcp-webresearch",
      "url": "https://github.com/mzxrai/mcp-webresearch",
      "imageUrl": "/freedevtools/mcp/pfp/mzxrai.webp",
      "description": "Integrates real-time web research capabilities by utilizing Google search for obtaining information and extracting content from webpages. It allows for tracking research sessions, including visited pages and search queries, while also enabling screenshot captures.",
      "stars": 281,
      "forks": 69,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-10-02T21:03:39Z",
      "readme_content": "# MCP Web Research Server\n\nA Model Context Protocol (MCP) server for web research. \n\nBring real-time info into Claude and easily research any topic.\n\n## Features\n\n- Google search integration\n- Webpage content extraction\n- Research session tracking (list of visited pages, search queries, etc.)\n- Screenshot capture\n\n## Prerequisites\n\n- [Node.js](https://nodejs.org/) >= 18 (includes `npm` and `npx`)\n- [Claude Desktop app](https://claude.ai/download)\n\n## Installation\n\nFirst, ensure you've downloaded and installed the [Claude Desktop app](https://claude.ai/download) and you have npm installed.\n\nNext, add this entry to your `claude_desktop_config.json` (on Mac, found at `~/Library/Application\\ Support/Claude/claude_desktop_config.json`):\n\n```json\n{\n  \"mcpServers\": {\n    \"webresearch\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@mzxrai/mcp-webresearch@latest\"]\n    }\n  }\n}\n```\n\nThis config allows Claude Desktop to automatically start the web research MCP server when needed.\n\n## Usage\n\nSimply start a chat with Claude and send a prompt that would benefit from web research. If you'd like a prebuilt prompt customized for deeper web research, you can use the `agentic-research` prompt that we provide through this package. Access that prompt in Claude Desktop by clicking the Paperclip icon in the chat input and then selecting `Choose an integration` → `webresearch` → `agentic-research`.\n\n<img src=\"https://i.ibb.co/N6Y3C0q/Screenshot-2024-12-05-at-11-01-27-PM.png\" alt=\"Example screenshot of web research\" width=\"400\"/>\n\n### Tools\n\n1. `search_google`\n   - Performs Google searches and extracts results\n   - Arguments: `{ query: string }`\n\n2. `visit_page`\n   - Visits a webpage and extracts its content\n   - Arguments: `{ url: string, takeScreenshot?: boolean }`\n\n3. `take_screenshot`\n   - Takes a screenshot of the current page\n   - No arguments required\n\n### Prompts\n\n#### `agentic-research`\nA guided research prompt that helps Claude conduct thorough web research. The prompt instructs Claude to:\n- Start with broad searches to understand the topic landscape\n- Prioritize high-quality, authoritative sources\n- Iteratively refine the research direction based on findings\n- Keep you informed and let you guide the research interactively\n- Always cite sources with URLs\n\n### Resources\n\nWe expose two things as MCP resources: (1) captured webpage screenshots, and (2) the research session.\n\n#### Screenshots\n\nWhen you take a screenshot, it's saved as an MCP resource. You can access captured screenshots in Claude Desktop via the Paperclip icon.\n\n#### Research Session\n\nThe server maintains a research session that includes:\n- Search queries\n- Visited pages\n- Extracted content\n- Screenshots\n- Timestamps\n\n### Suggestions\n\nFor the best results, if you choose not to use the `agentic-research` prompt when doing your research, it may be helpful to suggest high-quality sources for Claude to use when researching general topics. For example, you could prompt `news today from reuters or AP` instead of `news today`.\n\n## Problems\n\nThis is very much pre-alpha code. And it is also AIGC, so expect bugs.\n\nIf you run into issues, it may be helpful to check Claude Desktop's MCP logs:\n\n```bash\ntail -n 20 -f ~/Library/Logs/Claude/mcp*.log\n```\n\n## Development\n\n```bash\n# Install dependencies\npnpm install\n\n# Build the project\npnpm build\n\n# Watch for changes\npnpm watch\n\n# Run in development mode\npnpm dev\n```\n\n## Requirements\n\n- Node.js >= 18\n- Playwright (automatically installed as a dependency)\n\n## Verified Platforms\n\n- [x] macOS\n- [ ] Linux\n\n## License\n\nMIT\n\n## Author\n\n[mzxrai](https://github.com/mzxrai) ",
      "npm_url": "https://www.npmjs.com/package/@mzxrai/mcp-webresearch",
      "npm_downloads": 40957,
      "keywords": [
        "webresearch",
        "mzxrai",
        "search",
        "mcp webresearch",
        "webresearch integrates",
        "web search"
      ],
      "category": "web-search"
    },
    "nachoal--perplexity-mcp": {
      "owner": "nachoal",
      "name": "perplexity-mcp",
      "url": "https://github.com/nachoal/perplexity-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/nachoal.webp",
      "description": "Web search functionality using the Perplexity API to retrieve up-to-date information with comprehensive results, including sources and citations, while allowing filtering by time periods.",
      "stars": 8,
      "forks": 1,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-03-16T10:12:32Z",
      "readme_content": "# Perplexity Web Search MCP Server\n\nA simple MCP (Model Context Protocol) server that provides web search functionality using the Perplexity API. This server allows Claude or other MCP-compatible AI assistants to search the web and get up-to-date information.\n\n## Features\n\n- Search the web with Perplexity's powerful search capabilities\n- Get comprehensive search results with sources and citations\n- Filter results by time period (day, week, month, year)\n- Includes a ready-to-use prompt template for web searches\n- Supports loading API key from environment variables or .env file\n\n## Installation\n\n1. Clone this repository\n2. Install dependencies:\n   ```\n   pip install -e .\n   ```\n   or\n   ```\n   uv pip install -e .\n   ```\n\n## Configuration\n\nYou can set the `PERPLEXITY_API_KEY` environment variable with your Perplexity API key:\n\n```bash\nexport PERPLEXITY_API_KEY=\"your-api-key-here\"\n```\n\nAlternatively, you can create a `.env` file in the project root with the following content:\n\n```\nPERPLEXITY_API_KEY=your-api-key-here\n```\n\nA sample `.env.example` file is provided for reference.\n\nTo get a Perplexity API key:\n1. Visit [Perplexity API Settings](https://www.perplexity.ai/settings/api)\n2. Create an account if you don't have one\n3. Generate an API key\n\n## Usage\n\n### Running the server\n\n```bash\npython server.py\n```\n\n### Testing the server\n\nYou can test the server functionality without running the full MCP server using the included test script:\n\n```bash\npython test_server.py \"your search query here\" --recency month\n```\n\nOptions for `--recency` are: day, week, month (default), year\n\n### Using with Claude Desktop\n\n1. Edit your Claude Desktop configuration file:\n   - On macOS: `~/Library/Application\\ Support/Claude/claude_desktop_config.json`\n   - On Windows: `%APPDATA%/Claude/claude_desktop_config.json`\n\n2. Add the following configuration:\n   ```json\n   {\n     \"perplexity-mcp\": {\n       \"env\": {\n         \"PERPLEXITY_API_KEY\": \"your-api-key-here\"\n       },\n       \"command\": \"python\",\n       \"args\": [\n         \"/path/to/server.py\"\n       ]\n     }\n   }\n   ```\n\n3. Restart Claude Desktop\n\n### Example Prompts for Claude\n\n- \"Search the web for the latest news about artificial intelligence\"\n- \"Use Perplexity to find information about climate change published in the past week\"\n- \"Search for recent research papers on quantum computing from the past month\"\n\n## API Reference\n\n### Tool: `search_web(query: str, recency: str = \"month\") -> str`\n\nSearch the web using Perplexity API and return results.\n\n**Parameters:**\n- `query`: The search query string\n- `recency`: Filter results by time period - 'day', 'week', 'month' (default), or 'year'\n\n**Returns:**\nA comprehensive text response containing:\n1. A detailed summary of the search results\n2. Key facts and information found\n3. Sources with URLs for verification\n4. Any conflicting information if present\n\n### Prompt: `web_search_prompt(query: str, recency: str = \"month\") -> str`\n\nCreates a prompt template for searching the web with Perplexity.\n\n**Parameters:**\n- `query`: The search query\n- `recency`: Time period filter - 'day', 'week', 'month' (default), or 'year'\n\n**Returns:**\nA formatted prompt string that instructs the AI to:\n1. Search for the specified query\n2. Focus on results from the specified time period\n3. Summarize key findings\n4. Highlight important facts\n5. Mention conflicting information\n6. Cite sources with links\n\n## License\n\nMIT\n",
      "npm_url": "https://www.npmjs.com/package/perplexity-mcp",
      "npm_downloads": 8172,
      "keywords": [
        "search",
        "perplexity",
        "comprehensive",
        "perplexity api",
        "web search",
        "search functionality"
      ],
      "category": "web-search"
    },
    "nanameru--ai-chat-pitatto": {
      "owner": "nanameru",
      "name": "ai-chat-pitatto",
      "url": "https://github.com/nanameru/ai-chat-pitatto",
      "imageUrl": "/freedevtools/mcp/pfp/null.webp",
      "description": "An AI chatbot built with Next.js that supports advanced routing, server-side rendering, and multi-provider large language model (LLM) integration. It offers secure chat history and user data storage with customizable options using an AI SDK and modern UI components.",
      "stars": 0,
      "forks": 0,
      "license": "Unknown",
      "language": "Unknown",
      "updated_at": "",
      "readme_content": "",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "chatbot",
        "chat",
        "nanameru",
        "ai chatbot",
        "chatbot built",
        "ai chat"
      ],
      "category": "web-search"
    },
    "narphorium--mcp-memex": {
      "owner": "narphorium",
      "name": "mcp-memex",
      "url": "https://github.com/narphorium/mcp-memex",
      "imageUrl": "/freedevtools/mcp/pfp/narphorium.webp",
      "description": "Analyze web content and enhance your knowledge base by extracting information from URLs, storing it as Markdown files for easy access. Integrates seamlessly with Obsidian to facilitate questioning and retrieval of insights from the curated content.",
      "stars": 9,
      "forks": 1,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-05-05T23:05:46Z",
      "readme_content": "# Memex for Model Context Protocol\n\nMemex is a tool for Model Context Protocol (MCP) that allows you to analyze web content and add it to your knowledge base.\n\nThe tool was inspired by the [Memex](docs/as_we_may_think.pdf) project by [Vannevar Bush](https://en.wikipedia.org/wiki/Vannevar_Bush).\n\n## Requirements\n\nYou will need API keys for the following services:\n\n- [Claude API](https://www.anthropic.com/en/claude)\n- [FireCrawl API](https://www.firecrawl.com/)\n- [Voyage API](https://voyageai.com/)\n\nThe knowledge base produced by this tool is stored as Markdown files so they can be viewed with any Markdown viewer but [Obsidian](https://obsidian.md/) is recommended.\n\n## Installation\n\n```bash\npip install mcp-memex\n```\n\nAdd the following to your `claude_desktop_config.json` and replace the placeholders with the actual paths and API keys:\n\n```json\n{\n  \"mcpServers\": {\n    \"memex\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"--directory\",\n        \"PATH_TO_LOCAL_MEMEX_REPO\",\n        \"run\",\n        \"mcp-memex\",\n        \"--index\",\n        \"PATH_TO_MEMEX_INDEX\",\n        \"--workspace\",\n        \"PATH_TO_OBSIDIAN_VAULT\"\n      ],\n      \"env\": {\n        \"ANTHROPIC_API_KEY\": \"YOUR-API-KEY\",\n        \"FIRECRAWL_API_KEY\": \"YOUR-API-KEY\",\n        \"VOYAGE_API_KEY\": \"YOUR-API-KEY\"\n      }\n    }\n  }\n}\n```\n\n## Usage\n\nStart by asking Claude a question with a list of URLs to reference.\n\n```\nWhat is the capital of France? \"https://en.wikipedia.org/wiki/France\"\n```\n\nOnce Claude has finished analyzing the content, you will see the results in your Obsidian vault. You can then ask questions about the content and Memex will use the knowledge base to answer your questions.\n\n```\nWhat is the capital of France?\n```\n\n## Development\n\nTo run the tool locally, you can use the following command:\n\n```bash\nnpx @modelcontextprotocol/inspector \\\n  uv \\\n  --directory PATH_TO_LOCAL_MEMEX_REPO \\\n  run \\\n  mcp-memex \\\n  --index PATH_TO_MEMEX_INDEX \\\n  --workspace PATH_TO_OBSIDIAN_VAULT\n```\n\nThen open the inspector and connect to the server.\n\nhttp://localhost:5173?timeout=30000",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "memex",
        "narphorium",
        "retrieval",
        "search narphorium",
        "analyze web",
        "memex analyze"
      ],
      "category": "web-search"
    },
    "nash-app--nash-mcp": {
      "owner": "nash-app",
      "name": "nash-mcp",
      "url": "https://github.com/nash-app/nash-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/null.webp",
      "description": "Execute shell commands, run Python code, manage reusable tasks, and fetch web content while ensuring secure handling of credentials. Provides error handling for both command execution and Python execution.",
      "stars": 0,
      "forks": 0,
      "license": "Unknown",
      "language": "Unknown",
      "updated_at": "",
      "readme_content": "",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "nash",
        "python",
        "commands",
        "search nash",
        "nash app",
        "app nash"
      ],
      "category": "web-search"
    },
    "natebennett27--mcp-maigret": {
      "owner": "natebennett27",
      "name": "mcp-maigret",
      "url": "https://github.com/natebennett27/mcp-maigret",
      "imageUrl": "/freedevtools/mcp/pfp/null.webp",
      "description": "Collect user account information from various social networks and analyze URLs to gather public data, enhancing research capabilities for OSINT investigations.",
      "stars": 0,
      "forks": 0,
      "license": "Unknown",
      "language": "Unknown",
      "updated_at": "",
      "readme_content": "",
      "npm_url": "https://www.npmjs.com/package/mcp-maigret",
      "npm_downloads": 5622,
      "keywords": [
        "search",
        "urls",
        "web",
        "analyze urls",
        "social networks",
        "search natebennett27"
      ],
      "category": "web-search"
    },
    "neoparad--serp-api-server": {
      "owner": "neoparad",
      "name": "serp-api-server",
      "url": "https://github.com/neoparad/serp-api-server",
      "imageUrl": "/freedevtools/mcp/pfp/neoparad.webp",
      "description": "Access real-time search engine results and data through a straightforward API. Retrieve and analyze search data from various search engines to enhance applications with powerful search capabilities.",
      "stars": 0,
      "forks": 0,
      "license": "No License",
      "language": "JavaScript",
      "updated_at": "2025-03-21T22:39:45Z",
      "readme_content": "# serp-api-server",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "api",
        "search",
        "serp",
        "serp api",
        "search engine",
        "search neoparad"
      ],
      "category": "web-search"
    },
    "newerton--mcp-mercado-livre": {
      "owner": "newerton",
      "name": "mcp-mercado-livre",
      "url": "https://github.com/newerton/mcp-mercado-livre",
      "imageUrl": "/freedevtools/mcp/pfp/newerton.webp",
      "description": "Scrapes product data, including prices and availability, directly from Mercado Livre. Integrates with Mercado Livre's API to provide reliable and up-to-date marketplace information.",
      "stars": 1,
      "forks": 2,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-08-31T17:28:35Z",
      "readme_content": "<div align=\"center\">\n\n# Mercado Livre - MCP Server\n\nThis project is part of the Model Context Protocol (MCP) ecosystem and provides tools for integrating with external sources and managing specific domain models. It is designed to demonstrate how to build an MCP server that scrapes products from Mercado Livre, with strong data validation to ensure reliability.\n\n</div>\n\n<table style=\"border-collapse: collapse; width: 100%; table-layout: fixed;\">\n<tr>\n<td style=\"width: 40%; padding: 15px; vertical-align: middle; border: none;\">An integration that enables MCP tools to scrape product data, such as prices and availability, directly from Mercado Livre.</td>\n<td style=\"width: 60%; padding: 0; vertical-align: middle; border: none; min-width: 300px; text-align: center;\"><a href=\"https://glama.ai/mcp/servers/@newerton/mcp-mercado-livre\">\n  <img style=\"max-width: 100%; height: auto; min-width: 300px;\" src=\"https://glama.ai/mcp/servers/@newerton/mcp-mercado-livre/badge\" alt=\"Mercado Livre - MCP Server\" />\n</a></td>\n</tr>\n</table>\n\n## Table of Contents\n\n- [Features](#features)\n- [Architecture](#architecture)\n- [Installation](#installation)\n- [MCP Server Configuration in VSCode](#mcp-server-configuration-in-vscode)\n- [MCP Server Output in VSCode](#mcp-server-output-in-vscode)\n- [Contribution](#contribution)\n- [License](#license)\n\n## Features\n\n- **get-produtos**: Fetch basic product information.\n- Input validation using [Zod](https://github.com/colinhacks/zod).\n- Integration with the Mercado Livre API using `fetch` (infrastructure layer).\n\n## Architecture\n\nThe project follows a layered architecture inspired by **Domain-Driven Design** (DDD) patterns:\n\n- **Domain** (`src/domain`):\n  Defines interfaces and types that represent data structures (e.g., `Mercado Livre`).\n\n- **Infrastructure** (`src/infrastructure`):\n  Implements external services, such as `MercadoLivreApiService`, responsible for making HTTP calls to the Mercado Livre API.\n\n- **Application** (`src/application`):\n  Contains business logic in `MercadoLivreService`, which processes and formats data from the infrastructure.\n\n- **Interface** (`src/interface`):\n  Includes controllers (`MercadoLivreToolsController`) that register tools in the MCP server, define validation schemas, and return results.\n\n- **Entry Point** (`src/main.ts`):\n  Initializes the `McpServer`, configures the transport (`StdioServerTransport`), instantiates services and controllers, and starts listening on _stdio_.\n\nThe folder structure is as follows:\n```\nsrc/\n├── domain/\n│   └── models/           # Domain interfaces\n├── infrastructure/\n│   └── services/         # External API implementations (Mercado Livre)\n├── application/\n│   └── services/         # Business logic and data formatting\n├── interface/\n│   └── controllers/      # MCP tool registration and validation\n└── main.ts               # Server entry point\nbuild/                    # Compiled JavaScript code\n.vscode/                  # Contains the mcp.json file, MCP Server config\n```\n\n## Installation\n\n```bash\ngit clone git@github.com:newerton/mcp-mercado-livre.git\ncd mcp-mercado-livre\nnpm install\nnpm run build\n```\n\n## MCP Server Configuration in VSCode\n\n1. Press `Ctrl+Shift+P` and select \"MCP: List Servers\"\n2. Select \"products\" and then \"Start Server\"\n\n## MCP Server Output in VSCode\n\n1. Press `Ctrl+Shift+P` and select \"MCP: List Servers\"\n2. Select \"products\" and then \"Show Output\"\n\n## Contribution\n\nPull requests are welcome! Feel free to open issues and discuss improvements.\n\n## License\n\nThis project is licensed under the MIT license - see the [LICENSE](https://github.com/imprvhub/mcp-claude-hackernews/blob/main/LICENSE) file for details.\n\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "mercado",
        "newerton",
        "livre",
        "livre api",
        "search newerton",
        "mcp mercado"
      ],
      "category": "web-search"
    },
    "ngoiyaeric--mapbox-mcp-server": {
      "owner": "ngoiyaeric",
      "name": "mapbox-mcp-server",
      "url": "https://github.com/ngoiyaeric/mapbox-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/ngoiyaeric.webp",
      "description": "Integrate navigation and geocoding services using the Mapbox API to access directions, travel matrices, and real-time mapping functionalities.",
      "stars": 0,
      "forks": 0,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-04-08T07:40:49Z",
      "readme_content": "<!--\n * @Author: AidenYangX\n * @Email: xscs709560271@gmail.com\n * @Date: 2024-12-21 23:30:55\n * @Description: Mapbox MCP Server\n-->\n\n# Mapbox MCP Server\n\n[![smithery badge](https://smithery.ai/badge/mapbox-mcp-server)](https://smithery.ai/server/mapbox-mcp-server)\n\nMCP Server for the Mapbox API.\n\n## Features\n\n### Navigation Tools\n\n1. `mapbox_directions`\n\n   - Get directions between coordinates\n   - Inputs:\n     - `coordinates` ({latitude: number, longitude: number}[])\n     - `profile` (optional): \"driving-traffic\", \"driving\", \"walking\", \"cycling\"\n   - Returns: route details with steps, distance, duration\n\n2. `mapbox_directions_by_places`\n\n   - Get directions between places using their names\n   - Inputs:\n     - `places` (string[]): Array of place names\n     - `profile` (optional): \"driving-traffic\", \"driving\", \"walking\", \"cycling\"\n     - `language` (optional): Two-letter language code (e.g., \"zh\", \"en\")\n   - Returns:\n     - Geocoding results for each place\n     - Route details with steps, distance, duration\n     - Any errors that occurred during processing\n\n3. `mapbox_matrix`\n\n   - Calculate travel time and distance matrices between coordinates\n   - Inputs:\n     - `coordinates` ({latitude: number, longitude: number}[])\n     - `profile` (optional): \"driving\", \"walking\", \"cycling\"\n     - `annotations` (optional): \"duration\", \"distance\", \"duration,distance\"\n     - `sources` (optional): Indices of source coordinates\n     - `destinations` (optional): Indices of destination coordinates\n   - Returns: Matrix of durations and/or distances between points\n\n4. `mapbox_matrix_by_places`\n   - Calculate travel time and distance matrices between places using their names\n   - Inputs:\n     - `places` (string[]): Array of place names (2-25 places)\n     - `profile` (optional): \"driving\", \"walking\", \"cycling\"\n     - `annotations` (optional): \"duration\", \"distance\", \"duration,distance\"\n     - `language` (optional): Two-letter language code\n     - `sources` (optional): Indices of source places\n     - `destinations` (optional): Indices of destination places\n   - Returns:\n     - Geocoding results for each place\n     - Matrix of durations and/or distances\n     - Any errors that occurred during processing\n\n### Search Tools\n\n1. `mapbox_geocoding`\n   - Search for places and convert addresses into coordinates\n   - Inputs:\n     - `searchText` (string): The place or address to search for\n     - `limit` (optional): Maximum number of results (1-10)\n     - `types` (optional): Filter by place types (country, region, place, etc.)\n     - `language` (optional): Two-letter language code\n     - `fuzzyMatch` (optional): Enable/disable fuzzy matching\n   - Returns: Detailed location information including coordinates and properties\n\n## Claude Desktop Integration\n\nAdd this configuration to your Claude Desktop config file (typically located at `~/Library/Application Support/Claude/claude_desktop_config.json`):\n\n```json\n{\n  \"mcpServers\": {\n    \"mapbox-mcp-server\": {\n      \"command\": \"node\",\n      \"args\": [\"/absolute/path/to/mapbox-mcp-server/build/index.js\"],\n      \"env\": {\n        \"MAPBOX_ACCESS_TOKEN\": \"your-api-key\"\n      }\n    }\n  }\n}\n```\n\n## Setup\n\n### Installing via Smithery\n\nTo install mapbox-mcp-server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@ngoiyaeric/mapbox-mcp-server):\n\n```bash\nnpx -y @smithery/cli install mapbox-mcp-server --client claude\n```\n\n### Prerequisites\n\n- Node.js 16 or higher\n- TypeScript 4.5 or higher\n- A valid Mapbox API key\n\n### API Key\n\nGet a Mapbox API key by following the instructions [here](https://console.mapbox.com/account/access-tokens/).\n\nSet your API key as an environment variable:\n\n```bash\nexport MAPBOX_ACCESS_TOKEN=your_api_key_here\n```\n\n## Rate Limits\n\n- Directions API: 300 requests per minute\n- Matrix API:\n  - 60 requests per minute for driving/walking/cycling\n  - 30 requests per minute for driving-traffic\n- Geocoding API: 600 requests per minute\n\n## Deployment\n\n### Structure\n\nIn mapbox-mcp-server, we use the following structure to manage the server's handlers:\n\n- `src/server/handlers/base.ts`: Base class for all handlers\n- `src/server/registry.ts`: Registry for all handlers\n- `src/server/main.ts`: Main entry point for the server\n\nEach feature module follows this structure:\n\n```plaintext\nsrc/\n├── types/          # Type definitions\n├── schemas/        # Zod schemas for validation\n├── tools/\n│   ├── definitions/  # Tool definitions\n│   └── handlers/     # Tool implementations\n└── server/\n    └── handlers/     # Handler classes\n```\n\n---\n\n**Class Diagram**:\n\n\n---\n\n**Process Diagram**:\n\n\n## Error Handling\n\nAll tools implement comprehensive error handling:\n\n- Input validation errors\n- API request failures\n- Rate limit errors\n- Service-specific errors (e.g., no routes found, invalid coordinates)\n\n## License\n\nThis MCP server is licensed under the MIT License. This means you are free to use, modify, and distribute the software, subject to the terms and conditions of the MIT License. For more details, please see the LICENSE file in the project repository.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "mapbox",
        "geocoding",
        "navigation",
        "ngoiyaeric mapbox",
        "mapbox api",
        "using mapbox"
      ],
      "category": "web-search"
    },
    "ngoiyaeric--osm-mcp": {
      "owner": "ngoiyaeric",
      "name": "osm-mcp",
      "url": "https://github.com/ngoiyaeric/osm-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/ngoiyaeric.webp",
      "description": "Query and visualize map data using a web interface with real-time map interactions. Integrates PostgreSQL/PostGIS for OpenStreetMap data management and supports server-to-client communication.",
      "stars": 0,
      "forks": 0,
      "license": "No License",
      "language": "",
      "updated_at": "2025-04-08T07:42:32Z",
      "readme_content": "# MCP-OSM: OpenStreetMap Integration for MCP\n\nThis package provides OpenStreetMap integration for MCP, allowing users to query\nand visualize map data through an MCP interface.\n\n[](osm-mcp.webp)\n\n## Features\n\n- Web-based map viewer using Leaflet and OpenStreetMap\n- Server-to-client communication via Server-Sent Events (SSE)\n- MCP tools for map control (adding markers, polygons, setting view, getting view)\n- PostgreSQL/PostGIS query interface for OpenStreetMap data\n\n## Installation\n\nThis is my `claude_desktop_config.json`:\n```json\n{\n  \"mcpServers\": {\n    \"OSM PostgreSQL Server\": {\n      \"command\": \"/Users/wiseman/.local/bin/uv\",\n      \"args\": [\n        \"run\",\n        \"--env-file\",\n        \".env\",\n        \"--with\",\n        \"mcp[cli]\",\n        \"--with\",\n        \"psycopg2\",\n        \"--with-editable\",\n        \"/Users/wiseman/src/mcp-osm\",\n        \"--directory\",\n        \"/Users/wiseman/src/mcp-osm\",\n        \"mcp\",\n        \"run\",\n        \"mcp.py\"\n      ]\n    }\n  }\n}\n```\n\nWhen the MCP server starts it also starts a web server at http://localhost:8889/\nthat has the map interface.\n\n### Environment Variables\n\nThe following environment variables can be used to configure the MCP:\n\n- `FLASK_HOST` - Host for the Flask server (default: 127.0.0.1)\n- `FLASK_PORT` - Port for the Flask server (default: 8889)\n- `PGHOST` - PostgreSQL host (default: localhost)\n- `PGPORT` - PostgreSQL port (default: 5432)\n- `PGDB` - PostgreSQL database name (default: osm)\n- `PGUSER` - PostgreSQL username (default: postgres)\n- `PGPASSWORD` - PostgreSQL password (default: postgres)\n\n### MCP Tools\n\nThe following MCP tools are available:\n\n- `get_map_view` - Get the current map view\n- `set_map_view` - Set the map view to specific coordinates or bounds\n- `set_map_title` - Set the title displayed at the bottom right of the map\n- `add_map_marker` - Add a marker at specific coordinates\n- `add_map_line` - Add a line defined by a set of coordinates\n- `add_map_polygon` - Add a polygon defined by a set of coordinates\n- `query_osm_postgres` - Execute a SQL query against the OpenStreetMap database",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "openstreetmap",
        "osm",
        "map",
        "openstreetmap data",
        "postgis openstreetmap",
        "map data"
      ],
      "category": "web-search"
    },
    "nicholmikey--chrome-tools-MCP": {
      "owner": "nicholmikey",
      "name": "chrome-tools-MCP",
      "url": "https://github.com/nicholmikey/chrome-tools-MCP",
      "imageUrl": "/freedevtools/mcp/pfp/nicholmikey.webp",
      "description": "Control Chrome tabs, execute JavaScript, capture screenshots, and monitor network traffic. Enhance browser automation capabilities by integrating directly with Chrome's DevTools Protocol.",
      "stars": 48,
      "forks": 11,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-10-03T09:13:53Z",
      "readme_content": "# Chrome Tools MCP Server\n\nAn MCP server that provides tools for interacting with Chrome through its DevTools Protocol. This server enables remote control of Chrome tabs, including executing JavaScript, capturing screenshots, monitoring network traffic, and more.\n\n## Why use an MCP server like this?\nThis type of MCP Server is useful When you need to manually configure your browser to be in a certain state before you let an AI tool like Cline poke at it. You can also use this tool to listen to and pull network events into its context. \n\n## Features\n\n- List Chrome tabs\n- Execute JavaScript in tabs\n- Capture screenshots\n- Monitor network traffic\n- Navigate tabs to URLs\n- Query DOM elements\n- Click elements with console output capture\n\n## Installation\n\n```bash\nnpm install @nicholmikey/chrome-tools\n```\n\n## Configuration\n\nThe server can be configured through environment variables in your MCP settings:\n\n```json\n{\n  \"chrome-tools\": {\n    \"command\": \"node\",\n    \"args\": [\"path/to/chrome-tools/dist/index.js\"],\n    \"env\": {\n      \"CHROME_DEBUG_URL\": \"http://localhost:9222\",\n      \"CHROME_CONNECTION_TYPE\": \"direct\",\n      \"CHROME_ERROR_HELP\": \"custom error message\"\n    }\n  }\n}\n```\n\n### Environment Variables\n\n- `CHROME_DEBUG_URL`: The URL where Chrome's remote debugging interface is available (default: http://localhost:9222)\n- `CHROME_CONNECTION_TYPE`: Connection type identifier for logging (e.g., \"direct\", \"ssh-tunnel\", \"docker\")\n- `CHROME_ERROR_HELP`: Custom error message shown when connection fails\n\n## Setup Guide\n\n### Native Setup (Windows/Mac/Linux)\n\n1. Launch Chrome with remote debugging enabled:\n   ```bash\n   # Windows\n   \"C:\\Program Files\\Google\\Chrome\\Application\\chrome.exe\" --remote-debugging-port=9222\n\n   # Mac\n   /Applications/Google\\ Chrome.app/Contents/MacOS/Google\\ Chrome --remote-debugging-port=9222\n\n   # Linux\n   google-chrome --remote-debugging-port=9222\n   ```\n\n2. Configure MCP settings:\n   ```json\n   {\n     \"env\": {\n       \"CHROME_DEBUG_URL\": \"http://localhost:9222\",\n       \"CHROME_CONNECTION_TYPE\": \"direct\"\n     }\n   }\n   ```\n\n### WSL Setup\n\nWhen running in WSL, you'll need to set up an SSH tunnel to connect to Chrome running on Windows:\n\n1. Launch Chrome on Windows with remote debugging enabled\n2. Create an SSH tunnel:\n   ```bash\n   ssh -N -L 9222:localhost:9222 windowsuser@host\n   ```\n3. Configure MCP settings:\n   ```json\n   {\n     \"env\": {\n       \"CHROME_DEBUG_URL\": \"http://localhost:9222\",\n       \"CHROME_CONNECTION_TYPE\": \"ssh-tunnel\",\n       \"CHROME_ERROR_HELP\": \"Make sure the SSH tunnel is running: ssh -N -L 9222:localhost:9222 windowsuser@host\"\n     }\n   }\n   ```\n\n### Docker Setup\n\nWhen running Chrome in Docker:\n\n1. Launch Chrome container:\n   ```bash\n   docker run -d --name chrome -p 9222:9222 chromedp/headless-shell\n   ```\n\n2. Configure MCP settings:\n   ```json\n   {\n     \"env\": {\n       \"CHROME_DEBUG_URL\": \"http://localhost:9222\",\n       \"CHROME_CONNECTION_TYPE\": \"docker\"\n     }\n   }\n   ```\n\n## Tools\n\n### list_tabs\nLists all available Chrome tabs.\n\n### execute_script\nExecutes JavaScript code in a specified tab.\nParameters:\n- `tabId`: ID of the Chrome tab\n- `script`: JavaScript code to execute\n\n### capture_screenshot\nCaptures a screenshot of a specified tab, automatically optimizing it for AI model consumption.\nParameters:\n- `tabId`: ID of the Chrome tab\n- `format`: Image format (jpeg/png) - Note: This is only for initial capture. Final output uses WebP with PNG fallback\n- `quality`: JPEG quality (1-100) - Note: For initial capture only\n- `fullPage`: Capture full scrollable page\n\nImage Processing:\n1. WebP Optimization (Primary Format):\n   - First attempt: WebP with quality 80 and high compression effort\n   - Second attempt: WebP with quality 60 and near-lossless compression if first attempt exceeds 1MB\n2. PNG Fallback:\n   - Only used if WebP processing fails\n   - Includes maximum compression and color palette optimization\n3. Size Constraints:\n   - Maximum dimensions: 900x600 (maintains aspect ratio)\n   - Maximum file size: 1MB\n   - Progressive size reduction if needed\n\n### capture_network_events\nMonitors and captures network events from a specified tab.\nParameters:\n- `tabId`: ID of the Chrome tab\n- `duration`: Duration in seconds to capture\n- `filters`: Optional type and URL pattern filters\n\n### load_url\nNavigates a tab to a specified URL.\nParameters:\n- `tabId`: ID of the Chrome tab\n- `url`: URL to load\n\n### query_dom_elements\nQueries and retrieves detailed information about DOM elements matching a CSS selector.\nParameters:\n- `tabId`: ID of the Chrome tab\n- `selector`: CSS selector to find elements\nReturns:\n- Array of DOM elements with properties including:\n  - `nodeId`: Unique identifier for the node\n  - `tagName`: HTML tag name\n  - `textContent`: Text content of the element\n  - `attributes`: Object containing all element attributes\n  - `boundingBox`: Position and dimensions of the element\n  - `isVisible`: Whether the element is visible\n  - `ariaAttributes`: ARIA attributes for accessibility\n\n### click_element\nClicks on a DOM element and captures any console output triggered by the click.\nParameters:\n- `tabId`: ID of the Chrome tab\n- `selector`: CSS selector to find the element to click\nReturns:\n- Object containing:\n  - `message`: Success/failure message\n  - `consoleOutput`: Array of console messages triggered by the click\n\n## License\n\nMIT\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "chrome",
        "browser",
        "devtools",
        "chrome tools",
        "browser automation",
        "chrome devtools"
      ],
      "category": "web-search"
    },
    "nickclyde--duckduckgo-mcp-server": {
      "owner": "nickclyde",
      "name": "duckduckgo-mcp-server",
      "url": "https://github.com/nickclyde/duckduckgo-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/nickclyde.webp",
      "description": "Facilitates web search capabilities via DuckDuckGo, offering content fetching and parsing with a focus on formatting results for large language model integration. Includes advanced rate limiting features for both search requests and content retrieval.",
      "stars": 510,
      "forks": 110,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-10-02T17:09:47Z",
      "readme_content": "# DuckDuckGo Search MCP Server\n\n[![smithery badge](https://smithery.ai/badge/@nickclyde/duckduckgo-mcp-server)](https://smithery.ai/server/@nickclyde/duckduckgo-mcp-server)\n\nA Model Context Protocol (MCP) server that provides web search capabilities through DuckDuckGo, with additional features for content fetching and parsing.\n\n<a href=\"https://glama.ai/mcp/servers/phcus2gcpn\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/phcus2gcpn/badge\" alt=\"DuckDuckGo Server MCP server\" />\n</a>\n\n## Features\n\n- **Web Search**: Search DuckDuckGo with advanced rate limiting and result formatting\n- **Content Fetching**: Retrieve and parse webpage content with intelligent text extraction\n- **Rate Limiting**: Built-in protection against rate limits for both search and content fetching\n- **Error Handling**: Comprehensive error handling and logging\n- **LLM-Friendly Output**: Results formatted specifically for large language model consumption\n\n## Installation\n\n### Installing via Smithery\n\nTo install DuckDuckGo Search Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@nickclyde/duckduckgo-mcp-server):\n\n```bash\nnpx -y @smithery/cli install @nickclyde/duckduckgo-mcp-server --client claude\n```\n\n### Installing via `uv`\n\nInstall directly from PyPI using `uv`:\n\n```bash\nuv pip install duckduckgo-mcp-server\n```\n\n## Usage\n\n### Running with Claude Desktop\n\n1. Download [Claude Desktop](https://claude.ai/download)\n2. Create or edit your Claude Desktop configuration:\n   - On macOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\n   - On Windows: `%APPDATA%\\Claude\\claude_desktop_config.json`\n\nAdd the following configuration:\n\n```json\n{\n    \"mcpServers\": {\n        \"ddg-search\": {\n            \"command\": \"uvx\",\n            \"args\": [\"duckduckgo-mcp-server\"]\n        }\n    }\n}\n```\n\n3. Restart Claude Desktop\n\n### Development\n\nFor local development, you can use the MCP CLI:\n\n```bash\n# Run with the MCP Inspector\nmcp dev server.py\n\n# Install locally for testing with Claude Desktop\nmcp install server.py\n```\n## Available Tools\n\n### 1. Search Tool\n\n```python\nasync def search(query: str, max_results: int = 10) -> str\n```\n\nPerforms a web search on DuckDuckGo and returns formatted results.\n\n**Parameters:**\n- `query`: Search query string\n- `max_results`: Maximum number of results to return (default: 10)\n\n**Returns:**\nFormatted string containing search results with titles, URLs, and snippets.\n\n### 2. Content Fetching Tool\n\n```python\nasync def fetch_content(url: str) -> str\n```\n\nFetches and parses content from a webpage.\n\n**Parameters:**\n- `url`: The webpage URL to fetch content from\n\n**Returns:**\nCleaned and formatted text content from the webpage.\n\n## Features in Detail\n\n### Rate Limiting\n\n- Search: Limited to 30 requests per minute\n- Content Fetching: Limited to 20 requests per minute\n- Automatic queue management and wait times\n\n### Result Processing\n\n- Removes ads and irrelevant content\n- Cleans up DuckDuckGo redirect URLs\n- Formats results for optimal LLM consumption\n- Truncates long content appropriately\n\n### Error Handling\n\n- Comprehensive error catching and reporting\n- Detailed logging through MCP context\n- Graceful degradation on rate limits or timeouts\n\n## Contributing\n\nIssues and pull requests are welcome! Some areas for potential improvement:\n\n- Additional search parameters (region, language, etc.)\n- Enhanced content parsing options\n- Caching layer for frequently accessed content\n- Additional rate limiting strategies\n\n## License\n\nThis project is licensed under the MIT License.",
      "npm_url": "https://www.npmjs.com/package/duckduckgo-mcp-server",
      "npm_downloads": 10136,
      "keywords": [
        "duckduckgo",
        "retrieval",
        "search",
        "web search",
        "content retrieval",
        "duckduckgo mcp"
      ],
      "category": "web-search"
    },
    "nitish-raj--searxng-mcp-bridge": {
      "owner": "nitish-raj",
      "name": "searxng-mcp-bridge",
      "url": "https://github.com/nitish-raj/searxng-mcp-bridge",
      "imageUrl": "/freedevtools/mcp/pfp/nitish-raj.webp",
      "description": "Connect to a SearXNG instance to perform search queries via the Model Context Protocol (MCP), facilitating integration with compatible AI clients.",
      "stars": 5,
      "forks": 1,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-10-02T05:03:38Z",
      "readme_content": "# SearXNG MCP Bridge Server\n\n[![Release](https://github.com/nitish-raj/searxng-mcp-bridge/actions/workflows/release.yml/badge.svg)](https://github.com/nitish-raj/searxng-mcp-bridge/actions/workflows/release.yml)\n[![smithery badge](https://smithery.ai/badge/@nitish-raj/searxng-mcp-bridge)](https://smithery.ai/server/@nitish-raj/searxng-mcp-bridge)\n\nThis is a Model Context Protocol (MCP) server that acts as a bridge to a [SearXNG](https://github.com/searxng/searxng) instance. It allows compatible clients to perform searches using a configured SearXNG instance via MCP tools.\n\n## Quick Start (Using from npm)\n\n1. **Set up a SearXNG instance**:\n   ```bash\n   # Using Docker\n   docker run -d -p 8888:8080 --name searxng searxng/searxng\n   ```\n\n2. **Install and run the MCP bridge**\n\n   Default (STDIO, unchanged):\n   ```bash\n   # Run directly with npx (default - stdio transport)\n   npx -y @nitish-raj/searxng-mcp-bridge\n   ```\n\n   Optional: Run as an HTTP server (new, opt-in)\n   ```bash\n    # Using env variables (recommended)\n     TRANSPORT=http PORT=3002 HOST=127.0.0.1 SEARXNG_INSTANCE_URL=http://localhost:8080 npx -y @nitish-raj/searxng-mcp-bridge\n\n   # Or use the CLI flag form\n   npx -y @nitish-raj/searxng-mcp-bridge --transport=http\n\n   # Or run the built bundle\n   TRANSPORT=http node build/index.js\n   ```\n\n3. **Configure in your MCP settings file** (stdio / legacy clients)\n   Add to your MCP settings file (e.g., `~/.vscode-server/.../mcp_settings.json`):\n   ```json\n   {\n     \"mcpServers\": {\n       \"searxng-bridge\": {\n         \"command\": \"npx\",\n         \"args\": [\n          \"-y\",\n          \"@nitish-raj/searxng-mcp-bridge\"\n          ],\n         \"env\": {\n           \"SEARXNG_INSTANCE_URL\": \"YOUR_SEARXNG_INSTANCE_URL\"\n         },\n         \"disabled\": false\n       }\n     }\n   }\n   ```\n\n**Smithery Configuration**: When using Smithery, you can set `transport: \"http\"` in the Smithery config to run the bridge over HTTP instead of stdio. Smithery will set `TRANSPORT` in the process environment when launching the bridge.\n\n## Features\n\n* **Search Tool**: Perform web searches using SearXNG with configurable parameters\n* **Health Check**: Monitor SearXNG instance connectivity and performance\n* **Dual Transport**: Supports both STDIO (default) and HTTP transports\n* **Session Management**: HTTP transport includes session-based connections\n* **CORS Support**: Proper cross-origin headers for web client integration\n* **Rate Limiting**: Built-in protection against excessive requests (HTTP mode)\n\n## Configuration\n\n- `SEARXNG_INSTANCE_URL` — REQUIRED. The full URL of the SearXNG instance (e.g., `http://localhost:8080`).\n - `TRANSPORT` — Transport protocol: `stdio` (default) or `http`\n - `PORT` — HTTP server port. Default: `3000` (use `3002` for development, Smithery uses `8081`)\n - `HOST` — Server bind address. Default: `127.0.0.1` (use `0.0.0.0` for containers)\n - `CORS_ORIGIN` — Comma-separated list of allowed origins for CORS. Default: localhost:3002 (development) or `*` (production for Smithery)\n - `MCP_HTTP_BEARER` — Optional bearer token for HTTP authentication\n **HTTP Transport Features**:\n- Session management with `mcp-session-id` headers\n- Secure CORS with origin whitelist validation\n- Rate limiting (100 requests/minute per IP)\n- Optional bearer authentication via `MCP_HTTP_BEARER`\n- DNS rebinding protection\n\n**Security Notes**:\n- CORS uses secure whitelist in development (localhost:3002 only)\n- Production reflects specific origins for credentialed requests (CORS-compliant)\n- Set `CORS_ORIGIN` to customize allowed origins for your use case\n- Set `TRANSPORT=stdio` to revert to stdio mode\n\n## HTTP Transport\n\nThe HTTP transport implements the MCP Streamable HTTP specification (2025-03-26) with the following endpoints:\n\n**MCP Endpoints**:\n- `POST /mcp` - Send MCP requests\n- `GET /mcp` - Server-Sent Events for notifications  \n- `DELETE /mcp` - Terminate sessions\n- `OPTIONS /mcp` - CORS preflight requests\n\n**System Endpoints**:\n- `GET /healthz` - Health check and status\n\n**Test HTTP endpoint**:\n```bash\ncurl -X POST http://localhost:3002/mcp \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"jsonrpc\":\"2.0\",\"id\":1,\"method\":\"tools/list\",\"params\":{}}'\n```\n\nThis returns a JSON-RPC response with the list of available tools (`search` and `health_check`).\n\n## Using Smithery\n\nSmithery provides container deployment with automatic HTTP transport configuration. Install via Smithery:\n\n```bash\nnpx -y @smithery/cli install @nitish-raj/searxng-mcp-bridge --client claude\n```\n\nSmithery automatically handles:\n- Container deployment with HTTP transport\n- Port configuration (uses 8081 by default)\n- Environment variable management\n- Secure CORS configuration for web clients\n\n## Docker\n\nThe Dockerfile exposes port `8081` for Smithery compatibility (HTTP transport). To run the container and allow HTTP access:\n```bash\n# Build (example)\ndocker build -t searxng-mcp-bridge .\n\n# Run mapping port 8081 (Smithery default)\n docker run -d -p 8081:8081 --env SEARXNG_INSTANCE_URL=http://localhost:8080 --name searxng-mcp-bridge searxng-mcp-bridge\n\n# To run HTTP transport inside container:\n docker run -d -p 8081:8081 -e TRANSPORT=http -e PORT=8081 -e SEARXNG_INSTANCE_URL=http://localhost:8080 searxng-mcp-bridge\n```\n\nNote: when containerized set `HOST=0.0.0.0` or rely on the default exposed port mapping. Port 8081 is used for Smithery deployment compatibility.\n\n## Usage\n\n**STDIO Clients**: Use the tool unchanged - no configuration changes required.\n\n**HTTP Clients**: Connect to `http://localhost:3002/mcp` (development port) and send MCP JSON-RPC requests.\n\n**Smithery**: Smithery handles all configuration automatically.\n\n## Development\n\n* `npm install`: Install dependencies.\n* `npm run build`: Compile TypeScript to JavaScript.\n* `npm run watch`: Watch for changes and rebuild automatically.\n* `npm run inspector`: Run the MCP inspector to test the server.\n\n## Migration & Compatibility\n\n**Backward Compatibility**: \n- STDIO remains the default transport - existing users need no changes\n- All tool names, parameters, and responses remain unchanged\n- Configuration is opt-in via environment variables\n\n**Migration to HTTP**:\n- Set `TRANSPORT=http` to enable HTTP transport\n- Configure `PORT` and `HOST` as needed\n- Update client to use HTTP endpoint instead of stdio\n\n**Rollback**:\n- Set `TRANSPORT=stdio` or omit the variable to return to stdio\n",
      "npm_url": "https://www.npmjs.com/package/@nitish-raj/searxng-mcp-bridge",
      "npm_downloads": 0,
      "keywords": [
        "searxng",
        "search",
        "mcp",
        "searxng mcp",
        "connect searxng",
        "searxng instance"
      ],
      "category": "web-search"
    },
    "nkapila6--mcp-local-rag": {
      "owner": "nkapila6",
      "name": "mcp-local-rag",
      "url": "https://github.com/nkapila6/mcp-local-rag",
      "imageUrl": "/freedevtools/mcp/pfp/nkapila6.webp",
      "description": "Performs local retrieval-augmented generation (RAG) searches on input queries by fetching and processing search results to enhance the responses of AI models. Utilizes embeddings for similarity computation to extract relevant context from retrieved data.",
      "stars": 83,
      "forks": 16,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-10-01T12:41:08Z",
      "readme_content": "<a href='https://github.com/nkapila6/mcp-local-rag/'></a>\n\n<!-- omit from toc -->\n# mcp-local-rag\n\"primitive\" RAG-like web search model context protocol (MCP) server that runs locally. ✨ no APIs ✨\n\n```mermaid\n%%{init: {'theme': 'base'}}%%\nflowchart TD\n    A[User] -->|1.Submits LLM Query| B[Language Model]\n    B -->|2.Sends Query| C[mcp-local-rag Tool]\n    \n    subgraph mcp-local-rag Processing\n    C -->|Search DuckDuckGo| D[Fetch 10 search results]\n    D -->|Fetch Embeddings| E[Embeddings from Google's MediaPipe Text Embedder]\n    E -->|Compute Similarity| F[Rank Entries Against Query]\n    F -->|Select top k results| G[Context Extraction from URL]\n    end\n    \n    G -->|Returns Markdown from HTML content| B\n    B -->|3.Generated response with context| H[Final LLM Output]\n    H -->|5.Present result to user| A\n\n    classDef default stroke:#333,stroke-width:2px;\n    classDef process stroke:#333,stroke-width:2px;\n    classDef input stroke:#333,stroke-width:2px;\n    classDef output stroke:#333,stroke-width:2px;\n\n    class A input;\n    class B,C process;\n    class G output;\n```\n\n# Installation\nLocate your MCP config path [here](https://modelcontextprotocol.io/quickstart/user) or check your MCP client settings. \n\n### Run Directly via `uvx`\nThis is the easiest and quickest method. You need to install [uv](https://docs.astral.sh/uv/) for this to work. <br>\nAdd this to your MCP server configuration:\n```json\n{\n  \"mcpServers\": {\n    \"mcp-local-rag\":{\n      \"command\": \"uvx\",\n        \"args\": [\n          \"--python=3.10\",\n          \"--from\",\n          \"git+https://github.com/nkapila6/mcp-local-rag\",\n          \"mcp-local-rag\"\n        ]\n      }\n  }\n}\n```\n\n### Using Docker (recommended)\nEnsure you have [Docker](https://www.docker.com) installed.<br>\nAdd this to your MCP server configuration:\n```json\n{\n  \"mcpServers\": {\n    \"mcp-local-rag\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"--rm\",\n        \"-i\",\n        \"--init\",\n        \"-e\",\n        \"DOCKER_CONTAINER=true\",\n        \"ghcr.io/nkapila6/mcp-local-rag:latest\"\n      ]\n    }\n  }\n}\n```\n\n# Security audits\nMseeP does security audits on every MCP server, you can see the security audit of this MCP server by clicking [here](https://mseep.ai/app/nkapila6-mcp-local-rag).\n\n<a href='https://mseep.ai/app/nkapila6-mcp-local-rag'><img alt=\"nkapila6_mcp_local_rag_badge\" src='https://mseep.net/pr/nkapila6-mcp-local-rag-badge.png' width='auto' height='200'></a>\n\n# MCP Clients\nThe MCP server should work with any MCP client that supports tool calling. Has been tested on the below clients.\n\n- Claude Desktop\n- Cursor\n- Goose\n- Others? You try!\n\n# Examples on Claude Desktop\nWhen an LLM (like Claude) is asked a question requiring recent web information, it will trigger `mcp-local-rag`.\n\nWhen asked to fetch/lookup/search the web, the model prompts you to use MCP server for the chat.\n\nIn the example, have asked it about Google's latest Gemma models released yesterday. This is new info that Claude is not aware about.\n\n\n## Result\n`mcp-local-rag` performs a live web search, extracts context, and sends it back to the model—giving it fresh knowledge:\n\n\n\n# Contributing\nHave ideas or want to improve this project? Issues and pull requests are welcome!\n\n# License\nThis project is licensed under the MIT License.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "retrieval",
        "searches",
        "search",
        "searches input",
        "search results",
        "local retrieval"
      ],
      "category": "web-search"
    },
    "noobnooc--webhook-mcp": {
      "owner": "noobnooc",
      "name": "webhook-mcp",
      "url": "https://github.com/noobnooc/webhook-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/noobnooc.webp",
      "description": "Integrates webhook-based services with large language model applications, facilitating dynamic data retrieval and action handling. Supports connections to external webhooks for enhanced AI workflows.",
      "stars": 1,
      "forks": 2,
      "license": "No License",
      "language": "JavaScript",
      "updated_at": "2025-06-23T16:23:28Z",
      "readme_content": "# Webhook MCP Server\n\n[![Install in VS Code](https://img.shields.io/badge/VS_Code-NPM-0098FF?style=flat-square&logo=visualstudiocode&logoColor=white)](https://insiders.vscode.dev/redirect/mcp/install?name=notification&inputs=%5B%7B%22type%22%3A%20%22promptString%22%2C%22id%22%3A%20%22notification_webhook_url%22%2C%22description%22%3A%20%22Notification%20Webhook%20URL%22%2C%22password%22%3A%20true%7D%5D&config=%7B%22command%22%3A%20%22npx%22%2C%22args%22%3A%20%5B%22-y%22%2C%20%22webhook-mcp%22%5D%2C%22env%22%3A%20%7B%20%22WEBHOOK_URL%22%3A%20%22%24%7Binput%3Anotification_webhook_url%7D%22%20%7D%7D) [![Install in VS Code Insiders](https://img.shields.io/badge/VS_Code_Insiders-NPM-24bfa5?style=flat-square&logo=visualstudiocode&logoColor=white)](https://insiders.vscode.dev/redirect/mcp/install?name=notification&inputs=%5B%7B%22type%22%3A%20%22promptString%22%2C%22id%22%3A%20%22notification_webhook_url%22%2C%22description%22%3A%20%22Notification%20Webhook%20URL%22%2C%22password%22%3A%20true%7D%5D&config=%7B%22command%22%3A%20%22npx%22%2C%22args%22%3A%20%5B%22-y%22%2C%20%22webhook-mcp%22%5D%2C%22env%22%3A%20%7B%20%22WEBHOOK_URL%22%3A%20%22%24%7Binput%3Anotification_webhook_url%7D%22%20%7D%7D&quality=insiders)\n[![Install in VS Code](https://img.shields.io/badge/VS_Code-Docker-0098FF?style=flat-square&logo=visualstudiocode&logoColor=white)](https://insiders.vscode.dev/redirect/mcp/install?name=notification&inputs=%5B%7B%22type%22%3A%20%22promptString%22%2C%22id%22%3A%20%22notification_webhook_url%22%2C%22description%22%3A%20%22Notification%20Webhook%20URL%22%2C%22password%22%3A%20true%7D%5D&config=%7B%22command%22%3A%22docker%22%2C%22args%22%3A%5B%22run%22%2C%22-i%22%2C%22--rm%22%2C%22-e%22%2C%22WEBHOOK_URL%22%2C%22noobnooc%2Fwebhook-mcp%22%5D%2C%22env%22%3A%7B%22WEBHOOK_URL%22%3A%22%24%7Binput%3Anotification_webhook_url%7D%22%7D%7D) [![Install in VS Code Insiders](https://img.shields.io/badge/VS_Code_Insiders-Docker-24bfa5?style=flat-square&logo=visualstudiocode&logoColor=white)](https://insiders.vscode.dev/redirect/mcp/install?name=notification&inputs=%5B%7B%22type%22%3A%20%22promptString%22%2C%22id%22%3A%20%22notification_webhook_url%22%2C%22description%22%3A%20%22Notification%20Webhook%20URL%22%2C%22password%22%3A%20true%7D%5D&config=%7B%22command%22%3A%22docker%22%2C%22args%22%3A%5B%22run%22%2C%22-i%22%2C%22--rm%22%2C%22-e%22%2C%22WEBHOOK_URL%22%2C%22noobnooc%2Fwebhook-mcp%22%5D%2C%22env%22%3A%7B%22WEBHOOK_URL%22%3A%22%24%7Binput%3Anotification_webhook_url%7D%22%7D%7D&quality=insiders)\n[![smithery badge](https://smithery.ai/badge/@noobnooc/webhook-mcp)](https://smithery.ai/server/@noobnooc/webhook-mcp)\n\nA Model Context Protocol (MCP) server that sends webhook notifications when called.\n\nYou can use this server with webhook notification services like [Echobell](https://echobell.one) to get notified when long-running tasks are completed. Simply configure the server with your Echobell webhook URL (or another service's URL) and instruct your AI assistant to \"send me a notification when it's done\" within your task prompts.\n\n## Configuration\n\nThere are several ways to configure the MCP server:\n\n### Installing via Smithery\n\nTo install Webhook MCP Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@noobnooc/webhook-mcp):\n\n```bash\nnpx -y @smithery/cli install @noobnooc/webhook-mcp --client claude\n```\n\n### Claude & Cursor & Windsurf\n\nConfigure Claude, Cursor or Windsurf to use the MCP server by adding this to your configuration:\n\n- With NPM:\n\n```json\n{\n  \"mcpServers\": {\n    \"notification\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"webhook-mcp\"],\n      \"env\": {\n        \"WEBHOOK_URL\": \"your-webhook-url-here\"\n      }\n    }\n  }\n}\n```\n\n- With Docker:\n\n```json\n{\n  \"mcpServers\": {\n    \"notification\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"-i\",\n        \"--rm\",\n        \"-e\",\n        \"WEBHOOK_URL\",\n        \"noobnooc/webhook-mcp\"\n      ],\n      \"env\": {\n        \"WEBHOOK_URL\": \"<your-webhook-url-here>\"\n      }\n    }\n  }\n}\n```\n\n### VS Code\n\nAdd the following configuration to your VS Code `settings.json`:\n\n- With NPM:\n\n```json\n{\n  \"mcp\": {\n    \"servers\": {\n      \"notification\": {\n        \"command\": \"npx\",\n        \"args\": [\"-y\", \"webhook-mcp\"],\n        \"env\": {\n          \"WEBHOOK_URL\": \"your-webhook-url-here\"\n        }\n      }\n    }\n  }\n}\n```\n\n- With Docker:\n\n```json\n{\n  \"mcp\": {\n    \"servers\": {\n      \"notification\": {\n        \"command\": \"docker\",\n        \"args\": [\n          \"run\",\n          \"-i\",\n          \"--rm\",\n          \"-e\",\n          \"WEBHOOK_URL\",\n          \"noobnooc/webhook-mcp\"\n        ],\n        \"env\": {\n          \"WEBHOOK_URL\": \"<your-webhook-url-here>\"\n        }\n      }\n    }\n  }\n}\n```\n\n## Environment Variables\n\n- `WEBHOOK_URL` (required): The URL where webhook notifications will be sent\n\n## Parameters\n\nThe webhook can be called with optional parameters:\n\n- `message`: Custom message to include in the webhook payload\n- `url`: External URL to include in the webhook payload as `externalLink`\n\n## Development\n\nTo build the project:\n\n```bash\nnpm install\nnpm run build\n```\n\nTo run with the MCP inspector for debugging:\n\n```bash\nnpm run inspector\n```\n\n## Publishing\n\nTo build and publish the package:\n\n```bash\nnpm run publish\n```\n",
      "npm_url": "https://www.npmjs.com/package/webhook-mcp",
      "npm_downloads": 431,
      "keywords": [
        "webhook",
        "webhooks",
        "noobnooc",
        "webhook mcp",
        "noobnooc webhook",
        "webhook based"
      ],
      "category": "web-search"
    },
    "nottelabs--notte": {
      "owner": "nottelabs",
      "name": "notte",
      "url": "https://github.com/nottelabs/notte",
      "imageUrl": "/freedevtools/mcp/pfp/nottelabs.webp",
      "description": "Manage sessions and run agents for web observation, scraping, actions, and authentication on various websites. Control cloud browser sessions and execute tasks efficiently with Notte agents.",
      "stars": 1635,
      "forks": 147,
      "license": "Other",
      "language": "Python",
      "updated_at": "2025-10-03T04:12:25Z",
      "readme_content": "# Rapidly build reliable web automation agents\n\n<div align=\"center\">\n  <p>\n    The web agent framework built for <strong>speed</strong>, <strong>cost-efficiency</strong>, <strong>scale</strong>, and <strong>reliability</strong> <br/>\n    → Read more at: <a href=\"https://github.com/nottelabs/open-operator-evals\" target=\"_blank\" rel=\"noopener noreferrer\">open-operator-evals</a> • <a href=\"https://x.com/nottecore?ref=github\" target=\"_blank\" rel=\"noopener noreferrer\">X</a> • <a href=\"https://www.linkedin.com/company/nottelabsinc/?ref=github\" target=\"_blank\" rel=\"noopener noreferrer\">LinkedIn</a> • <a href=\"https://notte.cc?ref=github\" target=\"_blank\" rel=\"noopener noreferrer\">Landing</a> • <a href=\"https://console.notte.cc/?ref=github\" target=\"_blank\" rel=\"noopener noreferrer\">Console</a>\n  </p>\n</div>\n\n<p align=\"center\">\n  \n</p>\n\n[![GitHub stars](https://img.shields.io/github/stars/nottelabs/notte?style=social)](https://github.com/nottelabs/notte/stargazers)\n[![License: SSPL-1.0](https://img.shields.io/badge/License-SSPL%201.0-blue.svg)](https://spdx.org/licenses/SSPL-1.0.html)\n[![Python 3.11+](https://img.shields.io/badge/python-3.11+-blue.svg)](https://www.python.org/downloads/)\n[![PyPI version](https://img.shields.io/pypi/v/notte?color=blue)](https://pypi.org/project/notte/)\n[![PyPI Downloads](https://static.pepy.tech/badge/notte?color=blue)](https://pepy.tech/projects/notte)\n\n---\n\n# What is Notte?\n\nNotte provides all the essential tools for building and deploying AI agents that interact seamlessly with the web. Our full-stack framework combines AI agents with traditional scripting for maximum efficiency - letting you script deterministic parts and use AI only when needed, cutting costs by 50%+ while improving reliability. We allow you to develop, deploy, and scale your own agents and web automations, all with a single API. Read more in our documentation [here](https://docs.notte.cc) 🔥\n\n**Opensource Core:**\n- **[Run web agents](#using-python-sdk-recommended)** → Give AI agents natural language tasks to complete on websites\n- **[Structured Output](#structured-output)** → Get data in your exact format with Pydantic models\n- **[Site Interactions](#scraping)** → Observe website states, scrape data and execute actions using Playwright compatible primitives and natural language commands\n\n**API service (Recommended)**\n- **[Stealth Browser Sessions](#session-features)** → Browser instances with built-in CAPTCHA solving, proxies, and anti-detection\n- **[Hybrid Workflows](#workflows)** → Combine scripting and AI agents to reduce costs and improve reliability\n- **[Secrets Vaults](#agent-vault)** → Enterprise-grade credential management to store emails, passwords, MFA tokens, SSO, etc.\n- **[Digital Personas](#agent-persona)** → Create digital identities with unique emails, phones, and automated 2FA for account creation workflows\n\n# Quickstart\n\n```\npip install notte\npatchright install --with-deps chromium\n```\n\n### Run in local mode\n\nUse the following script to spinup an agent using opensource features (you'll need your own LLM API keys):\n\n```python\nimport notte\nfrom dotenv import load_dotenv\nload_dotenv()\n\nwith notte.Session(headless=False) as session:\n    agent = notte.Agent(session=session, reasoning_model='gemini/gemini-2.5-flash', max_steps=30)\n    response = agent.run(task=\"doom scroll cat memes on google images\")\n```\n\n### Using Python SDK (Recommended)\n\nWe also provide an effortless API that hosts the browser sessions for you - and provide plenty of premium features. To run the agent you'll need to first sign up on the [Notte Console](https://console.notte.cc) and create a free Notte API key 🔑\n\n```python\nfrom notte_sdk import NotteClient\nimport os\n\nclient = NotteClient(api_key=os.getenv(\"NOTTE_API_KEY\"))\n\nwith client.Session(headless=False) as session:\n    agent = client.Agent(session=session, reasoning_model='gemini/gemini-2.5-flash', max_steps=30)\n    response = agent.run(task=\"doom scroll cat memes on google images\")\n```\n\nOur setup allows you to experiment locally, then drop-in replace the import and prefix `notte` objects with `cli` to switch to SDK and get hosted browser sessions plus access to premium features!\n\n# Benchmarks\n\n| Rank | Provider                                                    | Agent Self-Report | LLM Evaluation | Time per Task | Task Reliability |\n| ---- | ----------------------------------------------------------- | ----------------- | -------------- | ------------- | ---------------- |\n| 🏆   | [Notte](https://github.com/nottelabs/notte)                 | **86.2%**         | **79.0%**      | **47s**       | **96.6%**        |\n| 2️⃣   | [Browser-Use](https://github.com/browser-use/browser-use)   | 77.3%             | 60.2%          | 113s          | 83.3%            |\n| 3️⃣   | [Convergence](https://github.com/convergence-ai/proxy-lite) | 38.4%             | 31.4%          | 83s           | 50%              |\n\nRead the full story here: [https://github.com/nottelabs/open-operator-evals](https://github.com/nottelabs/open-operator-evals)\n\n# Agent features\n\n## Structured output\n\nStructured output is a feature of the agent's run function that allows you to specify a Pydantic model as the `response_format` parameter. The agent will return data in the specified structure.\n\n```python\nfrom notte_sdk import NotteClient\nfrom pydantic import BaseModel\n\nclass HackerNewsPost(BaseModel):\n    title: str\n    url: str\n    points: int\n    author: str\n    comments_count: int\n\nclass TopPosts(BaseModel):\n    posts: list[HackerNewsPost]\n\nclient = NotteClient()\nwith client.Session(headless=False, browser_type=\"firefox\") as session:\n    agent = client.Agent(session=session, reasoning_model='gemini/gemini-2.5-flash', max_steps=15)\n    response = agent.run(\n        task=\"Go to Hacker News (news.ycombinator.com) and extract the top 5 posts with their titles, URLs, points, authors, and comment counts.\",\n        response_format=TopPosts,\n    )\nprint(response.answer)\n```\n\n## Agent vault\nVaults are tools you can attach to your Agent instance to securely store and manage credentials. The agent automatically uses these credentials when needed.\n\n```python\nfrom notte_sdk import NotteClient\n\nclient = NotteClient()\n\nwith client.Vault() as vault, client.Session(headless=False) as session:\n    vault.add_credentials(\n        url=\"https://x.com\",\n        username=\"your-email\",\n        password=\"your-password\",\n    )\n    agent = client.Agent(session=session, vault=vault, max_steps=10)\n    response = agent.run(\n      task=\"go to twitter; login and go to my messages\",\n    )\nprint(response.answer)\n```\n\n## Agent persona\n\nPersonas are tools you can attach to your Agent instance to provide digital identities with unique email addresses, phone numbers, and automated 2FA handling.\n\n```python\nfrom notte_sdk import NotteClient\n\nclient = NotteClient()\n\nwith client.Persona(create_phone_number=False) as persona:\n    with client.Session(browser_type=\"firefox\", headless=False) as session:\n        agent = client.Agent(session=session, persona=persona, max_steps=15)\n        response = agent.run(\n            task=\"Open the Google form and RSVP yes with your name\",\n            url=\"https://forms.google.com/your-form-url\",\n        )\nprint(response.answer)\n```\n\n# Session features\n\n## Stealth\n\nStealth features include automatic CAPTCHA solving and proxy configuration to enhance automation reliability and anonymity.\n\n```python\nfrom notte_sdk import NotteClient\nfrom notte_sdk.types import NotteProxy, ExternalProxy\n\nclient = NotteClient()\n\n# Built-in proxies with CAPTCHA solving\nwith client.Session(\n    solve_captchas=True,\n    proxies=True,  # US-based proxy\n    browser_type=\"firefox\",\n    headless=False\n) as session:\n    agent = client.Agent(session=session, max_steps=5)\n    response = agent.run(\n        task=\"Try to solve the CAPTCHA using internal tools\",\n        url=\"https://www.google.com/recaptcha/api2/demo\"\n    )\n\n# Custom proxy configuration\nproxy_settings = ExternalProxy(\n    server=\"http://your-proxy-server:port\",\n    username=\"your-username\",\n    password=\"your-password\",\n)\n\nwith client.Session(proxies=[proxy_settings]) as session:\n    agent = client.Agent(session=session, max_steps=5)\n    response = agent.run(task=\"Navigate to a website\")\n```\n\n## File download / upload\n\nFile Storage allows you to upload files to a session and download files that agents retrieve during their work. Files are session-scoped and persist beyond the session lifecycle.\n\n```python\nfrom notte_sdk import NotteClient\n\nclient = NotteClient()\nstorage = client.FileStorage()\n\n# Upload files before agent execution\nstorage.upload(\"/path/to/document.pdf\")\n\n# Create session with storage attached\nwith client.Session(storage=storage) as session:\n    agent = client.Agent(session=session, max_steps=5)\n    response = agent.run(\n        task=\"Upload the PDF document to the website and download the cat picture\",\n        url=\"https://example.com/upload\"\n    )\n\n# Download files that the agent downloaded\ndownloaded_files = storage.list(type=\"downloads\")\nfor file_name in downloaded_files:\n    storage.download(file_name=file_name, local_dir=\"./results\")\n```\n\n## Cookies / Auth Sessions\n\nCookies provide a flexible way to authenticate your sessions. While we recommend using the secure vault for credential management, cookies offer an alternative approach for certain use cases.\n\n```python\nfrom notte_sdk import NotteClient\nimport json\n\nclient = NotteClient()\n\n# Upload cookies for authentication\ncookies = [\n    {\n        \"name\": \"sb-db-auth-token\",\n        \"value\": \"base64-cookie-value\",\n        \"domain\": \"github.com\",\n        \"path\": \"/\",\n        \"expires\": 9778363203.913704,\n        \"httpOnly\": False,\n        \"secure\": False,\n        \"sameSite\": \"Lax\"\n    }\n]\n\nwith client.Session() as session:\n    session.set_cookies(cookies=cookies)  # or cookie_file=\"path/to/cookies.json\"\n    \n    agent = client.Agent(session=session, max_steps=5)\n    response = agent.run(\n        task=\"go to nottelabs/notte get repo info\",\n    )\n    \n    # Get cookies from the session\n    cookies_resp = session.get_cookies()\n    with open(\"cookies.json\", \"w\") as f:\n        json.dump(cookies_resp, f)\n```\n\n## CDP Browser compatibility\n\nYou can plug in any browser session provider you want and use our agent on top. Use external headless browser providers via CDP to benefit from Notte's agentic capabilities with any CDP-compatible browser.\n\n```python\nfrom notte_sdk import NotteClient\n\nclient = NotteClient()\ncdp_url = \"wss://your-external-cdp-url\"\n\nwith client.Session(cdp_url=cdp_url) as session:\n    agent = client.Agent(session=session)\n    response = agent.run(task=\"extract pricing plans from https://www.notte.cc/\")\n```\n\n# Workflows\n\nNotte's close compatibility with Playwright allows you to mix web automation primitives with agents for specific parts that require reasoning and adaptability. This hybrid approach cuts LLM costs and is much faster by using scripting for deterministic parts and agents only when needed.\n\n```python\nfrom notte_sdk import NotteClient\n\nclient = NotteClient()\n\nwith client.Session(headless=False, perception_type=\"fast\") as session:\n    # Script execution for deterministic navigation\n    session.execute({\"type\": \"goto\", \"url\": \"https://www.quince.com/women/organic-stretch-cotton-chino-short\"})\n    session.observe()\n\n    # Agent for reasoning-based selection\n    agent = client.Agent(session=session)\n    agent.run(task=\"just select the ivory color in size 6 option\")\n\n    # Script execution for deterministic actions\n    session.execute({\"type\": \"click\", \"selector\": \"internal:role=button[name=\\\"ADD TO CART\\\"i]\"})\n    session.execute({\"type\": \"click\", \"selector\": \"internal:role=button[name=\\\"CHECKOUT\\\"i]\"})\n```\n\n# Agent fallback for Workflows\n\nWorkflows are a powerful way to combine scripting and agents to reduce costs and improve reliability. However, deterministic parts of the workflow can still fail. To gracefully handle these failures with agents, you can use the `AgentFallback` class: \n\n```python\nimport notte\n\nwith notte.Session() as session:\n    _ = session.execute({\"type\": \"goto\", \"value\": \"https://shop.notte.cc/\"})\n    _ = session.observe()\n\n    with notte.AgentFallback(session, \"Go to cart\"):\n        # Force execution failure -> trigger an agent fallback to gracefully fix the issue\n        res = session.execute(type=\"click\", id=\"INVALID_ACTION_ID\")\n```\n\n# Scraping\n\nFor fast data extraction, we provide a dedicated scraping endpoint that automatically creates and manages sessions. You can pass custom instructions for structured outputs and enable stealth mode.\n\n```python\nfrom notte_sdk import NotteClient\nfrom pydantic import BaseModel\n\nclient = NotteClient()\n\n# Simple scraping\nresponse = client.scrape(\n    url=\"https://notte.cc\",\n    scrape_links=True,\n    only_main_content=True\n)\n\n# Structured scraping with custom instructions\nclass Article(BaseModel):\n    title: str\n    content: str\n    date: str\n\nresponse = client.scrape(\n    url=\"https://example.com/blog\",\n    response_format=Article,\n    instructions=\"Extract only the title, date and content of the articles\"\n)\n```\n\nOr directly with cURL\n```bash\ncurl -X POST 'https://api.notte.cc/scrape' \\\n  -H 'Authorization: Bearer <NOTTE-API-KEY>' \\\n  -H 'Content-Type: application/json' \\\n  -d '{\n    \"url\": \"https://notte.cc\",\n    \"only_main_content\": false,\n  }'\n```\n\n\n**Search:** We've built a cool demo of an LLM leveraging the scraping endpoint in an MCP server to make real-time search in an LLM chatbot - works like a charm! Available here: [https://search.notte.cc/](https://search.notte.cc/)\n\n# License\n\nThis project is licensed under the Server Side Public License v1.\nSee the [LICENSE](LICENSE) file for details.\n\n# Citation\n\nIf you use notte in your research or project, please cite:\n\n```bibtex\n@software{notte2025,\n  author = {Pinto, Andrea and Giordano, Lucas and {nottelabs-team}},\n  title = {Notte: Software suite for internet-native agentic systems},\n  url = {https://github.com/nottelabs/notte},\n  year = {2025},\n  publisher = {GitHub},\n  license = {SSPL-1.0}\n  version = {1.4.4},\n}\n```\n\nCopyright © 2025 Notte Labs, Inc.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "nottelabs",
        "agents",
        "sessions",
        "search nottelabs",
        "notte agents",
        "agents web"
      ],
      "category": "web-search"
    },
    "obinopaul--nba-mcp-server": {
      "owner": "obinopaul",
      "name": "nba-mcp-server",
      "url": "https://github.com/obinopaul/nba-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/obinopaul.webp",
      "description": "Access real-time NBA statistics and live game data, including player stats, team standings, and game results for integration into sports applications.",
      "stars": 2,
      "forks": 3,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-06-28T14:37:54Z",
      "readme_content": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/obinopaul-nba-mcp-server-badge.png)](https://mseep.ai/app/obinopaul-nba-mcp-server)\n\n# NBA MCP Server\n\nA Python server implementing Model Context Protocol (MCP) for NBA statistics and live game data.\n\n## Overview\n\nThis server provides a set of tools for accessing NBA data through the NBA API. It serves as a bridge between applications and the NBA's data services, offering both live game information and historical statistics.\n\n## Features\n\n- Live game data (scoreboard, box scores, play-by-play)\n- Player information and career statistics\n- Team game logs and statistics\n- League standings\n- Game results and schedules\n\n## Tools\n\n### Live Game Data\n\n- **nba_live_scoreboard**\n  - Fetch today's NBA scoreboard (live or latest)\n  - Returns game IDs, start times, scores, and broadcast details\n\n- **nba_live_boxscore**\n  - Fetch real-time box score for a given NBA game ID\n  - Provides detailed player and team statistics\n\n- **nba_live_play_by_play**\n  - Retrieve live play-by-play actions for a specific game\n  - Includes scoring plays, fouls, timeouts, and substitutions\n\n### Player Information\n\n- **nba_common_player_info**\n  - Retrieve basic information about a player\n  - Includes biographical data, height, weight, team, position\n\n- **nba_player_career_stats**\n  - Obtain a player's career statistics\n  - Available in different formats (per game, totals, per 36 minutes)\n\n- **nba_list_active_players**\n  - Return a list of all currently active NBA players\n\n- **nba_player_game_logs**\n  - Obtain a player's game statistics within a specified date range\n\n### Team Data\n\n- **nba_team_game_logs_by_name**\n  - Fetch a team's game logs using the team name\n  - Avoids needing to know the team's numeric ID\n\n- **nba_fetch_game_results**\n  - Fetch game results for a given team ID and date range\n\n- **nba_team_standings**\n  - Fetch NBA team standings for a given season and season type\n\n- **nba_team_stats_by_name**\n  - Fetch team statistics using the team name\n  - Supports different aggregation methods (totals, per game, etc.)\n\n- **nba_all_teams_stats**\n  - Fetch statistics for all NBA teams across multiple seasons\n\n### Schedule Information\n\n- **nba_list_todays_games**\n  - Returns scoreboard data for any specific date\n\n## Usage\n\nThe server is implemented using the MCP framework and can be run as a standalone service.\n\n```python\n# Start the server\npython nba_server.py\n# or\nmcp run nba_server.py\n```\n\n### Configuration\n\n- The server runs with a 30-second timeout for more reliable operation\n- Signal handlers are implemented for graceful shutdown (Ctrl+C)\n\n### Usage with Claude Desktop\n\n#### Option 1: Using Docker (Recommended)\n\n1. Clone this repository\n```\ngit clone https://github.com/obinopaul/nba-mcp-server.git\ncd nba-mcp-server\n```\n\n2. Install dependencies\n```\npip install -r requirements.txt\n```\n\n3. Build the Docker image\n```\ndocker build -t nba_mcp_server .\n```\n\n4. Run the Docker container\n```\ndocker run -d -p 5000:5000 --name nba_mcp_server nba_mcp_server\n```\n\n5. Add this to your `claude_desktop_config.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"nba_mcp_server\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"exec\",\n        \"-i\",\n        \"nba_mcp_server\",\n        \"python\",\n        \"nba_server.py\"\n      ]\n    }\n  }\n}\n```\n\n#### Option 2: Direct Python Execution\n\n1. Clone this repository\n```\ngit clone https://github.com/obinopaul/nba-mcp-server.git\ncd nba-mcp-server\n```\n\n2. Create a new environment\n```\nconda create --name your_env_name python=3.13\nconda activate your_env_name\n```\n\n3. Install dependencies\n```\npip install -r requirements.txt\n```\n\n4. Run NBA mcp server on the terminal\n```\nmcp run nba_server.py\n```\n\n5. Add this to your `claude_desktop_config.json`, adjusting the Python path as needed:\n\n```json\n{\n  \"mcpServers\": {\n    \"nba_mcp_server\": {\n      \"command\": \"/path/to/your/python\",\n      \"args\": [\n        \"/path/to/nba_server.py\"\n      ]\n    }\n  }\n}\n```\n\nAfter adding your chosen configuration, restart Claude Desktop to load the NBA server. You'll then be able to use all the NBA data tools in your conversations with Claude.\n\n\n## Technical Details\n\nThe server is built on:\n- NBA API (nba_api) Python package\n- MCP for API interface\n- Pydantic for input validation\n- Pandas for data manipulation\n\n## License\n\nThis MCP server is available under the MIT License.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "nba",
        "stats",
        "sports",
        "nba statistics",
        "player stats",
        "game data"
      ],
      "category": "web-search"
    },
    "oleggolimbievsky88--bmr-nextjs": {
      "owner": "oleggolimbievsky88",
      "name": "bmr-nextjs",
      "url": "https://github.com/oleggolimbievsky88/bmr-nextjs",
      "imageUrl": "/freedevtools/mcp/pfp/oleggolimbievsky88.webp",
      "description": "A Next.js project that facilitates the development and deployment of modern web applications by providing a framework optimized for performance and user experience. It features hot reloading for rapid development and built-in support for dynamic and responsive page creation.",
      "stars": 0,
      "forks": 0,
      "license": "No License",
      "language": "JavaScript",
      "updated_at": "2025-06-12T19:04:36Z",
      "readme_content": "This is a [Next.js](https://nextjs.org/) project bootstrapped with [`create-next-app`](https://github.com/vercel/next.js/tree/canary/packages/create-next-app).\r\n\r\n## Getting Started\r\n\r\nFirst, run the development server:\r\n\r\n```bash\r\nnpm run dev\r\n# or\r\nyarn dev\r\n# or\r\npnpm dev\r\n# or\r\nbun dev\r\n```\r\n\r\nOpen [http://localhost:3000](http://localhost:3000) with your browser to see the result.\r\n\r\nYou can start editing the page by modifying `app/page.js`. The page auto-updates as you edit the file.\r\n\r\nThis project uses [`next/font`](https://nextjs.org/docs/basic-features/font-optimization) to automatically optimize and load Inter, a custom Google Font.\r\n\r\n## Learn More\r\n\r\nTo learn more about Next.js, take a look at the following resources:\r\n\r\n- [Next.js Documentation](https://nextjs.org/docs) - learn about Next.js features and API.\r\n- [Learn Next.js](https://nextjs.org/learn) - an interactive Next.js tutorial.\r\n\r\nYou can check out [the Next.js GitHub repository](https://github.com/vercel/next.js/) - your feedback and contributions are welcome!\r\n\r\n## Deploy on Vercel\r\n\r\nThe easiest way to deploy your Next.js app is to use the [Vercel Platform](https://vercel.com/new?utm_medium=default-template&filter=next.js&utm_source=create-next-app&utm_campaign=create-next-app-readme) from the creators of Next.js.\r\n\r\nCheck out our [Next.js deployment documentation](https://nextjs.org/docs/deployment) for more details.\r\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "nextjs",
        "js",
        "web",
        "bmr nextjs",
        "nextjs js",
        "js project"
      ],
      "category": "web-search"
    },
    "olostep--olostep-mcp-server": {
      "owner": "olostep",
      "name": "olostep-mcp-server",
      "url": "https://github.com/olostep/olostep-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/olostep.webp",
      "description": "Integrates with Olostep for extracting web page content, structured Google search results, and mapping websites. Supports geo-targeted content retrieval and error handling for enhanced web interaction.",
      "stars": 2,
      "forks": 2,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-09-24T08:11:12Z",
      "readme_content": "# Olostep MCP Server\n\nA Model Context Protocol (MCP) server implementation that integrates with [Olostep](https://olostep.com) for web scraping, content extraction, and search capabilities.\nTo set up Olostep MCP Server, you need to have an API key. You can get the API key by signing up on the [Olostep website](https://olostep.com/auth).\n\n\n## Features\n\n- Web page content extraction with clean markdown formatting\n- Google search results with structured data extraction\n- Website URL discovery and mapping\n- Country-specific request routing for geo-targeted content\n- Configurable wait times for JavaScript-heavy websites\n- Comprehensive error handling and reporting\n- Simple API key configuration\n\n## Installation\n\n### Running with npx\n\n```bash\nenv OLOSTEP_API_KEY=your-api-key npx -y olostep-mcp\n```\n\n### Manual Installation\n\n```bash\nnpm install -g olostep-mcp\n```\n\n### Running on Claude Desktop\n\nAdd this to your `claude_desktop_config.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"mcp-server-olostep\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"olostep-mcp\"],\n      \"env\": {\n        \"OLOSTEP_API_KEY\": \"YOUR_API_KEY_HERE\"\n      }\n    }\n  }\n}\n```\n\nOr for a more straightforward way you can install via the Smithery CLI by running the following code in your device terminal\n\n```\nnpx -y @smithery/cli install @olostep/olostep-mcp-server --client claude\n```\n\n### Running on Windsurf\n\nAdd this to your `./codeium/windsurf/model_config.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"mcp-server-olostep\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"olostep-mcp\"],\n      \"env\": {\n        \"OLOSTEP_API_KEY\": \"YOUR_API_KEY_HERE\"\n      }\n    }\n  }\n}\n```\n\n### Running on Cursor\n\nTo configure Olostep MCP in Cursor:\n\n1. Open Cursor Settings\n2. Go to Features > MCP Servers \n3. Click \"+ Add New MCP Server\"\n4. Enter the following:\n   - Name: \"olostep-mcp\" (or your preferred name)\n   - Type: \"command\"\n   - Command: `env OLOSTEP_API_KEY=your-api-key npx -y olostep-mcp`\n\nReplace `your-api-key` with your Olostep API key.\n\n## Configuration\n\n### Environment Variables\n\n- `OLOSTEP_API_KEY`: Your Olostep API key (required)\n- `ORBIT_KEY`: An optional key for using Orbit to route requests.\n\n## Available Tools\n\n### 1. Get Webpage Content (`get_webpage_content`)\n\nRetrieves webpage content in clean markdown format with support for JavaScript rendering.\n\n```json\n{\n  \"name\": \"get_webpage_content\",\n  \"arguments\": {\n    \"url_to_scrape\": \"https://example.com\",\n    \"wait_before_scraping\": 1000,\n    \"country\": \"US\"\n  }\n}\n```\n\n#### Parameters:\n\n- `url_to_scrape`: The URL of the webpage to scrape (required)\n- `wait_before_scraping`: Time to wait in milliseconds before starting the scrape (default: 0)\n- `country`: Residential country to load the request from (e.g., US, CA, GB) (optional)\n\n#### Response:\n\n```json\n{\n  \"content\": [\n    {\n      \"type\": \"text\",\n      \"text\": \"# Example Website\\n\\nThis is the markdown content of the webpage...\"\n    }\n  ]\n}\n```\n\n### 2. Get Website URLs (`get_website_urls`)\n\nSearch and retrieve relevant URLs from a website, sorted by relevance to your query.\n\n```json\n{\n  \"name\": \"get_website_urls\",\n  \"arguments\": {\n    \"url\": \"https://example.com\",\n    \"search_query\": \"your search term\"\n  }\n}\n```\n\n#### Parameters:\n\n- `url`: The URL of the website to map (required)\n- `search_query`: The search query to sort URLs by (required)\n\n#### Response:\n\n```json\n{\n  \"content\": [\n    {\n      \"type\": \"text\",\n      \"text\": \"Found 42 URLs matching your query:\\n\\nhttps://example.com/page1\\nhttps://example.com/page2\\n...\"\n    }\n  ]\n}\n```\n\n### 3. Google Search (`google_search`)\n\nRetrieve structured data from Google search results.\n\n```json\n{\n  \"name\": \"google_search\",\n  \"arguments\": {\n    \"query\": \"your search query\",\n    \"country\": \"US\"\n  }\n}\n```\n\n#### Parameters:\n\n- `query`: The search query to perform (required)\n- `country`: Country code for localized results (e.g., US, GB) (default: \"US\")\n\n#### Response includes:\n\n- Organic search results with titles, links, and snippets\n- Knowledge graph data when available\n- Related questions (People Also Ask)\n- Related searches\n- Rich snippets and other structured data\n\n## Error Handling\n\nThe server provides robust error handling:\n\n- Detailed error messages for API issues\n- Network error reporting\n- Authentication failure handling\n- Rate limit information\n\nExample error response:\n\n```json\n{\n  \"isError\": true,\n  \"content\": [\n    {\n      \"type\": \"text\",\n      \"text\": \"Olostep API Error: 401 Unauthorized. Details: {\\\"error\\\":\\\"Invalid API key\\\"}\"\n    }\n  ]\n}\n```\n\n\n## License\n\nISC License\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "olostep",
        "web",
        "google",
        "search olostep",
        "olostep mcp",
        "olostep extracting"
      ],
      "category": "web-search"
    },
    "oneshot-engineering--mcp-webresearch": {
      "owner": "oneshot-engineering",
      "name": "mcp-webresearch",
      "url": "https://github.com/oneshot-engineering/mcp-webresearch",
      "imageUrl": "/freedevtools/mcp/pfp/oneshot-engineering.webp",
      "description": "Fetch real-time information from the web, extract content from webpages, and track research sessions with the ability to capture screenshots for better insights.",
      "stars": 1,
      "forks": 0,
      "license": "MIT License",
      "language": "",
      "updated_at": "2025-04-28T23:55:21Z",
      "readme_content": "# MCP Web Research Server\n\nA Model Context Protocol (MCP) server for web research. \n\nBring real-time info into Claude and easily research any topic.\n\n## Features\n\n- Google search integration\n- Webpage content extraction\n- Research session tracking (list of visited pages, search queries, etc.)\n- Screenshot capture\n\n## Prerequisites\n\n- [Node.js](https://nodejs.org/) >= 18 (includes `npm` and `npx`)\n- [Claude Desktop app](https://claude.ai/download)\n\n## Installation\n\nFirst, ensure you've downloaded and installed the [Claude Desktop app](https://claude.ai/download) and you have npm installed.\n\nNext, add this entry to your `claude_desktop_config.json` (on Mac, found at `~/Library/Application\\ Support/Claude/claude_desktop_config.json`):\n\n```json\n{\n  \"mcpServers\": {\n    \"webresearch\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@mzxrai/mcp-webresearch@latest\"]\n    }\n  }\n}\n```\n\nThis config allows Claude Desktop to automatically start the web research MCP server when needed.\n\n## Usage\n\nSimply start a chat with Claude and send a prompt that would benefit from web research. If you'd like a prebuilt prompt customized for deeper web research, you can use the `agentic-research` prompt that we provide through this package. Access that prompt in Claude Desktop by clicking the Paperclip icon in the chat input and then selecting `Choose an integration` → `webresearch` → `agentic-research`.\n\n<img src=\"https://i.ibb.co/N6Y3C0q/Screenshot-2024-12-05-at-11-01-27-PM.png\" alt=\"Example screenshot of web research\" width=\"400\"/>\n\n### Tools\n\n1. `search_google`\n   - Performs Google searches and extracts results\n   - Arguments: `{ query: string }`\n\n2. `visit_page`\n   - Visits a webpage and extracts its content\n   - Arguments: `{ url: string, takeScreenshot?: boolean }`\n\n3. `take_screenshot`\n   - Takes a screenshot of the current page\n   - No arguments required\n\n### Prompts\n\n#### `agentic-research`\nA guided research prompt that helps Claude conduct thorough web research. The prompt instructs Claude to:\n- Start with broad searches to understand the topic landscape\n- Prioritize high-quality, authoritative sources\n- Iteratively refine the research direction based on findings\n- Keep you informed and let you guide the research interactively\n- Always cite sources with URLs\n\n### Resources\n\nWe expose two things as MCP resources: (1) captured webpage screenshots, and (2) the research session.\n\n#### Screenshots\n\nWhen you take a screenshot, it's saved as an MCP resource. You can access captured screenshots in Claude Desktop via the Paperclip icon.\n\n#### Research Session\n\nThe server maintains a research session that includes:\n- Search queries\n- Visited pages\n- Extracted content\n- Screenshots\n- Timestamps\n\n### Suggestions\n\nFor the best results, if you choose not to use the `agentic-research` prompt when doing your research, it may be helpful to suggest high-quality sources for Claude to use when researching general topics. For example, you could prompt `news today from reuters or AP` instead of `news today`.\n\n## Problems\n\nThis is very much pre-alpha code. And it is also AIGC, so expect bugs.\n\nIf you run into issues, it may be helpful to check Claude Desktop's MCP logs:\n\n```bash\ntail -n 20 -f ~/Library/Logs/Claude/mcp*.log\n```\n\n## Development\n\n```bash\n# Install dependencies\npnpm install\n\n# Build the project\npnpm build\n\n# Watch for changes\npnpm watch\n\n# Run in development mode\npnpm dev\n```\n\n## Requirements\n\n- Node.js >= 18\n- Playwright (automatically installed as a dependency)\n\n## Verified Platforms\n\n- [x] macOS\n- [ ] Linux\n\n## License\n\nMIT\n\n## Author\n\n[mzxrai](https://github.com/mzxrai) ",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "webresearch",
        "webpages",
        "web",
        "mcp webresearch",
        "capture screenshots",
        "search oneshot"
      ],
      "category": "web-search"
    },
    "onurpolat05--n8n-Assistant": {
      "owner": "onurpolat05",
      "name": "n8n-Assistant",
      "url": "https://github.com/onurpolat05/n8n-Assistant",
      "imageUrl": "/freedevtools/mcp/pfp/onurpolat05.webp",
      "description": "Search for n8n documentation, example workflows, and community forums efficiently, while providing relevant information and resources. It employs asynchronous processing for improved response times and effective content retrieval.",
      "stars": 13,
      "forks": 5,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-27T18:50:24Z",
      "readme_content": "# n8n Assistant\n[![smithery badge](https://smithery.ai/badge/@onurpolat05/n8n-assistant)](https://smithery.ai/server/@onurpolat05/n8n-assistant)\n\nThis project contains a Multi-Channel Platform (MCP) server used to create an assistant integrated with n8n. The assistant can be used to search for n8n documentation, example workflows, and community forums.\n\n<a href=\"https://glama.ai/mcp/servers/@onurpolat05/n8n-Assistant\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@onurpolat05/n8n-Assistant/badge\" alt=\"n8n-asistans MCP server\" />\n</a>\n\n## Features\n\n- **Web Search**: Searches n8n documentation, workflows, and community forums based on a specific query.\n- **HTML Content Fetching**: Uses BeautifulSoup to extract the main content from search results.\n- **Asynchronous Processing**: Performs HTTP requests asynchronously, providing faster response times.\n\n## Requirements\n\n- Python 3.7 or higher\n- `httpx` library\n- `beautifulsoup4` library\n- `python-dotenv` library\n\n## Installation\n\n### Installing via Smithery\n\nTo install n8n-assistant for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@onurpolat05/n8n-assistant):\n\n```bash\nnpx -y @smithery/cli install @onurpolat05/n8n-assistant --client claude\n```\n\n### Manual Installation\n1. Clone this repository:\n   ```bash\n   git clone <repository-url>\n   cd <repository-directory>\n   ```\n\n2. Install the required dependencies:\n   ```bash\n   pip install -r requirements.txt\n   ```\n\n3. Create a `.env` file and add the necessary API keys:\n   ```plaintext\n   SERPER_API_KEY=your_api_key_here\n   ```\n\n## Usage\n\nTo start the assistant, run the following command:\n```bash\nuvicorn main:app --reload\n```\n\nThen, you can query the assistant for information related to n8n like this:\n```python\nawait get_n8n_info(\"HTTP Request node\", \"docs\")\n```\n\n## MCP Server\n\nThis project uses the `n8n-asistans` MCP server. The server is started with the following command:\n```json\n{\n    \"mcpServers\": {\n        \"n8n-asistans\": {\n            \"command\": \"uv\",\n            \"args\": [\n                \"--directory\",\n                \"/n8n-assistant\",\n                \"run\",\n                \"main.py\"\n            ],\n            \"env\":{\n                \"SERPER_API_KEY\": \"*********\"\n            }\n        }\n    }\n}\n```\n\n## Contributing\n\nIf you would like to contribute, please create a pull request or report issues.\n\n## License\n\nThis project is licensed under the MIT License.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "search",
        "retrieval",
        "n8n",
        "search n8n",
        "n8n documentation",
        "search onurpolat05"
      ],
      "category": "web-search"
    },
    "openags--paper-search-mcp": {
      "owner": "openags",
      "name": "paper-search-mcp",
      "url": "https://github.com/openags/paper-search-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/openags.webp",
      "description": "Search and download academic papers from various sources such as arXiv, PubMed, bioRxiv, and Sci-Hub. Integrates with large language models to streamline access to scholarly articles and enhance research workflows.",
      "stars": 368,
      "forks": 57,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-10-03T06:37:54Z",
      "readme_content": "# Paper Search MCP\n\nA Model Context Protocol (MCP) server for searching and downloading academic papers from multiple sources, including arXiv, PubMed, bioRxiv, and Sci-Hub (optional). Designed for seamless integration with large language models like Claude Desktop.\n\n![PyPI](https://img.shields.io/pypi/v/paper-search-mcp.svg) ![License](https://img.shields.io/badge/license-MIT-blue.svg) ![Python](https://img.shields.io/badge/python-3.10+-blue.svg)\n[![smithery badge](https://smithery.ai/badge/@openags/paper-search-mcp)](https://smithery.ai/server/@openags/paper-search-mcp)\n\n---\n\n## Table of Contents\n\n- [Overview](#overview)\n- [Features](#features)\n- [Installation](#installation)\n  - [Quick Start](#quick-start)\n    - [Install Package](#install-package)\n    - [Configure Claude Desktop](#configure-claude-desktop)\n  - [For Development](#for-development)\n    - [Setup Environment](#setup-environment)\n    - [Install Dependencies](#install-dependencies)\n- [Contributing](#contributing)\n- [Demo](#demo)\n- [License](#license)\n- [TODO](#todo)\n\n---\n\n## Overview\n\n`paper-search-mcp` is a Python-based MCP server that enables users to search and download academic papers from various platforms. It provides tools for searching papers (e.g., `search_arxiv`) and downloading PDFs (e.g., `download_arxiv`), making it ideal for researchers and AI-driven workflows. Built with the MCP Python SDK, it integrates seamlessly with LLM clients like Claude Desktop.\n\n---\n\n## Features\n\n- **Multi-Source Support**: Search and download papers from arXiv, PubMed, bioRxiv, medRxiv, Google Scholar, IACR ePrint Archive, Semantic Scholar.\n- **Standardized Output**: Papers are returned in a consistent dictionary format via the `Paper` class.\n- **Asynchronous Tools**: Efficiently handles network requests using `httpx`.\n- **MCP Integration**: Compatible with MCP clients for LLM context enhancement.\n- **Extensible Design**: Easily add new academic platforms by extending the `academic_platforms` module.\n\n---\n\n## Installation\n\n`paper-search-mcp` can be installed using `uv` or `pip`. Below are two approaches: a quick start for immediate use and a detailed setup for development.\n\n### Installing via Smithery\n\nTo install paper-search-mcp for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@openags/paper-search-mcp):\n\n```bash\nnpx -y @smithery/cli install @openags/paper-search-mcp --client claude\n```\n\n### Quick Start\n\nFor users who want to quickly run the server:\n\n1. **Install Package**:\n\n   ```bash\n   uv add paper-search-mcp\n   ```\n\n2. **Configure Claude Desktop**:\n   Add this configuration to `~/Library/Application Support/Claude/claude_desktop_config.json` (Mac) or `%APPDATA%\\Claude\\claude_desktop_config.json` (Windows):\n   ```json\n   {\n     \"mcpServers\": {\n       \"paper_search_server\": {\n         \"command\": \"uv\",\n         \"args\": [\n           \"run\",\n           \"--directory\",\n           \"/path/to/your/paper-search-mcp\",\n           \"-m\",\n           \"paper_search_mcp.server\"\n         ],\n         \"env\": {\n           \"SEMANTIC_SCHOLAR_API_KEY\": \"\" // Optional: For enhanced Semantic Scholar features\n         }\n       }\n     }\n   }\n   ```\n   > Note: Replace `/path/to/your/paper-search-mcp` with your actual installation path.\n\n### For Development\n\nFor developers who want to modify the code or contribute:\n\n1. **Setup Environment**:\n\n   ```bash\n   # Install uv if not installed\n   curl -LsSf https://astral.sh/uv/install.sh | sh\n\n   # Clone repository\n   git clone https://github.com/openags/paper-search-mcp.git\n   cd paper-search-mcp\n\n   # Create and activate virtual environment\n   uv venv\n   source .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\n   ```\n\n2. **Install Dependencies**:\n\n   ```bash\n   # Install project in editable mode\n   uv add -e .\n\n   # Add development dependencies (optional)\n   uv add pytest flake8\n   ```\n\n---\n\n## Contributing\n\nWe welcome contributions! Here's how to get started:\n\n1. **Fork the Repository**:\n   Click \"Fork\" on GitHub.\n\n2. **Clone and Set Up**:\n\n   ```bash\n   git clone https://github.com/yourusername/paper-search-mcp.git\n   cd paper-search-mcp\n   pip install -e \".[dev]\"  # Install dev dependencies (if added to pyproject.toml)\n   ```\n\n3. **Make Changes**:\n\n   - Add new platforms in `academic_platforms/`.\n   - Update tests in `tests/`.\n\n4. **Submit a Pull Request**:\n   Push changes and create a PR on GitHub.\n\n---\n\n## Demo\n\n\n\n## TODO\n\n### Planned Academic Platforms\n\n- [√] arXiv\n- [√] PubMed\n- [√] bioRxiv\n- [√] medRxiv\n- [√] Google Scholar\n- [√] IACR ePrint Archive\n- [√] Semantic Scholar\n- [ ] PubMed Central (PMC)\n- [ ] Science Direct\n- [ ] Springer Link\n- [ ] IEEE Xplore\n- [ ] ACM Digital Library\n- [ ] Web of Science\n- [ ] Scopus\n- [ ] JSTOR\n- [ ] ResearchGate\n- [ ] CORE\n- [ ] Microsoft Academic\n\n---\n\n## License\n\nThis project is licensed under the MIT License. See the LICENSE file for details.\n\n---\n\nHappy researching with `paper-search-mcp`! If you encounter issues, open a GitHub issue.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "scholarly",
        "search",
        "pubmed",
        "scholarly articles",
        "paper search",
        "access scholarly"
      ],
      "category": "web-search"
    },
    "openbnb-org--mcp-server-airbnb": {
      "owner": "openbnb-org",
      "name": "mcp-server-airbnb",
      "url": "https://github.com/openbnb-org/mcp-server-airbnb",
      "imageUrl": "/freedevtools/mcp/pfp/openbnb-org.webp",
      "description": "Search for Airbnb listings and retrieve detailed information about specific properties, utilizing structured data and adhering to Airbnb's guidelines without requiring an API key.",
      "stars": 299,
      "forks": 75,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-10-02T11:16:22Z",
      "readme_content": "# Airbnb Search & Listings - Desktop Extension (DXT)\n\nA comprehensive Desktop Extension for searching Airbnb listings with advanced filtering capabilities and detailed property information retrieval. Built as a Model Context Protocol (MCP) server packaged in the Desktop Extension (DXT) format for easy installation and use with compatible AI applications.\n\n## Features\n\n### 🔍 Advanced Search Capabilities\n- **Location-based search** with support for cities, states, and regions\n- **Google Maps Place ID** integration for precise location targeting\n- **Date filtering** with check-in and check-out date support\n- **Guest configuration** including adults, children, infants, and pets\n- **Price range filtering** with minimum and maximum price constraints\n- **Pagination support** for browsing through large result sets\n\n### 🏠 Detailed Property Information\n- **Comprehensive listing details** including amenities, policies, and highlights\n- **Location information** with coordinates and neighborhood details\n- **House rules and policies** for informed booking decisions\n- **Property descriptions** and key features\n- **Direct links** to Airbnb listings for easy booking\n\n### 🛡️ Security & Compliance\n- **Robots.txt compliance** with configurable override for testing\n- **Request timeout management** to prevent hanging requests\n- **Enhanced error handling** with detailed logging\n- **Rate limiting awareness** and respectful API usage\n- **Secure configuration** through DXT user settings\n\n## Installation\n\n### For Claude Desktop\nThis extension is packaged as a Desktop Extension (DXT) file. To install:\n\n1. Download the `.dxt` file from the releases page\n2. Open your compatible AI application (e.g., Claude Desktop)\n3. Install the extension through the application's extension manager\n4. Configure the extension settings as needed\n\n### For Cursor, etc.\n\nBefore starting make sure [Node.js](https://nodejs.org/) is installed on your desktop for `npx` to work.\n1. Go to: Cursor Settings > Tools & Integrations > New MCP Server\n\n2. Add one the following to your `mcp.json`:\n    ```json\n    {\n      \"mcpServers\": {\n        \"airbnb\": {\n          \"command\": \"npx\",\n          \"args\": [\n            \"-y\",\n            \"@openbnb/mcp-server-airbnb\"\n          ]\n        }\n      }\n    }\n    ```\n\n    To ignore robots.txt for all requests, use this version with `--ignore-robots-txt` args\n\n    ```json\n    {\n      \"mcpServers\": {\n        \"airbnb\": {\n          \"command\": \"npx\",\n          \"args\": [\n            \"-y\",\n            \"@openbnb/mcp-server-airbnb\",\n            \"--ignore-robots-txt\"\n          ]\n        }\n      }\n    }\n    ```\n3. Restart.\n\n\n## Configuration\n\nThe extension provides the following user-configurable options:\n\n### Ignore robots.txt\n- **Type**: Boolean (checkbox)\n- **Default**: `false`\n- **Description**: Bypass robots.txt restrictions when making requests to Airbnb\n- **Recommendation**: Keep disabled unless needed for testing purposes\n\n## Tools\n\n### `airbnb_search`\n\nSearch for Airbnb listings with comprehensive filtering options.\n\n**Parameters:**\n- `location` (required): Location to search (e.g., \"San Francisco, CA\")\n- `placeId` (optional): Google Maps Place ID (overrides location)\n- `checkin` (optional): Check-in date in YYYY-MM-DD format\n- `checkout` (optional): Check-out date in YYYY-MM-DD format\n- `adults` (optional): Number of adults (default: 1)\n- `children` (optional): Number of children (default: 0)\n- `infants` (optional): Number of infants (default: 0)\n- `pets` (optional): Number of pets (default: 0)\n- `minPrice` (optional): Minimum price per night\n- `maxPrice` (optional): Maximum price per night\n- `cursor` (optional): Pagination cursor for browsing results\n- `ignoreRobotsText` (optional): Override robots.txt for this request\n\n**Returns:**\n- Search results with property details, pricing, and direct links\n- Pagination information for browsing additional results\n- Search URL for reference\n\n### `airbnb_listing_details`\n\nGet detailed information about a specific Airbnb listing.\n\n**Parameters:**\n- `id` (required): Airbnb listing ID\n- `checkin` (optional): Check-in date in YYYY-MM-DD format\n- `checkout` (optional): Check-out date in YYYY-MM-DD format\n- `adults` (optional): Number of adults (default: 1)\n- `children` (optional): Number of children (default: 0)\n- `infants` (optional): Number of infants (default: 0)\n- `pets` (optional): Number of pets (default: 0)\n- `ignoreRobotsText` (optional): Override robots.txt for this request\n\n**Returns:**\n- Detailed property information including:\n  - Location details with coordinates\n  - Amenities and facilities\n  - House rules and policies\n  - Property highlights and descriptions\n  - Direct link to the listing\n\n## Technical Details\n\n### Architecture\n- **Runtime**: Node.js 18+\n- **Protocol**: Model Context Protocol (MCP) via stdio transport\n- **Format**: Desktop Extension (DXT) v0.1\n- **Dependencies**: Minimal external dependencies for security and reliability\n\n### Error Handling\n- Comprehensive error logging with timestamps\n- Graceful degradation when Airbnb's page structure changes\n- Timeout protection for network requests\n- Detailed error messages for troubleshooting\n\n### Security Measures\n- Robots.txt compliance by default\n- Request timeout limits\n- Input validation and sanitization\n- Secure environment variable handling\n- No sensitive data storage\n\n### Performance\n- Efficient HTML parsing with Cheerio\n- Request caching where appropriate\n- Minimal memory footprint\n- Fast startup and response times\n\n## Compatibility\n\n- **Platforms**: macOS, Windows, Linux\n- **Node.js**: 18.0.0 or higher\n- **Claude Desktop**: 0.10.0 or higher\n- **Other MCP clients**: Compatible with any MCP-supporting application\n\n## Development\n\n### Building from Source\n\n```bash\n# Install dependencies\nnpm install\n\n# Build the project\nnpm run build\n\n# Watch for changes during development\nnpm run watch\n```\n\n### Testing\n\nThe extension can be tested by running the MCP server directly:\n\n```bash\n# Run with robots.txt compliance (default)\nnode dist/index.js\n\n# Run with robots.txt ignored (for testing)\nnode dist/index.js --ignore-robots-txt\n```\n\n## Legal and Ethical Considerations\n\n- **Respect Airbnb's Terms of Service**: This extension is for legitimate research and booking assistance\n- **Robots.txt Compliance**: The extension respects robots.txt by default\n- **Rate Limiting**: Be mindful of request frequency to avoid overwhelming Airbnb's servers\n- **Data Usage**: Only extract publicly available information for legitimate purposes\n\n## Support\n\n- **Issues**: Report bugs and feature requests on [GitHub Issues](https://github.com/openbnb-org/mcp-server-airbnb/issues)\n- **Documentation**: Additional documentation available in the repository\n- **Community**: Join discussions about MCP and DXT development\n\n## License\n\nMIT License - see [LICENSE](LICENSE) file for details.\n\n## Contributing\n\nContributions are welcome! Please read the contributing guidelines and submit pull requests for any improvements.\n\n---\n\n**Note**: This extension is not affiliated with Airbnb, Inc. It is an independent tool designed to help users search and analyze publicly available Airbnb listings.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "airbnb",
        "openbnb",
        "listings",
        "airbnb search",
        "search airbnb",
        "airbnb listings"
      ],
      "category": "web-search"
    },
    "orange-fruit01--MCP-Test-Run": {
      "owner": "orange-fruit01",
      "name": "MCP-Test-Run",
      "url": "https://github.com/orange-fruit01/MCP-Test-Run",
      "imageUrl": "/freedevtools/mcp/pfp/orange-fruit01.webp",
      "description": "Provides tools for testing and validating the functionalities of the Model Context Protocol while ensuring integration with external data and tools.",
      "stars": 0,
      "forks": 0,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-03-17T16:41:56Z",
      "readme_content": "# MCP Service\n\nThis is a Model Control Protocol (MCP) service that provides tools for Cursor and Claude applications.\n\n## Connecting to the Service\n\n### For Cursor Users\n\n1. Open Cursor\n2. Go to Settings > AI > Advanced\n3. Under \"MCP Endpoint\", enter: `https://your-render-url.onrender.com`\n4. Save settings and restart Cursor\n\n### For Claude App Users\n\n1. Open Claude App\n2. Go to Settings > Advanced\n3. Under \"MCP Connection\", enter: `https://your-render-url.onrender.com`\n4. Save settings\n\n## Available Tools\n\n- **Web Crawler**: Crawls web pages and returns content as markdown\n\n## Deployment\n\nThis service is deployed on Render.com as a Docker container. The service is accessible at:\n\n```\nhttps://your-render-url.onrender.com\n```\n\nReplace `your-render-url.onrender.com` with the actual URL provided by Render after deployment.\n\n## Environment Variables\n\n- `MCP_HOST`: Host to bind the server (default: 0.0.0.0)\n- `MCP_PORT`: Port to bind the server (default: 8000)\n\n## Monitoring\n\nA health endpoint is available at `/health` for monitoring the service status. \n\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "testing",
        "protocol",
        "mcp",
        "context protocol",
        "tools testing",
        "fruit01 mcp"
      ],
      "category": "web-search"
    },
    "p1scess--mcp-servers": {
      "owner": "p1scess",
      "name": "mcp-servers",
      "url": "https://github.com/p1scess/mcp-servers",
      "imageUrl": "/freedevtools/mcp/pfp/p1scess.webp",
      "description": "Retrieve and process web content from URLs, converting HTML to markdown for easier information extraction. Enables chunked content retrieval for efficient parsing by LLMs.",
      "stars": 0,
      "forks": 0,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-03-27T13:42:30Z",
      "readme_content": "# Model Context Protocol servers\n\nThis repository is a collection of *reference implementations* for the [Model Context Protocol](https://modelcontextprotocol.io/) (MCP), as well as references\nto community built servers and additional resources.\n\nThe servers in this repository showcase the versatility and extensibility of MCP, demonstrating how it can be used to give Large Language Models (LLMs) secure, controlled access to tools and data sources.\nEach MCP server is implemented with either the [Typescript MCP SDK](https://github.com/modelcontextprotocol/typescript-sdk) or [Python MCP SDK](https://github.com/modelcontextprotocol/python-sdk).\n\n> Note: Lists in this README are maintained in alphabetical order to minimize merge conflicts when adding new items.\n\n## 🌟 Reference Servers\n\nThese servers aim to demonstrate MCP features and the TypeScript and Python SDKs.\n\n- **[AWS KB Retrieval](src/aws-kb-retrieval-server)** - Retrieval from AWS Knowledge Base using Bedrock Agent Runtime\n- **[Brave Search](src/brave-search)** - Web and local search using Brave's Search API\n- **[EverArt](src/everart)** - AI image generation using various models\n- **[Everything](src/everything)** - Reference / test server with prompts, resources, and tools\n- **[Fetch](src/fetch)** - Web content fetching and conversion for efficient LLM usage\n- **[Filesystem](src/filesystem)** - Secure file operations with configurable access controls\n- **[Git](src/git)** - Tools to read, search, and manipulate Git repositories\n- **[GitHub](src/github)** - Repository management, file operations, and GitHub API integration\n- **[GitLab](src/gitlab)** - GitLab API, enabling project management\n- **[Google Drive](src/gdrive)** - File access and search capabilities for Google Drive\n- **[Google Maps](src/google-maps)** - Location services, directions, and place details\n- **[Memory](src/memory)** - Knowledge graph-based persistent memory system\n- **[PostgreSQL](src/postgres)** - Read-only database access with schema inspection\n- **[Puppeteer](src/puppeteer)** - Browser automation and web scraping\n- **[Redis](src/redis)** - Interact with Redis key-value stores\n- **[Sentry](src/sentry)** - Retrieving and analyzing issues from Sentry.io\n- **[Sequential Thinking](src/sequentialthinking)** - Dynamic and reflective problem-solving through thought sequences\n- **[Slack](src/slack)** - Channel management and messaging capabilities\n- **[Sqlite](src/sqlite)** - Database interaction and business intelligence capabilities\n- **[Time](src/time)** - Time and timezone conversion capabilities\n\n## 🤝 Third-Party Servers\n\n### 🎖️ Official Integrations\n\nOfficial integrations are maintained by companies building production ready MCP servers for their platforms.\n\n- <img height=\"12\" width=\"12\" src=\"https://www.21st.dev/favicon.ico\" alt=\"21st.dev Logo\" /> **[21st.dev Magic](https://github.com/21st-dev/magic-mcp)** - Create crafted UI components inspired by the best 21st.dev design engineers.\n- <img height=\"12\" width=\"12\" src=\"https://invoxx-public-bucket.s3.eu-central-1.amazonaws.com/frontend-resources/adfin-logo-small.svg\" alt=\"Adfin Logo\" /> **[Adfin](https://github.com/Adfin-Engineering/mcp-server-adfin)** - The only platform you need to get paid - all payments in one place, invoicing and accounting reconciliations with [Adfin](https://www.adfin.com/).\n- <img height=\"12\" width=\"12\" src=\"https://www.agentql.com/favicon/favicon.png\" alt=\"AgentQL Logo\" /> **[AgentQL](https://github.com/tinyfish-io/agentql-mcp)** - Enable AI agents to get structured data from unstructured web with [AgentQL](https://www.agentql.com/).\n- <img height=\"12\" width=\"12\" src=\"https://agentrpc.com/favicon.ico\" alt=\"AgentRPC Logo\" /> **[AgentRPC](https://github.com/agentrpc/agentrpc)** - Connect to any function, any language, across network boundaries using [AgentRPC](https://www.agentrpc.com/).\n- <img height=\"12\" width=\"12\" src=\"https://aiven.io/favicon.ico\" alt=\"Aiven Logo\" /> **[Aiven](https://github.com/Aiven-Open/mcp-aiven)** - Navigate your [Aiven projects](https://go.aiven.io/mcp-server) and interact with the PostgreSQL®, Apache Kafka®, ClickHouse® and OpenSearch® services\n- <img height=\"12\" width=\"12\" src=\"https://apify.com/favicon.ico\" alt=\"Apify Logo\" /> **[Apify](https://github.com/apify/actors-mcp-server)** - [Actors MCP Server](https://apify.com/apify/actors-mcp-server): Use 3,000+ pre-built cloud tools to extract data from websites, e-commerce, social media, search engines, maps, and more\n- <img height=\"12\" width=\"12\" src=\"https://2052727.fs1.hubspotusercontent-na1.net/hubfs/2052727/cropped-cropped-apimaticio-favicon-1-32x32.png\" alt=\"APIMatic Logo\" /> **[APIMatic MCP](https://github.com/apimatic/apimatic-validator-mcp)** - APIMatic MCP Server is used to validate OpenAPI specifications using [APIMatic](https://www.apimatic.io/). The server processes OpenAPI files and returns validation summaries by leveraging APIMatic’s API.\n- <img height=\"12\" width=\"12\" src=\"https://resources.audiense.com/hubfs/favicon-1.png\" alt=\"Audiense Logo\" /> **[Audiense Insights](https://github.com/AudienseCo/mcp-audiense-insights)** - Marketing insights and audience analysis from [Audiense](https://www.audiense.com/products/audiense-insights) reports, covering demographic, cultural, influencer, and content engagement analysis.\n- <img height=\"12\" width=\"12\" src=\"https://axiom.co/favicon.ico\" alt=\"Axiom Logo\" /> **[Axiom](https://github.com/axiomhq/mcp-server-axiom)** - Query and analyze your Axiom logs, traces, and all other event data in natural language\n- <img height=\"12\" width=\"12\" src=\"https://www.bankless.com/favicon.ico\" alt=\"Bankless Logo\" /> **[Bankless Onchain](https://github.com/bankless/onchain-mcp)** - Query Onchain data, like ERC20 tokens, transaction history, smart contract state.\n- <img height=\"12\" width=\"12\" src=\"https://www.box.com/favicon.ico\" alt=\"Box Logo\" /> **[Box](https://github.com/box-community/mcp-server-box)** - Interact with the Intelligent Content Management platform through Box AI.\n- <img height=\"12\" width=\"12\" src=\"https://browserbase.com/favicon.ico\" alt=\"Browserbase Logo\" /> **[Browserbase](https://github.com/browserbase/mcp-server-browserbase)** - Automate browser interactions in the cloud (e.g. web navigation, data extraction, form filling, and more)\n- <img alt=\"favicon\" height=\"12\" width=\"12\" src=\"https://www.chargebee.com/static/resources/brand/favicon.png\" /> **[Chargebee](https://github.com/chargebee/agentkit/tree/main/modelcontextprotocol)** - MCP Server that connects AI agents to [Chargebee platform](https://www.chargebee.com).\n- <img height=\"12\" width=\"12\" src=\"https://trychroma.com/_next/static/media/chroma-logo.ae2d6e4b.svg\" /> **[Chroma](https://github.com/chroma-core/chroma-mcp)** - Embeddings, vector search, document storage, and full-text search with the open-source AI application database\n- <img height=\"12\" width=\"12\" src=\"https://www.chronulus.com/favicon/chronulus-logo-blue-on-alpha-square-128x128.ico\" alt=\"Chronulus AI Logo\" /> **[Chronulus AI](https://github.com/ChronulusAI/chronulus-mcp)** - Predict anything with Chronulus AI forecasting and prediction agents.\n- <img height=\"12\" width=\"12\" src=\"https://clickhouse.com/favicon.ico\" alt=\"ClickHouse Logo\" /> **[ClickHouse](https://github.com/ClickHouse/mcp-clickhouse)** - Query your [ClickHouse](https://clickhouse.com/) database server.\n- <img alt=\"cloudflare\" height=\"12\" width=\"12\" src=\"https://cdn.simpleicons.org/cloudflare\" /> **[Cloudflare](https://github.com/cloudflare/mcp-server-cloudflare)** - Deploy, configure & interrogate your resources on the Cloudflare developer platform (e.g. Workers/KV/R2/D1)\n- <img height=\"12\" width=\"12\" src=\"https://www.comet.com/favicon.ico\" alt=\"Comet Logo\" /> **[Comet Opik](https://github.com/comet-ml/opik-mcp)** - Query and analyze your [Opik](https://github.com/comet-ml/opik) logs, traces, prompts and all other telemtry data from your LLMs in natural language.\n- <img alt=\"favicon\" height=\"12\" width=\"12\" src=\"https://www.convex.dev/favicon.ico\" /> **[Convex](https://stack.convex.dev/convex-mcp-server)** - Introspect and query your apps deployed to Convex.\n- <img height=\"12\" width=\"12\" src=\"http://app.itsdart.com/static/img/favicon.png\" alt=\"Dart Logo\" /> **[Dart](https://github.com/its-dart/dart-mcp-server)** - Interact with task, doc, and project data in [Dart](https://itsdart.com), an AI-native project management tool\n- <img height=\"12\" width=\"12\" src=\"https://www.devhub.com/img/upload/favicon-196x196-dh.png\" alt=\"DevHub Logo\" /> **[DevHub](https://github.com/devhub/devhub-cms-mcp)** - Manage and utilize website content within the [DevHub](https://www.devhub.com) CMS platform\n- <img height=\"12\" width=\"12\" src=\"https://e2b.dev/favicon.ico\" alt=\"E2B Logo\" /> **[E2B](https://github.com/e2b-dev/mcp-server)** - Run code in secure sandboxes hosted by [E2B](https://e2b.dev)\n- <img height=\"12\" width=\"12\" src=\"https://static.edubase.net/media/brand/favicon/favicon-32x32.png\" alt=\"EduBase Logo\" /> **[EduBase](https://github.com/EduBase/MCP)** - Interact with [EduBase](https://www.edubase.net), a comprehensive e-learning platform with advanced quizzing, exam management, and content organization capabilities\n- <img height=\"12\" width=\"12\" src=\"https://esignatures.com/favicon.ico\" alt=\"eSignatures Logo\" /> **[eSignatures](https://github.com/esignaturescom/mcp-server-esignatures)** - Contract and template management for drafting, reviewing, and sending binding contracts.\n- <img height=\"12\" width=\"12\" src=\"https://exa.ai/images/favicon-32x32.png\" alt=\"Exa Logo\" /> **[Exa](https://github.com/exa-labs/exa-mcp-server)** - Search Engine made for AIs by [Exa](https://exa.ai)\n- <img height=\"12\" width=\"12\" src=\"https://fewsats.com/favicon.svg\" alt=\"Fewsats Logo\" /> **[Fewsats](https://github.com/Fewsats/fewsats-mcp)** - Enable AI Agents to purchase anything in a secure way using [Fewsats](https://fewsats.com)\n- <img height=\"12\" width=\"12\" src=\"https://fibery.io/favicon.svg\" alt=\"Fibery Logo\" /> **[Fibery](https://github.com/Fibery-inc/fibery-mcp-server)** - Perform queries and entity operations in your [Fibery](https://fibery.io) workspace.\n- <img height=\"12\" width=\"12\" src=\"https://financialdatasets.ai/favicon.ico\" alt=\"Financial Datasets Logo\" /> **[Financial Datasets](https://github.com/financial-datasets/mcp-server)** - Stock market API made for AI agents\n- <img height=\"12\" width=\"12\" src=\"https://firecrawl.dev/favicon.ico\" alt=\"Firecrawl Logo\" /> **[Firecrawl](https://github.com/mendableai/firecrawl-mcp-server)** - Extract web data with [Firecrawl](https://firecrawl.dev)\n- <img height=\"12\" width=\"12\" src=\"https://fireproof.storage/favicon.ico\" alt=\"Fireproof Logo\" /> **[Fireproof](https://github.com/fireproof-storage/mcp-database-server)** - Immutable ledger database with live synchronization\n- <img height=\"12\" width=\"12\" src=\"https://gitee.com/favicon.ico\" alt=\"Gitee Logo\" /> **[Gitee](https://github.com/oschina/mcp-gitee)** - Gitee API integration, repository, issue, and pull request management, and more.\n- <img height=\"12\" width=\"12\" src=\"https://cdn.prod.website-files.com/6605a2979ff17b2cd1939cd4/6605a460de47e7596ed84f06_icon256.png\" alt=\"gotoHuman Logo\" /> **[gotoHuman](https://github.com/gotohuman/gotohuman-mcp-server)** - Human-in-the-loop platform - Allow AI agents and automations to send requests for approval to your [gotoHuman](https://www.gotohuman.com) inbox.\n- <img height=\"12\" width=\"12\" src=\"https://grafana.com/favicon.ico\" alt=\"Grafana Logo\" /> **[Grafana](https://github.com/grafana/mcp-grafana)** - Search dashboards, investigate incidents and query datasources in your Grafana instance\n- <img height=\"12\" width=\"12\" src=\"https://framerusercontent.com/images/KCOWBYLKunDff1Dr452y6EfjiU.png\" alt=\"Graphlit Logo\" /> **[Graphlit](https://github.com/graphlit/graphlit-mcp-server)** - Ingest anything from Slack to Gmail to podcast feeds, in addition to web crawling, into a searchable [Graphlit](https://www.graphlit.com) project.\n- <img height=\"12\" width=\"12\" src=\"https://img.alicdn.com/imgextra/i3/O1CN01d9qrry1i6lTNa2BRa_!!6000000004364-2-tps-218-200.png\" alt=\"Hologres Logo\" /> **[Hologres](https://github.com/aliyun/alibabacloud-hologres-mcp-server)** - Connect to a [Hologres](https://www.alibabacloud.com/en/product/hologres) instance, get table metadata, query and analyze data.\n- <img height=\"12\" width=\"12\" src=\"https://hyperbrowser-assets-bucket.s3.us-east-1.amazonaws.com/Hyperbrowser-logo.png\" alt=\"Hyperbrowsers23 Logo\" /> **[Hyperbrowser](https://github.com/hyperbrowserai/mcp)** - [Hyperbrowser](https://www.hyperbrowser.ai/) is the next-generation platform empowering AI agents and enabling effortless, scalable browser automation.\n- **[IBM wxflows](https://github.com/IBM/wxflows/tree/main/examples/mcp/javascript)** - Tool platform by IBM to build, test and deploy tools for any data source\n- <img height=\"12\" width=\"12\" src=\"https://forevervm.com/icon.png\" alt=\"ForeverVM Logo\" /> **[ForeverVM](https://github.com/jamsocket/forevervm/tree/main/javascript/mcp-server)** - Run Python in a code sandbox.\n- <img height=\"12\" width=\"12\" src=\"https://www.getinboxzero.com/icon.png\" alt=\"Inbox Zero Logo\" /> **[Inbox Zero](https://github.com/elie222/inbox-zero/tree/main/apps/mcp-server)** - AI personal assistant for email [Inbox Zero](https://www.getinboxzero.com)\n-  **[Inkeep](https://github.com/inkeep/mcp-server-python)** - RAG Search over your content powered by [Inkeep](https://inkeep.com)\n- <img height=\"12\" width=\"12\" src=\"https://integration.app/favicon.ico\" alt=\"Integration App Icon\" /> **[Integration App](https://github.com/integration-app/mcp-server)** - Interact with any other SaaS applications on behalf of your customers.\n- <img alt=\"jetbrains\" height=\"12\" width=\"12\" src=\"https://cdn.simpleicons.org/jetbrains\" /> **[JetBrains](https://github.com/JetBrains/mcp-jetbrains)** – Work on your code with JetBrains IDEs\n- <img height=\"12\" width=\"12\" src=\"https://kagi.com/favicon.ico\" alt=\"Kagi Logo\" /> **[Kagi Search](https://github.com/kagisearch/kagimcp)** - Search the web using Kagi's search API\n- <img height=\"12\" width=\"12\" src=\"https://logfire.pydantic.dev/favicon.ico\" alt=\"Logfire Logo\" /> **[Logfire](https://github.com/pydantic/logfire-mcp)** - Provides access to OpenTelemetry traces and metrics through Logfire.\n- <img height=\"12\" width=\"12\" src=\"https://langfuse.com/favicon.ico\" alt=\"Langfuse Logo\" /> **[Langfuse Prompt Management](https://github.com/langfuse/mcp-server-langfuse)** - Open-source tool for collaborative editing, versioning, evaluating, and releasing prompts.\n- <img height=\"12\" width=\"12\" src=\"https://lingo.dev/favicon.ico\" alt=\"Lingo.dev Logo\" /> **[Lingo.dev](https://github.com/lingodotdev/lingo.dev/blob/main/mcp.md)** - Make your AI agent speak every language on the planet, using [Lingo.dev](https://lingo.dev) Localization Engine.\n- <img height=\"12\" width=\"12\" src=\"https://www.mailgun.com/favicon.ico\" alt=\"Mailgun Logo\" /> **[Mailgun](https://github.com/mailgun/mailgun-mcp-server)** - Interact with Mailgun API.\n- <img height=\"12\" width=\"12\" src=\"https://www.make.com/favicon.ico\" alt=\"Make Logo\" /> **[Make](https://github.com/integromat/make-mcp-server)** - Turn your [Make](https://www.make.com/) scenarios into callable tools for AI assistants.\n- <img height=\"12\" width=\"12\" src=\"https://www.meilisearch.com/favicon.ico\" alt=\"Meilisearch Logo\" /> **[Meilisearch](https://github.com/meilisearch/meilisearch-mcp)** - Interact & query with Meilisearch (Full-text & semantic search API)\n-  **[Metoro](https://github.com/metoro-io/metoro-mcp-server)** - Query and interact with kubernetes environments monitored by Metoro\n- <img alt=\"favicon_32x32\" height=\"12\" width=\"12\" src=\"https://milvus.io/favicon-32x32.png\" /> **[Milvus](https://github.com/zilliztech/mcp-server-milvus)** - Search, Query and interact with data in your Milvus Vector Database.\n- <img height=\"12\" width=\"12\" src=\"https://www.motherduck.com/favicon.ico\" alt=\"MotherDuck Logo\" /> **[MotherDuck](https://github.com/motherduckdb/mcp-server-motherduck)** - Query and analyze data with MotherDuck and local DuckDB\n- <img height=\"12\" width=\"12\" src=\"https://needle-ai.com/images/needle-logo-orange-2-rounded.png\" alt=\"Needle AI Logo\" /> **[Needle](https://github.com/needle-ai/needle-mcp)** - Production-ready RAG out of the box to search and retrieve data from your own documents.\n- <img height=\"12\" width=\"12\" src=\"https://neo4j.com/favicon.ico\" alt=\"Neo4j Logo\" /> **[Neo4j](https://github.com/neo4j-contrib/mcp-neo4j/)** - Neo4j graph database server (schema + read/write-cypher) and separate graph database backed memory\n- <img height=\"12\" width=\"12\" src=\"https://avatars.githubusercontent.com/u/183852044?s=48&v=4\" alt=\"Neon Logo\" /> **[Neon](https://github.com/neondatabase/mcp-server-neon)** - Interact with the Neon serverless Postgres platform\n- <img height=\"12\" width=\"12\" src=\"https://avatars.githubusercontent.com/u/82347605?s=48&v=4\" alt=\"OceanBase Logo\" /> **[OceanBase](https://github.com/oceanbase/mcp-oceanbase)** - MCP Server for OceanBase database and its tools\n- <img height=\"12\" width=\"12\" src=\"https://docs.octagonagents.com/logo.svg\" alt=\"Octagon Logo\" /> **[Octagon](https://github.com/OctagonAI/octagon-mcp-server)** - Deliver real-time investment research with extensive private and public market data.\n- <img height=\"12\" width=\"12\" src=\"https://oxylabs.io/favicon.ico\" alt=\"Oxylabs Logo\" /> **[Oxylabs](https://github.com/oxylabs/oxylabs-mcp)** - Scrape websites with Oxylabs Web API, supporting dynamic rendering and parsing for structured data extraction.\n- <img height=\"12\" width=\"12\" src=\"https://www.perplexity.ai/favicon.ico\" alt=\"Perplexity Logo\" /> **[Perplexity](https://github.com/ppl-ai/modelcontextprotocol)** - An MCP server that connects to Perplexity's Sonar API, enabling real-time web-wide research in conversational AI.\n- <img alt=\"logomark\" height=\"12\" width=\"12\" src=\"https://qdrant.tech/img/brand-resources-logos/logomark.svg\" /> **[Qdrant](https://github.com/qdrant/mcp-server-qdrant/)** - Implement semantic memory layer on top of the Qdrant vector search engine\n- <img alt=\"favicon\" height=\"12\" width=\"12\" src=\"https://www.ramp.com/favicon.ico\" /> **[Ramp](https://github.com/ramp-public/ramp-mcp)** - Interact with [Ramp](https://ramp.com)'s Developer API to run analysis on your spend and gain insights leveraging LLMs\n- **[Raygun](https://github.com/MindscapeHQ/mcp-server-raygun)** - Interact with your crash reporting and real using monitoring data on your Raygun account\n- <img height=\"12\" width=\"12\" src=\"https://www.rember.com/favicon.ico\" alt=\"Rember Logo\" /> **[Rember](https://github.com/rember/rember-mcp)** - Create spaced repetition flashcards in [Rember](https://rember.com) to remember anything you learn in your chats\n- <img height=\"12\" width=\"12\" src=\"https://riza.io/favicon.ico\" alt=\"Riza logo\" /> **[Riza](https://github.com/riza-io/riza-mcp)** - Arbitrary code execution and tool-use platform for LLMs by [Riza](https://riza.io)\n- <img alt=\"56912e614b35093426c515860f9f2234\" height=\"12\" width=\"12\" src=\"https://pics.fatwang2.com/56912e614b35093426c515860f9f2234.svg\" /> [Search1API](https://github.com/fatwang2/search1api-mcp) - One API for Search, Crawling, and Sitemaps\n- <img height=\"12\" width=\"12\" src=\"https://screenshotone.com/favicon.ico\" alt=\"ScreenshotOne Logo\" /> **[ScreenshotOne](https://github.com/screenshotone/mcp/)** - Render website screenshots with [ScreenshotOne](https://screenshotone.com/)\n- <img height=\"12\" width=\"12\" src=\"https://semgrep.dev/favicon.ico\" alt=\"Semgrep Logo\" /> **[Semgrep](https://github.com/semgrep/mcp)** - Enable AI agents to secure code with [Semgrep](https://semgrep.dev/).\n- <img height=\"12\" width=\"12\" src=\"https://www.starrocks.io/favicon.ico\" alt=\"StarRocks Logo\" /> **[StarRocks](https://github.com/StarRocks/mcp-server-starrocks)** - Interact with [StarRocks](https://www.starrocks.io/)\n- <img height=\"12\" width=\"12\" src=\"https://stripe.com/favicon.ico\" alt=\"Stripe Logo\" /> **[Stripe](https://github.com/stripe/agent-toolkit)** - Interact with Stripe API\n- <img height=\"12\" width=\"12\" src=\"https://tavily.com/favicon.ico\" alt=\"Tavily Logo\" /> **[Tavily](https://github.com/tavily-ai/tavily-mcp)** - Search engine for AI agents (search + extract) powered by [Tavily](https://tavily.com/)\n- <img height=\"12\" width=\"12\" src=\"https://thirdweb.com/favicon.ico\" alt=\"Thirdweb Logo\" /> **[Thirdweb](https://github.com/thirdweb-dev/ai/tree/main/python/thirdweb-mcp)** - Read/write to over 2k blockchains, enabling data querying, contract analysis/deployment, and transaction execution, powered by [Thirdweb](https://thirdweb.com/)\n- <img height=\"12\" width=\"12\" src=\"https://www.tinybird.co/favicon.ico\" alt=\"Tinybird Logo\" /> **[Tinybird](https://github.com/tinybirdco/mcp-tinybird)** - Interact with Tinybird serverless ClickHouse platform\n- <img height=\"12\" width=\"12\" src=\"https://unifai.network/favicon.ico\" alt=\"UnifAI Logo\" /> **[UnifAI](https://github.com/unifai-network/unifai-mcp-server)** - Dynamically search and call tools using [UnifAI Network](https://unifai.network)\n- <img height=\"12\" width=\"12\" src=\"https://framerusercontent.com/images/plcQevjrOYnyriuGw90NfQBPoQ.jpg\" alt=\"Unstructured Logo\" /> **[Unstructured](https://github.com/Unstructured-IO/UNS-MCP)** - Set up and interact with your unstructured data processing workflows in [Unstructured Platform](https://unstructured.io)\n- **[Vectorize](https://github.com/vectorize-io/vectorize-mcp-server/)** - [Vectorize](https://vectorize.io) MCP server for advanced retrieval, Private Deep Research, Anything-to-Markdown file extraction and text chunking.\n- <img height=\"12\" width=\"12\" src=\"https://verodat.io/assets/favicon-16x16.png\" alt=\"Verodat Logo\" /> **[Verodat](https://github.com/Verodat/verodat-mcp-server)** - Interact with Verodat AI Ready Data platform\n- <img height=\"12\" width=\"12\" src=\"https://www.veyrax.com/favicon.ico\" alt=\"VeyraX Logo\" /> **[VeyraX](https://github.com/VeyraX/veyrax-mcp)** - Single tool to control all 100+ API integrations, and UI components\n- <img height=\"12\" width=\"12\" src=\"https://www.xero.com/favicon.ico\" alt=\"Xero Logo\" /> **[Xero](https://github.com/XeroAPI/xero-mcp-server)** - Interact with the accounting data in your business using our official MCP server\n- **[ZenML](https://github.com/zenml-io/mcp-zenml)** - Interact with your MLOps and LLMOps pipelines through your [ZenML](https://www.zenml.io) MCP server\n\n### 🌎 Community Servers\n\nA growing set of community-developed and maintained servers demonstrates various applications of MCP across different domains.\n\n> **Note:** Community servers are **untested** and should be used at **your own risk**. They are not affiliated with or endorsed by Anthropic.\n- **[Ableton Live](https://github.com/Simon-Kansara/ableton-live-mcp-server)** - an MCP server to control Ableton Live.\n- **[Airbnb](https://github.com/openbnb-org/mcp-server-airbnb)** - Provides tools to search Airbnb and get listing details.\n- **[Algorand](https://github.com/GoPlausible/algorand-mcp)** - A comprehensive MCP server for tooling interactions (40+) and resource accessibility (60+) plus many useful prompts for interacting with the Algorand blockchain.\n- **[Airflow](https://github.com/yangkyeongmo/mcp-server-apache-airflow)** - A MCP Server that connects to [Apache Airflow](https://airflow.apache.org/) using official python client.\n- **[Airtable](https://github.com/domdomegg/airtable-mcp-server)** - Read and write access to [Airtable](https://airtable.com/) databases, with schema inspection.\n- **[Airtable](https://github.com/felores/airtable-mcp)** - Airtable Model Context Protocol Server.\n- **[AlphaVantage](https://github.com/calvernaz/alphavantage)** - MCP server for stock market data API [AlphaVantage](https://www.alphavantage.co)\n- **[Anki](https://github.com/scorzeth/anki-mcp-server)** - An MCP server for interacting with your [Anki](https://apps.ankiweb.net) decks and cards.\n- **[Any Chat Completions](https://github.com/pyroprompts/any-chat-completions-mcp)** - Interact with any OpenAI SDK Compatible Chat Completions API like OpenAI, Perplexity, Groq, xAI and many more.\n- **[Apple Calendar](https://github.com/Omar-v2/mcp-ical)** - An MCP server that allows you to interact with your MacOS Calendar through natural language, including features such as event creation, modification, schedule listing, finding free time slots etc.\n- **[ArangoDB](https://github.com/ravenwits/mcp-server-arangodb)** - MCP Server that provides database interaction capabilities through [ArangoDB](https://arangodb.com/).\n- **[Arduino](https://github.com/vishalmysore/choturobo)** - MCP Server that enables AI-powered robotics using Claude AI and Arduino (ESP32) for real-world automation and interaction with robots.\n- **[Atlassian](https://github.com/sooperset/mcp-atlassian)** - Interact with Atlassian Cloud products (Confluence and Jira) including searching/reading Confluence spaces/pages, accessing Jira issues, and project metadata.\n- **[AWS](https://github.com/rishikavikondala/mcp-server-aws)** - Perform operations on your AWS resources using an LLM.\n- **[AWS Athena](https://github.com/lishenxydlgzs/aws-athena-mcp)** - A MCP server for AWS Athena to run SQL queries on Glue Catalog.\n- **[AWS Cost Explorer](https://github.com/aarora79/aws-cost-explorer-mcp-server)** - Optimize your AWS spend (including Amazon Bedrock spend) with this MCP server by examining spend across regions, services, instance types and foundation models ([demo video](https://www.youtube.com/watch?v=WuVOmYLRFmI&feature=youtu.be)).\n- **[AWS Resources Operations](https://github.com/baryhuang/mcp-server-aws-resources-python)** - Run generated python code to securely query or modify any AWS resources supported by boto3.\n- **[AWS S3](https://github.com/aws-samples/sample-mcp-server-s3)** - A sample MCP server for AWS S3 that flexibly fetches objects from S3 such as PDF documents.\n- **[Azure ADX](https://github.com/pab1it0/adx-mcp-server)** - Query and analyze Azure Data Explorer databases.\n- **[Azure DevOps](https://github.com/Vortiago/mcp-azure-devops)** - An MCP server that provides a bridge to Azure DevOps services, enabling AI assistants to query and manage work items.\n- **[Base Free USDC Transfer](https://github.com/magnetai/mcp-free-usdc-transfer)** - Send USDC on [Base](https://base.org) for free using Claude AI! Built with [Coinbase CDP](https://docs.cdp.coinbase.com/mpc-wallet/docs/welcome).\n* **[Basic Memory](https://github.com/basicmachines-co/basic-memory)** - Local-first knowledge management system that builds a semantic graph from Markdown files, enabling persistent memory across conversations with LLMs.\n- **[BigQuery](https://github.com/LucasHild/mcp-server-bigquery)** (by LucasHild) - This server enables LLMs to inspect database schemas and execute queries on BigQuery.\n- **[BigQuery](https://github.com/ergut/mcp-bigquery-server)** (by ergut) - Server implementation for Google BigQuery integration that enables direct BigQuery database access and querying capabilities\n- **[Bing Web Search API](https://github.com/leehanchung/bing-search-mcp)** (by hanchunglee) - Server implementation for Microsoft Bing Web Search API.\n- **[Bitable MCP](https://github.com/lloydzhou/bitable-mcp)** (by lloydzhou) - MCP server provides access to Lark Bitable through the Model Context Protocol. It allows users to interact with Bitable tables using predefined tools.\n- **[Blender](https://github.com/ahujasid/blender-mcp)** (by ahujasid) - Blender integration allowing prompt enabled 3D scene creation, modeling and manipulation.\n- **[Bsc-mcp](https://github.com/TermiX-official/bsc-mcp)** The first MCP server that serves as the bridge between AI and BNB Chain, enabling AI agents to execute complex on-chain operations through seamless integration with the BNB Chain, including transfer, swap, launch, security check on any token and even more.\n- **[CFBD API](https://github.com/lenwood/cfbd-mcp-server)** - An MCP server for the [College Football Data API](https://collegefootballdata.com/).\n- **[ChatMCP](https://github.com/AI-QL/chat-mcp)** – An Open Source Cross-platform GUI Desktop application compatible with Linux, macOS, and Windows, enabling seamless interaction with MCP servers across dynamically selectable LLMs, by **[AIQL](https://github.com/AI-QL)**\n- **[ChatSum](https://github.com/mcpso/mcp-server-chatsum)** - Query and Summarize chat messages with LLM. by [mcpso](https://mcp.so)\n- **[Chroma](https://github.com/privetin/chroma)** - Vector database server for semantic document search and metadata filtering, built on Chroma\n- **[ClaudePost](https://github.com/ZilongXue/claude-post)** - ClaudePost enables seamless email management for Gmail, offering secure features like email search, reading, and sending.\n- **[Cloudinary](https://github.com/felores/cloudinary-mcp-server)** - Cloudinary Model Context Protocol Server to upload media to Cloudinary and get back the media link and details.\n- **[code-assistant](https://github.com/stippi/code-assistant)** - A coding assistant MCP server that allows to explore a code-base and make changes to code. Should be used with trusted repos only (insufficient protection against prompt injections).\n- **[code-executor](https://github.com/bazinga012/mcp_code_executor)** - An MCP server that allows LLMs to execute Python code within a specified Conda environment.\n- **[code-sandbox-mcp](https://github.com/Automata-Labs-team/code-sandbox-mcp)** - An MCP server to create secure code sandbox environment for executing code within Docker containers.\n- **[cognee-mcp](https://github.com/topoteretes/cognee/tree/main/cognee-mcp)** - GraphRAG memory server with customizable ingestion, data processing and search\n- **[coin_api_mcp](https://github.com/longmans/coin_api_mcp)** - Provides access to [coinmarketcap](https://coinmarketcap.com/) cryptocurrency data.\n- **[Contentful-mcp](https://github.com/ivo-toby/contentful-mcp)** - Read, update, delete, publish content in your [Contentful](https://contentful.com) space(s) from this MCP Server.\n- **[crypto-feargreed-mcp](https://github.com/kukapay/crypto-feargreed-mcp)**  -  Providing real-time and historical Crypto Fear & Greed Index data.\n- **[cryptopanic-mcp-server](https://github.com/kukapay/cryptopanic-mcp-server)** - Providing latest cryptocurrency news to AI agents, powered by CryptoPanic.\n- **[Dappier](https://github.com/DappierAI/dappier-mcp)** - Connect LLMs to real-time, rights-cleared, proprietary data from trusted sources. Access specialized models for Real-Time Web Search, News, Sports, Financial Data, Crypto, and premium publisher content. Explore data models at [marketplace.dappier.com](https://marketplace.dappier.com/marketplace).\n- **[Databricks](https://github.com/JordiNeil/mcp-databricks-server)** - Allows LLMs to run SQL queries, list and get details of jobs executions in a Databricks account.\n- **[Data Exploration](https://github.com/reading-plus-ai/mcp-server-data-exploration)** - MCP server for autonomous data exploration on .csv-based datasets, providing intelligent insights with minimal effort. NOTE: Will execute arbitrary Python code on your machine, please use with caution!\n- **[Dataset Viewer](https://github.com/privetin/dataset-viewer)** - Browse and analyze Hugging Face datasets with features like search, filtering, statistics, and data export\n- **[DBHub](https://github.com/bytebase/dbhub/)** - Universal database MCP server connecting to MySQL, PostgreSQL, SQLite, DuckDB and etc.\n- **[DeepSeek MCP Server](https://github.com/DMontgomery40/deepseek-mcp-server)** - Model Context Protocol server integrating DeepSeek's advanced language models, in addition to [other useful API endpoints](https://github.com/DMontgomery40/deepseek-mcp-server?tab=readme-ov-file#features)\n- **[Deepseek_R1](https://github.com/66julienmartin/MCP-server-Deepseek_R1)** - A Model Context Protocol (MCP) server implementation connecting Claude Desktop with DeepSeek's language models (R1/V3)\n- **[deepseek-thinker-mcp](https://github.com/ruixingshi/deepseek-thinker-mcp)** - A MCP (Model Context Protocol) provider Deepseek reasoning content to MCP-enabled AI Clients, like Claude Desktop. Supports access to Deepseek's thought processes from the Deepseek API service or from a local Ollama server.\n- **[Descope](https://github.com/descope-sample-apps/descope-mcp-server)** - An MCP server to integrate with [Descope](https://descope.com) to search audit logs, manage users, and more.\n- **[DevRev](https://github.com/kpsunil97/devrev-mcp-server)** - An MCP server to integrate with DevRev APIs to search through your DevRev Knowledge Graph where objects can be imported from diff. sources listed [here](https://devrev.ai/docs/import#available-sources).\n- **[Dicom](https://github.com/ChristianHinge/dicom-mcp)** - An MCP server to query and retrieve medical images and for parsing and reading dicom-encapsulated documents (pdf etc.). \n- **[Dify](https://github.com/YanxingLiu/dify-mcp-server)** - A simple implementation of an MCP server for dify workflows.\n- **[Discord](https://github.com/v-3/discordmcp)** - A MCP server to connect to Discord guilds through a bot and read and write messages in channels\n- **[Discord](https://github.com/SaseQ/discord-mcp)** - A MCP server, which connects to Discord through a bot, and provides comprehensive integration with Discord.\n- **[Discourse](https://github.com/AshDevFr/discourse-mcp-server)** - A MCP server to search Discourse posts on a Discourse forum.\n- **[Docker](https://github.com/ckreiling/mcp-server-docker)** - Integrate with Docker to manage containers, images, volumes, and networks.\n- **[Drupal](https://github.com/Omedia/mcp-server-drupal)** - Server for interacting with [Drupal](https://www.drupal.org/project/mcp) using STDIO transport layer.\n- **[dune-analytics-mcp](https://github.com/kukapay/dune-analytics-mcp)** -  A mcp server that bridges Dune Analytics data to AI agents.\n- **[Elasticsearch](https://github.com/cr7258/elasticsearch-mcp-server)** - MCP server implementation that provides Elasticsearch interaction.\n- **[ElevenLabs](https://github.com/mamertofabian/elevenlabs-mcp-server)** - A server that integrates with ElevenLabs text-to-speech API capable of generating full voiceovers with multiple voices.\n- **[Ergo Blockchain MCP](https://github.com/marctheshark3/ergo-mcp)** -An MCP server to integrate Ergo Blockchain Node and Explorer APIs for checking address balances, analyzing transactions, viewing transaction history, performing forensic analysis of addresses, searching for tokens, and monitoring network status.\n- **[Eunomia](https://github.com/whataboutyou-ai/eunomia-MCP-server)** - Extension of the Eunomia framework that connects Eunomia instruments with MCP servers\n- **[EVM MCP Server](https://github.com/mcpdotdirect/evm-mcp-server)** - Comprehensive blockchain services for 30+ EVM networks, supporting native tokens, ERC20, NFTs, smart contracts, transactions, and ENS resolution.\n- **[Everything Search](https://github.com/mamertofabian/mcp-everything-search)** - Fast file searching capabilities across Windows (using [Everything SDK](https://www.voidtools.com/support/everything/sdk/)), macOS (using mdfind command), and Linux (using locate/plocate command).\n- **[Excel](https://github.com/haris-musa/excel-mcp-server)** - Excel manipulation including data reading/writing, worksheet management, formatting, charts, and pivot table.\n- **[Fantasy PL](https://github.com/rishijatia/fantasy-pl-mcp)** - Give your coding agent direct access to up-to date Fantasy Premier League data\n- **[fastn.ai – Unified API MCP Server](https://github.com/fastnai/mcp-fastn)** - A remote, dynamic MCP server with a unified API that connects to 1,000+ tools, actions, and workflows, featuring built-in authentication and monitoring.\n- **[Fetch](https://github.com/zcaceres/fetch-mcp)** - A server that flexibly fetches HTML, JSON, Markdown, or plaintext.\n- **[Fingertip](https://github.com/fingertip-com/fingertip-mcp)** - MCP server for Fingertip.com to search and create new sites.\n- **[Figma](https://github.com/GLips/Figma-Context-MCP)** - Give your coding agent direct access to Figma file data, helping it one-shot design implementation.\n- **[Firebase](https://github.com/gannonh/firebase-mcp)** - Server to interact with Firebase services including Firebase Authentication, Firestore, and Firebase Storage.\n- **[FireCrawl](https://github.com/vrknetha/mcp-server-firecrawl)** - Advanced web scraping with JavaScript rendering, PDF support, and smart rate limiting\n- **[FlightRadar24](https://github.com/sunsetcoder/flightradar24-mcp-server)** - A Claude Desktop MCP server that helps you track flights in real-time using Flightradar24 data.\n- **[Ghost](https://github.com/MFYDev/ghost-mcp)** - A Model Context Protocol (MCP) server for interacting with Ghost CMS through LLM interfaces like Claude.\n- **[Github Actions](https://github.com/ko1ynnky/github-actions-mcp-server)** - A Model Context Protocol (MCP) server for interacting with Github Actions.\n- **[Glean](https://github.com/longyi1207/glean-mcp-server)** - A server that uses Glean API to search and chat.\n- **[Gmail](https://github.com/GongRzhe/Gmail-MCP-Server)** - A Model Context Protocol (MCP) server for Gmail integration in Claude Desktop with auto authentication support.\n- **[Gmail Headless](https://github.com/baryhuang/mcp-headless-gmail)** - Remote hostable MCP server that can get and send Gmail messages without local credential or file system setup.\n- **[Goal Story](https://github.com/hichana/goalstory-mcp)** - a Goal Tracker and Visualization Tool for personal and professional development.\n- **[GOAT](https://github.com/goat-sdk/goat/tree/main/typescript/examples/by-framework/model-context-protocol)** - Run more than +200 onchain actions on any blockchain including Ethereum, Solana and Base.\n- **[Godot](https://github.com/Coding-Solo/godot-mcp)** - A MCP server providing comprehensive Godot engine integration for project editing, debugging, and scene management.\n- **[Golang Filesystem Server](https://github.com/mark3labs/mcp-filesystem-server)** - Secure file operations with configurable access controls built with Go!\n- **[Goodnews](https://github.com/VectorInstitute/mcp-goodnews)** - A simple MCP server that delivers curated positive and uplifting news stories.\n- **[Google Calendar](https://github.com/v-3/google-calendar)** - Integration with Google Calendar to check schedules, find time, and add/delete events\n- **[Google Calendar](https://github.com/nspady/google-calendar-mcp)** - Google Calendar MCP Server for managing Google calendar events. Also supports searching for events by attributes like title and location.\n- **[Google Custom Search](https://github.com/adenot/mcp-google-search)** - Provides Google Search results via the Google Custom Search API\n- **[Google Tasks](https://github.com/zcaceres/gtasks-mcp)** - Google Tasks API Model Context Protocol Server.\n- **[GraphQL Schema](https://github.com/hannesj/mcp-graphql-schema)** - Allow LLMs to explore large GraphQL schemas without bloating the context.\n- **[HDW LinkedIn](https://github.com/horizondatawave/hdw-mcp-server)** - Access to profile data and management of user account with [HorizonDataWave.ai](https://horizondatawave.ai/).\n- **[Heurist Mesh Agent](https://github.com/heurist-network/heurist-mesh-mcp-server)** - Access specialized web3 AI agents for blockchain analysis, smart contract security, token metrics, and blockchain interactions through the [Heurist Mesh network](https://github.com/heurist-network/heurist-agent-framework/tree/main/mesh).\n- **[Holaspirit](https://github.com/syucream/holaspirit-mcp-server)** - Interact with [Holaspirit](https://www.holaspirit.com/).\n- **[Home Assistant](https://github.com/tevonsb/homeassistant-mcp)** - Interact with [Home Assistant](https://www.home-assistant.io/) including viewing and controlling lights, switches, sensors, and all other Home Assistant entities.\n- **[Home Assistant](https://github.com/voska/hass-mcp)** - Docker-ready MCP server for Home Assistant with entity management, domain summaries, automation support, and guided conversations. Includes pre-built container images for easy installation.\n- **[HubSpot](https://github.com/buryhuang/mcp-hubspot)** - HubSpot CRM integration for managing contacts and companies. Create and retrieve CRM data directly through Claude chat.\n- **[HuggingFace Spaces](https://github.com/evalstate/mcp-hfspace)** - Server for using HuggingFace Spaces, supporting Open Source Image, Audio, Text Models and more. Claude Desktop mode for easy integration.\n- **[Hyperliquid](https://github.com/mektigboy/server-hyperliquid)** - An MCP server implementation that integrates the Hyperliquid SDK for exchange data.\n- **[Image Generation](https://github.com/GongRzhe/Image-Generation-MCP-Server)** - This MCP server provides image generation capabilities using the Replicate Flux model.\n- **[InfluxDB](https://github.com/idoru/influxdb-mcp-server)** - Run queries against InfluxDB OSS API v2.\n- **[Inoyu](https://github.com/sergehuber/inoyu-mcp-unomi-server)** - Interact with an Apache Unomi CDP customer data platform to retrieve and update customer profiles\n- **[Intercom](https://github.com/raoulbia-ai/mcp-server-for-intercom)** - An MCP-compliant server for retrieving customer support tickets from Intercom. This tool enables AI assistants like Claude Desktop and Cline to access and analyze your Intercom support tickets.\n- **[iTerm MCP](https://github.com/ferrislucas/iterm-mcp)** - Integration with iTerm2 terminal emulator for macOS, enabling LLMs to execute and monitor terminal commands.\n- **[JavaFX](https://github.com/mcpso/mcp-server-javafx)** - Make drawings using a JavaFX canvas\n- **[JDBC](https://github.com/quarkiverse/quarkus-mcp-servers/tree/main/jdbc)** - Connect to any JDBC-compatible database and query, insert, update, delete, and more. Supports MySQL, PostgreSQL, Oracle, SQL Server, sqllite and [more](https://github.com/quarkiverse/quarkus-mcp-servers/tree/main/jdbc#supported-jdbc-variants).\n- **[JSON](https://github.com/GongRzhe/JSON-MCP-Server)** - JSON handling and processing server with advanced query capabilities using JSONPath syntax and support for array, string, numeric, and date operations.\n- **[KiCad MCP](https://github.com/lamaalrajih/kicad-mcp)** - MCP server for KiCad on Mac, Windows, and Linux.\n- **[Keycloak MCP](https://github.com/ChristophEnglisch/keycloak-model-context-protocol)** - This MCP server enables natural language interaction with Keycloak for user and realm management including creating, deleting, and listing users and realms.\n- **[Kibela](https://github.com/kiwamizamurai/mcp-kibela-server)** (by kiwamizamurai) - Interact with Kibela API.\n- **[kintone](https://github.com/macrat/mcp-server-kintone)** - Manage records and apps in [kintone](https://kintone.com) through LLM tools.\n- **[Kubernetes](https://github.com/Flux159/mcp-server-kubernetes)** - Connect to Kubernetes cluster and manage pods, deployments, and services.\n- **[Kubernetes and OpenShift](https://github.com/manusa/kubernetes-mcp-server)** - A powerful Kubernetes MCP server with additional support for OpenShift. Besides providing CRUD operations for any Kubernetes resource, this server provides specialized tools to interact with your cluster.\n- **[Langflow-DOC-QA-SERVER](https://github.com/GongRzhe/Langflow-DOC-QA-SERVER)** - A Model Context Protocol server for document Q&A powered by Langflow. It demonstrates core MCP concepts by providing a simple interface to query documents through a Langflow backend.\n- **[Lightdash](https://github.com/syucream/lightdash-mcp-server)** - Interact with [Lightdash](https://www.lightdash.com/), a BI tool.\n- **[Linear](https://github.com/jerhadf/linear-mcp-server)** - Allows LLM to interact with Linear's API for project management, including searching, creating, and updating issues.\n- **[Linear (Go)](https://github.com/geropl/linear-mcp-go)** - Allows LLM to interact with Linear's API via a single static binary.\n- **[LINE](https://github.com/amornpan/py-mcp-line)** (by amornpan) - Implementation for LINE Bot integration that enables Language Models to read and analyze LINE conversations through a standardized interface. Features asynchronous operation, comprehensive logging, webhook event handling, and support for various message types.\n- **[LlamaCloud](https://github.com/run-llama/mcp-server-llamacloud)** (by marcusschiesser) - Integrate the data stored in a managed index on [LlamaCloud](https://cloud.llamaindex.ai/)\n- **[llm-context](https://github.com/cyberchitta/llm-context.py)** - Provides a repo-packing MCP tool with configurable profiles that specify file inclusion/exclusion patterns and optional prompts.\n- **[mac-messages-mcp](https://github.com/carterlasalle/mac_messages_mcp)** - An MCP server that securely interfaces with your iMessage database via the Model Context Protocol (MCP), allowing LLMs to query and analyze iMessage conversations. It includes robust phone number validation, attachment processing, contact management, group chat handling, and full support for sending and receiving messages.\n- **[MariaDB](https://github.com/abel9851/mcp-server-mariadb)** - MariaDB database integration with configurable access controls in Python.\n- **[Maton](https://github.com/maton-ai/agent-toolkit/tree/main/modelcontextprotocol)** - Connect to your SaaS tools like HubSpot, Salesforce, and more.\n- **[MCP Compass](https://github.com/liuyoshio/mcp-compass)** - Suggest the right MCP server for your needs\n- **[MCP Create](https://github.com/tesla0225/mcp-create)** - A dynamic MCP server management service that creates, runs, and manages Model Context Protocol servers on-the-fly.\n- **[MCP Installer](https://github.com/anaisbetts/mcp-installer)** - This server is a server that installs other MCP servers for you.\n- **[mcp-k8s-go](https://github.com/strowk/mcp-k8s-go)** - Golang-based Kubernetes server for MCP to browse pods and their logs, events, namespaces and more. Built to be extensible.\n- **[mcp-local-rag](https://github.com/nkapila6/mcp-local-rag)** - \"primitive\" RAG-like web search model context protocol (MCP) server that runs locally using Google's MediaPipe Text Embedder and DuckDuckGo Search. ✨ no APIs required ✨.\n- **[mcp-proxy](https://github.com/sparfenyuk/mcp-proxy)** - Connect to MCP servers that run on SSE transport, or expose stdio servers as an SSE server.\n- **[mem0-mcp](https://github.com/mem0ai/mem0-mcp)** - A Model Context Protocol server for Mem0, which helps with managing coding preferences.\n- **[MSSQL](https://github.com/aekanun2020/mcp-server/)** - MSSQL database integration with configurable access controls and schema inspection\n- **[MSSQL](https://github.com/JexinSam/mssql_mcp_server)** (by jexin) - MCP Server for MSSQL database in Python\n- **[MSSQL-Python](https://github.com/amornpan/py-mcp-mssql)** (by amornpan) - A read-only Python implementation for MSSQL database access with enhanced security features, configurable access controls, and schema inspection capabilities. Focuses on safe database interaction through Python ecosystem.\n- **[MSSQL-MCP](https://github.com/daobataotie/mssql-mcp)** (by daobataotie) - MSSQL MCP that refer to the official website's SQLite MCP for modifications to adapt to MSSQL\n- **[Markdownify](https://github.com/zcaceres/mcp-markdownify-server)** - MCP to convert almost anything to Markdown (PPTX, HTML, PDF, Youtube Transcripts and more)\n- **[Mindmap](https://github.com/YuChenSSR/mindmap-mcp-server)** (by YuChenSSR) - A server that generates mindmaps from input containing markdown code.\n- **[Minima](https://github.com/dmayboroda/minima)** - MCP server for RAG on local files\n- **[MongoDB](https://github.com/kiliczsh/mcp-mongo-server)** - A Model Context Protocol Server for MongoDB.\n- **[MongoDB Lens](https://github.com/furey/mongodb-lens)** - Full Featured MCP Server for MongoDB Databases.\n- **[Monday.com](https://github.com/sakce/mcp-server-monday)** - MCP Server to interact with Monday.com boards and items.\n- **[Multicluster-MCP-Sever](https://github.com/yanmxa/multicluster-mcp-server)** - The gateway for GenAI systems to interact with multiple Kubernetes clusters.\n- **[MySQL](https://github.com/benborla/mcp-server-mysql)** (by benborla) - MySQL database integration in NodeJS with configurable access controls and schema inspection\n- **[MySQL](https://github.com/designcomputer/mysql_mcp_server)** (by DesignComputer) - MySQL database integration in Python with configurable access controls and schema inspection\n- **[n8n](https://github.com/leonardsellem/n8n-mcp-server)** - This MCP server provides tools and resources for AI assistants to manage n8n workflows and executions, including listing, creating, updating, and deleting workflows, as well as monitoring their execution status.\n- **[NASA](https://github.com/ProgramComputer/NASA-MCP-server)** (by ProgramComputer) - Access to a unified gateway of NASA's data sources including but not limited to APOD, NEO, EPIC, GIBS.\n- **[National Parks](https://github.com/KyrieTangSheng/mcp-server-nationalparks)** - The server provides latest information of park details, alerts, visitor centers, campgrounds, hiking trails, and events for U.S. National Parks.\n- **[NAVER](https://github.com/pfldy2850/py-mcp-naver)** (by pfldy2850) - This MCP server provides tools to interact with various Naver services, such as searching blogs, news, books, and more.\n- **[NS Travel Information](https://github.com/r-huijts/ns-mcp-server)** - Access Dutch Railways (NS) real-time train travel information and disruptions through the official NS API.\n- **[Neo4j](https://github.com/da-okazaki/mcp-neo4j-server)** - A community built server that interacts with Neo4j Graph Database.\n- **[Neovim](https://github.com/bigcodegen/mcp-neovim-server)** - An MCP Server for your Neovim session.\n- **[Notion](https://github.com/suekou/mcp-notion-server)** (by suekou) - Interact with Notion API.\n- **[Notion](https://github.com/v-3/notion-server)** (by v-3) - Notion MCP integration. Search, Read, Update, and Create pages through Claude chat.\n- **[ntfy-mcp](https://github.com/teddyzxcv/ntfy-mcp)** (by teddyzxcv) - The MCP server that keeps you informed by sending the notification on phone using ntfy\n- **[oatpp-mcp](https://github.com/oatpp/oatpp-mcp)** - C++ MCP integration for Oat++. Use [Oat++](https://oatpp.io) to build MCP servers.\n- **[Obsidian Markdown Notes](https://github.com/calclavia/mcp-obsidian)** - Read and search through your Obsidian vault or any directory containing Markdown notes\n- **[obsidian-mcp](https://github.com/StevenStavrakis/obsidian-mcp)** - (by Steven Stavrakis) An MCP server for Obsidian.md with tools for searching, reading, writing, and organizing notes.\n- **[OceanBase](https://github.com/yuanoOo/oceanbase_mcp_server)** - (by yuanoOo) A Model Context Protocol (MCP) server that enables secure interaction with OceanBase databases.\n- **[Okta](https://github.com/kapilduraphe/okta-mcp-server)** - Interact with Okta API.\n- **[OneNote](https://github.com/rajvirtual/MCP-Servers/tree/master/onenote)** - (by Rajesh Vijay) An MCP server that connects to Microsoft OneNote using the Microsoft Graph API. Reading notebooks, sections, and pages from OneNote,Creating new notebooks, sections, and pages in OneNote.\n- **[OpenAI WebSearch MCP](https://github.com/ConechoAI/openai-websearch-mcp)** - This is a Python-based MCP server that provides OpenAI `web_search` build-in tool.\n- **[OpenAPI](https://github.com/snaggle-ai/openapi-mcp-server)** - Interact with [OpenAPI](https://www.openapis.org/) APIs.\n- **[OpenAPI AnyApi](https://github.com/baryhuang/mcp-server-any-openapi)** - Interact with large [OpenAPI](https://www.openapis.org/) docs using built-in semantic search for endpoints. Allows for customizing the MCP server prefix.\n- **[OpenAPI Schema](https://github.com/hannesj/mcp-openapi-schema)** - Allow LLMs to explore large [OpenAPI](https://www.openapis.org/) schemas without bloating the context.\n- **[OpenCTI](https://github.com/Spathodea-Network/opencti-mcp)** - Interact with OpenCTI platform to retrieve threat intelligence data including reports, indicators, malware and threat actors.\n- **[OpenDota](https://github.com/asusevski/opendota-mcp-server)** - Interact with OpenDota API to retrieve Dota 2 match data, player statistics, and more.\n- **[OpenRPC](https://github.com/shanejonas/openrpc-mpc-server)** - Interact with and discover JSON-RPC APIs via [OpenRPC](https://open-rpc.org).\n- **[Open Strategy Partners Marketing Tools](https://github.com/open-strategy-partners/osp_marketing_tools)** - Content editing codes, value map, and positioning tools for product marketing.\n- **[Pandoc](https://github.com/vivekVells/mcp-pandoc)** - MCP server for seamless document format conversion using Pandoc, supporting Markdown, HTML, PDF, DOCX (.docx), csv and more.\n- **[PIF](https://github.com/hungryrobot1/MCP-PIF)** - A Personal Intelligence Framework (PIF), providing tools for file operations, structured reasoning, and journal-based documentation to support continuity and evolving human-AI collaboration across sessions.\n- **[Pinecone](https://github.com/sirmews/mcp-pinecone)** - MCP server for searching and uploading records to Pinecone. Allows for simple RAG features, leveraging Pinecone's Inference API.\n- **[Placid.app](https://github.com/felores/placid-mcp-server)** - Generate image and video creatives using Placid.app templates\n- **[Playwright](https://github.com/executeautomation/mcp-playwright)** - This MCP Server will help you run browser automation and webscraping using Playwright\n- **[Postman](https://github.com/shannonlal/mcp-postman)** - MCP server for running Postman Collections locally via Newman. Allows for simple execution of Postman Server and returns the results of whether the collection passed all the tests.\n- **[Productboard](https://github.com/kenjihikmatullah/productboard-mcp)** - Integrate the Productboard API into agentic workflows via MCP.\n- **[Prometheus](https://github.com/pab1it0/prometheus-mcp-server)** - Query and analyze Prometheus - open-source monitoring system.\n- **[Pulumi](https://github.com/dogukanakkaya/pulumi-mcp-server)** - MCP Server to Interact with Pulumi API, creates and lists Stacks\n- **[Pushover](https://github.com/ashiknesin/pushover-mcp)** - Send instant notifications to your devices using [Pushover.net](https://pushover.net/)\n- **[QGIS](https://github.com/jjsantos01/qgis_mcp)** - connects QGIS to Claude AI through the MCP. This integration enables prompt-assisted project creation, layer loading, code execution, and more.\n- **[QuickChart](https://github.com/GongRzhe/Quickchart-MCP-Server)** - A Model Context Protocol server for generating charts using QuickChart.io\n- **[Qwen_Max](https://github.com/66julienmartin/MCP-server-Qwen_Max)** - A Model Context Protocol (MCP) server implementation for the Qwen models.\n- **[RabbitMQ](https://github.com/kenliao94/mcp-server-rabbitmq)** - The MCP server that interacts with RabbitMQ to publish and consume messages.\n- **[RAG Web Browser](https://github.com/apify/mcp-server-rag-web-browser)** An MCP server for Apify's open-source RAG Web Browser [Actor](https://apify.com/apify/rag-web-browser) to perform web searches, scrape URLs, and return content in Markdown.\n- **[Reaper](https://github.com/dschuler36/reaper-mcp-server)** - Interact with your [Reaper](https://www.reaper.fm/) (Digital Audio Workstation) projects.\n- **[Redis](https://github.com/GongRzhe/REDIS-MCP-Server)** - Redis database operations and caching microservice server with support for key-value operations, expiration management, and pattern-based key listing.\n- **[Redis](https://github.com/prajwalnayak7/mcp-server-redis)** MCP server to interact with Redis Server, AWS Memory DB, etc for caching or other use-cases where in-memory and key-value based storage is appropriate\n- **[Rememberizer AI](https://github.com/skydeckai/mcp-server-rememberizer)** - An MCP server designed for interacting with the Rememberizer data source, facilitating enhanced knowledge retrieval.\n- **[Replicate](https://github.com/deepfates/mcp-replicate)** - Search, run and manage machine learning models on Replicate through a simple tool-based interface. Browse models, create predictions, track their status, and handle generated images.\n- **[Rquest](https://github.com/xxxbrian/mcp-rquest)** - An MCP server providing realistic browser-like HTTP request capabilities with accurate TLS/JA3/JA4 fingerprints for bypassing anti-bot measures.\n- **[Rijksmuseum](https://github.com/r-huijts/rijksmuseum-mcp)** - Interface with the Rijksmuseum API to search artworks, retrieve artwork details, access image tiles, and explore user collections.\n- **[Salesforce MCP](https://github.com/smn2gnt/MCP-Salesforce)** - Interact with Salesforce Data and Metadata\n- **[Scholarly](https://github.com/adityak74/mcp-scholarly)** - A MCP server to search for scholarly and academic articles.\n- **[scrapling-fetch](https://github.com/cyberchitta/scrapling-fetch-mcp)** - Access text content from bot-protected websites. Fetches HTML/markdown from sites with anti-automation measures using Scrapling.\n- **[SearXNG](https://github.com/ihor-sokoliuk/mcp-searxng)** - A Model Context Protocol Server for [SearXNG](https://docs.searxng.org)\n- **[ServiceNow](https://github.com/osomai/servicenow-mcp)** - A MCP server to interact with a ServiceNow instance\n- **[Siri Shortcuts](https://github.com/dvcrn/mcp-server-siri-shortcuts)** - MCP to interact with Siri Shortcuts on macOS. Exposes all Shortcuts as MCP tools.\n- **[Snowflake](https://github.com/isaacwasserman/mcp-snowflake-server)** - This MCP server enables LLMs to interact with Snowflake databases, allowing for secure and controlled data operations.\n- **[Solana Agent Kit](https://github.com/sendaifun/solana-agent-kit/tree/main/examples/agent-kit-mcp-server)** - This MCP server enables LLMs to interact with the Solana blockchain with help of Solana Agent Kit by SendAI, allowing for 40+ protcool actions and growing\n- **[Spotify](https://github.com/varunneal/spotify-mcp)** - This MCP allows an LLM to play and use Spotify.\n- **[Starwind UI](https://github.com/Boston343/starwind-ui-mcp/)** - This MCP provides relevant commands, documentation, and other information to allow LLMs to take full advantage of Starwind UI's open source Astro components.\n- **[Stripe](https://github.com/atharvagupta2003/mcp-stripe)** - This MCP allows integration with Stripe for handling payments, customers, and refunds.\n- **[TMDB](https://github.com/Laksh-star/mcp-server-tmdb)** - This MCP server integrates with The Movie Database (TMDB) API to provide movie information, search capabilities, and recommendations.\n- **[Tavily search](https://github.com/RamXX/mcp-tavily)** - An MCP server for Tavily's search & news API, with explicit site inclusions/exclusions\n- **[Telegram](https://github.com/chigwell/telegram-mcp)** - An MCP server that provides paginated chat reading, message retrieval, and message sending capabilities for Telegram through Telethon integration.\n- **[Terminal-Control](https://github.com/GongRzhe/terminal-controller-mcp)** - A MCP server that enables secure terminal command execution, directory navigation, and file system operations through a standardized interface.\n- **[Ticketmaster](https://github.com/delorenj/mcp-server-ticketmaster)** - Search for events, venues, and attractions through the Ticketmaster Discovery API\n- **[Todoist](https://github.com/abhiz123/todoist-mcp-server)** - Interact with Todoist to manage your tasks.\n- **[Typesense](https://github.com/suhail-ak-s/mcp-typesense-server)** - A Model Context Protocol (MCP) server implementation that provides AI models with access to Typesense search capabilities. This server enables LLMs to discover, search, and analyze data stored in Typesense collections.\n- **[Travel Planner](https://github.com/GongRzhe/TRAVEL-PLANNER-MCP-Server)** - Travel planning and itinerary management server integrating with Google Maps API for location search, place details, and route calculations.\n- **[Unity Catalog](https://github.com/ognis1205/mcp-server-unitycatalog)** - An MCP server that enables LLMs to interact with Unity Catalog AI, supporting CRUD operations on Unity Catalog Functions and executing them as MCP tools.\n- **[Unity3d Game Engine](https://github.com/CoderGamester/mcp-unity)** - An MCP server that enables LLMs to interact with Unity3d Game Engine, supporting access to a variety of the Unit's Editor engine tools (e.g. Console Logs, Test Runner logs, Editor functions, hierarchy state, etc) and executing them as MCP tools or gather them as resources.\n- **[Unity Integration (Advanced)](https://github.com/quazaai/UnityMCPIntegration)** - Advanced Unity3d Game Engine MCP which supports ,Execution of Any Editor Related Code Directly Inside of Unity, Fetch Logs, Get Editor State and Allow File Access of the Project making it much more useful in Script Editing or asset creation.\n- **[Vega-Lite](https://github.com/isaacwasserman/mcp-vegalite-server)** - Generate visualizations from fetched data using the VegaLite format and renderer.\n- **[Video Editor](https://github.com/burningion/video-editing-mcp)** - A Model Context Protocol Server to add, edit, and search videos with [Video Jungle](https://www.video-jungle.com/).\n- **[Virtual location (Google Street View,etc.)](https://github.com/mfukushim/map-traveler-mcp)** - Integrates Google Map, Google Street View, PixAI, Stability.ai, ComfyUI API and Bluesky to provide a virtual location simulation in LLM (written in Effect.ts)\n- **[VolcEngine TOS](https://github.com/dinghuazhou/sample-mcp-server-tos)** - A sample MCP server for VolcEngine TOS that flexibly get objects from TOS.\n- **[Wanaku MCP Router](https://github.com/wanaku-ai/wanaku/)** - The Wanaku MCP Router is a SSE-based MCP server that provides an extensible routing engine that allows integrating your enterprise systems with AI agents.\n- **[Webflow](https://github.com/kapilduraphe/webflow-mcp-server)** - Interfact with the Webflow APIs\n- **[whale-tracker-mcp](https://github.com/kukapay/whale-tracker-mcp)**  -  A mcp server for tracking cryptocurrency whale transactions. \n- **[Whois MCP](https://github.com/bharathvaj-ganesan/whois-mcp)** - MCP server that performs whois lookup against domain, IP, ASN and TLD. \n- **[WildFly MCP](https://github.com/wildfly-extras/wildfly-mcp)** - WildFly MCP server that enables LLM to interact with running WildFly servers (retrieve metrics, logs, invoke operations, ...).\n- **[Windows CLI](https://github.com/SimonB97/win-cli-mcp-server)** - MCP server for secure command-line interactions on Windows systems, enabling controlled access to PowerShell, CMD, and Git Bash shells.\n- **[World Bank data API](https://github.com/anshumax/world_bank_mcp_server)** - A server that fetches data indicators available with the World Bank as part of their data API\n- **[X (Twitter)](https://github.com/EnesCinr/twitter-mcp)** (by EnesCinr) - Interact with twitter API. Post tweets and search for tweets by query.\n- **[X (Twitter)](https://github.com/vidhupv/x-mcp)** (by vidhupv) - Create, manage and publish X/Twitter posts directly through Claude chat.\n- **[xcodebuild](https://github.com/ShenghaiWang/xcodebuild)**  - 🍎 Build iOS Xcode workspace/project and feed back errors to llm.\n- **[Xero-mcp-server](https://github.com/john-zhang-dev/xero-mcp)** - Enabling clients to interact with Xero system for streamlined accounting, invoicing, and business operations.\n- **[XiYan](https://github.com/XGenerationLab/xiyan_mcp_server)** - 🗄️ An MCP server that supports fetching data from a database using natural language queries, powered by XiyanSQL as the text-to-SQL LLM.\n- **[XMind](https://github.com/apeyroux/mcp-xmind)** - Read and search through your XMind directory containing XMind files.\n- **[YouTube](https://github.com/ZubeidHendricks/youtube-mcp-server)** - Comprehensive YouTube API integration for video management, Shorts creation, and analytics.\n\n## 📚 Frameworks\n\nThese are high-level frameworks that make it easier to build MCP servers or clients.\n\n### For servers\n\n* **[EasyMCP](https://github.com/zcaceres/easy-mcp/)** (TypeScript)\n- **[FastAPI to MCP auto generator](https://github.com/tadata-org/fastapi_mcp)** – A zero-configuration tool for automatically exposing FastAPI endpoints as MCP tools by **[Tadata](https://tadata.com/)**\n* **[FastMCP](https://github.com/punkpeye/fastmcp)** (TypeScript)\n* **[Foxy Contexts](https://github.com/strowk/foxy-contexts)** – A library to build MCP servers in Golang by **[strowk](https://github.com/strowk)**\n* **[MCP-Framework](https://mcp-framework.com)** Build MCP servers with elegance and speed in Typescript. Comes with a CLI to create your project with `mcp create app`. Get started with your first server in under 5 minutes by **[Alex Andru](https://github.com/QuantGeekDev)**\n* **[Quarkus MCP Server SDK](https://github.com/quarkiverse/quarkus-mcp-server)** (Java)\n* **[Template MCP Server](https://github.com/mcpdotdirect/template-mcp-server)** - A CLI tool to create a new Model Context Protocol server project with TypeScript support, dual transport options, and an extensible structure\n\n### For clients\n\n* **[codemirror-mcp](https://github.com/marimo-team/codemirror-mcp)** - CodeMirror extension that implements the Model Context Protocol (MCP) for resource mentions and prompt commands\n\n## 📚 Resources\n\nAdditional resources on MCP.\n\n- **[AiMCP](https://www.aimcp.info)** - A collection of MCP clients&servers to find the right mcp tools by **[Hekmon](https://github.com/hekmon8)**\n- **[Awesome Crypto MCP Servers by badkk](https://github.com/badkk/awesome-crypto-mcp-servers)** - A curated list of MCP servers by **[Luke Fan](https://github.com/badkk)**\n- **[Awesome MCP Servers by appcypher](https://github.com/appcypher/awesome-mcp-servers)** - A curated list of MCP servers by **[Stephen Akinyemi](https://github.com/appcypher)**\n- **[Awesome MCP Servers by punkpeye](https://github.com/punkpeye/awesome-mcp-servers)** (**[website](https://glama.ai/mcp/servers)**) - A curated list of MCP servers by **[Frank Fiegel](https://github.com/punkpeye)**\n- **[Awesome MCP Servers by wong2](https://github.com/wong2/awesome-mcp-servers)** (**[website](https://mcpservers.org)**) - A curated list of MCP servers by **[wong2](https://github.com/wong2)**\n- **[Discord Server](https://glama.ai/mcp/discord)** – A community discord server dedicated to MCP by **[Frank Fiegel](https://github.com/punkpeye)**\n- **[Discord Server (ModelContextProtocol)](https://discord.gg/jHEGxQu2a5)** – Connect with developers, share insights, and collaborate on projects in an active Discord community dedicated to the Model Context Protocol by **[Alex Andru](https://github.com/QuantGeekDev)**\n\n- **[MCP Badges](https://github.com/mcpx-dev/mcp-badges)** – Quickly highlight your MCP project with clear, eye-catching badges, by **[Ironben](https://github.com/nanbingxyz)**\n- **[MCP Servers Hub](https://github.com/apappascs/mcp-servers-hub)** (**[website](https://mcp-servers-hub-website.pages.dev/)**) - A curated list of MCP servers by **[apappascs](https://github.com/apappascs)**\n- **[MCP X Community](https://x.com/i/communities/1861891349609603310)** – A X community for MCP by **[Xiaoyi](https://x.com/chxy)**\n- **[mcp-cli](https://github.com/wong2/mcp-cli)** - A CLI inspector for the Model Context Protocol by **[wong2](https://github.com/wong2)**\n- **[mcp-get](https://mcp-get.com)** - Command line tool for installing and managing MCP servers by **[Michael Latman](https://github.com/michaellatman)**\n- **[mcp-guardian](https://github.com/eqtylab/mcp-guardian)** - GUI application + tools for proxying / managing control of MCP servers by **[EQTY Lab](https://eqtylab.io)**\n- **[mcp-manager](https://github.com/zueai/mcp-manager)** - Simple Web UI to install and manage MCP servers for Claude Desktop by **[Zue](https://github.com/zueai)**\n- **[MCPHub](https://github.com/Jeamee/MCPHub-Desktop)** – An Open Source MacOS & Windows GUI Desktop app for discovering, installing and managing MCP servers by **[Jeamee](https://github.com/jeamee)**\n- **[mcp.run](https://mcp.run)** - A hosted registry and control plane to install & run secure + portable MCP Servers.\n- **[mcp-dockmaster](https://mcp-dockmaster.com)** - An Open-Sourced UI to install and manage MCP servers for Windows, Linux and MacOS.\n- <img height=\"12\" width=\"12\" src=\"https://mkinf.io/favicon-lilac.png\" alt=\"mkinf Logo\" /> **[mkinf](https://mkinf.io)** - An Open Source registry of hosted MCP Servers to accelerate AI agent workflows.\n- **[Open-Sourced MCP Servers Directory](https://github.com/chatmcp/mcp-directory)** - A curated list of MCP servers by **[mcpso](https://mcp.so)**\n- <img height=\"12\" width=\"12\" src=\"https://opentools.com/favicon.ico\" alt=\"OpenTools Logo\" /> **[OpenTools](https://opentools.com)** - An open registry for finding, installing, and building with MCP servers by **[opentoolsteam](https://github.com/opentoolsteam)**\n- **[PulseMCP](https://www.pulsemcp.com)** ([API](https://www.pulsemcp.com/api)) - Community hub & weekly newsletter for discovering MCP servers, clients, articles, and news by **[Tadas Antanavicius](https://github.com/tadasant)**, **[Mike Coughlin](https://github.com/macoughl)**, and **[Ravina Patel](https://github.com/ravinahp)**\n- **[r/mcp](https://www.reddit.com/r/mcp)** – A Reddit community dedicated to MCP by **[Frank Fiegel](https://github.com/punkpeye)**\n- **[r/modelcontextprotocol](https://www.reddit.com/r/modelcontextprotocol)** – A Model Context Protocol community Reddit page - discuss ideas, get answers to your questions, network with like-minded people, and showcase your projects! by **[Alex Andru](https://github.com/QuantGeekDev)**\n\n\n- **[Smithery](https://smithery.ai/)** - A registry of MCP servers to find the right tools for your LLM agents by **[Henry Mao](https://github.com/calclavia)**\n- **[Toolbase](https://gettoolbase.ai)** - Desktop application that manages tools and MCP servers with just a few clicks - no coding required by **[gching](https://github.com/gching)**\n\n## 🚀 Getting Started\n\n### Using MCP Servers in this Repository\nTypescript-based servers in this repository can be used directly with `npx`.\n\nFor example, this will start the [Memory](src/memory) server:\n```sh\nnpx -y @modelcontextprotocol/server-memory\n```\n\nPython-based servers in this repository can be used directly with [`uvx`](https://docs.astral.sh/uv/concepts/tools/) or [`pip`](https://pypi.org/project/pip/). `uvx` is recommended for ease of use and setup.\n\nFor example, this will start the [Git](src/git) server:\n```sh\n# With uvx\nuvx mcp-server-git\n\n# With pip\npip install mcp-server-git\npython -m mcp_server_git\n```\n\nFollow [these](https://docs.astral.sh/uv/getting-started/installation/) instructions to install `uv` / `uvx` and [these](https://pip.pypa.io/en/stable/installation/) to install `pip`.\n\n### Using an MCP Client\nHowever, running a server on its own isn't very useful, and should instead be configured into an MCP client. For example, here's the Claude Desktop configuration to use the above server:\n\n```json\n{\n  \"mcpServers\": {\n    \"memory\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@modelcontextprotocol/server-memory\"]\n    }\n  }\n}\n```\n\nAdditional examples of using the Claude Desktop as an MCP client might look like:\n\n```json\n{\n  \"mcpServers\": {\n    \"filesystem\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@modelcontextprotocol/server-filesystem\", \"/path/to/allowed/files\"]\n    },\n    \"git\": {\n      \"command\": \"uvx\",\n      \"args\": [\"mcp-server-git\", \"--repository\", \"path/to/git/repo\"]\n    },\n    \"github\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@modelcontextprotocol/server-github\"],\n      \"env\": {\n        \"GITHUB_PERSONAL_ACCESS_TOKEN\": \"<YOUR_TOKEN>\"\n      }\n    },\n    \"postgres\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@modelcontextprotocol/server-postgres\", \"postgresql://localhost/mydb\"]\n    }\n  }\n}\n```\n\n## 🛠️ Creating Your Own Server\n\nInterested in creating your own MCP server? Visit the official documentation at [modelcontextprotocol.io](https://modelcontextprotocol.io/introduction) for comprehensive guides, best practices, and technical details on implementing MCP servers.\n\n## 🤝 Contributing\n\nSee [CONTRIBUTING.md](CONTRIBUTING.md) for information about contributing to this repository.\n\n## 🔒 Security\n\nSee [SECURITY.md](SECURITY.md) for reporting security vulnerabilities.\n\n## 📜 License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n\n## 💬 Community\n\n- [GitHub Discussions](https://github.com/orgs/modelcontextprotocol/discussions)\n\n## ⭐ Support\n\nIf you find MCP servers useful, please consider starring the repository and contributing new servers or improvements!\n\n---\n\nManaged by Anthropic, but built together with the community. The Model Context Protocol is open source and we encourage everyone to contribute their own servers and improvements!",
      "npm_url": "https://www.npmjs.com/package/mcp-servers",
      "npm_downloads": 854,
      "keywords": [
        "p1scess",
        "parsing",
        "markdown",
        "search p1scess",
        "p1scess mcp",
        "parsing llms"
      ],
      "category": "web-search"
    },
    "pab1it0--tripadvisor-mcp": {
      "owner": "pab1it0",
      "name": "tripadvisor-mcp",
      "url": "https://github.com/pab1it0/tripadvisor-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/pab1it0.webp",
      "description": "Access Tripadvisor location data, reviews, and photos through standardized MCP interfaces. Search for travel destinations and experiences, and retrieve detailed information based on user queries.",
      "stars": 48,
      "forks": 10,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-26T17:50:55Z",
      "readme_content": "# Tripadvisor MCP Server\n\nA [Model Context Protocol][mcp] (MCP) server for Tripadvisor Content API.\n\nThis provides access to Tripadvisor location data, reviews, and photos through standardized MCP interfaces, allowing AI assistants to search for travel destinations and experiences.\n\n[mcp]: https://modelcontextprotocol.io\n\n## Features\n\n- [x] Search for locations (hotels, restaurants, attractions) on Tripadvisor\n- [x] Get detailed information about specific locations\n- [x] Retrieve reviews and photos for locations\n- [x] Search for nearby locations based on coordinates\n- [x] API Key authentication\n- [x] Docker containerization support\n\n- [x] Provide interactive tools for AI assistants\n\nThe list of tools is configurable, so you can choose which tools you want to make available to the MCP client.\n\n## Usage\n\n1. Get your Tripadvisor Content API key from the [Tripadvisor Developer Portal](https://developer.tripadvisor.com/).\n\n2. Configure the environment variables for your Tripadvisor Content API, either through a `.env` file or system environment variables:\n\n```env\n# Required: Tripadvisor Content API configuration\nTRIPADVISOR_API_KEY=your_api_key_here\n```\n\n3. Add the server configuration to your client configuration file. For example, for Claude Desktop:\n\n```json\n{\n  \"mcpServers\": {\n    \"tripadvisor\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"--directory\",\n        \"<full path to tripadvisor-mcp directory>\",\n        \"run\",\n        \"src/tripadvisor_mcp/main.py\"\n      ],\n      \"env\": {\n        \"TRIPADVISOR_API_KEY\": \"your_api_key_here\"\n      }\n    }\n  }\n}\n```\n\n> Note: if you see `Error: spawn uv ENOENT` in Claude Desktop, you may need to specify the full path to `uv` or set the environment variable `NO_UV=1` in the configuration.\n\n## Docker Usage\n\nThis project includes Docker support for easy deployment and isolation.\n\n### Building the Docker Image\n\nBuild the Docker image using:\n\n```bash\ndocker build -t tripadvisor-mcp-server .\n```\n\n### Running with Docker\n\nYou can run the server using Docker in several ways:\n\n#### Using docker run directly:\n\n```bash\ndocker run -it --rm \\\n  -e TRIPADVISOR_API_KEY=your_api_key_here \\\n  tripadvisor-mcp-server\n```\n\n#### Using docker-compose:\n\nCreate a `.env` file with your Tripadvisor API key and then run:\n\n```bash\ndocker-compose up\n```\n\n### Running with Docker in Claude Desktop\n\nTo use the containerized server with Claude Desktop, update the configuration to use Docker with the environment variables:\n\n```json\n{\n  \"mcpServers\": {\n    \"tripadvisor\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"--rm\",\n        \"-i\",\n        \"-e\", \"TRIPADVISOR_API_KEY\",\n        \"tripadvisor-mcp-server\"\n      ],\n      \"env\": {\n        \"TRIPADVISOR_API_KEY\": \"your_api_key_here\"\n      }\n    }\n  }\n}\n```\n\nThis configuration passes the environment variables from Claude Desktop to the Docker container by using the `-e` flag with just the variable name, and providing the actual values in the `env` object.\n\n## Development\n\nContributions are welcome! Please open an issue or submit a pull request if you have any suggestions or improvements.\n\nThis project uses [`uv`](https://github.com/astral-sh/uv) to manage dependencies. Install `uv` following the instructions for your platform:\n\n```bash\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n```\n\nYou can then create a virtual environment and install the dependencies with:\n\n```bash\nuv venv\nsource .venv/bin/activate  # On Unix/macOS\n.venv\\Scripts\\activate     # On Windows\nuv pip install -e .\n```\n\n## Project Structure\n\nThe project has been organized with a `src` directory structure:\n\n```\ntripadvisor-mcp/\n├── src/\n│   └── tripadvisor_mcp/\n│       ├── __init__.py      # Package initialization\n│       ├── server.py        # MCP server implementation\n│       ├── main.py          # Main application logic\n├── Dockerfile               # Docker configuration\n├── docker-compose.yml       # Docker Compose configuration\n├── .dockerignore            # Docker ignore file\n├── pyproject.toml           # Project configuration\n└── README.md                # This file\n```\n\n### Testing\n\nThe project includes a test suite that ensures functionality and helps prevent regressions.\n\nRun the tests with pytest:\n\n```bash\n# Install development dependencies\nuv pip install -e \".[dev]\"\n\n# Run the tests\npytest\n\n# Run with coverage report\npytest --cov=src --cov-report=term-missing\n```\n\n### Tools\n\n| Tool | Category | Description |\n| --- | --- | --- |\n| `search_locations` | Search | Search for locations by query text, category, and other filters |\n| `search_nearby_locations` | Search | Find locations near specific coordinates |\n| `get_location_details` | Retrieval | Get detailed information about a location |\n| `get_location_reviews` | Retrieval | Retrieve reviews for a location |\n| `get_location_photos` | Retrieval | Get photos for a location |\n\n## License\n\nMIT\n\n---\n\n[mcp]: https://modelcontextprotocol.io\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "tripadvisor",
        "pab1it0",
        "mcp",
        "tripadvisor mcp",
        "pab1it0 tripadvisor",
        "access tripadvisor"
      ],
      "category": "web-search"
    },
    "pbteja1998--sourcesyncai-mcp": {
      "owner": "pbteja1998",
      "name": "sourcesyncai-mcp",
      "url": "https://github.com/pbteja1998/sourcesyncai-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/pbteja1998.webp",
      "description": "Integrates with a knowledge management platform to manage and organize documents, ingest content from various sources, and perform semantic and hybrid searches. Facilitates connections to external services for enhanced data retrieval and document management.",
      "stars": 3,
      "forks": 1,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-06-11T16:57:56Z",
      "readme_content": "# SourceSync.ai MCP Server\n\n[![smithery badge](https://smithery.ai/badge/@pbteja1998/sourcesyncai-mcp)](https://smithery.ai/server/@pbteja1998/sourcesyncai-mcp)\n\nA Model Context Protocol (MCP) server implementation for the [SourceSync.ai](https://sourcesync.ai) API. This server allows AI models to interact with SourceSync.ai's knowledge management platform through a standardized interface.\n\n## Features\n\n- Manage namespaces for organizing knowledge\n- Ingest content from various sources (text, URLs, websites, external services)\n- Retrieve, update, and manage documents stored in your knowledge base\n- Perform semantic and hybrid searches against your knowledge base\n- Access document content directly from parsed text URLs\n- Manage connections to external services\n- Default configuration support for seamless AI integration\n\n## Installation\n\n### Running with npx\n\n```bash\n# Install and run with your API key and tenant ID\nenv SOURCESYNC_API_KEY=your_api_key npx -y sourcesyncai-mcp\n```\n\n### Installing via Smithery\n\nTo install sourcesyncai-mcp for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@pbteja1998/sourcesyncai-mcp):\n\n```bash\nnpx -y @smithery/cli install @pbteja1998/sourcesyncai-mcp --client claude\n```\n\n### Manual Installation\n\n```bash\n# Clone the repository\ngit clone https://github.com/yourusername/sourcesyncai-mcp.git\ncd sourcesyncai-mcp\n\n# Install dependencies\nnpm install\n\n# Build the project\nnpm run build\n\n# Run the server\nnode dist/index.js\n```\n\n### Running on Cursor\n\nTo configure SourceSync.ai MCP in Cursor:\n\n1. Open Cursor Settings\n1. Go to `Features > MCP Servers`\n1. Click `+ Add New MCP Server`\n1. Enter the following:\n   - Name: `sourcesyncai-mcp` (or your preferred name)\n   - Type: `command`\n   - Command: `env SOURCESYNCAI_API_KEY=your-api-key npx -y sourcesyncai-mcp`\n\nAfter adding, you can use SourceSync.ai tools with Cursor's AI features by describing your knowledge management needs.\n\n### Running on Windsurf\n\nAdd this to your `./codeium/windsurf/model_config.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"sourcesyncai-mcp\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"soucesyncai-mcp\"],\n      \"env\": {\n        \"SOURCESYNC_API_KEY\": \"your_api_key\",\n        \"SOURCESYNC_NAMESPACE_ID\": \"your_namespace_id\",\n        \"SOURCESYNC_TENANT_ID\": \"your_tenant_id\"\n      }\n    }\n  }\n}\n```\n\n### Running on Claude Desktop\n\nTo use this MCP server with Claude Desktop:\n\n1. Locate the Claude Desktop configuration file:\n\n   - **macOS**: `~/Library/Application Support/Claude/claude_desktop_config.json`\n   - **Windows**: `%APPDATA%\\Claude\\claude_desktop_config.json`\n   - **Linux**: `~/.config/Claude/claude_desktop_config.json`\n\n2. Edit the configuration file to add the SourceSync.ai MCP server:\n\n```json\n{\n  \"mcpServers\": {\n    \"sourcesyncai-mcp\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"sourcesyncai-mcp\"],\n      \"env\": {\n        \"SOURCESYNC_API_KEY\": \"your_api_key\",\n        \"SOURCESYNC_NAMESPACE_ID\": \"your_namespace_id\",\n        \"SOURCESYNC_TENANT_ID\": \"your_tenant_id\"\n      }\n    }\n  }\n}\n```\n\n3. Save the configuration file and restart Claude Desktop\n\n## Configuration\n\n### Environment Variables\n\n#### Required\n\n- `SOURCESYNC_API_KEY`: Your SourceSync.ai API key (required)\n\n#### Optional\n\n- `SOURCESYNC_NAMESPACE_ID`: Default namespace ID to use for operations\n- `SOURCESYNC_TENANT_ID`: Your tenant ID (optional)\n\n### Configuration Examples\n\nBasic configuration with default values:\n\n```bash\nexport SOURCESYNC_API_KEY=your_api_key\nexport SOURCESYNC_TENANT_ID=your_tenant_id\nexport SOURCESYNC_NAMESPACE_ID=your_namespace_id\n```\n\n## Available Tools\n\n### Authentication\n\n- `validate_api_key`: Validate a SourceSync.ai API key\n\n```json\n{\n  \"name\": \"validate_api_key\",\n  \"arguments\": {}\n}\n```\n\n### Namespaces\n\n- `create_namespace`: Create a new namespace\n- `list_namespaces`: List all namespaces\n- `get_namespace`: Get details of a specific namespace\n- `update_namespace`: Update a namespace\n- `delete_namespace`: Delete a namespace\n\n```json\n{\n  \"name\": \"create_namespace\",\n  \"arguments\": {\n    \"name\": \"my-namespace\",\n    \"fileStorageConfig\": {\n      \"provider\": \"S3_COMPATIBLE\",\n      \"config\": {\n        \"endpoint\": \"s3.amazonaws.com\",\n        \"accessKey\": \"your_access_key\",\n        \"secretKey\": \"your_secret_key\",\n        \"bucket\": \"your_bucket\",\n        \"region\": \"us-east-1\"\n      }\n    },\n    \"vectorStorageConfig\": {\n      \"provider\": \"PINECONE\",\n      \"config\": {\n        \"apiKey\": \"your_pinecone_api_key\",\n        \"environment\": \"your_environment\",\n        \"index\": \"your_index\"\n      }\n    },\n    \"embeddingModelConfig\": {\n      \"provider\": \"OPENAI\",\n      \"config\": {\n        \"apiKey\": \"your_openai_api_key\",\n        \"model\": \"text-embedding-3-small\"\n      }\n    },\n    \"tenantId\": \"tenant_XXX\"\n  }\n}\n```\n\n```json\n{\n  \"name\": \"list_namespaces\",\n  \"arguments\": {\n    \"tenantId\": \"tenant_XXX\"\n  }\n}\n```\n\n```json\n{\n  \"name\": \"get_namespace\",\n  \"arguments\": {\n    \"namespaceId\": \"namespace_XXX\",\n    \"tenantId\": \"tenant_XXX\"\n  }\n}\n```\n\n```json\n{\n  \"name\": \"update_namespace\",\n  \"arguments\": {\n    \"namespaceId\": \"namespace_XXX\",\n    \"tenantId\": \"tenant_XXX\",\n    \"name\": \"updated-namespace-name\"\n  }\n}\n```\n\n```json\n{\n  \"name\": \"delete_namespace\",\n  \"arguments\": {\n    \"namespaceId\": \"namespace_XXX\",\n    \"tenantId\": \"tenant_XXX\"\n  }\n}\n```\n\n### Data Ingestion\n\n- `ingest_text`: Ingest text content\n- `ingest_urls`: Ingest content from URLs\n- `ingest_sitemap`: Ingest content from a sitemap\n- `ingest_website`: Ingest content from a website\n- `ingest_notion`: Ingest content from Notion\n- `ingest_google_drive`: Ingest content from Google Drive\n- `ingest_dropbox`: Ingest content from Dropbox\n- `ingest_onedrive`: Ingest content from OneDrive\n- `ingest_box`: Ingest content from Box\n- `get_ingest_job_run_status`: Get the status of an ingestion job run\n\n```json\n{\n  \"name\": \"ingest_text\",\n  \"arguments\": {\n    \"namespaceId\": \"your_namespace_id\",\n    \"ingestConfig\": {\n      \"source\": \"TEXT\",\n      \"config\": {\n        \"name\": \"example-document\",\n        \"text\": \"This is an example document for ingestion.\",\n        \"metadata\": {\n          \"category\": \"example\",\n          \"author\": \"AI Assistant\"\n        }\n      }\n    },\n    \"tenantId\": \"tenant_XXX\"\n  }\n}\n```\n\n```json\n{\n  \"name\": \"ingest_urls\",\n  \"arguments\": {\n    \"namespaceId\": \"your_namespace_id\",\n    \"ingestConfig\": {\n      \"source\": \"URLS\",\n      \"config\": {\n        \"urls\": [\"https://example.com/page1\", \"https://example.com/page2\"],\n        \"metadata\": {\n          \"source\": \"web\",\n          \"category\": \"documentation\"\n        }\n      }\n    },\n    \"tenantId\": \"tenant_XXX\"\n  }\n}\n```\n\n```json\n{\n  \"name\": \"ingest_sitemap\",\n  \"arguments\": {\n    \"namespaceId\": \"your_namespace_id\",\n    \"ingestConfig\": {\n      \"source\": \"SITEMAP\",\n      \"config\": {\n        \"url\": \"https://example.com/sitemap.xml\",\n        \"metadata\": {\n          \"source\": \"sitemap\",\n          \"website\": \"example.com\"\n        }\n      }\n    },\n    \"tenantId\": \"tenant_XXX\"\n  }\n}\n```\n\n```json\n{\n  \"name\": \"ingest_website\",\n  \"arguments\": {\n    \"namespaceId\": \"your_namespace_id\",\n    \"ingestConfig\": {\n      \"source\": \"WEBSITE\",\n      \"config\": {\n        \"url\": \"https://example.com\",\n        \"maxDepth\": 3,\n        \"maxPages\": 100,\n        \"metadata\": {\n          \"source\": \"website\",\n          \"domain\": \"example.com\"\n        }\n      }\n    },\n    \"tenantId\": \"tenant_XXX\"\n  }\n}\n```\n\n```json\n{\n  \"name\": \"ingest_notion\",\n  \"arguments\": {\n    \"namespaceId\": \"your_namespace_id\",\n    \"ingestConfig\": {\n      \"source\": \"NOTION\",\n      \"config\": {\n        \"connectionId\": \"your_notion_connection_id\",\n        \"metadata\": {\n          \"source\": \"notion\",\n          \"workspace\": \"My Workspace\"\n        }\n      }\n    },\n    \"tenantId\": \"your_tenant_id\"\n  }\n}\n```\n\n```json\n{\n  \"name\": \"ingest_google_drive\",\n  \"arguments\": {\n    \"namespaceId\": \"your_namespace_id\",\n    \"ingestConfig\": {\n      \"source\": \"GOOGLE_DRIVE\",\n      \"config\": {\n        \"connectionId\": \"connection_XXX\",\n        \"metadata\": {\n          \"source\": \"google_drive\",\n          \"owner\": \"user@example.com\"\n        }\n      }\n    },\n    \"tenantId\": \"tenant_XXX\"\n  }\n}\n```\n\n```json\n{\n  \"name\": \"ingest_dropbox\",\n  \"arguments\": {\n    \"namespaceId\": \"your_namespace_id\",\n    \"ingestConfig\": {\n      \"source\": \"DROPBOX\",\n      \"config\": {\n        \"connectionId\": \"connection_XXX\",\n        \"metadata\": {\n          \"source\": \"dropbox\",\n          \"account\": \"user@example.com\"\n        }\n      }\n    },\n    \"tenantId\": \"tenant_XXX\"\n  }\n}\n```\n\n```json\n{\n  \"name\": \"ingest_onedrive\",\n  \"arguments\": {\n    \"namespaceId\": \"your_namespace_id\",\n    \"ingestConfig\": {\n      \"source\": \"ONEDRIVE\",\n      \"config\": {\n        \"connectionId\": \"connection_XXX\",\n        \"metadata\": {\n          \"source\": \"onedrive\",\n          \"account\": \"user@example.com\"\n        }\n      }\n    },\n    \"tenantId\": \"tenant_XXX\"\n  }\n}\n```\n\n```json\n{\n  \"name\": \"ingest_box\",\n  \"arguments\": {\n    \"namespaceId\": \"your_namespace_id\",\n    \"ingestConfig\": {\n      \"source\": \"BOX\",\n      \"config\": {\n        \"connectionId\": \"connection_XXX\",\n        \"metadata\": {\n          \"source\": \"box\",\n          \"owner\": \"user@example.com\"\n        }\n      }\n    },\n    \"tenantId\": \"tenant_XXX\"\n  }\n}\n```\n\n```json\n{\n  \"name\": \"get_ingest_job_run_status\",\n  \"arguments\": {\n    \"namespaceId\": \"your_namespace_id\",\n    \"ingestJobRunId\": \"ingest_job_run_XXX\",\n    \"tenantId\": \"tenant_XXX\"\n  }\n}\n```\n\n### Documents\n\n- `getDocuments`: Retrieve documents with optional filters\n- `updateDocuments`: Update document metadata\n- `deleteDocuments`: Delete documents\n- `resyncDocuments`: Resync documents\n- `fetchUrlContent`: Fetch text content from document URLs\n\n```json\n{\n  \"name\": \"getDocuments\",\n  \"arguments\": {\n    \"namespaceId\": \"namespace_XXX\",\n    \"tenantId\": \"tenant_XXX\",\n    \"filterConfig\": {\n      \"documentTypes\": [\"PDF\"]\n    },\n    \"includeConfig\": {\n      \"parsedTextFileUrl\": true\n    }\n  }\n}\n```\n\n```json\n{\n  \"name\": \"updateDocuments\",\n  \"arguments\": {\n    \"namespaceId\": \"namespace_XXX\",\n    \"tenantId\": \"tenant_XXX\",\n    \"documentIds\": [\"doc_XXX\", \"doc_YYY\"],\n    \"filterConfig\": {\n      \"documentIds\": [\"doc_XXX\", \"doc_YYY\"]\n    },\n    \"data\": {\n      \"metadata\": {\n        \"status\": \"reviewed\",\n        \"category\": \"technical\"\n      }\n    }\n  }\n}\n```\n\n```json\n{\n  \"name\": \"deleteDocuments\",\n  \"arguments\": {\n    \"namespaceId\": \"namespace_XXX\",\n    \"tenantId\": \"tenant_XXX\",\n    \"documentIds\": [\"doc_XXX\", \"doc_YYY\"],\n    \"filterConfig\": {\n      \"documentIds\": [\"doc_XXX\", \"doc_YYY\"]\n    }\n  }\n}\n```\n\n```json\n{\n  \"name\": \"resyncDocuments\",\n  \"arguments\": {\n    \"namespaceId\": \"namespace_XXX\",\n    \"tenantId\": \"tenant_XXX\",\n    \"documentIds\": [\"doc_XXX\", \"doc_YYY\"],\n    \"filterConfig\": {\n      \"documentIds\": [\"doc_XXX\", \"doc_YYY\"]\n    }\n  }\n}\n```\n\n```json\n{\n  \"name\": \"fetchUrlContent\",\n  \"arguments\": {\n    \"url\": \"https://api.sourcesync.ai/v1/documents/doc_XXX/content?format=text\",\n    \"apiKey\": \"your_api_key\",\n    \"tenantId\": \"tenant_XXX\"\n  }\n}\n```\n\n### Search\n\n- `semantic_search`: Perform semantic search\n- `hybrid_search`: Perform hybrid search (semantic + keyword)\n\n```json\n{\n  \"name\": \"semantic_search\",\n  \"arguments\": {\n    \"namespaceId\": \"your_namespace_id\",\n    \"query\": \"example document\",\n    \"topK\": 5,\n    \"tenantId\": \"tenant_XXX\"\n  }\n}\n```\n\n```json\n{\n  \"name\": \"hybrid_search\",\n  \"arguments\": {\n    \"namespaceId\": \"your_namespace_id\",\n    \"query\": \"example document\",\n    \"topK\": 5,\n    \"tenantId\": \"tenant_XXX\",\n    \"hybridConfig\": {\n      \"semanticWeight\": 0.7,\n      \"keywordWeight\": 0.3\n    }\n  }\n}\n```\n\n### Connections\n\n- `create_connection`: Create a new connection to an external service\n- `list_connections`: List all connections\n- `get_connection`: Get details of a specific connection\n- `update_connection`: Update a connection\n- `revoke_connection`: Revoke a connection\n\n```json\n{\n  \"name\": \"create_connection\",\n  \"arguments\": {\n    \"tenantId\": \"tenant_XXX\",\n    \"namespaceId\": \"namespace_XXX\",\n    \"name\": \"My Connection\",\n    \"connector\": \"GOOGLE_DRIVE\",\n    \"clientRedirectUrl\": \"https://your-app.com/callback\"\n  }\n}\n```\n\n```json\n{\n  \"name\": \"list_connections\",\n  \"arguments\": {\n    \"tenantId\": \"tenant_XXX\",\n    \"namespaceId\": \"namespace_XXX\"\n  }\n}\n```\n\n```json\n{\n  \"name\": \"get_connection\",\n  \"arguments\": {\n    \"tenantId\": \"tenant_XXX\",\n    \"namespaceId\": \"namespace_XXX\",\n    \"connectionId\": \"connection_XXX\"\n  }\n}\n```\n\n```json\n{\n  \"name\": \"update_connection\",\n  \"arguments\": {\n    \"tenantId\": \"tenant_XXX\",\n    \"namespaceId\": \"namespace_XXX\",\n    \"connectionId\": \"connection_XXX\",\n    \"name\": \"Updated Connection Name\",\n    \"clientRedirectUrl\": \"https://your-app.com/updated-callback\"\n  }\n}\n```\n\n```json\n{\n  \"name\": \"revoke_connection\",\n  \"arguments\": {\n    \"tenantId\": \"tenant_XXX\",\n    \"namespaceId\": \"namespace_XXX\",\n    \"connectionId\": \"connection_XXX\"\n  }\n}\n```\n\n## Example Prompts\n\nHere are some example prompts you can use with Claude or Cursor after configuring the MCP server:\n\n- \"Search my SourceSync knowledge base for information about machine learning.\"\n- \"Ingest this article into my SourceSync knowledge base: [URL]\"\n- \"Create a new namespace in SourceSync for my project documentation.\"\n- \"List all the documents in my SourceSync namespace.\"\n- \"Get the text content of document [document_id] from my SourceSync namespace.\"\n\n## Troubleshooting\n\n### Connection Issues\n\nIf you encounter issues connecting the SourceSync.ai MCP server:\n\n1. **Verify Paths**: Ensure all paths in your configuration are absolute paths, not relative.\n2. **Check Permissions**: Ensure the server file has execution permissions (`chmod +x dist/index.js`).\n3. **Enable Developer Mode**: In Claude Desktop, enable Developer Mode and check the MCP Log File.\n4. **Test the Server**: Run the server directly from the command line:\n\n   ```bash\n   node /path/to/sourcesyncai-mcp/dist/index.js\n   ```\n\n5. **Restart AI Client**: After making changes, completely restart Claude Desktop or Cursor.\n6. **Check Environment Variables**: Ensure all required environment variables are correctly set.\n\n### Debug Logging\n\nFor detailed logging, add the DEBUG environment variable:\n\n```\n\n```\n\n## Development\n\n### Project Structure\n\n- `src/index.ts`: Main entry point and server setup\n- `src/schemas.ts`: Schema definitions for all tools\n- `src/sourcesync.ts`: Client for interacting with SourceSync.ai API\n- `src/sourcesync.types.ts`: TypeScript type definitions\n\n### Building and Testing\n\n```bash\n# Build the project\nnpm run build\n\n# Run tests\nnpm test\n```\n\n## License\n\nMIT\n\n## Links\n\n- [SourceSync.ai Documentation](https://sourcesync.ai)\n- [SourceSync.ai API Reference](https://sourcesync.ai/api-reference/authentication)\n- [Model Context Protocol](https://modelcontextprotocol.io/introduction)\n\nDocument content retrieval workflow:\n\n1. First, use `getDocuments` with `includeConfig.parsedTextFileUrl: true` to get documents with their content URLs\n2. Extract the URL from the document response\n3. Use `fetchUrlContent` to retrieve the actual content:\n\n```json\n{\n  \"name\": \"fetchUrlContent\",\n  \"arguments\": {\n    \"url\": \"https://example.com\"\n  }\n}\n```\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "searches",
        "retrieval",
        "search",
        "retrieval document",
        "searches facilitates",
        "web search"
      ],
      "category": "web-search"
    },
    "penghongru--RuoYi-Vue3": {
      "owner": "penghongru",
      "name": "RuoYi-Vue3",
      "url": "https://github.com/penghongru/RuoYi-Vue3",
      "imageUrl": "/freedevtools/mcp/pfp/penghongru.webp",
      "description": "A Java framework designed for rapid development of modern web applications using a front-end stack of Vue3 and Element Plus, facilitating user and role management among other built-in features.",
      "stars": 0,
      "forks": 0,
      "license": "MIT License",
      "language": "",
      "updated_at": "2023-09-26T10:56:52Z",
      "readme_content": "<p align=\"center\">\r\n\t<img alt=\"logo\" src=\"https://oscimg.oschina.net/oscnet/up-d3d0a9303e11d522a06cd263f3079027715.png\">\r\n</p>\r\n<h1 align=\"center\" style=\"margin: 30px 0 30px; font-weight: bold;\">RuoYi v3.8.6</h1>\r\n<h4 align=\"center\">基于SpringBoot+Vue3前后端分离的Java快速开发框架</h4>\r\n<p align=\"center\">\r\n\t<a href=\"https://gitee.com/y_project/RuoYi-Vue/stargazers\"><img alt=\"star_svg_theme_dark\" src=\"https://gitee.com/y_project/RuoYi-Vue/badge/star.svg?theme=dark\"></a>\r\n\t<a href=\"https://gitee.com/y_project/RuoYi-Vue\"><img alt=\"RuoYi_v3_8_6_brightgreen\" src=\"https://img.shields.io/badge/RuoYi-v3.8.6-brightgreen.svg\"></a>\r\n\t<a href=\"https://gitee.com/y_project/RuoYi-Vue/blob/master/LICENSE\"><img alt=\"apistatus\" src=\"https://img.shields.io/github/license/mashape/apistatus.svg\"></a>\r\n</p>\r\n\r\n## 平台简介\r\n\r\n* 本仓库为前端技术栈 [Vue3](https://v3.cn.vuejs.org) + [Element Plus](https://element-plus.org/zh-CN) + [Vite](https://cn.vitejs.dev) 版本。\r\n* 配套后端代码仓库地址[RuoYi-Vue](https://gitee.com/y_project/RuoYi-Vue) 或 [RuoYi-Vue-fast](https://github.com/yangzongzhuan/RuoYi-Vue-fast) 版本。\r\n* 前端技术栈（[Vue2](https://cn.vuejs.org) + [Element](https://github.com/ElemeFE/element) + [Vue CLI](https://cli.vuejs.org/zh)），请移步[RuoYi-Vue](https://gitee.com/y_project/RuoYi-Vue/tree/master/ruoyi-ui)。\r\n* 阿里云折扣场：[点我进入](http://aly.ruoyi.vip)，腾讯云秒杀场：[点我进入](http://txy.ruoyi.vip)&nbsp;&nbsp;\r\n* 阿里云优惠券：[点我领取](https://www.aliyun.com/minisite/goods?userCode=brki8iof&share_source=copy_link)，腾讯云优惠券：[点我领取](https://cloud.tencent.com/redirect.php?redirect=1025&cps_key=198c8df2ed259157187173bc7f4f32fd&from=console)&nbsp;&nbsp;\r\n\r\n## 前端运行\r\n\r\n```bash\r\n# 克隆项目\r\ngit clone https://github.com/yangzongzhuan/RuoYi-Vue3.git\r\n\r\n# 进入项目目录\r\ncd RuoYi-Vue3\r\n\r\n# 安装依赖\r\nyarn --registry=https://registry.npmmirror.com\r\n\r\n# 启动服务\r\nyarn dev\r\n\r\n# 构建测试环境 yarn build:stage\r\n# 构建生产环境 yarn build:prod\r\n# 前端访问地址 http://localhost:80\r\n```\r\n\r\n## 内置功能\r\n\r\n1.  用户管理：用户是系统操作者，该功能主要完成系统用户配置。\r\n2.  部门管理：配置系统组织机构（公司、部门、小组），树结构展现支持数据权限。\r\n3.  岗位管理：配置系统用户所属担任职务。\r\n4.  菜单管理：配置系统菜单，操作权限，按钮权限标识等。\r\n5.  角色管理：角色菜单权限分配、设置角色按机构进行数据范围权限划分。\r\n6.  字典管理：对系统中经常使用的一些较为固定的数据进行维护。\r\n7.  参数管理：对系统动态配置常用参数。\r\n8.  通知公告：系统通知公告信息发布维护。\r\n9.  操作日志：系统正常操作日志记录和查询；系统异常信息日志记录和查询。\r\n10. 登录日志：系统登录日志记录查询包含登录异常。\r\n11. 在线用户：当前系统中活跃用户状态监控。\r\n12. 定时任务：在线（添加、修改、删除)任务调度包含执行结果日志。\r\n13. 代码生成：前后端代码的生成（java、html、xml、sql）支持CRUD下载 。\r\n14. 系统接口：根据业务代码自动生成相关的api接口文档。\r\n15. 服务监控：监视当前系统CPU、内存、磁盘、堆栈等相关信息。\r\n16. 缓存监控：对系统的缓存信息查询，命令统计等。\r\n17. 在线构建器：拖动表单元素生成相应的HTML代码。\r\n18. 连接池监视：监视当前系统数据库连接池状态，可进行分析SQL找出系统性能瓶颈。\r\n\r\n## 在线体验\r\n\r\n- admin/admin123  \r\n- 陆陆续续收到一些打赏，为了更好的体验已用于演示服务器升级。谢谢各位小伙伴。\r\n\r\n演示地址：http://vue.ruoyi.vip  \r\n文档地址：http://doc.ruoyi.vip\r\n\r\n## 演示图\r\n\r\n<table>\r\n    <tr>\r\n        <td><img src=\"https://oscimg.oschina.net/oscnet/cd1f90be5f2684f4560c9519c0f2a232ee8.jpg\"/></td>\r\n        <td><img src=\"https://oscimg.oschina.net/oscnet/1cbcf0e6f257c7d3a063c0e3f2ff989e4b3.jpg\"/></td>\r\n    </tr>\r\n    <tr>\r\n        <td><img src=\"https://oscimg.oschina.net/oscnet/up-8074972883b5ba0622e13246738ebba237a.png\"/></td>\r\n        <td><img src=\"https://oscimg.oschina.net/oscnet/up-9f88719cdfca9af2e58b352a20e23d43b12.png\"/></td>\r\n    </tr>\r\n    <tr>\r\n        <td><img src=\"https://oscimg.oschina.net/oscnet/up-39bf2584ec3a529b0d5a3b70d15c9b37646.png\"/></td>\r\n        <td><img src=\"https://oscimg.oschina.net/oscnet/up-936ec82d1f4872e1bc980927654b6007307.png\"/></td>\r\n    </tr>\r\n\t<tr>\r\n        <td><img src=\"https://oscimg.oschina.net/oscnet/up-b2d62ceb95d2dd9b3fbe157bb70d26001e9.png\"/></td>\r\n        <td><img src=\"https://oscimg.oschina.net/oscnet/up-d67451d308b7a79ad6819723396f7c3d77a.png\"/></td>\r\n    </tr>\t \r\n    <tr>\r\n        <td><img src=\"https://oscimg.oschina.net/oscnet/5e8c387724954459291aafd5eb52b456f53.jpg\"/></td>\r\n        <td><img src=\"https://oscimg.oschina.net/oscnet/644e78da53c2e92a95dfda4f76e6d117c4b.jpg\"/></td>\r\n    </tr>\r\n\t<tr>\r\n        <td><img src=\"https://oscimg.oschina.net/oscnet/up-8370a0d02977eebf6dbf854c8450293c937.png\"/></td>\r\n        <td><img src=\"https://oscimg.oschina.net/oscnet/up-49003ed83f60f633e7153609a53a2b644f7.png\"/></td>\r\n    </tr>\r\n\t<tr>\r\n        <td><img src=\"https://oscimg.oschina.net/oscnet/up-d4fe726319ece268d4746602c39cffc0621.png\"/></td>\r\n        <td><img src=\"https://oscimg.oschina.net/oscnet/up-c195234bbcd30be6927f037a6755e6ab69c.png\"/></td>\r\n    </tr>\r\n    <tr>\r\n        <td><img src=\"https://oscimg.oschina.net/oscnet/b6115bc8c31de52951982e509930b20684a.jpg\"/></td>\r\n        <td><img src=\"https://oscimg.oschina.net/oscnet/up-5e4daac0bb59612c5038448acbcef235e3a.png\"/></td>\r\n    </tr>\r\n</table>\r\n\r\n\r\n## 若依前后端分离交流群\r\n\r\nQQ群： [![加入QQ群](https://img.shields.io/badge/已满-937441-blue.svg)](https://jq.qq.com/?_wv=1027&k=5bVB1og) [![加入QQ群](https://img.shields.io/badge/已满-887144332-blue.svg)](https://jq.qq.com/?_wv=1027&k=5eiA4DH) [![加入QQ群](https://img.shields.io/badge/已满-180251782-blue.svg)](https://jq.qq.com/?_wv=1027&k=5AxMKlC) [![加入QQ群](https://img.shields.io/badge/已满-104180207-blue.svg)](https://jq.qq.com/?_wv=1027&k=51G72yr) [![加入QQ群](https://img.shields.io/badge/已满-186866453-blue.svg)](https://jq.qq.com/?_wv=1027&k=VvjN2nvu) [![加入QQ群](https://img.shields.io/badge/已满-201396349-blue.svg)](https://jq.qq.com/?_wv=1027&k=5vYAqA05) [![加入QQ群](https://img.shields.io/badge/已满-101456076-blue.svg)](https://jq.qq.com/?_wv=1027&k=kOIINEb5) [![加入QQ群](https://img.shields.io/badge/已满-101539465-blue.svg)](https://jq.qq.com/?_wv=1027&k=UKtX5jhs) [![加入QQ群](https://img.shields.io/badge/已满-264312783-blue.svg)](https://jq.qq.com/?_wv=1027&k=EI9an8lJ) [![加入QQ群](https://img.shields.io/badge/已满-167385320-blue.svg)](https://jq.qq.com/?_wv=1027&k=SWCtLnMz) [![加入QQ群](https://img.shields.io/badge/已满-104748341-blue.svg)](https://jq.qq.com/?_wv=1027&k=96Dkdq0k) [![加入QQ群](https://img.shields.io/badge/已满-160110482-blue.svg)](https://jq.qq.com/?_wv=1027&k=0fsNiYZt) [![加入QQ群](https://img.shields.io/badge/已满-170801498-blue.svg)](https://jq.qq.com/?_wv=1027&k=7xw4xUG1) [![加入QQ群](https://img.shields.io/badge/已满-108482800-blue.svg)](https://jq.qq.com/?_wv=1027&k=eCx8eyoJ) [![加入QQ群](https://img.shields.io/badge/已满-101046199-blue.svg)](https://jq.qq.com/?_wv=1027&k=SpyH2875) [![加入QQ群](https://img.shields.io/badge/已满-136919097-blue.svg)](https://jq.qq.com/?_wv=1027&k=tKEt51dz) [![加入QQ群](https://img.shields.io/badge/143961921-blue.svg)](http://qm.qq.com/cgi-bin/qm/qr?_wv=1027&k=0vBbSb0ztbBgVtn3kJS-Q4HUNYwip89G&authKey=8irq5PhutrZmWIvsUsklBxhj57l%2F1nOZqjzigkXZVoZE451GG4JHPOqW7AW6cf0T&noverify=0&group_code=143961921) 点击按钮入群。",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "vue3",
        "framework",
        "java",
        "vue3 java",
        "java framework",
        "web applications"
      ],
      "category": "web-search"
    },
    "perplexityai--modelcontextprotocol": {
      "owner": "perplexityai",
      "name": "modelcontextprotocol",
      "url": "https://github.com/perplexityai/modelcontextprotocol",
      "imageUrl": "/freedevtools/mcp/pfp/perplexityai.webp",
      "description": "Integrates the Sonar API for live web searches, enabling conversations that leverage real-time web-wide research capabilities. Facilitates instant information retrieval directly from the web during interactions.",
      "stars": 1643,
      "forks": 229,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-10-02T16:20:54Z",
      "readme_content": "# Perplexity Ask MCP Server\n\nAn MCP server implementation that integrates the Sonar API to provide Claude with unparalleled real-time, web-wide research.\n\nPlease refer to the official [DeepWiki page](https://deepwiki.com/ppl-ai/modelcontextprotocol) for assistance with implementation. \n\n# High-level System Architecture\n\n*Credits: DeepWiki powered by Devin*\n\n\n\n\n\n\n\n\n\n\n## Tools\n\n- **perplexity_ask**\n  - Engage in a conversation with the Sonar API for live web searches.\n  - **Inputs:**\n    - `messages` (array): An array of conversation messages.\n      - Each message must include:\n        - `role` (string): The role of the message (e.g., `system`, `user`, `assistant`).\n        - `content` (string): The content of the message.\n\n## Configuration\n\n### Step 1: \n\nClone this repository:\n\n```bash\ngit clone git@github.com:ppl-ai/modelcontextprotocol.git\n```\n\nNavigate to the `perplexity-ask` directory and install the necessary dependencies:\n\n```bash\ncd modelcontextprotocol/perplexity-ask && npm install\n```\n\n### Step 2: Get a Sonar API Key\n\n1. Sign up for a [Sonar API account](https://docs.perplexity.ai/guides/getting-started).\n2. Follow the account setup instructions and generate your API key from the developer dashboard.\n3. Set the API key in your environment as `PERPLEXITY_API_KEY`.\n\n### Step 3: Configure Claude Desktop\n\n1. Download Claude desktop [here](https://claude.ai/download). \n\n2. Add this to your `claude_desktop_config.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"perplexity-ask\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"-i\",\n        \"--rm\",\n        \"-e\",\n        \"PERPLEXITY_API_KEY\",\n        \"mcp/perplexity-ask\"\n      ],\n      \"env\": {\n        \"PERPLEXITY_API_KEY\": \"YOUR_API_KEY_HERE\"\n      }\n    }\n  }\n}\n```\n\n### NPX\n\n```json\n{\n  \"mcpServers\": {\n    \"perplexity-ask\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"server-perplexity-ask\"\n      ],\n      \"env\": {\n        \"PERPLEXITY_API_KEY\": \"YOUR_API_KEY_HERE\"\n      }\n    }\n  }\n}\n```\n\nYou can access the file using:\n\n```bash\nvim ~/Library/Application\\ Support/Claude/claude_desktop_config.json\n```\n\n### Step 4: Build the Docker Image\n\nDocker build:\n\n```bash\ndocker build -t mcp/perplexity-ask:latest -f Dockerfile .\n```\n\n### Step 5: Testing\n\nLet's make sure Claude for Desktop is picking up the two tools we've exposed in our `perplexity-ask` server. You can do this by looking for the hammer icon:\n\n\n\nAfter clicking on the hammer icon, you should see the tools that come with the Filesystem MCP Server:\n\n\n\nIf you see both of these this means that the integration is active. Congratulations! This means Claude can now ask Perplexity. You can then simply use it as you would use the Perplexity web app.  \n\n### Step 6: Advanced parameters\n\nCurrently, the search parameters used are the default ones. You can modify any search parameter in the API call directly in the `index.ts` script. For this, please refer to the official [API documentation](https://docs.perplexity.ai/api-reference/chat-completions).\n\n### Troubleshooting \n\nThe Claude documentation provides an excellent [troubleshooting guide](https://modelcontextprotocol.io/docs/tools/debugging) you can refer to. However, you can still reach out to us at api@perplexity.ai for any additional support or [file a bug](https://github.com/ppl-ai/api-discussion/issues). \n\n\n# Cursor integration\n\nYou can also use our MCP with Cursor (or any other app that supports this). To use Sonar with Cursor, you can follow the following steps. \n\n### Step 1: Navigate to your Cursor settings:\n\n\n\n### Step 2: Navigate to the MCP directory\n\nAnd click on `Add new global MCP server`\n\n\n\n\n### Step 3: Insert the MCP Server Configuration from above \n\nThis is the same configuration you would use for any other application that supports MCP. \n\nYou should then see the application being part of your available tools like this:\n\n\n\n\n## License\n\nThis MCP server is licensed under the MIT License. This means you are free to use, modify, and distribute the software, subject to the terms and conditions of the MIT License. For more details, please see the LICENSE file in the project repository.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "searches",
        "sonar",
        "web",
        "web search",
        "web searches",
        "sonar api"
      ],
      "category": "web-search"
    },
    "pfldy2850--py-mcp-naver": {
      "owner": "pfldy2850",
      "name": "py-mcp-naver",
      "url": "https://github.com/pfldy2850/py-mcp-naver",
      "imageUrl": "/freedevtools/mcp/pfp/pfldy2850.webp",
      "description": "Interact with various Naver services to search for blogs, news, books, and more using the Naver OpenAPI. Facilitate powerful search capabilities and content validation for applications.",
      "stars": 111,
      "forks": 19,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-09T10:50:00Z",
      "readme_content": "# Naver MCP Server\n\nA server implementation for Naver OpenAPI using the Model Context Protocol (MCP). This project provides tools to interact with various Naver services, such as searching blogs, news, books, and more.\n\n\n## Pre-requisite\nTo use the Naver MCP server, you need to apply for access to the Naver Open API. \nYou can apply for Open API access at the link below:\n\nhttps://developers.naver.com/apps/#/register=datalab\n\n\n## Installation\n\n### from PyPi (Claude Desktop)\n\nInstall it to Claude Desktop with (uv):\n```sh\nuv pip install mcp-naver\n\nuv run python -m mcp-naver.hosts.claude_desktop \\\n  -e NAVER_CLIENT_ID=<YOUR NAVER CLIENT ID> \\\n  -e NAVER_CLIENT_SECRET=<YOUR NAVER CLIENT SECRET>\n```\n\nInstall it to Claude Desktop with:\n```sh\npip install mcp-naver\n\npython -m mcp-naver.hosts.claude_desktop \\\n  -e NAVER_CLIENT_ID=<YOUR NAVER CLIENT ID> \\\n  -e NAVER_CLIENT_SECRET=<YOUR NAVER CLIENT SECRET>\n```\n\n\n### from PyPi (Cursor)\n\nInstall it to Cursor with (uv):\n```sh\nuv pip install mcp-naver\n\nuv run python -m mcp-naver.hosts.cursor \\\n  -e NAVER_CLIENT_ID=<YOUR NAVER CLIENT ID> \\\n  -e NAVER_CLIENT_SECRET=<YOUR NAVER CLIENT SECRET>\n```\n\n\n### from source\n\n```sh\n# Clone the repository\ngit clone https://github.com/pfldy2850/py-mcp-naver.git\n\n# Navigate into the project directory\ncd py-mcp-naver\n\n# Synchronize dependencies\nuv sync --dev --all-extras\n```\n\nRun it with:\n```sh\n# Start the server (Using FastMCP CLI)\nfastmcp install mcp_naver/server.py -e NAVER_CLIENT_ID=<YOUR NAVER CLIENT ID> -e NAVER_CLIENT_SECRET=<YOUR NAVER CLIENT SECRET>\n```\n\nThe tool sets up everything you need to create an MCP server integrated with Naver OpenAPI.\n\n## Features\n\nThis server provides the following tools for interacting with Naver OpenAPI:\n\n- **Blog Search**: Search blog posts on Naver.\n- **News Search**: Search news articles on Naver.\n- **Book Search**: Search books and advanced book information.\n- **Adult Content Check**: Check if a search term is adult content.\n- **Encyclopedia Search**: Search encyclopedia entries.\n- **Cafe Article Search**: Search articles in Naver cafes.\n- **Q&A Search**: Search questions and answers on Naver.\n- **Local Search**: Search local information.\n- **Spelling Correction**: Correct spelling errors in text.\n- **Web Search**: Search web pages.\n- **Image Search**: Search images with filters.\n- **Shopping Search**: Search shopping items with filters.\n- **Document Search**: Search documents.\n\n\n\n## Naver MCP Tools\n\nThe following tools are implemented in the server:\n\n### Blog Search\nSearch blog posts on Naver.\n```python\nsearch_blog(query: str, display: int = 10, start: int = 1, sort: str = \"sim\")\n```\n\n### News Search\nSearch news articles on Naver.\n```python\nsearch_news(query: str, display: int = 10, start: int = 1, sort: str = \"sim\")\n```\n\n### Book Search\nSearch books on Naver.\n```python\nsearch_book(query: str, display: int = 10, start: int = 1, sort: str = \"sim\")\n```\n\n### Advanced Book Search\nGet detailed book information using title or ISBN.\n```python\nget_book_adv(query: str = None, d_titl: str = None, d_isbn: str = None, ...)\n```\n\n### Adult Content Check\nCheck if a search term is adult content.\n```python\nadult_check(query: str)\n```\n\n### Encyclopedia Search\nSearch encyclopedia entries on Naver.\n```python\nsearch_encyc(query: str, display: int = 10, start: int = 1)\n```\n\n### Cafe Article Search\nSearch articles in Naver cafes.\n```python\nsearch_cafe_article(query: str, display: int = 10, start: int = 1, sort: str = \"sim\")\n```\n\n### Q&A Search\nSearch questions and answers on Naver.\n```python\nsearch_kin(query: str, display: int = 10, start: int = 1, sort: str = \"sim\")\n```\n\n### Local Search\nSearch local information on Naver.\n```python\nsearch_local(query: str, display: int = 10, start: int = 1, sort: str = \"random\")\n```\n\n### Spelling Correction\nCorrect spelling errors in a given text.\n```python\nfix_spelling(query: str)\n```\n\n### Web Search\nSearch web pages on Naver.\n```python\nsearch_webkr(query: str, display: int = 10, start: int = 1)\n```\n\n### Image Search\nSearch images on Naver with filters.\n```python\nsearch_image(query: str, display: int = 10, start: int = 1, sort: str = \"sim\", filter: str = \"all\")\n```\n\n### Shopping Search\nSearch shopping items on Naver with filters.\n```python\nsearch_shop(query: str, display: int = 10, start: int = 1, sort: str = \"sim\", filter: str = None, exclude: str = None)\n```\n\n### Document Search\nSearch documents on Naver.\n```python\nsearch_doc(query: str, display: int = 10, start: int = 1)\n```\n\n## License\n\nThis project is open source software [licensed as MIT](https://opensource.org/licenses/MIT).",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "naver",
        "openapi",
        "search",
        "naver openapi",
        "naver services",
        "using naver"
      ],
      "category": "web-search"
    },
    "phpmac--fetch_mcp": {
      "owner": "phpmac",
      "name": "fetch_mcp",
      "url": "https://github.com/phpmac/fetch_mcp",
      "imageUrl": "/freedevtools/mcp/pfp/phpmac.webp",
      "description": "Fetch web content in various formats like HTML, JSON, plain text, and Markdown. Retrieve and transform website data on demand with customizable headers.",
      "stars": 1,
      "forks": 0,
      "license": "No License",
      "language": "TypeScript",
      "updated_at": "2025-04-14T07:20:07Z",
      "readme_content": "# Fetch MCP Server\n\n[English](./README.md) | [中文](./README_ZH.md)\n\nThis MCP server provides functionality to fetch web content in various formats, including HTML, JSON, plain text, and Markdown.\n\n### Tools\n\n- **fetch_html**\n\n  - Fetch website content and return as HTML\n  - Input parameters:\n    - `url` (string, required): URL of the website to fetch\n    - `headers` (object, optional): Custom headers to include in the request\n  - Returns the raw HTML content of the webpage\n\n- **fetch_json**\n\n  - Fetch JSON file from URL\n  - Input parameters:\n    - `url` (string, required): URL of the JSON to fetch\n    - `headers` (object, optional): Custom headers to include in the request\n  - Returns the parsed JSON content\n\n- **fetch_txt**\n\n  - Fetch website content and return as plain text (no HTML)\n  - Input parameters:\n    - `url` (string, required): URL of the website to fetch\n    - `headers` (object, optional): Custom headers to include in the request\n  - Returns the text content of the webpage with HTML tags, scripts, and styles removed\n\n- **fetch_markdown**\n  - Fetch website content and return as Markdown\n  - Input parameters:\n    - `url` (string, required): URL of the website to fetch\n    - `headers` (object, optional): Custom headers to include in the request\n  - Returns the webpage content converted to Markdown format\n\n### 2 Ways to Start\n\n1. bun\n\n```bash\nbun i\nbun start\n```\n\n2. docker\n\n```bash\ndocker compose up --build -d\n```\n\n### Usage\n\n```json\n{\n  \"mcpServers\": {\n    \"fetch-mcp\": {\n      \"transport\": \"sse\",\n      \"url\": \"http://localhost:3000/sse\",\n      \"headers\": {\n        \"Authorization\": \"Bearer your-token-here\",\n        \"X-Custom-Header\": \"custom-value\"\n      },\n      \"useNodeEventSource\": true\n    }\n  }\n}\n```\n\n### Resources\n\nThis server does not provide any persistent resources. It is designed to fetch and transform web content on demand.\n\n### References\n\n- [Original Repository zcaceres/fetch-mcp](https://github.com/zcaceres/fetch-mcp)\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "phpmac",
        "fetch_mcp",
        "fetch",
        "search phpmac",
        "phpmac fetch_mcp",
        "fetch web"
      ],
      "category": "web-search"
    },
    "pinkpixel-dev--prysm-mcp-server": {
      "owner": "pinkpixel-dev",
      "name": "prysm-mcp-server",
      "url": "https://github.com/pinkpixel-dev/prysm-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/pinkpixel-dev.webp",
      "description": "A server designed for web scraping that offers various modes for optimized content retrieval, URL analysis for efficient scraping strategies, and output formatting in markdown, HTML, or JSON. Additionally, it supports image extraction and provides functionalities for saving results to a specified directory.",
      "stars": 2,
      "forks": 4,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-06-02T01:40:09Z",
      "readme_content": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/pinkpixel-dev-prysm-mcp-server-badge.png)](https://mseep.ai/app/pinkpixel-dev-prysm-mcp-server)\n\n# 🔍 Prysm MCP Server\n\nThe Prysm MCP (Model Context Protocol) Server enables AI assistants like Claude and others to scrape web content with high accuracy and flexibility.\n\n## ✨ Features\n\n- 🎯 **Multiple Scraping Modes**: Choose from focused (speed), balanced (default), or deep (thorough) modes\n- 🧠 **Content Analysis**: Analyze URLs to determine the best scraping approach\n- 📄 **Format Flexibility**: Format results as markdown, HTML, or JSON\n- 🖼️ **Image Support**: Optionally extract and even download images\n- 🔍 **Smart Scrolling**: Configure scroll behavior for single-page applications\n- 📱 **Responsive**: Adapts to different website layouts and structures\n- 💾 **File Output**: Save formatted results to your preferred directory\n\n## 🚀 Quick Start\n\n### Installation\n\n```bash\n# Recommended: Install the LLM-optimized version\nnpm install -g @pinkpixel/prysm-mcp\n\n# Or install the standard version\nnpm install -g prysm-mcp\n\n# Or clone and build\ngit clone https://github.com/pinkpixel-dev/prysm-mcp.git\ncd prysm-mcp\nnpm install\nnpm run build\n```\n\n### Integration Guides\n\nWe provide detailed integration guides for popular MCP-compatible applications:\n\n- [Cursor Integration Guide](./docs/cursor-mcp-integration.md)\n- [Claude Desktop Integration Guide](./docs/claude-desktop-mcp-integration.md)\n- [Windsurf Integration Guide](./docs/windsurf-mcp-integration.md)\n- [Cline Integration Guide](./docs/cline-mcp-integration.md)\n- [Roo Code Integration Guide](./docs/roo-code-mcp-integration.md)\n- [Open WebUI Integration Guide](./docs/openwebui-mcp-integration.md)\n\n### Usage\n\nThere are multiple ways to set up Prysm MCP Server:\n\n#### Using mcp.json Configuration\n\nCreate a `mcp.json` file in the appropriate location according to the above guides. \n\n```json\n{\n  \"mcpServers\": {\n    \"prysm-scraper\": {\n      \"description\": \"Prysm web scraper with custom output directories\",\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"@pinkpixel/prysm-mcp\"\n      ],\n      \"env\": {\n        \"PRYSM_OUTPUT_DIR\": \"${workspaceFolder}/scrape_results\",\n        \"PRYSM_IMAGE_OUTPUT_DIR\": \"${workspaceFolder}/scrape_results/images\"\n      }\n    }\n  }\n}\n```\n\n## 🛠️ Tools\n\nThe server provides the following tools:\n\n### `scrapeFocused`\n\nFast web scraping optimized for speed (fewer scrolls, main content only).\n\n```\nPlease scrape https://example.com using the focused mode\n```\n\n**Available Parameters:**\n- `url` (required): URL to scrape\n- `maxScrolls` (optional): Maximum number of scroll attempts (default: 5)\n- `scrollDelay` (optional): Delay between scrolls in ms (default: 1000)\n- `scrapeImages` (optional): Whether to include images in results\n- `downloadImages` (optional): Whether to download images locally\n- `maxImages` (optional): Maximum images to extract\n- `output` (optional): Output directory for downloaded images\n\n### `scrapeBalanced`\n\nBalanced web scraping approach with good coverage and reasonable speed.\n\n```\nPlease scrape https://example.com using the balanced mode\n```\n\n**Available Parameters:**\n- Same as `scrapeFocused` with different defaults\n- `maxScrolls` default: 10\n- `scrollDelay` default: 2000\n- Adds `timeout` parameter to limit total scraping time (default: 30000ms)\n\n### `scrapeDeep`\n\nMaximum extraction web scraping (slower but thorough).\n\n```\nPlease scrape https://example.com using the deep mode with maximum scrolls\n```\n\n**Available Parameters:**\n- Same as `scrapeFocused` with different defaults\n- `maxScrolls` default: 20\n- `scrollDelay` default: 3000\n- `maxImages` default: 100\n\n### `formatResult`\n\nFormat scraped data into different structured formats (markdown, HTML, JSON).\n\n```\nFormat the scraped data as markdown\n```\n\n**Available Parameters:**\n- `data` (required): The scraped data to format\n- `format` (required): Output format - \"markdown\", \"html\", or \"json\"\n- `includeImages` (optional): Whether to include images in output (default: true)\n- `output` (optional): File path to save the formatted result\n\nYou can also save formatted results to a file by specifying an output path:\n\n```\nFormat the scraped data as markdown and save it to \"my-results/output.md\"\n```\n\n## ⚙️ Configuration\n\n### Output Directory\n\nBy default, when saving formatted results, files will be saved to `~/prysm-mcp/output/`. You can customize this in two ways:\n\n1. **Environment Variables**: Set environment variables to your preferred directories:\n\n```bash\n# Linux/macOS\nexport PRYSM_OUTPUT_DIR=\"/path/to/custom/directory\"\nexport PRYSM_IMAGE_OUTPUT_DIR=\"/path/to/custom/image/directory\"\n\n# Windows (Command Prompt)\nset PRYSM_OUTPUT_DIR=C:\\path\\to\\custom\\directory\nset PRYSM_IMAGE_OUTPUT_DIR=C:\\path\\to\\custom\\image\\directory\n\n# Windows (PowerShell)\n$env:PRYSM_OUTPUT_DIR=\"C:\\path\\to\\custom\\directory\"\n$env:PRYSM_IMAGE_OUTPUT_DIR=\"C:\\path\\to\\custom\\image\\directory\"\n```\n\n2. **Tool Parameter**: Specify output paths directly when calling the tools:\n\n```\n# For general results\nFormat the scraped data as markdown and save it to \"/absolute/path/to/file.md\"\n\n# For image downloads when scraping\nPlease scrape https://example.com and download images to \"/absolute/path/to/images\"\n```\n\n3. **MCP Configuration**: In your MCP configuration file (e.g., `.cursor/mcp.json`), you can set these environment variables:\n\n```json\n{\n  \"mcpServers\": {\n    \"prysm-scraper\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@pinkpixel/prysm-mcp\"],\n      \"env\": {\n        \"PRYSM_OUTPUT_DIR\": \"${workspaceFolder}/scrape_results\",\n        \"PRYSM_IMAGE_OUTPUT_DIR\": \"${workspaceFolder}/scrape_results/images\"\n      }\n    }\n  }\n}\n```\n\nIf `PRYSM_IMAGE_OUTPUT_DIR` is not specified, it will default to a subfolder named `images` inside the `PRYSM_OUTPUT_DIR`.\n\nIf you provide only a relative path or filename, it will be saved relative to the configured output directory.\n\n### Path Handling Rules\n\nThe `formatResult` tool handles paths in the following ways:\n\n- **Absolute paths**: Used exactly as provided (`/home/user/file.md`)\n- **Relative paths**: Saved relative to the configured output directory (`subfolder/file.md`)\n- **Filename only**: Saved in the configured output directory (`output.md`)\n- **Directory path**: If the path points to a directory, a filename is auto-generated based on content and timestamp\n\n## 🏗️ Development\n\n```bash\n# Install dependencies\nnpm install\n\n# Build the project\nnpm run build\n\n# Run the server locally\nnode bin/prysm-mcp\n\n# Debug MCP communication\nDEBUG=mcp:* node bin/prysm-mcp\n\n# Set custom output directories\nPRYSM_OUTPUT_DIR=./my-output PRYSM_IMAGE_OUTPUT_DIR=./my-output/images node bin/prysm-mcp\n```\n\n### Running via npx\n\nYou can run the server directly with npx without installing:\n\n```bash\n# Run with default settings\nnpx @pinkpixel/prysm-mcp\n\n# Run with custom output directories\nPRYSM_OUTPUT_DIR=./my-output PRYSM_IMAGE_OUTPUT_DIR=./my-output/images npx @pinkpixel/prysm-mcp\n```\n\n## 📋 License\n\nMIT\n\n## 🙏 Credits\n\nDeveloped by [Pink Pixel](https://pinkpixel.dev)\n\nPowered by the [Model Context Protocol](https://modelcontextprotocol.io) and [Puppeteer](https://pptr.dev/) ",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "scraping",
        "pinkpixel",
        "web",
        "search pinkpixel",
        "web scraping",
        "efficient scraping"
      ],
      "category": "web-search"
    },
    "pjookim--mcp-visit-korea": {
      "owner": "pjookim",
      "name": "mcp-visit-korea",
      "url": "https://github.com/pjookim/mcp-visit-korea",
      "imageUrl": "/freedevtools/mcp/pfp/pjookim.webp",
      "description": "Provides information related to tourism in Korea, including attractions, festivals, and accommodations. Supports various search methods based on area codes, keywords, or locations for detailed tourism content.",
      "stars": 2,
      "forks": 1,
      "license": "No License",
      "language": "JavaScript",
      "updated_at": "2025-09-17T06:03:50Z",
      "readme_content": "# Korea Tour MCP Server\n[![smithery badge](https://smithery.ai/badge/@pjookim/mcp-visit-korea)](https://smithery.ai/server/@pjookim/mcp-visit-korea)\nMCP (Model Context Protocol) server providing Korean tourism information.\n\n## Features\n- Area code lookup: Query metropolitan cities/provinces and sub-regions\n- Tourism information search: Support for region-based, keyword-based, and location-based searches\n- Tourism content detail lookup: Provides detailed information about attractions, festivals, accommodations, etc.\n\n\n### Installing via Smithery\n\nTo install mcp-visit-korea for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@pjookim/mcp-visit-korea):\n\n```bash\nnpx -y @smithery/cli install @pjookim/mcp-visit-korea --client claude\n```\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "pjookim",
        "mcp",
        "korea",
        "tourism korea",
        "visit korea",
        "mcp visit"
      ],
      "category": "web-search"
    },
    "pl728--substack-fetcher-mcp": {
      "owner": "pl728",
      "name": "substack-fetcher-mcp",
      "url": "https://github.com/pl728/substack-fetcher-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/pl728.webp",
      "description": "Fetch and read articles from Trade Companion by Adam Mancini on Substack, providing access to subscriber-only content in a clean, readable format. It integrates with Claude AI for an enhanced reading experience.",
      "stars": 1,
      "forks": 2,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-03-06T07:42:32Z",
      "readme_content": "# Substack Reader\n\nA tool to fetch and read articles from Trade Companion by Adam Mancini on Substack.\n\n## Setup\n\n### Prerequisites\n\n1. Python 3.8+\n2. uv package manager for Python\n3. Claude AI assistant\n\n### Installation\n\n1. Install uv package manager if you don't have it already:\n   ```bash\n   curl -sSf https://install.ultraviolet.dev | sh\n   ```\n\n2. Create and activate a virtual environment:\n   ```bash\n   uv venv\n   source .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\n   ```\n\n3. Install dependencies using the pyproject.toml file:\n   ```bash\n   uv pip install -e .\n   ```\n\n### Setting up Substack Authentication\n\nTo access subscriber-only content, you'll need to provide your Substack cookies:\n\n1. Install the Cookie-Editor extension for your browser:\n   - [Chrome Web Store](https://chrome.google.com/webstore/detail/cookie-editor/hlkenndednhfkekhgcdicdfddnkalmdm)\n   - [Firefox Add-ons](https://addons.mozilla.org/en-US/firefox/addon/cookie-editor/)\n\n2. Log in to your Substack account at [tradecompanion.substack.com](https://tradecompanion.substack.com)\n\n3. Click on the Cookie-Editor extension icon\n\n4. Click \"Export\" and select \"Export as JSON\" (This copies the cookies to your clipboard)\n\n5. Create a file named `substack_cookies.json` in the root directory of this project\n\n6. Paste the copied cookies into this file and save\n\n## Usage with Claude\n\nThis tool is designed to be used with Claude AI assistant. To set it up:\n\n1. Configure Claude to use this MCP server by adding the following to your Claude config file:\n\n```json\n{\n  \"mcpServers\": {\n    \"substack_reader\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"--directory\",\n        \"/path/to/substack_reader\",\n        \"run\",\n        \"substack_reader.py\"\n      ]\n    }\n  },\n  \"globalShortcut\": \"Ctrl+Space\"\n}\n```\n\nReplace `/path/to/substack_reader` with the actual path to your substack_reader directory.\n\n2. When properly configured, Claude will automatically connect to this MCP server when launched.\n\n3. You can then ask Claude to fetch the latest Trade Companion article.\n\n## Features\n\n- Fetches the latest Trade Companion articles by Adam Mancini\n- Extracts article content in plain text format\n- Preserves headings, paragraphs, and list items\n- Excludes the \"My Trade Methodology Fundamentals\" article\n\n## Privacy Note\n\nYour Substack cookies are stored locally in the `substack_cookies.json` file and are only used to authenticate requests to Substack. They are not sent anywhere else or exposed in any way.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "substack",
        "fetcher",
        "fetch",
        "substack fetcher",
        "pl728 substack",
        "fetch read"
      ],
      "category": "web-search"
    },
    "podaac--cmr-mcp": {
      "owner": "podaac",
      "name": "cmr-mcp",
      "url": "https://github.com/podaac/cmr-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/podaac.webp",
      "description": "Integrate AI with NASA's Catalog of datasets through Earthdata, enabling seamless searches for Earthdata metadata. Enhance data discovery with intelligent search capabilities by accessing the common metadata repository (CMR).",
      "stars": 4,
      "forks": 2,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-08-10T14:48:04Z",
      "readme_content": "# Model Context Protocol (MCP) for NASA Earthdata Search (CMR)\n\nThis module is a [model context protocol](https://modelcontextprotocol.io/introduction) (MCP) for NASA's earthdata common metedata repository (CMR). The goal of this MCP server is to integrate AI retrievals with NASA Catalog of datasets by way of Earthaccess.\n\n## Dependencies\nuv -  a rust based python package manager\na LLM client, such as Claude desktop or chatGPT desktop (for consuming the MCP)\n\n## Install and Run\n\nClone the repository to your local environment, or where your LLM client is running.\n\n```\ngit clone https://github.com/podaac/cmr-mcp.git\ncd cmr-mcp\n```\n\n\n### Install uv \n\n```\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n```\n\n\n```\nuv venv\nsource .venv/bin/activate\n```\n\n###  Install packages with uv\n```\nuv sync\n```\n\nuse the outputs of `which uv` (UV_LIB) and `PWD` (CMR_MCP_INSTALL) to update the following configuration.\n\n\n## Adding to AI Framework\n\nIn this example we'll use Claude desktop.\n\nUpdate the `claude_desktop_config.json` file (sometimes this must be created). On a mac, this is often found in `~/Library/Application\\ Support/Claude/claude_desktop_config.json`\n\nAdd the following configuration, filling in the values of UV_LIB and CMR_MCP_INSTALL - don't use environment variables here.\n\n```\n{\n    \"mcpServers\": {\n        \"cmr\": {\n            \"command\": \"$UV_LIB$\",\n            \"args\": [\n                \"--directory\",\n                \"$CMR_MCP_INSTALL$\",\n                \"run\",\n                \"cmr-search.py\"\n            ]\n        }\n    }\n}\n```\n\n## Use the MCP Server\n\nSimply prompt your agent to `search cmr for...` data. Below is a simple example of this in action.\n\n\n\nOther prompts that can work:\n\n1. Search CMR for datasets from 2024 to 2025\n2. Search CMR for PO.DAAC datasets from 2020 to 2024 with keyword Climate",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "earthdata",
        "metadata",
        "nasa",
        "searches earthdata",
        "earthdata metadata",
        "metadata repository"
      ],
      "category": "web-search"
    },
    "pragmar--mcp-server-webcrawl": {
      "owner": "pragmar",
      "name": "mcp-server-webcrawl",
      "url": "https://github.com/pragmar/mcp-server-webcrawl",
      "imageUrl": "/freedevtools/mcp/pfp/pragmar.webp",
      "description": "Integrate web crawl data with AI language models for content filtering and analysis. Offers a full-text search interface and supports multiple crawlers for enhanced data insights.",
      "stars": 26,
      "forks": 6,
      "license": "Other",
      "language": "HTML",
      "updated_at": "2025-09-24T20:10:05Z",
      "readme_content": "<p align=\"center\">\r\n  \r\n</p>\r\n\r\n<p align=\"center\">\r\n  <a href=\"https://pragmar.com/mcp-server-webcrawl/\" style=\"margin: 0 10px;\">Website</a> |\r\n  <a href=\"https://github.com/pragmar/mcp-server-webcrawl\" style=\"margin: 0 10px;\">GitHub</a> |\r\n  <a href=\"https://pragmar.github.io/mcp-server-webcrawl/\" style=\"margin: 0 10px;\">Docs</a> |\r\n  <a href=\"https://pypi.org/project/mcp-server-webcrawl/\" style=\"margin: 0 10px;\">PyPi</a>\r\n</p>\r\n\r\n# mcp-server-webcrawl\r\n\r\nAdvanced search and retrieval for web crawler data. With **mcp-server-webcrawl**, your AI client filters and analyzes web content under your direction or autonomously. The server includes a fulltext search interface with boolean support, and resource filtering by type, HTTP status, and more.\r\n\r\n**mcp-server-webcrawl** provides the LLM a complete menu with which to search, and works with a variety of web crawlers:\r\n\r\n| Crawler/Format | Description | Platforms | Setup Guide |\r\n|---|---|---|---|\r\n| [**ArchiveBox**][1] | Web archiving tool | macOS/Linux | [Setup Guide][8] |\r\n| [**HTTrack**][2] | GUI mirroring tool | macOS/Windows/Linux | [Setup Guide][9] |\r\n| [**InterroBot**][3] | GUI crawler and analyzer | macOS/Windows/Linux | [Setup Guide][10] |\r\n| [**Katana**][4] | CLI security-focused crawler | macOS/Windows/Linux | [Setup Guide][11] |\r\n| [**SiteOne**][5] | GUI crawler and analyzer | macOS/Windows/Linux | [Setup Guide][12] |\r\n| [**WARC**][6] | Standard web archive format | varies by client | [Setup Guide][13] |\r\n| [**wget**][7] | CLI website mirroring tool | macOS/Linux | [Setup Guide][14] |\r\n\r\n[1]: https://archivebox.io\r\n[2]: https://github.com/xroche/httrack\r\n[3]: https://interro.bot\r\n[4]: https://github.com/projectdiscovery/katana\r\n[5]: https://crawler.siteone.io\r\n[6]: https://en.wikipedia.org/wiki/WARC_(file_format)\r\n[7]: https://en.wikipedia.org/wiki/Wget\r\n[8]: https://pragmar.github.io/mcp-server-webcrawl/guides/archivebox.html\r\n[9]: https://pragmar.github.io/mcp-server-webcrawl/guides/httrack.html\r\n[10]: https://pragmar.github.io/mcp-server-webcrawl/guides/interrobot.html\r\n[11]: https://pragmar.github.io/mcp-server-webcrawl/guides/katana.html\r\n[12]: https://pragmar.github.io/mcp-server-webcrawl/guides/siteone.html\r\n[13]: https://pragmar.github.io/mcp-server-webcrawl/guides/warc.html\r\n[14]: https://pragmar.github.io/mcp-server-webcrawl/guides/wget.html\r\n\r\n**mcp-server-webcrawl** is free and open source, and requires Claude Desktop and Python (>=3.10). It is installed on the command line, via pip install:\r\n\r\n```bash\r\npip install mcp-server-webcrawl\r\n```\r\n\r\nFor step-by-step MCP server setup, refer to the [Setup Guides](https://pragmar.github.io/mcp-server-webcrawl/guides.html).\r\n\r\n## Features\r\n\r\n* Claude Desktop ready\r\n* Multi-crawler compatible\r\n* Filter by type, status, and more\r\n* Boolean search support\r\n* Support for Markdown and snippets\r\n* Roll your own website knowledgebase\r\n\r\n## Prompt Routines\r\n\r\n**mcp-server-webcrawl** provides the toolkit necessary to search web crawl data freestyle, figuring it out as you go, reacting to each query. This is what it was designed for.\r\n\r\nIt is also capable of running routines (as prompts). You can write these yourself, or use the ones provided. These prompts are **copy and paste**, and used as raw Markdown. They are enabled by the advanced search provided to the LLM; queries and logic can be embedded in a procedural set of instructions, or even an input loop as is the case with Gopher Service.\r\n\r\n| Prompt | Download | Category | Description |\r\n|--------|----------|----------|-------------|\r\n|🔍 **SEO Audit** | [`auditseo.md`](https://raw.githubusercontent.com/pragmar/mcp-server-webcrawl/master/prompts/auditseo.md) | audit | Technical SEO (search engine optimization) analysis. Covers the basics, with options to dive deeper. |\r\n|🔗 **404 Audit** | [`audit404.md`](https://raw.githubusercontent.com/pragmar/mcp-server-webcrawl/master/prompts/audit404.md) | audit | Broken link detection and pattern analysis. Not only finds issues, but suggests fixes. |\r\n|⚡&nbsp;**Performance&nbsp;Audit** | [`auditperf.md`](https://raw.githubusercontent.com/pragmar/mcp-server-webcrawl/master/prompts/auditperf.md) | audit | Website speed and optimization analysis. Real talk. |\r\n|📁 **File Audit** | [`auditfiles.md`](https://raw.githubusercontent.com/pragmar/mcp-server-webcrawl/master/prompts/auditfiles.md) | audit | File organization and asset analysis. Discover the composition of your website. |\r\n|🌐 **Gopher Interface** | [`gopher.md`](https://raw.githubusercontent.com/pragmar/mcp-server-webcrawl/master/prompts/gopher.md) | interface | An old-fashioned search interface inspired by the Gopher clients of yesteryear. |\r\n|⚙️ **Search Test** | [`testsearch.md`](https://raw.githubusercontent.com/pragmar/mcp-server-webcrawl/master/prompts/testsearch.md) | self-test | A battery of tests to check for Boolean logical inconsistencies in the search query parser and subsequent FTS5 conversion. |\r\n\r\nIf you want to shortcut the site selection (one less query), paste the markdown and in the same request, type \"run pasted for [site name or URL].\" It will figure it out. When pasted without additional context, you should be prompted to select from a list of crawled sites.\r\n\r\n## Boolean Search Syntax\r\n\r\nThe query engine supports field-specific (`field: value`) searches and complex boolean expressions. Fulltext is supported as a combination of the url, content, and headers fields.\r\n\r\nWhile the API interface is designed to be consumed by the LLM directly, it can be helpful to familiarize yourself with the search syntax. Searches generated by the LLM are inspectable, but generally collapsed in the UI. If you need to see the query, expand the MCP collapsible.\r\n\r\n**Example Queries**\r\n\r\n| Query Example | Description |\r\n|--------------|-------------|\r\n| privacy | fulltext single keyword match |\r\n| \"privacy policy\" | fulltext match exact phrase |\r\n| boundar* | fulltext wildcard matches results starting with *boundar* (boundary, boundaries) |\r\n| id: 12345 | id field matches a specific resource by ID |\r\n| url: example.com/somedir | url field matches results with URL containing example.com/somedir |\r\n| type: html | type field matches for HTML pages only |\r\n| status: 200 | status field matches specific HTTP status codes (equal to 200) |\r\n| status: >=400 | status field matches specific HTTP status code (greater than or equal to 400) |\r\n| content: h1 | content field matches content (HTTP response body, often, but not always HTML) |\r\n| headers: text/xml | headers field matches HTTP response headers |\r\n| privacy AND policy | fulltext matches both |\r\n| privacy OR policy | fulltext matches either |\r\n| policy NOT privacy | fulltext matches policies not containing privacy |\r\n| (login OR signin) AND form | fulltext matches fulltext login or signin with form |\r\n| type: html AND status: 200 | fulltext matches only HTML pages with HTTP success |\r\n\r\n## Field Search Definitions\r\n\r\nField search provides search precision, allowing you to specify which columns of the search index to filter. Rather than searching the entire content, you can restrict your query to specific attributes like URLs, headers, or content body. This approach improves efficiency when looking for specific attributes or patterns within crawl data.\r\n\r\n| Field | Description |\r\n|-------|-------------|\r\n| id | database ID |\r\n| url | resource URL |\r\n| type | enumerated list of types (see types table) |\r\n| size | file size in bytes |\r\n| status | HTTP response codes |\r\n| headers | HTTP response headers |\r\n| content | HTTP body—HTML, CSS, JS, and more |\r\n\r\n## Field Content\r\n\r\nA subset of fields can be independently requested with results, while core fields are always on. Use of headers and content can consume tokens quickly. Use judiciously, or use extras to crunch more results into the context window. Fields are a top level argument, independent of any field searching taking place in the query.\r\n\r\n| Field | Description |\r\n|-------|-------------|\r\n| id | always available |\r\n| url | always available |\r\n| type | always available |\r\n| status | always available |\r\n| created | on request |\r\n| modified | on request |\r\n| size | on request |\r\n| headers | on request |\r\n| content | on request |\r\n\r\n## Content Types\r\n\r\nCrawls contain resource types beyond HTML pages. The `type:` field search allows filtering by broad content type groups, particularly useful when filtering images without complex extension queries. For example, you might search for `type: html NOT content: login` to find pages without \"login,\" or `type: img` to analyze image resources. The table below lists all supported content types in the search system.\r\n\r\n| Type | Description |\r\n|------|-------------|\r\n| html | webpages |\r\n| iframe | iframes |\r\n| img | web images |\r\n| audio | web audio files |\r\n| video | web video files |\r\n| font | web font files |\r\n| style | CSS stylesheets |\r\n| script | JavaScript files |\r\n| rss | RSS syndication feeds |\r\n| text | plain text content |\r\n| pdf | PDF files |\r\n| doc | MS Word documents |\r\n| other | uncategorized |\r\n\r\n## Extras\r\n\r\nThe `extras` parameter provides additional processing options, transforming HTTP data (markdown, snippets, regex, xpath), or connecting the LLM to external data (thumbnails). These options can be combined as needed to achieve the desired result format.\r\n\r\n| Extra | Description |\r\n|-------|-------------|\r\n| thumbnails | Generates base64 encoded images to be viewed and analyzed by AI models. Enables image description, content analysis, and visual understanding while keeping token output minimal. Works with images, which can be filtered using `type: img` in queries. SVG is not supported. |\r\n| markdown | Provides the HTML content field as concise Markdown, reducing token usage and improving readability for LLMs. Works with HTML, which can be filtered using `type: html` in queries. |\r\n| regex | Extracts regular expression matches from crawled files such as HTML, CSS, JavaScript, etc. Not as precise a tool as XPath for HTML, but supports any text file as a data source. One or more regex patterns can be requested, using the `extrasRegex` argument. |\r\n| snippets | Matches fulltext queries to contextual keyword usage within the content. When used without requesting the content field (or markdown extra), it can provide an efficient means of refining a search without pulling down the complete page contents. Also great for rendering old school hit-highlighted results as a list, like Google search in 1999. Works with HTML, CSS, JS, or any text-based, crawled file. |\r\n| xpath | Extracts XPath selector data, used in scraping HTML content. Use XPath's text() selector for text-only, element selectors return outerHTML. Only supported with `type: html`, other types will be ignored. One or more XPath selectors (//h1, count(//h1), etc.) can be requested, using the `extrasXpath` argument. |\r\n\r\nExtras provide a means of producing token-efficient HTTP content responses. Markdown produces roughly 1/3 the bytes of the source HTML, snippets are generally 500 or so bytes per result, and XPath can be as specific or broad as you choose. The more focused your requests, the more results you can fit into your LLM session.\r\n\r\nThe idea, of course, is that the LLM takes care of this for you. If you notice your LLM developing an affinity to the \"content\" field (full HTML), a nudge in chat to budget tokens using the extras feature should be all that is needed.\r\n\r\n## Interactive Mode\r\n\r\n**No AI, just classic Boolean search of your web-archives in a terminal.**\r\n\r\nmcp-server-webcrawl can double as a terminal search for your web archives. You can run it against your local archives, but it gets more interesting when you realize you can ssh into any remote host and view archives sitting on that host. No downloads, syncs, multifactor logins, or other common drudgery required. With interactive mode, you can be in and searching a crawl sitting on a remote server in no time at all.\r\n\r\nLaunch with --crawler and --datasource to load into search immediately, or use setup datasrc and crawler in-app.\r\n\r\n```bash\r\nmcp-server-webcrawl --crawler wget --datasrc /path/to/datasrc --interactive\r\n# or manually enter crawler and datasrc in the UI\r\nmcp-server-webcrawl --interactive\r\n```\r\n\r\nInteractive mode is a way to search through tranches of crawled data, whenever, whereever... in a terminal.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "webcrawl",
        "search",
        "crawl",
        "web crawl",
        "crawl data",
        "web search"
      ],
      "category": "web-search"
    },
    "prashalruchiranga--arxiv-mcp-server": {
      "owner": "prashalruchiranga",
      "name": "arxiv-mcp-server",
      "url": "https://github.com/prashalruchiranga/arxiv-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/prashalruchiranga.webp",
      "description": "Interact with the arXiv API to retrieve scholarly article metadata, download PDFs, and search for articles using natural language queries. Enhance research workflows by directly accessing arXiv content within a large language model environment.",
      "stars": 31,
      "forks": 6,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-10-03T07:45:53Z",
      "readme_content": "# arXiv MCP Server\n\n[![MCP Compatible](https://img.shields.io/badge/MCP-Compatible-purple.svg)](https://modelcontextprotocol.io)\n[![Python](https://img.shields.io/badge/python-3.13+-blue.svg)](https://www.python.org/downloads/)\n[![License](https://img.shields.io/badge/license-MIT-blue.svg)](https://opensource.org/licenses/MIT)\n[![smithery badge](https://smithery.ai/badge/@prashalruchiranga/arxiv-mcp-server)](https://smithery.ai/server/@prashalruchiranga/arxiv-mcp-server)\n\nA Model Context Protocol (MCP) server that enables interacting with the arXiv API using natural language.\n\n## Features\n- Retrieve metadata about scholarly articles hosted on arXiv.org\n- Download articles in PDF format to the local machine\n- Search arXiv database for a particular query\n- Retrieve articles and load them into a large language model (LLM) context\n\n## Tools\n- **get_article_url**\n    - Retrieve the URL of an article hosted on arXiv.org based on its title\n        - `title` (String): Article title\n- **download_article**\n    - Download the article hosted on arXiv.org as a PDF file \n        - `title` (String): Article title\n- **load_article_to_context**\n    - Load the article hosted on arXiv.org into context of a LLM \n        - `title` (String): Article title\n- **get_details**\n    - Retrieve metadata of an article hosted on arXiv.org based on its title\n        - `title` (String): Article title\n- **search_arxiv**\n    - Performs a search query on the arXiv API based on specified parameters and returns matching article metadata\n        - `all_fields` (String): General keyword search across all metadata fields\n        - `title` (String): Keyword(s) to search for within the titles of articles\n        - `author` (String): Author name(s) to filter results by\n        - `abstract` (String): Keyword(s) to search for within article abstracts\n        - `start` (int): Index of the first result to return\n\n## Setup\n\n### MacOS\n\nClone the repository\n```\ngit clone https://github.com/prashalruchiranga/arxiv-mcp-server.git\ncd arxiv-mcp-server\n```\nInstall `uv` package manager. For more details on installing, visit the [official uv documentation](https://docs.astral.sh/uv/getting-started/installation/).\n```\n# Using Homebrew\nbrew install uv\n\n# or\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n```\n\nCreate and activate virtual environment.\n```\nuv venv --python=python3.13\nsource .venv/bin/activate\n```\n\nInstall development dependencies.\n```\nuv sync\n```\n\n### Windows\n\nInstall `uv` package manager. For more details on installing, visit the [official uv documentation](https://docs.astral.sh/uv/getting-started/installation/).\n```\n# Use irm to download the script and execute it with iex\npowershell -ExecutionPolicy ByPass -c \"irm https://astral.sh/uv/install.ps1 | iex\"\n```\nClose and reopen the shell, then clone the repository.\n```\ngit clone https://github.com/prashalruchiranga/arxiv-mcp-server.git\ncd arxiv-mcp-server\n```\n\nCreate and activate virtual environment.\n```\nuv venv --python=python3.13\nsource .venv\\Scripts\\activate\n```\n\nInstall development dependencies.\n```\nuv sync\n```\n\n## Usage with Claude Desktop\nTo enable this integration, add the server configuration to your `claude_desktop_config.json` file. Make sure to create the file if it doesn’t exist.\n\nOn MacOS: `~/Library/Application Support/Claude/claude_desktop_config.json` On Windows: `%APPDATA%/Roaming/Claude/claude_desktop_config.json`\n\n```\n{\n  \"mcpServers\": {\n    \"arxiv-server\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"--directory\",\n        \"/ABSOLUTE/PATH/TO/PARENT/FOLDER/arxiv-mcp-server/src/arxiv_server\",\n        \"run\",\n        \"server.py\"\n      ],\n      \"env\": {\n        \"DOWNLOAD_PATH\": \"/ABSOLUTE/PATH/TO/DOWNLOADS/FOLDER\"\n      }\n    }\n  }\n}\n```\n\nYou may need to put the full path to the uv executable in the command field. You can get this by running `which uv` on MacOS or `where uv` on Windows.\n\n## Example Prompts\n```\nCan you get the details of 'Reasoning to Learn from Latent Thoughts' paper?\n```\n```\nGet the papers authored or co-authored by Yann Lecun on convolutional neural networks\n```\n```\nDownload the attention is all you need paper\n```\n```\nCan you get the papers by Andrew NG which have 'convolutional neural networks' in title?\n```\n```\nCan you display the paper?\n```\n```\nList the titles of papers by Yann LeCun. Paginate through the API until there are 30 titles\n```\n\n## License\n\nLicensed under MIT. See the [LICENSE](https://github.com/prashalruchiranga/arxiv-mcp-server/blob/main/LICENSE).\n",
      "npm_url": "https://www.npmjs.com/package/arxiv-mcp-server",
      "npm_downloads": 800,
      "keywords": [
        "scholarly",
        "search",
        "arxiv",
        "retrieve scholarly",
        "search articles",
        "arxiv content"
      ],
      "category": "web-search"
    },
    "prathammanocha--wordpress-mcp-server": {
      "owner": "prathammanocha",
      "name": "wordpress-mcp-server",
      "url": "https://github.com/prathammanocha/wordpress-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/prathammanocha.webp",
      "description": "Enables programmatic management of WordPress sites through the WordPress REST API, facilitating operations on posts, users, comments, categories, tags, and custom endpoints.",
      "stars": 46,
      "forks": 13,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-09-30T06:21:37Z",
      "readme_content": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/prathammanocha-wordpress-mcp-server-badge.png)](https://mseep.ai/app/prathammanocha-wordpress-mcp-server)\n\n# Comprehensive WordPress MCP Server\n\nA comprehensive Model Context Protocol (MCP) server that enables AI assistants to interact with WordPress sites through the WordPress REST API. This server provides tools for managing all aspects of WordPress programmatically, including posts, users, comments, categories, tags, and custom endpoints.\n\n<a href=\"https://glama.ai/mcp/servers/@prathammanocha/wordpress-mcp-server\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@prathammanocha/wordpress-mcp-server/badge\" alt=\"WordPress Server MCP server\" />\n</a>\n\n## Features\n\n### Post Management\n- Create, retrieve, update, and delete WordPress posts\n- Filter posts by various parameters\n- Pagination support for post listings\n\n### User Management\n- Retrieve user information by ID or login\n- Update user details\n- Delete users\n\n### Comments Management\n- Create, retrieve, update, and delete comments\n- Filter comments by post\n- Pagination support for comment listings\n\n### Taxonomy Management\n- Manage categories and tags\n- Create, retrieve, update, and delete taxonomies\n- Find categories and tags by slug\n\n### Site Information\n- Retrieve general WordPress site information\n\n### Custom Requests\n- Support for custom REST API endpoints\n- Custom HTTP methods (GET, POST, PUT, DELETE)\n- Custom data and parameters\n\n## Prerequisites\n\n- Node.js v18 or higher\n- A WordPress site with REST API enabled\n- WordPress application password for authentication\n\n## Installation\n\n1. Clone this repository:\n```bash\ngit clone [repository-url]\ncd wordpress-mcp-server\n```\n\n2. Install dependencies:\n```bash\nnpm install\n```\n\n3. Build the server:\n```bash\nnpm run build\n```\n\n## WordPress Configuration\n\nBefore using the server, you need to set up your WordPress site:\n\n1. Ensure your WordPress site has REST API enabled (enabled by default in WordPress 4.7+)\n2. Create an application password:\n   - Log in to your WordPress admin panel\n   - Go to Users → Profile\n   - Scroll down to \"Application Passwords\"\n   - Enter a name for the application (e.g., \"MCP Server\")\n   - Click \"Add New Application Password\"\n   - Copy the generated password (you won't be able to see it again)\n\n## MCP Configuration\n\nAdd the server to your MCP settings file (usually located at `~/AppData/Roaming/Code/User/globalStorage/saoudrizwan.claude-dev/settings/cline_mcp_settings.json`):\n\n```json\n{\n  \"mcpServers\": {\n    \"wordpress\": {\n      \"command\": \"node\",\n      \"args\": [\"path/to/wordpress-mcp-server/build/index.js\"]\n    }\n  }\n}\n```\n\n## Available Tools\n\n### Post Management\n\n#### 1. create_post\nCreates a new WordPress post.\n\n**Parameters:**\n- `siteUrl` (required): Your WordPress site URL\n- `username` (required): WordPress username\n- `password` (required): WordPress application password\n- `title` (required): Post title\n- `content` (required): Post content\n- `status` (optional): Post status ('draft', 'publish', or 'private', defaults to 'draft')\n\n**Example:**\n```json\n{\n  \"tool\": \"create_post\",\n  \"siteUrl\": \"https://example.com\",\n  \"username\": \"admin\",\n  \"password\": \"xxxx xxxx xxxx xxxx\",\n  \"title\": \"My First Post\",\n  \"content\": \"Hello, world!\",\n  \"status\": \"draft\"\n}\n```\n\n#### 2. get_posts\nRetrieves WordPress posts with pagination.\n\n**Parameters:**\n- `siteUrl` (required): Your WordPress site URL\n- `username` (required): WordPress username\n- `password` (required): WordPress application password\n- `perPage` (optional): Number of posts per page (default: 10)\n- `page` (optional): Page number (default: 1)\n- `customParams` (optional): Additional query parameters\n\n**Example:**\n```json\n{\n  \"tool\": \"get_posts\",\n  \"siteUrl\": \"https://example.com\",\n  \"username\": \"admin\",\n  \"password\": \"xxxx xxxx xxxx xxxx\",\n  \"perPage\": 5,\n  \"page\": 1\n}\n```\n\n#### 3. update_post\nUpdates an existing WordPress post.\n\n**Parameters:**\n- `siteUrl` (required): Your WordPress site URL\n- `username` (required): WordPress username\n- `password` (required): WordPress application password\n- `postId` (required): ID of the post to update\n- `title` (optional): New post title\n- `content` (optional): New post content\n- `status` (optional): New post status ('draft', 'publish', or 'private')\n\n**Example:**\n```json\n{\n  \"tool\": \"update_post\",\n  \"siteUrl\": \"https://example.com\",\n  \"username\": \"admin\",\n  \"password\": \"xxxx xxxx xxxx xxxx\",\n  \"postId\": 123,\n  \"title\": \"Updated Title\",\n  \"content\": \"Updated content\",\n  \"status\": \"publish\"\n}\n```\n\n#### 4. delete_post\nDeletes a WordPress post.\n\n**Parameters:**\n- `siteUrl` (required): Your WordPress site URL\n- `username` (required): WordPress username\n- `password` (required): WordPress application password\n- `postId` (required): ID of the post to delete\n\n**Example:**\n```json\n{\n  \"tool\": \"delete_post\",\n  \"siteUrl\": \"https://example.com\",\n  \"username\": \"admin\",\n  \"password\": \"xxxx xxxx xxxx xxxx\",\n  \"postId\": 123\n}\n```\n\n### User Management\n\n#### 1. get_users\nRetrieves WordPress users.\n\n**Parameters:**\n- `siteUrl` (required): Your WordPress site URL\n- `username` (required): WordPress username\n- `password` (required): WordPress application password\n- `perPage` (optional): Number of users per page (default: 10)\n- `page` (optional): Page number (default: 1)\n\n#### 2. get_user\nRetrieves a specific WordPress user by ID.\n\n**Parameters:**\n- `siteUrl` (required): Your WordPress site URL\n- `username` (required): WordPress username\n- `password` (required): WordPress application password\n- `userId` (required): ID of the user to retrieve\n\n#### 3. get_user_by_login\nRetrieves a WordPress user by login name.\n\n**Parameters:**\n- `siteUrl` (required): Your WordPress site URL\n- `username` (required): WordPress username\n- `password` (required): WordPress application password\n- `userLogin` (required): Login name of the user to retrieve\n\n### Comment Management\n\n#### 1. get_comments\nRetrieves WordPress comments.\n\n**Parameters:**\n- `siteUrl` (required): Your WordPress site URL\n- `username` (required): WordPress username\n- `password` (required): WordPress application password\n- `perPage` (optional): Number of comments per page (default: 10)\n- `page` (optional): Page number (default: 1)\n- `postIdForComment` (optional): Filter comments by post ID\n\n#### 2. create_comment\nCreates a new comment on a post.\n\n**Parameters:**\n- `siteUrl` (required): Your WordPress site URL\n- `username` (required): WordPress username\n- `password` (required): WordPress application password\n- `postIdForComment` (required): ID of the post to comment on\n- `commentContent` (required): Content of the comment\n- `customData` (optional): Additional comment data\n\n### Category and Tag Management\n\n#### 1. get_categories\nRetrieves WordPress categories.\n\n**Parameters:**\n- `siteUrl` (required): Your WordPress site URL\n- `username` (required): WordPress username\n- `password` (required): WordPress application password\n- `perPage` (optional): Number of categories per page (default: 10)\n- `page` (optional): Page number (default: 1)\n\n#### 2. create_category\nCreates a new WordPress category.\n\n**Parameters:**\n- `siteUrl` (required): Your WordPress site URL\n- `username` (required): WordPress username\n- `password` (required): WordPress application password\n- `categoryName` (required): Name of the category to create\n- `customData` (optional): Additional category data (description, parent, etc.)\n\n### Custom Requests\n\n#### 1. custom_request\nMakes a custom request to any WordPress REST API endpoint.\n\n**Parameters:**\n- `siteUrl` (required): Your WordPress site URL\n- `username` (required): WordPress username\n- `password` (required): WordPress application password\n- `customEndpoint` (required): API endpoint path\n- `customMethod` (optional): HTTP method ('GET', 'POST', 'PUT', 'DELETE', default: 'GET')\n- `customData` (optional): Data for POST/PUT requests\n- `customParams` (optional): URL parameters for GET requests\n\n**Example:**\n```json\n{\n  \"tool\": \"custom_request\",\n  \"siteUrl\": \"https://example.com\",\n  \"username\": \"admin\",\n  \"password\": \"xxxx xxxx xxxx xxxx\",\n  \"customEndpoint\": \"wp/v2/media\",\n  \"customMethod\": \"GET\",\n  \"customParams\": {\n    \"per_page\": 5\n  }\n}\n```\n\n## Response Format\n\nAll tools return responses in the following format:\n\n### Success Response\n```json\n{\n  \"success\": true,\n  \"data\": {\n    // WordPress API response data\n  },\n  \"meta\": {\n    // Optional metadata (pagination info, etc.)\n  }\n}\n```\n\n### Error Response\n```json\n{\n  \"success\": false,\n  \"error\": \"Error message here\"\n}\n```\n\n## Security Considerations\n\n- Always use HTTPS URLs for your WordPress site\n- Use application passwords instead of your main WordPress password\n- Keep your application passwords secure and don't share them\n- Consider using WordPress roles and capabilities to limit access\n- Regularly rotate application passwords\n\n## Development\n\nTo contribute to the development:\n\n1. Fork the repository\n2. Create a feature branch\n3. Make your changes\n4. Run tests (when available)\n5. Submit a pull request\n\nFor development mode with automatic recompilation:\n```bash\nnpm run dev\n```\n\n## License\n\nThis project is licensed under the ISC License.\n\n## Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.",
      "npm_url": "https://www.npmjs.com/package/wordpress-mcp-server",
      "npm_downloads": 78,
      "keywords": [
        "wordpress",
        "api",
        "web",
        "wordpress mcp",
        "prathammanocha wordpress",
        "management wordpress"
      ],
      "category": "web-search"
    },
    "privilegemendes--amadeus-mcp-server-standalone": {
      "owner": "privilegemendes",
      "name": "amadeus-mcp-server-standalone",
      "url": "https://github.com/privilegemendes/amadeus-mcp-server-standalone",
      "imageUrl": "/freedevtools/mcp/pfp/privilegemendes.webp",
      "description": "Connect to the Amadeus API for comprehensive flight search, booking, and price analysis. Access information on flights, airports, and economical travel dates to enhance travel planning.",
      "stars": 4,
      "forks": 7,
      "license": "No License",
      "language": "TypeScript",
      "updated_at": "2025-09-16T07:06:31Z",
      "readme_content": "# Amadeus MCP Server\n\nThis is a Model Context Protocol (MCP) server that connects to the Amadeus API to provide flight search, booking, and analysis capabilities for AI assistants.\n\n## Features\n\n- **Flight Search**: Find flights between airports with various parameters\n- **Airport Information**: Search for airports by keyword, city, or country\n- **Price Analysis**: Get price metrics for routes to determine if current prices are high or low\n- **Cheapest Dates**: Find the most economical dates to travel\n- **Flight Details**: Get detailed information about specific flight offers\n\n## Prompts\n\nThe server provides several pre-configured prompts for common travel planning scenarios:\n\n1. **Analyze Flight Prices** (`analyze-flight-prices`): Analyze flight prices for a route with insights on pricing trends\n2. **Find Best Deals** (`find-best-deals`): Find the best flight deals for a specific route and date\n3. **Plan Multi-City Trip** (`plan-multi-city-trip`): Plan a complete multi-city itinerary with optimal routing\n4. **Find Cheapest Travel Dates** (`find-cheapest-travel-dates`): Identify the most economical dates to travel\n\n## Setup\n\n### Prerequisites\n\n- Node.js 16.x or higher\n- Amadeus API credentials (Client ID and Secret)\n\n### Installation\n\n1. Clone the repository:\n```\ngit clone https://github.com/yourusername/amadeus-mcp-server.git\ncd amadeus-mcp-server\n```\n\n2. Install dependencies:\n```\nnpm install\n```\n\n3. Create a `.env` file in the root directory with your Amadeus API credentials:\n```\nAMADEUS_CLIENT_ID=your_client_id\nAMADEUS_CLIENT_SECRET=your_client_secret\n```\n\n### Running the Server\n\nBuild and start the server:\n```\nnpm run build\nnpm start\n```\n\nFor development:\n```\nnpm run dev\n```\n\n### Testing and Development\n\nThis project uses Jest for testing and Biome for linting and formatting.\n\nRun unit tests:\n```\nnpx jest\n```\n\nRun tests with watch mode:\n```\nnpx jest --watch\n```\n\nRun tests with coverage:\n```\nnpx jest --coverage\n```\n\nRun integration tests (requires Amadeus API credentials):\n```\nnpm run test:integration\n```\n\nRun linting:\n```\nnpm run lint\n```\n\nFormat code:\n```\nnpm run format\n```\n\n## Integration Testing\n\nThe project includes comprehensive integration tests that verify the server's interaction with the real Amadeus API. These tests help ensure that our API clients work correctly with the actual API endpoints and handle responses appropriately.\n\n### Requirements for Integration Tests\n\n- **Amadeus API Credentials**: Tests require valid Amadeus API credentials in the `.env` file:\n  ```\n  AMADEUS_CLIENT_ID=your_client_id\n  AMADEUS_CLIENT_SECRET=your_client_secret\n  ```\n\n- **Test Environment**: Tests are configured to use the Amadeus Test Environment, not the production API.\n\n### Running Integration Tests\n\n```\nnpm run test:integration\n```\n\nThe integration tests are located in `__tests__/integration` and validate the following API features:\n\n- **Airport Search**: Searching for airports by code or keyword\n- **Flight Search**: Finding flights for one-way and round-trip journeys\n- **Price Analysis**: Getting price metrics for specific routes\n\n### Best Practices for Integration Testing\n\n1. **API Rate Limits**: The tests include automatic rate limit handling with exponential backoff to avoid API throttling. When running tests frequently, you may still encounter rate limits.\n\n2. **Conditional Testing**: Tests are designed to skip automatically if API credentials are missing, allowing the test suite to run without errors in environments without credentials.\n\n3. **Test in Isolation**: When developing a new feature, you can run specific test files:\n   ```\n   npx jest __tests__/integration/flight-search.test.js\n   ```\n\n4. **Longer Timeouts**: Integration tests use longer timeouts (60 seconds) to accommodate network latency and retries.\n\n5. **Mock for CI/CD**: For continuous integration pipelines where real API access isn't available, use `__tests__/amadeus-mock.test.js` which runs without actual API calls.\n\n## Integration\n\nTo use this MCP server with OpenAI's Assistant API or other compatible AI systems, configure the assistant to connect to this server's endpoint.\n\n## Tools\n\nThe server provides the following tools:\n\n### `search-flights`\nSearch for flight offers between two locations.\n\n### `search-airports`\nSearch for airports by keyword, city name, or IATA code.\n\n### `flight-price-analysis`\nGet price metrics for a flight route to determine if current prices are high or low.\n\n### `get-flight-details`\nGet detailed information about a specific flight offer.\n\n### `find-cheapest-dates`\nFind the cheapest dates to fly for a given route.\n\n## Resources\n\nThe server provides schema resources for:\n\n- Flight offers (`schema://flight-offers`)\n- Airports (`schema://airports`)\n\n## License\n\nMIT ",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "amadeus",
        "api",
        "search",
        "amadeus api",
        "amadeus mcp",
        "connect amadeus"
      ],
      "category": "web-search"
    },
    "priyankark--a11y-mcp": {
      "owner": "priyankark",
      "name": "a11y-mcp",
      "url": "https://github.com/priyankark/a11y-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/priyankark.webp",
      "description": "Perform accessibility audits on webpages and generate detailed reports and summaries that comply with WCAG standards. Integrate the findings into an AI-assisted workflow to fix accessibility issues efficiently.",
      "stars": 27,
      "forks": 7,
      "license": "Mozilla Public License 2.0",
      "language": "JavaScript",
      "updated_at": "2025-09-29T08:04:09Z",
      "readme_content": "# A11y MCP Server\n\nAn MCP (Model Context Protocol) server for performing accessibility audits on webpages using axe-core. Use the results in an agentic loop with your favorite AI assistants (Cline/Cursor/GH Copilot) and let them fix a11y issues for you!\n\n<a href=\"https://glama.ai/mcp/servers/@priyankark/a11y-mcp\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@priyankark/a11y-mcp/badge\" alt=\"a11y-mcp MCP server\" />\n</a>\n\n## Features\n\n- Perform detailed accessibility audits on any webpage\n- Get a summary of accessibility issues\n- Filter audits by specific WCAG criteria\n- Include HTML snippets in the results for easier debugging\n\n## Installation\n\n```bash\n# Install globally\nnpm install -g a11y-mcp\n\n# Or use directly with npx\nnpx a11y-mcp\n```\n\n## Configuration\n\nTo use this MCP server with Cline, you need to add it to your MCP settings configuration file.\n\n### MCP configuration \n\nAdd the following to the `mcpServers` object:\n\n```json\n{\n  \"mcpServers\": {\n    \"a11y\": {\n      \"command\": \"npx\",\n      \"args\": [\"a11y-mcp\"],\n      \"disabled\": false,\n      \"autoApprove\": []\n    }\n  }\n}\n```\n\n## Available Tools\n\n### audit_webpage\n\nPerforms a detailed accessibility audit on a webpage.\n\n**Parameters:**\n- `url` (required): URL of the webpage to audit\n- `includeHtml` (optional): Whether to include HTML snippets in the results (default: false)\n- `tags` (optional): Array of specific accessibility tags to check (e.g., wcag2a, wcag2aa, wcag21a, best-practice)\n\n**Example:**\n```\nUse the a11y MCP server to audit example.com for accessibility issues\n```\n\n### get_summary\n\nGets a summary of accessibility issues for a webpage.\n\n**Parameters:**\n- `url` (required): URL of the webpage to audit\n\n**Example:**\n```\nGive me an accessibility summary of example.com\n```\n\n## Example Usage\n\nOnce configured, you can ask Claude to use the MCP server to perform accessibility audits:\n\n1. \"Can you check example.com for accessibility issues?\"\n2. \"Audit my website at https://mywebsite.com for WCAG 2.1 AA compliance\"\n3. \"Give me a summary of accessibility issues on https://example.com\"\n4. \"Check if my local development server at http://localhost:3000 has any critical accessibility problems\"\n\n## Development\n\nTo run the server locally for development:\n\n```bash\nnpm start\n```\n\n## Releasing\n\nThis project includes a release script to help with versioning and publishing to npm. The script handles version bumping, running tests, git tagging, and npm publishing.\n\nTo release a new version:\n\n```bash\n# Make sure the script is executable\nchmod +x release.sh\n\n# Release a patch version (default)\n./release.sh\n\n# Release a minor version\n./release.sh --minor\n\n# Release a major version\n./release.sh --major\n\n# Release a specific version\n./release.sh --version=1.2.3\n\n# Skip git operations\n./release.sh --skip-git\n\n# Dry run (no changes will be made)\n./release.sh --dry-run\n\n# Force release even with uncommitted changes\n./release.sh --force\n```\n\nFor more information, run:\n\n```bash\n./release.sh --help\n```\n\n## License\nMPL 2.0\n\n## Credits\nThis project builds atop the awesome work done by [axe-core](https://github.com/dequelabs/axe-core)",
      "npm_url": "https://www.npmjs.com/package/a11y-mcp",
      "npm_downloads": 4946,
      "keywords": [
        "accessibility",
        "webpages",
        "search",
        "accessibility audits",
        "perform accessibility",
        "fix accessibility"
      ],
      "category": "web-search"
    },
    "priyankark--lighthouse-mcp": {
      "owner": "priyankark",
      "name": "lighthouse-mcp",
      "url": "https://github.com/priyankark/lighthouse-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/priyankark.webp",
      "description": "Run comprehensive audits on web pages to measure performance metrics, simulate different device environments, and apply network conditions. Provides detailed insights and scores to help optimize website performance.",
      "stars": 45,
      "forks": 8,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-10-01T00:44:25Z",
      "readme_content": "# Lighthouse MCP Server\n\nAn MCP server that wraps around Google's Lighthouse tool to help measure various performance metrics for web pages.\n\n## Features\n\n- Run comprehensive Lighthouse audits on any URL\n- Get performance scores and metrics\n- Configure device emulation (mobile/desktop)\n- Control network throttling\n- Select specific audit categories\n\n## Installation\n\n### Option 1: From MCP Registry (Recommended)\n\nThis server is available in the [Model Context Protocol Registry](https://registry.modelcontextprotocol.io/servers/io.github.priyankark/lighthouse-mcp). Install it using your MCP client or Claude Desktop.\n\n### Option 2: Using npx\n\nYou can run the tool directly using npx without installation:\n\n```bash\nnpx lighthouse-mcp\n```\n\n### Option 3: Global Installation\n\nInstall the package globally from npm:\n\n```bash\nnpm install -g lighthouse-mcp\n```\n\nThen run it:\n\n```bash\nlighthouse-mcp\n```\n\n### Option 4: Local Development\n\n1. Clone this repository\n2. Install dependencies:\n   ```bash\n   npm install\n   ```\n3. Build the project:\n   ```bash\n   npm run build\n   ```\n4. Run the server:\n   ```bash\n   npm start\n   ```\n\n## MCP Configuration\n\n### When installed via npm (global or npx)\n\nAdd the following to your MCP settings configuration file:\n\n```json\n{\n  \"mcpServers\": {\n    \"lighthouse\": {\n      \"command\": \"npx\",\n      \"args\": [\"lighthouse-mcp\"],\n      \"disabled\": false,\n      \"autoApprove\": []\n    }\n  }\n}\n```\n\n### When using local development version\n\nAdd the following to your MCP settings configuration file:\n\n```json\n{\n  \"mcpServers\": {\n    \"lighthouse\": {\n      \"command\": \"node\",\n      \"args\": [\"/absolute/path/to/lighthouse-mcp/build/index.js\"],\n      \"disabled\": false,\n      \"autoApprove\": []\n    }\n  }\n}\n```\n\nReplace `/absolute/path/to/lighthouse-mcp` with the actual path to this project.\n\n## Available Tools\n\n### run_audit\n\nRun a comprehensive Lighthouse audit on a URL.\n\n**Parameters:**\n- `url` (required): The URL to audit\n- `categories` (optional): Array of categories to audit (defaults to all)\n  - Options: \"performance\", \"accessibility\", \"best-practices\", \"seo\", \"pwa\"\n- `device` (optional): Device to emulate (defaults to \"mobile\")\n  - Options: \"mobile\", \"desktop\"\n- `throttling` (optional): Whether to apply network throttling (defaults to true)\n\n**Example:**\n```json\n{\n  \"url\": \"https://example.com\",\n  \"categories\": [\"performance\", \"accessibility\"],\n  \"device\": \"desktop\",\n  \"throttling\": false\n}\n```\n\n### get_performance_score\n\nGet just the performance score for a URL.\n\n**Parameters:**\n- `url` (required): The URL to audit\n- `device` (optional): Device to emulate (defaults to \"mobile\")\n  - Options: \"mobile\", \"desktop\"\n\n**Example:**\n```json\n{\n  \"url\": \"https://example.com\",\n  \"device\": \"mobile\"\n}\n```\n\n## Example Usage\n\nOnce the MCP server is configured, you can use it with Claude:\n\n```\nWhat's the performance score for example.com?\n```\n\nClaude will use the `get_performance_score` tool to analyze the website and return the results.\n\n## Requirements\n\n- Node.js 16+\n- Chrome/Chromium browser (for Lighthouse)\n\n## Endorsements\n<a href=\"https://glama.ai/mcp/servers/@priyankark/lighthouse-mcp\">\n  <img alt=\"badge\" width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@priyankark/lighthouse-mcp/badge\" />\n</a>\n",
      "npm_url": "https://www.npmjs.com/package/lighthouse-mcp",
      "npm_downloads": 6218,
      "keywords": [
        "performance",
        "web",
        "lighthouse",
        "website performance",
        "performance metrics",
        "priyankark lighthouse"
      ],
      "category": "web-search"
    },
    "probelabs--docs-mcp": {
      "owner": "probelabs",
      "name": "docs-mcp",
      "url": "https://github.com/probelabs/docs-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/probelabs.webp",
      "description": "Enables AI assistants to search and interact with documentation or codebases by pointing to a Git repository or local folder, allowing for natural language queries about the contents.",
      "stars": 60,
      "forks": 16,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-09-21T01:52:20Z",
      "readme_content": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/buger-docs-mcp-badge.png)](https://mseep.ai/app/buger-docs-mcp)\n\n# Docs MCP Server\n[![smithery badge](https://smithery.ai/badge/@buger/docs-mcp)](https://smithery.ai/server/@buger/docs-mcp)\n\nThis project provides a flexible Model Context Protocol (MCP) server, powered by [Probe](https://probeai.dev/), designed to make documentation or codebases searchable by AI assistants.\n\n<a href=\"https://glama.ai/mcp/servers/@buger/docs-mcp\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@buger/docs-mcp/badge\" alt=\"Docs Server MCP server\" />\n</a>\n\nYou can chat with code or your docs, simply by pointing to git repo or a folder.\n```\nnpx -y @buger/docs-mcp@latest --gitUrl https://github.com/buger/probe\n```\n\n**Use Cases:**\n\n*   **Chat with any GitHub Repository:** Point the server to a public or private Git repository to enable natural language queries about its contents.\n*   **Search Your Documentation:** Integrate your project's documentation (from a local directory or Git) for easy searching.\n*   **Build Custom MCP Servers:** Use this project as a template to create your own official MCP servers tailored to specific documentation sets or even codebases.\n\nThe content source (documentation or code) can be **pre-built** into the package during the `npm run build` step, or configured **dynamically** at runtime using local directories or Git repositories. By default, when using a `gitUrl` without enabling auto-updates, the server downloads a `.tar.gz` archive for faster startup. Full Git cloning is used only when `autoUpdateInterval` is greater than 0.\n\n## Features\n\n- **Powered by Probe:** Leverages the [Probe](https://probeai.dev/) search engine for efficient and relevant results.\n- **Flexible Content Sources:** Include a specific local directory or clone a Git repository.\n- **Pre-build Content:** Optionally bundle documentation/code content directly into the package.\n- **Dynamic Configuration:** Configure content sources, Git settings, and MCP tool details via config file, CLI arguments, or environment variables.\n- **Automatic Git Updates:** Keep content fresh by automatically pulling changes from a Git repository at a configurable interval.\n- **Customizable MCP Tool:** Define the name and description of the search tool exposed to AI assistants.\n- **AI Integration:** Seamlessly integrates with AI assistants supporting the Model Context Protocol (MCP).\n\n## Usage\n\nThe primary way to use this server is via `npx`, which downloads and runs the package without needing a local installation. This makes it easy to integrate with AI assistants and MCP clients (like IDE extensions).\n\n### Installing via Smithery\n\nTo install Docs MCP Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@buger/docs-mcp):\n\n```bash\nnpx -y @smithery/cli install @buger/docs-mcp --client claude\n```\n\n### Integrating with MCP Clients (e.g., IDEs)\n\nYou can configure your MCP client to launch this server using `npx`. Here are examples of how you might configure a client (syntax may vary based on the specific client):\n\n**Example 1: Dynamically Searching a Git Repository (Tyk Docs)**\n\nThis configuration tells the client to run the latest `@buger/docs-mcp` package using `npx`, pointing it dynamically to the Tyk documentation repository. The `-y` argument automatically confirms the `npx` installation prompt. The `--toolName` and `--toolDescription` arguments customize how the search tool appears to the AI assistant.\n\n```json\n{\n  \"mcpServers\": {\n    \"tyk-docs-search\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"@buger/docs-mcp@latest\",\n        \"--gitUrl\",\n        \"https://github.com/TykTechnologies/tyk-docs\",\n        \"--toolName\",\n        \"search_tyk_docs\",\n        \"--toolDescription\",\n        \"Search Tyk API Management Documentation\"\n      ],\n      \"enabled\": true\n    }\n  }\n}\n```\n\nAlternatively, some clients might allow specifying the full command directly. You could achieve the same as Example 1 using:\n\n```bash\nnpx -y @buger/docs-mcp@latest --gitUrl https://github.com/TykTechnologies/tyk-docs --toolName search_tyk_docs --toolDescription \"Search Tyk API Management Documentation\"\n```\n\n**Example 2: Using a Pre-built, Branded MCP Server (e.g., Tyk Package)**\n\nIf a team publishes a pre-built package containing specific documentation (like `@tyk-technologies/docs-mcp`), the configuration becomes simpler as the content source and tool details are baked into that package. The `-y` argument is still recommended for `npx`.\n\n```json\n{\n  \"mcpServers\": {\n    \"tyk-official-docs\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"@tyk-technologies/docs-mcp@latest\"\n      ],\n      \"enabled\": true\n    }\n  }\n}\n```\n\nThis approach is ideal for distributing standardized search experiences for official documentation or codebases. See the \"Creating Your Own Pre-built MCP Server\" section below.\n\nHere is example on how Tyk team have build own documentation MCP server https://github.com/TykTechnologies/docs-mcp. \n\n## Configuration\n\nCreate a `docs-mcp.config.json` file in the root directory to define the **default** content source and MCP tool details used during the build and at runtime (unless overridden by CLI arguments or environment variables).\n\n### Example 1: Using a Local Directory\n\n```json\n{\n  \"includeDir\": \"/Users/username/projects/my-project/docs\",\n  \"toolName\": \"search_my_project_docs\",\n  \"toolDescription\": \"Search the documentation for My Project.\",\n  \"ignorePatterns\": [\n    \"node_modules\",\n    \".git\",\n    \"build\",\n    \"*.log\"\n  ]\n}\n```\n\n### Example 2: Using a Git Repository\n\n```json\n{\n  \"gitUrl\": \"https://github.com/your-org/your-codebase.git\",\n  \"gitRef\": \"develop\",\n  \"autoUpdateInterval\": 15,\n  \"toolName\": \"search_codebase\",\n  \"toolDescription\": \"Search the main company codebase.\",\n  \"ignorePatterns\": [\n    \"*.test.js\",\n    \"dist/\",\n    \"__snapshots__\"\n  ]\n}\n```\n\n### Configuration Options\n\n- `includeDir`: **(Build/Runtime)** Absolute path to a local directory whose contents will be copied to the `data` directory during build, or used directly at runtime if `dataDir` is not specified. Use this OR `gitUrl`.\n- `gitUrl`: **(Build/Runtime)** URL of the Git repository. Use this OR `includeDir`.\n    - If `autoUpdateInterval` is 0 (default), the server attempts to download a `.tar.gz` archive directly (currently assumes GitHub URL structure: `https://github.com/{owner}/{repo}/archive/{ref}.tar.gz`). This is faster but doesn't support updates.\n    - If `autoUpdateInterval` > 0, the server performs a `git clone` and enables periodic updates.\n- `gitRef`: **(Build/Runtime)** The branch, tag, or commit hash to use from the `gitUrl` (default: `main`). Used for both tarball download and Git clone/pull.\n- `autoUpdateInterval`: **(Runtime)** Interval in minutes to automatically check for Git updates (default: 0, meaning disabled). Setting this to a value > 0 enables Git cloning and periodic `git pull` operations. Requires the `git` command to be available in the system path.\n- `dataDir`: **(Runtime)** Path to the directory containing the content to be searched at runtime. Overrides content sourced from `includeDir` or `gitUrl` defined in the config file or built into the package. Useful for pointing the server to live data without rebuilding.\n- `toolName`: **(Build/Runtime)** The name of the MCP tool exposed by the server (default: `search_docs`). Choose a descriptive name relevant to the content.\n- `toolDescription`: **(Build/Runtime)** The description of the MCP tool shown to AI assistants (default: \"Search documentation using the probe search engine.\").\n- `ignorePatterns`: **(Build/Runtime)** An array of glob patterns.\n- `enableBuildCleanup`: **(Build)** If `true` (default), removes common binary/media files (images, videos, archives, etc.) and files larger than 100KB from the `data` directory after the build step. Set to `false` to disable this cleanup.\n    - If using `includeDir` during build: Files matching these patterns are excluded when copying to `data`. `.gitignore` rules are also respected.\n    - If using `gitUrl` or `dataDir` at runtime: Files matching these patterns within the `data` directory are ignored by the search indexer.\n\n**Precedence:**\n\n1.  **Runtime Configuration (Highest):** CLI arguments (`--dataDir`, `--gitUrl`, etc.) and Environment Variables (`DATA_DIR`, `GIT_URL`, etc.) override all other settings. CLI arguments take precedence over Environment Variables.\n2.  **Build-time Configuration:** Settings in `docs-mcp.config.json` (`includeDir`, `gitUrl`, `toolName`, etc.) define defaults used during `npm run build` and also serve as runtime defaults if not overridden.\n3.  **Default Values (Lowest):** Internal defaults are used if no configuration is provided (e.g., `toolName: 'search_docs'`, `autoUpdateInterval: 5`).\n\nNote: If both `includeDir` and `gitUrl` are provided in the *same* configuration source (e.g., both in the config file, or both as CLI args), `gitUrl` takes precedence.\n\n## Creating Your Own Pre-built MCP Server\n\nYou can use this project as a template to create and publish your own npm package with documentation or code pre-built into it. This provides a zero-configuration experience for users (like Example 2 above).\n\n1.  **Fork/Clone this Repository:** Start with this project's code.\n2.  **Configure `docs-mcp.config.json`:** Define the `includeDir` or `gitUrl` pointing to your content source. Set the default `toolName` and `toolDescription`.\n3.  **Update `package.json`:** Change the `name` (e.g., `@my-org/my-docs-mcp`), `version`, `description`, etc.\n4.  **Build:** Run `npm run build`. This clones/copies your content into the `data` directory and makes the package ready.\n5.  **Publish:** Run `npm publish` (you'll need npm authentication configured).\n\nNow, users can run your specific documentation server easily: `npx @my-org/my-docs-mcp@latest`.\n\n*(The previous \"Running\", \"Dynamic Configuration at Runtime\", and \"Environment Variables\" sections have been removed as `npx` usage with arguments within client configurations is now the primary documented method.)*\n\n## Using with AI Assistants\n\nThis MCP server exposes a search tool to connected AI assistants via the Model Context Protocol. The tool's name and description are configurable (see Configuration section). It searches the content within the currently active `data` directory (determined by build settings, config file, CLI args, or environment variables).\n\n**Tool Parameters:**\n\n- `query`: A natural language query or keywords describing what to search for (e.g., \"how to configure the gateway\", \"database connection example\", \"user authentication\"). The server uses Probe's search capabilities to find relevant content. (Required)\n- `page`: The page number for results when dealing with many matches. Defaults to 1 if omitted. (Optional)\n\n**Example Tool Call (using `search_tyk_docs` from Usage Example 1):**\n\n```json\n{\n  \"tool_name\": \"search_tyk_docs\",\n  \"arguments\": {\n    \"query\": \"gateway rate limiting\",\n    \"page\": 1 // Requesting the first page\n  }\n}\n```\n\n**Example Tool Call (using the tool from the `@tyk/docs-mcp` package):**\n\nAssuming the pre-built package `@tyk/docs-mcp` defined its tool name as `search_tyk_official_docs`:\n\n```json\n{\n  \"tool_name\": \"search_tyk_official_docs\",\n  \"arguments\": {\n    \"query\": \"dashboard api access\",\n    \"page\": 2 // Requesting the second page\n  }\n}\n```\n\n*(The previous \"Publishing as an npm Package\" section has been replaced by the \"Creating Your Own Pre-built MCP Server\" section above.)*\n\n## License\n\nMIT",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "documentation",
        "search",
        "probelabs",
        "search probelabs",
        "documentation codebases",
        "probelabs docs"
      ],
      "category": "web-search"
    },
    "pskill9--hn-server": {
      "owner": "pskill9",
      "name": "hn-server",
      "url": "https://github.com/pskill9/hn-server",
      "imageUrl": "/freedevtools/mcp/pfp/pskill9.webp",
      "description": "Fetch and parse stories from Hacker News, providing structured data on various story types like top, new, ask, show, and jobs.",
      "stars": 35,
      "forks": 8,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-08-10T18:49:11Z",
      "readme_content": "# Hacker News MCP Server\n\nA Model Context Protocol (MCP) server that provides tools for fetching stories from Hacker News. This server parses the HTML content from news.ycombinator.com and provides structured data for different types of stories (top, new, ask, show, jobs).\n\n<a href=\"https://glama.ai/mcp/servers/oge85xl22f\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/oge85xl22f/badge\" alt=\"Hacker News MCP server\" /></a>\n\n## Features\n\n- Fetch different types of stories (top, new, ask, show, jobs)\n- Get structured data including titles, URLs, points, authors, timestamps, and comment counts\n- Configurable limit on number of stories returned\n- Clean error handling and validation\n\n## Installation\n\n1. Clone the repository:\n```bash\ngit clone https://github.com/pskill9/hn-server\ncd hn-server\n```\n\n2. Install dependencies:\n```bash\nnpm install\n```\n\n3. Build the server:\n```bash\nnpm run build\n```\n\n4. Add to your MCP settings configuration file (location depends on your system):\n\nFor VSCode Claude extension:\n```json\n{\n  \"mcpServers\": {\n    \"hacker-news\": {\n      \"command\": \"node\",\n      \"args\": [\"/path/to/hn-server/build/index.js\"]\n    }\n  }\n}\n```\n\n## Usage\n\nThe server provides a tool called `get_stories` that can be used to fetch stories from Hacker News.\n\n### Tool: get_stories\n\nParameters:\n- `type` (string): Type of stories to fetch\n  - Options: 'top', 'new', 'ask', 'show', 'jobs'\n  - Default: 'top'\n- `limit` (number): Number of stories to return\n  - Range: 1-30\n  - Default: 10\n\nExample usage:\n```typescript\nuse_mcp_tool with:\nserver_name: \"hacker-news\"\ntool_name: \"get_stories\"\narguments: {\n  \"type\": \"top\",\n  \"limit\": 5\n}\n```\n\nSample output:\n```json\n[\n  {\n    \"title\": \"Example Story Title\",\n    \"url\": \"https://example.com/story\",\n    \"points\": 100,\n    \"author\": \"username\",\n    \"time\": \"2024-12-28T00:03:05\",\n    \"commentCount\": 50,\n    \"rank\": 1\n  },\n  // ... more stories\n]\n```\n\n## Integrating with Claude\n\nTo use this MCP server with Claude, you'll need to:\n\n1. Have the Claude desktop app or VSCode Claude extension installed\n2. Configure the MCP server in your settings\n3. Use Claude's natural language interface to interact with Hacker News\n\n### Configuration\n\nFor the Claude desktop app, add the server configuration to:\n```json\n// ~/Library/Application Support/Claude/claude_desktop_config.json (macOS)\n// %APPDATA%\\Claude\\claude_desktop_config.json (Windows)\n{\n  \"mcpServers\": {\n    \"hacker-news\": {\n      \"command\": \"node\",\n      \"args\": [\"/path/to/hn-server/build/index.js\"]\n    }\n  }\n}\n```\n\nFor the VSCode Claude extension, add to:\n```json\n// VSCode Settings JSON\n{\n  \"mcpServers\": {\n    \"hacker-news\": {\n      \"command\": \"node\",\n      \"args\": [\"/path/to/hn-server/build/index.js\"]\n    }\n  }\n}\n```\n\n### Example Interactions\n\nOnce configured, you can interact with Claude using natural language to fetch Hacker News stories. Examples:\n\n- \"Show me the top 5 stories from Hacker News\"\n- \"What are the latest Ask HN posts?\"\n- \"Get me the top Show HN submissions from today\"\n\nClaude will automatically use the appropriate parameters to fetch the stories you want.\n\n\n\n### Story Object Structure\n\nEach story object contains:\n- `title` (string): The story title\n- `url` (string, optional): URL of the story (may be internal HN URL for text posts)\n- `points` (number): Number of upvotes\n- `author` (string): Username of the poster\n- `time` (string): Timestamp of when the story was posted\n- `commentCount` (number): Number of comments\n- `rank` (number): Position in the list\n\n## Development\n\nThe server is built using:\n- TypeScript\n- Model Context Protocol SDK\n- Axios for HTTP requests\n- Cheerio for HTML parsing\n\nTo modify the server:\n\n1. Make changes to `src/index.ts`\n2. Rebuild:\n```bash\nnpm run build\n```\n\n## Error Handling\n\nThe server includes robust error handling for:\n- Invalid story types\n- Network failures\n- HTML parsing errors\n- Invalid parameter values\n\nErrors are returned with appropriate error codes and descriptive messages.\n\n## Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n\n## License\n\nMIT License - feel free to use this in your own projects.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "pskill9",
        "hacker",
        "search",
        "search pskill9",
        "hacker news",
        "stories hacker"
      ],
      "category": "web-search"
    },
    "pskill9--web-search": {
      "owner": "pskill9",
      "name": "web-search",
      "url": "https://github.com/pskill9/web-search",
      "imageUrl": "/freedevtools/mcp/pfp/pskill9.webp",
      "description": "Enables free web searching using Google search results without requiring API keys. Returns structured results including titles, URLs, and descriptions, configurable by the number of results returned per search.",
      "stars": 293,
      "forks": 116,
      "license": "No License",
      "language": "JavaScript",
      "updated_at": "2025-10-02T11:28:26Z",
      "readme_content": "# Web Search MCP Server\n\nA Model Context Protocol (MCP) server that enables free web searching using Google search results, with no API keys required.\n\n## Features\n\n- Search the web using Google search results\n- No API keys or authentication required\n- Returns structured results with titles, URLs, and descriptions\n- Configurable number of results per search\n\n## Installation\n\n1. Clone or download this repository\n2. Install dependencies:\n```bash\nnpm install\n```\n3. Build the server:\n```bash\nnpm run build\n```\n4. Add the server to your MCP configuration:\n\nFor VSCode (Claude Dev Extension):\n```json\n{\n  \"mcpServers\": {\n    \"web-search\": {\n      \"command\": \"node\",\n      \"args\": [\"/path/to/web-search/build/index.js\"]\n    }\n  }\n}\n```\n\nFor Claude Desktop:\n```json\n{\n  \"mcpServers\": {\n    \"web-search\": {\n      \"command\": \"node\",\n      \"args\": [\"/path/to/web-search/build/index.js\"]\n    }\n  }\n}\n```\n\n## Usage\n\nThe server provides a single tool named `search` that accepts the following parameters:\n\n```typescript\n{\n  \"query\": string,    // The search query\n  \"limit\": number     // Optional: Number of results to return (default: 5, max: 10)\n}\n```\n\nExample usage:\n```typescript\nuse_mcp_tool({\n  server_name: \"web-search\",\n  tool_name: \"search\",\n  arguments: {\n    query: \"your search query\",\n    limit: 3  // optional\n  }\n})\n```\n\nExample response:\n```json\n[\n  {\n    \"title\": \"Example Search Result\",\n    \"url\": \"https://example.com\",\n    \"description\": \"Description of the search result...\"\n  }\n]\n```\n\n## Limitations\n\nSince this tool uses web scraping of Google search results, there are some important limitations to be aware of:\n\n1. **Rate Limiting**: Google may temporarily block requests if too many searches are performed in a short time. To avoid this:\n   - Keep searches to a reasonable frequency\n   - Use the limit parameter judiciously\n   - Consider implementing delays between searches if needed\n\n2. **Result Accuracy**: \n   - The tool relies on Google's HTML structure, which may change\n   - Some results might be missing descriptions or other metadata\n   - Complex search operators may not work as expected\n\n3. **Legal Considerations**:\n   - This tool is intended for personal use\n   - Respect Google's terms of service\n   - Consider implementing appropriate rate limiting for your use case\n\n## Contributing\n\nFeel free to submit issues and enhancement requests!\n",
      "npm_url": "https://www.npmjs.com/package/web-search",
      "npm_downloads": 629,
      "keywords": [
        "pskill9",
        "search",
        "google",
        "search pskill9",
        "pskill9 web",
        "web search"
      ],
      "category": "web-search"
    },
    "pskill9--website-downloader": {
      "owner": "pskill9",
      "name": "website-downloader",
      "url": "https://github.com/pskill9/website-downloader",
      "imageUrl": "/freedevtools/mcp/pfp/pskill9.webp",
      "description": "Downloads entire websites for offline access, preserving the website structure and converting links for local use.",
      "stars": 138,
      "forks": 26,
      "license": "No License",
      "language": "JavaScript",
      "updated_at": "2025-09-25T02:50:05Z",
      "readme_content": "# Website Downloader MCP Server\n\nThis MCP server provides a tool to download entire websites using wget. It preserves the website structure and converts links to work locally.\n\n<a href=\"https://glama.ai/mcp/servers/egcwr79vu2\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/egcwr79vu2/badge\" alt=\"Google Workspace Server MCP server\" /></a>\n\n## Prerequisites\n\nThe server requires `wget` to be installed on your system.\n\n### Installing wget\n\n#### macOS\nUsing Homebrew:\n```bash\nbrew install wget\n```\n\n#### Linux (Debian/Ubuntu)\n```bash\nsudo apt-get update\nsudo apt-get install wget\n```\n\n#### Linux (Red Hat/Fedora)\n```bash\nsudo dnf install wget\n```\n\n#### Windows\n1. Using [Chocolatey](https://chocolatey.org/):\n```bash\nchoco install wget\n```\n\n2. Or download the binary from: https://eternallybored.org/misc/wget/\n   - Download the latest wget.exe\n   - Place it in a directory that's in your PATH (e.g., C:\\Windows\\System32)\n\n## Usage\n\nThe server provides a tool called `download_website` with the following parameters:\n\n- `url` (required): The URL of the website to download\n- `outputPath` (optional): The directory where the website should be downloaded. Defaults to the current directory.\n- `depth` (optional): Maximum depth level for recursive downloading. Defaults to infinite. Set to 0 for just the specified page, 1 for direct links, etc.\n\n### Example\n\n```json\n{\n  \"url\": \"https://example.com\",\n  \"outputPath\": \"/path/to/output\",\n  \"depth\": 2  // Optional: Download up to 2 levels deep\n}\n```\n\n## Features\n\nThe website downloader:\n- Downloads recursively with infinite depth\n- Includes all page requisites (CSS, images, etc.)\n- Converts links to work locally\n- Adds appropriate extensions to files\n- Restricts downloads to the same domain\n- Preserves the website structure\n\n## Installation\n\n1. Build the server:\n```bash\nnpm install\nnpm run build\n```\n\n2. Add to MCP settings:\n```json\n{\n  \"mcpServers\": {\n    \"website-downloader\": {\n      \"command\": \"node\",\n      \"args\": [\"/path/to/website-downloader/build/index.js\"]\n    }\n  }\n}\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "pskill9",
        "downloader",
        "downloads",
        "pskill9 website",
        "website downloader",
        "search pskill9"
      ],
      "category": "web-search"
    },
    "puremd--puremd-mcp": {
      "owner": "puremd",
      "name": "puremd-mcp",
      "url": "https://github.com/puremd/puremd-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/puremd.webp",
      "description": "Access web content in markdown format by prefixing URLs with `pure.md/`, facilitating seamless retrieval of web pages while avoiding bot detection. It converts various formats like HTML and PDFs into markdown and globally caches responses for efficiency.",
      "stars": 41,
      "forks": 3,
      "license": "No License",
      "language": "JavaScript",
      "updated_at": "2025-09-23T01:32:32Z",
      "readme_content": "# pure.md MCP server\n\n[![smithery badge](https://smithery.ai/badge/@puremd/puremd-mcp)](https://smithery.ai/server/@puremd/puremd-mcp)\n\nWelcome to the Model Context Protocol (MCP) server for [pure.md](https://pure.md).\n\n![pure.md - Markdown delivery network for LLMs](https://pure.md/assets/og.png)\n\n[pure.md](https://pure.md) lets your scripts, APIs, apps, agents, etc reliably access web content in markdown format -- simply prefix any URL with `pure.md/`.\nIt avoids bot detection and renders JavaScript for SPAs, and can convert HTML, PDFs, images, and more into pure markdown. Like a CDN for markdown content, it globally caches responses for future requests to the same resource, relieving stress on origin web servers.\n\n**Without puremd-mcp, local agents may fail to fetch web content.** puremd-mcp teaches MCP clients like Cursor, Windsurf, and Claude Desktop how to adopt the functionality of pure.md, giving them web unblocking and searching capabilities.\n\npuremd-mcp comes with two tools:\n\n- `unblock-url` - Extract markdown from web pages without getting blocked\n- `search-web` - Search the web for a query and concatenate results into markdown\n\nThe [Model Context Protocol](https://modelcontextprotocol.io/introduction), developed by Anthropic, is an open standard that enables AI systems to seamlessly interact with an ecosystem of tooling. With it, MCP clients like Cursor, Windsurf, and Claude Desktop can learn how to use a variety of APIs and other functionality.\n\n## Authentication\n\nGenerating an API key is an optional step that unlocks higher rate limits. If you'd like to use the pure.md MCP server anonymously, simply set your `PUREMD_API_KEY` value to empty string (`\"\"`).\n\n1. Sign up for a new account at [pure.md](https://pure.md) &mdash; it's free to sign up!\n2. In the dashboard, generate a new API token\n3. Copy the token, and use it for the `PUREMD_API_KEY` value in your MCP client's configuration file (see below)\n\n## Client configuration\n\n### Cursor\n\nAdd the following to your `~/.cursor/mcp.json` file:\n\n```json\n{\n  \"mcpServers\": {\n    \"pure.md\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"puremd-mcp\"],\n      \"env\": {\n        \"PUREMD_API_KEY\": \"<TOKEN>\"\n      }\n    }\n  }\n}\n```\n\n### Windsurf\n\nAdd the following to your `./codeium/windsurf/model_config.json` file:\n\n```json\n{\n  \"mcpServers\": {\n    \"pure.md\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"puremd-mcp\"],\n      \"env\": {\n        \"PUREMD_API_KEY\": \"<TOKEN>\"\n      }\n    }\n  }\n}\n```\n\n### Claude Desktop\n\nAdd the following to your `~/Library/Application\\ Support/Claude/claude_desktop_config.json` file:\n\n```json\n{\n  \"mcpServers\": {\n    \"pure.md\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"puremd-mcp\"],\n      \"env\": {\n        \"PUREMD_API_KEY\": \"<TOKEN>\"\n      }\n    }\n  }\n}\n```\n\n### Installing via Smithery\n\nTo install puremd-mcp for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@puremd/puremd-mcp):\n\n```bash\nnpx -y @smithery/cli install @puremd/puremd-mcp --client claude\n```\n",
      "npm_url": "https://www.npmjs.com/package/puremd-mcp",
      "npm_downloads": 4519,
      "keywords": [
        "puremd",
        "markdown",
        "html",
        "search puremd",
        "puremd puremd",
        "puremd mcp"
      ],
      "category": "web-search"
    },
    "qpd-v--mcp-DEEPwebresearch": {
      "owner": "qpd-v",
      "name": "mcp-DEEPwebresearch",
      "url": "https://github.com/qpd-v/mcp-DEEPwebresearch",
      "imageUrl": "/freedevtools/mcp/pfp/qpd-v.webp",
      "description": "Provides real-time web research capabilities by enabling direct extraction of content from webpages. Optimized for performance within MCP timeout limits, facilitating efficient access to web data.",
      "stars": 77,
      "forks": 21,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-10-03T04:12:37Z",
      "readme_content": "# MCP Deep Web Research Server (v0.3.0)\n\n[![Node.js Version](https://img.shields.io/badge/node-%3E%3D18-brightgreen.svg)](https://nodejs.org/)\n[![TypeScript](https://img.shields.io/badge/TypeScript-5.0-blue.svg)](https://www.typescriptlang.org/)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n\nA Model Context Protocol (MCP) server for advanced web research.\n\n<a href=\"https://glama.ai/mcp/servers/5afpizjl6x\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/5afpizjl6x/badge\" alt=\"Web Research Server MCP server\" /></a>\n\n## Latest Changes\n\n- Added visit_page tool for direct webpage content extraction\n- Optimized performance to work within MCP timeout limits\n  * Reduced default maxDepth and maxBranching parameters\n  * Improved page loading efficiency\n  * Added timeout checks throughout the process\n  * Enhanced error handling for timeouts\n\n> This project is a fork of [mcp-webresearch](https://github.com/mzxrai/mcp-webresearch) by [mzxrai](https://github.com/mzxrai), enhanced with additional features for deep web research capabilities. We're grateful to the original creators for their foundational work.\n\nBring real-time info into Claude with intelligent search queuing, enhanced content extraction, and deep research capabilities.\n\n## Features\n\n- Intelligent Search Queue System\n  - Batch search operations with rate limiting\n  - Queue management with progress tracking\n  - Error recovery and automatic retries\n  - Search result deduplication\n\n- Enhanced Content Extraction\n  - TF-IDF based relevance scoring\n  - Keyword proximity analysis\n  - Content section weighting\n  - Readability scoring\n  - Improved HTML structure parsing\n  - Structured data extraction\n  - Better content cleaning and formatting\n\n- Core Features\n  - Google search integration\n  - Webpage content extraction\n  - Research session tracking\n  - Markdown conversion with improved formatting\n\n## Prerequisites\n\n- [Node.js](https://nodejs.org/) >= 18 (includes `npm` and `npx`)\n- [Claude Desktop app](https://claude.ai/download)\n\n## Installation\n\n### Global Installation (Recommended)\n\n```bash\n# Install globally using npm\nnpm install -g mcp-deepwebresearch\n\n# Or using yarn\nyarn global add mcp-deepwebresearch\n\n# Or using pnpm\npnpm add -g mcp-deepwebresearch\n```\n\n### Local Project Installation\n\n```bash\n# Using npm\nnpm install mcp-deepwebresearch\n\n# Using yarn\nyarn add mcp-deepwebresearch\n\n# Using pnpm\npnpm add mcp-deepwebresearch\n```\n\n### Claude Desktop Integration\n\nAfter installing the package, add this entry to your `claude_desktop_config.json`:\n\n#### Windows\n```json\n{\n  \"mcpServers\": {\n    \"deepwebresearch\": {\n      \"command\": \"mcp-deepwebresearch\",\n      \"args\": []\n    }\n  }\n}\n```\nLocation: `%APPDATA%\\Claude\\claude_desktop_config.json`\n\n#### macOS\n```json\n{\n  \"mcpServers\": {\n    \"deepwebresearch\": {\n      \"command\": \"mcp-deepwebresearch\",\n      \"args\": []\n    }\n  }\n}\n```\nLocation: `~/Library/Application Support/Claude/claude_desktop_config.json`\n\nThis config allows Claude Desktop to automatically start the web research MCP server when needed.\n\n### First-time Setup\n\nAfter installation, run this command to install required browser dependencies:\n```bash\nnpx playwright install chromium\n```\n\n## Usage\n\nSimply start a chat with Claude and send a prompt that would benefit from web research. If you'd like a prebuilt prompt customized for deeper web research, you can use the `agentic-research` prompt that we provide through this package. Access that prompt in Claude Desktop by clicking the Paperclip icon in the chat input and then selecting `Choose an integration` → `deepwebresearch` → `agentic-research`.\n\n### Tools\n\n1. `deep_research`\n   - Performs comprehensive research with content analysis\n   - Arguments:\n     ```typescript\n     {\n       topic: string;\n       maxDepth?: number;      // default: 2\n       maxBranching?: number;  // default: 3\n       timeout?: number;       // default: 55000 (55 seconds)\n       minRelevanceScore?: number;  // default: 0.7\n     }\n     ```\n   - Returns:\n     ```typescript\n     {\n       findings: {\n         mainTopics: Array<{name: string, importance: number}>;\n         keyInsights: Array<{text: string, confidence: number}>;\n         sources: Array<{url: string, credibilityScore: number}>;\n       };\n       progress: {\n         completedSteps: number;\n         totalSteps: number;\n         processedUrls: number;\n       };\n       timing: {\n         started: string;\n         completed?: string;\n         duration?: number;\n         operations?: {\n           parallelSearch?: number;\n           deduplication?: number;\n           topResultsProcessing?: number;\n           remainingResultsProcessing?: number;\n           total?: number;\n         };\n       };\n     }\n     ```\n\n2. `parallel_search`\n   - Performs multiple Google searches in parallel with intelligent queuing\n   - Arguments: `{ queries: string[], maxParallel?: number }`\n   - Note: maxParallel is limited to 5 to ensure reliable performance\n\n3. `visit_page`\n   - Visit a webpage and extract its content\n   - Arguments: `{ url: string }`\n   - Returns:\n     ```typescript\n     {\n       url: string;\n       title: string;\n       content: string;  // Markdown formatted content\n     }\n     ```\n\n### Prompts\n\n#### `agentic-research`\nA guided research prompt that helps Claude conduct thorough web research. The prompt instructs Claude to:\n- Start with broad searches to understand the topic landscape\n- Prioritize high-quality, authoritative sources\n- Iteratively refine the research direction based on findings\n- Keep you informed and let you guide the research interactively\n- Always cite sources with URLs\n\n## Configuration Options\n\nThe server can be configured through environment variables:\n\n- `MAX_PARALLEL_SEARCHES`: Maximum number of concurrent searches (default: 5)\n- `SEARCH_DELAY_MS`: Delay between searches in milliseconds (default: 200)\n- `MAX_RETRIES`: Number of retry attempts for failed requests (default: 3)\n- `TIMEOUT_MS`: Request timeout in milliseconds (default: 55000)\n- `LOG_LEVEL`: Logging level (default: 'info')\n\n## Error Handling\n\n### Common Issues\n\n1. Rate Limiting\n   - Symptom: \"Too many requests\" error\n   - Solution: Increase `SEARCH_DELAY_MS` or decrease `MAX_PARALLEL_SEARCHES`\n\n2. Network Timeouts\n   - Symptom: \"Request timed out\" error\n   - Solution: Ensure requests complete within the 60-second MCP timeout\n\n3. Browser Issues\n   - Symptom: \"Browser failed to launch\" error\n   - Solution: Ensure Playwright is properly installed (`npx playwright install`)\n\n### Debugging\n\nThis is beta software. If you run into issues:\n\n1. Check Claude Desktop's MCP logs:\n   ```bash\n   # On macOS\n   tail -n 20 -f ~/Library/Logs/Claude/mcp*.log\n   \n   # On Windows\n   Get-Content -Path \"$env:APPDATA\\Claude\\logs\\mcp*.log\" -Tail 20 -Wait\n   ```\n\n2. Enable debug logging:\n   ```bash\n   export LOG_LEVEL=debug\n   ```\n\n## Development\n\n### Setup\n\n```bash\n# Install dependencies\npnpm install\n\n# Build the project\npnpm build\n\n# Watch for changes\npnpm watch\n\n# Run in development mode\npnpm dev\n```\n\n### Testing\n\n```bash\n# Run all tests\npnpm test\n\n# Run tests in watch mode\npnpm test:watch\n\n# Run tests with coverage\npnpm test:coverage\n```\n\n### Code Quality\n\n```bash\n# Run linter\npnpm lint\n\n# Fix linting issues\npnpm lint:fix\n\n# Type check\npnpm type-check\n```\n\n## Contributing\n\n1. Fork the repository\n2. Create your feature branch (`git checkout -b feature/amazing-feature`)\n3. Commit your changes (`git commit -m 'Add some amazing feature'`)\n4. Push to the branch (`git push origin feature/amazing-feature`)\n5. Open a Pull Request\n\n### Coding Standards\n\n- Follow TypeScript best practices\n- Maintain test coverage above 80%\n- Document new features and APIs\n- Update CHANGELOG.md for significant changes\n- Follow semantic versioning\n\n### Performance Considerations\n\n- Use batch operations where possible\n- Implement proper error handling and retries\n- Consider memory usage with large datasets\n- Cache results when appropriate\n- Use streaming for large content\n\n## Requirements\n\n- Node.js >= 18\n- Playwright (automatically installed as a dependency)\n\n## Verified Platforms\n\n- [x] macOS\n- [x] Windows\n- [ ] Linux\n\n## License\n\nMIT\n\n## Credits\n\nThis project builds upon the excellent work of [mcp-webresearch](https://github.com/mzxrai/mcp-webresearch) by [mzxrai](https://github.com/mzxrai). The original codebase provided the foundation for our enhanced features and capabilities.\n\n## Author\n\n[qpd-v](https://github.com/qpd-v)\n",
      "npm_url": "https://www.npmjs.com/package/mcp-deepwebresearch",
      "npm_downloads": 1114,
      "keywords": [
        "deepwebresearch",
        "webpages",
        "web",
        "mcp deepwebresearch",
        "deepwebresearch provides",
        "web search"
      ],
      "category": "web-search"
    },
    "qwang07--duck-duck-mcp": {
      "owner": "qwang07",
      "name": "duck-duck-mcp",
      "url": "https://github.com/qwang07/duck-duck-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/qwang07.webp",
      "description": "Enables web search using the DuckDuckGo search engine, providing structured search results along with metadata and smart content classification.",
      "stars": 2,
      "forks": 6,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-05-07T07:59:37Z",
      "readme_content": "# Duck Duck MCP\n[![smithery badge](https://smithery.ai/badge/duck-duck-mcp)](https://smithery.ai/server/duck-duck-mcp)\n\n一个基于 DuckDuckGo 搜索引擎的 Model Context Protocol (MCP) 服务器实现。\n\n<a href=\"https://glama.ai/mcp/servers/c0qz8cvfpi\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/c0qz8cvfpi/badge\" alt=\"Duck Duck MCP server\" /></a>\n\n## 功能特性\n\n- 使用 DuckDuckGo 进行网络搜索\n- 支持自定义搜索结果数量（默认 50 条）\n- 支持区域设置（默认 zh-cn）\n- 支持安全搜索级别设置（OFF/MODERATE/STRICT）\n- 提供结构化的搜索结果，包含元数据\n- 智能内容分类（文档/文章/社交媒体）\n- 自动语言检测\n- 主题标签识别\n\n## 安装\n\n### 安装通过 Smithery\n\n通过 [Smithery](https://smithery.ai/server/duck-duck-mcp) 将 DuckDuckGo 搜索自动安装到 Claude 桌面端：\n\n```bash\nnpx -y @smithery/cli install duck-duck-mcp --client claude\n```\n\n不需要安装，可以直接通过 npx 运行：\n```bash\nnpx -y @smithery/cli install duck-duck-mcp --client claude\n```\n\n### 使用方法\n\n这是一个 MCP 服务器实现，主要用于与支持 MCP 协议的 AI 客户端（如 Claude）集成。\n\n### 启动服务器：\n```bash\n# 使用 npx（推荐）\nnpx duck-duck-mcp\n\n# 或者如果已经全局安装\nmcp-server-search\n```\n\n### 搜索参数示例：\n```json\n{\n  \"query\": \"搜索关键词\",\n  \"options\": {\n    \"region\": \"zh-cn\",\n    \"safeSearch\": \"MODERATE\",\n    \"numResults\": 50\n  }\n}\n```\n\n### 返回结果格式：\n```json\n{\n  \"type\": \"search_results\",\n  \"data\": [\n    {\n      \"title\": \"标题\",\n      \"url\": \"网址\",\n      \"description\": \"描述\",\n      \"metadata\": {\n        \"type\": \"article|documentation|social|other\",\n        \"source\": \"域名\"\n      }\n    }\n  ],\n  \"metadata\": {\n    \"query\": \"搜索关键词\",\n    \"timestamp\": \"时间戳\",\n    \"resultCount\": 50,\n    \"searchContext\": {\n      \"region\": \"zh-cn\",\n      \"safeSearch\": \"MODERATE\"\n    },\n    \"queryAnalysis\": {\n      \"language\": \"zh-cn|en\",\n      \"topics\": [\"technology\", \"documentation\"]\n    }\n  }\n}\n```\n\n## License\n\nMIT\n",
      "npm_url": "https://www.npmjs.com/package/duck-duck-mcp",
      "npm_downloads": 3851,
      "keywords": [
        "duckduckgo",
        "search",
        "duck",
        "duckduckgo search",
        "using duckduckgo",
        "search engine"
      ],
      "category": "web-search"
    },
    "r-huijts--opentk-mcp": {
      "owner": "r-huijts",
      "name": "opentk-mcp",
      "url": "https://github.com/r-huijts/opentk-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/r-huijts.webp",
      "description": "Provides access to Dutch parliamentary documents, debates, and member information through a standardized interface for natural language queries, facilitating research and analysis of legislative activities.",
      "stars": 15,
      "forks": 2,
      "license": "MIT License",
      "language": "HTML",
      "updated_at": "2025-09-24T07:23:36Z",
      "readme_content": "# OpenTK Model Context Protocol Server\n\n> **Important Attribution**: This MCP server is built as a wrapper around the excellent [OpenTK project](https://berthub.eu/tkconv/) created by [Bert Hubert](https://berthub.eu/). The OpenTK project provides unprecedented access to Dutch parliamentary data through a user-friendly interface. Learn more about the project in Bert's article: [Welkom bij OpenTK](https://berthub.eu/articles/posts/welkom-bij-opentk/). All credit for the underlying data access and processing goes to Bert Hubert and his contributions to open government data.\n\nA bridge between large language models (LLMs) and Dutch parliamentary data through a standardized interface. This MCP server provides access to Dutch parliamentary documents, debates, and member information from the Tweede Kamer.\n\n<a href=\"https://glama.ai/mcp/servers/@r-huijts/opentk-mcp\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@r-huijts/opentk-mcp/badge\" alt=\"OpenTK Model Context Protocol Server MCP server\" />\n</a>\n\n## Real-World Natural Language Interaction Examples\n\n## Example 1: Comparing Party Positions on AI Policies\nUser Query: \"When comparing the activities of opposition parties PvdA, GroenLinks, and Volt with government party BBB in the Dutch House of Representatives in the field of AI, what are actions they can undertake together in the short term that align with the positions and views they have demonstrated over the past year? Please use sources from OpenTK.\"\n\n## Example 2: Researching Parliamentary Discussions on Climate Policy\nUser Query: \"I'd like to analyze recent parliamentary debates on climate policy and emission reduction targets in the Netherlands. Can you help me identify key discussions and the main positions taken by different parties over the past six months?\"\n\n## Example 3: Information About a Specific MP's Voting Record\nUser Query: \"What is MP Pieter Omtzigt's voting record on healthcare reform legislation, and how does his position differ from other independent members? Has he introduced any motions on this topic?\"\n\n## Example 4: Finding Recent Housing Legislation Developments\nUser Query: \"What are the most significant parliamentary documents and debates about affordable housing legislation from the past year? I'm particularly interested in proposals addressing the rental market crisis.\"\n\n## Example 5: Finding MPs with Specific Committee Memberships\nUser Query: \"Which MPs currently serve on both the Finance Committee and the Economic Affairs Committee? What parties do they represent, and have they recently submitted any joint initiatives?\"\n\n## Example 6: Identifying Upcoming Parliamentary Activities on Digital Security\nUser Query: \"Are there any scheduled committee meetings or debates about cybersecurity and digital infrastructure planned for the next month? Which ministers will be participating and what specific topics will be addressed?\"\n\n## Project Concept\n\nThe OpenTK project is a Model Context Protocol (MCP) server that provides access to Dutch parliamentary data through a standardized interface. It serves as a bridge between large language models (LLMs) and the Dutch Parliament's information systems, allowing AI assistants to search, retrieve, and analyze parliamentary documents, debates, and member information.\n\nThe server uses the `@modelcontextprotocol/sdk` to implement the MCP specification, which enables structured communication between AI models and external data sources. By exposing parliamentary data through well-defined tools and endpoints, OpenTK makes it possible for AI assistants to:\n\n1. Search for parliamentary documents using complex queries\n2. Access information about Members of Parliament\n3. Retrieve official documents in various formats and read the full content of the documents\n4. Analyze parliamentary activities and proceedings\n5. Track legislative cases and government pledges\n\nThe project leverages Bert Hubert's tkconv service as its primary data source, which provides a more accessible API than the official Dutch Parliament APIs.\n\n## Installation\n\n### 1. Quick Start with NPM Package (Recommended)\n\nThe fastest way to get started is using the published npm package:\n\n```bash\nnpx @r-huijts/opentk-mcp\n```\n\n### 2. Using Claude Desktop with NPM Package\n\nUpdate your Claude configuration file (`~/Library/Application Support/Claude/claude_desktop_config.json`):\n\n```json\n{\n  \"mcpServers\": {\n    \"opentk\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"@r-huijts/opentk-mcp\"\n      ]\n    }\n  }\n}\n```\n\n**Alternative configurations:**\n\nFor MultiServerMCPClient (Python):\n```python\nmcp_client = MultiServerMCPClient({\n    \"opentk\": {\n        \"command\": \"npx\",\n        \"args\": [\"-y\", \"@r-huijts/opentk-mcp\"],\n        \"transport\": \"stdio\",\n    }\n})\n```\n\n### 3. From Source (Development)\n\nIf you want to modify the code or contribute to development:\n\n**Clone Repository:**\n```bash\ngit clone https://github.com/r-huijts/opentk-mcp.git\ncd opentk-mcp\n```\n\n**Install Dependencies:**\n```bash\nnpm install\n```\n\n**Build the Project:**\n```bash\nnpm run build\n```\n\n**Start the Server:**\n```bash\nnpm start\n```\n\n**Configure Claude Desktop for local development:**\n\nUpdate your Claude configuration file:\n\n```json\n{\n  \"mcpServers\": {\n    \"opentk-local\": {\n      \"command\": \"node\",\n      \"args\": [\n        \"/absolute/path/to/your/opentk-mcp/dist/index.js\"\n      ]\n    }\n  }\n}\n```\n\nMake sure to replace `/absolute/path/to/your/opentk-mcp/` with the actual path to your installation.\n\n### 4. Publishing (for maintainers)\n\nTo publish a new version of the scoped package:\n\n```bash\nnpm run build\nnpm publish --access=public\n```\n\nNote: Scoped packages require the `--access=public` flag to be publicly available.\n\n## Search Functionality\n\nThe search functionality is particularly sophisticated, supporting:\n\n- Simple keyword searches: `kunstmatige intelligentie`\n- Exact phrase searches: `\"kunstmatige intelligentie\"`\n- Exclusion searches: `Hubert NOT Bruls`\n- Boolean operators: `OR`, `NEAR()`\n\nThe implementation handles various edge cases:\n- Preserves quotes in search queries\n- Uses proper content type headers\n- Implements fallback mechanisms for API errors\n- Provides meaningful error messages\n\n## Error Handling\n\nThe API service includes robust error handling:\n- Graceful handling of API errors (4xx, 5xx)\n- Fallback to simplified queries when complex ones fail\n- Detailed error messages for debugging\n- Proper logging to stderr (not stdout, which would break the stdio transport)\n\n## Configuration\n\nThe server connects to Bert Hubert's [tkconv service](https://berthub.eu/tkconv/) as its primary data source, which provides a more accessible API than the official Dutch Parliament APIs. This service, created by Bert Hubert, does the heavy lifting of collecting, organizing, and making available Dutch parliamentary data in a developer-friendly format. Our MCP server builds upon this foundation to create a standardized interface for AI assistants to interact with this valuable data.\n\n## License\n\nMIT\n\n## Conclusion\n\nThe OpenTK MCP server provides a robust and well-structured interface to Dutch parliamentary data, making it accessible to AI assistants through the Model Context Protocol. Its modular design, comprehensive API, and thorough testing ensure reliable access to parliamentary information for AI-assisted research, analysis, and information retrieval.\n\nOnce configured, Claude will be able to access Dutch parliamentary data through the OpenTK MCP server. The server exposes all the tools described in the [Usage](#usage) section above.",
      "npm_url": "https://www.npmjs.com/package/opentk-mcp",
      "npm_downloads": 0,
      "keywords": [
        "parliamentary",
        "huijts",
        "legislative",
        "dutch parliamentary",
        "parliamentary documents",
        "search huijts"
      ],
      "category": "web-search"
    },
    "r-huijts--rijksmuseum-mcp": {
      "owner": "r-huijts",
      "name": "rijksmuseum-mcp",
      "url": "https://github.com/r-huijts/rijksmuseum-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/r-huijts.webp",
      "description": "Interact with the Rijksmuseum API for searching and retrieving artwork details, accessing high-resolution images, and managing user collections.",
      "stars": 58,
      "forks": 12,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-09-29T15:45:22Z",
      "readme_content": "![rijksmuseum logo](https://upload.wikimedia.org/wikipedia/commons/thumb/d/d1/Logo_Rijksmuseum.svg/799px-Logo_Rijksmuseum.svg.png)\n\n# Rijksmuseum MCP Server\n\nA Model Context Protocol (MCP) server that provides access to the Rijksmuseum's collection through natural language interactions. This server enables AI models to explore, analyze, and interact with artworks and collections from the Rijksmuseum.\n\n<a href=\"https://glama.ai/mcp/servers/4rmiexp64y\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/4rmiexp64y/badge\" alt=\"Rijksmuseum Server MCP server\" /></a>\n\n## Features\n\nThe server provides several tools for interacting with the Rijksmuseum's collection:\n\n### 1. Search Artworks (`search_artwork`)\nSearch and filter artworks using various criteria including:\n- Text-based search\n- Artist name\n- Artwork type\n- Materials and techniques\n- Time periods\n- Colors\n- And more\n\n### 2. Artwork Details (`get_artwork_details`)\nRetrieve comprehensive information about specific artworks, including:\n- Basic details (title, artist, dates)\n- Physical properties\n- Historical context\n- Visual information\n- Curatorial information\n- Exhibition history\n\n### 3. High-Resolution Images (`get_artwork_image`)\nAccess high-resolution image data with deep zoom capabilities:\n- Multiple zoom levels\n- Tile-based image loading\n- Full resolution support\n- Position information\n\n### 4. User Collections (`get_user_sets` & `get_user_set_details`)\nExplore user-created collections:\n- Browse curated sets\n- View thematic groupings\n- Analyze collection patterns\n- Access detailed set information\n\n### 5. Image Viewing (`open_image_in_browser`)\nOpen artwork images directly in your browser for detailed viewing.\n\n### 6. Artist Timeline (`get_artist_timeline`)\nGenerate chronological timelines of artists' works:\n- Track artistic development\n- Analyze periods and styles\n- Study career progression\n\n## Example Use Cases\n\nHere are some example queries you can ask the AI when using this server:\n\n### Artwork Discovery\n```\n\"Show me all paintings by Rembrandt from the 1640s\"\n\"Find artworks that prominently feature the color blue\"\n\"What are the most famous masterpieces in the collection?\"\n\"Search for still life paintings from the Dutch Golden Age\"\n```\n\n### Artwork Analysis\n```\n\"Tell me everything about The Night Watch\"\n\"What are the dimensions and materials used in Van Gogh's Self Portrait?\"\n\"Show me high-resolution details of the brushwork in Vermeer's The Milkmaid\"\n\"Compare the colors used in different versions of The Potato Eaters\"\n```\n\n### Artist Research\n```\n\"Create a timeline of Rembrandt's self-portraits\"\n\"How did Van Gogh's use of color evolve throughout his career?\"\n\"Show me all works by Frans Hals in chronological order\"\n\"What techniques did Jan Steen use in his paintings?\"\n```\n\n### Thematic Exploration\n```\n\"Find all artworks depicting biblical scenes\"\n\"Show me paintings of Amsterdam in the 17th century\"\n\"What artworks feature flowers or still life arrangements?\"\n\"Find portraits that include musical instruments\"\n```\n\n### Collection Analysis\n```\n\"Show me the most popular user-curated collections\"\n\"Find sets that focus on landscape paintings\"\n\"What are the recent additions to the museum's collection?\"\n\"Show me collections featuring works from multiple artists\"\n```\n\n### Visual Details\n```\n\"Let me examine the details in the background of The Night Watch\"\n\"Show me a close-up of the jewelry in Girl with a Pearl Earring\"\n\"Can you display the highest resolution version of The Jewish Bride?\"\n\"I want to study the facial expressions in The Syndics\"\n```\n\n## Getting Started\n\nYou can install this server in two ways:\n\n### 1. Using Claude Desktop with NPM Package\nUpdate your Claude configuration file (`~/Library/Application Support/Claude/claude_desktop_config.json`):\n\n```json\n{\n  \"mcpServers\": {\n    \"rijksmuseum-server\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"mcp-server-rijksmuseum\"\n      ],\n      \"env\": {\n        \"RIJKSMUSEUM_API_KEY\": \"your_api_key_here\"\n      }\n    }\n  }\n}\n```\nYou can get an API key from the [Rijksmuseum API Portal](https://data.rijksmuseum.nl/docs/api/).\n\n### 2. From Source\n1. Clone this repository\n2. Install dependencies:\n   ```bash\n   npm install\n   ```\n3. Copy the example environment file:\n   ```bash\n   cp .env.example .env\n   ```\n4. Add your Rijksmuseum API key to the `.env` file:\n   ```\n   RIJKSMUSEUM_API_KEY=your_api_key_here\n   ```\n5. Then update your Claude configuration file:\n   ```json\n   {\n     \"mcpServers\": {\n       \"rijksmuseum-server\": {\n         \"command\": \"node\",\n         \"args\": [\n           \"/path/to/rijksmuseum-server/build/index.js\"\n         ],\n         \"env\": {\n           \"RIJKSMUSEUM_API_KEY\": \"your_api_key_here\"\n         }\n       }\n     }\n   }\n   ```\n\nMake sure to:\n- Replace `/path/to/rijksmuseum-server` with the actual path to your installation\n- Add your Rijksmuseum API key in the `env` section\n\nAfter updating the configuration, restart Claude Desktop for the changes to take effect.\n\n## Configuration\n\nThe server can be configured through environment variables:\n- `RIJKSMUSEUM_API_KEY`: Your Rijksmuseum API key (required)\n- `PORT`: Server port (default: 3000)\n- `LOG_LEVEL`: Logging level (default: 'info')\n\n## API Documentation\n\nFor detailed information about the Rijksmuseum API endpoints used by this server, visit:\n[Rijksmuseum API Documentation](https://data.rijksmuseum.nl/object-metadata/api/)\n\n## Contributing\n\nContributions are welcome! Please feel free to submit pull requests or create issues for bugs and feature requests.\n\n## License\n\nThis project is licensed under the MIT License - see the LICENSE file for details.\n",
      "npm_url": "https://www.npmjs.com/package/rijksmuseum-mcp",
      "npm_downloads": 83,
      "keywords": [
        "rijksmuseum",
        "huijts",
        "search",
        "rijksmuseum api",
        "search huijts",
        "huijts rijksmuseum"
      ],
      "category": "web-search"
    },
    "raccoonaihq--raccoonai-mcp-server": {
      "owner": "raccoonaihq",
      "name": "raccoonai-mcp-server",
      "url": "https://github.com/raccoonaihq/raccoonai-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/raccoonaihq.webp",
      "description": "Enables web browsing, data extraction, and automation of complex web tasks using the LAM API. Automates interactions with websites by performing actions like filling out forms and navigating UI elements.",
      "stars": 0,
      "forks": 3,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-03-13T10:40:11Z",
      "readme_content": "# Raccoon AI MCP Server\n\n[![smithery badge](https://smithery.ai/badge/@raccoonaihq/raccoonai-mcp-server)](https://smithery.ai/server/@raccoonaihq/raccoonai-mcp-server)\n[![MCP Spec](https://img.shields.io/badge/mcp-compatible-green)](https://modelcontextprotocol.io)\n\nRaccoon AI's Model Context Protocol (MCP) server that enables leveraging the LAM API for web browsing, data extraction, and complex web tasks automation.\n\n## What can you do with this?\n\n- Search and browse websites\n- Fill out forms and navigate UI elements\n- Extract structured data based on defined schemas\n- Handle multistep processes across websites\n\n## Prerequisites\n\nBefore using the Raccoon LAM MCP server, you'll need:\n\n- Python 3.8 or higher\n- [Claude Desktop](https://claude.ai/download) or another MCP-compatible client\n- Raccoon AI Secret Key and your Raccoon Passcode\n\n## Installation\n\n### Using Smithery\n\n```bash\nnpx -y @smithery/cli@latest install @raccoonaihq/raccoonai-mcp-server --client claude\n```\n\n\n### From source\n\n```bash\ngit clone https://github.com/raccoonaihq/raccoonai-mcp-server.git\n```\n```bash\ncd raccoonai-mcp-server\n```\n```bash\nuv pip install -e .\n```\n\n#### To configure in Claude Desktop\n\n```bash\nmcp install src/raccoonai_mcp_server/server.py -v RACCOON_SECRET_KEY=<RACCOON_SECRET_KEY> -v RACCOON_PASSCODE=<RACCOON_PASSCODE>\n```\n\nReplace `<RACCOON_SECRET_KEY>` and `<RACCOON_PASSCODE>` with your actual creds. You can find them [here](https://platform.flyingraccoon.tech).\n\n## Examples\n\nHere are some example prompts that can be used with Claude to perform a variety of web tasks:\n\n1. Can you extract product information from Amazon.com for the top-rated gaming keyboards?\n2. Find and summarize the latest news articles about renewable energy technologies.\n3. Find the 3 latest iPhone models and extract the details in a schema.\n4. Do a deepsearch and generate a detailed report on Small Language Models.\n\n## Documentation\n\nFor more information, refer to:\n- [Raccoon LAM API Documentation](https://docs.flyingraccoon.tech/reference/lam/run)\n- [Model Context Protocol Documentation](https://modelcontextprotocol.io)",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "raccoonaihq",
        "web",
        "raccoonai",
        "lam api",
        "search raccoonaihq",
        "complex web"
      ],
      "category": "web-search"
    },
    "ralf-boltshauser--mcp-domain-checker-server": {
      "owner": "ralf-boltshauser",
      "name": "mcp-domain-checker-server",
      "url": "https://github.com/ralf-boltshauser/mcp-domain-checker-server",
      "imageUrl": "/freedevtools/mcp/pfp/ralf-boltshauser.webp",
      "description": "Provides real-time domain availability checks and pricing information, integrating with Vercel's API. Handles intelligent TLD defaults and returns detailed responses including error information.",
      "stars": 0,
      "forks": 1,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-04-14T07:19:10Z",
      "readme_content": "# DomainGenius MCP Server\n\nThe DomainGenius MCP server powers the domain search functionality of [DomainGenius](https://domaingenius.ch/), providing real-time domain availability checks and pricing information.\n\n## Features\n\n- Real-time domain availability checking\n- Domain pricing information retrieval\n- Automatic TLD handling (defaults to .com if not specified)\n- Error handling and detailed response format\n- Seamless integration with AI-powered domain search tools\n\n## Quick Start\n\n1. Install dependencies:\n```bash\npnpm install\n```\n\n2. Set up your environment:\n```bash\ncp .env.example .env\n```\nThen add your Vercel API token to the `.env` file.\n\n3. Build and run:\n```bash\npnpm run build\npnpm start\n```\n\n## Development\n\nFor development with hot-reloading:\n```bash\npnpm run dev\n```\n\n## License\n\nMIT License - see [LICENSE](LICENSE) for details. ",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "tld",
        "domain",
        "ralf",
        "domain checker",
        "domain availability",
        "vercel api"
      ],
      "category": "web-search"
    },
    "random-robbie--mcp-web-browser": {
      "owner": "random-robbie",
      "name": "mcp-web-browser",
      "url": "https://github.com/random-robbie/mcp-web-browser",
      "imageUrl": "/freedevtools/mcp/pfp/random-robbie.webp",
      "description": "Advanced web browsing capabilities enabling interactions with websites through a headless browser, capable of content extraction, form interaction, and tab management.",
      "stars": 23,
      "forks": 8,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-18T13:54:58Z",
      "readme_content": "# MCP Web Browser Server\n\nAn advanced web browsing server for the Model Context Protocol (MCP) powered by Playwright, enabling headless browser interactions through a flexible, secure API.\n\n<a href=\"https://glama.ai/mcp/servers/lwqlaw6k6d\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/lwqlaw6k6d/badge\" alt=\"Web Browser Server MCP server\" /></a>\n\n## 🌐 Features\n\n- **Headless Web Browsing**: Navigate to any website with SSL certificate validation bypass\n- **Full Page Content Extraction**: Retrieve complete HTML content, including dynamically loaded JavaScript\n- **Multi-Tab Support**: Create, manage, and switch between multiple browser tabs\n- **Advanced Web Interaction Tools**:\n  - Extract text content\n  - Click page elements\n  - Input text into form fields\n  - Capture screenshots\n  - Extract page links with filtering capabilities\n  - Scroll pages in any direction\n  - Execute JavaScript on pages\n  - Refresh pages\n  - Wait for navigation to complete\n- **Resource Management**: Automatic cleanup of unused resources after inactivity\n- **Enhanced Page Information**: Get detailed metadata about the current page\n\n## 🚀 Quick Start\n\n### Prerequisites\n\n- Python 3.10+\n- MCP SDK\n- Playwright\n\n### Installation\n\n```bash\n# Install MCP and Playwright\npip install mcp playwright\n\n# Install browser dependencies\nplaywright install\n```\n\n### Configuration for Claude Desktop\n\nAdd to your `claude_desktop_config.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"web-browser\": {\n      \"command\": \"python\",\n      \"args\": [\n        \"/path/to/your/server.py\"\n      ]\n    }\n  }\n}\n```\n\n## 💡 Usage Examples\n\n### Basic Web Navigation\n\n```python\n# Browse to a website\npage_content = browse_to(\"https://example.com\")\n\n# Extract page text\ntext_content = extract_text_content()\n\n# Extract text from a specific element\ntitle_text = extract_text_content(\"h1.title\")\n```\n\n### Web Interaction\n\n```python\n# Navigate to a page\nbrowse_to(\"https://example.com/login\")\n\n# Input text into a form\ninput_text(\"#username\", \"your_username\")\ninput_text(\"#password\", \"your_password\")\n\n# Click a login button\nclick_element(\"#login-button\")\n```\n\n### Screenshot Capture\n\n```python\n# Capture full page screenshot\nfull_page_screenshot = get_page_screenshots(full_page=True)\n\n# Capture specific element screenshot\nelement_screenshot = get_page_screenshots(selector=\"#main-content\")\n```\n\n### Link Extraction\n\n```python\n# Get all links on the page\npage_links = get_page_links()\n\n# Get links matching a pattern\nfiltered_links = get_page_links(filter_pattern=\"contact\")\n```\n\n### Multi-Tab Browsing\n\n```python\n# Create a new tab\ntab_id = create_new_tab(\"https://example.com\")\n\n# Create another tab\nanother_tab_id = create_new_tab(\"https://example.org\")\n\n# List all open tabs\ntabs = list_tabs()\n\n# Switch between tabs\nswitch_tab(tab_id)\n\n# Close a tab\nclose_tab(another_tab_id)\n```\n\n### Advanced Interactions\n\n```python\n# Scroll the page\nscroll_page(direction=\"down\", amount=\"page\")\n\n# Execute JavaScript on the page\nresult = execute_javascript(\"return document.title\")\n\n# Get detailed page information\npage_info = get_page_info()\n\n# Refresh the current page\nrefresh_page()\n\n# Wait for navigation to complete\nwait_for_navigation(timeout_ms=5000)\n```\n\n## 🛡️ Security Features\n\n- SSL certificate validation bypass\n- Secure browser context management\n- Custom user-agent configuration\n- Error handling and comprehensive logging\n- Configurable timeout settings\n- CSP bypass control\n- Protection against cookie stealing\n\n## 🔧 Troubleshooting\n\n### Common Issues\n\n- **SSL Certificate Errors**: Automatically bypassed\n- **Slow Page Load**: Adjust timeout in `browse_to()` method\n- **Element Not Found**: Verify selectors carefully\n- **Browser Resource Usage**: Auto-cleanup after inactivity period\n\n### Logging\n\nAll significant events are logged with detailed information for easy debugging.\n\n## 📋 Tool Parameters\n\n### `browse_to(url: str, context: Optional[Any] = None)`\n- `url`: Website to navigate to\n- `context`: Optional context object (currently unused)\n\n### `extract_text_content(selector: Optional[str] = None, context: Optional[Any] = None)`\n- `selector`: Optional CSS selector to extract specific content\n- `context`: Optional context object (currently unused)\n\n### `click_element(selector: str, context: Optional[Any] = None)`\n- `selector`: CSS selector of the element to click\n- `context`: Optional context object (currently unused)\n\n### `get_page_screenshots(full_page: bool = False, selector: Optional[str] = None, context: Optional[Any] = None)`\n- `full_page`: Capture entire page screenshot\n- `selector`: Optional element to screenshot\n- `context`: Optional context object (currently unused)\n\n### `get_page_links(filter_pattern: Optional[str] = None, context: Optional[Any] = None)`\n- `filter_pattern`: Optional text pattern to filter links\n- `context`: Optional context object (currently unused)\n\n### `input_text(selector: str, text: str, context: Optional[Any] = None)`\n- `selector`: CSS selector of input element\n- `text`: Text to input\n- `context`: Optional context object (currently unused)\n\n### `create_new_tab(url: Optional[str] = None, context: Optional[Any] = None)`\n- `url`: Optional URL to navigate to in the new tab\n- `context`: Optional context object (currently unused)\n\n### `switch_tab(tab_id: str, context: Optional[Any] = None)`\n- `tab_id`: ID of the tab to switch to\n- `context`: Optional context object (currently unused)\n\n### `list_tabs(context: Optional[Any] = None)`\n- `context`: Optional context object (currently unused)\n\n### `close_tab(tab_id: Optional[str] = None, context: Optional[Any] = None)`\n- `tab_id`: Optional ID of the tab to close (defaults to current tab)\n- `context`: Optional context object (currently unused)\n\n### `refresh_page(context: Optional[Any] = None)`\n- `context`: Optional context object (currently unused)\n\n### `get_page_info(context: Optional[Any] = None)`\n- `context`: Optional context object (currently unused)\n\n### `scroll_page(direction: str = \"down\", amount: str = \"page\", context: Optional[Any] = None)`\n- `direction`: Direction to scroll ('up', 'down', 'left', 'right')\n- `amount`: Amount to scroll ('page', 'half', or a number)\n- `context`: Optional context object (currently unused)\n\n### `wait_for_navigation(timeout_ms: int = 10000, context: Optional[Any] = None)`\n- `timeout_ms`: Maximum time to wait in milliseconds\n- `context`: Optional context object (currently unused)\n\n### `execute_javascript(script: str, context: Optional[Any] = None)`\n- `script`: JavaScript code to execute\n- `context`: Optional context object (currently unused)\n\n## 🤝 Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n\n### Development Setup\n\n```bash\n# Clone the repository\ngit clone https://github.com/random-robbie/mcp-web-browser.git\n\n# Create virtual environment\npython -m venv venv\nsource venv/bin/activate  # On Windows use `venv\\Scripts\\activate`\n\n# Install dependencies\npip install -e .[dev]\n```\n\n## 📄 License\n\nMIT License\n\n## 🔗 Related Projects\n\n- [Model Context Protocol](https://modelcontextprotocol.io)\n- [Playwright](https://playwright.dev)\n- [Claude Desktop](https://claude.ai/desktop)\n\n## 💬 Support\n\nFor issues and questions, please [open an issue](https://github.com/random-robbie/mcp-web-browser/issues) on GitHub.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "browser",
        "browsing",
        "web",
        "web browsing",
        "web browser",
        "advanced web"
      ],
      "category": "web-search"
    },
    "raultoto--ai_articles": {
      "owner": "raultoto",
      "name": "ai_articles",
      "url": "https://github.com/raultoto/ai_articles",
      "imageUrl": "/freedevtools/mcp/pfp/raultoto.webp",
      "description": "Generate insightful articles on artificial intelligence topics and provide curated content to enhance understanding of the latest trends in AI.",
      "stars": 0,
      "forks": 0,
      "license": "No License",
      "language": "",
      "updated_at": "2025-04-13T17:47:19Z",
      "readme_content": "# ai_articles",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "ai_articles",
        "ai",
        "articles",
        "ai_articles generate",
        "trends ai",
        "articles artificial"
      ],
      "category": "web-search"
    },
    "ravinahp--flights-mcp": {
      "owner": "ravinahp",
      "name": "flights-mcp",
      "url": "https://github.com/ravinahp/flights-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/ravinahp.webp",
      "description": "Search and retrieve flight information leveraging the Duffel API, particularly adept at managing complex travel plans with contextual memory of previous searches.",
      "stars": 137,
      "forks": 31,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-27T21:51:29Z",
      "readme_content": "# Find Flights MCP Server\nMCP server for searching and retrieving flight information using Duffel API.\n\n## How it Works\n![Flight](https://github.com/user-attachments/assets/3ee342a4-c2da-4d4e-a43c-79ae4590d893)\n\n## Video Demo\nhttps://github.com/user-attachments/assets/c111aa4c-9559-4d74-a2f6-60e322c273d4\n\n## Why This is Helpful\nWhile tools like Google Flights work great for simple trips, this tool shines when dealing with complex travel plans. Here's why:\n\n- **Contextual Memory**: Claude remembers all your previous flight searches in the chat, so you don't need to keep multiple tabs open to compare prices\n- **Flexible Date Search**: Easily search across multiple days to find the best prices without manually checking each date\n- **Complex Itineraries**: Perfect for multi-city trips, one-stop flights, or when you need to compare different route options you can just ask!\n- **Natural Conversation**: Just describe what you're looking for - no more clicking through calendar interfaces or juggling search parameters down to parsing city names, dates, and times.\n\nThink of it as having a travel agent in your chat who remembers everything you've discussed and can instantly search across dates and routes.\n\n## Features\n- Search for flights between multiple destinations\n- Support for one-way, round-trip, and multi-city flight queries\n- Detailed flight offer information\n- Flexible search parameters (departure times, cabin class, number of passengers)\n- Automatic handling of flight connections\n- Search for flights within multiple days to find the best flight for your trip (slower)\n## Prerequisites\n- Python 3.x\n- Duffel API Live Key\n\n## Getting Your Duffel API Key\nDuffel requires account verification and payment information setup, but this MCP server only uses the API for searching flights - no actual bookings or charges will be made to your account.\n\nTry using duffel_test first to see the power of this tool. If you end up liking it, you can go through the verification process below to use the live key.\n\n### Test Mode First (Recommended)\nYou can start with a test API key (`duffel_test`) to try out the functionality with simulated data before going through the full verification process:\n1. Visit [Duffel's registration page](https://app.duffel.com/join)\n2. Create an account (you can select \"Personal Use\" for Company Name)\n3. Navigate to More > Developer to find your test API key (one is already provided)\n\n### Getting a Live API Key\nTo access real flight data, follow these steps:\n1. In the Duffel dashboard, toggle \"Test Mode\" off in the top left corner\n2. The verification process requires multiple steps - you'll need to toggle test mode off repeatedly:\n   - First toggle: Verify your email address\n   - Toggle again: Complete company information (Personal Use is fine)\n   - Toggle again: Add payment information (required by Duffel but NO CHARGES will be made by this MCP server)\n   - Toggle again: Complete any remaining verification steps\n   - Final toggle: Access live mode after clicking \"Agree and Submit\"\n3. Once fully verified, go to More > Developer > Create Live Token\n4. Copy your live API key\n\n💡 TIP: Each time you complete a verification step, you'll need to toggle test mode off again to proceed to the next step. Keep toggling until you've completed all requirements.\n\n⚠️ IMPORTANT NOTES:\n- Your payment information is handled directly by Duffel and is not accessed or stored by the MCP server\n- This MCP server is READ-ONLY - it can only search for flights, not book them\n- No charges will be made to your payment method through this integration\n- All sensitive information (including API keys) stays local to your machine\n- You can start with the test API key (`duffel_test`) to evaluate the functionality\n- The verification process may take some time - this is a standard Duffel requirement\n\n### Security Note\nThis MCP server only uses Duffel's search endpoints and cannot make bookings or charges. Your payment information is solely for Duffel's verification process and is never accessed by or shared with the MCP server.\n\n### Note on API Usage Limits\n- Check Duffel's current pricing and usage limits\n- Different tiers available based on your requirements\n- Recommended to review current pricing on their website\n\n## Installation\n\n### Installing via Smithery\n\nTo install Find Flights for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@ravinahp/travel-mcp):\n\n```bash\nnpx -y @smithery/cli install @ravinahp/travel-mcp --client claude\n```\n\n### Manual Installation\nClone the repository:\n```bash\ngit clone https://github.com/ravinahp/flights-mcp\ncd flights-mcp\n```\n\nInstall dependencies using uv:\n```bash\nuv sync\n```\nNote: We use uv instead of pip since the project uses pyproject.toml for dependency management.\n\n## Configure as MCP Server\nTo add this tool as an MCP server, modify your Claude desktop configuration file.\n\nConfiguration file locations:\n- MacOS: `~/Library/Application\\ Support/Claude/claude_desktop_config.json`\n- Windows: `%APPDATA%/Claude/claude_desktop_config.json`\n\nAdd the following configuration to your JSON file:\n```json\n{\n    \"flights-mcp\": {\n        \"command\": \"uv\",\n        \"args\": [\n            \"--directory\",\n            \"/Users/YOUR_USERNAME/Code/flights-mcp\",\n            \"run\",\n            \"flights-mcp\"\n        ],\n        \"env\": {\n            \"DUFFEL_API_KEY_LIVE\": \"your_duffel_live_api_key_here\"\n        }\n    }\n}\n```\n\n⚠️ IMPORTANT:\n- Replace `YOUR_USERNAME` with your actual system username\n- Replace `your_duffel_live_api_key_here` with your actual Duffel Live API key\n- Ensure the directory path matches your local installation\n\n## Deployment\n### Building\nPrepare the package:\n```bash\n# Sync dependencies and update lockfile\nuv sync\n\n# Build package\nuv build\n```\nThis will create distributions in the `dist/` directory.\n\n## Debugging\nFor the best debugging experience, use the MCP Inspector:\n```bash\nnpx @modelcontextprotocol/inspector uv --directory /path/to/find-flights-mcp run flights-mcp\n```\n\nThe Inspector provides:\n- Real-time request/response monitoring\n- Input/output validation\n- Error tracking\n- Performance metrics\n\n## Available Tools\n\n### 1. Search Flights\n```python\n@mcp.tool()\nasync def search_flights(params: FlightSearch) -> str:\n    \"\"\"Search for flights based on parameters.\"\"\"\n```\nSupports three flight types:\n- One-way flights\n- Round-trip flights\n- Multi-city flights\n\nParameters include:\n- `type`: Flight type ('one_way', 'round_trip', 'multi_city')\n- `origin`: Origin airport code\n- `destination`: Destination airport code\n- `departure_date`: Departure date (YYYY-MM-DD)\n- Optional parameters:\n  - `return_date`: Return date for round-trips\n  - `adults`: Number of adult passengers\n  - `cabin_class`: Preferred cabin class\n  - `departure_time`: Specific departure time range\n  - `arrival_time`: Specific arrival time range\n  - `max_connections`: Maximum number of connections\n\n### 2. Get Offer Details\n```python\n@mcp.tool()\nasync def get_offer_details(params: OfferDetails) -> str:\n    \"\"\"Get detailed information about a specific flight offer.\"\"\"\n```\nRetrieves comprehensive details for a specific flight offer using its unique ID.\n\n### 3. Search Multi-City Flights\n```python\n@mcp.tool(name=\"search_multi_city\")\nasync def search_multi_city(params: MultiCityRequest) -> str:\n    \"\"\"Search for multi-city flights.\"\"\"\n```\nSpecialized tool for complex multi-city flight itineraries.\n\nParameters include:\n- `segments`: List of flight segments\n- `adults`: Number of adult passengers\n- `cabin_class`: Preferred cabin class\n- `max_connections`: Maximum number of connections\n\n## Use Cases\n### Some Example (But try it out yourself!)\nYou can use these tools to find flights with various complexities:\n- \"Find a one-way flight from SFO to NYC on Jan 7 for 2 adults in business class\"\n- \"Search for a round-trip flight from LAX to London, departing Jan 8 and returning Jan 15\"\n- \"Plan a multi-city trip from New York to Paris on Jan 7, then to Rome on Jan 10, and back to New York on Jan 15\"\n- \"What is the cheapest flight from SFO to LAX from Jan 7 to Jan 15 for 2 adults in economy class?\"\n- You can even search for flights within multiple days to find the best flight for your trip. Right now, the reccomendation is to only search for one-way or round-trip flights this way. Example: \"Find the cheapest flight from SFO to LAX from Jan 7 to Jan 10 for 2 adults in economy class\"\n\n## Response Format\nThe tools return JSON-formatted responses with:\n- Flight offer details\n- Pricing information\n- Slice (route) details\n- Carrier information\n- Connection details\n\n## Error Handling\nThe service includes robust error handling for:\n- API request failures\n- Invalid airport codes\n- Missing or invalid API keys\n- Network timeouts\n- Invalid search parameters\n\n## Contributing\n[Add guidelines for contribution, if applicable]\n\n## License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n\n## Performance Notes\n- Searches are limited to 50 offers for one-way/round-trip flights\n- Multi-city searches are limited to 10 offers\n- Supplier timeout is set to 15-30 seconds depending on the search type\n\n### Cabin Classes\nAvailable cabin classes:\n- `economy`: Standard economy class\n- `premium_economy`: Premium economy class\n- `business`: Business class\n- `first`: First class\n\nExample request with cabin class:\n```json\n{\n  \"params\": {\n    \"type\": \"one_way\",\n    \"adults\": 1,\n    \"origin\": \"SFO\",\n    \"destination\": \"LAX\",\n    \"departure_date\": \"2025-01-12\",\n    \"cabin_class\": \"business\"  // Specify desired cabin class\n  }\n}\n```\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "flights",
        "searches",
        "flight",
        "flight information",
        "retrieve flight",
        "travel plans"
      ],
      "category": "web-search"
    },
    "regenrek--deepwiki-mcp": {
      "owner": "regenrek",
      "name": "deepwiki-mcp",
      "url": "https://github.com/regenrek/deepwiki-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/regenrek.webp",
      "description": "Crawls Deepwiki.com documentation, converting it into Markdown format by removing unnecessary HTML elements and adjusting links for better readability. Supports fetching multiple pages and offers structured output formats for knowledge retrieval.",
      "stars": 1030,
      "forks": 59,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-10-03T02:47:13Z",
      "readme_content": "# Deepwiki MCP Server\n\n> ⚠️ **IMPORTANT NOTICE**: This server is currently not working since DeepWiki has cut off the possibility to scrape it. We recommend using the official DeepWiki MCP server at https://docs.devin.ai/work-with-devin/deepwiki-mcp for the time being.\n\nThis is an **unofficial Deepwiki MCP Server**\n\nIt takes a Deepwiki URL via MCP, crawls all relevant pages, converts them to Markdown, and returns either one document or a list by page.\n\n## Features\n\n- 🔒 **Domain Safety**: Only processes URLs from deepwiki.com\n- 🧹 **HTML Sanitization**: Strips headers, footers, navigation, scripts, and ads\n- 🔗 **Link Rewriting**: Adjusts links to work in Markdown\n- 📄 **Multiple Output Formats**: Get one document or structured pages\n- 🚀 **Performance**: Fast crawling with adjustable concurrency and depth\n- **NLP**: It's to search just for the library name\n\n## Usage\n\nPrompts you can use:\n\n```\ndeepwiki fetch how can i use gpt-image-1 with \"vercel ai\" sdk\n```\n\n```\ndeepwiki fetch how can i create new blocks in shadcn?\n```\n\n```\ndeepwiki fetch i want to understand how X works\n```\n\nFetch complete Documentation (Default)\n```\nuse deepwiki https://deepwiki.com/shadcn-ui/ui\nuse deepwiki multiple pages https://deepwiki.com/shadcn-ui/ui\n```\n\nSingle Page\n```\nuse deepwiki fetch single page https://deepwiki.com/tailwindlabs/tailwindcss/2.2-theme-system\n```\n\nGet by shortform\n```\nuse deepwiki fetch tailwindlabs/tailwindcss\n```\n\n```\ndeepwiki fetch library\n\ndeepwiki fetch url\ndeepwiki fetch <name>/<repo>\n\ndeepwiki multiple pages ...\ndeepwiki single page url ...\n```\n\n## Cursor\n\nAdd this to `.cursor/mcp.json` file.\n\n```\n{\n  \"mcpServers\": {\n    \"mcp-deepwiki\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"mcp-deepwiki@latest\"]\n    }\n  }\n}\n```\n\n\n\n### MCP Tool Integration\n\nThe package registers a tool named `deepwiki_fetch` that you can use with any MCP-compatible client:\n\n```json\n{\n  \"action\": \"deepwiki_fetch\",\n  \"params\": {\n    \"url\": \"https://deepwiki.com/user/repo\",\n    \"mode\": \"aggregate\",\n    \"maxDepth\": \"1\"\n  }\n}\n```\n\n#### Parameters\n\n- `url` (required): The starting URL of the Deepwiki repository\n- `mode` (optional): Output mode, either \"aggregate\" for a single Markdown document (default) or \"pages\" for structured page data\n- `maxDepth` (optional): Maximum depth of pages to crawl (default: 10)\n\n### Response Format\n\n#### Success Response (Aggregate Mode)\n\n```json\n{\n  \"status\": \"ok\",\n  \"data\": \"# Page Title\\n\\nPage content...\\n\\n---\\n\\n# Another Page\\n\\nMore content...\",\n  \"totalPages\": 5,\n  \"totalBytes\": 25000,\n  \"elapsedMs\": 1200\n}\n```\n\n#### Success Response (Pages Mode)\n\n```json\n{\n  \"status\": \"ok\",\n  \"data\": [\n    {\n      \"path\": \"index\",\n      \"markdown\": \"# Home Page\\n\\nWelcome to the repository.\"\n    },\n    {\n      \"path\": \"section/page1\",\n      \"markdown\": \"# First Page\\n\\nThis is the first page content.\"\n    }\n  ],\n  \"totalPages\": 2,\n  \"totalBytes\": 12000,\n  \"elapsedMs\": 800\n}\n```\n\n#### Error Response\n\n```json\n{\n  \"status\": \"error\",\n  \"code\": \"DOMAIN_NOT_ALLOWED\",\n  \"message\": \"Only deepwiki.com domains are allowed\"\n}\n```\n\n#### Partial Success Response\n\n```json\n{\n  \"status\": \"partial\",\n  \"data\": \"# Page Title\\n\\nPage content...\",\n  \"errors\": [\n    {\n      \"url\": \"https://deepwiki.com/user/repo/page2\",\n      \"reason\": \"HTTP error: 404\"\n    }\n  ],\n  \"totalPages\": 1,\n  \"totalBytes\": 5000,\n  \"elapsedMs\": 950\n}\n```\n\n### Progress Events\n\nWhen using the tool, you'll receive progress events during crawling:\n\n```\nFetched https://deepwiki.com/user/repo: 12500 bytes in 450ms (status: 200)\nFetched https://deepwiki.com/user/repo/page1: 8750 bytes in 320ms (status: 200)\nFetched https://deepwiki.com/user/repo/page2: 6200 bytes in 280ms (status: 200)\n```\n\n## Local Development - Installation\n\n### Local Usage\n\n```\n{\n  \"mcpServers\": {\n    \"mcp-deepwiki\": {\n      \"command\": \"node\",\n      \"args\": [\"./bin/cli.mjs\"]\n    }\n  }\n}\n```\n\n### From Source\n\n```bash\n# Clone the repository\ngit clone https://github.com/regenrek/deepwiki-mcp.git\ncd deepwiki-mcp\n\n# Install dependencies\nnpm install\n\n# Build the package\nnpm run build\n```\n\n#### Direct API Calls\n\nFor HTTP transport, you can make direct API calls:\n\n```bash\ncurl -X POST http://localhost:3000/mcp \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"id\": \"req-1\",\n    \"action\": \"deepwiki_fetch\",\n    \"params\": {\n      \"url\": \"https://deepwiki.com/user/repo\",\n      \"mode\": \"aggregate\"\n    }\n  }'\n```\n\n## Configuration\n\n### Environment Variables\n\n- `DEEPWIKI_MAX_CONCURRENCY`: Maximum concurrent requests (default: 5)\n- `DEEPWIKI_REQUEST_TIMEOUT`: Request timeout in milliseconds (default: 30000)\n- `DEEPWIKI_MAX_RETRIES`: Maximum retry attempts for failed requests (default: 3)\n- `DEEPWIKI_RETRY_DELAY`: Base delay for retry backoff in milliseconds (default: 250)\n\nTo configure these, create a `.env` file in the project root:\n\n```\nDEEPWIKI_MAX_CONCURRENCY=10\nDEEPWIKI_REQUEST_TIMEOUT=60000\nDEEPWIKI_MAX_RETRIES=5\nDEEPWIKI_RETRY_DELAY=500\n```\n\n## Docker Deployment (Untested)\n\nBuild and run the Docker image:\n\n```bash\n# Build the image\ndocker build -t mcp-deepwiki .\n\n# Run with stdio transport (for development)\ndocker run -it --rm mcp-deepwiki\n\n# Run with HTTP transport (for production)\ndocker run -d -p 3000:3000 mcp-deepwiki --http --port 3000\n\n# Run with environment variables\ndocker run -d -p 3000:3000 \\\n  -e DEEPWIKI_MAX_CONCURRENCY=10 \\\n  -e DEEPWIKI_REQUEST_TIMEOUT=60000 \\\n  mcp-deepwiki --http --port 3000\n```\n\n## Development\n\n```bash\n# Install dependencies\npnpm install\n\n# Run in development mode with stdio\npnpm run dev-stdio\n\n# Run tests\npnpm test\n\n# Run linter\npnpm run lint\n\n# Build the package\npnpm run build\n```\n\n## Troubleshooting\n\n### Common Issues\n\n1. **Permission Denied**: If you get EACCES errors when running the CLI, make sure to make the binary executable:\n   ```bash\n   chmod +x ./node_modules/.bin/mcp-deepwiki\n   ```\n\n2. **Connection Refused**: Make sure the port is available and not blocked by a firewall:\n   ```bash\n   # Check if port is in use\n   lsof -i :3000\n   ```\n\n3. **Timeout Errors**: For large repositories, consider increasing the timeout and concurrency:\n   ```\n   DEEPWIKI_REQUEST_TIMEOUT=60000 DEEPWIKI_MAX_CONCURRENCY=10 npx mcp-deepwiki\n   ```\n\n## Contributing\n\nWe welcome contributions! Please see [CONTRIBUTING.md](CONTRIBUTING.md) for details.\n\n## License\n\nMIT\n\n## Links\n\n- X/Twitter: [@kregenrek](https://x.com/kregenrek)\n- Bluesky: [@kevinkern.dev](https://bsky.app/profile/kevinkern.dev)\n\n## Courses\n- Learn Cursor AI: [Ultimate Cursor Course](https://www.instructa.ai/en/cursor-ai)\n- Learn to build software with AI: [instructa.ai](https://www.instructa.ai)\n\n## See my other projects:\n\n* [AI Prompts](https://github.com/instructa/ai-prompts/blob/main/README.md) - Curated AI Prompts for Cursor AI, Cline, Windsurf and Github Copilot\n* [codefetch](https://github.com/regenrek/codefetch) - Turn code into Markdown for LLMs with one simple terminal command\n* [aidex](https://github.com/regenrek/aidex) A CLI tool that provides detailed information about AI language models, helping developers choose the right model for their needs.# tool-starter",
      "npm_url": "https://www.npmjs.com/package/deepwiki-mcp",
      "npm_downloads": 706,
      "keywords": [
        "deepwiki",
        "markdown",
        "documentation",
        "crawls deepwiki",
        "deepwiki com",
        "regenrek deepwiki"
      ],
      "category": "web-search"
    },
    "rileyedwards77--perplexity-mcp-server": {
      "owner": "rileyedwards77",
      "name": "perplexity-mcp-server",
      "url": "https://github.com/rileyedwards77/perplexity-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/rileyedwards77.webp",
      "description": "Integrates with Perplexity AI for chatting, searching, and retrieving documentation. Facilitates access to various functionalities of the Perplexity AI API through MCP-based systems.",
      "stars": 1,
      "forks": 4,
      "license": "No License",
      "language": "JavaScript",
      "updated_at": "2025-09-10T05:26:53Z",
      "readme_content": "# Perplexity AI MCP Server\n\nThis repository contains the source code for a Model Context Protocol (MCP) server that provides access to the Perplexity AI API.  This server allows users to interact with Perplexity AI through various tools, including chatting, searching, and retrieving documentation.\n\n## Purpose\n\nThis server simplifies the integration of Perplexity AI into MCP-based systems.  It provides a convenient and standardized way to access Perplexity AI's capabilities.\n\n## Setup\n\n1.  **Install Node.js and npm:** Ensure you have Node.js and npm installed on your system.\n2.  **Clone the repository:** Clone this repository to your local machine.\n3.  **Install dependencies:** Navigate to the project directory and run `npm install`.\n4.  **Configure API Key:** Set the `PERPLEXITY_API_KEY` environment variable to your Perplexity API key.\n5.  **Run the server:** Run `npm start` to start the server.\n\n## Usage\n\nThe server exposes several tools that can be accessed through the MCP system.  Refer to the MCP documentation for details on how to use these tools.\n\n## Technologies Used\n\n*   TypeScript\n*   @modelcontextprotocol/sdk\n*   axios\n\n## Known Issues\n\n*   The Perplexity API may be unreliable.  Error handling is included to gracefully handle API failures.\n\n## Contributing\n\nContributions are welcome!  Please open an issue or submit a pull request.\n",
      "npm_url": "https://www.npmjs.com/package/perplexity-mcp-server",
      "npm_downloads": 1579,
      "keywords": [
        "searching",
        "search",
        "perplexity",
        "perplexity ai",
        "ai chatting",
        "chatting searching"
      ],
      "category": "web-search"
    },
    "rockerritesh--scraper-mcp-smithery": {
      "owner": "rockerritesh",
      "name": "scraper-mcp-smithery",
      "url": "https://github.com/rockerritesh/scraper-mcp-smithery",
      "imageUrl": "/freedevtools/mcp/pfp/rockerritesh.webp",
      "description": "Enhance web scraping capabilities with tools to efficiently extract and manipulate data. Automate data collection through asynchronous web search operations and integrate smoothly with existing applications using a FastMCP server foundation.",
      "stars": 1,
      "forks": 0,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-07-07T03:03:11Z",
      "readme_content": "# Web Scraper MCP for Smithery\r\n\r\nA robust MCP (Model Context Protocol) server for web scraping operations, deployed on [Smithery](https://smithery.ai/) - the orchestration layer for AI agents. This extension converts any website into clean, structured markdown format with automatic ChromeDriver management.\r\n\r\n## 🌟 Available on Smithery\r\n\r\nThis MCP server is part of [Smithery's marketplace](https://smithery.ai/) with **7953+ skills and extensions** built by the community. Deploy instantly to integrate web scraping capabilities into your AI agents.\r\n\r\n## ✨ Features\r\n\r\n- **🚀 High Performance**: Direct function integration with uv package manager for optimal speed\r\n- **🔄 Zero Configuration**: Automatic ChromeDriver management with version compatibility\r\n- **🌐 Smart URL Processing**: Auto-adds HTTPS protocol and validates URLs\r\n- **📝 Markdown Conversion**: Converts web content to clean, structured markdown\r\n- **⚡ Async Operations**: Non-blocking web scraping with proper async/await\r\n- **🛡️ Production Ready**: Comprehensive error handling and graceful fallbacks\r\n- **🐳 Smithery Optimized**: Containerized deployment with security best practices\r\n\r\n## 📋 Prerequisites\r\n\r\n- **Smithery Account** - [Sign up at smithery.ai](https://smithery.ai/)\r\n- **Python 3.12+** (for local development)\r\n- **UV** package manager\r\n- **Google Chrome** (automatically managed in deployment)\r\n\r\n## 🚀 Smithery Deployment\r\n\r\n### Deploy to Smithery Platform\r\n\r\n1. **Visit** [Smithery Web Scraper MCP](https://smithery.ai/)\r\n2. **Click \"Deploy Server\"** to add to your agent\r\n3. **Configure** with your preferred settings\r\n4. **Start scraping** websites instantly!\r\n\r\n### Local Development\r\n\r\n```bash\r\n# Clone the repository\r\ngit clone https://github.com/rockerritesh/scraper-mcp-smithery.git\r\ncd scraper-mcp-smithery\r\n\r\n# Install dependencies with uv\r\nuv sync\r\n\r\n# Run the MCP development server\r\nuv run mcp dev server.py\r\n```\r\n\r\n\r\n### Direct Python Usage (Development)\r\n\r\n```python\r\nfrom scraper_doc import scrape_website\r\n\r\n# Scrape a website\r\ncontent = scrape_website(\"https://example.com\")\r\nprint(content)  # Returns markdown formatted content\r\n```\r\n\r\n### URL Format Requirements\r\n\r\n- **✅ Supported**: `https://example.com`, `http://example.com`\r\n- **✅ Auto-fixed**: `example.com` → `https://example.com`\r\n- **❌ Invalid**: Malformed URLs return descriptive error messages\r\n\r\n## 🏗️ Smithery Architecture\r\n\r\n### Integration Flow\r\n\r\n```\r\nSmithery Agent → MCP Protocol → search_web_tool → Chrome/Selenium → Markdown Output\r\n```\r\n\r\n### Platform Benefits\r\n\r\n- **🎯 Zero Setup**: Deploy instantly without infrastructure management\r\n- **📊 Monitoring**: Built-in health checks and performance metrics\r\n- **🔗 Agent Integration**: Seamless connection to Smithery's AI orchestration\r\n- **📈 Scalability**: Automatic scaling based on usage patterns\r\n\r\n### Key Improvements\r\n\r\n- **❌ Old**: Subprocess calls with performance overhead\r\n- **✅ New**: Direct function imports with async execution\r\n- **🎯 Result**: ~3x faster performance on Smithery platform\r\n\r\n## 🛠️ Development & Testing\r\n\r\n### Local Testing\r\n\r\n```bash\r\n# Test the scraper directly\r\nuv run python scraper_doc.py https://example.com\r\n\r\n# Test with output directory\r\nuv run python scraper_doc.py https://example.com ./output\r\n\r\n# Run MCP development server\r\nuv run mcp dev server.py\r\n```\r\n\r\n### Debug Mode\r\n\r\n```bash\r\nMCP_DEBUG=1 uv run mcp dev server.py\r\n```\r\n\r\n### Dependencies (Managed by UV)\r\n\r\n- **mcp[cli]** - Model Context Protocol framework\r\n- **selenium** - Web browser automation\r\n- **webdriver-manager** - Automatic ChromeDriver management\r\n- **requests** - HTTP client for image downloads\r\n- **python-dotenv** - Environment variable management\r\n\r\n## 🐛 Troubleshooting\r\n\r\n### Common Smithery Issues\r\n\r\n- **Deployment Timeout**: Usually resolves automatically; check Smithery status\r\n- **Tool Not Found**: Ensure proper MCP tool registration in server.py\r\n- **Memory Limits**: Large pages may require optimization (handled automatically)\r\n\r\n### ChromeDriver Issues\r\n\r\nAutomatically resolved by webdriver-manager, but for local development:\r\n\r\n```bash\r\n# Clear webdriver cache if needed\r\nrm -rf ~/.wdm/\r\n\r\n# Verify Chrome installation\r\ngoogle-chrome --version\r\n```\r\n\r\n## 📊 Performance on Smithery\r\n\r\n- **🚀 Scraping Speed**: 2-5 seconds per page\r\n- **💾 Memory Usage**: ~50-100MB per operation\r\n- **⚡ Concurrent Support**: Multiple async operations\r\n- **🔄 Auto-scaling**: Handled by Smithery platform\r\n\r\n## 🔐 Security Features\r\n\r\n- **🛡️ Sandboxed Execution**: Chrome runs with security flags\r\n- **👤 Non-root User**: Enhanced container security\r\n- **🔒 URL Validation**: Prevents malicious URL processing\r\n- **📊 Audit Logging**: Smithery platform monitoring\r\n\r\n## 🌐 Smithery Integration Examples\r\n\r\n### In Chat Agents\r\n\r\n```\r\nAgent: \"Can you scrape the latest news from example.com?\"\r\nWeb Scraper MCP: *Scrapes and returns structured content*\r\nAgent: \"Here's the latest news in markdown format...\"\r\n```\r\n\r\n### In Automation Workflows\r\n\r\n```\r\nTrigger → Smithery Agent → Web Scraper MCP → Content Analysis → Action\r\n```\r\n\r\n## 📚 Resources\r\n\r\n- **[Smithery Platform](https://smithery.ai/)** - Deploy and manage MCP servers\r\n- **[Smithery Documentation](https://smithery.ai/docs)** - Platform guides and API reference\r\n- **[MCP Specification](https://github.com/modelcontextprotocol/specification)** - Protocol documentation\r\n- **[Community Discord](https://smithery.ai/discord)** - Get help and share ideas\r\n\r\n## 📜 License\r\n\r\nMIT License - see [LICENSE](LICENSE) file for details.\r\n\r\n## 🤝 Contributing to Smithery Ecosystem\r\n\r\n1. Fork this repository\r\n2. Create a feature branch\r\n3. Test on Smithery platform\r\n4. Submit a pull request\r\n5. Share in [Smithery community](https://smithery.ai/)\r\n\r\n---\r\n\r\n**🚀 Deployed on [Smithery](https://smithery.ai/) | Built with FastMCP, Selenium, and UV | Part of 7953+ community extensions**\r\n```\r\n\r\nThis README provides clear setup instructions while highlighting the tool's async capabilities and Smithery integration. The structure follows best practices for developer tools documentation.\r\n\r\n---\r\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "scraping",
        "scraper",
        "fastmcp",
        "web scraping",
        "scraper mcp",
        "scraping capabilities"
      ],
      "category": "web-search"
    },
    "rogeriolembo--mcp-api": {
      "owner": "rogeriolembo",
      "name": "mcp-api",
      "url": "https://github.com/rogeriolembo/mcp-api",
      "imageUrl": "/freedevtools/mcp/pfp/rogeriolembo.webp",
      "description": "Integrate with the Sonar API to conduct real-time web searches through conversational interactions, enabling enhanced research capabilities with live data.",
      "stars": 0,
      "forks": 0,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-03-31T14:11:12Z",
      "readme_content": "# sysauto Ask MCP Server\n\nAn MCP server implementation that integrates the Sonar API to provide Claude with unparalleled real-time, web-wide research.\n\n\n\n\n## Tools\n\n- **sysauto_ask**\n  - Engage in a conversation with the Sonar API for live web searches.\n  - **Inputs:**\n    - `messages` (array): An array of conversation messages.\n      - Each message must include:\n        - `role` (string): The role of the message (e.g., `system`, `user`, `assistant`).\n        - `content` (string): The content of the message.\n\n## Configuration\n\n### Step 1: \n\nClone this repository:\n\n```bash\ngit clone git@github.com:rogeriolembo/mcp-api.git\n```\n\nNavigate to the `sysauto-ask` directory and install the necessary dependencies:\n\n```bash\ncd mcp-api/sysauto-ask && npm install\n```\n\n### Step 2: Get a Sonar API Key\n\n1. Sign up for a [Sonar API account](https://docs.sysauto.ai/guides/getting-started).\n2. Follow the account setup instructions and generate your API key from the developer dashboard.\n3. Set the API key in your environment as `sysauto_API_KEY`.\n\n### Step 3: Configure Claude Desktop\n\n1. Download Claude desktop [here](https://claude.ai/download). \n\n2. Add this to your `claude_desktop_config.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"sysauto-ask\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"-i\",\n        \"--rm\",\n        \"-e\",\n        \"SYSAUTO_API_KEY\",\n        \"mcp/sysauto-ask\"\n      ],\n      \"env\": {\n        \"SYSAUTO_API_KEY\": \"YOUR_API_KEY_HERE\"\n      }\n    }\n  }\n}\n```\n\n### NPX\n\n```json\n{\n  \"mcpServers\": {\n    \"sysauto-ask\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"@rogeriolembo/server-sysauto-ask\"\n      ],\n      \"env\": {\n        \"SYSAUTO_API_KEY\": \"YOUR_API_KEY_HERE\"\n      }\n    }\n  }\n}\n```\n\nnpm publish --access public\nnpm publish",
      "npm_url": "https://www.npmjs.com/package/mcp-api",
      "npm_downloads": 83,
      "keywords": [
        "sonar",
        "searches",
        "web",
        "sonar api",
        "web search",
        "integrate sonar"
      ],
      "category": "web-search"
    },
    "ronantakizawa--a11ymcp": {
      "owner": "ronantakizawa",
      "name": "a11ymcp",
      "url": "https://github.com/ronantakizawa/a11ymcp",
      "imageUrl": "/freedevtools/mcp/pfp/ronantakizawa.webp",
      "description": "Run WCAG compliance tests and accessibility assessments using the axe-core API. Users can test raw HTML or URLs for accessibility issues and color contrast evaluation.",
      "stars": 33,
      "forks": 7,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-09-27T06:06:07Z",
      "readme_content": "# Web Accessibility-Testing MCP Server (A11y MCP)\n[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/ronantakizawa-a11ymcp-badge.png)](https://mseep.ai/app/ronantakizawa-a11ymcp)\n[![smithery badge](https://smithery.ai/badge/@ronantakizawa/a11ymcp)](https://smithery.ai/server/@ronantakizawa/a11ymcp)\n[![Verified on MseeP](https://mseep.ai/badge.svg)](https://mseep.ai/app/01361aeb-0dce-45d6-80fb-76ff443dbfc8)\n\n<a href=\"https://glama.ai/mcp/servers/@ronantakizawa/a11ymcp\">\n  <img alt=\"badge\" width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@ronantakizawa/a11ymcp/badge\" />\n</a>\n\n![a11ymcpwide](https://github.com/user-attachments/assets/a37c4a9e-da5e-49ac-9705-0ef87a1d5c17)\n\n\nA11y MCP is an MCP (Model Context Protocol) server that gives LLMs access to web accessibility testing APIs. \n\nThis server uses the Deque Axe-core API and Puppeteer to allow LLMs to analyze web content for WCAG compliance and identify accessibility issues.\n\nNOTE: This is not an official MCP server from Deque Labs.\n\nLeave a star if you enjoyed the project! 🌟\n\n## Features\n\n- **Test web pages**: Test any public URL for accessibility issues\n- **Test HTML snippets**: Test raw HTML strings for accessibility issues\n- **WCAG compliance testing**: Check content against various WCAG standards (2.0, 2.1, 2.2)\n- **Customizable tests**: Specify which accessibility tags/standards to test against\n- **Rule exploration**: Get information about available accessibility rules\n- **Color contrast analysis**: Check color combinations for WCAG compliance\n- **ARIA validation**: Test proper usage of ARIA attributes\n- **Orientation lock detection**: Identify content that forces specific screen orientations\n\n## Installation\nTo use this server with Claude Desktop, you need to configure it in the MCP settings:\n\n**For macOS:**\nEdit the file at `'~/Library/Application Support/Claude/claude_desktop_config.json'`\n\n```\n{\n  \"mcpServers\": {\n    \"a11y-accessibility\": {\n    \"command\": \"npx\",\n    \"args\": [\n      \"-y\",\n      \"a11y-mcp-server\"\n    ]\n   }\n  }\n}\n```\n\n**For Windows:**\nEdit the file at `%APPDATA%\\Claude\\settings\\claude_mcp_settings.json`\n\n**For Linux:**\nEdit the file at `~/.config/Claude/settings/claude_mcp_settings.json`\nReplace `/path/to/axe-mcp-server/build/index.js` with the actual path to your compiled server file.\n\n\n## Available Tools\n\n### test_accessibility\n\nTests a URL for accessibility issues.\n\n**Parameters:**\n- `url` (required): The URL of the web page to test\n- `tags` (optional): Array of WCAG tags to test against (e.g., [\"wcag2aa\"])\n\nExample\n\n```\n{\n \"url\": \"https://example.com\",\n \"tags\": [\"wcag2aa\"]\n}\n```\n### test_html_string\n\nTests an HTML string for accessibility issues.\nParameters:\n\n* html (required): The HTML content to test\n* tags (optional): Array of WCAG tags to test against (e.g., [\"wcag2aa\"])\n\nExample\n\n```\n{\n  \"html\": \"<div><img src='image.jpg'></div>\",\n  \"tags\": [\"wcag2aa\"]\n}\n```\n\n### get_rules\n\nGet information about available accessibility rules with optional filtering.\n\n### check_color_contrast\n\nCheck if a foreground and background color combination meets WCAG contrast requirements.\n\n**Parameters:**\n\n- `foreground` (required): Foreground color in hex format (e.g., \"#000000\")\n- `background` (required): Background color in hex format (e.g., \"#FFFFFF\")\n- `fontSize` (optional): Font size in pixels (default: 16)\n- `isBold` (optional): Whether the text is bold (default: false)\n\nExample\n\n```\n{\n  \"foreground\": \"#777777\",\n  \"background\": \"#EEEEEE\",\n  \"fontSize\": 16,\n  \"isBold\": false\n}\n```\n\n### check_color_contrast\n\nCheck if ARIA attributes are used correctly in HTML.\n\n**Parameters:**\n\n- `html` (required): HTML content to test for ARIA attribute usage\n\nExample\n\n```\n{\n  \"html\": \"<div role='button' aria-pressed='false'>Click me</div>\"\n}\n```\n\n### check_orientation_lock\n\nCheck if content forces a specific orientation.\n\n**Parameters:**\n\n- `html` (required): HTML content to test for orientation lock issues\n\nExample\n\n```\n{\n  \"html\": \"<html><head><meta name='viewport' content='width=device-width, orientation=portrait'></head><body>Content</body></html>\"\n}\n```\n\n## Response Format\nThe server returns accessibility test results in a structured JSON format:\n```\n{\n  \"violations\": [\n    {\n      \"id\": \"color-contrast\",\n      \"impact\": \"serious\",\n      \"description\": \"Ensure the contrast between foreground and background colors meets WCAG 2 AA minimum contrast ratio thresholds\",\n      \"help\": \"Elements must meet minimum color contrast ratio thresholds\",\n      \"helpUrl\": \"https://dequeuniversity.com/rules/axe/4.10/color-contrast\",\n      \"affectedNodes\": [\n        {\n          \"html\": \"<div style=\\\"color: #aaa; background-color: #eee;\\\">Low contrast text</div>\",\n          \"target\": [\"div\"],\n          \"failureSummary\": \"Fix any of the following: Element has insufficient color contrast of 1.98 (foreground color: #aaa, background color: #eee, font size: 12.0pt, font weight: normal)\"\n        }\n      ]\n    }\n  ],\n  \"passes\": 1,\n  \"incomplete\": 0,\n  \"inapplicable\": 2,\n  \"timestamp\": \"2025-04-25T16:45:33.655Z\",\n  \"url\": \"about:blank\",\n  \"testEngine\": {\n    \"name\": \"axe-core\",\n    \"version\": \"4.10.3\"\n  },\n  \"testRunner\": {\n    \"name\": \"axe\"\n  },\n  \"testEnvironment\": {\n    \"userAgent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) HeadlessChrome/135.0.0.0 Safari/537.36\",\n    \"windowWidth\": 800,\n    \"windowHeight\": 600,\n    \"orientationAngle\": 0,\n    \"orientationType\": \"portrait-primary\"\n  }\n}\n```\n\n## Dependencies\n\n- @modelcontextprotocol/sdk\n- puppeteer\n- @axe-core/puppeteer\n- axe-core\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "accessibility",
        "wcag",
        "tests",
        "tests accessibility",
        "accessibility assessments",
        "wcag compliance"
      ],
      "category": "web-search"
    },
    "ropon--mcp_demo": {
      "owner": "ropon",
      "name": "mcp_demo",
      "url": "https://github.com/ropon/mcp_demo",
      "imageUrl": "/freedevtools/mcp/pfp/ropon.webp",
      "description": "Query IP addresses to retrieve related information and insights in real-time. This server provides essential data for applications that require IP data analysis.",
      "stars": 0,
      "forks": 0,
      "license": "Apache License 2.0",
      "language": "Go",
      "updated_at": "2025-04-07T01:54:49Z",
      "readme_content": "# mcp_demo\nmcp demo ip query\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "ip",
        "ropon",
        "mcp_demo",
        "query ip",
        "ip data",
        "search ropon"
      ],
      "category": "web-search"
    },
    "rsagacom--chatgpt-on-wechat": {
      "owner": "rsagacom",
      "name": "chatgpt-on-wechat",
      "url": "https://github.com/rsagacom/chatgpt-on-wechat",
      "imageUrl": "/freedevtools/mcp/pfp/rsagacom.webp",
      "description": "A multi-platform intelligent dialogue service that supports text, voice, and image interactions. It can connect to various AI models and allows for custom enterprise AI applications through plugin extensions.",
      "stars": 0,
      "forks": 0,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2024-01-28T14:00:49Z",
      "readme_content": "# 简介\n\n> 本项目是基于大模型的智能对话机器人，支持微信、企业微信、公众号、飞书、钉钉接入，可选择GPT3.5/GPT4.0/Claude/文心一言/讯飞星火/通义千问/Gemini/LinkAI，能处理文本、语音和图片，通过插件访问操作系统和互联网等外部资源，支持基于自有知识库定制企业AI应用。\n\n最新版本支持的功能如下：\n\n- [x] **多端部署：** 有多种部署方式可选择且功能完备，目前已支持个人微信、微信公众号和、企业微信、飞书、钉钉等部署方式\n- [x] **基础对话：** 私聊及群聊的消息智能回复，支持多轮会话上下文记忆，支持 GPT-3.5, GPT-4, claude, Gemini, 文心一言, 讯飞星火, 通义千问\n- [x] **语音能力：** 可识别语音消息，通过文字或语音回复，支持 azure, baidu, google, openai(whisper/tts) 等多种语音模型\n- [x] **图像能力：** 支持图片生成、图片识别、图生图（如照片修复），可选择 Dall-E-3, stable diffusion, replicate, midjourney, vision模型\n- [x] **丰富插件：** 支持个性化插件扩展，已实现多角色切换、文字冒险、敏感词过滤、聊天记录总结、文档总结和对话、联网搜索等插件\n- [x] **知识库：** 通过上传知识库文件自定义专属机器人，可作为数字分身、智能客服、私域助手使用，基于 [LinkAI](https://link-ai.tech) 实现\n\n# 演示\n\nhttps://github.com/zhayujie/chatgpt-on-wechat/assets/26161723/d5154020-36e3-41db-8706-40ce9f3f1b1e\n\nDemo made by [Visionn](https://www.wangpc.cc/)\n\n# 商业支持\n\n> 我们还提供企业级的 **AI应用平台**，包含知识库、Agent插件、应用管理等能力，支持多平台聚合的应用接入、客户端管理、对话管理，以及提供\nSaaS服务、私有化部署、稳定托管接入 等多种模式。\n>\n> 目前已在私域运营、智能客服、企业效率助手等场景积累了丰富的 AI 解决方案， 在电商、文教、健康、新消费等各行业沉淀了 AI 落地的最佳实践，致力于打造助力中小企业拥抱 AI 的一站式平台。\n\n企业服务和商用咨询可联系产品顾问：\n\n<img width=\"240\" src=\"https://img-1317903499.cos.ap-guangzhou.myqcloud.com/docs/product-manager-qrcode.jpg\">\n\n# 开源社区\n\n添加小助手微信加入开源项目交流群：\n\n\n\n# 更新日志\n\n>**2023.11.11：** [1.5.3版本](https://github.com/zhayujie/chatgpt-on-wechat/releases/tag/1.5.3) 和 [1.5.4版本](https://github.com/zhayujie/chatgpt-on-wechat/releases/tag/1.5.4)，新增Google Gemini、通义千问模型\n\n>**2023.11.10：** [1.5.2版本](https://github.com/zhayujie/chatgpt-on-wechat/releases/tag/1.5.2)，新增飞书通道、图像识别对话、黑名单配置\n\n>**2023.11.10：** [1.5.0版本](https://github.com/zhayujie/chatgpt-on-wechat/releases/tag/1.5.0)，新增 `gpt-4-turbo`, `dall-e-3`, `tts` 模型接入，完善图像理解&生成、语音识别&生成的多模态能力\n\n>**2023.10.16：** 支持通过意图识别使用LinkAI联网搜索、数学计算、网页访问等插件，参考[插件文档](https://docs.link-ai.tech/platform/plugins)\n\n>**2023.09.26：** 插件增加 文件/文章链接 一键总结和对话的功能，使用参考：[插件说明](https://github.com/zhayujie/chatgpt-on-wechat/tree/master/plugins/linkai#3%E6%96%87%E6%A1%A3%E6%80%BB%E7%BB%93%E5%AF%B9%E8%AF%9D%E5%8A%9F%E8%83%BD)\n\n>**2023.08.08：** 接入百度文心一言模型，通过 [插件](https://github.com/zhayujie/chatgpt-on-wechat/tree/master/plugins/linkai) 支持 Midjourney 绘图\n\n>**2023.06.12：** 接入 [LinkAI](https://link-ai.tech/console) 平台，可在线创建领域知识库，并接入微信、公众号及企业微信中，打造专属客服机器人。使用参考 [接入文档](https://link-ai.tech/platform/link-app/wechat)。\n\n>**2023.04.26：** 支持企业微信应用号部署，兼容插件，并支持语音图片交互，私人助理理想选择，[使用文档](https://github.com/zhayujie/chatgpt-on-wechat/blob/master/channel/wechatcom/README.md)。(contributed by [@lanvent](https://github.com/lanvent) in [#944](https://github.com/zhayujie/chatgpt-on-wechat/pull/944))\n\n>**2023.04.05：** 支持微信公众号部署，兼容插件，并支持语音图片交互，[使用文档](https://github.com/zhayujie/chatgpt-on-wechat/blob/master/channel/wechatmp/README.md)。(contributed by [@JS00000](https://github.com/JS00000) in [#686](https://github.com/zhayujie/chatgpt-on-wechat/pull/686))\n\n>**2023.04.05：** 增加能让ChatGPT使用工具的`tool`插件，[使用文档](https://github.com/goldfishh/chatgpt-on-wechat/blob/master/plugins/tool/README.md)。工具相关issue可反馈至[chatgpt-tool-hub](https://github.com/goldfishh/chatgpt-tool-hub)。(contributed by [@goldfishh](https://github.com/goldfishh) in [#663](https://github.com/zhayujie/chatgpt-on-wechat/pull/663))\n\n>**2023.03.25：** 支持插件化开发，目前已实现 多角色切换、文字冒险游戏、管理员指令、Stable Diffusion等插件，使用参考 [#578](https://github.com/zhayujie/chatgpt-on-wechat/issues/578)。(contributed by [@lanvent](https://github.com/lanvent) in [#565](https://github.com/zhayujie/chatgpt-on-wechat/pull/565))\n\n>**2023.03.09：** 基于 `whisper API`(后续已接入更多的语音`API`服务) 实现对微信语音消息的解析和回复，添加配置项 `\"speech_recognition\":true` 即可启用，使用参考 [#415](https://github.com/zhayujie/chatgpt-on-wechat/issues/415)。(contributed by [wanggang1987](https://github.com/wanggang1987) in [#385](https://github.com/zhayujie/chatgpt-on-wechat/pull/385))\n\n>**2023.02.09：** 扫码登录存在账号限制风险，请谨慎使用，参考[#58](https://github.com/AutumnWhj/ChatGPT-wechat-bot/issues/158)\n\n# 快速开始\n\n快速开始文档：[项目搭建文档](https://docs.link-ai.tech/cow/quick-start)\n\n## 准备\n\n### 1. 账号注册\n\n项目默认使用OpenAI接口，需前往 [OpenAI注册页面](https://beta.openai.com/signup) 创建账号，创建完账号则前往 [API管理页面](https://beta.openai.com/account/api-keys) 创建一个 API Key 并保存下来，后面需要在项目中配置这个key。接口需要海外网络访问及绑定信用卡支付。\n\n> 默认对话模型是 openai 的 gpt-3.5-turbo，计费方式是约每 1000tokens (约750个英文单词 或 500汉字，包含请求和回复) 消耗 $0.002，图片生成是Dell E模型，每张消耗 $0.016。\n\n项目同时也支持使用 LinkAI 接口，无需代理，可使用 文心、讯飞、GPT-3、GPT-4 等模型，支持 定制化知识库、联网搜索、MJ绘图、文档总结和对话等能力。修改配置即可一键切换，参考 [接入文档](https://link-ai.tech/platform/link-app/wechat)。\n\n### 2.运行环境\n\n支持 Linux、MacOS、Windows 系统（可在Linux服务器上长期运行)，同时需安装 `Python`。\n> 建议Python版本在 3.7.1~3.9.X 之间，推荐3.8版本，3.10及以上版本在 MacOS 可用，其他系统上不确定能否正常运行。\n\n> 注意：Docker 或 Railway 部署无需安装python环境和下载源码，可直接快进到下一节。\n\n**(1) 克隆项目代码：**\n\n```bash\ngit clone https://github.com/zhayujie/chatgpt-on-wechat\ncd chatgpt-on-wechat/\n```\n\n注: 如遇到网络问题可选择国内镜像 https://gitee.com/zhayujie/chatgpt-on-wechat\n\n**(2) 安装核心依赖 (必选)：**\n> 能够使用`itchat`创建机器人，并具有文字交流功能所需的最小依赖集合。\n```bash\npip3 install -r requirements.txt\n```\n\n**(3) 拓展依赖 (可选，建议安装)：**\n\n```bash\npip3 install -r requirements-optional.txt\n```\n> 如果某项依赖安装失败可注释掉对应的行再继续\n\n## 配置\n\n配置文件的模板在根目录的`config-template.json`中，需复制该模板创建最终生效的 `config.json` 文件：\n\n```bash\n  cp config-template.json config.json\n```\n\n然后在`config.json`中填入配置，以下是对默认配置的说明，可根据需要进行自定义修改（请去掉注释）：\n\n```bash\n# config.json文件内容示例\n{\n  \"open_ai_api_key\": \"YOUR API KEY\",                          # 填入上面创建的 OpenAI API KEY\n  \"model\": \"gpt-3.5-turbo\",                                   # 模型名称, 支持 gpt-3.5-turbo, gpt-3.5-turbo-16k, gpt-4, wenxin, xunfei\n  \"proxy\": \"\",                                                # 代理客户端的ip和端口，国内环境开启代理的需要填写该项，如 \"127.0.0.1:7890\"\n  \"single_chat_prefix\": [\"bot\", \"@bot\"],                      # 私聊时文本需要包含该前缀才能触发机器人回复\n  \"single_chat_reply_prefix\": \"[bot] \",                       # 私聊时自动回复的前缀，用于区分真人\n  \"group_chat_prefix\": [\"@bot\"],                              # 群聊时包含该前缀则会触发机器人回复\n  \"group_name_white_list\": [\"ChatGPT测试群\", \"ChatGPT测试群2\"], # 开启自动回复的群名称列表\n  \"group_chat_in_one_session\": [\"ChatGPT测试群\"],              # 支持会话上下文共享的群名称  \n  \"image_create_prefix\": [\"画\", \"看\", \"找\"],                   # 开启图片回复的前缀\n  \"conversation_max_tokens\": 1000,                            # 支持上下文记忆的最多字符数\n  \"speech_recognition\": false,                                # 是否开启语音识别\n  \"group_speech_recognition\": false,                          # 是否开启群组语音识别\n  \"use_azure_chatgpt\": false,                                 # 是否使用Azure ChatGPT service代替openai ChatGPT service. 当设置为true时需要设置 open_ai_api_base，如 https://xxx.openai.azure.com/\n  \"azure_deployment_id\": \"\",                                  # 采用Azure ChatGPT时，模型部署名称\n  \"azure_api_version\": \"\",                                    # 采用Azure ChatGPT时，API版本\n  \"character_desc\": \"你是ChatGPT, 一个由OpenAI训练的大型语言模型, 你旨在回答并解决人们的任何问题，并且可以使用多种语言与人交流。\",  # 人格描述\n  # 订阅消息，公众号和企业微信channel中请填写，当被订阅时会自动回复，可使用特殊占位符。目前支持的占位符有{trigger_prefix}，在程序中它会自动替换成bot的触发词。\n  \"subscribe_msg\": \"感谢您的关注！\\n这里是ChatGPT，可以自由对话。\\n支持语音对话。\\n支持图片输出，画字开头的消息将按要求创作图片。\\n支持角色扮演和文字冒险等丰富插件。\\n输入{trigger_prefix}#help 查看详细指令。\",\n  \"use_linkai\": false,                                        # 是否使用LinkAI接口，默认关闭，开启后可国内访问，使用知识库和MJ\n  \"linkai_api_key\": \"\",                                       # LinkAI Api Key\n  \"linkai_app_code\": \"\"                                       # LinkAI 应用code\n}\n```\n**配置说明：**\n\n**1.个人聊天**\n\n+ 个人聊天中，需要以 \"bot\"或\"@bot\" 为开头的内容触发机器人，对应配置项 `single_chat_prefix` (如果不需要以前缀触发可以填写  `\"single_chat_prefix\": [\"\"]`)\n+ 机器人回复的内容会以 \"[bot] \" 作为前缀， 以区分真人，对应的配置项为 `single_chat_reply_prefix` (如果不需要前缀可以填写 `\"single_chat_reply_prefix\": \"\"`)\n\n**2.群组聊天**\n\n+ 群组聊天中，群名称需配置在 `group_name_white_list ` 中才能开启群聊自动回复。如果想对所有群聊生效，可以直接填写 `\"group_name_white_list\": [\"ALL_GROUP\"]`\n+ 默认只要被人 @ 就会触发机器人自动回复；另外群聊天中只要检测到以 \"@bot\" 开头的内容，同样会自动回复（方便自己触发），这对应配置项 `group_chat_prefix`\n+ 可选配置: `group_name_keyword_white_list`配置项支持模糊匹配群名称，`group_chat_keyword`配置项则支持模糊匹配群消息内容，用法与上述两个配置项相同。（Contributed by [evolay](https://github.com/evolay))\n+ `group_chat_in_one_session`：使群聊共享一个会话上下文，配置 `[\"ALL_GROUP\"]` 则作用于所有群聊\n\n**3.语音识别**\n\n+ 添加 `\"speech_recognition\": true` 将开启语音识别，默认使用openai的whisper模型识别为文字，同时以文字回复，该参数仅支持私聊 (注意由于语音消息无法匹配前缀，一旦开启将对所有语音自动回复，支持语音触发画图)；\n+ 添加 `\"group_speech_recognition\": true` 将开启群组语音识别，默认使用openai的whisper模型识别为文字，同时以文字回复，参数仅支持群聊 (会匹配group_chat_prefix和group_chat_keyword, 支持语音触发画图)；\n+ 添加 `\"voice_reply_voice\": true` 将开启语音回复语音（同时作用于私聊和群聊），但是需要配置对应语音合成平台的key，由于itchat协议的限制，只能发送语音mp3文件，若使用wechaty则回复的是微信语音。\n\n**4.其他配置**\n\n+ `model`: 模型名称，目前支持 `gpt-3.5-turbo`, `text-davinci-003`, `gpt-4`, `gpt-4-32k`, `wenxin` , `claude` ,  `xunfei`(其中gpt-4 api暂未完全开放，申请通过后可使用)\n+ `temperature`,`frequency_penalty`,`presence_penalty`: Chat API接口参数，详情参考[OpenAI官方文档。](https://platform.openai.com/docs/api-reference/chat)\n+ `proxy`：由于目前 `openai` 接口国内无法访问，需配置代理客户端的地址，详情参考  [#351](https://github.com/zhayujie/chatgpt-on-wechat/issues/351)\n+ 对于图像生成，在满足个人或群组触发条件外，还需要额外的关键词前缀来触发，对应配置 `image_create_prefix `\n+ 关于OpenAI对话及图片接口的参数配置（内容自由度、回复字数限制、图片大小等），可以参考 [对话接口](https://beta.openai.com/docs/api-reference/completions) 和 [图像接口](https://beta.openai.com/docs/api-reference/completions)  文档，在[`config.py`](https://github.com/zhayujie/chatgpt-on-wechat/blob/master/config.py)中检查哪些参数在本项目中是可配置的。\n+ `conversation_max_tokens`：表示能够记忆的上下文最大字数（一问一答为一组对话，如果累积的对话字数超出限制，就会优先移除最早的一组对话）\n+ `rate_limit_chatgpt`，`rate_limit_dalle`：每分钟最高问答速率、画图速率，超速后排队按序处理。\n+ `clear_memory_commands`: 对话内指令，主动清空前文记忆，字符串数组可自定义指令别名。\n+ `hot_reload`: 程序退出后，暂存微信扫码状态，默认关闭。\n+ `character_desc` 配置中保存着你对机器人说的一段话，他会记住这段话并作为他的设定，你可以为他定制任何人格      (关于会话上下文的更多内容参考该 [issue](https://github.com/zhayujie/chatgpt-on-wechat/issues/43))\n+ `subscribe_msg`：订阅消息，公众号和企业微信channel中请填写，当被订阅时会自动回复， 可使用特殊占位符。目前支持的占位符有{trigger_prefix}，在程序中它会自动替换成bot的触发词。\n\n**5.LinkAI配置 (可选)**\n\n+ `use_linkai`: 是否使用LinkAI接口，开启后可国内访问，使用知识库和 `Midjourney` 绘画, 参考 [文档](https://link-ai.tech/platform/link-app/wechat)\n+ `linkai_api_key`: LinkAI Api Key，可在 [控制台](https://link-ai.tech/console/interface) 创建\n+ `linkai_app_code`: LinkAI 应用code，选填\n\n**本说明文档可能会未及时更新，当前所有可选的配置项均在该[`config.py`](https://github.com/zhayujie/chatgpt-on-wechat/blob/master/config.py)中列出。**\n\n## 运行\n\n### 1.本地运行\n\n如果是开发机 **本地运行**，直接在项目根目录下执行：\n\n```bash\npython3 app.py                                    # windows环境下该命令通常为 python app.py\n```\n\n终端输出二维码后，使用微信进行扫码，当输出 \"Start auto replying\" 时表示自动回复程序已经成功运行了（注意：用于登录的微信需要在支付处已完成实名认证）。扫码登录后你的账号就成为机器人了，可以在微信手机端通过配置的关键词触发自动回复 (任意好友发送消息给你，或是自己发消息给好友)，参考[#142](https://github.com/zhayujie/chatgpt-on-wechat/issues/142)。\n\n### 2.服务器部署\n\n使用nohup命令在后台运行程序：\n\n```bash\nnohup python3 app.py & tail -f nohup.out          # 在后台运行程序并通过日志输出二维码\n```\n扫码登录后程序即可运行于服务器后台，此时可通过 `ctrl+c` 关闭日志，不会影响后台程序的运行。使用 `ps -ef | grep app.py | grep -v grep` 命令可查看运行于后台的进程，如果想要重新启动程序可以先 `kill` 掉对应的进程。日志关闭后如果想要再次打开只需输入 `tail -f nohup.out`。此外，`scripts` 目录下有一键运行、关闭程序的脚本供使用。\n\n> **多账号支持：** 将项目复制多份，分别启动程序，用不同账号扫码登录即可实现同时运行。\n\n> **特殊指令：** 用户向机器人发送 **#reset** 即可清空该用户的上下文记忆。\n\n\n### 3.Docker部署\n\n> 使用docker部署无需下载源码和安装依赖，只需要获取 docker-compose.yml 配置文件并启动容器即可。\n\n> 前提是需要安装好 `docker` 及 `docker-compose`，安装成功的表现是执行 `docker -v` 和 `docker-compose version` (或 docker compose version) 可以查看到版本号，可前往 [docker官网](https://docs.docker.com/engine/install/) 进行下载。\n\n#### (1) 下载 docker-compose.yml 文件\n\n```bash\nwget https://open-1317903499.cos.ap-guangzhou.myqcloud.com/docker-compose.yml\n```\n\n下载完成后打开 `docker-compose.yml` 修改所需配置，如 `OPEN_AI_API_KEY` 和 `GROUP_NAME_WHITE_LIST` 等。\n\n#### (2) 启动容器\n\n在 `docker-compose.yml` 所在目录下执行以下命令启动容器：\n\n```bash\nsudo docker compose up -d\n```\n\n运行 `sudo docker ps` 能查看到 NAMES 为 chatgpt-on-wechat 的容器即表示运行成功。\n\n注意：\n\n - 如果 `docker-compose` 是 1.X 版本 则需要执行 `sudo  docker-compose up -d` 来启动容器\n - 该命令会自动去 [docker hub](https://hub.docker.com/r/zhayujie/chatgpt-on-wechat) 拉取 latest 版本的镜像，latest 镜像会在每次项目 release 新的版本时生成\n\n最后运行以下命令可查看容器运行日志，扫描日志中的二维码即可完成登录：\n\n```bash\nsudo docker logs -f chatgpt-on-wechat\n```\n\n#### (3) 插件使用\n\n如果需要在docker容器中修改插件配置，可通过挂载的方式完成，将 [插件配置文件](https://github.com/zhayujie/chatgpt-on-wechat/blob/master/plugins/config.json.template)\n重命名为 `config.json`，放置于 `docker-compose.yml` 相同目录下，并在 `docker-compose.yml` 中的 `chatgpt-on-wechat` 部分下添加 `volumes` 映射:\n\n```\nvolumes:\n  - ./config.json:/app/plugins/config.json\n```\n\n### 4. Railway部署\n\n> Railway 每月提供5刀和最多500小时的免费额度。 (07.11更新: 目前大部分账号已无法免费部署)\n\n1. 进入 [Railway](https://railway.app/template/qApznZ?referralCode=RC3znh)\n2. 点击 `Deploy Now` 按钮。\n3. 设置环境变量来重载程序运行的参数，例如`open_ai_api_key`, `character_desc`。\n\n**一键部署:**\n  \n  [![Deploy on Railway](https://railway.app/button.svg)](https://railway.app/template/qApznZ?referralCode=RC3znh)\n\n## 常见问题\n\nFAQs： <https://github.com/zhayujie/chatgpt-on-wechat/wiki/FAQs>\n\n或直接在线咨询 [项目小助手](https://link-ai.tech/app/Kv2fXJcH)  (beta版本，语料完善中，回复仅供参考)\n\n## 开发\n\n欢迎接入更多应用，参考 [Terminal代码](https://github.com/zhayujie/chatgpt-on-wechat/blob/master/channel/terminal/terminal_channel.py) 实现接收和发送消息逻辑即可接入。 同时欢迎增加新的插件，参考 [插件说明文档](https://github.com/zhayujie/chatgpt-on-wechat/tree/master/plugins)。\n\n## 联系\n\n欢迎提交PR、Issues，以及Star支持一下。程序运行遇到问题可以查看 [常见问题列表](https://github.com/zhayujie/chatgpt-on-wechat/wiki/FAQs) ，其次前往 [Issues](https://github.com/zhayujie/chatgpt-on-wechat/issues) 中搜索。个人开发者可加入开源交流群参与更多讨论，企业用户可联系[产品顾问](https://img-1317903499.cos.ap-guangzhou.myqcloud.com/docs/product-manager-qrcode.jpg)咨询。",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "chatgpt",
        "dialogue",
        "wechat",
        "dialogue service",
        "rsagacom chatgpt",
        "chatgpt wechat"
      ],
      "category": "web-search"
    },
    "ruradium--mcp-reddit": {
      "owner": "ruradium",
      "name": "mcp-reddit",
      "url": "https://github.com/ruradium/mcp-reddit",
      "imageUrl": "/freedevtools/mcp/pfp/ruradium.webp",
      "description": "Fetch and analyze Reddit content, including hot threads and detailed posts with comments from any subreddit. Supports various post types such as text, link, and gallery to provide comprehensive insights.",
      "stars": 2,
      "forks": 1,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-08-02T05:42:54Z",
      "readme_content": "# MCP Reddit Server\n[![smithery badge](https://smithery.ai/badge/@adhikasp/mcp-reddit)](https://smithery.ai/server/@adhikasp/mcp-reddit)\n\nA [Model Context Protocol (MCP)](https://modelcontextprotocol.io/introduction) server that provides tools for fetching and analyzing Reddit content.\n\n<a href=\"https://glama.ai/mcp/servers/3cg9gdyors\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/3cg9gdyors/badge\" alt=\"mcp-reddit MCP server\" /></a>\n\n## Features\n\n- Fetch hot threads from any subreddit\n- Get detailed post content including comments\n- Support for different post types (text, link, gallery)\n\n## Installation\n\n### Installing via Smithery\n\nTo install Reddit Content for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@adhikasp/mcp-reddit):\n\n```bash\nnpx -y @smithery/cli install @adhikasp/mcp-reddit --client claude\n```\n\n### Manual Installation\n```json\n{\n  \"reddit\": {\n    \"command\": \"uvx\",\n    \"args\": [\"--from\", \"git+https://github.com/adhikasp/mcp-reddit.git\", \"mcp-reddit\"],\n    \"env\": {}\n  }\n}\n```\n\n## Usage\n\nUsing [mcp-client-cli](https://github.com/adhikasp/mcp-client-cli):\n\n```\n$ llm what are latest hot thread in r/victoria3\n\nI'll fetch the latest hot threads from the Victoria 3 subreddit for you.\n\nTool Calls:\n  fetch_hot_threads\n  Args:\n    subreddit: victoria3\n\n\nBased on the hot threads, here are the key highlights from the Victoria 3 subreddit:\n\n1. Dev Diary #126 - Update 1.8 Overview\n   - Major updates planned for the game, including:\n     - Political Movement Rework (Ideological Forces)\n     - Discrimination Rework\n     - Food Availability, Famines, and Harvest Incidents\n     - Additional features like Companies owning buildings and Bulk Nationalization\n\n2. Dev Diary #138 - Pivot of Empire Update\n   - Update 1.8 \"Masala Chai\" has been released\n   - Focuses on India with new Journal Entries, Events, and Immersion Pack\n   - 10 new achievements added\n   - Save games from 1.7.7 are not compatible with 1.8\n\n3. Interesting Community Discussions:\n   - A player shared a detailed experience of retaking Constantinople as Greece, highlighting the complex population dynamics\n   - Humorous posts about game mechanics, such as investment rights and political movements\n   - Various memes and gameplay screenshots showcasing unique game situations\n\nThe most upvoted thread is the Dev Diary #126, which provides an in-depth look at the upcoming game mechanics improvements, particularly the reworks to political movements and discrimination systems.\n\nWould you like me to elaborate on any of these points or provide more details about the Victoria 3 update?\n``` \n",
      "npm_url": "https://www.npmjs.com/package/mcp-reddit-server",
      "npm_downloads": 90,
      "keywords": [
        "ruradium",
        "reddit",
        "search",
        "ruradium mcp",
        "search ruradium",
        "reddit fetch"
      ],
      "category": "web-search"
    },
    "rvydhya--youtube_transcriptor": {
      "owner": "rvydhya",
      "name": "youtube_transcriptor",
      "url": "https://github.com/rvydhya/youtube_transcriptor",
      "imageUrl": "/freedevtools/mcp/pfp/rvydhya.webp",
      "description": "Transcribes YouTube videos by extracting transcripts, including both manual and autogenerated captions, using the provided video URL. Supports integration with MCP clients for enhanced workflows involving video transcription.",
      "stars": 0,
      "forks": 1,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-04-23T13:38:39Z",
      "readme_content": "# YouTube Transcriptor MCP Tool\n\nThis is a Model Context Protocol (MCP) tool for transcribing YouTube videos using the `youtube-transcript-api`.\n\n## Features\n- Extracts and transcribes from YouTube videos (manual or autogenerated). Enables retrieval of transcripts from YouTube videos.\n- Exposes a single tool: `transcribe_video(video: str)`.\n\n## Usage\n\n### 1. Prerequisites\n- Python 3.12+\n- Install dependencies:\n  ```sh\n  pip install -r requirements.txt\n  ```\n\n### 2. Running the Tool\nYou can run the tool directly:\n```sh\npython youtube.py\n```\n\n### 3. Using with VS Code (Manual MCP Config)\nTo use this tool as an MCP server in VS Code, add the following to your `.vscode/settings.json` or your MCP client configuration:\n\n```json\n\"youtube_transcriptor\": {\n  \"type\": \"stdio\",\n  \"command\": \"python\",\n  \"args\": [\"PATH\\\\transcription\\\\youtube.py\"]\n}\n```\nReplace `PATH` with the absolute path to your workspace root.\n\n### 4. Using the Tool\nOnce configured, you can call the `transcribe_video` tool from your MCP client or compatible VS Code extension, passing a YouTube video URL as the argument.\n\n---\n\n**Example:**\n```python\nresult = transcribe_video(\"https://www.youtube.com/watch?v=VIDEO_ID\")\nprint(result)\n```\n\n---\n\n## License\nMIT\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "youtube_transcriptor",
        "transcription",
        "transcripts",
        "rvydhya youtube_transcriptor",
        "youtube_transcriptor transcribes",
        "transcribes youtube"
      ],
      "category": "web-search"
    },
    "sagacious-satadru--Documentation-MCP": {
      "owner": "sagacious-satadru",
      "name": "Documentation-MCP",
      "url": "https://github.com/sagacious-satadru/Documentation-MCP",
      "imageUrl": "/freedevtools/mcp/pfp/sagacious-satadru.webp",
      "description": "Search and access documentation from popular AI libraries such as LangChain and LlamaIndex. Enhance conversations with contextual knowledge by leveraging intelligent documentation extraction.",
      "stars": 3,
      "forks": 1,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-07-18T09:03:58Z",
      "readme_content": "# Documentation MCP Server 📚🔍\n\nA Model Context Protocol (MCP) server that enables Claude to search and access documentation from popular libraries like LangChain, LlamaIndex, and OpenAI directly within conversations.\n\n## What is MCP? 🤔\n\nMCP (Model Context Protocol) is an open protocol that standardizes how applications provide context to Large Language Models. Think of it as a universal connector that lets AI assistants like Claude access external data sources and tools.\n\n\n\n\n\n\n## Features ✨\n\n- **Documentation Search Tool**: Search through documentation of popular AI libraries\n- **Supported Libraries**:\n  - [LangChain](https://python.langchain.com/docs) 🔗\n  - [LlamaIndex](https://docs.llamaindex.ai/en/stable) 🦙\n  - [OpenAI](https://platform.openai.com/docs) 🤖\n- **Smart Extraction**: Intelligently parses HTML content to extract the most relevant information\n- **Configurable Results**: Limit the amount of text returned based on your needs\n\n## How It Works 🛠️\n\n1. The server uses the Serper API to perform Google searches with site-specific queries\n2. It fetches the content from the search results\n3. BeautifulSoup extracts the most relevant text from main content areas\n4. Claude can access this information through the `get_docs` tool\n\n## System Requirements 🖥️\n\n- Python 3.11 or higher\n- `uv` package manager\n- A Serper API key\n\n## Setup Instructions 🚀\n\n### 1. Install uv Package Manager\n\n```bash\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n```\n\n### 2. Clone and Set Up the Project\n\n```bash\n# Clone or download the project\ncd documentation\n\n# Create and activate virtual environment\nuv venv\n# On Windows:\n.venv\\Scripts\\activate\n# On macOS/Linux:\nsource .venv/bin/activate\n\n# Install dependencies\nuv pip install -e .\n```\n\n### 3. Configure the Serper API Key\n\nCreate a `.env` file in the project directory with your Serper API key:\n\n```\nSERPER_API_KEY=your_serper_api_key_here\n```\n\nYou can get a Serper API key by signing up at [serper.dev](https://serper.dev).\n\n### 4. Configure Claude Desktop\n\nEdit your Claude Desktop configuration file at:\n- Windows: `/C:/Users/[Your Username]/AppData/Roaming/Claude/claude_desktop_config.json`\n\n- macOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\n\nAdd the following to the `mcpServers` section:\n\n```json\n\"documentation\": {\n  \"command\": \"uv\",\n  \"args\": [\n    \"--directory\",\n    \"/ABSOLUTE/PATH/TO/YOUR/documentation\",\n    \"run\",\n    \"main.py\"\n  ]\n}\n```\n\nReplace `/ABSOLUTE/PATH/TO/YOUR/documentation` with the absolute path to your project directory.\n\n### 5. Restart Claude Desktop\n\nClose and reopen Claude Desktop to apply the new configuration.\n\n## Using the Documentation Tool 🧩\n\nOnce connected, you can ask Claude to use the documentation tool:\n\n> \"Can you look up information about vector stores in LangChain documentation?\"\n\nClaude will use the `get_docs` tool to search for relevant information and provide you with documentation excerpts.\n\n## Tool Parameters 📋\n\nThe `get_docs` tool accepts the following parameters:\n\n- `query`: The search term (e.g., \"vector stores\", \"embedding models\")\n- `library`: Which library to search (langchain, llama-index, or openai)\n- `max_chars`: Maximum characters to return (default: 1000)\n\n## Troubleshooting 🛠️\n\n- **Claude can't find the server**: Verify the path in `/C:/Users/fcbsa/AppData/Roaming/Claude/claude_desktop_config.json` is correct\n- **Search returns no results**: Check your Serper API key and internet connection\n- **Timeout errors**: The server might be experiencing connectivity issues or rate limits\n\n## License 📜\n\nThis project is provided as an educational example of MCP server implementation.\n\n## Acknowledgements 🙏\n\n- Built using the [MCP SDK](https://github.com/modelcontextprotocol)\n- Powered by [Serper API](https://serper.dev) for Google search integration\n- Uses [BeautifulSoup4](https://www.crummy.com/software/BeautifulSoup/) for HTML parsing\n- Inspired by the growing MCP community\n\n---\n\n*This MCP server enhances Claude's capabilities by providing direct access to documentation resources. Explore, learn, and build better AI applications with contextual knowledge from the docs!*",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "search",
        "documentation",
        "ai",
        "intelligent documentation",
        "documentation extraction",
        "ai libraries"
      ],
      "category": "web-search"
    },
    "saginawj--mcp-reddit-companion": {
      "owner": "saginawj",
      "name": "mcp-reddit-companion",
      "url": "https://github.com/saginawj/mcp-reddit-companion",
      "imageUrl": "/freedevtools/mcp/pfp/saginawj.webp",
      "description": "Enables natural language interactions with personalized Reddit feeds, allowing users to analyze and summarize content relevant to their interests.",
      "stars": 0,
      "forks": 2,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-04-27T17:56:32Z",
      "readme_content": "[![smithery badge](https://smithery.ai/badge/@saginawj/mcp-reddit-companion)](https://smithery.ai/server/@saginawj/mcp-reddit-companion)\n\n# MCP Reddit Companion\n\nAn MCP tool that enables natural language interaction with your personal Reddit experience. Create custom curated feeds on Reddit and use your favorite LLM client to analyze, summarize, and engage with content that matters to you.\n\n## Example LLM Commands\n\nHere are some example commands you can use with your LLM client:\n\n```python\n# Basic Feed Interaction\n\"Show me the latest posts from my 'tech-news' feed\"\n\"Summarize the top posts from my 'science' feed\"\n\"What are the trending topics in my 'programming' feed?\"\n\n# Content Analysis\n\"What are the common themes in my 'ai' feed?\"\n\"Which of my recent posts got the most engagement?\"\n\"Summarize the discussions in my 'philosophy' feed\"\n\n# Personal Activity\n\"Show me my recent Reddit activity\"\n\"What comments have I received on my posts?\"\n\"Are there any unread messages in my inbox?\"\n\n# Engagement Tracking\n\"How are my recent posts performing?\"\n\"Show me the most active discussions in my feeds\"\n\"What posts got the most comments in my 'news' feed?\"\n```\n\n## Prerequisites\n\n- Python 3.11+\n- Reddit account credentials (username, password)\n- Docker (optional, for containerized deployment)\n- An MCP-compatible LLM client (like Cursor)\n\n## Installation\n\n### Local Installation\n\n1. Clone the repository:\n```bash\ngit clone https://github.com/yourusername/mcp-reddit.git\ncd mcp-reddit\n```\n\n2. Install dependencies using uv:\n```bash\nuv pip install .\n```\n\n3. Create a `.env` file with your Reddit API credentials:\n```env\nREDDIT_CLIENT_ID=your_client_id\nREDDIT_CLIENT_SECRET=your_client_secret\nREDDIT_USERNAME=your_username\nREDDIT_PASSWORD=your_password\n```\n\n### Docker Installation\n\n1. Build the Docker image:\n```bash\ndocker build -t mcp-reddit .\n```\n\n2. Run the container (for passwords with special characters):\n```bash\ndocker run \\\n  -e REDDIT_CLIENT_ID='your_client_id' \\\n  -e REDDIT_CLIENT_SECRET='your_client_secret' \\\n  -e REDDIT_USERNAME='your_username' \\\n  -e REDDIT_PASSWORD='your_password' \\\n  mcp-reddit\n```\n\nNote: If your password contains special characters (like !, $, etc.), make sure to:\n1. Use single quotes around the password\n2. Escape any special characters with a backslash\n3. Or use double quotes and escape the special characters\n\nExample with special characters:\n```bash\ndocker run \\\n  -e REDDIT_CLIENT_ID='your_client_id' \\\n  -e REDDIT_CLIENT_SECRET='your_client_secret' \\\n  -e REDDIT_USERNAME='your_username' \\\n  -e REDDIT_PASSWORD='your\\!password' \\\n  mcp-reddit\n```\n\n## Usage\n\n### Local Usage\n\n1. Start the MCP server:\n```bash\nuv run mcp dev src/mcp_reddit_companion/server.py\n```\n\nor \n\n2. Install in Claude Desktop\n```bash\nuv run mcp install src/mcp_reddit_companion/server.py \n```\n\n### Docker Usage\n\nThe MCP server will start automatically when the container runs. Connect your LLM client to interact with your Reddit feeds.\n\n## Configuration\n\n### Cursor Integration\n\nTo use with Cursor, add the following to your `~/.cursor/mcp.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"reddit-companion\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"--directory\",\n        \"C:\\\\code\\\\personal\\\\mcp-reddit\",\n        \"run\",\n        \"mcp\",\n        \"dev\",\n        \"src/mcp_reddit_companion/server.py\"\n      ]\n    }\n  }\n}\n```\n\n## License\n\nMIT\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "feeds",
        "search",
        "personalized",
        "reddit companion",
        "reddit feeds",
        "personalized reddit"
      ],
      "category": "web-search"
    },
    "saginawj--mcp-server-youtube": {
      "owner": "saginawj",
      "name": "mcp-server-youtube",
      "url": "https://github.com/saginawj/mcp-server-youtube",
      "imageUrl": "/freedevtools/mcp/pfp/saginawj.webp",
      "description": "Enable natural language interaction with YouTube to fetch trending videos and recent uploads from subscribed channels. Seamlessly access and manage YouTube content without manual API handling.",
      "stars": 0,
      "forks": 0,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-04-27T19:09:56Z",
      "readme_content": "[![smithery badge](https://smithery.ai/badge/@saginawj/mcp-youtube-companion)](https://smithery.ai/server/@saginawj/mcp-youtube-companion)\n\n# MCP YouTube Companion\n\nAn MCP tool that enables natural language interaction with your personal YouTube experience. Fetch trending videos, get recent uploads from your subscribed channels, and interact with YouTube content via your favorite LLM client.\n\n## Overview\n\nThis MCP tool allows you to interact with YouTube through natural language queries, providing an easy way to access trending videos and user-specific content, such as recent uploads from subscribed channels, all without needing to manually interact with the YouTube API.\n\n## Example LLM Commands\n\nHere are some example commands you can use with your LLM client:\n\n```python\n# Fetch Trending Videos\n\"Show me the latest trending videos on YouTube\"\n\"What's trending in the US on YouTube?\"\n\n# Fetch Subscribed Channel Feeds\n\"Show me the latest uploads from my subscribed channels\"\n\"What's the most recent video from the 'Tech' channel?\"\n\n# User Activity\n\"What was my latest activity on YouTube?\"\n\"Show me my most recent comments on videos\"\n\"Have I uploaded anything recently?\"\n```\n\n## Prerequisites\n\nA Google account with YouTube access\n\nYouTube API credentials (Client ID, Client Secret, and Refresh Token)\n\n\n## Authentication\n\n### Get YouTube API Credentials:\n\nNavigate to the Google Cloud Console.\n\nCreate a project and enable the YouTube Data API v3.\n\nGenerate OAuth credentials (Client ID, Client Secret) and get your refresh token.\n\n### Set Up OAuth Authentication:\n\nUse/build a frontend/api that OAuth with Google to exchange an access code for a refresh token, then use the refresh token to connect with the MCP Server\n\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "youtube",
        "saginawj",
        "web",
        "server youtube",
        "manage youtube",
        "youtube fetch"
      ],
      "category": "web-search"
    },
    "salamentic--google-flights-mcp": {
      "owner": "salamentic",
      "name": "google-flights-mcp",
      "url": "https://github.com/salamentic/google-flights-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/salamentic.webp",
      "description": "Create travel plans with detailed flight searches, generate itineraries, and access airport code information. Utilize templates for common travel-related queries to streamline the planning process.",
      "stars": 6,
      "forks": 3,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-09-21T15:31:39Z",
      "readme_content": "# Flight Planner MCP Server\n\nA Model Context Protocol server that creates travel agent-level flight plans using the fast-flights API.\n\n## Features\n\n- Search for one-way and round-trip flights\n- Create comprehensive travel plans based on trip parameters\n- Get airport code information\n- Use templates for common travel queries\n\n## Installation\n\n1. Make sure you have Python 3.10 or higher installed\n2. Install the required packages:\n\n```bash\npip install mcp fast-flights\n```\n\n## Usage\n\n### Running the Server\n\nYou can run the server directly:\n\n```bash\npython flight_planner_server.py\n```\n\n### Integrating with Claude Desktop\n\n1. Install [Claude Desktop](https://claude.ai/download)\n2. Create or edit your Claude Desktop configuration file:\n   - macOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\n   - Windows: `%APPDATA%\\Claude\\claude_desktop_config.json`\n\n3. Add the flight-planner server configuration:\n\n```json\n{\n  \"mcpServers\": {\n    \"flight-planner\": {\n      \"command\": \"python\",\n      \"args\": [\n        \"/PATH/TO/flight_planner_server.py\"\n      ],\n      \"env\": {\n        \"PYTHONPATH\": \"/PATH/TO/PROJECT\"\n      }\n    }\n  }\n}\n```\n\n4. Replace `/PATH/TO/` with the actual path to your server file\n5. Restart Claude Desktop\n\n### Using the MCP Inspector\n\nFor testing and development, you can use the MCP Inspector:\n\n```bash\n# Install the inspector\nnpm install -g @modelcontextprotocol/inspector\n\n# Run the inspector with your server\nnpx @modelcontextprotocol/inspector python flight_planner_server.py\n```\n\n## Available Tools\n\n- `search_one_way_flights`: Search for one-way flights between airports\n- `search_round_trip_flights`: Search for round-trip flights between airports  \n- `create_travel_plan`: Generate a comprehensive travel plan\n\n## Available Resources\n\n- `airport_codes://{query}`: Get airport code information based on a search query\n\n## Available Prompts\n\n- `flight_search_prompt`: Template for searching flights\n- `travel_plan_prompt`: Template for creating a comprehensive travel plan\n\n## Example Queries for Claude\n\nOnce integrated with Claude Desktop, you can ask things like:\n\n- \"What flights are available from NYC to SFO on 2025-04-15?\"\n- \"Can you create a travel plan for my business trip from LAX to TPE from 2025-05-01 to 2025-05-08?\"\n- \"Help me find airport codes for Tokyo.\"\n- \"What's the best time to book flights from Boston to London for a summer vacation?\"\n\n## License\n\nMIT\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "flights",
        "planning",
        "flight",
        "flights mcp",
        "google flights",
        "flight searches"
      ],
      "category": "web-search"
    },
    "sanjoy1234--multi-search-mcp": {
      "owner": "sanjoy1234",
      "name": "multi-search-mcp",
      "url": "https://github.com/sanjoy1234/multi-search-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/sanjoy1234.webp",
      "description": "Enables multi-source search capabilities by unifying multiple search engines through a simple API. Facilitates the retrieval of information from Google, Brave News, and DuckDuckGo, presenting results in a consolidated format.",
      "stars": 0,
      "forks": 0,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-04-21T18:40:33Z",
      "readme_content": "[![smithery badge](https://smithery.ai/badge/@sanjoy1234/multi-search-mcp)](https://smithery.ai/server/@sanjoy1234/multi-search-mcp)\n\n# Multi‑Search MCP Server\n\nA lightweight MCP server that **unifies multiple search engines** under one simple stdio API. Query Google, Brave News, and DuckDuckGo—without juggling three different SDKs or payload formats.\n\n---\n\n## 🔍 What It Does\n\n- **Google Search** via SerpApi (JSON‑first, optional key)\n- **Brave News** retrieval through Brave’s News API (key required)\n- **DuckDuckGo Instant Answers** via public API (no key needed)\n- Exposes each as a distinct MCP “tool” for chatbots, agents, or local scripts\n\n---\n\n## 🚀 Why It Matters\n\n- **One API to rule them all**  \n  Call `google_search()`, `brave_search()`, or `duck_search()`, and get back ready‑to‑consume JSON without provider boilerplate.\n\n- **Rapid tool development**  \n  Spin up new search‑powered agents in minutes, not days.\n\n- **Minimal key management**  \n  Only Brave needs a key; DuckDuckGo works out‑of‑the‑box; Google key is optional.\n\n- **Easy deployment**  \n  Deploy via Docker or Smithery.ai with a single click.\n\n---\n\nFor feedback or issues, join us on the Smithery Discord or open an issue in this repo.\n\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "search",
        "google",
        "retrieval",
        "multi search",
        "multiple search",
        "search engines"
      ],
      "category": "web-search"
    },
    "scrapeless-ai--scrapeless-mcp-server": {
      "owner": "scrapeless-ai",
      "name": "scrapeless-mcp-server",
      "url": "https://github.com/scrapeless-ai/scrapeless-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/scrapeless-ai.webp",
      "description": "Retrieve and summarize data from Google Search results, including various Google services, to enhance AI decision-making capabilities. Integrate real-time search context into AI applications for improved interaction and research tools.",
      "stars": 53,
      "forks": 10,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-09-30T09:42:40Z",
      "readme_content": "# Scrapeless MCP Server\n\n**Welcome to the official Scrapeless Model Context Protocol (MCP) Server** — a powerful integration layer that empowers LLMs, AI Agents, and AI applications to interact with the web in real time.\n\nBuilt on the open MCP standard, Scrapeless MCP Server seamlessly connects models like **ChatGPT**, **Claude**, and tools like **Cursor** and **Windsurf** to a wide range of external capabilities, including:\n\n- **Google services integration** (Search, Trends)\n- **Browser automation** for page-level navigation and interaction\n- **Scrape** dynamic, JS-heavy sites—export as HTML, Markdown, or screenshots\n\nWhether you're building an AI research assistant, a coding copilot, or autonomous web agents, this server provides the dynamic context and real-world data your workflows need—**without getting blocked**.\n\n## Usage Examples\n\n1. Automated Web Interaction and Data Extraction with Claude\n\nUsing Scrapeless MCP Browser, Claude can perform complex tasks such as web navigation, clicking, scrolling, and scraping through conversational commands, with real-time preview of web interaction results via `live sessions`.\n\n\n\n2. Bypassing Cloudflare to Retrieve Target Page Content\n\nUsing the Scrapeless MCP Browser service, the Cloudflare page is automatically accessed, and after the process is completed, the page content is extracted and returned in Markdown format.\n\n\n\n3. Extracting Dynamically Rendered Page Content and Writing to File\n\nUsing the Scrapeless MCP Universal API, the JavaScript-rendered content of the target page above is scraped, exported in Markdown format, and finally written to a local file named **`text.md`**.\n\n\n\n4. Automated SERP Scraping\n\nUsing the Scrapeless MCP Server, query the keyword “web scraping” on Google Search, retrieve the first 10 search results (including title, link, and summary), and write the content to the file named `serp.text`.\n\n\n\nHere are some additional examples of how to use these servers:\n\n| Example                                                                                                                           |\n| --------------------------------------------------------------------------------------------------------------------------------- |\n| Search scrapeless by Google search.                                                                                               |\n| Find the search interest for \"AI\" over the last year.                                                                             |\n| Use a browser to visit [chatgpt.com](http://chatgpt.com), search for \"What's the weather like today?\", and summarize the results. |\n| Scrape the HTML content of [scrapeless.com](http://scrapeless.com) page.                                                          |\n| Scrape the Markdown content of [scrapeless.com](http://scrapeless.com) page.                                                      |\n| Get screenshots of [scrapeless.com](http://scrapeless.com).                                                                       |\n\n## Setup Guide\n\n1. Get Scrapeless Key\n\n- [Log in](https://app.scrapeless.com/passport/login?utm_source=github&utm_medium=github-mcp&utm_campaign=mcp) to the Scrapeless Dashboard（Free trial available）\n- Then click \"**Setting**\" on the left -> select \"**API Key Management**\" -> click \"**Create API Key**\". Finally, click the API Key you created to **copy** it.\n\n\n\n2. Configure Your MCP Client\n\nScrapeless MCP Server supports both **Stdio** and **Streamable HTTP** transport modes.\n\n🖥️ Stdio (Local Execution)\n\n```JSON\n{\n  \"mcpServers\": {\n    \"Scrapeless MCP Server\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"scrapeless-mcp-server\"],\n      \"env\": {\n        \"SCRAPELESS_KEY\": \"YOUR_SCRAPELESS_KEY\"\n      }\n    }\n  }\n}\n```\n\n🌐 Streamable HTTP (Hosted API Mode)\n\n```JSON\n{\n  \"mcpServers\": {\n    \"Scrapeless MCP Server\": {\n      \"type\": \"streamable-http\",\n      \"url\": \"https://api.scrapeless.com/mcp\",\n      \"headers\": {\n        \"x-api-token\": \"YOUR_SCRAPELESS_KEY\"\n      },\n      \"disabled\": false,\n      \"alwaysAllow\": []\n    }\n  }\n}\n```\n\n#### Advanced Options\n\nCustomize browser session behavior with optional parameters. These can be set via environment variables (for Stdio) or HTTP headers (for Streamable HTTP):\n\n| Stdio (Env Var)         | Streamable HTTP (HTTP Header) | Description                                                                                                                  |\n| ----------------------- | ----------------------------- | ---------------------------------------------------------------------------------------------------------------------------- |\n| BROWSER_PROFILE_ID      | x-browser-profile-id          | Specifies a reusable browser profile ID for session continuity.                                                              |\n| BROWSER_PROFILE_PERSIST | x-browser-profile-persist     | Enables persistent storage for cookies, local storage, etc.                                                                  |\n| BROWSER_SESSION_TTL     | x-browser-session-ttl         | Defines the **maximum session timeout** in seconds. The session will automatically expire after this duration of inactivity. |\n\n## Integration with Claude Desktop\n\n1. Open **Claude Desktop**\n2. Navigate to: `Settings` → `Tools` → `MCP Servers`\n3. Click **\"Add MCP Server\"**\n4. Paste either the `Stdio` or `Streamable HTTP` config above\n5. Save and enable the server\n6. Claude will now be able to issue web queries, extract content, and interact with pages using Scrapeless\n\n## Integration with Cursor IDE\n\n1. Open **Cursor**\n2. Press `Cmd + Shift + P` and search for: `Configure MCP Servers`\n3. Add the Scrapeless MCP config using the format above\n4. Save the file and restart Cursor (if needed)\n5. Now you can ask Cursor things like:\n   1. `\"Search StackOverflow for a solution to this error\"`\n   2. `\"Scrape the HTML from this page\"`\n6. And it will use Scrapeless in the background.\n\n## Supported MCP Tools\n\n| Name               | Description                                                    |\n| ------------------ | -------------------------------------------------------------- |\n| google_search      | Universal information search engine.                           |\n| google_trends      | Get trending search data from Google Trends.                   |\n| browser_create     | Create or reuse a cloud browser session using Scrapeless.      |\n| browser_close      | Closes the current session by disconnecting the cloud browser. |\n| browser_goto       | Navigate browser to a specified URL.                           |\n| browser_go_back    | Go back one step in browser history.                           |\n| browser_go_forward | Go forward one step in browser history.                        |\n| browser_click      | Click a specific element on the page.                          |\n| browser_type       | Type text into a specified input field.                        |\n| browser_press_key  | Simulate a key press.                                          |\n| browser_wait_for   | Wait for a specific page element to appear.                    |\n| browser_wait       | Pause execution for a fixed duration.                          |\n| browser_screenshot | Capture a screenshot of the current page.                      |\n| browser_get_html   | Get the full HTML of the current page.                         |\n| browser_get_text   | Get all visible text from the current page.                    |\n| browser_scroll     | Scroll to the bottom of the page.                              |\n| browser_scroll_to  | Scroll a specific element into view.                           |\n| scrape_html        | Scrape a URL and return its full HTML content.                 |\n| scrape_markdown    | Scrape a URL and return its content as Markdown.               |\n| scrape_screenshot  | Capture a high-quality screenshot of any webpage.              |\n\n## Security Best Practices\n\nWhen using Scrapeless MCP Server with LLMs (like ChatGPT, Claude, or Cursor), it's critical to handle all scraped or extracted web content with care. **Web data is untrusted by default**, and improper handling may expose your application to prompt injection or other security vulnerabilities.\n\n#### ✅ Recommended Practices\n\n- **Never pass raw scraped content directly into LLM prompts.** Raw HTML, JavaScript, or user-generated text may contain hidden injection payloads.\n- **Sanitize and validate all extracted content.** Strip or escape potentially harmful tags and scripts before using content in downstream logic or AI models.\n- **Prefer structured extraction over free-form text.** Use tools like `scrape_html`, `scrape_markdown`, or targeted `browser_get_text` with known-safe selectors to extract only the content you trust.\n- **Apply domain or selector whitelisting** when scraping dynamically generated pages, to restrict data flow to known and trusted sources.\n- **Log and monitor all outbound requests** made via browser or scraping tools, especially if you're handling sensitive data, tokens, or internal network access.\n\n#### 🚫 Avoid\n\n- Injecting scraped HTML directly into prompts\n- Letting users specify arbitrary URLs or CSS selectors without validation\n- Storing unfiltered scraped content for future prompt usage\n\n## Community\n\n- [MCP Server Discord](https://backend.scrapeless.com/app/api/v1/public/links/discord)\n\n## Contact Us\n\nFor questions, suggestions, or collaboration inquiries, feel free to contact us via:\n\n- Email: [market@scrapeless.com](mailto:market@scrapeless.com)\n- Official Website: [https://www.scrapeless.com](https://www.scrapeless.com/)\n- Community Forum: https://discord.gg/Np4CAHxB9a",
      "npm_url": "https://www.npmjs.com/package/scrapeless-mcp-server",
      "npm_downloads": 7193,
      "keywords": [
        "google",
        "search",
        "ai",
        "scrapeless ai",
        "ai scrapeless",
        "search scrapeless"
      ],
      "category": "web-search"
    },
    "sdairs--claudekeep": {
      "owner": "sdairs",
      "name": "claudekeep",
      "url": "https://github.com/sdairs/claudekeep",
      "imageUrl": "/freedevtools/mcp/pfp/sdairs.webp",
      "description": "A server implementation that enables the saving and sharing of AI conversations from Claude Desktop, featuring both a private chat storage and a public chat display web app. This implementation utilizes the Model Context Protocol (MCP) to manage interactions with AI chat logs.",
      "stars": 10,
      "forks": 6,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-09-24T20:22:05Z",
      "readme_content": "Anthropic sent a trademark take down notice for claudekeep, so this little experiment is over. The code remains available in case its useful for anyone working with MCP.\n\nIf it wasn't clear enough, Claudekeep is not affiliated in any way with, nor endorsed by, Anthropic.\n\n# ClaudeKeep\n\nClaudeKeep is an experiment with Model Context Protocol (MCP) to save and share your AI conversations from Claude Desktop.\n\nIt includes:\n- an MCP server implementation that allows you to ask Claude to save your chat\n- a web app that allows you to view your private chats and see public chats\n\n## WARNING - THIS IS AN EXPERIMENT\n\nThis is an experiment. Please do not assume that it works perfectly or is secure.\n\nWhile I have done some testing, I make no guarantees that I've caught every edge case to make sure your chats are not exposed. I suggest you don't test it with sensitive chats.\n\n## How to use it?\n\n### 1. Login and get a token\n\nGo to [https://claudekeep.com](https://claudekeep.com) and hit **Login**. This will attempt to log you in via OAuth with GitHub. At the moment, this is the only OAuth provider supported.\n\nOnce logged in, in the top right you'll see a box with a JWT token, copy it.\n\n### 2. Configure Claude Desktop to use the MCP server\n\nTo use with Claude Desktop, you need to add the server config to the following file:\n\nOn MacOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\n\nOn Windows: `%APPDATA%/Claude/claude_desktop_config.json`\n\nUse this config and then restart Claude Desktop (you must completely kill it CMD+Q style and then restart it):\n\n```json\n{\n  \"mcpServers\": {\n    \"claudekeep-mcp\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"claudekeep-mcp\"\n      ],\n      \"env\": {\n        \"CLAUDEKEEP_TOKEN\": \"<YOUR_TOKEN>\"\n      }\n    }\n  }\n}\n```\n\nClaude Desktop can be awkward with reading your PATH. See the [MCP readme](./apps/mcp/README.md) for more info if the MCP server doesn't work.\n\n### 3. Select the MCP and chat with Claude\n\nWhen you open Claude Desktop, there is a little paperclip icon. Hover over it and there will be a little plug icon. Click that and pick `default` under `claudekeep-mcp`. This will attach the default prompt to Claude. \n\nNow just chat with Claude as normal.\n\nEvery message you write will trigger the `store_message` tool. This will store the message locally in Claude Desktop.\n\nWhen you want to save your chat, just ask Claude. Claude will run the `save_chat` tool. By default chats are always stored as private. If you want to make your chat public straight away, let claude know when you ask it to save.\n\nFor example (but remember, it's an LLM, it's interpreting your langauge, so you can ask however you want and it will probably hopefully do the right thing):\n\nTo save a a private chat:\n```\nyou: save this chat\n```\n\nTo save a public chat:\n```\nyou: save this chat and make it public\n```\n\n## Need help?\n\nRaise an issue or contact me on [BlueSky](https://bsky.app/profile/alasdairb.com).\n\n## Refresh your token\n\nIf you accidentally expose your token, login and hit the little refresh icon next to the token. You'll see a warning, click the confirm button and it will generate a new token. The old token will be destroyed and is not recoverable.\n\n## Abuse\n\nI hope people are good and don't share dodgy chats, but it's the internet, so 🤷‍♂️ it'll probably happen. I'll do my best to catch it, but please nudge me on BlueSky if I miss something.\n\nNote that your chats are stored against your GitHub account, so while public chats are anonymous to other users, they're not anonymous on the server.\n",
      "npm_url": "https://www.npmjs.com/package/claudekeep-mcp",
      "npm_downloads": 394,
      "keywords": [
        "chat",
        "conversations",
        "search",
        "ai chat",
        "ai conversations",
        "chat storage"
      ],
      "category": "web-search"
    },
    "sengokudaikon--mcp-perplexity": {
      "owner": "sengokudaikon",
      "name": "mcp-perplexity",
      "url": "https://github.com/sengokudaikon/mcp-perplexity",
      "imageUrl": "/freedevtools/mcp/pfp/sengokudaikon.webp",
      "description": "Integrate with the Perplexity API to query programming responses and manage chat history. Maintain conversations and list previous chats using a local data storage system.",
      "stars": 0,
      "forks": 0,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-04-07T11:16:21Z",
      "readme_content": "# Perplexity Chat MCP Server\n\nThe Perplexity MCP Server provides a Python-based interface to the Perplexity API, offering tools for querying responses, maintaining chat history, and managing conversations. It supports model configuration via environment variables and stores chat data locally. Built with Python and setuptools, it's designed for integration with development environments.\n\nThe MCP Server is desined to mimick how users interact with the Perplexity Chat on their browser by allowing your models to ask questions, continue conversations, and list all your chats.\n\n[![smithery badge](https://smithery.ai/badge/@daniel-lxs/mcp-perplexity)](https://smithery.ai/server/@daniel-lxs/mcp-perplexity) [![Release and Publish](https://github.com/daniel-lxs/mcp-perplexity/actions/workflows/release.yml/badge.svg)](https://github.com/daniel-lxs/mcp-perplexity/actions/workflows/release.yml)\n\n\n\n<a href=\"https://glama.ai/mcp/servers/0nggjl0ohi\">\n  <img alt=\"badge\" width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/0nggjl0ohi/badge\" />\n</a>\n\n## Components\n\n### Tools\n\n- **ask_perplexity**: Request expert programming assistance through Perplexity. Focuses on coding solutions, error debugging, and technical explanations. Returns responses with source citations and alternative suggestions.\n- **chat_perplexity**: Maintains ongoing conversations with Perplexity AI. Creates new chats or continues existing ones with full history context. Returns chat ID for future continuation.\n- **list_chats_perplexity**: Lists all available chat conversations with Perplexity AI. Returns chat IDs, titles, and creation dates (displayed in relative time format, e.g., \"5 minutes ago\", \"2 days ago\"). Results are paginated with 50 chats per page.\n- **read_chat_perplexity**: Retrieves the complete conversation history for a specific chat. Returns the full chat history with all messages and their timestamps. No API calls are made to Perplexity - this only reads from local storage.\n\n## Key Features\n\n- **Model Configuration via Environment Variable:**  Allows you to specify the Perplexity model using the `PERPLEXITY_MODEL` environment variable for flexible model selection.\n\n  You can also specify `PERPLEXITY_MODEL_ASK` and `PERPLEXITY_MODEL_CHAT` to use different models for the `ask_perplexity` and `chat_perplexity` tools, respectively.\n\n  These will override `PERPLEXITY_MODEL`. You can check which models are available on the [Perplexity](https://docs.perplexity.ai/guides/model-cards) documentation.\n- **Persistent Chat History:** The `chat_perplexity` tool maintains ongoing conversations with Perplexity AI. Creates new chats or continues existing ones with full history context. Returns chat ID for future continuation.\n- **Streaming Responses with Progress Reporting:** Uses progress reporting to prevent timeouts on slow responses.\n\n## Quickstart\n\n### Prerequisites\n\nBefore using this MCP server, ensure you have:\n\n- Python 3.10 or higher\n- [uvx](https://docs.astral.sh/uv/#installation) package manager installed\n\nNote: Installation instructions for uvx are available [here](https://docs.astral.sh/uv/#installation).\n\n### Configuration for All Clients\n\nTo use this MCP server, configure your client with these settings (configuration method varies by client):\n\n```json\n\"mcpServers\": {\n  \"mcp-perplexity\": {\n    \"command\": \"uvx\",\n    \"args\": [\"mcp-perplexity\"],\n    \"env\": {\n      \"PERPLEXITY_API_KEY\": \"your-api-key\",\n      \"PERPLEXITY_MODEL\": \"sonar-pro\",\n      \"DB_PATH\": \"chats.db\"\n    }\n  }\n}\n```\n\n## Environment Variables\n\nConfigure the MCP Perplexity server using the following environment variables:\n\n| Variable | Description | Default Value | Required |\n|----------|-------------|---------------|----------|\n| `PERPLEXITY_API_KEY` | Your Perplexity API key | None | Yes |\n| `PERPLEXITY_MODEL` | Default model for interactions | `sonar-pro` | No |\n| `PERPLEXITY_MODEL_ASK` | Specific model for `ask_perplexity` tool | Uses `PERPLEXITY_MODEL` | No |\n| `PERPLEXITY_MODEL_CHAT` | Specific model for `chat_perplexity` tool | Uses `PERPLEXITY_MODEL` | No |\n| `DB_PATH` | Path to store chat history database | `chats.db` | No |\n| `WEB_UI_ENABLED` | Enable or disable web UI | `false` | No |\n| `WEB_UI_PORT` | Port for web UI | `8050` | No |\n| `WEB_UI_HOST` | Host for web UI | `127.0.0.1` | No |\n| `DEBUG_LOGS` | Enable detailed logging | `false` | No |\n\n#### Using Smithery CLI\n```bash\nnpx -y @smithery/cli@latest run @daniel-lxs/mcp-perplexity --config \"{\\\"perplexityApiKey\\\":\\\"pplx-abc\\\",\\\"perplexityModel\\\":\\\"sonar-pro\\\"}\"\n```\n\n## Usage\n\n### ask_perplexity\n\nThe `ask_perplexity` tool is used for specific questions, this tool doesn't maintain a chat history, every request is a new chat.\n\nThe tool will return a response from Perplexity AI using the `PERPLEXITY_MODEL_ASK` model if specified, otherwise it will use the `PERPLEXITY_MODEL` model.\n\n### chat_perplexity\n\nThe `chat_perplexity` tool is used for ongoing conversations, this tool maintains a chat history.\nA chat is identified by a chat ID, this ID is returned by the tool when a new chat is created. Chat IDs look like this: `wild-horse-12`.\n\nThis tool is useful for debugging, research, and any other task that requires a chat history.\n\nThe tool will return a response from Perplexity AI using the `PERPLEXITY_MODEL_CHAT` model if specified, otherwise it will use the `PERPLEXITY_MODEL` model.\n\n### list_chats_perplexity\nLists all available chat conversations.  It returns a paginated list of chats, showing the chat ID, title, and creation time (in relative format).  You can specify the page number using the `page` argument (defaults to 1, with 50 chats per page).\n\n### read_chat_perplexity\nRetrieves the complete conversation history for a given `chat_id`.  This tool returns all messages in the chat, including timestamps and roles (user or assistant). This tool does *not* make any API calls to Perplexity; it only reads from the local database.\n\n## Web UI\n\nThe MCP Perplexity server now includes a web interface for easier interaction and management of chats.\n\n### Features\n- Interactive chat interface\n- Chat history management\n- Real-time message display\n\n### Screenshots\n\n#### Chat List View\n![image](https://github.com/user-attachments/assets/a8aebd19-f58a-4d6c-988e-ea1c1ca7f174)\n\n#### Chat Interface\n![image](https://github.com/user-attachments/assets/627bfcdb-2214-47e6-a55e-3987737ad00f)\n\n### Accessing the Web UI\n\nWhen `WEB_UI_ENABLED` is set to `true`, the web UI will be available at `http://WEB_UI_HOST:WEB_UI_PORT`. \n\nBy default, this is `http://127.0.0.1:8050`.\n\n## Development\n\nThis project uses setuptools for development and builds. To get started:\n\n1. Create a virtual environment:\n   ```bash\n   python -m venv .venv\n   source .venv/bin/activate  # On Linux/macOS\n   # or\n   .venv\\Scripts\\activate  # On Windows\n   ```\n\n2. Install the project in editable mode with all dependencies:\n   ```bash\n   pip install -e .\n   ```\n\n3. Build the project:\n   ```bash\n   python -m build\n   ```\n\nThe virtual environment will contain all required dependencies for development.\n\n## Contributing\n\nThis project is open to contributions. Please see the [CONTRIBUTING.md](CONTRIBUTING.md) file for more information.\n\n## License\n\nThis project is licensed under the MIT License. See the [LICENSE](LICENSE) file for details.\n\n\n\n\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "chats",
        "chat",
        "perplexity",
        "perplexity api",
        "chats using",
        "chat history"
      ],
      "category": "web-search"
    },
    "sengokudaikon--opendeepsearch_mcp": {
      "owner": "sengokudaikon",
      "name": "opendeepsearch_mcp",
      "url": "https://github.com/sengokudaikon/opendeepsearch_mcp",
      "imageUrl": "/freedevtools/mcp/pfp/sengokudaikon.webp",
      "description": "Interact with OpenDeepSearch's search functionalities through a standardized interface, enabling LLM applications to leverage advanced web search capabilities. Integrates seamlessly with MCP-compatible clients for enhanced search integration.",
      "stars": 2,
      "forks": 1,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-09-24T09:05:06Z",
      "readme_content": "# OpenDeepSearch MCP Server\n\nThis is a Model Context Protocol (MCP) server for OpenDeepSearch that allows LLM applications to interact with OpenDeepSearch's search capabilities.\n\n## Features\n\n- Exposes OpenDeepSearch's search functionality as MCP tools\n- Integrates with Claude Desktop and other MCP-compatible clients\n- Provides a standardized interface for LLM applications to access web search capabilities\n\n## Setup\n\nThis project uses `uv` for dependency management.\n\n1.  **Install `uv`**: Follow the instructions [here](https://docs.astral.sh/uv/install/).\n2.  **Sync Dependencies**: Navigate to the `mcp_server` directory and run:\n    ```bash\n    uv sync\n    ```\n    This will install dependencies based on `pyproject.toml` and `uv.lock`.\n\n## Configuration\n\nThe server requires certain environment variables to function correctly, especially API keys for the underlying services. These can be set directly in your environment or passed via the MCP client configuration (e.g., using Smithery CLI).\n\n| Variable               | Description                                                                      | Required | Default | Notes                                                                                         |\n|:-----------------------|:---------------------------------------------------------------------------------|:---------|:--------|:----------------------------------------------------------------------------------------------|\n| **LLM Providers**      | **(Provide at least one)**                                                       |          |         |                                                                                               |\n| `OPENAI_API_KEY`       | API key for OpenAI LLM.                                                          | Optional | None    | Needed if using OpenAI models.                                                                |\n| `OPENAI_BASE_URL`      | Custom base URL for OpenAI compatible endpoints.                                 | Optional | None    |                                                                                               |\n| `ANTHROPIC_API_KEY`    | API key for Anthropic LLM.                                                       | Optional | None    | Needed if using Anthropic models.                                                             |\n| `OPENROUTER_API_KEY`   | API key for OpenRouter.                                                          | Optional | None    | Needed if using OpenRouter models.                                                            |\n| `FIREWORKS_API_KEY`    | API key for Fireworks AI.                                                        | Optional | None    | Needed if using Fireworks models.                                                             |\n| `GEMINI_API_KEY`       | API key for Google Gemini.                                                       | Optional | None    | Needed if using Gemini models.                                                                |\n| `AZURE_API_KEY`        | API key for Azure OpenAI Service.                                                | Optional | None    | Needed if using Azure OpenAI models.                                                          |\n| `AZURE_API_BASE`       | API base URL for Azure OpenAI Service.                                           | Optional | None    | Needed if using Azure OpenAI models.                                                          |\n| `AZURE_API_VERSION`    | API version for Azure OpenAI Service.                                            | Optional | None    | Needed if using Azure OpenAI models.                                                          |\n| `AZURE_DEPLOYMENT_ID`  | Deployment ID for Azure OpenAI Service.                                          | Optional | None    | Needed if using Azure OpenAI models.                                                          |\n| `DEEPSEEK_API_KEY`     | API key for DeepSeek.                                                            | Optional | None    | Needed if using DeepSeek models.                                                              |\n| **Search Providers**   |                                                                                  |          |         |                                                                                               |\n| `SERPER_API_KEY`       | API key for Serper search provider.                                              | Optional | None    | Required if `search_provider` is set to `'serper'` (either by default or via tool argument).  |\n| `SEARXNG_INSTANCE_URL` | URL of your SearXNG instance.                                                    | Optional | None    | Required if `search_provider` is set to `'searxng'` (either by default or via tool argument). |\n| `SEARXNG_API_KEY`      | API key for your SearXNG instance (if required by the instance).                 | Optional | None    | Used if `search_provider` is set to `'searxng'`.                                              |\n| **Rerankers**          |                                                                                  |          |         |                                                                                               |\n| `JINA_API_KEY`         | API key for Jina AI Reranker.                                                    | Optional | None    | Required if `reranker` is set to `'jina'` (either by default or via tool argument).           |\n| **Other Tools**        |                                                                                  |          |         |                                                                                               |\n| `WOLFRAM_ALPHA_APP_ID` | App ID for WolframAlpha tool integration (if enabled in the agent).              | Optional | None    |                                                                                               |\n| **Server Behavior**    |                                                                                  |          |         |                                                                                               |\n| `LOG_LEVEL`            | Controls the server's logging verbosity (DEBUG, INFO, WARNING, ERROR, CRITICAL). | Optional | INFO    | Can also be set via the `--log-level` CLI argument passed by `smithery.yaml`.                 |\n\n**Note:** API keys passed directly as arguments to the `perform_search` tool (`serper_api_key`, `searxng_api_key`, `jina_api_key`) will temporarily override the environment variables for that specific call.\n\n## Usage with Smithery CLI\n\nYou can run this server using the Smithery CLI and the provided `smithery.yaml` configuration file. This allows you to easily manage the required environment variables.\n\n```bash\n# Example: Run with OpenRouter key and Serper key\nnpx -y @smithery/cli@latest run . --config '{\"openrouterApiKey\":\"sk-or-...\", \"serperApiKey\":\"your-serper-key\"}'\n\n# Example: Run with OpenAI key and SearXNG\nnpx -y @smithery/cli@latest run . --config '{\"openaiApiKey\":\"sk-...\", \"searxngInstanceUrl\":\"https://your-searxng-instance.com\"}'\n\n# Example: Run with Gemini key\nnpx -y @smithery/cli@latest run . --config '{\"geminiApiKey\":\"...\"}'\n\n# Example: Run with Azure keys\nnpx -y @smithery/cli@latest run . --config '{\"azureApiKey\":\"...\", \"azureApiBase\":\"https://your-azure.openai.azure.com/\", \"azureApiVersion\":\"2024-02-01\", \"azureDeploymentId\":\"your-deployment\"}'\n```\n\nThe `smithery.yaml` file defines the necessary configuration schema. Refer to it for all available options.\n\n## Development\n\nThis package follows the MCP specification and provides tools for search functionality through OpenDeepSearch.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "opendeepsearch_mcp",
        "search",
        "opendeepsearch",
        "opendeepsearch search",
        "search integration",
        "search capabilities"
      ],
      "category": "web-search"
    },
    "shalomeir--daily": {
      "owner": "shalomeir",
      "name": "daily",
      "url": "https://github.com/shalomeir/daily",
      "imageUrl": "/freedevtools/mcp/pfp/shalomeir.webp",
      "description": "Provides a personalized news feed for developers, connects them to communities, and enables search functionalities to facilitate learning and collaboration in the developer ecosystem.",
      "stars": 0,
      "forks": 0,
      "license": "GNU Affero General Public License v3.0",
      "language": "",
      "updated_at": "2025-04-16T04:32:42Z",
      "readme_content": "<a name=\"readme-top\"></a>\n\n<div align=\"center\">\n\n\n\n<br>\n\n# Welcome to the daily.dev repository\n\nWe know how hard it is to be a developer. It doesn't have to be. <br/>\ndaily.dev is the homepage every developer deserves. <br/>\n Personalized news feed, dev communities and search, much better than what’s out there. Maybe ;)\n\n[Product Docs][product-docs-link] · [Changelog][changelog-link] · [Report a Bug][report-bug-link] · [Request a Feature][github-discussions-link] · [Swag Store][swag-store-link] · [Brand Assets][brand-book-link]\n\n<!-- SHIELD GROUP -->\n\n[![][chrome-users-shield]][chrome-users-link]\n[![][extension-rating-shield]][extension-rating-link]\n[![][latest-version-shield]][latest-version-link]<br/>\n[![][github-stars-shield]][github-stars-link]\n[![][github-license-shield]][github-license-link]<br/>\n\n**Help more developers suffer less by sharing daily.dev**\n\n[![][share-x-shield]][share-x-link]\n[![][share-telegram-shield]][share-telegram-link]\n[![][share-whatsapp-shield]][share-whatsapp-link]\n[![][share-reddit-shield]][share-reddit-link]\n[![][share-mastodon-shield]][share-mastodon-link]\n[![][share-linkedin-shield]][share-linkedin-link]\n\n<sup>Want to contribute? Get started with Gitpod by clicking the button below</sup>\n\n<p align=\"center\">\n  <a href=\"https://gitpod.io/#https://github.com/dailydotdev/apps/\">\n    <img src=\"https://gitpod.io/button/open-in-gitpod.svg\" alt=\"Open in Gitpod\">\n  </a>\n</p>\n\n\n</br>\n<a href=\"https://youtu.be/igZCEr3HwCg\"><strong>👀 Watch it in action → </strong></a>\n\n</div>\n\n## 💜 About daily.dev\n\n> daily.dev is a professional network for developers to learn, collaborate, and grow together\n 👩‍💻\n\nWith daily.dev you can discover a wide variety of professional knowledge, create groups where you can collaborate with other developers you appreciate, and discuss the latest trends in the developer ecosystem. It works offline and is available both as a browser extension and as a Progressive Web App (PWA). \n\n> \\[!IMPORTANT]\n>\n> Star us to show your support and love for daily.dev ⭐️\n\n\n\n<details>\n  <summary><kbd>Star History</kbd></summary>\n  <picture>\n    <source media=\"(prefers-color-scheme: light)\" srcset=\"https://api.star-history.com/svg?repos=dailydotdev%2Fdaily&theme=light&type=Date\">\n    <img alt=\"svg_repos_dailydotdev_daily_type_Date\" width=\"100%\" src=\"https://api.star-history.com/svg?repos=dailydotdev%2Fdaily&type=Date\">\n  </picture>\n</details>\n\n## 📌 Get daily.dev\n\ndaily.dev is currently available for Google Chrome and Microsoft Edge. There's also a PWA for mobile devices. \n\nGet it now on:\n\n<p align=\"center\">\n    <a href=\"https://r.daily.dev/chrome\">\n    <img src=\"https://img.shields.io/badge/%20-Chrome-red?logo=google-chrome&logoColor=white\" alt=\"Download for Chrome\" />\n    </a>\n    <a href=\"https://microsoftedge.microsoft.com/addons/detail/dailydev-news-for-busy/cbdhgldgiancdheindpekpcbkccpjaeb\">\n    <img src=\"https://img.shields.io/badge/%20-Edge-blue?logo=microsoft-edge&logoColor=white\" alt=\"Download for Edge\" />\n    </a>\n    <a href=\"http://go.daily.dev/\">\n    <img src=\"https://img.shields.io/badge/%20-Mobile-502ab0\" alt=\"Download for Mobile\" />\n    </a>\n</p>\n\n## 📯 Philosophy\n\nWe recognize that developers today have the greatest power as a professional group to drive change and affect lives of billions. There are many platforms that provide developers with tools that serve to their success, or the goals of their workplace, but daily.dev is by-design for developers themselves. \n\nWe, as developers, know how challenging it is to grow professionally with so much going on, and that's why we built daily.dev - to make it easy for us to navigate abundance of content and discover all the knowledge they need with zero effort.\n\nYou can use daily.dev to:\n\n* 👨‍💻 Learn and stay up-to-date\n* 🙌 Interact based on the latest trends in our ecosystem\n* 🚀 Collaborate with other developers you know and appreciate\n\n## 🌲 daily.dev projects\n\ndaily.dev might look simple on the surface but actually, it is powered by a complex and robust system of different applications. It contains several services, some are big, others are micro and easy to maintain. Below is the list of different projects that we maintain under daily.dev.\n\n### 🙌 Community & Docs\n\n*  [daily](https://github.com/dailydotdev/daily) - This is the repository you are currently at. It serves as a central place for all the projects. It contains documentation, community ideas, suggestions, and whatnot.\n\n*  [docs](https://github.com/dailydotdev/docs) - This is the repository for official product documentation of daily.dev.\n\n\n### 🎨 Frontend\n\n* [apps](https://github.com/dailydotdev/apps) - Monorepo with all the frontend related projects since daily.dev 3.0. This includes both the extension and the webapp.\n\n### 🎈 Others\n\n* [action-devcard](https://github.com/dailydotdev/action-devcard) - GitHub Actions for adding daily DevCard to GitHub profile. Read [the guide](https://daily.dev/blog/adding-the-daily-devcard-to-your-github-profile) to set it up.\n\n## 🚀 Running daily.dev locally\n\nTo spin up a local environment, we suggest using GitPod. Everything is already configured and should work out of the box. We have a GitPod button above, click on it and let's roll!\n\n> Note that currently, not all services are ready (or needed) for local environment so Daily Redirector and Daily Monetization and Authentication services are not available for you.\n\n## 🙌 Want to contribute?\n\nWe are open to all kinds of contributions. If you want to:\n* 🤔 Suggest a feature\n* 🐛 Report an issue\n* 📖 Improve documentation\n* 👨‍💻 Contribute to the code\n\nYou are more than welcome. Before contributing, kindly check our [guidelines](https://github.com/dailydotdev/.github/blob/master/CONTRIBUTING.md).\n\n## 💬 What do you think of daily.dev?\n\n<div align=\"left\">\n    <p><a href=\"https://twitter.com/dailydotdev/\"><img alt=\"Twitter @dailydotdev\" align=\"center\" src=\"https://img.shields.io/badge/twitter-%231DA1F2.svg?&style=for-the-badge&logo=twitter&logoColor=white\" /></a>&nbsp; Tweet us @dailydotdev to share your thoughts and stay up-to-date. </p>\n    <p><a href=\"https://www.producthunt.com/products/daily-dev\"><img alt=\"daily.dev at ProductHunt\" align=\"center\" src=\"https://img.shields.io/badge/producthunt-%23DA552F.svg?&style=for-the-badge&logo=product-hunt&logoColor=white\" /></a>&nbsp; Check out our Product Hunt page.</p>\n    <p><a href=\"https://daily.dev\"><img alt=\"daily.dev Website\" align=\"center\" src=\"https://img.shields.io/badge/Daily Website-%233693F3.svg?&style=for-the-badge&logo=icloud&logoColor=white\" /></a>&nbsp; Visit our home for a bunch of useful links.</p>\n    <p><a href=\"https://chrome.google.com/webstore/detail/daily-20-source-for-busy/jlmpjdjjbgclbocgajdjefcidcncaied\"><img alt=\"daily.dev at ChomeStore\" align=\"center\" src=\"https://img.shields.io/badge/Chrome Web Store-%234285F4.svg?&style=for-the-badge&logo=google-chrome&logoColor=white\" /></a>&nbsp; See our Chrome Store page to grab the extension or share your feedback.</p>\n    <p><a href=\"https://microsoftedge.microsoft.com/addons/detail/dailydev-news-for-busy/cbdhgldgiancdheindpekpcbkccpjaeb\"><img alt=\"daily.dev at EdgeAddons\" align=\"center\" src=\"https://img.shields.io/badge/Edge Addons-%230078D7.svg?&style=for-the-badge&logo=microsoft-edge&logoColor=white\" /></a>&nbsp; Check us out on Microsoft Edge Addons and let us know your thoughts.</p>\n\n## 📑 License\nLicensed under [AGPL-3.0](https://github.com/dailydotdev/daily/blob/master/LICENSE).\n\n<div align=\"right\">\n\n[![][back-to-top]](#readme-top)\n\n</div>\n\n<!-- LINK GROUP -->\n\n[back-to-top]: https://img.shields.io/badge/-BACK_TO_TOP-151515?style=flat-square\n[product-docs-link]: https://docs.daily.dev/docs/intro\n[changelog-link]: https://app.daily.dev/sources/daily_updates\n[report-bug-link]: https://github.com/dailydotdev/daily/issues/new?assignees=&labels=Type%3A+Bug&projects=&template=---bug-report.yml&title=%F0%9F%90%9B+BUG%3A+\n[github-discussions-link]: https://github.com/dailydotdev/daily/discussions/new?category=feature-requests\n[swag-store-link]: https://store.daily.dev/\n[brand-book-link]: https://brand.daily.dev/\n\n<!-- SHIELDS GROUP -->\n\n[chrome-users-shield]: https://img.shields.io/chrome-web-store/users/jlmpjdjjbgclbocgajdjefcidcncaied?style=flat-square&logo=googlechrome&logoColor=white&label=chrome%20active%20users&labelColor=black&color=9E15D9\n[chrome-users-link]: https://chromewebstore.google.com/detail/dailydev-the-homepage-dev/jlmpjdjjbgclbocgajdjefcidcncaied\n[extension-rating-shield]: https://img.shields.io/amo/rating/daily?style=flat-square&labelColor=black&color=0FC54F\n[extension-rating-link]: https://api.daily.dev/get\n[latest-version-shield]: https://img.shields.io/chrome-web-store/v/jlmpjdjjbgclbocgajdjefcidcncaied?style=flat-square&label=latest%20version&labelColor=black&color=0FC54F\n[latest-version-link]: https://api.daily.dev/get\n[github-stars-shield]: https://img.shields.io/github/stars/dailydotdev/daily?style=flat-square&logo=github&labelColor=black&color=508CF9\n[github-stars-link]: https://github.com/dailydotdev/daily/stargazers\n[github-license-shield]: https://img.shields.io/github/license/dailydotdev/daily?style=flat-square&logo=github&labelColor=black&color=508CF9\n[github-license-link]: https://github.com/dailydotdev/daily/issues\n\n<!-- SHARE BUTTONS GROUP -->\n\n[share-linkedin-link]: https://www.linkedin.com/shareArticle?mini=true&url=https%3A//daily.dev\n[share-linkedin-shield]: https://img.shields.io/badge/-share%20on%20linkedin-black?labelColor=black&logo=linkedin&logoColor=white&style=flat-square\n[share-mastodon-link]: https://mastodon.social/share?text=I%20recently%20started%20using%20daily.dev%20-%20It's%20like%20a%20newsfeed%20but%20just%20for%20dev%20content.%20Pretty%20handy%20for%20staying%20up%20to%20date%20without%20the%20usual%20internet%20rabbit%20hole.%20Might%20be%20a%20nice%20break%20from%20the%20usual%20sites.&url=\n[share-mastodon-shield]: https://img.shields.io/badge/-share%20on%20mastodon-black?labelColor=black&logo=mastodon&logoColor=white&style=flat-square\n[share-reddit-link]: http://www.reddit.com/submit?url=https%3A%2F%2Fdaily.dev&title=I%20recently%20started%20using%20daily.dev%20-%20It's%20like%20a%20newsfeed%20but%20just%20for%20dev%20content.%20Pretty%20handy%20for%20staying%20up%20to%20date%20without%20the%20usual%20internet%20rabbit%20hole.%20Might%20be%20a%20nice%20break%20from%20the%20usual%20sites.\n[share-reddit-shield]: https://img.shields.io/badge/-share%20on%20reddit-black?labelColor=black&logo=reddit&logoColor=white&style=flat-square\n[share-telegram-link]: https://t.me/share/url?url=https%3A//daily.dev&text=I%20recently%20started%20using%20daily.dev%20-%20It's%20like%20a%20newsfeed%20but%20just%20for%20dev%20content.%20Pretty%20handy%20for%20staying%20up%20to%20date%20without%20the%20usual%20internet%20rabbit%20hole.%20Might%20be%20a%20nice%20break%20from%20the%20usual%20sites.\n[share-telegram-shield]: https://img.shields.io/badge/-share%20on%20telegram-black?labelColor=black&logo=telegram&logoColor=white&style=flat-square\n[share-whatsapp-link]: https://api.whatsapp.com/send?text=I%20recently%20started%20using%20daily.dev%20-%20It's%20like%20a%20newsfeed%20but%20just%20for%20dev%20content.%20Pretty%20handy%20for%20staying%20up%20to%20date%20without%20the%20usual%20internet%20rabbit%20hole.%20Might%20be%20a%20nice%20break%20from%20the%20usual%20sites.%20https%3A%2F%2Fdaily.dev\n[share-whatsapp-shield]: https://img.shields.io/badge/-share%20on%20whatsapp-black?labelColor=black&logo=whatsapp&logoColor=white&style=flat-square\n[share-x-link]: https://twitter.com/intent/tweet?text=I%20recently%20started%20using%20daily.dev%20-%20It's%20like%20a%20newsfeed%20but%20just%20for%20dev%20content.%20Pretty%20handy%20for%20staying%20up%20to%20date%20without%20the%20usual%20internet%20rabbit%20hole.%20Might%20be%20a%20nice%20break%20from%20the%20usual%20sites.&url=\n[share-x-shield]: https://img.shields.io/badge/-share%20on%20x-black?labelColor=black&logo=x&logoColor=white&style=flat-square",
      "npm_url": "https://www.npmjs.com/package/daily",
      "npm_downloads": 580,
      "keywords": [
        "shalomeir",
        "search",
        "news",
        "search shalomeir",
        "shalomeir daily",
        "web search"
      ],
      "category": "web-search"
    },
    "shariqriazz--vertex-ai-mcp-server": {
      "owner": "shariqriazz",
      "name": "vertex-ai-mcp-server",
      "url": "https://github.com/shariqriazz/vertex-ai-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/shariqriazz.webp",
      "description": "Provides seamless access to Google Cloud's Vertex AI Gemini models for coding assistance and general query answering, with enhanced query responses grounded in web search and direct knowledge.",
      "stars": 85,
      "forks": 16,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-09-15T07:47:01Z",
      "readme_content": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/shariqriazz-vertex-ai-mcp-server-badge.png)](https://mseep.ai/app/shariqriazz-vertex-ai-mcp-server)\n\n# Vertex AI MCP Server\n[![smithery badge](https://smithery.ai/badge/@shariqriazz/vertex-ai-mcp-server)](https://smithery.ai/server/@shariqriazz/vertex-ai-mcp-server)\n\nThis project implements a Model Context Protocol (MCP) server that provides a comprehensive suite of tools for interacting with Google Cloud's Vertex AI Gemini models, focusing on coding assistance and general query answering.\n\n<a href=\"https://glama.ai/mcp/servers/@shariqriazz/vertex-ai-mcp-server\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@shariqriazz/vertex-ai-mcp-server/badge\" alt=\"Vertex AI Server MCP server\" />\n</a>\n\n## Features\n\n*   Provides access to Vertex AI Gemini models via numerous MCP tools.\n*   Supports web search grounding (`answer_query_websearch`) and direct knowledge answering (`answer_query_direct`).\n*   Configurable model ID, temperature, streaming behavior, max output tokens, and retry settings via environment variables.\n*   Uses streaming API by default for potentially better responsiveness.\n*   Includes basic retry logic for transient API errors.\n*   Minimal safety filters applied (`BLOCK_NONE`) to reduce potential blocking (use with caution).\n\n## Tools Provided\n\n### Query & Generation (AI Focused)\n*   `answer_query_websearch`: Answers a natural language query using the configured Vertex AI model enhanced with Google Search results.\n*   `answer_query_direct`: Answers a natural language query using only the internal knowledge of the configured Vertex AI model.\n*   `explain_topic_with_docs`: Provides a detailed explanation for a query about a specific software topic by synthesizing information primarily from official documentation found via web search.\n*   `get_doc_snippets`: Provides precise, authoritative code snippets or concise answers for technical queries by searching official documentation.\n*   `generate_project_guidelines`: Generates a structured project guidelines document (Markdown) based on a specified list of technologies (optionally with versions), using web search for best practices.\n\n### Research & Analysis Tools\n*   `code_analysis_with_docs`: Analyzes code snippets by comparing them with best practices from official documentation, identifying potential bugs, performance issues, and security vulnerabilities.\n*   `technical_comparison`: Compares multiple technologies, frameworks, or libraries based on specific criteria, providing detailed comparison tables with pros/cons and use cases.\n*   `architecture_pattern_recommendation`: Suggests architecture patterns for specific use cases based on industry best practices, with implementation examples and considerations.\n*   `dependency_vulnerability_scan`: Analyzes project dependencies for known security vulnerabilities, providing detailed information and mitigation strategies.\n*   `database_schema_analyzer`: Reviews database schemas for normalization, indexing, and performance issues, suggesting improvements based on database-specific best practices.\n*   `security_best_practices_advisor`: Provides security recommendations for specific technologies or scenarios, with code examples for implementing secure practices.\n*   `testing_strategy_generator`: Creates comprehensive testing strategies for applications or features, suggesting appropriate testing types with coverage goals.\n*   `regulatory_compliance_advisor`: Provides guidance on regulatory requirements for specific industries (GDPR, HIPAA, etc.), with implementation approaches for compliance.\n*   `microservice_design_assistant`: Helps design microservice architectures for specific domains, with service boundary recommendations and communication patterns.\n*   `documentation_generator`: Creates comprehensive documentation for code, APIs, or systems, following industry best practices for technical documentation.\n\n### Filesystem Operations\n*   `read_file_content`: Read the complete contents of one or more files. Provide a single path string or an array of path strings.\n*   `write_file_content`: Create new files or completely overwrite existing files. The 'writes' argument accepts a single object (`{path, content}`) or an array of such objects.\n*   `edit_file_content`: Makes line-based edits to a text file, returning a diff preview or applying changes.\n*   `list_directory_contents`: Lists files and directories directly within a specified path (non-recursive).\n*   `get_directory_tree`: Gets a recursive tree view of files and directories as JSON.\n*   `move_file_or_directory`: Moves or renames files and directories.\n*   `search_filesystem`: Recursively searches for files/directories matching a name pattern, with optional exclusions.\n*   `get_filesystem_info`: Retrieves detailed metadata (size, dates, type, permissions) about a file or directory.\n*   `execute_terminal_command`: Execute a shell command, optionally specifying `cwd` and `timeout`. Returns stdout/stderr.\n\n### Combined AI + Filesystem Operations\n*   `save_generate_project_guidelines`: Generates project guidelines based on a tech stack and saves the result to a specified file path.\n*   `save_doc_snippet`: Finds code snippets from documentation and saves the result to a specified file path.\n*   `save_topic_explanation`: Generates a detailed explanation of a topic based on documentation and saves the result to a specified file path.\n*   `save_answer_query_direct`: Answers a query using only internal knowledge and saves the answer to a specified file path.\n*   `save_answer_query_websearch`: Answers a query using web search results and saves the answer to a specified file path.\n\n*(Note: Input/output schemas for each tool are defined in their respective files within `src/tools/` and exposed via the MCP server.)*\n\n## Prerequisites\n\n*   Node.js (v18+)\n*   Bun (`npm install -g bun`)\n*   Google Cloud Project with Billing enabled.\n*   Vertex AI API enabled in the GCP project.\n*   Google Cloud Authentication configured in your environment (Application Default Credentials via `gcloud auth application-default login` is recommended, or a Service Account Key).\n\n## Setup & Installation\n\n1.  **Clone/Place Project:** Ensure the project files are in your desired location.\n2.  **Install Dependencies:**\n    ```bash\n    bun install\n    ```\n3.  **Configure Environment:**\n    *   Create a `.env` file in the project root (copy `.env.example`).\n    *   Set the required and optional environment variables as described in `.env.example`.\n        *   Set `AI_PROVIDER` to either `\"vertex\"` or `\"gemini\"`.\n        *   If `AI_PROVIDER=\"vertex\"`, `GOOGLE_CLOUD_PROJECT` is required.\n        *   If `AI_PROVIDER=\"gemini\"`, `GEMINI_API_KEY` is required.\n4.  **Build the Server:**\n    ```bash\n    bun run build\n    ```\n    This compiles the TypeScript code to `build/index.js`.\n\n## Usage (Standalone / NPX)\n\nOnce published to npm, you can run this server directly using `npx`:\n\n```bash\n# Ensure required environment variables are set (e.g., GOOGLE_CLOUD_PROJECT)\nbunx vertex-ai-mcp-server\n```\n\nAlternatively, install it globally:\n\n```bash\nbun install -g vertex-ai-mcp-server\n# Then run:\nvertex-ai-mcp-server\n```\n\n**Note:** Running standalone requires setting necessary environment variables (like `GOOGLE_CLOUD_PROJECT`, `GOOGLE_CLOUD_LOCATION`, authentication credentials if not using ADC) in your shell environment before executing the command.\n\n### Installing via Smithery\n\nTo install Vertex AI Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@shariqriazz/vertex-ai-mcp-server):\n\n```bash\nbunx -y @smithery/cli install @shariqriazz/vertex-ai-mcp-server --client claude\n```\n\n## Running with Cline\n\n1.  **Configure MCP Settings:** Add/update the configuration in your Cline MCP settings file (e.g., `.roo/mcp.json`). You have two primary ways to configure the command:\n\n    **Option A: Using Node (Direct Path - Recommended for Development)**\n\n    This method uses `node` to run the compiled script directly. It's useful during development when you have the code cloned locally.\n\n    ```json\n    {\n      \"mcpServers\": {\n        \"vertex-ai-mcp-server\": {\n          \"command\": \"node\",\n          \"args\": [\n            \"/full/path/to/your/vertex-ai-mcp-server/build/index.js\" // Use absolute path or ensure it's relative to where Cline runs node\n          ],\n          \"env\": {\n            // --- General AI Configuration ---\n            \"AI_PROVIDER\": \"vertex\", // \"vertex\" or \"gemini\"\n            // --- Required (Conditional) ---\n            \"GOOGLE_CLOUD_PROJECT\": \"YOUR_GCP_PROJECT_ID\", // Required if AI_PROVIDER=\"vertex\"\n            // \"GEMINI_API_KEY\": \"YOUR_GEMINI_API_KEY\", // Required if AI_PROVIDER=\"gemini\"\n            // --- Optional Model Selection ---\n            \"VERTEX_MODEL_ID\": \"gemini-2.5-pro-exp-03-25\", // If AI_PROVIDER=\"vertex\" (Example override)\n            \"GEMINI_MODEL_ID\": \"gemini-2.5-pro-exp-03-25\", // If AI_PROVIDER=\"gemini\"\n            // --- Optional AI Parameters ---\n            \"GOOGLE_CLOUD_LOCATION\": \"us-central1\", // Specific to Vertex AI\n            \"AI_TEMPERATURE\": \"0.0\",\n            \"AI_USE_STREAMING\": \"true\",\n            \"AI_MAX_OUTPUT_TOKENS\": \"65536\", // Default from .env.example\n            \"AI_MAX_RETRIES\": \"3\",\n            \"AI_RETRY_DELAY_MS\": \"1000\",\n            // --- Optional Vertex Authentication ---\n            // \"GOOGLE_APPLICATION_CREDENTIALS\": \"/path/to/your/service-account-key.json\" // If using Service Account Key for Vertex\n          },\n          \"disabled\": false,\n          \"alwaysAllow\": [\n             // Add tool names here if you don't want confirmation prompts\n             // e.g., \"answer_query_websearch\"\n          ],\n          \"timeout\": 3600 // Optional: Timeout in seconds\n        }\n        // Add other servers here...\n      }\n    }\n    ```\n    *   **Important:** Ensure the `args` path points correctly to the `build/index.js` file. Using an absolute path might be more reliable.\n\n    **Option B: Using NPX (Requires Package Published to npm)**\n\n    This method uses `npx` to automatically download and run the server package from the npm registry. This is convenient if you don't want to clone the repository.\n\n    ```json\n    {\n      \"mcpServers\": {\n        \"vertex-ai-mcp-server\": {\n          \"command\": \"bunx\", // Use bunx\n          \"args\": [\n            \"-y\", // Auto-confirm installation\n            \"vertex-ai-mcp-server\" // The npm package name\n          ],\n          \"env\": {\n            // --- General AI Configuration ---\n            \"AI_PROVIDER\": \"vertex\", // \"vertex\" or \"gemini\"\n            // --- Required (Conditional) ---\n            \"GOOGLE_CLOUD_PROJECT\": \"YOUR_GCP_PROJECT_ID\", // Required if AI_PROVIDER=\"vertex\"\n            // \"GEMINI_API_KEY\": \"YOUR_GEMINI_API_KEY\", // Required if AI_PROVIDER=\"gemini\"\n            // --- Optional Model Selection ---\n            \"VERTEX_MODEL_ID\": \"gemini-2.5-pro-exp-03-25\", // If AI_PROVIDER=\"vertex\" (Example override)\n            \"GEMINI_MODEL_ID\": \"gemini-2.5-pro-exp-03-25\", // If AI_PROVIDER=\"gemini\"\n            // --- Optional AI Parameters ---\n            \"GOOGLE_CLOUD_LOCATION\": \"us-central1\", // Specific to Vertex AI\n            \"AI_TEMPERATURE\": \"0.0\",\n            \"AI_USE_STREAMING\": \"true\",\n            \"AI_MAX_OUTPUT_TOKENS\": \"65536\", // Default from .env.example\n            \"AI_MAX_RETRIES\": \"3\",\n            \"AI_RETRY_DELAY_MS\": \"1000\",\n            // --- Optional Vertex Authentication ---\n            // \"GOOGLE_APPLICATION_CREDENTIALS\": \"/path/to/your/service-account-key.json\" // If using Service Account Key for Vertex\n          },\n          \"disabled\": false,\n          \"alwaysAllow\": [\n             // Add tool names here if you don't want confirmation prompts\n             // e.g., \"answer_query_websearch\"\n          ],\n          \"timeout\": 3600 // Optional: Timeout in seconds\n        }\n        // Add other servers here...\n      }\n    }\n    ```\n    *   Ensure the environment variables in the `env` block are correctly set, either matching `.env` or explicitly defined here. Remove comments from the actual JSON file.\n\n2.  **Restart/Reload Cline:** Cline should detect the configuration change and start the server.\n\n3.  **Use Tools:** You can now use the extensive list of tools via Cline.\n\n## Development\n\n*   **Watch Mode:** `bun run watch`\n*   **Linting:** `bun run lint`\n*   **Formatting:** `bun run format`\n## License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n",
      "npm_url": "https://www.npmjs.com/package/vertex-ai-mcp-server",
      "npm_downloads": 1981,
      "keywords": [
        "search",
        "shariqriazz",
        "google",
        "search shariqriazz",
        "shariqriazz vertex",
        "cloud vertex"
      ],
      "category": "web-search"
    },
    "shiquda--mediawiki-mcp-server": {
      "owner": "shiquda",
      "name": "mediawiki-mcp-server",
      "url": "https://github.com/shiquda/mediawiki-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/shiquda.webp",
      "description": "Interact with Wikipedia's API to search and retrieve content efficiently, providing access to rich data and insights from various wiki sites.",
      "stars": 15,
      "forks": 5,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-09-13T13:24:50Z",
      "readme_content": "<b>Outdated. Please use https://github.com/ProfessionalWiki/MediaWiki-MCP-Server instead!</b>\n\n# MediaWiki MCP Server 🚀\n\n[![smithery badge](https://smithery.ai/badge/@shiquda/mediawiki-mcp-server)](https://smithery.ai/server/@shiquda/mediawiki-mcp-server) ![Python_3_13_informational_logo_style_flat_logoColor_00bfff_color_005566_labelColor_00bfe6](https://img.shields.io/badge/Python-3.13-informational?logo=&style=flat&logoColor=00bfff&color=005566&labelColor=00bfe6) ![build_with_uv_informational_logo_style_flat_logoColor_333333_color_622867_labelColor_de5fe9](https://img.shields.io/badge/build%20with-uv-informational?logo=&style=flat&logoColor=333333&color=622867&labelColor=de5fe9) \n\n[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/shiquda-mediawiki-mcp-server-badge.png)](https://mseep.ai/app/shiquda-mediawiki-mcp-server)\n\nA MCP server that provides seamless interaction with Wikipedia's API. This tool allows you to search and retrieve Wikipedia content with LLMs 🤖!\n\n<https://github.com/user-attachments/assets/b5d9c5f3-a60e-48ea-8b4b-f1a7524d4fbb>\n\n## Features ✨\n\n- 🔍 Search wiki pages with customizable wiki site. e.g. wikipedia.org, fandom.com, wiki.gg and more!\n- 📖 Retrieve detailed page content\n\n## Usage 💻\n\n1. Ensure that uv is installed on your device.\n2. Configure in your client:\n\nThe server defaults to using <https://en.wikipedia.org/>. Also, you can make the server search other wiki sites!\n\nTo see if a wiki site works with this server, check if it uses MediaWiki software (usually shown by an icon at the bottom of the site).\n\nTo check further and find the endpoint (usually the website's domain, like <https://mediawiki.org/>), check by going to base-url/rest.php/v1/page in a browser (like <https://noita.wiki.gg/rest.php/v1/page>) and see if the output looks right. If not, add '/w' to the base URL and try again.\n\nThen, set this endpoint as --base-url:\n\n\n\nAvailable transports: stdio (default), streamable-http (http://localhost/mcp), and SSE (http://localhost/sse). See -h for all options.\n\n```bash\nuvx mediawiki-mcp-server --h\nusage: main.py [-h] [--base-url BASE_URL] [--http] [--sse] [--port PORT]\n\nMediaWiki MCP Server\n\noptions:\n  -h, --help           show this help message and exit\n  --base-url BASE_URL  Base URL for the MediaWiki API (default: https://en.wikipedia.org/w/``)\n  --http               Run server as streamable-http (instead of stdio)\n  --sse                Run server as sse-http (instead of stdio)\n  --port PORT          Default port for http transport (default: 8000)\n\n```\n\nExample JSON configurations:\n\n```json\n{\n  \"mcpServers\": {\n    \"mediawiki-mcp-server\": {\n      \"command\": \"uvx\",\n      \"args\": [\n        \"mediawiki-mcp-server\",\n        \"--base-url\", \"https://example.com/\"\n      ],\n      \"env\": {\n        \"HTTP_PROXY\": \"http://example.com:port\"\n      }\n    }\n  }\n}\n```\n\nOr, if you want to run this server from source:\n\n```json\n{\n  \"mcpServers\": {\n    \"mediawiki-mcp-server\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"run\",\n        \"--directory\", \n        \"mediawiki-mcp-server\",\n        \"path/to/project/src/mediawiki_mcp_server\",\n        \"--base-url\", \"https://example.com/\"\n      ],\n      \"env\": {\n        \"HTTP_PROXY\": \"http://example.com:port\"\n      }\n    }\n  }\n}\n```\n\n## Supported Tools 🛠\n\n### Search\n\n- `query`: Search term (preferably short and focused)\n- `limit`: Maximum number of results to return (default: 5)\n\n### Get Page\n\n- `title`: The exact title of the Wikipedia page to retrieve\n\n## Development 👨‍💻\n\n```bash\nnpx @modelcontextprotocol/inspector uv run mediawiki-mcp-server\n```\n\nHere are some documents that might help:\n\n- <https://www.mediawiki.org/api/rest_v1/>\n\n## Contributing 🤝\n\nThis server is under development. Contributions are welcome! Feel free to submit issues and pull requests.\n\n## Related Projects ♥️\n\n- [Cherry Studio](https://github.com/CherryHQ/cherry-studio): A desktop client that supports for multiple LLM providers. MCP is supported.",
      "npm_url": "https://www.npmjs.com/package/mediawiki-mcp-server",
      "npm_downloads": 320,
      "keywords": [
        "mediawiki",
        "wiki",
        "wikipedia",
        "wikipedia api",
        "shiquda mediawiki",
        "interact wikipedia"
      ],
      "category": "web-search"
    },
    "skrapeai--skrape-mcp": {
      "owner": "skrapeai",
      "name": "skrape-mcp",
      "url": "https://github.com/skrapeai/skrape-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/skrapeai.webp",
      "description": "Convert web pages into clean, structured Markdown suitable for large language model (LLM) consumption, streamlining the process of feeding web content into AI applications.",
      "stars": 12,
      "forks": 9,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-08-14T23:24:30Z",
      "readme_content": "# Skrape MCP Server\n\n[![smithery badge](https://smithery.ai/badge/@skrapeai/skrape-mcp)](https://smithery.ai/server/@skrapeai/skrape-mcp)\n\nConvert webpages into clean, LLM-ready Markdown using [skrape.ai](https://skrape.ai). An MCP server that seamlessly integrates web scraping with Claude Desktop and other MCP-compatible applications.\n\n## Key Features\n\n- **Clean Output**: Removes ads, navigation, and irrelevant content\n- **JavaScript Support**: Handles dynamic content rendering\n- **LLM-Optimized**: Structured Markdown perfect for AI consumption\n- **Consistent Format**: Uniform structure regardless of source\n\n## Features\n\n### Tools\n\n- `get_markdown` - Convert any webpage to LLM-ready Markdown\n  - Takes any input URL and optional parameters\n  - Returns clean, structured Markdown optimized for LLM consumption\n  - Supports JavaScript rendering for dynamic content\n  - Optional JSON response format for advanced integrations\n\n## Installation\n\n### Installing via Smithery\n\nTo install Skrape MCP Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@skrapeai/skrape-mcp):\n\n```bash\nnpx -y @smithery/cli install @skrapeai/skrape-mcp --client claude\n```\n\n### Manual Installation\n\n1. Get your API key from [skrape.ai](https://skrape.ai)\n\n1. Install dependencies:\n\n```bash\nnpm install\n```\n\n1. Build the server:\n\n```bash\nnpm run build\n```\n\n1. Add the server config to Claude Desktop:\n\nOn MacOS:\n\n```bash\nnano ~/Library/Application\\ Support/Claude/claude_desktop_config.json\n```\n\nOn Windows:\n\n```bash\nnotepad %APPDATA%/Claude/claude_desktop_config.json\n```\n\nAdd this configuration (replace paths and API key with your values):\n\n```json\n{\n  \"mcpServers\": {\n    \"skrape\": {\n      \"command\": \"node\",\n      \"args\": [\"path/to/skrape-mcp/build/index.js\"],\n      \"env\": {\n        \"SKRAPE_API_KEY\": \"your-key-here\"\n      }\n    }\n  }\n}\n```\n\n## Using with LLMs\n\nHere's how to use the server with Claude or other LLM models:\n\n1. First, ensure the server is properly configured in your LLM application\n2. Then, you can ask the ALLMI to fetch and process any webpage:\n\n```\nConvert this webpage to markdown: https://example.com\n\nClaude will use the MCP tool like this:\n<use_mcp_tool>\n<server_name>skrape</server_name>\n<tool_name>get_markdown</tool_name>\n<arguments>\n{\n  \"url\": \"https://example.com\",\n  \"options\": {\n    \"renderJs\": true\n  }\n}\n</arguments>\n</use_mcp_tool>\n```\n\nThe resulting Markdown will be clean, structured, and ready for LLM processing.\n\n### Advanced Options\n\nThe `get_markdown` tool accepts these parameters:\n\n- `url` (required): Any webpage URL to convert\n- `returnJson` (optional): Set to `true` to get the full JSON response instead of just markdown\n- `options` (optional): Additional scraping options\n  - `renderJs`: Whether to render JavaScript before scraping (default: true)\n\nExample with all options:\n\n```\n<use_mcp_tool>\n<server_name>skrape</server_name>\n<tool_name>get_markdown</tool_name>\n<arguments>\n{\n  \"url\": \"https://example.com\",\n  \"returnJson\": true,\n  \"options\": {\n    \"renderJs\": false\n  }\n}\n</arguments>\n</use_mcp_tool>\n```\n\n## Development\n\nFor development with auto-rebuild:\n\n```bash\nnpm run watch\n```\n\n### Debugging\n\nSince MCP servers communicate over stdio, debugging can be challenging. We recommend using the [MCP Inspector](https://github.com/modelcontextprotocol/inspector):\n\n```bash\nnpm run inspector\n```\n\nThe Inspector will provide a URL to access debugging tools in your browser.\n\n---\n\n<a href=\"https://glama.ai/mcp/servers/7i81qzgkzd\">\n<img alt=\"badge\" width=\"190\" height=\"100\" src=\"https://glama.ai/mcp/servers/7i81qzgkzd/badge\" />\n</a>\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "markdown",
        "web",
        "pages",
        "markdown suitable",
        "structured markdown",
        "web content"
      ],
      "category": "web-search"
    },
    "skydeckai--skydeckai-code": {
      "owner": "skydeckai",
      "name": "skydeckai-code",
      "url": "https://github.com/skydeckai/skydeckai-code",
      "imageUrl": "/freedevtools/mcp/pfp/skydeckai.webp",
      "description": "Provides tools for AI-driven development workflows, including file system operations, code analysis across multiple programming languages, code execution, and web content fetching with HTML-to-markdown conversion.",
      "stars": 74,
      "forks": 20,
      "license": "Apache License 2.0",
      "language": "Python",
      "updated_at": "2025-09-13T16:37:02Z",
      "readme_content": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/skydeckai-code-badge.png)](https://mseep.ai/app/skydeckai-code)\n\n# SkyDeckAI Code\n\nAn MCP server that provides a comprehensive set of tools for AI-driven development workflows. Features include file system operations, code analysis using tree-sitter for multiple programming languages, code execution, web content fetching with HTML-to-markdown conversion, multi-engine web search, code content searching, and system information retrieval. Designed to enhance AI's capability to assist in software development tasks by providing direct access to both local and remote resources.\n\n# Formerly Known As MCP-Server-AIDD\n\nThis mcp server was formerly known as `mcp-server-aidd`. It was renamed to `skydeckai-code` to credit the team at [SkyDeck.ai](https://skydeck.ai) with creating this application along with [East Agile](https://eastagile.com). But more importantly we realized that the term AI Driven Development (AIDD) was just not catching on. People did not understand at a glance what it was about. And nor did LLMs. \"Code\" was far more intuitive. And linguistically intuitive is important in the world of agentic AI.\n\n[![Verified on MseeP](https://mseep.ai/badge.svg)](https://mseep.ai/app/fe7a40fd-30c1-4767-84f9-d33bf997497e)\n\n## Installation\n\n```bash\n# Using uvx\nuvx skydeckai-code\n```\n\n## Claude Desktop Setup\n\nAdd to your `claude_desktop_config.json`:\n\n```json\n{\n    \"mcpServers\": {\n        \"skydeckai-code\": {\n            \"command\": \"uvx\",\n            \"args\": [\"skydeckai-code\"]\n        }\n    }\n}\n```\n\n## SkyDeck AI Helper App\n\nIf you're using MseeP AI Helper app, you can search for \"SkyDeckAI Code\" and install it.\n\n\n\n## Key Features\n\n-   File system operations (read, write, edit, move, copy, delete)\n-   Directory management and traversal\n-   Multi-language code analysis using tree-sitter\n-   Code content searching with regex pattern matching\n-   Multi-language code execution with safety measures\n-   Web content fetching from APIs and websites with HTML-to-markdown conversion\n-   Multi-engine web search with reliable fallback mechanisms\n-   Batch operations for parallel and serial tool execution\n-   Security controls with configurable workspace boundaries\n-   Screenshot and screen context tools\n-   Image handling tools\n\n## Available Tools (29)\n\n| Category         | Tool Name                  | Description                                  |\n| ---------------- | -------------------------- | -------------------------------------------- |\n| **File System**  | `get_allowed_directory`    | Get the current working directory path       |\n|                  | `update_allowed_directory` | Change the working directory                 |\n|                  | `create_directory`         | Create a new directory or nested directories |\n|                  | `write_file`               | Create or overwrite a file with new content  |\n|                  | `edit_file`                | Make line-based edits to a text file         |\n|                  | `read_file`                | Read the contents of one or more files       |\n|                  | `list_directory`           | Get listing of files and directories         |\n|                  | `move_file`                | Move or rename a file or directory           |\n|                  | `copy_file`                | Copy a file or directory to a new location   |\n|                  | `search_files`             | Search for files matching a name pattern     |\n|                  | `delete_file`              | Delete a file or empty directory             |\n|                  | `get_file_info`            | Get detailed file metadata                   |\n|                  | `directory_tree`           | Get a recursive tree view of directories     |\n|                  | `read_image_file`          | Read an image file as base64 data            |\n| **Code Tools**   | `codebase_mapper`          | Analyze code structure across files          |\n|                  | `search_code`              | Find text patterns in code files             |\n|                  | `execute_code`             | Run code in various languages                |\n|                  | `execute_shell_script`     | Run shell/bash scripts                       |\n| **Web Tools**    | `web_fetch`                | Get content from a URL                       |\n|                  | `web_search`               | Perform a web search                         |\n| **Screen Tools** | `capture_screenshot`       | Take a screenshot of screen or window        |\n|                  | `get_active_apps`          | List running applications                    |\n|                  | `get_available_windows`    | List all open windows                        |\n| **System**       | `get_system_info`          | Get detailed system information              |\n| **Utility**      | `batch_tools`              | Run multiple tool operations together        |\n|                  | `think`                    | Document reasoning without making changes    |\n| **Todo**         | `todo_read`                | Read current workspace todo list             |\n|                  | `todo_write`               | Replace entire todo list with validation     |\n|                  | `todo_update`              | Update specific todo item by ID              |\n\n## Detailed Tool Documentation\n\n### Basic File Operations\n\n| Tool          | Parameters                                                 | Returns                                       |\n| ------------- | ---------------------------------------------------------- | --------------------------------------------- |\n| read_file     | files: [{path: string, offset?: integer, limit?: integer}] | File content (single or multiple files)       |\n| write_file    | path: string, content: string                              | Success confirmation                          |\n| move_file     | source: string, destination: string                        | Success confirmation                          |\n| copy_file     | source: string, destination: string, recursive?: boolean   | Success confirmation                          |\n| delete_file   | path: string                                               | Success confirmation                          |\n| get_file_info | path: string                                               | File metadata (size, timestamps, permissions) |\n\n### Complex File Operations\n\n#### edit_file\n\nPattern-based file editing with preview support:\n\n```json\n{\n    \"path\": \"src/main.py\",\n    \"edits\": [\n        {\n            \"oldText\": \"def old_function():\",\n            \"newText\": \"def new_function():\"\n        }\n    ],\n    \"dryRun\": false,\n    \"options\": {\n        \"partialMatch\": true\n    }\n}\n```\n\nReturns: Diff of changes or preview in dry run mode.\n\n### Directory Operations\n\n| Tool                     | Parameters                                               | Returns                        |\n| ------------------------ | -------------------------------------------------------- | ------------------------------ |\n| get_allowed_directory    | none                                                     | Current allowed directory path |\n| update_allowed_directory | directory: string (absolute path)                        | Success confirmation           |\n| list_directory           | path: string                                             | Directory contents list        |\n| create_directory         | path: string                                             | Success confirmation           |\n| search_files             | pattern: string, path?: string, include_hidden?: boolean | Matching files list            |\n\nThe `search_files` tool searches for files by name pattern, while the `search_code` tool searches within file contents using regex. Use `search_files` when looking for files with specific names or extensions, and `search_code` when searching for specific text patterns inside files.\n\n#### directory_tree\n\nGenerates complete directory structure:\n\n```json\n{\n    \"path\": \"src\",\n    \"include_hidden\": false\n}\n```\n\nReturns: JSON tree structure of directory contents.\n\n### Code Analysis\n\n#### codebase_mapper\n\nAnalyzes source code structure:\n\n```json\n{\n    \"path\": \"src\"\n}\n```\n\nReturns:\n\n-   Classes and their methods\n-   Functions and parameters\n-   Module structure\n-   Code organization statistics\n-   Inheritance relationships\n\nSupported Languages:\n\n-   Python (.py)\n-   JavaScript (.js/.jsx, .mjs, .cjs)\n-   TypeScript (.ts/.tsx)\n-   Java (.java)\n-   C++ (.cpp, .hpp, .cc)\n-   Ruby (.rb, .rake)\n-   Go (.go)\n-   Rust (.rs)\n-   PHP (.php)\n-   C# (.cs)\n-   Kotlin (.kt, .kts)\n\n#### search_code\n\nFast content search tool using regular expressions:\n\n```json\n{\n    \"patterns\": [\"function\\\\s+\\\\w+\", \"class\\\\s+\\\\w+\"],\n    \"include\": \"*.js\",\n    \"exclude\": \"node_modules/**\",\n    \"max_results\": 50,\n    \"case_sensitive\": false,\n    \"path\": \"src\"\n}\n```\n\n**Parameters:**\n| Parameter | Type | Required | Description |\n|-----------|------|----------|-------------|\n| patterns | array of strings | Yes | List of regular expression patterns to search for in file contents |\n| include | string | No | File pattern to include (glob syntax, default: \"\\*\") |\n| exclude | string | No | File pattern to exclude (glob syntax, default: \"\") |\n| max_results | integer | No | Maximum results to return per pattern (default: 100) |\n| case_sensitive | boolean | No | Whether search is case-sensitive (default: false) |\n| path | string | No | Base directory to search from (default: \".\") |\n\n**Returns:**\nMatching lines grouped by file with line numbers, sorted by file modification time with newest files first.\n\nThis tool uses ripgrep when available for optimal performance, with a Python fallback implementation. It's ideal for finding specific code patterns like function declarations, imports, variable usages, or error handling.\n\n### System Information\n\n| Tool            | Parameters | Returns                      |\n| --------------- | ---------- | ---------------------------- |\n| get_system_info | none       | Comprehensive system details |\n\nReturns:\n\n```json\n{\n  \"working_directory\": \"/path/to/project\",\n  \"system\": {\n    \"os\", \"os_version\", \"architecture\", \"python_version\"\n  },\n  \"wifi_network\": \"MyWiFi\",\n  \"cpu\": {\n    \"physical_cores\", \"logical_cores\", \"total_cpu_usage\"\n  },\n  \"memory\": { \"total\", \"available\", \"used_percentage\" },\n  \"disk\": { \"total\", \"free\", \"used_percentage\" },\n  \"mac_details\": {  // Only present on macOS\n    \"model\": \"Mac mini\",\n    \"chip\": \"Apple M2\",\n    \"serial_number\": \"XXX\"\n  }\n}\n```\n\nProvides essential system information in a clean, readable format.\n\n### Screen Context and Image Tools\n\n#### get_active_apps\n\nReturns a list of currently active applications on the user's system.\n\n```json\n{\n    \"with_details\": true\n}\n```\n\n**Parameters:**\n| Parameter | Type | Required | Description |\n|-----------|---------|----------|---------------------------------------|\n| with_details | boolean | No | Whether to include additional details about each application (default: false) |\n\n**Returns:**\n\n```json\n{\n    \"success\": true,\n    \"platform\": \"macos\",\n    \"app_count\": 12,\n    \"apps\": [\n        {\n            \"name\": \"Firefox\",\n            \"has_windows\": true,\n            \"window_count\": 3,\n            \"visible_windows\": [\n                { \"name\": \"GitHub - Mozilla Firefox\", \"width\": 1200, \"height\": 800 }\n            ]\n        },\n        {\n            \"name\": \"VSCode\",\n            \"has_windows\": true\n        }\n    ]\n}\n```\n\nThis tool provides valuable context about applications currently running on the user's system, which can help with providing more relevant assistance.\n\n#### get_available_windows\n\nReturns detailed information about all available windows currently displayed on the user's screen.\n\n```json\n{}\n```\n\n**Returns:**\n\n```json\n{\n    \"success\": true,\n    \"platform\": \"macos\",\n    \"count\": 8,\n    \"windows\": [\n        {\n            \"id\": 42,\n            \"title\": \"Document.txt - Notepad\",\n            \"app\": \"Notepad\",\n            \"visible\": true\n        },\n        {\n            \"title\": \"Terminal\",\n            \"app\": \"Terminal\",\n            \"visible\": true,\n            \"active\": true\n        }\n    ]\n}\n```\n\nThis tool helps understand what's visible on the user's screen and can be used for context-aware assistance.\n\n#### capture_screenshot\n\nCaptures a screenshot of the user's screen or a specific window.\n\n```json\n{\n    \"output_path\": \"screenshots/capture.png\",\n    \"capture_mode\": {\n        \"type\": \"named_window\",\n        \"window_name\": \"Visual Studio Code\"\n    }\n}\n```\n\n**Parameters:**\n| Parameter | Type | Required | Description |\n|-----------|---------|----------|---------------------------------------|\n| output_path | string | No | Path where the screenshot should be saved (default: generated path) |\n| capture_mode | object | No | Specifies what to capture |\n| capture_mode.type | string | No | Type of screenshot: 'full', 'active_window', or 'named_window' (default: 'full') |\n| capture_mode.window_name | string | No | Name of window to capture (required when type is 'named_window') |\n\n**Returns:**\n\n```json\n{\n    \"success\": true,\n    \"path\": \"/path/to/screenshots/capture.png\"\n}\n```\n\nThis tool captures screenshots for visualization, debugging, or context-aware assistance.\n\n#### read_image_file\n\nReads an image file from the file system and returns its contents as a base64-encoded string.\n\n```json\n{\n    \"path\": \"images/logo.png\"\n}\n```\n\n**Parameters:**\n| Parameter | Type | Required | Description |\n|-----------|---------|----------|---------------------------------------|\n| path | string | Yes | Path to the image file to read |\n| max_size | integer | No | Maximum file size in bytes (default: 100MB) |\n\n**Returns:**\nBase64-encoded image data that can be displayed or processed.\n\nThis tool supports common image formats like PNG, JPEG, GIF, and WebP, and automatically resizes images for optimal viewing.\n\n### Web Tools\n\n#### web_fetch\n\nFetches content from a URL and optionally saves it to a file.\n\n```json\n{\n    \"url\": \"https://api.github.com/users/octocat\",\n    \"headers\": {\n        \"Accept\": \"application/json\"\n    },\n    \"timeout\": 15,\n    \"save_to_file\": \"downloads/octocat.json\",\n    \"convert_html_to_markdown\": true\n}\n```\n\n**Parameters:**\n| Parameter | Type | Required | Description |\n|-----------|---------|----------|---------------------------------------|\n| url | string | Yes | URL to fetch content from (http/https only) |\n| headers | object | No | Optional HTTP headers to include in the request |\n| timeout | integer | No | Maximum time to wait for response (default: 10s) |\n| save_to_file | string | No | Path to save response content (within allowed directory) |\n| convert_html_to_markdown | boolean | No | When true, converts HTML content to markdown for better readability (default: true) |\n\n**Returns:**\nResponse content as text with HTTP status code and size information. For binary content, returns metadata and saves to file if requested. When convert_html_to_markdown is enabled, HTML content is automatically converted to markdown format for better readability.\n\nThis tool can be used to access web APIs, fetch documentation, or download content from the web while respecting size limits (10MB max) and security constraints.\n\n#### web_search\n\nPerforms a robust web search using multiple search engines and returns concise, relevant results.\n\n```json\n{\n    \"query\": \"latest python release features\",\n    \"num_results\": 8,\n    \"convert_html_to_markdown\": true,\n    \"search_engine\": \"bing\"\n}\n```\n\n**Parameters:**\n| Parameter | Type | Required | Description |\n|-----------|---------|----------|---------------------------------------|\n| query | string | Yes | The search query to process. Be specific for better results. |\n| num_results | integer | No | Maximum number of search results to return (default: 10, max: 20) |\n| convert_html_to_markdown | boolean | No | When true, content will be converted from HTML to markdown for better readability (default: true) |\n| search_engine | string | No | Specifies which search engine to use: \"auto\" (default), \"bing\", or \"duckduckgo\" |\n\n**Returns:**\nA list of search results formatted in markdown, including titles, URLs, and snippets for each result. Results are deduplicated and organized hierarchically for easy reading.\n\nThis tool uses a multi-engine approach that tries different search engines with various parsing strategies to ensure reliable results. You can specify a preferred engine, but some engines may block automated access, in which case the tool will fall back to alternative engines when \"auto\" is selected.\n\n### Utility Tools\n\n#### batch_tools\n\nExecute multiple tool invocations in a single request with parallel execution when possible.\n\n```json\n{\n    \"description\": \"Setup new project\",\n    \"sequential\": true,\n    \"invocations\": [\n        {\n            \"tool\": \"create_directory\",\n            \"arguments\": {\n                \"path\": \"src\"\n            }\n        },\n        {\n            \"tool\": \"write_file\",\n            \"arguments\": {\n                \"path\": \"README.md\",\n                \"content\": \"# New Project\\n\\nThis is a new project.\"\n            }\n        },\n        {\n            \"tool\": \"execute_shell_script\",\n            \"arguments\": {\n                \"script\": \"git init\"\n            }\n        }\n    ]\n}\n```\n\n**Parameters:**\n| Parameter | Type | Required | Description |\n|-----------|---------|----------|---------------------------------------|\n| description | string | Yes | Short description of the batch operation |\n| sequential | boolean | No | Whether to run tools in sequence (default: false) |\n| invocations | array | Yes | List of tool invocations to execute |\n| invocations[].tool | string | Yes | Name of the tool to invoke |\n| invocations[].arguments | object | Yes | Arguments for the specified tool |\n\n**Returns:**\nCombined results from all tool invocations, grouped by tool with success/error status for each. Results are presented in the original invocation order with clear section headers.\n\nThis tool provides efficient execution of multiple operations in a single request. When `sequential` is false (default), tools are executed in parallel for better performance. When `sequential` is true, tools are executed in order, and if any tool fails, execution stops.\n\n**IMPORTANT**: All tools in the batch execute in the same working directory context. If a tool creates a directory and a subsequent tool needs to work inside that directory, you must either:\n\n1. Use paths relative to the current working directory (e.g., \"project/src\" rather than just \"src\"), or\n2. Include an explicit tool invocation to change directories using `update_allowed_directory`\n\n#### think\n\nA tool for complex reasoning and brainstorming without making changes to the repository.\n\n```json\n{\n    \"thought\": \"Let me analyze the performance issue in the codebase:\\n\\n## Root Cause Analysis\\n\\n1. The database query is inefficient because:\\n   - It doesn't use proper indexing\\n   - It fetches more columns than needed\\n   - The JOIN operation is unnecessarily complex\\n\\n## Potential Solutions\\n\\n1. **Add database indexes**:\\n   - Create an index on the user_id column\\n   - Create a composite index on (created_at, status)\\n\\n2. **Optimize the query**:\\n   - Select only necessary columns\\n   - Rewrite the JOIN using a subquery\\n   - Add LIMIT clause for pagination\\n\\n3. **Add caching layer**:\\n   - Cache frequent queries using Redis\\n   - Implement cache invalidation strategy\\n\\nAfter weighing the options, solution #2 seems to be the simplest to implement with the highest impact.\"\n}\n```\n\n**Parameters:**\n| Parameter | Type | Required | Description |\n|-----------|---------|----------|---------------------------------------|\n| thought | string | Yes | Your detailed thoughts, analysis or reasoning process |\n\n**Returns:**\nYour thoughts formatted as markdown, with a note indicating this was a thinking exercise.\n\nThis tool is useful for thinking through complex problems, brainstorming solutions, or laying out implementation plans without making any actual changes. It's a great way to document your reasoning process, evaluate different approaches, or plan out a multi-step strategy before taking action.\n\n### Code Execution\n\n#### execute_code\n\nExecutes code in various programming languages with safety measures and restrictions.\n\n```json\n{\n    \"language\": \"python\",\n    \"code\": \"print('Hello, World!')\",\n    \"timeout\": 5\n}\n```\n\n**Supported Languages:**\n\n-   Python (python3)\n-   JavaScript (Node.js)\n-   Ruby\n-   PHP\n-   Go\n-   Rust\n\n**Parameters:**\n| Parameter | Type | Required | Description |\n|-----------|---------|----------|---------------------------------------|\n| language | string | Yes | Programming language to use |\n| code | string | Yes | Code to execute |\n| timeout | integer | No | Maximum execution time (default: 5s) |\n\n**Requirements:**\n\n-   Respective language runtimes must be installed\n-   Commands must be available in system PATH\n-   Proper permissions for temporary file creation\n\n⚠️ **Security Warning:**\nThis tool executes arbitrary code on your system. Always:\n\n1. Review code thoroughly before execution\n2. Understand the code's purpose and expected outcome\n3. Never execute untrusted code\n4. Be aware of potential system impacts\n5. Monitor execution output\n\n#### execute_shell_script\n\nExecutes shell scripts (bash/sh) with safety measures and restrictions.\n\n```json\n{\n    \"script\": \"echo \\\"Current directory:\\\" && pwd\",\n    \"timeout\": 300\n}\n```\n\n**Parameters:**\n| Parameter | Type | Required | Description |\n|-----------|---------|----------|---------------------------------------|\n| script | string | Yes | Shell script to execute |\n| timeout | integer | No | Maximum execution time (default: 300s, max: 600s) |\n\n**Features:**\n\n-   Uses /bin/sh for maximum compatibility across systems\n-   Executes within the allowed directory\n-   Separate stdout and stderr output\n-   Proper error handling and timeout controls\n\n⚠️ **Security Warning:**\nThis tool executes arbitrary shell commands on your system. Always:\n\n1. Review the script thoroughly before execution\n2. Understand the script's purpose and expected outcome\n3. Never execute untrusted scripts\n4. Be aware of potential system impacts\n5. Monitor execution output\n\n### Todo Tools\n\nThe todo tools provide sequential task management capabilities for workspace-first development workflows. Tasks are executed in order without priority systems, ensuring structured progress through development phases.\n\n#### todo_read\n\nRead the current todo list for the workspace.\n\n```json\n{}\n```\n\n**Returns:**\n```json\n{\n  \"todos\": [\n    {\n      \"id\": \"abc123\",\n      \"content\": \"Implement user authentication\",\n      \"status\": \"in_progress\",\n      \"metadata\": {\n        \"custom_key\": \"custom_value\"\n      },\n      \"created_at\": \"2023-10-01T10:00:00Z\",\n      \"updated_at\": \"2023-10-01T11:30:00Z\"\n    }\n  ],\n  \"count\": 1,\n  \"workspace\": \"/path/to/workspace\"\n}\n```\n\n#### todo_write\n\nReplace the entire todo list for sequential execution workflow. Tasks are executed in array order, building upon previous work.\n\n```json\n{\n  \"todos\": [\n    {\n      \"id\": \"task1\",\n      \"content\": \"Set up database schema\",\n      \"status\": \"pending\"\n    },\n    {\n      \"id\": \"task2\", \n      \"content\": \"Create API endpoints\",\n      \"status\": \"pending\",\n      \"metadata\": {\n        \"custom_key\": \"custom_value\"\n      }\n    }\n  ]\n}\n```\n\n**Sequential Workflow Rules:**\n- Each todo must have unique ID\n- Only one task can be \"in_progress\" at a time (sequential execution)\n- Tasks execute in array order - no priority system\n- Required fields: id, content, status\n- Status values: \"pending\", \"in_progress\", \"completed\"\n- Workspace-first: Todo management is mandatory for all workspace operations\n\n#### todo_update\n\nUpdate a specific todo item by ID for sequential workflow progression.\n\n```json\n{\n  \"todo_id\": \"task1\",\n  \"updates\": {\n    \"status\": \"in_progress\",\n    \"metadata\": {\n        \"new_key\": \"new_value\"\n    }\n  }\n}\n```\n\n**Returns:**\n```json\n{\n  \"success\": true,\n  \"updated_todo\": {\n    \"id\": \"task1\",\n    \"content\": \"Set up database schema\",\n    \"status\": \"in_progress\",\n    \"updated_at\": \"2023-10-01T12:00:00Z\",\n    \"metadata\": {\n        \"new_key\": \"new_value\"\n    }\n  },\n  \"counts\": {\n    \"pending\": 1,\n    \"in_progress\": 1,\n    \"completed\": 0,\n    \"total\": 2\n  },\n  \"workspace\": \"/path/to/workspace\"\n}\n```\n\nThe todo system maintains separate sequential task lists for each workspace, enforcing mandatory usage for all workspace operations. Tasks execute in order, building upon previous work without priority-based scheduling.\n\n## Configuration\n\nConfiguration file: `~/.skydeckai_code/config.json`\n\n```json\n{\n    \"allowed_directory\": \"/path/to/workspace\"\n}\n```\n\n## Debugging\n\nUse MCP Inspector for debugging:\n\n```bash\nnpx @modelcontextprotocol/inspector run\n```\n\n## Security\n\n-   Operations restricted to configured allowed directory\n-   Path traversal prevention\n-   File permission preservation\n-   Safe operation handling\n\n## Upcoming Features\n\n-   GitHub tools:\n    -   PR Description Generator\n    -   Code Review\n    -   Actions Manager\n-   Pivotal Tracker tools:\n    -   Story Generator\n    -   Story Manager\n\n## Development Status\n\nCurrently in active development. Features and API may change.\n\n## License\n\nApache License 2.0 - see [LICENSE](LICENSE)\n\n[![Star History Chart](https://api.star-history.com/svg?repos=skydeckai/skydeckai-code&type=Date)](https://www.star-history.com/#skydeckai/skydeckai-code&Date)",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "skydeckai",
        "programming",
        "code",
        "skydeckai code",
        "search skydeckai",
        "skydeckai skydeckai"
      ],
      "category": "web-search"
    },
    "sourcebot-dev--sourcebot": {
      "owner": "sourcebot-dev",
      "name": "sourcebot",
      "url": "https://github.com/sourcebot-dev/sourcebot",
      "imageUrl": "/freedevtools/mcp/pfp/sourcebot-dev.webp",
      "description": "Enables regex-based code search across multiple code hosts such as GitHub, GitLab, and BitBucket to retrieve relevant code snippets and repository information. Provides LLM agents with enhanced context for improved reasoning and response quality without requiring local repository checkouts.",
      "stars": 2609,
      "forks": 149,
      "license": "Other",
      "language": "TypeScript",
      "updated_at": "2025-10-03T09:35:30Z",
      "readme_content": "<div align=\"center\">\n<picture>\n  <source media=\"(prefers-color-scheme: dark)\" srcset=\".github/images/logo_dark.png\">\n  \n</picture>\n</div>\n<div align=\"center\">\n   <div>\n      <h3>\n         <a href=\"https://docs.sourcebot.dev/self-hosting/overview\">\n            <strong>Self Host</strong>\n         </a> · \n         <a href=\"https://demo.sourcebot.dev\">\n            <strong>Public Demo</strong>\n         </a>\n      </h3>\n   </div>\n\n   <div>\n      <a href=\"https://docs.sourcebot.dev/\"><strong>Docs</strong></a> ·\n      <a href=\"https://github.com/sourcebot-dev/sourcebot/issues/459\"><strong>Roadmap</strong></a> ·\n      <a href=\"https://github.com/sourcebot-dev/sourcebot/issues/new?template=bug_report.yml\"><strong>Report Bug</strong></a> ·\n      <a href=\"https://github.com/sourcebot-dev/sourcebot/issues/new?template=feature_request.md\"><strong>Feature Request</strong></a> ·\n      <a href=\"https://www.sourcebot.dev/changelog\"><strong>Changelog</strong></a>\n   </div>\n   <br/>\n   <div>\n   </div>\n</div>\n<p align=\"center\">\n  <a href=\"mailto:team@sourcebot.dev\"><img alt=\"Email_Us_brightgreen\" src=\"https://img.shields.io/badge/Email%20Us-brightgreen\" /></a>\n  <a href=\"https://github.com/sourcebot-dev/sourcebot/actions/workflows/ghcr-publish.yml\"><img alt=\"ghcr_publish\" src=\"https://img.shields.io/github/actions/workflow/status/sourcebot-dev/sourcebot/ghcr-publish.yml\"/><a>\n  <a href=\"https://github.com/sourcebot-dev/sourcebot/stargazers\"><img alt=\"sourcebot\" src=\"https://img.shields.io/github/stars/sourcebot-dev/sourcebot\" /></a>\n</p>\n<p align=\"center\">\n</p>\n\nSourcebot is a self-hosted tool that helps you understand your codebase. \n\n- **Ask Sourcebot:** Ask questions about your codebase and have Sourcebot provide detailed answers grounded with inline citations.\n- **Code search:** Search and navigate across all your repos and branches, no matter where they’re hosted.\n\nTry it out in our [public demo](https://demo.sourcebot.dev)!\n\nhttps://github.com/user-attachments/assets/ed66a622-e38f-4947-a531-86df1e1e0218\n\n# Features\n![Sourcebot Features](https://github.com/user-attachments/assets/3aed7348-7aeb-4af3-89da-b617c3db2e02)\n\n## Ask Sourcebot\nAsk Sourcebot gives you the ability to ask complex questions about your codebase in natural language.\n\nIt uses Sourcebot's existing code search and navigation tools to allow reasoning models to search your code, follow code nav references, and provide an answer that's rich with inline citations and navigable code snippets.\n\nhttps://github.com/user-attachments/assets/8212cd16-683f-468f-8ea5-67455c0931e2\n\n## Code Search\nSearch across all your repos/branches across any code host platform. Blazingly fast, and supports regular expressions, repo/language search filters, boolean logic, and more.\n\nhttps://github.com/user-attachments/assets/3b381452-d329-4949-b6f2-2fc38952e481\n\n## Code Navigation\nIDE-level code navigation (goto definition and find references) across all your repos.\n\nhttps://github.com/user-attachments/assets/e2da2829-71cc-40af-98b4-7ba52e945530\n\n## Built-in File Explorer\nExplore every file across all of your repos. Modern UI with syntax highlighting, file tree, code navigation, etc.\n\nhttps://github.com/user-attachments/assets/31ec0669-707d-4e03-b511-1bc33d44197a\n\n# Deploy Sourcebot\n\nSourcebot can be deployed in seconds using our official docker image. Visit our [docs](https://docs.sourcebot.dev/docs/deployment-guide) for more information.\n\n1. Create a config\n```sh\ntouch config.json\necho '{\n    \"$schema\": \"https://raw.githubusercontent.com/sourcebot-dev/sourcebot/main/schemas/v3/index.json\",\n    \"connections\": {\n        // Comments are supported\n        \"starter-connection\": {\n            \"type\": \"github\",\n            \"repos\": [\n                \"sourcebot-dev/sourcebot\"\n            ]\n        }\n    }\n}' > config.json\n```\n\n2. Run the docker container\n```sh\ndocker run \\\n  -p 3000:3000 \\\n  --pull=always \\\n  --rm \\\n  -v $(pwd):/data \\\n  -e CONFIG_PATH=/data/config.json \\\n  --name sourcebot \\\n  ghcr.io/sourcebot-dev/sourcebot:latest\n```\n<details>\n<summary>What does this command do?</summary>\n\n- Pull and run the Sourcebot docker image from [ghcr.io/sourcebot-dev/sourcebot:latest](https://github.com/sourcebot-dev/sourcebot/pkgs/container/sourcebot).\n- Mount the current directory (`-v $(pwd):/data`) to allow Sourcebot to persist the `.sourcebot` cache.\n- Clones sourcebot at `HEAD` into `.sourcebot/github/sourcebot-dev/sourcebot`.\n- Indexes sourcebot into a .zoekt index file in `.sourcebot/index/`.\n- Map port 3000 between your machine and the docker image.\n- Starts the web server on port 3000.\n</details>\n</br>\n\n3. Visit `http://localhost:3000` to start using Sourcebot\n</br>\n\nTo configure Sourcebot (index your own repos, connect your LLMs, etc), check out our [docs](https://docs.sourcebot.dev/docs/configuration/config-file).\n\n> [!NOTE]\n> Sourcebot collects <a href=\"https://demo.sourcebot.dev/~/search?query=captureEvent%5C(%20repo%3Asourcebot\">anonymous usage data</a> by default to help us improve the product. No sensitive data is collected, but if you'd like to disable this you can do so by setting the `SOURCEBOT_TELEMETRY_DISABLED` environment\n> variable to `true`. Please refer to our [telemetry docs](https://docs.sourcebot.dev/self-hosting/overview#telemetry) for more information.\n\n# Build from source\n>[!NOTE]\n> Building from source is only required if you'd like to contribute. If you'd just like to use Sourcebot, we recommend checking out our self-hosting [docs](https://docs.sourcebot.dev/self-hosting/overview).\n\nIf you'd like to build from source, please checkout the `CONTRIBUTING.md` file for more information.",
      "npm_url": "https://www.npmjs.com/package/sourcebot",
      "npm_downloads": 141,
      "keywords": [
        "sourcebot",
        "github",
        "snippets",
        "search sourcebot",
        "dev sourcebot",
        "sourcebot dev"
      ],
      "category": "web-search"
    },
    "spences10--mcp-duckduckgo-search": {
      "owner": "spences10",
      "name": "mcp-duckduckgo-search",
      "url": "https://github.com/spences10/mcp-duckduckgo-search",
      "imageUrl": "/freedevtools/mcp/pfp/spences10.webp",
      "description": "Integrates DuckDuckGo search capabilities with large language models, providing comprehensive web search functionalities with various result types and filtering options.",
      "stars": 3,
      "forks": 7,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-09-10T19:12:36Z",
      "readme_content": "# mcp-duckduckgo-search\n\n---\n\n## ⚠️ Notice\n\n**This repository is no longer maintained.**\n\nThe functionality of this tool is now available in [mcp-omnisearch](https://github.com/spences10/mcp-omnisearch), which combines multiple MCP tools in one unified package.\n\nPlease use [mcp-omnisearch](https://github.com/spences10/mcp-omnisearch) instead.\n\n---\n\nA Model Context Protocol (MCP) server for integrating DuckDuckGo\nsearch capabilities with LLMs. This server provides comprehensive web\nsearch functionality with support for various result types and\nfiltering options.\n\n<a href=\"https://glama.ai/mcp/servers/v99lwtriyk\">\n  <img alt=\"badge\" width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/v99lwtriyk/badge\" />\n</a>\n\n## Features\n\n- 🔍 Comprehensive web search using DuckDuckGo's search engine\n- 📊 Rich result types including:\n  - Knowledge graph information\n  - Organic search results\n  - News articles\n  - Video content\n  - Image results\n  - Related searches\n- 🌍 Region-specific search support\n- 🛡️ Configurable safe search levels\n- 📅 Date-based filtering options\n- 📄 Pagination support\n- 💾 Built-in result caching\n- 🔒 Safe search options (off, moderate, strict)\n\n## Configuration\n\nThis server requires configuration through your MCP client. Here are\nexamples for different environments:\n\n### Cline Configuration\n\nAdd this to your Cline MCP settings:\n\n```json\n{\n\t\"mcpServers\": {\n\t\t\"mcp-duckduckgo-search\": {\n\t\t\t\"command\": \"npx\",\n\t\t\t\"args\": [\"-y\", \"mcp-duckduckgo-search\"],\n\t\t\t\"env\": {\n\t\t\t\t\"SERPAPI_KEY\": \"your-serpapi-api-key\"\n\t\t\t}\n\t\t}\n\t}\n}\n```\n\n### Claude Desktop with WSL Configuration\n\nFor WSL environments, add this to your Claude Desktop configuration:\n\n```json\n{\n\t\"mcpServers\": {\n\t\t\"mcp-duckduckgo-search\": {\n\t\t\t\"command\": \"wsl.exe\",\n\t\t\t\"args\": [\n\t\t\t\t\"bash\",\n\t\t\t\t\"-c\",\n\t\t\t\t\"source ~/.nvm/nvm.sh && SERPAPI_KEY=your-serpapi-api-key /home/username/.nvm/versions/node/v20.12.1/bin/npx mcp-duckduckgo-search\"\n\t\t\t]\n\t\t}\n\t}\n}\n```\n\n### Environment Variables\n\nThe server requires the following environment variable:\n\n- `SERPAPI_KEY`: Your SerpAPI key (required)\n\n## API\n\nThe server implements a single MCP tool with configurable parameters:\n\n### ddg_search\n\nPerform web searches using the DuckDuckGo search engine.\n\nParameters:\n\n- `query` (string, required): Search query\n- `region` (string, optional): Region code (e.g., us-en, uk-en)\n  (default: us-en)\n- `safe_search` (string, optional): Safe search level (off, moderate,\n  strict) (default: moderate)\n- `date_filter` (string, optional): Filter results by date:\n  - 'd': past day\n  - 'w': past week\n  - 'm': past month\n  - 'y': past year\n  - Custom range: '2023-01-01..2023-12-31'\n- `start` (number, optional): Result offset for pagination\n- `no_cache` (boolean, optional): Bypass cache for fresh results\n  (default: false)\n\nResponse includes:\n\n- Knowledge graph data when available\n- Organic search results\n- News articles\n- Video content\n- Image results\n- Related searches\n- Search metadata\n\n## Development\n\n### Setup\n\n1. Clone the repository\n2. Install dependencies:\n\n```bash\npnpm install\n```\n\n3. Build the project:\n\n```bash\npnpm build\n```\n\n4. Run in development mode:\n\n```bash\npnpm dev\n```\n\n### Publishing\n\nThe project uses changesets for version management. To publish:\n\n1. Create a changeset:\n\n```bash\npnpm changeset\n```\n\n2. Version the package:\n\n```bash\npnpm changeset version\n```\n\n3. Publish to npm:\n\n```bash\npnpm release\n```\n\n## Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n\n## License\n\nMIT License - see the [LICENSE](LICENSE) file for details.\n\n## Acknowledgments\n\n- Built on the\n  [Model Context Protocol](https://github.com/modelcontextprotocol)\n- Powered by [DuckDuckGo](https://duckduckgo.com) through\n  [SerpAPI](https://serpapi.com)\n",
      "npm_url": "https://www.npmjs.com/package/mcp-duckduckgo-search",
      "npm_downloads": 879,
      "keywords": [
        "search",
        "duckduckgo",
        "filtering",
        "duckduckgo search",
        "web search",
        "search capabilities"
      ],
      "category": "web-search"
    },
    "spences10--mcp-jinaai-grounding": {
      "owner": "spences10",
      "name": "mcp-jinaai-grounding",
      "url": "https://github.com/spences10/mcp-jinaai-grounding",
      "imageUrl": "/freedevtools/mcp/pfp/spences10.webp",
      "description": "Integrates Jina.ai's Grounding API with language models to provide real-time web content for enhancing responses. It offers efficient web content grounding capabilities optimized for generating factual information in LLM outputs.",
      "stars": 1,
      "forks": 2,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-09-10T19:14:19Z",
      "readme_content": "# mcp-jinaai-grounding\n\n---\n\n## ⚠️ Notice\n\n**This repository is no longer maintained.**\n\nThe functionality of this tool is now available in [mcp-omnisearch](https://github.com/spences10/mcp-omnisearch), which combines multiple MCP tools in one unified package.\n\nPlease use [mcp-omnisearch](https://github.com/spences10/mcp-omnisearch) instead.\n\n---\n\nA Model Context Protocol (MCP) server for integrating Jina.ai's\nGrounding API with LLMs. This server provides efficient and\ncomprehensive web content grounding capabilities, optimized for\nenhancing LLM responses with factual, real-time web content.\n\n<a href=\"https://glama.ai/mcp/servers/urkuhet67l\">\n  <img alt=\"badge\" width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/urkuhet67l/badge\" />\n</a>\n\n## Features\n\n- 🌐 Advanced web content grounding through Jina.ai Grounding API\n- 🚀 Real-time content verification and fact-checking\n- 📚 Comprehensive web content analysis\n- 🔄 Clean format optimized for LLMs\n- 🎯 Precise content relevance scoring\n- 🏗️ Built on the Model Context Protocol\n\n## Configuration\n\nThis server requires configuration through your MCP client. Here are\nexamples for different environments:\n\n### Cline Configuration\n\nAdd this to your Cline MCP settings:\n\n```json\n{\n\t\"mcpServers\": {\n\t\t\"jinaai-grounding\": {\n\t\t\t\"command\": \"node\",\n\t\t\t\"args\": [\"-y\", \"mcp-jinaai-grounding\"],\n\t\t\t\"env\": {\n\t\t\t\t\"JINAAI_API_KEY\": \"your-jinaai-api-key\"\n\t\t\t}\n\t\t}\n\t}\n}\n```\n\n### Claude Desktop with WSL Configuration\n\nFor WSL environments, add this to your Claude Desktop configuration:\n\n```json\n{\n\t\"mcpServers\": {\n\t\t\"jinaai-grounding\": {\n\t\t\t\"command\": \"wsl.exe\",\n\t\t\t\"args\": [\n\t\t\t\t\"bash\",\n\t\t\t\t\"-c\",\n\t\t\t\t\"JINAAI_API_KEY=your-jinaai-api-key npx mcp-jinaai-grounding\"\n\t\t\t]\n\t\t}\n\t}\n}\n```\n\n### Environment Variables\n\nThe server requires the following environment variable:\n\n- `JINAAI_API_KEY`: Your Jina.ai API key (required)\n\n## API\n\nThe server implements MCP tools for grounding LLM responses with web\ncontent:\n\n### ground_content\n\nGround LLM responses with real-time web content using Jina.ai\nGrounding.\n\nParameters:\n\n- `query` (string, required): The text to ground with web content\n- `no_cache` (boolean, optional): Bypass cache for fresh results.\n  Defaults to false\n- `format` (string, optional): Response format (\"json\" or \"text\").\n  Defaults to \"text\"\n- `token_budget` (number, optional): Maximum number of tokens for this\n  request\n- `browser_locale` (string, optional): Browser locale for rendering\n  content\n- `stream` (boolean, optional): Enable stream mode for large pages.\n  Defaults to false\n- `gather_links` (boolean, optional): Gather all links at the end of\n  response. Defaults to false\n- `gather_images` (boolean, optional): Gather all images at the end of\n  response. Defaults to false\n- `image_caption` (boolean, optional): Caption images in the content.\n  Defaults to false\n- `enable_iframe` (boolean, optional): Extract content from iframes.\n  Defaults to false\n- `enable_shadow_dom` (boolean, optional): Extract content from shadow\n  DOM. Defaults to false\n- `resolve_redirects` (boolean, optional): Follow redirect chains to\n  final URL. Defaults to true\n\n## Development\n\n### Setup\n\n1. Clone the repository\n2. Install dependencies:\n\n```bash\npnpm install\n```\n\n3. Build the project:\n\n```bash\npnpm run build\n```\n\n4. Run in development mode:\n\n```bash\npnpm run dev\n```\n\n### Publishing\n\n1. Update version in package.json\n2. Build the project:\n\n```bash\npnpm run build\n```\n\n3. Publish to npm:\n\n```bash\npnpm run release\n```\n\n## Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n\n## License\n\nMIT License - see the [LICENSE](LICENSE) file for details.\n\n## Acknowledgments\n\n- Built on the\n  [Model Context Protocol](https://github.com/modelcontextprotocol)\n- Powered by [Jina.ai Grounding API](https://jina.ai)\n",
      "npm_url": "https://www.npmjs.com/package/mcp-jinaai-grounding",
      "npm_downloads": 4123,
      "keywords": [
        "grounding",
        "web",
        "api",
        "grounding api",
        "content grounding",
        "jinaai grounding"
      ],
      "category": "web-search"
    },
    "spences10--mcp-jinaai-reader": {
      "owner": "spences10",
      "name": "mcp-jinaai-reader",
      "url": "https://github.com/spences10/mcp-jinaai-reader",
      "imageUrl": "/freedevtools/mcp/pfp/spences10.webp",
      "description": "Integrates Jina.ai's Reader API for efficient web content extraction, enabling analysis and processing of documentation and web content.",
      "stars": 30,
      "forks": 4,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-09-10T19:15:34Z",
      "readme_content": "# mcp-jinaai-reader\n---\n\n## ⚠️ Notice\n\n**This repository is no longer maintained.**\n\nThe functionality of this tool is now available in [mcp-omnisearch](https://github.com/spences10/mcp-omnisearch), which combines multiple MCP tools in one unified package.\n\nPlease use [mcp-omnisearch](https://github.com/spences10/mcp-omnisearch) instead.\n\n---\n\nA Model Context Protocol (MCP) server for integrating Jina.ai's Reader\nAPI with LLMs. This server provides efficient and comprehensive web\ncontent extraction capabilities, optimized for documentation and web\ncontent analysis.\n\n<a href=\"https://glama.ai/mcp/servers/a75afsx9cx\">\n  <img alt=\"badge\" width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/a75afsx9cx/badge\" />\n</a>\n\n## Features\n\n- 📚 Advanced web content extraction through Jina.ai Reader API\n- 🚀 Fast and efficient content retrieval\n- 📄 Complete text extraction with preserved structure\n- 🔄 Clean format optimized for LLMs\n- 🌐 Support for various content types including documentation\n- 🏗️ Built on the Model Context Protocol\n\n## Configuration\n\nThis server requires configuration through your MCP client. Here are\nexamples for different environments:\n\n### Cline Configuration\n\nAdd this to your Cline MCP settings:\n\n```json\n{\n\t\"mcpServers\": {\n\t\t\"jinaai-reader\": {\n\t\t\t\"command\": \"node\",\n\t\t\t\"args\": [\"-y\", \"mcp-jinaai-reader\"],\n\t\t\t\"env\": {\n\t\t\t\t\"JINAAI_API_KEY\": \"your-jinaai-api-key\"\n\t\t\t}\n\t\t}\n\t}\n}\n```\n\n### Claude Desktop with WSL Configuration\n\nFor WSL environments, add this to your Claude Desktop configuration:\n\n```json\n{\n\t\"mcpServers\": {\n\t\t\"jinaai-reader\": {\n\t\t\t\"command\": \"wsl.exe\",\n\t\t\t\"args\": [\n\t\t\t\t\"bash\",\n\t\t\t\t\"-c\",\n\t\t\t\t\"JINAAI_API_KEY=your-jinaai-api-key npx mcp-jinaai-reader\"\n\t\t\t]\n\t\t}\n\t}\n}\n```\n\n### Environment Variables\n\nThe server requires the following environment variable:\n\n- `JINAAI_API_KEY`: Your Jina.ai API key (required)\n\n## API\n\nThe server implements a single MCP tool with configurable parameters:\n\n### read_url\n\nConvert any URL to LLM-friendly text using Jina.ai Reader.\n\nParameters:\n\n- `url` (string, required): URL to process\n- `no_cache` (boolean, optional): Bypass cache for fresh results.\n  Defaults to false\n- `format` (string, optional): Response format (\"json\" or \"stream\").\n  Defaults to \"json\"\n- `timeout` (number, optional): Maximum time in seconds to wait for\n  webpage load\n- `target_selector` (string, optional): CSS selector to focus on\n  specific elements\n- `wait_for_selector` (string, optional): CSS selector to wait for\n  specific elements\n- `remove_selector` (string, optional): CSS selector to exclude\n  specific elements\n- `with_links_summary` (boolean, optional): Gather all links at the\n  end of response\n- `with_images_summary` (boolean, optional): Gather all images at the\n  end of response\n- `with_generated_alt` (boolean, optional): Add alt text to images\n  lacking captions\n- `with_iframe` (boolean, optional): Include iframe content in\n  response\n\n## Development\n\n### Setup\n\n1. Clone the repository\n2. Install dependencies:\n\n```bash\nnpm install\n```\n\n3. Build the project:\n\n```bash\nnpm run build\n```\n\n4. Run in development mode:\n\n```bash\nnpm run dev\n```\n\n### Publishing\n\n1. Update version in package.json\n2. Build the project:\n\n```bash\nnpm run build\n```\n\n3. Publish to npm:\n\n```bash\nnpm publish\n```\n\n## Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n\n## License\n\nMIT License - see the [LICENSE](LICENSE) file for details.\n\n## Acknowledgments\n\n- Built on the\n  [Model Context Protocol](https://github.com/modelcontextprotocol)\n- Powered by [Jina.ai Reader API](https://jina.ai)\n",
      "npm_url": "https://www.npmjs.com/package/mcp-jinaai-reader",
      "npm_downloads": 4448,
      "keywords": [
        "jina",
        "search",
        "documentation",
        "content extraction",
        "jinaai reader",
        "documentation web"
      ],
      "category": "web-search"
    },
    "spences10--mcp-jinaai-search": {
      "owner": "spences10",
      "name": "mcp-jinaai-search",
      "url": "https://github.com/spences10/mcp-jinaai-search",
      "imageUrl": "/freedevtools/mcp/pfp/spences10.webp",
      "description": "Integrates Jina.ai's Search API with language models for efficient web search capabilities, optimized for retrieving clean and relevant content. Provides advanced search functionality for accessing comprehensive web information.",
      "stars": 3,
      "forks": 4,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-09-10T19:15:03Z",
      "readme_content": "# mcp-jinaai-search\n\n---\n\n## ⚠️ Notice\n\n**This repository is no longer maintained.**\n\nThe functionality of this tool is now available in [mcp-omnisearch](https://github.com/spences10/mcp-omnisearch), which combines multiple MCP tools in one unified package.\n\nPlease use [mcp-omnisearch](https://github.com/spences10/mcp-omnisearch) instead.\n\n---\n\nA Model Context Protocol (MCP) server for integrating Jina.ai's Search\nAPI with LLMs. This server provides efficient and comprehensive web\nsearch capabilities, optimised for retrieving clean, LLM-friendly\ncontent from the web.\n\n<a href=\"https://glama.ai/mcp/servers/u6603w196t\">\n  <img alt=\"badge\" width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/u6603w196t/badge\" />\n</a>\n\n## Features\n\n- 🔍 Advanced web search through Jina.ai Search API\n- 🚀 Fast and efficient content retrieval\n- 📄 Clean text extraction with preserved structure\n- 🧠 Content optimised for LLMs\n- 🌐 Support for various content types including documentation\n- 🏗️ Built on the Model Context Protocol\n- 🔄 Configurable caching for performance\n- 🖼️ Optional image and link gathering\n- 🌍 Localisation support through browser locale\n- 🎯 Token budget control for response size\n\n## Configuration\n\nThis server requires configuration through your MCP client. Here are\nexamples for different environments:\n\n### Cline Configuration\n\nAdd this to your Cline MCP settings:\n\n```json\n{\n\t\"mcpServers\": {\n\t\t\"jinaai-search\": {\n\t\t\t\"command\": \"node\",\n\t\t\t\"args\": [\"-y\", \"mcp-jinaai-search\"],\n\t\t\t\"env\": {\n\t\t\t\t\"JINAAI_API_KEY\": \"your-jinaai-api-key\"\n\t\t\t}\n\t\t}\n\t}\n}\n```\n\n### Claude Desktop with WSL Configuration\n\nFor WSL environments, add this to your Claude Desktop configuration:\n\n```json\n{\n\t\"mcpServers\": {\n\t\t\"jinaai-search\": {\n\t\t\t\"command\": \"wsl.exe\",\n\t\t\t\"args\": [\n\t\t\t\t\"bash\",\n\t\t\t\t\"-c\",\n\t\t\t\t\"JINAAI_API_KEY=your-jinaai-api-key npx mcp-jinaai-search\"\n\t\t\t]\n\t\t}\n\t}\n}\n```\n\n### Environment Variables\n\nThe server requires the following environment variable:\n\n- `JINAAI_API_KEY`: Your Jina.ai API key (required)\n\n## API\n\nThe server implements a single MCP tool with configurable parameters:\n\n### search\n\nSearch the web and get clean, LLM-friendly content using Jina.ai\nReader. Returns top 5 results with URLs and clean content.\n\nParameters:\n\n- `query` (string, required): Search query\n- `format` (string, optional): Response format (\"json\" or \"text\").\n  Defaults to \"text\"\n- `no_cache` (boolean, optional): Bypass cache for fresh results.\n  Defaults to false\n- `token_budget` (number, optional): Maximum number of tokens for this\n  request\n- `browser_locale` (string, optional): Browser locale for rendering\n  content\n- `stream` (boolean, optional): Enable stream mode for large pages.\n  Defaults to false\n- `gather_links` (boolean, optional): Gather all links at the end of\n  response. Defaults to false\n- `gather_images` (boolean, optional): Gather all images at the end of\n  response. Defaults to false\n- `image_caption` (boolean, optional): Caption images in the content.\n  Defaults to false\n- `enable_iframe` (boolean, optional): Extract content from iframes.\n  Defaults to false\n- `enable_shadow_dom` (boolean, optional): Extract content from shadow\n  DOM. Defaults to false\n- `resolve_redirects` (boolean, optional): Follow redirect chains to\n  final URL. Defaults to true\n\n## Development\n\n### Setup\n\n1. Clone the repository\n2. Install dependencies:\n\n```bash\npnpm install\n```\n\n3. Build the project:\n\n```bash\npnpm run build\n```\n\n4. Run in development mode:\n\n```bash\npnpm run dev\n```\n\n### Publishing\n\n1. Create a changeset:\n\n```bash\npnpm changeset\n```\n\n2. Version the package:\n\n```bash\npnpm version\n```\n\n3. Build and publish:\n\n```bash\npnpm release\n```\n\n## Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n\n## License\n\nMIT License - see the [LICENSE](LICENSE) file for details.\n\n## Acknowledgments\n\n- Built on the\n  [Model Context Protocol](https://github.com/modelcontextprotocol)\n- Powered by [Jina.ai Search API](https://jina.ai)\n",
      "npm_url": "https://www.npmjs.com/package/mcp-jinaai-search",
      "npm_downloads": 4113,
      "keywords": [
        "search",
        "jinaai",
        "jina",
        "jinaai search",
        "web search",
        "search api"
      ],
      "category": "web-search"
    },
    "spences10--mcp-perplexity-search": {
      "owner": "spences10",
      "name": "mcp-perplexity-search",
      "url": "https://github.com/spences10/mcp-perplexity-search",
      "imageUrl": "/freedevtools/mcp/pfp/spences10.webp",
      "description": "Integrates Perplexity's AI API with large language models for enhanced chat completion capabilities, featuring specialized prompt templates for a variety of use cases.",
      "stars": 8,
      "forks": 4,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-09-10T19:13:02Z",
      "readme_content": "# mcp-perplexity-search\n\n---\n\n## ⚠️ Notice\n\n**This repository is no longer maintained.**\n\nThe functionality of this tool is now available in [mcp-omnisearch](https://github.com/spences10/mcp-omnisearch), which combines multiple MCP tools in one unified package.\n\nPlease use [mcp-omnisearch](https://github.com/spences10/mcp-omnisearch) instead.\n\n---\n\nA Model Context Protocol (MCP) server for integrating Perplexity's AI\nAPI with LLMs. This server provides advanced chat completion\ncapabilities with specialized prompt templates for various use cases.\n\n<a href=\"https://glama.ai/mcp/servers/zlqdizpsr9\">\n  <img alt=\"badge\" width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/zlqdizpsr9/badge\" />\n</a>\n\n## Features\n\n- 🤖 Advanced chat completion using Perplexity's AI models\n- 📝 Predefined prompt templates for common scenarios:\n  - Technical documentation generation\n  - Security best practices analysis\n  - Code review and improvements\n  - API documentation in structured format\n- 🎯 Custom template support for specialized use cases\n- 📊 Multiple output formats (text, markdown, JSON)\n- 🔍 Optional source URL inclusion in responses\n- ⚙️ Configurable model parameters (temperature, max tokens)\n- 🚀 Support for various Perplexity models including Sonar and LLaMA\n\n## Configuration\n\nThis server requires configuration through your MCP client. Here are\nexamples for different environments:\n\n### Cline Configuration\n\nAdd this to your Cline MCP settings:\n\n```json\n{\n\t\"mcpServers\": {\n\t\t\"mcp-perplexity-search\": {\n\t\t\t\"command\": \"npx\",\n\t\t\t\"args\": [\"-y\", \"mcp-perplexity-search\"],\n\t\t\t\"env\": {\n\t\t\t\t\"PERPLEXITY_API_KEY\": \"your-perplexity-api-key\"\n\t\t\t}\n\t\t}\n\t}\n}\n```\n\n### Claude Desktop with WSL Configuration\n\nFor WSL environments, add this to your Claude Desktop configuration:\n\n```json\n{\n\t\"mcpServers\": {\n\t\t\"mcp-perplexity-search\": {\n\t\t\t\"command\": \"wsl.exe\",\n\t\t\t\"args\": [\n\t\t\t\t\"bash\",\n\t\t\t\t\"-c\",\n\t\t\t\t\"source ~/.nvm/nvm.sh && PERPLEXITY_API_KEY=your-perplexity-api-key /home/username/.nvm/versions/node/v20.12.1/bin/npx mcp-perplexity-search\"\n\t\t\t]\n\t\t}\n\t}\n}\n```\n\n### Environment Variables\n\nThe server requires the following environment variable:\n\n- `PERPLEXITY_API_KEY`: Your Perplexity API key (required)\n\n## API\n\nThe server implements a single MCP tool with configurable parameters:\n\n### chat_completion\n\nGenerate chat completions using the Perplexity API with support for\nspecialized prompt templates.\n\nParameters:\n\n- `messages` (array, required): Array of message objects with:\n  - `role` (string): 'system', 'user', or 'assistant'\n  - `content` (string): The message content\n- `prompt_template` (string, optional): Predefined template to use:\n  - `technical_docs`: Technical documentation with code examples\n  - `security_practices`: Security implementation guidelines\n  - `code_review`: Code analysis and improvements\n  - `api_docs`: API documentation in JSON format\n- `custom_template` (object, optional): Custom prompt template with:\n  - `system` (string): System message for assistant behaviour\n  - `format` (string): Output format preference\n  - `include_sources` (boolean): Whether to include sources\n- `format` (string, optional): 'text', 'markdown', or 'json' (default:\n  'text')\n- `include_sources` (boolean, optional): Include source URLs (default:\n  false)\n- `model` (string, optional): Perplexity model to use (default:\n  'sonar')\n- `temperature` (number, optional): Output randomness (0-1, default:\n  0.7)\n- `max_tokens` (number, optional): Maximum response length\n  (default: 1024)\n\n## Development\n\n### Setup\n\n1. Clone the repository\n2. Install dependencies:\n\n```bash\npnpm install\n```\n\n3. Build the project:\n\n```bash\npnpm build\n```\n\n4. Run in development mode:\n\n```bash\npnpm dev\n```\n\n### Publishing\n\nThe project uses changesets for version management. To publish:\n\n1. Create a changeset:\n\n```bash\npnpm changeset\n```\n\n2. Version the package:\n\n```bash\npnpm changeset version\n```\n\n3. Publish to npm:\n\n```bash\npnpm release\n```\n\n## Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n\n## License\n\nMIT License - see the [LICENSE](LICENSE) file for details.\n\n## Acknowledgments\n\n- Built on the\n  [Model Context Protocol](https://github.com/modelcontextprotocol)\n- Powered by\n  [Perplexity SONAR](https://docs.perplexity.ai/api-reference/chat-completions)\n",
      "npm_url": "https://www.npmjs.com/package/mcp-perplexity-search",
      "npm_downloads": 5987,
      "keywords": [
        "search",
        "completion",
        "ai",
        "chat completion",
        "perplexity ai",
        "perplexity search"
      ],
      "category": "web-search"
    },
    "spragginsdesigns--perplexity-mcp-server": {
      "owner": "spragginsdesigns",
      "name": "perplexity-mcp-server",
      "url": "https://github.com/spragginsdesigns/perplexity-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/spragginsdesigns.webp",
      "description": "Enables web searching capabilities by utilizing the Perplexity AI API to fetch and present web search results. It serves as an interface for retrieving information from the internet based on user queries.",
      "stars": 3,
      "forks": 4,
      "license": "No License",
      "language": "TypeScript",
      "updated_at": "2025-06-20T01:37:15Z",
      "readme_content": "# Perplexity MCP Server\n\nThis is a simple MCP server that allows you to search the web using Perplexity AI.\n\n## \n\nMCP Review Certified: https://mcpreview.com/mcp-servers/spragginsdesigns/perplexity-mcp-server\n\n## Installation\n\n### Prerequisites\n- Node.js 18+ (Download from [nodejs.org](https://nodejs.org/))\n- Git (Download from [git-scm.com](https://git-scm.com/download/win))\n- A Perplexity AI API key\n\n### Steps for Windows\n\n1. Clone the repository:\n```bash\ngit clone [repository-url]\ncd perplexity-mcp-server\n```\n\n2. Install dependencies:\n```bash\nnpm install\n```\n\n3. Create environment file:\n   - Create a new file named `.env` in the root directory\n   - Add your Perplexity AI API key:\n   ```env\n   PERPLEXITY_API_KEY=your_api_key_here\n   ```\n\n## Usage on Windows\n\n### Development Mode\n```bash\nnpm run dev\n```\n\n### Production Mode\n1. Build the project:\n```bash\nnpm run build\n```\n\n2. Start the server:\n```bash\nnpm start\n```\n\n### Using the Windows Batch File (Recommended)\nTo avoid environment variable issues on Windows, use the included batch file:\n\n1. Build the project first:\n```bash\nnpm run build\n```\n\n2. Run the batch file:\n```bash\n.\\run-perplexity-server.bat\n```\n\nAlternatively, double-click the `run-perplexity-server.bat` file in Windows Explorer.\n\nFor detailed instructions, see the [Windows Setup Guide](./docs/windows-setup.md).\n\n## Troubleshooting Windows Issues\n\n### Common Issues\n\n1. **Permission Errors**\n   - Run Command Prompt or PowerShell as Administrator\n   - Check file permissions in the project directory\n\n2. **Environment Variables**\n   - Ensure `.env` file is in the root directory\n   - No spaces around the `=` sign in `.env` file\n   - Restart terminal after making changes to environment variables\n   - Use the provided batch file to avoid environment variable issues\n\n3. **Node.js Issues**\n   - Verify Node.js installation: `node --version`\n   - Ensure npm is installed: `npm --version`\n\n### Error Messages\n\nIf you see `Error: ENOENT: no such file or directory`, ensure:\n- All paths use correct Windows-style separators\n- You're in the correct directory\n- Required files exist\n\n## Project Structure\n```\nperplexity-mcp-server/\n├── src/\n│   └── index.ts          # Main server implementation\n├── dist/                 # Compiled output\n├── .env                  # Environment variables\n├── package.json          # Project configuration\n├── run-perplexity-server.bat # Windows batch file\n└── tsconfig.json         # TypeScript configuration\n```\n\n## API Usage\n\nThe server provides a single tool `perplexity_search` for web searches:\n\n```json\n{\n  \"name\": \"perplexity_search\",\n  \"arguments\": {\n    \"query\": \"your search query here\"\n  }\n}\n```\n\n## License\n\nMIT\n",
      "npm_url": "https://www.npmjs.com/package/perplexity-mcp-server",
      "npm_downloads": 1579,
      "keywords": [
        "perplexity",
        "searching",
        "search",
        "utilizing perplexity",
        "search spragginsdesigns",
        "web searching"
      ],
      "category": "web-search"
    },
    "steel-dev--steel-mcp-server": {
      "owner": "steel-dev",
      "name": "steel-mcp-server",
      "url": "https://github.com/steel-dev/steel-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/steel-dev.webp",
      "description": "Connects LLMs to the web using Puppeteer-based tools, enabling actions like clicking, scrolling, typing, and taking screenshots. Facilitates tasks such as searching for recipes, tracking shipments, comparing product prices, and filling out online forms.",
      "stars": 43,
      "forks": 9,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-10-02T15:08:00Z",
      "readme_content": "# Steel MCP Server\n\n[![smithery badge](https://smithery.ai/badge/@steel-dev/steel-mcp-server)](https://smithery.ai/server/@steel-dev/steel-mcp-server)\n\nhttps://github.com/user-attachments/assets/25848033-40ea-4fa4-96f9-83b6153a0212\n\n\nA Model Context Protocol (MCP) server that enables LLMs like Claude to navigate the web through Puppeteer-based tools and Steel. Based on the Web Voyager framework, it provides tools for all the standard web actions click clicking/scrolling/typing/etc and taking screenshots.\n\nAsk Claude to help you with tasks like:\n- \"Search for a recipe and save the ingredients list\"\n- \"Track a package delivery status\"\n- \"Find and compare prices for a specific product\"\n- \"Fill out an online job application\"\n\n<a href=\"https://glama.ai/mcp/servers/tbd32geble\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/tbd32geble/badge\" alt=\"Steel Server MCP server\" /></a>\n\n## 🚀 Quick Start\n\nBelow is a streamlined guide to run Steel Voyager inside Claude Desktop. You only need to adjust the environment options to switch between Steel Cloud and a local/self-hosted instance.\n\n### Prerequisites\n\n1. Latest versions of Git and Node.js installed\n2. [Claude Desktop](https://claude.ai/download) installed\n3. (Optional) [Steel Docker image](https://github.com/steel-dev/steel-browser) running locally, if you plan to self-host\n4. (Optional) If running Steel Cloud, bring your API key. Get one [here](https://app.steel.dev/settings/api-keys).\n\n---\n\n### A) Quick Start (Steel Cloud)\n\n1. Clone and build the project:\n\n   ```bash\n   git clone https://github.com/steel-dev/steel-mcp-server.git\n   cd steel-mcp-server\n   npm install\n   npm run build\n   ```\n\n2. Configure Claude Desktop (`~/Library/Application Support/Claude/claude_desktop_config.json`) by adding a server entry:\n\n   ```json\n   {\n     \"mcpServers\": {\n       \"steel-puppeteer\": {\n         \"command\": \"node\",\n         \"args\": [\"path/to/steel-voyager/dist/index.js\"],\n         \"env\": {\n           \"STEEL_LOCAL\": \"false\",\n           \"STEEL_API_KEY\": \"YOUR_STEEL_API_KEY_HERE\",\n           \"GLOBAL_WAIT_SECONDS\": \"1\"\n         }\n       }\n     }\n   }\n   ```\n\n   - Replace \"YOUR_STEEL_API_KEY_HERE\" with your valid Steel API key.\n   - Make sure \"STEEL_LOCAL\" is set to \"false\" for cloud mode.\n\n3. Start Claude Desktop. It will automatically launch this MCP server in Cloud mode.\n\n4. (Optional) You can view or manage active Steel Browser sessions in your [dashboard](https://app.steel.dev).\n\n---\n\n### B) Quick Start (Local / Self-Hosted Steel)\n\n1. Ensure your local or self-hosted Steel service is running (e.g., using the open-source Steel Docker image).\n\n2. Clone and build the project (same as above if not done yet):\n\n   ```bash\n   git clone https://github.com/steel-dev/steel-mcp-server.git\n   cd steel-mcp-server\n   npm install\n   npm run build\n   ```\n\n3. Configure Claude Desktop (`~/Library/Application Support/Claude/claude_desktop_config.json`) for local mode:\n\n   ```json\n   {\n     \"mcpServers\": {\n       \"steel-puppeteer\": {\n         \"command\": \"node\",\n         \"args\": [\"path/to/steel-voyager/dist/index.js\"],\n         \"env\": {\n           \"STEEL_LOCAL\": \"true\",\n           \"STEEL_BASE_URL\": \"http://localhost:3000\",\n           \"GLOBAL_WAIT_SECONDS\": \"1\"\n         }\n       }\n     }\n   }\n   ```\n\n   - \"STEEL_LOCAL\" must be \"true\".\n   - If self hosting on a cloud server, configure \"STEEL_BASE_URL\" to point to your local/self-hosted Steel URL.\n\n4. Start Claude Desktop, which will connect to your locally running Steel and launch Steel Voyager in local mode.\n\n5. (Optional) To view sessions locally, you can visit your self-hosted dashboard ([localhost:5173](http://localhost:5173/)) or logs specific to your Steel runtime environment.\n\n---\n\nThat’s it! Once Claude Desktop starts, it will orchestrate the MCP server behind the scenes and let you interact with the web automation capabilities through Steel Voyager.\n\nFor more info on getting set up or if you're having issues, check out the MCP set-up docs: https://modelcontextprotocol.io/quickstart/user\n\n## Components\n\n### Tools\n\n- **navigate**\n\n  - Navigate to any URL in the browser\n  - Inputs:\n    - `url` (string, required): URL to navigate to (e.g. \"https://example.com\").\n\n- **search**\n\n  - Perform a Google search by navigating to \"https://www.google.com/search?q=encodedQuery\".\n  - Inputs:\n    - `query` (string, required): Text to search for on Google.\n\n- **click**\n\n  - Click elements on the page using numbered labels\n  - Inputs:\n    - `label` (number, required): The label number of the element to click.\n\n- **type**\n\n  - Type text into input fields using numbered labels\n  - Inputs:\n    - `label` (number, required): The label number of the input field.\n    - `text` (string, required): Text to type into the field.\n    - `replaceText` (boolean, optional): If true, replaces any existing text in the field.\n\n- **scroll_down**\n\n  - Scroll down the page\n  - Inputs:\n    - `pixels` (integer, optional): Number of pixels to scroll down. If not specified, scrolls by one full page.\n\n- **scroll_up**\n\n  - Scroll up the page\n  - Inputs:\n    - `pixels` (integer, optional): Number of pixels to scroll up. If not specified, scrolls by one full page.\n\n- **go_back**\n\n  - Navigate to the previous page in browser history\n  - No inputs required\n\n- **wait**\n\n  - Wait for up to 10 seconds, useful for pages that load slowly or need more time for dynamic content to appear.\n  - Inputs:\n    - `seconds` (number, required): Number of seconds to wait (0 to 10).\n\n- **save_unmarked_screenshot**\n  - Capture the current page without bounding boxes or highlights and store it as a resource.\n  - Inputs:\n    - `resourceName` (string, optional): Name to store the screenshot under (e.g. \"before_login\"). If omitted, a generic name is generated automatically.\n\n### Resources\n\n- **Screenshots**:\n  Each saved screenshot is accessible via an MCP resource URI in the form of:\n  • `screenshot://RESOURCE_NAME`\n\n  The server stores these screenshots whenever you specify the \"save_unmarked_screenshot\" tool or when an action concludes (for most tools) with an annotated screenshot. These images can be retrieved through a standard MCP resource retrieval request.\n\n(Note: While console logs are still collected for analysis and debugging, they are not exposed as retrievable resources in this implementation. They appear in the server’s logs but are not served via MCP resource URIs.)\n\n## Key Features\n\n- Browser automation with Puppeteer\n- Steel integration for browser session management\n- Visual element identification through numbered labels\n- Screenshot capabilities\n- Basic web interaction (navigation, clicking, form filling)\n- Lazy-loading support through scrolling\n- Local and remote Steel instance support\n\n## Understanding Bounding Boxes\n\nWhen interacting with pages, Steel Puppeteer adds visual overlays to help identify interactive elements:\n\n- Each interactive element (buttons, links, inputs) gets a unique numbered label\n- Colored boxes outline the elements' boundaries\n- Labels appear above or inside elements for easy reference\n- Use these numbers when specifying elements for click or type operations\n\n\n## Configuration\n\nSteel Voyager can run in two modes: \"Local\" or \"Cloud\". This behavior is controlled by environment variables. Below is a concise overview:\n\n| Environment Variable | Default                 | Description                                                                                                                                                                                                                    |\n| -------------------- | ----------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |\n| STEEL_LOCAL          | \"false\"                 | Determines if Steel Voyager runs in local (true) or cloud (false) mode.                                                                                                                                                        |\n| STEEL_API_KEY        | (none)                  | Required only when STEEL_LOCAL = \"false\". Used to authenticate requests with the Steel endpoint.                                                                                                                               |\n| STEEL_BASE_URL       | \"https://api.steel.dev\" | The base URL for the Steel API. Override this if self-hosting the Steel server (either locally or in your own cloud environment). If STEEL_LOCAL = \"true\" and STEEL_BASE_URL is unset, it defaults to \"http://localhost:3000\". |\n| GLOBAL_WAIT_SECONDS  | (none)                  | Optional. Number of seconds to wait after each tool action (for instance, to allow slow-loading pages).                                                                                                                        |\n\n### Local Mode\n\n1. Set STEEL_LOCAL=\"true\".\n2. (Optional) Set STEEL_BASE_URL to point to the Steel server if you host it on a custom domain. Otherwise, Steel Voyager will default to http://localhost:3000.\n3. No API key is required in this mode.\n4. Puppeteer will connect via ws://0.0.0.0:3000\n\nExample:\n\nexport STEEL_LOCAL=\"true\"\n\nexport STEEL_BASE_URL=\"http://localhost:3000\" # only if overriding\n\n### Cloud Mode\n\n1. Set STEEL_LOCAL=\"false\".\n2. Set STEEL_API_KEY so Steel Voyager can authenticate with the Steel cloud service (or your self-hosted Steel if you changed STEEL_BASE_URL).\n3. STEEL_BASE_URL defaults to https://api.steel.dev; override this if you have a self-hosted Steel instance running on another endpoint.\n4. Puppeteer will connect via wss://connect.steel.dev?sessionId=…&apiKey=…\n\nExample:\n\nexport STEEL_LOCAL=\"false\"\n\nexport STEEL_API_KEY=\"YOUR_STEEL_API_KEY_HERE\"\n\n### Claude Desktop Configuration\n\nTo use Steel Voyager with Claude Desktop, add something like this to your config file (often located at\n~/Library/Application Support/Claude/claude_desktop_config.json):\n\n```json\n{\n  \"mcpServers\": {\n    \"steel-puppeteer\": {\n      \"command\": \"node\",\n      \"args\": [\"path/to/steel-puppeteer/dist/index.js\"],\n      \"env\": {\n        \"STEEL_LOCAL\": \"false\",\n        \"STEEL_API_KEY\": \"your_api_key_here\"\n      }\n    }\n  }\n}\n```\n\nAdjust the environment variables to match your desired mode:\n\n• If running locally/self hosted, keep `\"STEEL_LOCAL\": \"true\"` and optionally `\"STEEL_BASE_URL\": \"http://localhost:3000\"`.  \n• If running in cloud mode, remove `\"STEEL_LOCAL\": \"true\"`, add `\"STEEL_LOCAL\": \"false\"`, and supply `\"STEEL_API_KEY\": \"<YourKey>\"`\nThis will allow Claude Desktop to start Steel Voyager in the correct mode.\n\n## Installation & Running\n\n### Installing via Smithery\n\nTo install Steel MCP Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@steel-dev/steel-mcp-server):\n\n```bash\nnpx -y @smithery/cli install @steel-dev/steel-mcp-server --client claude\n```\n\n### Local Development\n\n1. Clone the repository\n2. Install dependencies:\n   ```bash\n   npm install\n   ```\n3. Build the project:\n   ```bash\n   npm run build\n   ```\n4. Start the server:\n   ```bash\n   npm start\n   ```\n\n\n## Example Usage 📹\n\nWe asked Claude to impress us with it's new abilities and it decided to research the latest developments with sora then create an interactive visualization to demonstrate the data behind the model and how it works 🤯\n\n\nhttps://github.com/user-attachments/assets/8d4293ea-03fc-459f-ba6b-291f5b017ad7\n\n*Sorry for quality, github forces us to keep the videos under 10mb :/\n\n## Troubleshooting\n\nCommon issues and solutions:\n\n1. Verify your Steel API key when using cloud service and ensure your local Steel instance is running. Check that you have proper network connectivity to the service.\n\n2. If you're having issues with how pages are being rendered or marked up and sent to claude, try to add a delay in your config via the `GLOBAL_WAIT_SECONDS` env variable.\n\n3. Ensure the page has fully loaded and check your viewport size settings. Make sure your system has sufficient available memory for capturing screenshots.\n\n4. Session clean up isn't the best right now so you may need to manually release sessions as they're spun up to execute tasks.\n\n5. Prompting claude the right way can go a long way in improving performance and avoiding silly mistakes it may produce.\n\n6. Leverage the session viewer to analyse where your model may be getting stopped out.\n\n7. After ~15-20 browser actions claude starts to slow down as it's context window gets filled but with images. It shouldn't be horrible but we've noticed some latency here, especially with the Claude Desktop client lagging behind.\n\n## Contributing\n\nThis project is experimental and under active development. Contributions are welcome!\n\n1. Fork the repository\n2. Create a feature branch\n3. Submit a pull request\n\nPlease include:\n\n- Clear description of changes\n- Motivation\n- Documentation updates\n\n## Disclaimer\n\n⚠️ This project is experimental and based on the Web Voyager codebase. Use in production environments at your own risk.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "puppeteer",
        "llms",
        "web",
        "llms web",
        "puppeteer based",
        "using puppeteer"
      ],
      "category": "web-search"
    },
    "stickerdaniel--linkedin-mcp-server": {
      "owner": "stickerdaniel",
      "name": "linkedin-mcp-server",
      "url": "https://github.com/stickerdaniel/linkedin-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/stickerdaniel.webp",
      "description": "Interact with LinkedIn to scrape profiles, analyze companies, and search for job listings. Provides insights and recommendations based on LinkedIn data.",
      "stars": 436,
      "forks": 76,
      "license": "Apache License 2.0",
      "language": "Python",
      "updated_at": "2025-10-03T04:13:07Z",
      "readme_content": "# LinkedIn MCP Server\n\n<p align=\"left\">\n  <a href=\"https://github.com/stickerdaniel/linkedin-mcp-server/actions/workflows/ci.yml\" target=\"_blank\"><img src=\"https://github.com/stickerdaniel/linkedin-mcp-server/actions/workflows/ci.yml/badge.svg?branch=main\" alt=\"CI Status\"></a>\n  <a href=\"https://github.com/stickerdaniel/linkedin-mcp-server/actions/workflows/release.yml\" target=\"_blank\"><img src=\"https://github.com/stickerdaniel/linkedin-mcp-server/actions/workflows/release.yml/badge.svg?branch=main\" alt=\"Release\"></a>\n  <a href=\"https://github.com/stickerdaniel/linkedin-mcp-server/blob/main/LICENSE\" target=\"_blank\"><img src=\"https://img.shields.io/badge/License-Apache%202.0-brightgreen?labelColor=32383f\" alt=\"License\"></a>\n</p>\n\nThrough this LinkedIn MCP server, AI assistants like Claude can connect to your LinkedIn. Give access to profiles and companies, get your recommended jobs, or search for keywords. All from a Docker container on your machine.\n\n## Installation Methods\n\n[![Docker](https://img.shields.io/badge/Docker-Universal_MCP-008fe2?style=for-the-badge&logo=docker&logoColor=008fe2)](#-docker-setup-recommended---universal)\n[![Install DXT Extension](https://img.shields.io/badge/Claude_Desktop_DXT-d97757?style=for-the-badge&logo=anthropic)](#-claude-desktop-dxt-extension)\n[![uvx](https://img.shields.io/badge/uvx-Quick_Install-de5fe9?style=for-the-badge&logo=data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iNDEiIGhlaWdodD0iNDEiIHZpZXdCb3g9IjAgMCA0MSA0MSIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPHBhdGggZD0iTS01LjI4NjE5ZS0wNiAwLjE2ODYyOUwwLjA4NDMwOTggMjAuMTY4NUwwLjE1MTc2MiAzNi4xNjgzQzAuMTYxMDc1IDM4LjM3NzQgMS45NTk0NyA0MC4xNjA3IDQuMTY4NTkgNDAuMTUxNEwyMC4xNjg0IDQwLjA4NEwzMC4xNjg0IDQwLjA0MThMMzEuMTg1MiA0MC4wMzc1QzMzLjM4NzcgNDAuMDI4MiAzNS4xNjgzIDM4LjIwMjYgMzUuMTY4MyAzNlYzNkwzNy4wMDAzIDM2TDM3LjAwMDMgMzkuOTk5Mkw0MC4xNjgzIDM5Ljk5OTZMMzkuOTk5NiAtOS45NDY1M2UtMDdMMjEuNTk5OCAwLjA3NzU2ODlMMjEuNjc3NCAxNi4wMTg1TDIxLjY3NzQgMjUuOTk5OEwyMC4wNzc0IDI1Ljk5OThMMTguMzk5OCAyNS45OTk4TDE4LjQ3NzQgMTYuMDMyTDE4LjM5OTggMC4wOTEwNTkzTC01LjI4NjE5ZS0wNiAwLjE2ODYyOVoiIGZpbGw9IiNERTVGRTkiLz4KPC9zdmc+Cg==)](#-uvx-setup-quick-install---universal)\n[![Development](https://img.shields.io/badge/Development-Local-ffdc53?style=for-the-badge&logo=python&logoColor=ffdc53)](#-local-setup-develop--contribute)\n\nhttps://github.com/user-attachments/assets/eb84419a-6eaf-47bd-ac52-37bc59c83680\n\n## Usage Examples\n```\nWhat are my recommended jobs I can apply to?\n```\n```\nResearch the background of this candidate https://www.linkedin.com/in/stickerdaniel/\n```\n```\nGet this company profile for partnership discussions https://www.linkedin.com/company/inframs/\n```\n```\nSuggest improvements for my CV to target this job posting https://www.linkedin.com/jobs/view/4252026496\n```\n\n## Features & Tool Status\n> [!TIP]\n> - **Profile Scraping** (`get_person_profile`): Get detailed information from a LinkedIn profile including work history, education, skills, and connections\n> - **Company Analysis** (`get_company_profile`): Extract comprehensive company information from a LinkedIn company profile name\n> - **Job Details** (`get_job_details`): Retrieve specific job posting details using LinkedIn job IDs\n> - **Job Search** (`search_jobs`): Search for jobs with filters like keywords and location\n> - **Recommended Jobs** (`get_recommended_jobs`): Get personalized job recommendations based on your profile\n> - **Session Management** (`close_session`): Properly close browser session and clean up resources\n\n> [!NOTE]\n> July 2025: All tools are currently functional and actively maintained. If you encounter any issues, please report them in the [GitHub issues](https://github.com/stickerdaniel/linkedin-mcp-server/issues).\n\n<br/>\n<br/>\n\n## 🐳 Docker Setup (Recommended - Universal)\n\n**Prerequisites:** Make sure you have [Docker](https://www.docker.com/get-started/) installed and running.\n\n### Installation\n\n**Client Configuration:**\n```json\n{\n  \"mcpServers\": {\n    \"linkedin\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\", \"--rm\", \"-i\",\n        \"-e\", \"LINKEDIN_COOKIE\",\n        \"stickerdaniel/linkedin-mcp-server:latest\"\n      ],\n      \"env\": {\n        \"LINKEDIN_COOKIE\": \"li_at=YOUR_COOKIE_VALUE\"\n      }\n    }\n  }\n}\n```\n\n### Getting the LinkedIn Cookie\n<details>\n<summary><b>🌐 Chrome DevTools Guide</b></summary>\n\n1. Open LinkedIn and login\n2. Open Chrome DevTools (F12 or right-click → Inspect)\n3. Go to **Application** > **Storage** > **Cookies** > **https://www.linkedin.com**\n4. Find the cookie named `li_at`\n5. Copy the **Value** field (this is your LinkedIn session cookie)\n6. Use this value as your `LINKEDIN_COOKIE` in the configuration\n\n</details>\n<details>\n<summary><b>🐳 Docker get-cookie method</b></summary>\n\n**Run the server with the `--get-cookie` flag:**\n```bash\ndocker run -it --rm \\\n  stickerdaniel/linkedin-mcp-server:latest \\\n  --get-cookie\n```\nCopy the cookie from the output and set it as `LINKEDIN_COOKIE` in your client configuration. If this fails with a captcha challenge, use the method above.\n</details>\n\n> [!NOTE]\n> The cookie will expire during the next 30 days. Just get the new cookie and update your client config. There are also many cookie manager extensions that you can use to quickly copy the cookie.\n\n### Docker Setup Help\n<details>\n<summary><b>🔧 Configuration</b></summary>\n\n**Transport Modes:**\n- **Default (stdio)**: Standard communication for local MCP servers\n- **Streamable HTTP**: For a web-based MCP server\n\n**CLI Options:**\n- `--log-level {DEBUG,INFO,WARNING,ERROR}` - Set logging level (default: WARNING)\n- `--no-lazy-init` - Login to LinkedIn immediately instead of waiting for the first tool call\n- `--transport {stdio,streamable-http}` - Set transport mode\n- `--host HOST` - HTTP server host (default: 127.0.0.1)\n- `--port PORT` - HTTP server port (default: 8000)\n- `--path PATH` - HTTP server path (default: /mcp)\n- `--get-cookie` - Attempt to login with email and password and extract the LinkedIn cookie\n- `--cookie {cookie}` - Pass a specific LinkedIn cookie for login\n- `--user-agent {user_agent}` - Specify custom user agent string to prevent anti-scraping detection\n\n**HTTP Mode Example (for web-based MCP clients):**\n```bash\ndocker run -it --rm \\\n  -e LINKEDIN_COOKIE=\"li_at=YOUR_COOKIE_VALUE\" \\\n  -p 8080:8080 \\\n  stickerdaniel/linkedin-mcp-server:latest \\\n  --transport streamable-http --host 0.0.0.0 --port 8080 --path /mcp\n```\n\n**Test with mcp inspector:**\n1. Install and run mcp inspector ```bunx @modelcontextprotocol/inspector```\n2. Click pre-filled token url to open the inspector in your browser\n3. Select `Streamable HTTP` as `Transport Type`\n4. Set `URL` to `http://localhost:8080/mcp`\n5. Connect\n6. Test tools\n\n</details>\n\n<details>\n<summary><b>❗ Troubleshooting</b></summary>\n\n**Docker issues:**\n- Make sure [Docker](https://www.docker.com/get-started/) is installed\n- Check if Docker is running: `docker ps`\n\n**Login issues:**\n- Ensure your LinkedIn cookie is set and correct\n- Make sure you have only one active LinkedIn session per cookie at a time. Trying to open multiple sessions with the same cookie will result in a cookie invalid error.\n- LinkedIn may require a login confirmation in the LinkedIn mobile app for --get-cookie\n- You might get a captcha challenge if you logged in a lot of times in a short period of time, then try again later or follow the [local setup instructions](#-local-setup-develop--contribute) to run the server manually in --no-headless mode where you can debug the login process (solve captcha manually)\n</details>\n\n<br/>\n<br/>\n\n## 📦 Claude Desktop (DXT Extension)\n\n**Prerequisites:** [Claude Desktop](https://claude.ai/download) and [Docker](https://www.docker.com/get-started/) installed\n\n**One-click installation** for Claude Desktop users:\n1. Download the [DXT extension](https://github.com/stickerdaniel/linkedin-mcp-server/releases/latest)\n2. Double-click to install into Claude Desktop\n3. Set your LinkedIn cookie in the extension settings\n\n### Getting the LinkedIn Cookie\n<details>\n<summary><b>🌐 Chrome DevTools Guide</b></summary>\n\n1. Open LinkedIn and login\n2. Open Chrome DevTools (F12 or right-click → Inspect)\n3. Go to **Application** > **Storage** > **Cookies** > **https://www.linkedin.com**\n4. Find the cookie named `li_at`\n5. Copy the **Value** field (this is your LinkedIn session cookie)\n6. Use this value as your `LINKEDIN_COOKIE` in the configuration\n\n</details>\n<details>\n<summary><b>🐳 Docker get-cookie method</b></summary>\n\n**Run the server with the `--get-cookie` flag:**\n```bash\ndocker run -it --rm \\\n  stickerdaniel/linkedin-mcp-server:latest \\\n  --get-cookie\n```\nCopy the cookie from the output and set it as `LINKEDIN_COOKIE` in your client configuration. If this fails with a captcha challenge, use the method above.\n</details>\n\n> [!NOTE]\n> The cookie will expire during the next 30 days. Just get the new cookie and update your client config. There are also many cookie manager extensions that you can use to quickly copy the cookie.\n\n### DXT Extension Setup Help\n<details>\n<summary><b>❗ Troubleshooting</b></summary>\n\n**Docker issues:**\n- Make sure [Docker](https://www.docker.com/get-started/) is installed\n- Check if Docker is running: `docker ps`\n\n**Login issues:**\n- Ensure your LinkedIn cookie is set and correct\n- Make sure you have only one active LinkedIn session per cookie at a time. Trying to open multiple sessions with the same cookie will result in a cookie invalid error.\n- LinkedIn may require a login confirmation in the LinkedIn mobile app for --get-cookie\n- You might get a captcha challenge if you logged in a lot of times in a short period of time, then try again later or follow the [local setup instructions](#-local-setup-develop--contribute) to run the server manually in --no-headless mode where you can debug the login process (solve captcha manually)\n</details>\n\n<br/>\n<br/>\n\n## 🚀 uvx Setup (Quick Install - Universal)\n\n**Prerequisites:** Make sure you have [uv](https://docs.astral.sh/uv/) installed.\n\n### Installation\n\nRun directly from GitHub without cloning:\n\n```bash\n# Run directly from GitHub (latest version)\nuvx --from git+https://github.com/stickerdaniel/linkedin-mcp-server linkedin-mcp-server --help\n\n# Run with your LinkedIn cookie\nuvx --from git+https://github.com/stickerdaniel/linkedin-mcp-server linkedin-mcp-server --cookie \"li_at=YOUR_COOKIE_VALUE\"\n```\n\n### Getting the LinkedIn Cookie\n<details>\n<summary><b>🌐 Chrome DevTools Guide</b></summary>\n\n1. Open LinkedIn and login\n2. Open Chrome DevTools (F12 or right-click → Inspect)\n3. Go to **Application** > **Storage** > **Cookies** > **https://www.linkedin.com**\n4. Find the cookie named `li_at`\n5. Copy the **Value** field (this is your LinkedIn session cookie)\n6. Use this value as your `LINKEDIN_COOKIE` in the configuration\n\n</details>\n\n<details>\n<summary><b>🚀 uvx get-cookie method</b></summary>\n\n**Run the server with the `--get-cookie` flag:**\n```bash\nuvx --from git+https://github.com/stickerdaniel/linkedin-mcp-server \\\n  linkedin-mcp-server --get-cookie\n```\nCopy the cookie from the output and set it as `LINKEDIN_COOKIE` in your client configuration. If this fails with a captcha challenge, use the method above.\n</details>\n\n> [!NOTE]\n> The cookie will expire during the next 30 days. Just get the new cookie and update your client config. There are also many cookie manager extensions that you can use to quickly copy the cookie.\n\n### uvx Setup Help\n<details>\n<summary><b>🔧 Configuration</b></summary>\n\n**Client Configuration:**\n```json\n{\n  \"mcpServers\": {\n    \"linkedin\": {\n      \"command\": \"uvx\",\n      \"args\": [\n        \"--from\",\n        \"git+https://github.com/stickerdaniel/linkedin-mcp-server\",\n        \"linkedin-mcp-server\"\n      ],\n      \"env\": {\n        \"LINKEDIN_COOKIE\": \"li_at=YOUR_COOKIE_VALUE\"\n      }\n    }\n  }\n}\n```\n\n**Transport Modes:**\n- **Default (stdio)**: Standard communication for local MCP servers\n- **Streamable HTTP**: For web-based MCP server\n\n**CLI Options:**\n- `--log-level {DEBUG,INFO,WARNING,ERROR}` - Set logging level (default: WARNING)\n- `--no-lazy-init` - Login to LinkedIn immediately instead of waiting for the first tool call\n- `--transport {stdio,streamable-http}` - Set transport mode\n- `--host HOST` - HTTP server host (default: 127.0.0.1)\n- `--port PORT` - HTTP server port (default: 8000)\n- `--path PATH` - HTTP server path (default: /mcp)\n- `--get-cookie` - Attempt to login with email and password and extract the LinkedIn cookie\n- `--cookie {cookie}` - Pass a specific LinkedIn cookie for login\n- `--user-agent {user_agent}` - Specify custom user agent string to prevent anti-scraping detection\n\n**Basic Usage Examples:**\n```bash\n# Run with cookie from environment variable\nLINKEDIN_COOKIE=\"YOUR_COOKIE_VALUE\" uvx --from git+https://github.com/stickerdaniel/linkedin-mcp-server linkedin-mcp-server\n\n# Run with cookie via flag\nuvx --from git+https://github.com/stickerdaniel/linkedin-mcp-server linkedin-mcp-server --cookie \"YOUR_COOKIE_VALUE\"\n\n# Run with debug logging\nuvx --from git+https://github.com/stickerdaniel/linkedin-mcp-server linkedin-mcp-server --log-level DEBUG\n\n# Extract cookie with credentials\nuvx --from git+https://github.com/stickerdaniel/linkedin-mcp-server linkedin-mcp-server --get-cookie\n```\n\n**HTTP Mode Example (for web-based MCP clients):**\n```bash\nuvx --from git+https://github.com/stickerdaniel/linkedin-mcp-server linkedin-mcp-server \\\n  --transport streamable-http --host 127.0.0.1 --port 8080 --path /mcp\n```\n\n**Test with mcp inspector:**\n1. Install and run mcp inspector ```bunx @modelcontextprotocol/inspector```\n2. Click pre-filled token url to open the inspector in your browser\n3. Select `Streamable HTTP` as `Transport Type`\n4. Set `URL` to `http://localhost:8080/mcp`\n5. Connect\n6. Test tools\n\n</details>\n\n<details>\n<summary><b>❗ Troubleshooting</b></summary>\n\n**Installation issues:**\n- Ensure you have uv installed: `curl -LsSf https://astral.sh/uv/install.sh | sh`\n- Check uv version: `uv --version` (should be 0.4.0 or higher)\n\n**Cookie issues:**\n- Ensure your LinkedIn cookie is set and correct\n- Cookie can be passed via `--cookie` flag or `LINKEDIN_COOKIE` environment variable\n- Make sure you have only one active LinkedIn session per cookie at a time\n\n**Login issues:**\n- LinkedIn may require a login confirmation in the LinkedIn mobile app for --get-cookie\n- You might get a captcha challenge if you logged in a lot of times in a short period\n</details>\n\n<br/>\n<br/>\n\n## 🐍 Local Setup (Develop & Contribute)\n\n**Prerequisites:** [Chrome browser](https://www.google.com/chrome/) and [Git](https://git-scm.com/downloads) installed\n\n**ChromeDriver Setup:**\n1. **Check Chrome version**: Chrome → menu (⋮) → Help → About Google Chrome\n2. **Download matching ChromeDriver**: [Chrome for Testing](https://googlechromelabs.github.io/chrome-for-testing/)\n3. **Make it accessible**:\n   - Place ChromeDriver in PATH (`/usr/local/bin` on macOS/Linux)\n   - Or set: `export CHROMEDRIVER_PATH=/path/to/chromedriver`\n   - if no CHROMEDRIVER_PATH is set, the server will try to find it automatically by checking common locations\n\n### Installation\n\n```bash\n# 1. Clone repository\ngit clone https://github.com/stickerdaniel/linkedin-mcp-server\ncd linkedin-mcp-server\n\n# 2. Install UV package manager\ncurl -LsSf https://astral.sh/uv/install.sh | sh\nuv python # install python if you don't have it\n\n# 3. Install dependencies and dev dependencies\nuv sync\nuv sync --group dev\n\n# 4. Install pre-commit hooks\nuv run pre-commit install\n\n# 5. Start the server once manually\n# You will be prompted to enter your LinkedIn credentials, and they will be securely stored in your OS keychain\n# Once logged in, your cookie will be stored in your OS keychain and used for subsequent runs until it expires\nuv run -m linkedin_mcp_server --no-headless --no-lazy-init\n```\n\n### Local Setup Help\n<details>\n<summary><b>🔧 Configuration</b></summary>\n\n**CLI Options:**\n- `--no-headless` - Show browser window (debugging)\n- `--log-level {DEBUG,INFO,WARNING,ERROR}` - Set logging level (default: WARNING)\n- `--no-lazy-init` - Login to LinkedIn immediately instead of waiting for the first tool call\n- `--get-cookie` - Login with email and password and extract the LinkedIn cookie\n- `--clear-keychain` - Clear all stored LinkedIn credentials and cookies from system keychain\n- `--cookie {cookie}` - Pass a specific LinkedIn cookie for login\n- `--user-agent {user_agent}` - Specify custom user agent string to prevent anti-scraping detection\n- `--transport {stdio,streamable-http}` - Set transport mode\n- `--host HOST` - HTTP server host (default: 127.0.0.1)\n- `--port PORT` - HTTP server port (default: 8000)\n- `--path PATH` - HTTP server path (default: /mcp)\n- `--help` - Show help\n\n**HTTP Mode Example (for web-based MCP clients):**\n```bash\nuv run -m linkedin_mcp_server --transport streamable-http --host 127.0.0.1 --port 8000 --path /mcp\n```\n\n**Claude Desktop:**\n```**json**\n{\n  \"mcpServers\": {\n    \"linkedin\": {\n      \"command\": \"uv\",\n      \"args\": [\"--directory\", \"/path/to/linkedin-mcp-server\", \"run\", \"-m\", \"linkedin_mcp_server\"]\n    }\n  }\n}\n```\n\n</details>\n\n<details>\n<summary><b>❗ Troubleshooting</b></summary>\n\n**Login/Scraping issues:**\n- Use `--no-headless` to see browser actions (captcha challenge, LinkedIn mobile app 2fa, ...)\n- Add `--no-lazy-init` to attempt to login to LinkedIn immediately instead of waiting for the first tool call\n- Add `--log-level DEBUG` to see more detailed logging\n- Make sure you have only one active LinkedIn session per cookie at a time. Trying to open multiple sessions with the same cookie will result in a cookie invalid error. E.g. if you have a logged in browser session with a docker container, you can't use the same cookie to login with the local setup while the docker container is running / session is not closed.\n\n**ChromeDriver issues:**\n- Ensure Chrome and ChromeDriver versions match\n- Check ChromeDriver is in PATH or set `CHROMEDRIVER_PATH` in your env\n\n**Python issues:**\n- Check Python version: `uv python --version` (should be 3.12+)\n- Reinstall dependencies: `uv sync --reinstall`\n\n</details>\n\nFeel free to open an [issue](https://github.com/stickerdaniel/linkedin-mcp-server/issues) or [PR](https://github.com/stickerdaniel/linkedin-mcp-server/pulls)!\n\n\n<br/>\n<br/>\n\n\n## Acknowledgements\nBuilt with [LinkedIn Scraper](https://github.com/joeyism/linkedin_scraper) by [@joeyism](https://github.com/joeyism) and [FastMCP](https://gofastmcp.com/).\n\n⚠️ Use in accordance with [LinkedIn's Terms of Service](https://www.linkedin.com/legal/user-agreement). Web scraping may violate LinkedIn's terms. This tool is for personal use only.\n\n## Star History\n\n<picture>\n  <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://api.star-history.com/svg?repos=stickerdaniel/linkedin-mcp-server&type=Date&theme=dark\" />\n  <source media=\"(prefers-color-scheme: light)\" srcset=\"https://api.star-history.com/svg?repos=stickerdaniel/linkedin-mcp-server&type=Date\" />\n  <img alt=\"Star History Chart\" src=\"https://api.star-history.com/svg?repos=stickerdaniel/linkedin-mcp-server&type=Date\" />\n</picture>\n\n\n## License\n\nThis project is licensed under the Apache 2.0 license.\n\n<br>\n",
      "npm_url": "https://www.npmjs.com/package/linkedin-mcp-server",
      "npm_downloads": 543,
      "keywords": [
        "linkedin",
        "web",
        "listings",
        "linkedin mcp",
        "based linkedin",
        "linkedin scrape"
      ],
      "category": "web-search"
    },
    "sunsetcoder--flightradar24-mcp-server": {
      "owner": "sunsetcoder",
      "name": "flightradar24-mcp-server",
      "url": "https://github.com/sunsetcoder/flightradar24-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/sunsetcoder.webp",
      "description": "Track flights in real-time using Flightradar24 data, providing information on specific flight times and airport statuses, and monitoring emergency flights.",
      "stars": 45,
      "forks": 7,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-09-30T05:20:17Z",
      "readme_content": "# Flightradar24 MCP Server 🛩️\n\nA Claude Desktop MCP server that helps you track flights in real-time using Flightradar24 data. Perfect for aviation enthusiasts, travel planners, or anyone curious about flights overhead!\n\n## What Can This Do? ✨\n\n- 🔍 Track any flight in real-time\n- ⏰ Get arrival and departure times for specific flights\n- 🌉 View the status of flights at an airport\n- 🚨 Monitor emergency flights\n\n<img width=\"1466\" alt=\"Anthropic Claude MCP Hackathon - FlightRadar24 MCP server\" src=\"https://github.com/user-attachments/assets/719444ae-2c8b-4441-84f8-5150337d871f\" />\n\n## Setup Guide 🚀\n\n### 1. Prerequisites\n- [Claude Desktop](https://claude.ai/desktop) installed on your computer\n- A Flightradar24 API key (get one from [Flightradar24's website](https://www.flightradar24.com/premium))*\n\n### 2. Installation\n\n1. Clone this repository somewhere on your computer:\n   ```bash\n   git clone https://github.com/sunsetcoder/flightradar24-mcp-server.git\n   ```\n\n2. Install dependencies & build the project:\n   ```bash\n   cd flightradar24-mcp-server\n   npm install\n   npm run build\n   ```\n\n### 3. Integration with Claude Desktop\n\n1. Open your Claude Desktop configuration file:\n   ```\n   # On Mac:\n   ~/Library/Application Support/Claude/claude_desktop_config.json\n   \n   # On Windows:\n   %APPDATA%/Claude/claude_desktop_config.json\n   ```\n\n2. Add the following to the `mcpServers` object in your config:\n   ```json\n   {\n     \"mcpServers\": {\n       \"flightradar24-server\": {\n         \"command\": \"node\",\n         \"args\": [\n           \"/Users/<username>/<FULL_PATH...>/flightradar24-mcp-server/dist/index.js\"\n         ],\n         \"env\": {\n           \"FR24_API_KEY\": \"your_api_key_here\",\n           \"FR24_API_URL\": \"https://fr24api.flightradar24.com\"\n         }\n       }\n     }\n   }\n   ```\n\n3. Important Steps:\n   - Replace `/FULL/PATH/TO/flightradar24-mcp-server` with the actual full path to where you cloned the repository\n   - Add your Flightradar24 API key in the `env` section\n   - Make sure to use forward slashes (`/`) in the path, even on Windows\n\n4. Restart Claude Desktop for the changes to take effect\n\n## Environment Setup\n\n1. Copy `.env.example` to `.env`:\n   ```bash\n   cp .env.example .env\n   ```\n\n2. Update the `.env` file with your actual Flightradar24 API key:\n   ```env\n   FR24_API_KEY=your_actual_api_key_here\n   ```\n\nNote: Never commit your actual API key to version control. The `.env` file is ignored by git for security reasons.\n\n## Let's Try It Out! 🎮\n\nOnce the server is configured, you can ask Claude questions like:\n\n1. \"What's the ETA for United Airlines flight UA123?\"\n2. \"Show me all flights currently at SFO\"\n3. \"Are there any emergency flights in the area?\"\n4. \"Show me all international flights arriving at SFO in the next 2 hours\"\n5. \"How many commercial flights are currently over the Pacific Ocean?\"\n6. \"Identify any flights that have declared an emergency in the California region\"\n\nExample conversation with Claude:\n```\nYou: What's the status of flight UA123?\nClaude: Let me check that for you...\n[Claude will use the MCP server to fetch real-time flight information]\n```\n\n## Common Questions & Troubleshooting 🤔\n\n### \"Claude can't connect to the server\"\n- Check if the path in `claude_desktop_config.json` is correct\n- Make sure you're using the full absolute path\n- Verify your API key is correct\n- Try restarting Claude Desktop\n\n### \"The server isn't responding\"\n- Make sure your Flightradar24 API key is valid\n- Check if the API URL is correct\n- Look for any error messages in server logs\n\n### FlightRadar API Access\n- Note: Using Flightradar24's API requires a [subscription](https://fr24api.flightradar24.com/subscriptions-and-credits)\n\n## Need More Help? 🆘\n\n1. Make sure Claude Desktop is properly installed\n2. Verify your Flightradar24 API key is active\n3. Check the path in your configuration file is correct\n4. Look for error messages in MCP server logs\n\n## License 📄\n\nMIT\n\n---\n\nMade with ❤️ for aviation enthusiasts\n",
      "npm_url": "https://www.npmjs.com/package/flightradar24-mcp-server",
      "npm_downloads": 967,
      "keywords": [
        "flightradar24",
        "flights",
        "sunsetcoder",
        "flightradar24 mcp",
        "sunsetcoder flightradar24",
        "flightradar24 data"
      ],
      "category": "web-search"
    },
    "surya-madhav--MCP": {
      "owner": "surya-madhav",
      "name": "MCP",
      "url": "https://github.com/surya-madhav/MCP",
      "imageUrl": "/freedevtools/mcp/pfp/surya-madhav.webp",
      "description": "Interact with web content through standardized tools to fetch and summarize information from any website.",
      "stars": 1,
      "forks": 1,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-08-19T09:09:35Z",
      "readme_content": "# MCP Web Tools Server\n\nA Model Context Protocol (MCP) server that provides tools for web-related operations. This server allows LLMs to interact with web content through standardized tools.\n\n## Current Tools\n\n- **web_scrape**: Converts a URL to use r.jina.ai as a prefix and returns the markdown content\n\n## Installation\n\n1. Clone this repository:\n   ```bash\n   git clone <repository-url>\n   cd MCP\n   ```\n\n2. Install the required dependencies:\n   ```bash\n   pip install -r requirements.txt\n   ```\n\n   Alternatively, you can use [uv](https://github.com/astral-sh/uv) for faster installation:\n   ```bash\n   uv pip install -r requirements.txt\n   ```\n\n## Running the Server and UI\n\nThis repository includes convenient scripts to run either the MCP server or the Streamlit UI.\n\n### Using the Run Scripts\n\nOn macOS/Linux:\n```bash\n# Run the server with stdio transport (default)\n./run.sh server\n\n# Run the server with SSE transport\n./run.sh server --transport sse --host localhost --port 5000\n\n# Run the Streamlit UI\n./run.sh ui\n```\n\nOn Windows:\n```cmd\n# Run the server with stdio transport (default)\nrun.bat server\n\n# Run the server with SSE transport\nrun.bat server --transport sse --host localhost --port 5000\n\n# Run the Streamlit UI\nrun.bat ui\n```\n\n### Running Manually\n\nAlternatively, you can run the server directly:\n\n#### Using stdio (default)\n\n```bash\npython server.py\n```\n\n#### Using SSE\n\n```bash\npython server.py --transport sse --host localhost --port 5000\n```\n\nThis will start an HTTP server on `localhost:5000` that accepts MCP connections.\n\nAnd to run the Streamlit UI manually:\n\n```bash\nstreamlit run streamlit_app.py\n```\n\n## Testing with MCP Inspector\n\nThe MCP Inspector is a tool for testing and debugging MCP servers. You can use it to interact with your server:\n\n1. Install the MCP Inspector:\n   ```bash\n   npm install -g @modelcontextprotocol/inspector\n   ```\n\n2. Run the Inspector with your server:\n   ```bash\n   npx @modelcontextprotocol/inspector python server.py\n   ```\n\n3. Use the Inspector interface to test the `web_scrape` tool by providing a URL like `example.com` and viewing the returned markdown content.\n\n## Integrating with Claude for Desktop\n\nTo use this server with Claude for Desktop:\n\n1. Make sure you have [Claude for Desktop](https://claude.ai/download) installed.\n\n2. Open the Claude for Desktop configuration file:\n   - Mac: `~/Library/Application Support/Claude/claude_desktop_config.json`\n   - Windows: `%APPDATA%\\Claude\\claude_desktop_config.json`\n\n3. Add the following configuration (adjust the path as needed):\n\n```json\n{\n  \"mcpServers\": {\n    \"web-tools\": {\n      \"command\": \"python\",\n      \"args\": [\n        \"/absolute/path/to/MCP/server.py\"\n      ]\n    }\n  }\n}\n```\n\n4. Restart Claude for Desktop.\n\n5. You should now see the web_scrape tool available in Claude's interface. You can ask Claude to fetch content from a website, and it will use the tool.\n\n## Example Usage\n\nOnce integrated with Claude, you can ask questions like:\n\n- \"What's on the homepage of example.com?\"\n- \"Can you fetch and summarize the content from mozilla.org?\"\n- \"Get the content from wikipedia.org/wiki/Model_Context_Protocol and explain it to me.\"\n\nClaude will use the web_scrape tool to fetch the content and provide it in its response.\n\n## Adding More Tools\n\nTo add more tools to this server:\n\n1. Create a new Python file in the `tools/` directory, e.g., `tools/new_tool.py`.\n\n2. Implement your tool function, following a similar pattern to the existing tools.\n\n3. Import your tool in `server.py` and register it with the MCP server:\n\n```python\n# Import your new tool\nfrom tools.new_tool import new_tool_function\n\n# Register the tool with the MCP server\n@mcp.tool()\nasync def new_tool(param1: str, param2: int) -> str:\n    \"\"\"\n    Description of what your tool does.\n    \n    Args:\n        param1: Description of param1\n        param2: Description of param2\n        \n    Returns:\n        Description of return value\n    \"\"\"\n    return await new_tool_function(param1, param2)\n```\n\n4. Restart the server to apply the changes.\n\n## Streamlit UI\n\nThis repository includes a Streamlit application that allows you to connect to and test all your MCP servers configured in Claude for Desktop.\n\n### Running the Streamlit UI\n\n```bash\nstreamlit run streamlit_app.py\n```\n\nThis will start the Streamlit server and open a web browser with the UI.\n\n### Features\n\n- Load and parse your Claude for Desktop configuration file\n- View all configured MCP servers\n- Connect to any server and view its available tools\n- Test tools by providing input parameters and viewing results\n- See available resources and prompts\n\n### Usage\n\n1. Start the Streamlit app\n2. Enter the path to your Claude for Desktop configuration file (default path is pre-filled)\n3. Click \"Load Servers\" to see all available MCP servers\n4. Select a server tab and click \"Connect\" to load its tools\n5. Select a tool and provide the required parameters\n6. Click \"Execute\" to run the tool and see the results\n\n## Troubleshooting\n\n- **Missing dependencies**: Make sure all dependencies in `requirements.txt` are installed.\n- **Connection issues**: Check that the server is running and the configuration in Claude for Desktop points to the correct path.\n- **Tool execution errors**: Look for error messages in the server output.\n- **Streamlit UI issues**: Make sure Streamlit is properly installed and the configuration file path is correct.\n\n## License\n\nThis project is available under the MIT License. See the LICENSE file for more details.\n",
      "npm_url": "https://www.npmjs.com/package/mcp",
      "npm_downloads": 17624,
      "keywords": [
        "web",
        "surya",
        "search",
        "web search",
        "web content",
        "search surya"
      ],
      "category": "web-search"
    },
    "suthio--brave-deep-research-mcp": {
      "owner": "suthio",
      "name": "brave-deep-research-mcp",
      "url": "https://github.com/suthio/brave-deep-research-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/suthio.webp",
      "description": "Connects to Brave Search for web searches and uses Puppeteer for extracting full content from webpages, enabling in-depth research beyond basic search results. Retrieves detailed insights and processes linked information for comprehensive analysis.",
      "stars": 3,
      "forks": 3,
      "license": "No License",
      "language": "TypeScript",
      "updated_at": "2025-07-05T13:31:15Z",
      "readme_content": "# @suthio/brave-deep-research-mcp\n\nA Model Context Protocol (MCP) server that combines Brave Search with Puppeteer-powered content extraction for deep research capabilities. This server allows AI assistants to perform comprehensive web searches by not only retrieving search results but also visiting the pages to extract full content and explore linked pages.\n\n## Comparison with Standard Brave Search MCP Server\n\n### Standard Brave Search MCP Server:\n- **Search Capability**: Uses the Brave Search API to perform basic web searches\n- **Data Retrieval**: Returns only the search results (title, URL, and snippet) provided by the API\n- **Content Depth**: No access to full webpage content beyond the search snippets\n- **Page Exploration**: No ability to visit pages or follow links\n- **Information Scope**: Limited to the brief information available in search results\n- **Content Processing**: No content extraction or cleaning capabilities\n- **Customization**: Limited to basic search parameters (query, count, offset)\n- **Use Case**: Best for quick searches where only an overview is needed\n\n### Brave Deep Research MCP Server (this project):\n- **Search Capability**: Uses Brave Search API for initial results, then enhances with web scraping\n- **Data Retrieval**: Extracts complete page content from each search result\n- **Content Depth**: Provides full webpage content with main text extraction\n- **Page Exploration**: Can traverse links to explore related content at configurable depths\n- **Information Scope**: Accesses comprehensive information across multiple related pages\n- **Content Processing**: Intelligently identifies and extracts main content, filtering out navigation, ads, footers, etc.\n- **Customization**: Configurable depth of exploration, result count, headless mode, and timeouts\n- **Use Case**: Ideal for in-depth research requiring detailed information and context\n\n### Practical Differences in an Example Query\n\nFor a query like \"climate change mitigation technologies\":\n\n**Standard Brave Search MCP**:\n```\nTitle: \"Latest Climate Change Mitigation Technologies - Example Site\"\nURL: \"https://example.com/climate-tech\"\nSnippet: \"Various technologies are being developed to mitigate climate change, including carbon capture...\"\n```\n(Limited to just these search result snippets)\n\n**Brave Deep Research MCP**:\n```\n# Latest Climate Change Mitigation Technologies - Example Site\nURL: https://example.com/climate-tech\n\n## Content\nCarbon capture and storage (CCS) technology has advanced significantly in recent years. The latest direct air capture facilities can now remove CO2 at a cost of $250 per ton, down from $600 just five years ago. Implementation challenges remain, including...\n\n[Followed by several pages of detailed content from the original page and linked pages]\n```\n\n## Features\n\n- **Deep Search**: Go beyond search results to extract complete page content\n- **Configurable Depth**: Specify how many levels of links to follow from initial results\n- **Content Extraction**: Intelligently identify and extract main content from pages\n- **Metadata Extraction**: Get titles, descriptions, and structured content\n- **Debug Mode**: Configurable logging for troubleshooting\n- **Headless Mode Toggle**: Run browser in visible or headless mode\n\n## Installation\n\n```bash\n# Install from npm\nnpm install -g @suthio/brave-deep-research-mcp\n\n# Or clone the repository\ngit clone https://github.com/suthio/brave-deep-research-mcp.git\ncd brave-deep-research-mcp\nnpm install\nnpm run build\n```\n\n## Configuration\n\nCreate a `.env` file based on the provided `.env.example`:\n\n```bash\n# Copy the example env file\ncp .env.example .env\n\n# Edit the file to add your Brave API key and other settings\nnano .env\n```\n\n### Environment Variables\n\n- `BRAVE_API_KEY`: Your Brave Search API key (required)\n- `PUPPETEER_HEADLESS`: Whether to run Puppeteer in headless mode (default: true)\n- `PAGE_TIMEOUT`: Timeout for page loading in milliseconds (default: 30000)\n- `DEBUG_MODE`: Enable detailed debug logging (default: false)\n\n## Usage\n\n### Running from command line\n\n```bash\n# If installed globally via npm\nbrave-deep-research-mcp\n\n# Or run directly from the package\nnpx @suthio/brave-deep-research-mcp\n\n# Or run locally after cloning\nnpm start\n```\n\n### Using with Claude for Desktop\n\nTo use this server with Claude for Desktop:\n\n1. Install the package:\n```bash\nnpm install -g @suthio/brave-deep-research-mcp\n```\n\n2. Edit the Claude for Desktop configuration file:\n   - On macOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\n   - On Windows: `%APPDATA%\\Claude\\claude_desktop_config.json`\n\n3. Add the following to the `mcpServers` section:\n```json\n{\n  \"mcpServers\": {\n    \"brave-deep-research\": {\n      \"command\": \"npx\",\n      \"args\": [\"@suthio/brave-deep-research-mcp\"],\n      \"env\": {\n        \"BRAVE_API_KEY\": \"your_brave_api_key_here\",\n        \"PUPPETEER_HEADLESS\": \"true\"\n      }\n    }\n  }\n}\n```\n\n4. Restart Claude for Desktop\n5. You can now use the deep-search tool in your conversations\n\n### Example Queries\n\n- \"Use deep-search to research the latest developments in quantum computing\"\n- \"Perform a deep search on climate change mitigation strategies with depth 2\"\n- \"Deep search for information about sustainable architecture, with 5 results\"\n\n## Tool Parameters\n\nThe `deep-search` tool accepts the following parameters:\n\n- `query` (required): The search query\n- `results` (optional): Number of search results to process (default: 3, max: 10)\n- `depth` (optional): Depth of link traversal for each result (default: 1, max: 3)\n\n## Development\n\n```bash\n# Clone the repository\ngit clone https://github.com/suthio/brave-deep-research-mcp.git\ncd brave-deep-research-mcp\n\n# Install dependencies\nnpm install\n\n# Run in development mode\nnpm run dev\n\n# Build the project\nnpm run build\n```\n\n## How It Works\n\n1. The tool first performs a search using the Brave Search API to get initial results\n2. For each search result, it launches a Puppeteer browser to visit the page\n3. It extracts the main content, metadata, and links from each page\n4. If depth > 1, it follows links on the page and repeats the process\n5. All extracted content is formatted and returned to the AI assistant\n\n## License\n\nMIT",
      "npm_url": "https://www.npmjs.com/package/@suthio/brave-deep-research-mcp",
      "npm_downloads": 1662,
      "keywords": [
        "puppeteer",
        "searches",
        "webpages",
        "brave search",
        "uses puppeteer",
        "search web"
      ],
      "category": "web-search"
    },
    "t0mst0ne--pubmed-mcp-easy": {
      "owner": "t0mst0ne",
      "name": "pubmed-mcp-easy",
      "url": "https://github.com/t0mst0ne/pubmed-mcp-easy",
      "imageUrl": "/freedevtools/mcp/pfp/t0mst0ne.webp",
      "description": "Search and retrieve biomedical literature and research papers from PubMed. Supports advanced searches, batch querying, and retrieval of abstracts and full texts of open access articles.",
      "stars": 0,
      "forks": 1,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-06-18T01:29:22Z",
      "readme_content": "# PubMed MCP Server\n\n[![smithery badge](https://smithery.ai/badge/@t0mst0ne/pubmed-mcp-easy)](https://smithery.ai/server/@t0mst0ne/pubmed-mcp-easy)\n\nEnhanced Python-based PubMed MCP Server with API key support for faster and unlimited downloads.\n\n## Certified by MCP Review\n[https://mcpreview.com/mcp-servers/t0mst0ne/pubmed-mcp-easy]\n\n## Features\n\n- Search PubMed for biomedical literature and research papers\n- Find similar articles, cited articles, and citing articles\n- Retrieve abstracts and full text of open access articles\n- Batch search and advanced search capabilities\n- API key support for faster and unlimited downloads\n\n## API Key and Email Setup\n\nThe NCBI E-utilities API recommends using an API key and email address with your requests. This brings several benefits:\n\n- **Higher rate limits**: 10 requests/second instead of 3\n- **More results per query**: Up to 200 results per request instead of 100\n- **Priority service**: Better queue position for your requests\n\n### How to Get an API Key\n\n1. Create an NCBI account at [https://www.ncbi.nlm.nih.gov/account/](https://www.ncbi.nlm.nih.gov/account/)\n2. Go to the API Keys Management page\n3. Generate a new API key\n\n### Setting Up API Key and Email\n\nThe easiest way to set up your API key and email is using the setup script:\n\n```bash\npython setup_api.py\n```\n\nThis interactive script will guide you through the process and offer multiple setup options.\n\nAlternatively, you can set up your API key and email manually using one of these three methods:\n\n#### 1. Environment Variables\n\n```bash\nexport NCBI_API_KEY=your_api_key_here\nexport NCBI_EMAIL=your_email@example.com\n```\n\n#### 2. Command Line Arguments\n\n```bash\npython python-pubmed-mcp-enhanced.py --api-key your_api_key_here --email your_email@example.com\n```\n\n#### 3. Configuration File\n\nCreate a `config.json` file based on the example:\n\n```bash\ncp config.json.example config.json\n```\n\nEdit `config.json` to include your API key and email:\n\n```json\n{\n  \"api_key\": \"your_api_key_here\",\n  \"email\": \"your_email@example.com\"\n}\n```\n\nThen run the server with the config file:\n\n```bash\npython python-pubmed-mcp-enhanced.py --config config.json\n```\n\n## Usage\n\n### Installing via Smithery\n\nTo install pubmed-mcp-easy for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@t0mst0ne/pubmed-mcp-easy):\n\n```bash\nnpx -y @smithery/cli install @t0mst0ne/pubmed-mcp-easy --client claude\n```\n\n### Standard Usage\n\nRun the server:\n\n```bash\npython python-pubmed-mcp-enhanced.py\n```\n\n### Claude Desktop Integration\n\nTo integrate with Claude Desktop, add the following to your `claude_desktop_config.json` file:\n\n```json\n\"pubmed-easy\": {\n    \"command\": \"/opt/anaconda3/bin/python\",\n    \"args\": [\n        \"/GITHUB_cloned_dir/pubmed-mcp-easy/python-pubmed-mcp-enhanced.py\", \"--config\", \"config.json\"\n    ]\n}\n```\n\nMake sure to:\n1. Replace `/opt/anaconda3/bin/python` with the path to your Python executable\n2. Replace `/GITHUB_cloned_dir` with the actual path to your GitHub directory\n3. Create a `config.json` file with your API key and email as described above\n\nAfter adding this configuration, you can access PubMed tools directly from Claude Desktop.\n\n### Available Tools\n\nThe server provides the following MCP tools:\n\n- `pubmed_search`: Search for articles by keyword or query\n- `pubmed_similar`: Find similar articles\n- `pubmed_cites`: Find articles cited by a specific paper\n- `pubmed_cited_by`: Find articles that cite a specific paper\n- `pubmed_abstract`: Retrieve the abstract of an article\n- `pubmed_open_access`: Check if an article is open access\n- `pubmed_full_text`: Retrieve the full text of an open access article\n- `pubmed_batch_search`: Perform multiple searches in parallel\n- `pubmed_author_search`: Search for papers by a specific author\n- `pubmed_advanced_search`: Perform advanced field-based searches\n- `pubmed_journal_search`: Search for papers in a specific journal\n\n## Important Notes\n\n1. Including an email address is recommended by NCBI as it allows them to contact you if there are issues with your requests.\n2. If you make heavy use of the E-utilities, NCBI recommends that you limit large jobs to either weekends or between 9 pm and 5 am Eastern Time weekdays.\n3. Always be considerate in your usage and follow NCBI's usage guidelines.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "pubmed",
        "searches",
        "abstracts",
        "papers pubmed",
        "retrieval abstracts",
        "biomedical literature"
      ],
      "category": "web-search"
    },
    "tahaswx--mcp-server-serper": {
      "owner": "tahaswx",
      "name": "mcp-server-serper",
      "url": "https://github.com/tahaswx/mcp-server-serper",
      "imageUrl": "/freedevtools/mcp/pfp/tahaswx.webp",
      "description": "Provides web search functionality and webpage content extraction using the Serper API, enabling real-time data retrieval for applications.",
      "stars": 0,
      "forks": 0,
      "license": "MIT License",
      "language": "",
      "updated_at": "2025-03-17T21:57:42Z",
      "readme_content": "# Serper Search and Scrape MCP Server\n[![smithery badge](https://smithery.ai/badge/@marcopesani/mcp-server-serper)](https://smithery.ai/server/@marcopesani/mcp-server-serper)\n\nA TypeScript-based MCP server that provides web search and webpage scraping capabilities using the Serper API. This server integrates with Claude Desktop to enable powerful web search and content extraction features.\n\n<a href=\"https://glama.ai/mcp/servers/5zk327i0pj\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/5zk327i0pj/badge\" alt=\"serper-search-scrape-mcp-server MCP server\" />\n</a>\n\n## Features\n\n### Tools\n\n- `google_search` - Perform web searches via Serper API\n  - Rich search results including organic results, knowledge graph, \"people also ask\", and related searches\n  - Supports region and language targeting\n  - Optional parameters for location, pagination, time filters, and autocorrection\n  - Supports advanced search operators:\n    - `site`: Limit results to specific domain\n    - `filetype`: Limit to specific file types (e.g., 'pdf', 'doc')\n    - `inurl`: Search for pages with word in URL\n    - `intitle`: Search for pages with word in title\n    - `related`: Find similar websites\n    - `cache`: View Google's cached version of a specific URL\n    - `before`: Date before in YYYY-MM-DD format\n    - `after`: Date after in YYYY-MM-DD format\n    - `exact`: Exact phrase match\n    - `exclude`: Terms to exclude from search results\n    - `or`: Alternative terms (OR operator)\n  \n- `scrape` - Extract content from web pages\n  - Get plain text and optional markdown content\n  - Includes JSON-LD and head metadata\n  - Preserves document structure\n\n## Requirements\n\n- Node.js >= 18\n- Serper API key (set as `SERPER_API_KEY` environment variable)\n\n## Development\n\nInstall dependencies:\n```bash\nnpm install\n```\n\nBuild the server:\n```bash\nnpm run build\n```\n\nFor development with auto-rebuild:\n```bash\nnpm run watch\n```\n\nRun tests:\n```bash\nnpm test                  # Run all tests\nnpm run test:watch       # Run tests in watch mode\nnpm run test:coverage    # Run tests with coverage\nnpm run test:integration # Run integration tests\n```\n\n### Environment Variables\n\nCreate a `.env` file in the root directory:\n\n```\nSERPER_API_KEY=your_api_key_here\n```\n\n### Debugging\n\nSince MCP servers communicate over stdio, debugging can be challenging. We recommend using the [MCP Inspector](https://github.com/modelcontextprotocol/inspector), which is available as a package script:\n\n```bash\nnpm run inspector\n```\n\nThe Inspector will provide a URL to access debugging tools in your browser.\n\n## Installation\n\n### Installing via Smithery\n\nTo install Serper Search and Scrape for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@marcopesani/mcp-server-serper):\n\n```bash\nnpx -y @smithery/cli install @marcopesani/mcp-server-serper --client claude\n```\n\n### Claude Desktop\n\nAdd the server config at:\n- MacOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\n- Windows: `%APPDATA%/Claude/claude_desktop_config.json`\n\n```json\n{\n  \"mcpServers\": {\n    \"serper-search\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"serper-search-scrape-mcp-server\"],\n      \"env\": {\n        \"SERPER_API_KEY\": \"your_api_key_here\"\n      }\n    }\n  }\n}\n```\n\n### Cline\n\n1. Open the Cline extension settings\n2. Open \"MCP Servers\" tab\n3. Click on \"Configure MCP Servers\"\n4. Add the server config:\n\n```json\n{\n  \"mcpServers\": {\n    \"github.com/marcopesani/mcp-server-serper\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"serper-search-scrape-mcp-server\"],\n      \"env\": {\n        \"SERPER_API_KEY\": \"your_api_key_here\"\n      },\n      \"disabled\": false,\n      \"autoApprove\": [\"google_search\", \"scrape\"]\n    }\n  }\n}\n```\n\nAdditional Cline configuration options:\n- `disabled`: Set to `false` to enable the server\n- `autoApprove`: List of tools that don't require explicit approval for each use\n\n### Cursor\n\n1. Open the Cursor settings\n2. Open \"Features\" settings\n3. In the \"MCP Servers\" section, click on \"Add new MCP Server\"\n4. Choose a name, and select \"command\" as \"Type\"\n5. In the \"Command\" field, enter the following:\n\n```\nenv SERPER_API_KEY=your_api_key_here npx -y serper-search-scrape-mcp-server\n```\n\n### Docker\n\nYou can also run the server using Docker. First, build the image:\n\n```bash\ndocker build -t mcp-server-serper .\n```\n\nThen run the container with your Serper API key:\n\n```bash\ndocker run -e SERPER_API_KEY=your_api_key_here mcp-server-serper\n```\n\nAlternatively, if you have your environment variables in a `.env` file:\n\n```bash\ndocker run --env-file .env mcp-server-serper\n```\n\nFor development, you might want to mount your source code as a volume:\n\n```bash\ndocker run -v $(pwd):/app --env-file .env mcp-server-serper\n```\n\nNote: Make sure to replace `your_api_key_here` with your actual Serper API key.",
      "npm_url": "https://www.npmjs.com/package/mcp-server-serper",
      "npm_downloads": 487,
      "keywords": [
        "serper",
        "retrieval",
        "webpage",
        "using serper",
        "server serper",
        "serper api"
      ],
      "category": "web-search"
    },
    "tanevanwifferen--usescraper-mcp-server": {
      "owner": "tanevanwifferen",
      "name": "usescraper-mcp-server",
      "url": "https://github.com/tanevanwifferen/usescraper-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/tanevanwifferen.webp",
      "description": "Provides web scraping capabilities using the UseScraper API to extract content from web pages in various formats.",
      "stars": 4,
      "forks": 4,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-09-21T18:43:04Z",
      "readme_content": "# UseScraper MCP Server\n\n[![smithery badge](https://smithery.ai/badge/usescraper-server)](https://smithery.ai/server/usescraper-server)\nThis is a TypeScript-based MCP server that provides web scraping capabilities using the UseScraper API. It exposes a single tool 'scrape' that can extract content from web pages in various formats.\n\n<a href=\"https://glama.ai/mcp/servers/oqq8he02cy\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/oqq8he02cy/badge\" alt=\"UseScraper Server MCP server\" /></a>\n\n\n## Features\n\n### Tools\n- `scrape` - Extract content from a webpage\n  - **Parameters**:\n    - `url` (required): The URL of the webpage to scrape\n    - `format` (optional): The format to save the content (text, html, markdown). Default: markdown\n    - `advanced_proxy` (optional): Use advanced proxy to circumvent bot detection. Default: false\n    - `extract_object` (optional): Object specifying data to extract\n\n## Installation\n\n### Installing via Smithery\n\nTo install UseScraper for Claude Desktop automatically via [Smithery](https://smithery.ai/server/usescraper-server):\n\n```bash\nnpx -y @smithery/cli install usescraper-server --client claude\n```\n\n### Manual Installation\n1. Clone the repository:\n   ```bash\n   git clone https://github.com/your-repo/usescraper-server.git\n   cd usescraper-server\n   ```\n\n2. Install dependencies:\n   ```bash\n   npm install\n   ```\n\n3. Build the server:\n   ```bash\n   npm run build\n   ```\n\n## Configuration\n\nTo use with Claude Desktop, add the server config:\n\nOn MacOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\nOn Windows: `%APPDATA%/Claude/claude_desktop_config.json`\n\n```json\n{\n  \"mcpServers\": {\n    \"usescraper-server\": {\n      \"command\": \"node\",\n      \"args\": [\"/path/to/usescraper-server/build/index.js\"],\n      \"env\": {\n        \"USESCRAPER_API_KEY\": \"your-api-key-here\"\n      }\n    }\n  }\n}\n```\n\nReplace `/path/to/usescraper-server` with the actual path to the server and `your-api-key-here` with your UseScraper API key.\n\n## Usage\n\nOnce configured, you can use the 'scrape' tool through the MCP interface. Example usage:\n\n```json\n{\n  \"name\": \"scrape\",\n  \"arguments\": {\n    \"url\": \"https://example.com\",\n    \"format\": \"markdown\"\n  }\n}\n```\n\n## Development\n\nFor development with auto-rebuild:\n```bash\nnpm run watch\n```\n\n### Debugging\n\nSince MCP servers communicate over stdio, debugging can be challenging. We recommend using the [MCP Inspector](https://github.com/modelcontextprotocol/inspector), which is available as a package script:\n\n```bash\nnpm run inspector\n```\n\nThe Inspector will provide a URL to access debugging tools in your browser.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "scraping",
        "usescraper",
        "extract",
        "web scraping",
        "scraping capabilities",
        "extract content"
      ],
      "category": "web-search"
    },
    "tanigami--mcp-server-perplexity": {
      "owner": "tanigami",
      "name": "mcp-server-perplexity",
      "url": "https://github.com/tanigami/mcp-server-perplexity",
      "imageUrl": "/freedevtools/mcp/pfp/tanigami.webp",
      "description": "MCP Server that connects to the Perplexity API, enabling users to request chat completions with citations. It serves as an interface for utilizing Perplexity's capabilities within the MCP framework.",
      "stars": 83,
      "forks": 33,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-07T20:53:41Z",
      "readme_content": "# Perplexity MCP Server\n\n[![smithery badge](https://smithery.ai/badge/mcp-server-perplexity)](https://smithery.ai/server/mcp-server-perplexity)\n\nMCP Server for the Perplexity API.\n\n> :warning: **Limitations:**\n> - The Claude Desktop client may timeout if Perplexity processing takes too long\n> - This issue might be resolved if Claude Desktop implements support for long running operations and progress reporting in the future\n> - Implementation updates to handle these features will be made if they become available\n\n<a href=\"https://glama.ai/mcp/servers/hchfq9bydq\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/hchfq9bydq/badge\" alt=\"Perplexity Server MCP server\" /></a>\n\n## Components\n\n### Tools\n\n- **ask_perplexity**: Request chat completion with citations from Perplexity  \n\n## Quickstart\n\n### Install\n\n#### Claude Desktop\n\n- On macOS: `~/Library/Application\\ Support/Claude/claude_desktop_config.json`  \n- On Windows: `%APPDATA%/Claude/claude_desktop_config.json`\n\n```\n\"mcpServers\": {\n  \"Perplexity\": {\n    \"command\": \"uvx\",\n    \"args\": [\n      \"mcp-server-perplexity\"\n    ],\n    \"env\": {\n      \"PERPLEXITY_API_KEY\": \"your-perplexity-api-key\"\n    }\n  }\n}\n```\n",
      "npm_url": "https://www.npmjs.com/package/mcp-server-perplexity",
      "npm_downloads": 934,
      "keywords": [
        "mcp",
        "perplexity",
        "chat",
        "perplexity api",
        "perplexity mcp",
        "tanigami mcp"
      ],
      "category": "web-search"
    },
    "tankz--younioffer.github.io": {
      "owner": "tankz",
      "name": "younioffer.github.io",
      "url": "https://github.com/tankz/younioffer.github.io",
      "imageUrl": "/freedevtools/mcp/pfp/tankz.webp",
      "description": "Prologue is a Jekyll theme designed for creating responsive, single-page websites with integrated blogging capabilities and a user-friendly setup. It features a minimalistic design, a sticky sidebar for navigation, and compatibility with Formspree for easy contact form integration.",
      "stars": 0,
      "forks": 0,
      "license": "Other",
      "language": "CSS",
      "updated_at": "2018-04-25T10:02:47Z",
      "readme_content": "# Prologue - Jekyll Theme\n\n[![Gem Version](https://badge.fury.io/rb/jekyll-theme-prologue.svg)](https://badge.fury.io/rb/jekyll-theme-prologue)\n\n\n\nThis is Prologue, a simple, single page responsive site template from [HTML5 UP](https://html5up.net/prologue), now available as a blog-aware Jekyll theme from [Chris Bobbe](https://chrisbobbe.github.io). It features a clean, minimalistic design and a sticky sidebar with navigation-linked scrolling.\n\n**Demo**: https://chrisbobbe.github.io/jekyll-theme-prologue/\n\n# Added Features\n\n* **Blogging and multi-page features you expect from Jekyll**\n* Compatible with GitHub Pages\n* **[Formspree.io](https://formspree.io/) contact form integration** - just add your email to the `_config.yml` and it works!\n* Build your homepage with **custom scrolly sections** in the _sections folder\n * Set a **cover photo** for any section (not just the first), with alt text for screen readers and SEO\n* Add your **social profiles** easily in `_config.yml`.\n* Automatic search engine optimization (SEO) **meta tags** based on info you provide in `_config.yml` and frontmatter\n* **Google Analytics** built-in; just put your [Tracking ID](https://support.google.com/analytics/answer/1008080?hl=en) in `_config.yml` as `google_analytics`\n* Custom **404 page** (called 404.html; to activate, move it to your project directory).\n\n# Installation\n\nThere are two ways to get started (choose one):\n\n1. **Install the [jekyll-theme-prologue gem](https://rubygems.org/gems/jekyll-theme-prologue).** Instructions are in the [Jekyll docs](https://jekyllrb.com/docs/themes/#installing-a-theme). After running `bundle install`, you can find the theme files by running `open $(bundle show jekyll-theme-prologue)`.  A sample working `_config.yml` file ships with the gem; if you want to activate it, move it to your project's root directory. It will do nothing until you move it there, replacing the default `_config.yml` file.\n2. **Fork or clone the [GitHub repository](https://github.com/chrisbobbe/jekyll-theme-prologue).** If you want to use [GitHub Pages](https://pages.github.com/), create a branch named `gh-pages`, and replace `theme: jekyll-theme-prologue` with `remote_theme: chrisbobbe/jekyll-theme-prologue` in the provided `_config.yml` ([GitHub Pages now supports open-source themes on GitHub](https://github.com/blog/2464-use-any-theme-with-github-pages)).\n\nNext, make sure that `url` and `base_url` are set for your own website in `_config.yml`. For local testing, make them both blank. Add a photo avatar to your project, then set `avatar: path/to/your/avatar.jpg` in _config.yml; for example, `avatar: assets/images/avatar.jpg` (48x48 pixels works best). Poke around the sample `_config.yml` file to see how you can add your social profiles.\n\n# Build your homepage\n\n1. **Your `_config.yml` file must include the following line or your homepage won't work**: `collections: [sections]`. This tells Jekyll to look in the _sections folder (which you will create) for your content and render it all on one page.\n\n2. **Create a `_sections` folder** in your project's root directory and start adding content to your homepage. Set a cover photo in any of the sections by adding `cover-photo: path/to/photo.jpg` and `cover-photo-alt: your alt text here` to the section's frontmatter. Sample content is provided in the [GitHub repository](https://github.com/chrisbobbe/jekyll-theme-prologue/tree/master/_sections).\n\nAll new sections should be added as html or Markdown documents in the `_sections` folder. The following section variables can be set with [frontmatter](https://jekyllrb.com/docs/frontmatter/):\n- `title` (required)\n- `order` (required; orders the sequence of sections on the page. Example: `1`)\n- `cover-photo` (optional; sets a background image for the section. Example: `assets/images/banner.jpg`)\n- `cover-photo-alt` (required if using a cover photo. Describes the photo for screen readers and SEO; e.g. \"Dome of Light art installation, Kaohsiung, Taiwan\")\n- `icon` (optional; see [Font Awesome](http://fontawesome.io/icons/) for icon codes. Example: `fa-github`)\n- `auto-header` (optional; \"use-title\" is default, \"none\" for no header, or custom header text)\n- `hide` (optional; if `true`, the section won't appear)\n\n# Start blogging!\n\nJekyll has great resources to get you started writing blog posts. Check out [this Jekyll Docs page](https://jekyllrb.com/docs/posts/) first. When you've written a post or two, copy the following into a new file in your project directory called `blog.html`, and you'll see a link to your blog from the homepage:\n\n```\n---\nlayout: blog\ntitle: My Blog\n---\n```\n\n-- and that's it!\n\n# Add a page\n\nTo add a page, just make a new .html or .md file in your project directory. There's an example called `reading-list` [provided](https://github.com/chrisbobbe/jekyll-theme-prologue/blob/master/reading-list.md) with the GitHub repository. Add this frontmatter:\n\n```\n---\ntitle: My New Page\nlayout: page\n---\n```\n\nYou can also set these page variables in the frontmatter, if you want:\n- `subtitle`\n- `order` (orders links in the nav menu, e.g. `1`)\n- `icon` (optional; see [Font Awesome](http://fontawesome.io/icons/) for icon codes. Example: `fa-github`)\n- `hide` (optional; if `true`, a link won't appear in the nav menu. All this does is remove the nav link; your page will still be served to anyone who has the URL.)\n\n**This same set of frontmatter variables (including `title`) can also be set in `index.md` and `blog.html`.** You may want to give them titles, or hide the homepage link with `hide: true` if the homepage is the only page.\n\nFor advanced SEO, this theme also lets you add `permalink` (see [Jekyll Docs](https://jekyllrb.com/docs/permalinks/#where-to-configure-permalinks)), `robots` (string, e.g. \"noindex, nofollow\"), and `canonical` (boolean; true is default) to any page or post.\n\n# Contributing\n\nPlease feel free to submit issues and feature requests!\n\n# Credits\n\nThanks to @andrewbanchich for his many Jekyll adaptations of HTML5 UP's elegant themes, which helped and inspired me, and of course many thanks to HTML5 UP.\n\nOriginal README from HTML5 UP:\n\n```\nPrologue by HTML5 UP\nhtml5up.net | @ajlkn\nFree for personal and commercial use under the CCA 3.0 license (html5up.net/license)\n\n\nThis is Prologue, a simple, single page responsive site template. It features a\nclean, minimalistic design and a sticky sidebar with navigation-linked scrolling.\n\nDemo content images* are courtesy of the ridiculously talented Felicia Simion. Check out\nmore of her amazing work over at deviantART:\n\nhttp://ineedchemicalx.deviantart.com/\n\n(* = Not included! Only meant for use with my own on-site demo, so please do NOT download\nand/or use any of Felicia's work without her explicit permission!)\n\nDemo banner images* courtesy of Unsplash, a radtastic collection of CC0 (public domain)\nimages you can use for pretty much whatever.\n\n(* = Not included)\n\nAJ\naj@lkn.io | @ajlkn\n\nPS: Not sure how to get that contact form working? Give formspree.io a try (it's awesome).\n\n\nCredits:\n\n\tDemo Images:\n\t\tFelicia Simion (ineedchemicalx.deviantart.com)\n\t\tUnsplash (unsplash.com)\n\n\tIcons:\n\t\tFont Awesome (fortawesome.github.com/Font-Awesome)\n\n\tOther\n\t\tjQuery (jquery.com)\n\t\thtml5shiv.js (@afarkas @jdalton @jon_neal @rem)\n\t\tCSS3 Pie (css3pie.com)\n\t\tbackground-size polyfill (github.com/louisremi)\n\t\tRespond.js (j.mp/respondjs)\n\t\tjquery.scrolly (@ajlkn)\n\t\tjquery.scrollzer (@ajlkn)\n\t\tSkel (skel.io)\n```",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "jekyll",
        "prologue",
        "theme",
        "prologue jekyll",
        "jekyll theme",
        "younioffer github"
      ],
      "category": "web-search"
    },
    "tatn--mcp-server-fetch-python": {
      "owner": "tatn",
      "name": "mcp-server-fetch-python",
      "url": "https://github.com/tatn/mcp-server-fetch-python",
      "imageUrl": "/freedevtools/mcp/pfp/tatn.webp",
      "description": "Fetch and transform web content into various formats, supporting both raw text extraction from various URLs and JavaScript-rendered content. The server facilitates extracting content from web pages, including media files, in a structured manner.",
      "stars": 7,
      "forks": 1,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-08-19T02:28:53Z",
      "readme_content": "# mcp-server-fetch-python\n\nAn MCP server for fetching and transforming web content into various formats. This server provides comprehensive tools for extracting content from web pages, including support for JavaScript-rendered content and media files.\n\n<a href=\"https://glama.ai/mcp/servers/8d0zm2o56d\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/8d0zm2o56d/badge\" alt=\"Server Fetch Python MCP server\" /></a>\n\n## Features\n\n### Tools\n\nThe server provides four specialized tools:\n\n- **get-raw-text**: Extracts raw text content directly from URLs without browser rendering\n  - Arguments:\n    - `url`: URL of the target web page (text, JSON, XML, csv, tsv, etc.) (required)\n  - Best used for structured data formats or when fast, direct access is needed\n\n- **get-rendered-html**: Fetches fully rendered HTML content using a headless browser\n  - Arguments:\n    - `url`: URL of the target web page (required)\n  - Essential for modern web applications and SPAs that require JavaScript rendering\n\n- **get-markdown**: Converts web page content to well-formatted Markdown\n  - Arguments:\n    - `url`: URL of the target web page (required)\n  - Preserves structural elements while providing clean, readable text output\n\n- **get-markdown-from-media**: Performs AI-powered content extraction from media files\n  - Arguments:\n    - `url`: URL of the target media file (images, videos) (required)\n  - Utilizes computer vision and OCR for visual content analysis\n  - Requires a valid OPENAI_API_KEY to be set in environment variables\n  - Will return an error message if the API key is not set or if there are issues processing the media file\n\n## Usage\n\n### Claude Desktop\n\nTo use with Claude Desktop, add the server configuration:\n\nOn MacOS:  `~/Library/Application\\ Support/Claude/claude_desktop_config.json`  \nOn Windows: `%APPDATA%/Claude/claude_desktop_config.json`\n\n```json\n\"mcpServers\": {\n  \"mcp-server-fetch-python\": {\n    \"command\": \"uvx\",\n    \"args\": [\n      \"mcp-server-fetch-python\"\n    ]\n  }\n}\n```\n\n\n## Environment Variables\n\nThe following environment variables can be configured:\n\n- **OPENAI_API_KEY**: Required for using the `get-markdown-from-media` tool. This key is needed for AI-powered image analysis and content extraction.\n- **PYTHONIOENCODING**: Set to \"utf-8\" if you encounter character encoding issues in the output.\n- **MODEL_NAME**: Specifies the model name to use. Defaults to \"gpt-4o\".\n\n```json\n\"mcpServers\": {\n  \"mcp-server-fetch-python\": {\n    \"command\": \"uvx\",\n    \"args\": [\n      \"mcp-server-fetch-python\"\n    ],\n    \"env\": {\n        \"OPENAI_API_KEY\": \"sk-****\",\n        \"PYTHONIOENCODING\": \"utf-8\",\n        \"MODEL_NAME\": \"gpt-4o\",        \n    }\n  }\n}\n```\n\n\n### Local Installation\n\nAlternatively, you can install and run the server locally:\n\n```powershell\ngit clone https://github.com/tatn/mcp-server-fetch-python.git\ncd mcp-server-fetch-python\nuv sync\nuv build\n```\n\nThen add the following configuration to Claude Desktop config file:\n\n```json\n\"mcpServers\": {\n  \"mcp-server-fetch-python\": {\n    \"command\": \"uv\",\n    \"args\": [\n      \"--directory\",\n      \"path\\\\to\\\\mcp-server-fetch-python\",  # Replace with actual path to the cloned repository\n      \"run\",\n      \"mcp-server-fetch-python\"\n    ]\n  }\n}\n```\n\n## Development\n\n### Debugging\n\nYou can start the MCP Inspector using [npx](https://docs.npmjs.com/cli/v11/commands/npx)with the following commands:\n\n\n```bash\nnpx @modelcontextprotocol/inspector uvx mcp-server-fetch-python\n```\n\n```bash\nnpx @modelcontextprotocol/inspector uv --directory path\\\\to\\\\mcp-server-fetch-python run mcp-server-fetch-python\n```\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "fetch",
        "web",
        "extracting",
        "python fetch",
        "web content",
        "fetch python"
      ],
      "category": "web-search"
    },
    "tatn--mcp-server-fetch-typescript": {
      "owner": "tatn",
      "name": "mcp-server-fetch-typescript",
      "url": "https://github.com/tatn/mcp-server-fetch-typescript",
      "imageUrl": "/freedevtools/mcp/pfp/tatn.webp",
      "description": "Retrieves and converts web content using various formats and rendering methods, suitable for both data extraction and web scraping tasks. It allows access to text-based resources and provides raw text content from specified URLs without additional processing.",
      "stars": 3,
      "forks": 6,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-09-19T14:03:40Z",
      "readme_content": "# mcp-server-fetch-typescript MCP Server\n\nA Model Context Protocol server that provides web content fetching and conversion capabilities. This server implements a comprehensive web content retrieval system with support for various formats and rendering methods, making it ideal for tasks ranging from simple data extraction to sophisticated web scraping.\n\n<a href=\"https://glama.ai/mcp/servers/iyfpvfkgyx\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/iyfpvfkgyx/badge\" alt=\"Server Fetch TypeScript MCP server\" /></a>\n\n## Features\n\n### Tools\n\n- `get_raw_text` - Retrieve raw text content directly from URLs\n  - Takes `url` as a required parameter pointing to text-based resources\n  - Returns unprocessed text content without browser rendering\n  - Ideal for JSON, XML, CSV, TSV, or plain text files\n  - Best used when fast, direct access to source content is needed\n\n- `get_rendered_html` - Fetch fully rendered HTML content\n  - Takes `url` as a required parameter\n  - Returns complete HTML content after JavaScript execution\n  - Uses Playwright for headless browser rendering\n  - Essential for modern web applications and SPAs\n\n- `get_markdown` - Convert web content to Markdown format\n  - Takes `url` as a required parameter\n  - Returns well-formatted Markdown preserving structural elements\n  - Supports tables and definition lists\n  - Recommended for content archiving and documentation\n\n- `get_markdown_summary` - Extract and convert main content\n  - Takes `url` as a required parameter\n  - Returns clean Markdown focusing on main content\n  - Automatically removes navigation, headers, footers\n  - Perfect for article and blog post extraction\n\n## Installation\n\n### As a Global Package\n\n```bash\nnpm install -g mcp-server-fetch-typescript\n```\n\n### As a Project Dependency\n\n```bash\nnpm install mcp-server-fetch-typescript\n```\n\n## Usage\n\n### Using with Claude Desktop\n\nTo use with Claude Desktop, add the server config:\n\nOn MacOS: `~/Library/Application Support/Claude/claude_desktop_config.json`  \nOn Windows: `%APPDATA%/Claude/claude_desktop_config.json`\n\n```json\n\"mcpServers\": {\n  \"mcp-server-fetch-typescript\": {\n    \"command\": \"npx\",\n    \"args\": [\n      \"-y\",\n      \"mcp-server-fetch-typescript\"\n    ]\n  }\n}\n```\n\nor Add the following configuration:\n\n```bash\ngit clone https://github.com/tatn/mcp-server-fetch-typescript.git\ncd mcp-server-fetch-typescript\nnpm install\nnpm run build\n```\n\n```json\n\"mcpServers\": {\n  \"mcp-server-fetch-typescript\": {\n    \"command\": \"node\",\n    \"args\": [\n      \"/path/to/mcp-server-fetch-typescript/build/index.js\"\n    ]\n  }\n}\n```\n\n### Debugging\n\nTo debug the MCP server:\n\n```bash\nnpx @modelcontextprotocol/inspector npx -y mcp-server-fetch-typescript\n```\n\n```bash\nnpx @modelcontextprotocol/inspector node /path/to/mcp-server-fetch-typescript/build/index.js\n```\n\n\n",
      "npm_url": "https://www.npmjs.com/package/mcp-server-fetch-typescript",
      "npm_downloads": 4154,
      "keywords": [
        "typescript",
        "fetch",
        "scraping",
        "extraction web",
        "fetch typescript",
        "typescript retrieves"
      ],
      "category": "web-search"
    },
    "tcpipuk--mcp-server": {
      "owner": "tcpipuk",
      "name": "mcp-server",
      "url": "https://github.com/tcpipuk/mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/tcpipuk.webp",
      "description": "Empowers AI assistants to safely access external tools and websites, facilitating better problem-solving capabilities. It provides clear feedback and helpful error messages during operations.",
      "stars": 3,
      "forks": 0,
      "license": "GNU General Public License v3.0",
      "language": "Python",
      "updated_at": "2025-08-18T22:24:29Z",
      "readme_content": "# MCP Server\n\nGive your AI assistants the power to help you more effectively. This server lets them safely access\nwebsites and search the web - with clear feedback about what's happening and helpful error messages\nwhen things go wrong.\n\n- [🛠️ What tools does this server offer?](#️-what-tools-does-this-server-offer)\n- [🏎️ How can I run it?](#️-how-can-i-run-it)\n  - [🐋 Using Docker (recommended)](#-using-docker-recommended)\n  - [💻 Running locally](#-running-locally)\n- [🔌 How to connect](#-how-to-connect)\n- [📚 Learn more about MCP](#-learn-more-about-mcp)\n- [📄 License](#-license)\n\n## 🛠️ What tools does this server offer?\n\nThe server provides two powerful tools that help AI assistants solve real-world problems:\n\n| Tool               | What it can do                                                              |\n| ------------------ | --------------------------------------------------------------------------- |\n| [Search](docs/search.md) | Search the web via SearXNG for current information, specific resources, or to perform calculations. |\n| [Web](docs/web.md) | Access websites and process their content. Can convert pages to markdown for easy reading, get the raw content, or extract links. |\n\n## 🏎️ How can I run it?\n\n### 🐋 Using Docker (recommended)\n\nThe server runs in Docker containers to keep things safe and simple. Here's how to get started:\n\n1. [Install Docker](https://docs.docker.com/engine/install/) if you haven't already\n2. Create a file called `docker-compose.yml` with:\n\n   ```yaml:docker-compose.yml\n   services:\n     mcp-server:\n       environment:\n         # Required: URL for your SearXNG instance's Search API\n         - SEARXNG_QUERY_URL=http://searxng:8080\n         # Optional: Configure network mode (SSE) for LibreChat etc.\n         - SSE_HOST=0.0.0.0\n         - SSE_PORT=8080\n         # Optional: Set a custom User-Agent for web requests\n         - USER_AGENT=MCP-Server/1.0 (github.com/tcpipuk/mcp-server)\n       image: ghcr.io/tcpipuk/mcp-server/server:latest\n       ports: # Only needed if using SSE_HOST/SSE_PORT\n         - \"8080:8080\" # Expose port 8080 on host\n       restart: unless-stopped\n       stop_grace_period: 1s\n\n     # Example SearXNG service (optional, adapt as needed)\n     # searxng:\n     #   environment:\n     #     - SEARXNG_BASE_URL=http://searxng:8080 # Ensure SearXNG knows its own URL\n     #   image: searxng/searxng:latest\n     #   restart: unless-stopped\n     #   volumes:\n     #     - ./searxng:/etc/searxng:rw\n   ```\n\n   > **Important**: You *must* provide the `SEARXNG_QUERY_URL` environment variable, pointing to\n   > the Search API endpoint of your SearXNG instance (usually ending in `/` or `/search`).\n   >\n   > Setting `SSE_HOST` and `SSE_PORT` enables network mode (Server-Sent Events), recommended for\n   > multi-container setups like LibreChat. If omitted, the server uses standard I/O.\n\n3. Run `docker compose up -d` to start the server container (and optionally SearXNG).\n\nMost people use this with either:\n\n- [Claude Desktop](https://modelcontextprotocol.io/quickstart/user) - connects directly via stdio\n  (omit `SSE_HOST`/`SSE_PORT` in `docker-compose.yml`).\n- [LibreChat](https://www.librechat.ai/docs/local) - connects over the network via SSE.\n\nFor LibreChat, add this to your `librechat.yaml` (assuming `SSE_PORT=8080`):\n\n```yaml:librechat.yaml\nmcpServers:\n  mcp-server:\n    iconPath: \"/path/to/icon.png\" # Optional: Custom icon\n    label: \"MCP Web/Search\" # Optional: Custom label shown in UI\n    type: sse\n    url: http://mcp-server:8080/sse # Adjust host/port if needed\n```\n\n### 💻 Running locally\n\n1. Install `uv` (requires Python 3.13+):\n\n   ```bash\n   curl -LsSf https://astral.sh/uv/install.sh | sh\n   ```\n\n   > **Note:** If you already have `uv` installed, update it with `uv self update`.\n\n2. Create and activate a virtual environment:\n\n   ```bash\n   uv venv\n   source .venv/bin/activate  # Linux/macOS\n   # or\n   .venv\\Scripts\\activate     # Windows\n   ```\n\n3. Install dependencies from the lockfile:\n\n   ```bash\n   uv sync\n   ```\n\n4. Set required environment variables:\n\n   ```bash\n   # Required: URL for your SearXNG instance's Search API\n   export SEARXNG_QUERY_URL=\"http://your-searxng-instance.local:8080\"\n   # Optional: Custom User-Agent\n   export USER_AGENT=\"CustomAgent/1.0\"\n   ```\n\n5. Run the server:\n\n   ```bash\n   # For network (SSE) mode (e.g., for LibreChat)\n   mcp-server --sse-host 0.0.0.0 --sse-port 3001\n\n   # For direct stdio mode (e.g., for Claude Desktop)\n   mcp-server\n   ```\n\nAvailable arguments:\n\n- `--sse-host`: SSE listening address (e.g., `0.0.0.0`). Enables SSE mode.\n- `--sse-port`: SSE listening port (e.g., `3001`). Enables SSE mode.\n- `--user-agent`: Custom User-Agent string (overrides `USER_AGENT` env var).\n\n> **Note**: If neither `--sse-host` nor `--sse-port` are provided (and `SSE_HOST`/`SSE_PORT` env\n> vars are not set), the server defaults to `stdio` mode. The `SEARXNG_QUERY_URL` environment\n> variable is *always* required.\n\n## 🔌 How to connect\n\nYou can connect to the server in two ways:\n\n| Method                    | What it means                                           | When to use it                                  |\n| ------------------------- | ------------------------------------------------------- | ----------------------------------------------- |\n| Network connection (SSE)  | The server listens on a network port for connections.   | Best for LibreChat or other networked clients.  |\n| Direct connection (stdio) | The server communicates directly via standard input/out. | Useful for local testing or Claude Desktop. |\n\n## 📚 Learn more about MCP\n\nHere are a few resources to get you started:\n\n- [MCP Specification](https://spec.modelcontextprotocol.io/)\n- [MCP Python SDK](https://github.com/modelcontextprotocol/python-sdk)\n- [MCP Example Servers](https://github.com/modelcontextprotocol/servers)\n\n## 📄 License\n\nThis project is licensed under the GPLv3. See the [LICENSE](LICENSE) file for full details.\n",
      "npm_url": "https://www.npmjs.com/package/mcp-server",
      "npm_downloads": 29732,
      "keywords": [
        "tcpipuk",
        "mcp",
        "ai",
        "tcpipuk mcp",
        "search tcpipuk",
        "mcp server"
      ],
      "category": "web-search"
    },
    "techkwon--mcp-gemini": {
      "owner": "techkwon",
      "name": "mcp-gemini",
      "url": "https://github.com/techkwon/mcp-gemini",
      "imageUrl": "/freedevtools/mcp/pfp/techkwon.webp",
      "description": "Leverages Google's Gemini API to generate text, create and analyze images, perform video analysis on YouTube content, and conduct web searches. Provides a range of advanced AI functionalities for various applications.",
      "stars": 4,
      "forks": 2,
      "license": "No License",
      "language": "TypeScript",
      "updated_at": "2025-07-23T14:36:23Z",
      "readme_content": "# MCP Gemini API 서버\n\nCursor와 Claude를 위한 Google Gemini API 서버입니다. 텍스트 생성, 이미지 분석, 비디오 분석 등 Gemini의 다양한 기능을 제공합니다.\n\n## 주요 기능\n\n- 텍스트 생성 (gemini-2.0-flash 모델 사용)\n- 이미지 생성 및 분석\n- YouTube 비디오 분석\n- 웹 검색\n\n## 시작하기\n\n### 필수 요구사항\n\n- Node.js 18.0.0 이상\n- npm 또는 yarn\n- Google API 키 (Gemini API 접근용)\n\n### 설치\n\n```bash\n# 저장소 클론\ngit clone https://github.com/techkwon/mcp-gemini.git\ncd mcp-gemini\n\n# 의존성 설치\nnpm install\n```\n\n### 환경 설정\n\n1. `config.ts` 파일에 Google API 키 설정:\n\n```typescript\nexport default {\n  googleApiKey: \"your_api_key_here\",\n  // 기타 설정...\n};\n```\n\n### 빌드 및 실행\n\n```bash\n# TypeScript 빌드\nnpm run build\n\n# 서버 시작 (PM2 사용)\nnpm start\n\n# 개발 모드로 실행\nnpm run dev\n```\n\n### PM2 서버 관리\n\n서버는 PM2를 통해 자동으로 관리됩니다. 다음 명령어로 서버를 관리할 수 있습니다:\n\n```bash\n# 서버 상태 확인\nnpm run status\n\n# 서버 로그 확인\nnpm run logs\n\n# 서버 중지\nnpm run stop\n\n# 서버 재시작\nnpm run restart\n\n# 시스템 재시작 시 자동 실행 설정\npm2 startup\npm2 save\n```\n\n## Cursor/Claude 연동\n\n### MCP 설정\n\n`~/.cursor/mcp.json` 파일에 다음 설정을 추가하세요:\n\n```json\n{\n  \"github.com/techkwon/mcp-gemini\": {\n    \"command\": \"npm\",\n    \"args\": [\"start\"],\n    \"cwd\": \"<프로젝트_경로>\",\n    \"env\": {\n      \"NODE_ENV\": \"production\"\n    },\n    \"disabled\": false,\n    \"autoStart\": true,\n    \"autoApprove\": [\n      \"gem-generate\",\n      \"gem-generate-image\",\n      \"gem-analyze-video\",\n      \"gem-search\"\n    ]\n  }\n}\n```\n\n### API 엔드포인트\n\n- `/gem-generate`: 텍스트 생성\n- `/gem-generate-image`: 이미지 생성/분석\n- `/gem-analyze-video`: YouTube 비디오 분석\n- `/gem-search`: 웹 검색\n\n## 주요 업데이트\n\n### 최신 버전 (2024-03)\n- PM2를 통한 서버 자동화 구현\n- gemini-2.0-flash 모델로 통일\n- 자동 재시작 및 오류 복구 기능 추가\n- 환경 설정 개선\n\n### 이전 버전\n- YouTube 비디오 분석 기능 추가\n- 이미지 생성/분석 기능 개선\n- 웹 검색 기능 추가\n\n## 문제 해결\n\n### 일반적인 문제\n\n1. **서버가 시작되지 않는 경우**\n   ```bash\n   # PM2 로그 확인\n   npm run logs\n   \n   # PM2 프로세스 상태 확인\n   npm run status\n   ```\n\n2. **API 키 오류**\n   - `config.ts` 파일에서 API 키가 올바르게 설정되었는지 확인\n   - Gemini API 할당량 및 권한 확인\n\n3. **메모리 사용량 문제**\n   - `ecosystem.config.js`에서 메모리 제한 설정 확인\n   - PM2 모니터링으로 메모리 사용량 추적\n\n## 기여하기\n\n1. Fork the Project\n2. Create your Feature Branch (`git checkout -b feature/AmazingFeature`)\n3. Commit your Changes (`git commit -m 'Add some AmazingFeature'`)\n4. Push to the Branch (`git push origin feature/AmazingFeature`)\n5. Open a Pull Request\n\n## 라이선스\n\n이 프로젝트는 MIT 라이선스를 따릅니다. 자세한 내용은 [LICENSE](LICENSE) 파일을 참조하세요.\n\n## 연락처\n\n프로젝트 관리자: techkwon\n이메일: techkwon@example.com\n프로젝트 링크: [https://github.com/techkwon/mcp-gemini](https://github.com/techkwon/mcp-gemini)\n\n## 주요 의존성\n\n- @google/generative-ai: ^0.1.3 (Gemini API SDK)\n- @fastify/cors: ^8.5.0 (CORS 지원)\n- fastify: ^4.29.0 (웹 서버 프레임워크)\n- googleapis: ^148.0.0 (Google API 지원)\n- typescript: ^5.0.0\n- zod: ^3.24.2 (데이터 검증)\n- pino: ^8.21.0 (로깅)\n\n## Claude 데스크톱 앱 통합 가이드\n\n### 설정 파일 위치\nClaude 데스크톱 앱의 설정 파일은 다음 경로에 위치합니다:\n- Windows: `%APPDATA%/Claude/config.json`\n- macOS: `~/Library/Application Support/Claude/config.json`\n\n### JSON 설정 예시\n\n```json\n{\n  \"apis\": [\n    {\n      \"name\": \"MCP Gemini\",\n      \"url\": \"http://localhost:8000\",\n      \"methods\": [\n        {\n          \"name\": \"텍스트 생성\",\n          \"method\": \"gem-generate\",\n          \"template\": {\n            \"jsonrpc\": \"2.0\",\n            \"id\": \"{uuid}\",\n            \"method\": \"gem-generate\",\n            \"params\": {\n              \"prompt\": \"{input}\"\n            }\n          }\n        },\n        {\n          \"name\": \"이미지 생성\",\n          \"method\": \"gem-generate-image\",\n          \"template\": {\n            \"jsonrpc\": \"2.0\",\n            \"id\": \"{uuid}\",\n            \"method\": \"gem-generate-image\",\n            \"params\": {\n              \"prompt\": \"{input}\"\n            }\n          }\n        },\n        {\n          \"name\": \"비디오 분석\",\n          \"method\": \"gem-analyze-video\",\n          \"template\": {\n            \"jsonrpc\": \"2.0\",\n            \"id\": \"{uuid}\",\n            \"method\": \"gem-analyze-video\",\n            \"params\": {\n              \"videoUrl\": \"{input}\",\n              \"query\": \"이 영상의 주요 내용을 요약해주세요\"\n            }\n          }\n        },\n        {\n          \"name\": \"웹 검색\",\n          \"method\": \"gem-search\",\n          \"template\": {\n            \"jsonrpc\": \"2.0\",\n            \"id\": \"{uuid}\",\n            \"method\": \"gem-search\",\n            \"params\": {\n              \"query\": \"{input}\"\n            }\n          }\n        }\n      ]\n    }\n  ]\n}\n```\n\n### 변수 설명\n\n- `{uuid}`: 자동으로 생성되는 고유 요청 ID\n- `{input}`: Claude 채팅창에 입력한 텍스트\n\n### 사용 방법\n\n1. Claude 데스크톱 앱의 설정 파일을 엽니다.\n2. 위의 JSON 설정을 기존 설정에 추가합니다.\n3. Claude 데스크톱 앱을 재시작합니다.\n4. 채팅창에서 다음과 같이 사용할 수 있습니다:\n\n```\n@MCP Gemini.텍스트 생성 한국의 전통 음식에 대해 설명해주세요\n@MCP Gemini.이미지 생성 한옥마을의 아름다운 풍경\n@MCP Gemini.비디오 분석 https://youtube.com/watch?v=VIDEO_ID\n@MCP Gemini.웹 검색 최신 인공지능 기술 동향\n```\n\n### 응답 형식\n\n모든 API 응답은 다음 형식을 따릅니다:\n\n```json\n{\n  \"jsonrpc\": \"2.0\",\n  \"id\": \"요청에서 보낸 ID\",\n  \"result\": {\n    \"content\": \"응답 내용\"\n  }\n}\n```\n\n### 오류 응답\n\n오류가 발생한 경우 다음 형식으로 응답합니다:\n\n```json\n{\n  \"jsonrpc\": \"2.0\",\n  \"id\": \"요청에서 보낸 ID\",\n  \"error\": {\n    \"code\": 오류코드,\n    \"message\": \"오류 메시지\",\n    \"data\": {\n      \"details\": \"상세 오류 정보\"\n    }\n  }\n}\n```\n\n## 오류 처리\n\n서버는 다음과 같은 상황에서 적절한 오류 응답을 반환합니다:\n\n- 400: 잘못된 요청 형식\n- 401: 인증 오류 (API 키 관련)\n- 500: 서버 내부 오류\n\n## 보안 고려사항\n\n- API 키는 반드시 환경 변수로 관리하세요\n- 프로덕션 환경에서는 적절한 보안 설정을 추가하세요\n- 민감한 정보는 로그에 기록하지 않도록 주의하세요\n\n## 문제 해결\n\n### 포트 충돌\n이미 8000번 포트가 사용 중인 경우:\n```bash\n# 기존 Node.js 프로세스 종료\npkill -f \"node\"\n```\n\n### 서버 안정성\n서버가 예기치 않게 종료되는 경우:\n- PM2나 다른 프로세스 관리자 사용을 고려하세요\n- 로그를 확인하여 종료 원인을 파악하세요\n\n## 개발 가이드\n\n### 로깅\n- Pino 로거를 사용하여 구조화된 로깅을 구현했습니다\n- 개발 환경에서는 pino-pretty를 통해 가독성 있는 로그가 출력됩니다\n\n### 타입 안정성\n- TypeScript와 Zod를 사용하여 런타임 타입 안정성을 보장합니다\n- API 요청/응답에 대한 스키마 검증이 구현되어 있습니다\n\n## CLINE MCP 마켓플레이스 등록 가이드\n\n### 사전 준비사항\n\n1. GitHub 저장소가 공개되어 있어야 합니다\n2. README.md 파일에 명확한 설치 및 설정 방법이 포함되어 있어야 합니다\n3. (선택사항) `llms-install.md` 파일을 통해 AI 에이전트를 위한 추가 설치 가이드를 제공할 수 있습니다\n\n### 등록 절차\n\n1. [CLINE MCP 마켓플레이스 저장소](https://github.com/cline/mcp-marketplace)에 새로운 이슈를 생성합니다\n\n2. 이슈에 다음 정보를 포함합니다:\n   - **GitHub 저장소 URL:** https://github.com/techkwon/mcp-gemini\n   - **로고 이미지:** 400×400 크기의 PNG 파일\n   - **추가 이유:** 이 MCP 서버가 CLINE 사용자들에게 제공할 수 있는 가치\n   예시:\n   ```markdown\n   ## MCP Gemini 서버 등록 요청\n   \n   ### GitHub 저장소\n   https://github.com/techkwon/mcp-gemini\n   \n   ### 주요 기능\n   - Gemini API를 활용한 텍스트 생성\n   - 이미지 생성 및 편집 (gemini-2.0-flash-exp 모델 사용)\n   - YouTube 비디오 콘텐츠 분석\n   - 웹 검색 기능\n   \n   ### 사용자 이점\n   - 최신 Gemini 모델을 MCP 프로토콜을 통해 쉽게 활용\n   - 다양한 미디어 형식(텍스트, 이미지, 비디오) 처리 가능\n   - 명확한 JSON-RPC 인터페이스로 쉬운 통합\n   - 상세한 문서화와 예제 제공\n   ```\n\n3. CLINE이 README.md만으로 서버를 성공적으로 설치할 수 있는지 테스트합니다\n\n### 승인 절차\n\n1. CLINE 팀이 제출된 MCP 서버를 검토합니다\n2. 보안 및 안정성 검증을 진행합니다\n3. 승인되면 마켓플레이스에 등록되어 모든 CLINE 사용자가 접근할 수 있게 됩니다\n\n### 설치 가이드 최적화\n\n`llms-install.md` 파일을 생성하여 AI 에이전트를 위한 추가 설치 가이드를 제공할 수 있습니다:\n\n```markdown\n# MCP Gemini 서버 설치 가이드 (AI 에이전트용)\n\n## 환경 요구사항\n- Node.js 18.0.0 이상\n- npm 또는 yarn\n- Google AI Studio API 키\n\n## 설치 단계\n1. 저장소 클론\n2. 의존성 설치: `npm install`\n3. 환경 변수 설정: GOOGLE_API_KEY 추가\n4. 빌드: `npm run build`\n5. 서버 실행: `npm run start`\n\n## 설정 검증\n- 8000번 포트 사용 가능 여부 확인\n- API 키 유효성 검증\n- CORS 설정 확인\n\n## 문제 해결\n- 포트 충돌 시 해결 방법\n- API 키 오류 해결 방법\n- 일반적인 설치 문제 해결 가이드\n``` ",
      "npm_url": "https://www.npmjs.com/package/mcp-gemini",
      "npm_downloads": 176,
      "keywords": [
        "google",
        "gemini",
        "ai",
        "google gemini",
        "gemini api",
        "mcp gemini"
      ],
      "category": "web-search"
    },
    "tedlikeskix--mcp-ip-geolocator": {
      "owner": "tedlikeskix",
      "name": "mcp-ip-geolocator",
      "url": "https://github.com/tedlikeskix/mcp-ip-geolocator",
      "imageUrl": "/freedevtools/mcp/pfp/tedlikeskix.webp",
      "description": "Provides detailed geolocation information for any IP address, including network data, ISP, and timezone details. No API key or registration is required for access.",
      "stars": 3,
      "forks": 6,
      "license": "No License",
      "language": "TypeScript",
      "updated_at": "2025-08-06T11:07:34Z",
      "readme_content": "# MCP IP Geolocation Server\n\nA Model Context Protocol (MCP) server that provides IP geolocation services via IP-API.com. Free to use, no API key required.\n\n## Features\n\n- Get detailed location information for any IP address\n- Network information including ISP and AS number\n- Timezone data\n- No API key or registration required\n- Clean, formatted output for Claude\n\n## Quick Start\n\n1. Install globally:\n```bash\nnpm install -g mcp-ip-geolocator\n```\n\n2. Run the server:\n```bash\nmcp-ip-geolocator\n```\n\n## Local Development\n\n1. Clone the repository:\n```bash\ngit clone https://github.com/tedlikeskix/mcp-ip-geolocator.git\ncd mcp-ip-geolocator\n```\n\n2. Install dependencies:\n```bash\nyarn install\n```\n\n3. Build and run:\n```bash\nyarn build\nyarn start\n```\n\n## Usage with Claude\n\nOnce running, connect to the server in Claude Desktop. Example usage:\n\n```\nClaude, can you check the location of IP address 8.8.8.8?\n```\n\nClaude will use the tool to fetch and display location information.\n\n## API Response Format\n\nThe tool returns structured data including:\n- City, region, and country\n- Latitude and longitude\n- Timezone\n- ISP and organization\n- AS number\n\n## Rate Limiting\n\nIP-API.com's free tier includes:\n- 45 requests per minute\n- IPv4 and IPv6 support\n- No API key needed\n\n## License\n\nMIT License - feel free to use and modify!",
      "npm_url": "https://www.npmjs.com/package/mcp-ip-geolocator",
      "npm_downloads": 128,
      "keywords": [
        "geolocator",
        "geolocation",
        "address",
        "ip geolocator",
        "geolocator provides",
        "geolocation information"
      ],
      "category": "web-search"
    },
    "terryso--mcp-pinterest": {
      "owner": "terryso",
      "name": "mcp-pinterest",
      "url": "https://github.com/terryso/mcp-pinterest",
      "imageUrl": "/freedevtools/mcp/pfp/terryso.webp",
      "description": "Search for images on Pinterest using keywords and retrieve detailed information about those images.",
      "stars": 16,
      "forks": 7,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-08-14T02:40:14Z",
      "readme_content": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/terryso-mcp-pinterest-badge.png)](https://mseep.ai/app/terryso-mcp-pinterest)\n\n# Pinterest MCP Server\n\n[![npm version](https://badge.fury.io/js/pinterest-mcp-server.svg)](https://badge.fury.io/js/pinterest-mcp-server)\n![NPM Downloads](https://img.shields.io/npm/dw/pinterest-mcp-server)\n[![smithery badge](https://smithery.ai/badge/@terryso/mcp-pinterest)](https://smithery.ai/server/@terryso/mcp-pinterest)\n[![Verified on MseeP](https://mseep.ai/badge.svg)](https://mseep.ai/app/23da55f4-6d6a-4dc9-af31-cd6073eb1b27)\n\nA Model Context Protocol (MCP) server for Pinterest image search and information retrieval.\n\n## Features\n\n- Search for images on Pinterest by keywords\n- Retrieve detailed information about Pinterest images\n- Seamless integration with Cursor IDE through MCP\n- Support for headless browser mode\n- Limit control for search results\n- Search and download images from Pinterest\n\n## Prerequisites\n\n- [Node.js](https://nodejs.org/) (v18 or higher)\n- [Cursor IDE](https://cursor.sh/) for MCP integration\n\n## Installation\n\n### Using NPX (Recommended)\n\nThe easiest way to use Pinterest MCP Server is via npx:\n\n```bash\nnpx pinterest-mcp-server\n```\n\nYou can configure the server with command-line options:\n\n```bash\n# Specify download directory\nnpx pinterest-mcp-server --downloadDir /path/to/downloads\n\n# Specify filename template\nnpx pinterest-mcp-server --filenameTemplate \"pinterest_{id}\"\n\n# Specify both options\nnpx pinterest-mcp-server --downloadDir ./images --filenameTemplate \"pinterest_{id}\"\n```\n\n### Global Installation\n\nTo install the package globally and use it directly from the command line:\n\n```bash\nnpm install -g pinterest-mcp-server\n```\n\nAfter installation, you can run the server with:\n\n```bash\npinterest-mcp-server\n```\n\nWith the same command line options as the NPX version:\n\n```bash\npinterest-mcp-server --downloadDir /path/to/downloads --filenameTemplate \"pinterest_{id}\"\n```\n\n### Installing via Smithery\n\nTo install mcp-pinterest for Claude Desktop automatically via [Smithery](https://smithery.ai/server/mcp-pinterest):\n\n```bash\nnpx -y @smithery/cli install mcp-pinterest --client claude\n```\n\n### Manual Installation\n\n1. Clone this repository:\n   ```bash\n   git clone https://github.com/terryso/mcp-pinterest.git pinterest-mcp-server\n   cd pinterest-mcp-server\n   ```\n\n2. Install dependencies:\n   ```bash\n   npm install\n   ```\n\n3. Build the server:\n   ```bash\n   npm run build\n   ```\n\n4. Run the server:\n   ```bash\n   npm start\n   ```\n\n## Configuring as MCP Server in Cursor\n\n1. Open Cursor IDE\n2. Go to Settings (⚙️) > Extensions > MCP\n3. Click \"Add Server\"\n4. Enter the following details:\n   - Name: Pinterest MCP\n   - Type: Command\n   - Command: `node`\n   - Args: `[\"/path/to/mcp-pinterest/dist/pinterest-mcp-server.js\"]`\n\n   或者直接编辑Cursor的MCP配置文件（通常位于`~/.cursor/mcp.json`），添加以下内容：\n   ```json\n   \"pinterest\": {\n     \"command\": \"node\",\n     \"args\": [\"/path/to/mcp-pinterest/dist/pinterest-mcp-server.js\"]\n   }\n   ```\n5. Click \"Save\"\n\n### Alternative: Using NPX for Cursor Configuration\n\nYou can also configure Cursor to use the npx version of the server:\n\n1. Open Cursor IDE\n2. Go to Settings (⚙️) > Extensions > MCP\n3. Click \"Add Server\"\n4. Enter the following details:\n   - Name: Pinterest MCP\n   - Type: Command\n   - Command: `npx`\n   - Args: `[\"pinterest-mcp-server\"]`\n5. Click \"Save\"\n\n### Complete Configuration Example with Environment Variables\n\nFor the most flexibility, you can configure the server with environment variables in your Cursor MCP configuration:\n\n```json\n\"pinterest\": {\n  \"command\": \"npx\",\n  \"env\": {\n    \"MCP_PINTEREST_DOWNLOAD_DIR\": \"/Users/xxx/Desktop/Images\",\n    \"MCP_PINTEREST_FILENAME_TEMPLATE\": \"pin_{imageId}_{timestamp}.{fileExtension}\",\n    \"MCP_PINTEREST_PROXY_SERVER\": \"http://127.0.0.1:7890\"\n  },\n  \"args\": [\"pinterest-mcp-server\"]\n}\n```\n\nThis configuration:\n- Uses npx to run the server\n- Sets a custom download directory on your desktop\n- Uses a custom filename template with both image ID and timestamp\n- Configures a proxy server for users in regions where Pinterest might be blocked\n\nAdd this to your `~/.cursor/mcp.json` file or set up through the Cursor IDE interface.\n\n## Available MCP Functions\n\nThe server exposes the following MCP functions:\n\n- `pinterest_search`: Search for images on Pinterest by keyword\n  - Parameters:\n    - `keyword`: Search term (required)\n    - `limit`: Number of images to return (default: 10)\n    - `headless`: Whether to use headless browser mode (default: true)\n\n- `pinterest_get_image_info`: Get detailed information about a Pinterest image\n  - Parameters:\n    - `image_url`: URL of the Pinterest image (required)\n\n- `pinterest_search_and_download`: Search and download images from Pinterest\n  - Parameters:\n    - `keyword`: Search term (required)\n    - `limit`: Number of images to return (default: 10)\n    - `headless`: Whether to use headless browser mode (default: true)\n\n## Example Usage in Cursor\n\nOnce configured, you can use the Pinterest MCP functions directly in Cursor's AI chat:\n\n```\nSearch for robot images on Pinterest\n```\n\nThe AI will use the MCP server to search Pinterest and display the results.\n\n### Example Screenshot\n\n\n\n*Screenshot showing a search for 20 images of 三上悠亚 with all images successfully downloaded.*\n\n## Development\n\n### Project Structure\n\n- `pinterest-mcp-server.ts`: Main server file\n- `dist/pinterest-mcp-server.js`: Built JavaScript file for production\n- `package.json`: Project configuration and dependencies\n\n### Adding New Features\n\nTo add new MCP functions:\n\n1. Modify `pinterest-mcp-server.ts`\n2. Register new functions using the MCP SDK\n3. Implement the function logic\n4. Rebuild with `npm run build`\n\n## Troubleshooting\n\n- If the server fails to start, check if the port is already in use\n- Ensure all dependencies are correctly installed with `npm install`\n- Make sure TypeScript is properly configured with a `tsconfig.json` file\n- If you encounter build errors, try running `npm install -D typescript @types/node`\n- Verify network connectivity for Pinterest access\n\n## License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n\n## Configuration Options\n\n### Command Line Options (NPX Mode)\n\nWhen using the server via npx, you can configure it using the following command line options:\n\n- `--downloadDir`: Specifies the root directory for downloading images\n  ```bash\n  npx pinterest-mcp-server --downloadDir /path/to/downloads\n  ```\n\n- `--filenameTemplate`: Specifies the filename template for downloaded images\n  ```bash\n  npx pinterest-mcp-server --filenameTemplate \"pin_{imageId}_{timestamp}\"\n  ```\n\n- `--port`: Specifies the port for the server to listen on (default: 3000)\n  ```bash\n  npx pinterest-mcp-server --port 8080\n  ```\n\n- `--proxyServer`: Specifies the proxy server to use for connecting to Pinterest\n  ```bash\n  npx pinterest-mcp-server --proxyServer \"http://127.0.0.1:7890\"\n  ```\n\nYou can combine multiple options:\n```bash\nnpx pinterest-mcp-server --downloadDir ./images --filenameTemplate \"pinterest_{id}\" --port 8080 --proxyServer \"http://127.0.0.1:7890\"\n```\n\n### Environment Variables\n\nThe server also supports the following environment variables for configuration:\n\n- `MCP_PINTEREST_DOWNLOAD_DIR`: Specifies the root directory for downloading images. If not set, the default is the `../downloads` directory relative to the server script.\n- `MCP_PINTEREST_FILENAME_TEMPLATE`: Specifies the filename template for downloaded images. If not set, the default is `pinterest_{imageId}.{fileExtension}`.\n- `MCP_PINTEREST_PROXY_SERVER`: Specifies the proxy server to use for connecting to Pinterest. Format should be `protocol://host:port`, for example `http://127.0.0.1:7890` or `socks5://127.0.0.1:1080`.\n\nThese environment variables can be set in several ways:\n1. Directly in your terminal (as shown in the examples below)\n2. In your Cursor MCP configuration through the `env` field (see [Complete Configuration Example](#complete-configuration-example-with-environment-variables))\n3. In a `.env` file in the project root directory\n4. Through command line options with npx (as shown in the [Command Line Options](#command-line-options-npx-mode) section)\n\n### Usage\n\n#### Setting Download Directory\n\n1. Using npx with command line options:\n```bash\nnpx pinterest-mcp-server --downloadDir /path/to/your/download/directory\n```\n\n2. Set the download directory using an environment variable:\n\n```bash\n# Linux/macOS\nexport MCP_PINTEREST_DOWNLOAD_DIR=/path/to/your/download/directory\nnpx pinterest-mcp-server\n\n# Windows (CMD)\nset MCP_PINTEREST_DOWNLOAD_DIR=C:\\path\\to\\your\\download\\directory\nnpx pinterest-mcp-server\n\n# Windows (PowerShell)\n$env:MCP_PINTEREST_DOWNLOAD_DIR=\"C:\\path\\to\\your\\download\\directory\"\nnpx pinterest-mcp-server\n```\n\n3. If the environment variable is not set, the server will use the default download directory (relative to the server script's `../downloads`).\n\n#### Setting Filename Template\n\n1. Using npx with command line options:\n```bash\nnpx pinterest-mcp-server --filenameTemplate \"pin_{imageId}_{timestamp}.{fileExtension}\"\n```\n\n2. Using an environment variable:\n\n```bash\n# Linux/macOS\nexport MCP_PINTEREST_FILENAME_TEMPLATE=\"pin_{imageId}_{timestamp}.{fileExtension}\"\nnpx pinterest-mcp-server\n\n# Windows (CMD)\nset MCP_PINTEREST_FILENAME_TEMPLATE=\"pin_{imageId}_{timestamp}.{fileExtension}\"\nnpx pinterest-mcp-server\n\n# Windows (PowerShell)\n$env:MCP_PINTEREST_FILENAME_TEMPLATE=\"pin_{imageId}_{timestamp}.{fileExtension}\"\nnpx pinterest-mcp-server\n```\n\nThe template supports the following variables:\n- `{imageId}`: The unique ID of the Pinterest image\n- `{fileExtension}`: The file extension (e.g., jpg, png)\n- `{timestamp}`: Current UTC timestamp in YYYYMMDDHHMMSS format\n- `{index}`: The index number when downloading multiple images (starts from 1)\n\nExample templates:\n- `pinterest_{imageId}.{fileExtension}` (default)\n- `pin_{timestamp}_{imageId}.{fileExtension}`\n- `pinterest_image_{index}_{imageId}.{fileExtension}`\n- `{timestamp}_pinterest.{fileExtension}`\n\nIf the template is invalid (e.g., contains unsupported variables or has mismatched brackets), the server will log a warning and use the default template.\n\n#### Setting Proxy Server\n\nIf you need to use a proxy to access Pinterest (especially in regions where Pinterest might be restricted), you can set the proxy configuration:\n\n1. Using npx with command line options:\n```bash\nnpx pinterest-mcp-server --proxyServer \"http://127.0.0.1:7890\"\n```\n\n2. Using an environment variable:\n\n```bash\n# Linux/macOS\nexport MCP_PINTEREST_PROXY_SERVER=\"http://127.0.0.1:7890\"\nnpx pinterest-mcp-server\n\n# Windows (CMD)\nset MCP_PINTEREST_PROXY_SERVER=http://127.0.0.1:7890\nnpx pinterest-mcp-server\n\n# Windows (PowerShell)\n$env:MCP_PINTEREST_PROXY_SERVER=\"http://127.0.0.1:7890\"\nnpx pinterest-mcp-server\n```\n\nSupported proxy protocols:\n- HTTP: `http://host:port`\n- HTTPS: `https://host:port`\n- SOCKS4: `socks4://host:port`\n- SOCKS5: `socks5://host:port`\n\nThe proxy configuration affects both the browser used for searching and the image downloading process.\n\n#### Notes\n\n- The server will verify the existence and writability of the download directory when starting. If the directory does not exist, it will attempt to create it; if it cannot be created or written to, the server will exit.\n- Clients should not specify download paths or filename templates through parameters when calling download-related tools, as all downloads will use the server's environment variable configuration or defaults.\n- The server automatically sanitizes filenames by replacing illegal characters (such as `/`, `\\`, `:`, `*`, `?`, `\"`, `<`, `>`, `|`) with underscores.\n\n#### Interface Description\n\nThe server provides the following MCP tools:\n\n1. `pinterest_search`: Search for Pinterest images by keyword\n2. `pinterest_get_image_info`: Get detailed information about a Pinterest image\n3. `pinterest_search_and_download`: Search and download Pinterest images\n\nFor detailed interface parameter references, please refer to the MCP tool definitions. \n\n## ⭐ Star History\n\n[![Star History Chart](https://api.star-history.com/svg?repos=terryso/mcp-pinterest&type=Date)](https://www.star-history.com/#terryso/mcp-pinterest&Date)",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "pinterest",
        "images",
        "search",
        "images pinterest",
        "search images",
        "mcp pinterest"
      ],
      "category": "web-search"
    },
    "terryso--meta_tag_genie": {
      "owner": "terryso",
      "name": "meta_tag_genie",
      "url": "https://github.com/terryso/meta_tag_genie",
      "imageUrl": "/freedevtools/mcp/pfp/terryso.webp",
      "description": "Manage image metadata to improve macOS Spotlight search functionality by adding tags, descriptions, people, and location data to JPG, PNG, and HEIC images through a standard MCP interface over Stdio.",
      "stars": 1,
      "forks": 2,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-05-11T16:58:49Z",
      "readme_content": "# MetaTag Genie\n\n[![npm version](https://badge.fury.io/js/metatag-genie.svg)](https://badge.fury.io/js/metatag-genie)\n![NPM Downloads](https://img.shields.io/npm/dw/metatag-genie)\n[![Node.js Version](https://img.shields.io/node/v/metatag-genie)](https://nodejs.org)\n[![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg)](https://github.com/terryso/metatag-genie/pulls)\n[![smithery badge](https://smithery.ai/badge/@terryso/meta_tag_genie)](https://smithery.ai/server/@terryso/meta_tag_genie)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![DeepWiki](https://img.shields.io/badge/DeepWiki-项目文档-blue)](https://deepwiki.com/terryso/metatag-genie)\n\n## 简介\n\nMetaTag Genie 是一个 macOS Stdio MCP 服务，专为写入图片元数据以增强 Spotlight 搜索而设计。该服务可被 AI 代理或其他需要本地管理图片元数据的应用程序调用，通过标准输入输出（Stdio）与客户端通信，提供符合 MCP (Machine Comprehension Protocol) 规范的接口。\n\n## 功能特性\n\n- 通过 Stdio 暴露符合 MCP 规范的服务\n- 提供 `writeImageMetadata` MCP Tool\n- 支持向 JPG, PNG, HEIC 图片写入元数据\n- 支持的元数据类型：\n  - 标签 (Tags)\n  - 描述 (Description)\n  - 人物 (People - 作为关键词)\n  - 地点 (Location - 文本)\n- 写入的元数据可被 macOS Spotlight 搜索\n\n## 系统要求\n\n- **Node.js 版本**：Node.js 22.x LTS 或更高版本\n- **隐含依赖**：本项目使用 `exiftool-vendored`，它会自动管理 ExifTool 的依赖\n\n## 安装\n\n### 安装 Smithery\n\n要自动将 MetaTag Genie 安装到 Claude Desktop，请使用 [Smithery](https://smithery.ai/server/@terryso/meta_tag_genie)：\n\n```bash\nnpx -y @smithery/cli install @terryso/metatag_genie --client claude\n```\n\n### 通过NPM安装（推荐）\n\n```bash\n# 全局安装\nnpm install -g metatag-genie\n```\n\n使用全局安装后，可以通过以下命令直接运行：\n\n```bash\nmetatag-genie\n```\n\n或者，您可以不安装直接使用npx运行：\n\n```bash\nnpx metatag-genie\n```\n\n这种方式不需要全局安装，也是AI代理等客户端调用的推荐方式。\n\n### 通过源码安装\n\n```bash\n# 1. 克隆仓库\ngit clone <repository-url>\ncd metatag-genie\n\n# 2. 安装依赖\nnpm install\n# 或者: yarn install\n\n# 3. 编译 TypeScript 代码\nnpm run build\n# 或者: yarn build\n\n# 4. 本地链接（可选，用于测试npx和命令行调用）\nnpm link\n```\n\n## 运行服务\n\n### 通过NPX运行（推荐）\n\n安装包后，可以直接通过npx运行：\n\n```bash\nnpx metatag-genie\n```\n\n这种方式不需要全局安装包，适合作为AI代理或其他客户端的调用方式。\n\n### 开发模式\n\n开发过程中，可以直接使用TypeScript源码运行服务：\n\n```bash\nnpm run start:dev\n```\n\n此命令使用`ts-node`直接运行TypeScript代码，无需预先编译，适合快速开发和调试。\n\n### 生产模式\n\n构建后，可以通过以下命令运行编译好的JavaScript代码：\n\n```bash\nnpm start\n# 或者直接: node dist/main.js\n```\n\n服务启动后会监听标准输入输出（Stdio），等待MCP客户端连接并发送JSON-RPC消息。\n\n### 与MCP客户端集成\n\nAI代理（如Cursor）或其他客户端需要在其配置中指定命令的完整路径：\n\n- 基于Node.js：`/usr/local/bin/node /path/to/metatag-genie/dist/main.js`\n- 使用npx：`/usr/local/bin/npx metatag-genie`\n\n注意：本服务不监听网络端口，仅通过标准输入输出通信。\n\n### Cursor MCP集成\n\n在Cursor编辑器中，可以通过以下步骤集成MetaTag Genie：\n\n1. 在项目根目录创建`.cursor/mcp.json`文件\n2. 添加以下配置内容：\n\n```json\n{\n  \"mcpServers\": {\n    \"MetaTagGenie\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"metatag-genie\"\n      ]\n    }\n  }\n}\n```\n\n配置完成后，Cursor中的AI助手将能够使用MetaTag Genie提供的writeImageMetadata工具，直接为图片添加元数据，增强Spotlight搜索体验。\n\n## MCP 交互协议\n\n### 基础\n\n通信基于 Stdio 上的 JSON-RPC 2.0 协议。\n\n### 初始化流程\n\n客户端需要先发送 `initialize` 请求，服务器响应 `InitializeResult`，然后客户端发送 `initialized` 通知完成握手。\n\n### Tool 调用\n\n#### 示例：writeImageMetadata 调用\n\n**JSON-RPC 请求示例**:\n\n```json\n{\n  \"jsonrpc\": \"2.0\",\n  \"id\": \"request-id-123\",\n  \"method\": \"writeImageMetadata\",\n  \"params\": {\n    \"filePath\": \"/Users/username/Pictures/photo.jpg\",\n    \"metadata\": {\n      \"tags\": [\"Vacation\", \"Beach\"],\n      \"description\": \"Sunset view from the hotel.\",\n      \"people\": [\"Alice\", \"Bob\"],\n      \"location\": \"Hawaii, USA\"\n    },\n    \"overwrite\": true\n  }\n}\n```\n\n**JSON-RPC 成功响应示例**:\n\n```json\n{\n  \"jsonrpc\": \"2.0\",\n  \"id\": \"request-id-123\",\n  \"result\": {\n    \"success\": true,\n    \"filePath\": \"/Users/username/Pictures/photo.jpg\",\n    \"message\": \"Metadata successfully written.\"\n  }\n}\n```\n\n**注意**：实际使用时 `filePath` 需要是有效的绝对路径。\n\n完整的 Tool 参数、返回值和错误代码定义，请参阅 [MCP Tools 定义文档](./docs/mcp-tools-definition.md)。\n\n## 运行测试\n\n```bash\n# 运行所有单元测试和集成测试\nnpm test\n# 或者: yarn test\n\n# 在监视模式下运行测试\nnpm run test:watch\n# 或者: yarn test:watch\n\n# 运行测试并生成覆盖率报告 (输出到 coverage/ 目录)\nnpm run test:cov\n# 或者: yarn test:cov\n```\n\n关于不同测试层级的更多信息，请参阅 [测试策略文档](./docs/testing-strategy.md)。\n\n## 开发与贡献\n\n### 代码风格\n\n项目使用 ESLint 和 Prettier 强制代码风格，详细规范请参阅 [编码规范文档](./docs/coding-standards.md)。\n\n请在提交代码前运行以下命令：\n\n```bash\n# 检查代码风格\nnpm run lint\n\n# 自动格式化代码\nnpm run format\n```\n\n### 分支策略\n\n- `master` 分支用于发布稳定版本\n- 开发新功能或修复 Bug 时，从 `master` 创建特性分支（例如 `feature/add-png-support` 或 `fix/handle-exiftool-error`）\n- 完成后提交 Pull Request (PR) 到 `master` 分支\n\n### Pull Request (PR) 流程\n\n- PR 应包含清晰的描述，说明变更内容和原因\n- PR 需要通过所有 CI 检查 (Linting, Tests, Build)\n- 鼓励进行代码审查\n\n### 报告问题\n\n如需报告 Bug 或提出功能建议，请在项目的 Issue Tracker 中创建新的 Issue，并提供尽可能详细的信息。\n\n### 持续集成与自动发布\n\n本项目使用GitHub Actions进行持续集成和自动发布：\n\n- 每次Push和PR会自动运行测试和构建\n- 创建Release后会自动发布到NPM\n\n提交PR时请确保通过所有CI检查。如需发布新版本：\n\n1. 更新`package.json`中的版本号\n2. 创建一个新的GitHub Release\n3. GitHub Actions会自动将包发布到NPM\n\n## 文档链接\n\n- [架构文档](./docs/architecture.md)\n- [技术栈](./docs/tech-stack.md)\n- [项目结构](./docs/project-structure.md)\n- [MCP Tool 定义](./docs/mcp-tools-definition.md)\n- [测试策略](./docs/testing-strategy.md)\n- [编码规范](./docs/coding-standards.md)\n- [元数据字段映射](./docs/metadata-field-mapping.md)\n- [手动 Spotlight 测试计划](./docs/manual-spotlight-tests.md)\n\n## 许可证\n\n本项目采用 MIT 许可证。详情请参阅项目根目录下的 LICENSE 文件。 \n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "meta_tag_genie",
        "metadata",
        "spotlight",
        "image metadata",
        "macos spotlight",
        "meta_tag_genie manage"
      ],
      "category": "web-search"
    },
    "the0807--GeekNews-MCP-Server": {
      "owner": "the0807",
      "name": "GeekNews-MCP-Server",
      "url": "https://github.com/the0807/GeekNews-MCP-Server",
      "imageUrl": "/freedevtools/mcp/pfp/the0807.webp",
      "description": "Fetch articles from GeekNews using web scraping capabilities, providing access to the latest news stories along with various metadata like titles, URLs, points, authors, and comments. The server caches data to optimize performance and includes tools for retrieving both current articles and weekly summaries.",
      "stars": 16,
      "forks": 5,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-01T05:44:01Z",
      "readme_content": "# GeekNews MCP Server\n\n[![smithery badge](https://smithery.ai/badge/@the0807/geeknews-mcp-server)](https://smithery.ai/server/@the0807/geeknews-mcp-server)\n\n이 프로젝트는 [GeekNews](https://news.hada.io)에서 아티클을 가져오는 Model Context Protocol(MCP) 서버입니다. Python으로 구현되었으며, BeautifulSoup을 사용하여 웹 스크래핑을 수행합니다. 서버 부하를 줄이기 위해 하루에 한 번 자동으로 데이터를 파싱하고 캐시에 저장하는 기능을 제공합니다.\n\n## 기능\n|    |    |\n|:-------------------------:|:-------------------------:|\n|||\n\n### 도구 (Tools)\n\n- `get_articles` 도구: GeekNews에서 아티클을 가져오는 기능\n  - 아티클 유형(top, new, ask, show)과 반환할 아티클 수를 지정할 수 있음\n  - 각 응답에는 제목, URL, 포인트, 작성자, 시간, 댓글 수, 순위 정보가 포함됨\n  - 캐시된 데이터를 사용하여 서버 부하 감소\n\n- `get_weekly_news` 도구: GeekNews에서 주간 뉴스를 가져오는 기능\n  - 특정 주간 뉴스 ID를 지정하거나 가장 최근 주간 뉴스를 가져올 수 있음\n  - 주간 뉴스의 제목, 번호, ID, 내용, URL, 아이템 목록 등의 정보를 제공\n  - 각 아이템에는 제목, URL, 순위 정보가 포함됨\n  - 캐시된 데이터를 사용하여 서버 부하 감소\n\n### 캐시 기능\n\n- 하루에 한 번 자동으로 데이터를 파싱하고 캐시에 저장\n- 캐시된 데이터가 유효한 경우 GeekNews 서버에 요청하지 않고 캐시된 데이터 사용\n- 캐시 데이터는 24시간 동안 유효하며, 이후 자동으로 갱신\n- 스케줄러가 주기적으로 캐시 유효성을 검사하고 필요시 갱신\n\n## 사용법\n\n- Smithery를 이용한 설치\n\n   🚀 [geeknews-mcp-server](https://smithery.ai/server/@the0807/geeknews-mcp-server)\n\n- MCP 설정 파일에 서버 정보를 추가\n\n   ```json\n   {\n   \"mcpServers\": {\n      \"geeknews-mcp-server\": {\n         \"command\": \"npx\",\n         \"args\": [\n         \"-y\",\n         \"@smithery/cli@latest\",\n         \"run\",\n         \"@the0807/geeknews-mcp-server\",\n         \"--key\",\n         \"smithery에서 발급 받은 키\"\n         ]\n      }\n   }\n   }\n   ```\n\n## 로컬 설치 방법\n\n1. Git Clone\n\n   ```bash\n   git clone https://github.com/the0807/GeekNews-MCP-Server\n   cd GeekNews-MCP-Server\n   ```\n\n2. uv로 환경 세팅\n\n   ```bash\n   uv sync\n   ```\n\n3. 가상환경 실행\n\n   ```bash\n   uv venv\n   source .venv/bin/activate\n   ```\n\n4. MCP Inspector로 서버 테스트\n\n   ```bash\n   uv run mcp\n   mcp dev main.py\n\n   # 터미널에 나오는 URL(MCP Inspector)로 접속하여 서버 테스트\n   ```\n\n## 코드 구조\n\n- `src/models.py`: 아티클 정보를 저장하는 데이터 클래스 정의\n- `src/parser.py`: GeekNews 웹사이트의 HTML을 파싱하여 아티클 정보를 추출\n- `src/client.py`: GeekNews 웹사이트에서 데이터를 가져오는 HTTP 클라이언트\n- `src/config.py`: 설정과 상수 정의\n- `src/cache.py`: 캐시 관리 기능 제공\n- `src/scheduler.py`: 주기적인 데이터 갱신 스케줄러\n- `src/server.py`: MCP 서버 구현\n- `main.py`: 서버 실행 진입점\n\n> [!Note]\n> - 이 서버는 GeekNews 웹사이트의 HTML 구조에 의존합니다. 웹사이트 구조가 변경되면 파싱 로직을 업데이트해야 할 수 있습니다.\n> - 캐시 데이터는 사용자의 홈 디렉토리 아래 `.cache/geeknews-mcp` 폴더에 저장됩니다.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "geeknews",
        "scraping",
        "articles",
        "geeknews using",
        "articles geeknews",
        "geeknews mcp"
      ],
      "category": "web-search"
    },
    "theclarityproject--mcp-server": {
      "owner": "theclarityproject",
      "name": "mcp-server",
      "url": "https://github.com/theclarityproject/mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/theclarityproject.webp",
      "description": "Transform browser traffic into Model Context Protocols, enabling real-time data access for AI by extracting web API functionalities from HAR files. Capture network requests and convert them into tools for AI assistants like Claude.",
      "stars": 1,
      "forks": 0,
      "license": "No License",
      "language": "JavaScript",
      "updated_at": "2025-09-19T23:32:31Z",
      "readme_content": "# Clarity MCP Bootleg Market\n\nTurn browser traffic into tool calls. Smuggle your network requests into an MCP.\n\n## What's This All About?\n\nClarity is where you come when you need to get your hands on some Model Context Protocols (MCPs). Our server connects AI assistants like Claude with web APIs extracted from your HAR files. Think of it as a way to \"borrow\" functionality from websites and turn it into something AI can use.\n\n## How It Works\n\n```mermaid\nsequenceDiagram\n    participant Client as MCP Client (Claude)\n    participant Server as Clarity Server\n    participant Backend as Clarity Backend\n    participant API as Target Web APIs\n\n    note over Client,API: HAR File Processing\n    Backend->>Backend: Process HAR file\n    Backend->>Backend: Extract API endpoints\n    Backend->>Backend: Generate tools with AI\n    Backend->>Backend: Store in database\n\n    note over Client,API: Runtime Flow\n    Client->>Server: Connect via stdio\n    Client->>Server: ListTools request\n    Server->>Backend: Forward request\n    Backend->>Server: Return available tools\n    Server->>Client: Deliver tools to client\n\n    Client->>Server: CallTool request\n    Server->>Server: Transform tool name\n    Server->>Backend: Forward to backend\n    Backend->>API: Execute API call\n    API->>Backend: Return response\n    Backend->>Server: Return result\n    Server->>Client: Deliver to client\n```\n\n## Installation\n\n```bash\n# Install the package\nnpm install -g @lekt9/clarity-mcp\n\n# Set up your credentials\nexport CLARITY_API_KEY=your_api_key\n# Optional: Specify a specific MCP ID if you have multiple MCPs\nexport CLARITY_MCP_ID=your_mcp_id\n\n# Run the server\nclarity-mcp\n```\n\n## Creating MCPs\n\nTo create your own Model Context Protocols from HAR files:\n\n1. Visit [https://www.theclarityproject.net](https://www.theclarityproject.net)\n2. Upload your HAR file\n3. The platform will process the file and generate the necessary tools\n4. Obtain your API key from the website\n\n### The Underground MCP Market\n\n#### Premium Agent Tools\n\nMCPs are the secret sauce that AI agents crave. These Model Context Protocols are powerful tools that agents use to interact with external services and fetch real-time data. Without them, AI is trapped in its own bubble.\n\n#### APIs in Designer Clothes\n\nLet's be real - MCPs are just APIs with fancy clothes on. They follow OpenAPI specs like any other API, but they're packaged nicely for AI consumption. It's the same product, different wrapping - pure marketing genius.\n\n#### Reverse Engineering the Good Stuff\n\nEvery website out there is constantly sending network requests. With the right tools, you can capture these requests, reverse engineer them, and create your own bootleg copy of their API. It's like photocopying someone else's premium content.\n\n#### From Traffic to Premium MCP\n\nOur system takes your ordinary HAR files and transforms them into high-quality MCPs that any AI agent would pay top dollar for. We analyze the endpoints, parameters, and responses to create a perfect replica that tools can consume.\n\n> **Tip**: When an AI agent uses a tool, it's making an API call through an MCP. By creating these protocols, you can extend AI capabilities to interact with virtually any digital service.\n\n### Smuggling Guide\n\nA HAR (HTTP Archive) file captures all the network requests your browser makes while interacting with a website. Follow these steps to create one:\n\n#### Step 1: Open Developer Tools\n\nRight-click anywhere on the webpage and select \"Inspect\" or \"Inspect Element\". Alternatively, use these keyboard shortcuts:\n\n- Chrome/Edge/Firefox (Windows): F12 or Ctrl+Shift+I\n- Chrome/Edge/Firefox (Mac): Option+Cmd+I\n- Safari (Mac): Enable Develop menu first (Safari > Preferences > Advanced > Show Develop menu), then Option+Cmd+I\n\n#### Step 2: Record Network Activity\n\nIn the Developer Tools panel, navigate to the \"Network\" tab.\n\n- Ensure recording is active (usually a red circle icon).\n- Check the \"Preserve log\" option to keep requests across page loads.\n- Optional: Clear existing logs using the clear icon for a cleaner capture.\n- Perform the actions on the website that trigger the API calls you want to capture (e.g., logging in, loading data, submitting a form).\n\nInteract with the site thoroughly to ensure all desired requests are recorded.\n\n#### Step 3: Export HAR File\n\nOnce you've captured the necessary activity, right-click anywhere in the list of network requests.\n\n- Select \"Save all as HAR with content\" (or similar wording like \"Export HAR...\").\n- Choose a location to save the `.har` file.\n\nYou can now upload this HAR file using the uploader on the Clarity website.\n\n> **Important**: HAR files can contain sensitive information like cookies, authentication tokens, and personal data. Handle them securely and avoid sharing them publicly.\n\n## Usage with Claude\n\n1. Install the Claude Desktop application\n2. Open Claude settings and navigate to the MCP section\n3. Add a new MCP server with this command:\n   ```\n   npx -y @smithery/cli@latest install @lekt9/clarity-mcp --claude\n   ```\n4. Start a conversation with Claude and use the tools provided by your Clarity MCP\n\n## Environment Variables\n\n- `CLARITY_API_KEY`: API key for authenticating with the Clarity backend\n- `CLARITY_MCP_ID`: Optional MCP ID to specify which MCP to use (useful if you have multiple MCPs)\n- `NEXTJS_APP_URL`: URL of the Clarity backend (defaults to https://clarity.org/)\n\n## Technical Details\n\n### Architecture\n\nThe server follows a simple proxy architecture:\n\n1. **MCP Protocol Handling**: Implements the MCP protocol for client communication\n2. **Request Forwarding**: Forwards client requests to the Clarity backend\n3. **Response Transformation**: Transforms responses to match MCP protocol requirements\n\n### Key Components\n\n- **Server Class**: Main server implementation using the MCP SDK\n- **Request Handlers**: Handlers for ListTools and CallTool requests\n- **Name Transformation**: Functions to convert between naming conventions\n- **Error Handling**: Proper error propagation and handling\n\n## Development\n\nTo run the server in development mode:\n\n```bash\n# Clone the repository\ngit clone https://github.com/lekt9/clarity-mcp.git\ncd clarity-mcp\n\n# Install dependencies\nnpm install\n\n# Build the server\nnpm run build\n```\n\n## Limitations\n\n- The server requires a valid API key to make authenticated requests to the backend\n- Tool execution depends on the availability of the Clarity backend\n- The server does not cache tool definitions or results\n\n## License\n\nMIT\n",
      "npm_url": "https://www.npmjs.com/package/mcp-server",
      "npm_downloads": 0,
      "keywords": [
        "ai",
        "api",
        "web",
        "access ai",
        "ai assistants",
        "ai extracting"
      ],
      "category": "web-search"
    },
    "thedaviddias--mcp-llms-txt-explorer": {
      "owner": "thedaviddias",
      "name": "mcp-llms-txt-explorer",
      "url": "https://github.com/thedaviddias/mcp-llms-txt-explorer",
      "imageUrl": "/freedevtools/mcp/pfp/thedaviddias.webp",
      "description": "Explore and analyze websites implementing the llms.txt standard, validating and parsing llms.txt files for structured insights about compliant websites.",
      "stars": 70,
      "forks": 14,
      "license": "Other",
      "language": "JavaScript",
      "updated_at": "2025-09-26T15:46:08Z",
      "readme_content": "# MCP LLMS.txt Explorer\n\n<a href=\"https://glama.ai/mcp/servers/lhyj3pva0z\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/lhyj3pva0z/badge\" alt=\"LLMS.txt Explorer MCP server\" />\n</a>\n\n[![smithery badge](https://smithery.ai/badge/@thedaviddias/mcp-llms-txt-explorer)](https://smithery.ai/server/@thedaviddias/mcp-llms-txt-explorer)\n\nA Model Context Protocol server for exploring websites with llms.txt files. This server helps you discover and analyze websites that implement the llms.txt standard.\n\n## Features\n\n### Resources\n- Check websites for llms.txt and llms-full.txt files\n- Parse and validate llms.txt file contents\n- Access structured data about compliant websites\n\n### Tools\n- `check_website` - Check if a website has llms.txt files\n  - Takes domain URL as input\n  - Returns file locations and validation status\n- `list_websites` - List known websites with llms.txt files\n  - Returns structured data about compliant websites\n  - Supports filtering by file type (llms.txt/llms-full.txt)\n\n## Development\n\nInstall dependencies:\n```bash\npnpm install\n```\n\nBuild the server:\n```bash\npnpm run build\n```\n\nFor development with auto-rebuild:\n```bash\npnpm run watch\n```\n\n## Installation\n\n### Installing via Smithery\n\nTo install mcp-llms-txt-explorer for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@thedaviddias/mcp-llms-txt-explorer):\n\n```bash\nnpx -y @smithery/cli install @thedaviddias/mcp-llms-txt-explorer --client claude\n```\n\n### Installing Manually\nTo use this server:\n\n```bash\n# Clone the repository\ngit clone https://github.com/thedaviddias/mcp-llms-txt-explorer.git\ncd mcp-llms-txt-explorer\n\n# Install dependencies\npnpm install\n\n# Build the server\npnpm run build\n```\n\n### Configuration with Claude Desktop\n\nTo use with Claude Desktop, add the server config:\n\nOn MacOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\nOn Windows: `%APPDATA%/Claude/claude_desktop_config.json`\n\n```json\n{\n  \"mcpServers\": {\n    \"llms-txt-explorer\": {\n      \"command\": \"node\",\n      \"args\": [\"/path/to/llms-txt-explorer/build/index.js\"],\n    }\n  }\n}\n```\n\nFor npx usage, you can use:\n```json\n{\n  \"mcpServers\": {\n    \"llms-txt-explorer\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@thedaviddias/mcp-llms-txt-explorer\"]\n    }\n  }\n}\n```\n\n### Debugging\n\nSince MCP servers communicate over stdio, debugging can be challenging. We recommend using the [MCP Inspector](https://github.com/modelcontextprotocol/inspector), which is available as a package script:\n\n```bash\npnpm run inspector\n```\n\nThe Inspector will provide a URL to access debugging tools in your browser.\n\n## License\n\nThis project is licensed under the MIT License—see the LICENSE file for details.",
      "npm_url": "https://www.npmjs.com/package/@thedaviddias/mcp-llms-txt-explorer",
      "npm_downloads": 3178,
      "keywords": [
        "txt",
        "llms",
        "web",
        "llms txt",
        "parsing llms",
        "compliant websites"
      ],
      "category": "web-search"
    },
    "thomasvan--mcp-brave-search": {
      "owner": "thomasvan",
      "name": "mcp-brave-search",
      "url": "https://github.com/thomasvan/mcp-brave-search",
      "imageUrl": "/freedevtools/mcp/pfp/thomasvan.webp",
      "description": "Utilizes Brave Search to perform web searches and retrieve local business information, facilitating seamless integration of search capabilities into MCP-compatible AI workflows.",
      "stars": 1,
      "forks": 0,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-05T03:32:48Z",
      "readme_content": "# Brave Search MCP Server\n\n[![smithery badge](https://smithery.ai/badge/@thomasvan/mcp-brave-search)](https://smithery.ai/server/@thomasvan/mcp-brave-search)\n\nThis project implements a Model Context Protocol (MCP) server for Brave Search, allowing integration with AI assistants like Claude.\n\n## Prerequisites\n\n- Python 3.11+\n- [uv](https://github.com/astral-sh/uv) - A fast Python package installer and resolver\n\n## Installation\n\n### Installing via Smithery\n\nTo install Brave Search MCP server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@thomasvan/mcp-brave-search):\n\n```bash\nnpx -y @smithery/cli install @thomasvan/mcp-brave-search --client claude\n```\n\n### Manual Installation\n1. Clone the repository:\n   ```\n   git clone https://github.com/thomasvan/mcp-brave-search.git\n   cd mcp-brave-search\n   ```\n\n2. Create a virtual environment and install dependencies using uv:\n   ```\n   uv venv\n   source .venv/bin/activate  # On Windows, use: .venv\\Scripts\\activate\n   uv pip install -r requirements.txt\n   ```\n\n3. Set up your Brave Search API key:\n   ```\n   export BRAVE_API_KEY=your_api_key_here\n   ```\n   On Windows, use: `set BRAVE_API_KEY=your_api_key_here`\n\n## Usage\n\n1. Configure your MCP settings file (e.g., `claude_desktop_config.json`) to include the Brave Search MCP server:\n\n   ```json\n   {\n     \"mcpServers\": {\n       \"brave-search\": {\n         \"command\": \"uv\",\n         \"args\": [\n           \"--directory\",\n           \"path-to\\\\mcp-python\\\\mcp-brave-search\\\\src\",\n           \"run\",\n           \"server.py\"\n         ],\n         \"env\": {\n           \"BRAVE_API_KEY\": \"YOUR_BRAVE_API_KEY_HERE\"\n         }\n       }\n     }\n   }\n   ```\n\n   Replace `YOUR_BRAVE_API_KEY_HERE` with your actual Brave API key.\n\n2. Start the Brave Search MCP server by running your MCP-compatible AI assistant with the updated configuration.\n\n3. The server will now be running and ready to accept requests from MCP clients.\n\n4. You can now use the Brave Search functionality in your MCP-compatible AI assistant (like Claude) by invoking the available tools.\n\n## Available Tools\n\nThe server provides two main tools:\n\n1. `brave_web_search`: Performs a web search using the Brave Search API.\n2. `brave_local_search`: Searches for local businesses and places.\n\nRefer to the tool docstrings in `src/server.py` for detailed usage information.\n\n## Development\n\nTo make changes to the project:\n\n1. Modify the code in the `src` directory as needed.\n2. Update the `requirements.txt` file if you add or remove dependencies:\n   ```\n   uv pip freeze > requirements.txt\n   ```\n3. Restart the server to apply changes.\n\n## Testing\n\nThe project includes both unit tests and integration tests:\n\n### Installing Test Dependencies\n\n```bash\nuv pip install pytest pytest-asyncio pytest-cov\n```\n\n### Running Unit Tests\n\nUnit tests can be run without an API key and use mocks to simulate API responses:\n\n```bash\n# Run all unit tests\npython -m pytest tests/unit/\n\n# Run with verbose output\npython -m pytest tests/unit/ -v\n```\n\n### Running Integration Tests\n\nIntegration tests require a valid Brave API key and make real API calls:\n\n```bash\n# Run integration tests with your API key\nBRAVE_API_KEY_INTEGRATION=\"your_api_key_here\" python -m pytest tests/integration/ -v\n```\n\n### Test Coverage\n\nTo check test coverage:\n\n```bash\npython -m pytest --cov=src/mcp_brave_search\n```\n\n## Troubleshooting\n\nIf you encounter any issues:\n\n1. Ensure your Brave API key is correctly set.\n2. Check that all dependencies are installed.\n3. Verify that you're using a compatible Python version.\n4. If you make changes to the code, make sure to restart the server.\n\n## License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "searches",
        "search",
        "mcp",
        "brave search",
        "search utilizes",
        "web search"
      ],
      "category": "web-search"
    },
    "tinyfish-io--agentql-mcp": {
      "owner": "tinyfish-io",
      "name": "agentql-mcp",
      "url": "https://github.com/tinyfish-io/agentql-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/tinyfish-io.webp",
      "description": "Extract structured data from web pages using specified prompts to identify the desired fields from a given URL. This server enables integration of real-time data insights into applications.",
      "stars": 114,
      "forks": 27,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-10-01T11:02:29Z",
      "readme_content": "# AgentQL MCP Server\n\nThis is a Model Context Protocol (MCP) server that integrates [AgentQL](https://agentql.com)'s data extraction capabilities.\n\n## Features\n\n### Tools\n\n- `extract-web-data` - extract structured data from a given 'url', using 'prompt' as a description of actual data and its fields to extract.\n\n## Installation\n\nTo use AgentQL MCP Server to extract data from web pages, you need to install it via npm, get an API key from our [Dev Portal](https://dev.agentql.com), and configure it in your favorite app that supports MCP.\n\n### Install the package\n\n```bash\nnpm install -g agentql-mcp\n```\n\n### Configure Claude\n\n- Open Claude Desktop **Settings** via `⌘`+`,` (don't confuse with Claude Account Settings)\n- Go to **Developer** sidebar section\n- Click **Edit Config** and open `claude_desktop_config.json` file\n- Add `agentql` server inside `mcpServers` dictionary in the config file\n- Restart the app\n\n```json title=\"claude_desktop_config.json\"\n{\n  \"mcpServers\": {\n    \"agentql\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"agentql-mcp\"],\n      \"env\": {\n        \"AGENTQL_API_KEY\": \"YOUR_API_KEY\"\n      }\n    }\n  }\n}\n```\n\nRead more about MCP configuration in Claude [here](https://modelcontextprotocol.io/quickstart/user).\n\n### Configure VS Code\n\nFor one-click installation, click one of the install buttons below:\n\n[![Install with NPX in VS Code](https://img.shields.io/badge/VS_Code-NPM-0098FF?style=flat-square&logo=visualstudiocode&logoColor=white)](https://insiders.vscode.dev/redirect/mcp/install?name=agentql&config=%7B%22command%22%3A%22npx%22%2C%22args%22%3A%5B%22-y%22%2C%22agentql-mcp%22%5D%2C%22env%22%3A%7B%22AGENTQL_API_KEY%22%3A%22%24%7Binput%3AapiKey%7D%22%7D%7D&inputs=%5B%7B%22type%22%3A%22promptString%22%2C%22id%22%3A%22apiKey%22%2C%22description%22%3A%22AgentQL+API+Key%22%2C%22password%22%3Atrue%7D%5D) [![Install with NPX in VS Code Insiders](https://img.shields.io/badge/VS_Code_Insiders-NPM-24bfa5?style=flat-square&logo=visualstudiocode&logoColor=white)](https://insiders.vscode.dev/redirect/mcp/install?name=agentql&config=%7B%22command%22%3A%22npx%22%2C%22args%22%3A%5B%22-y%22%2C%22agentql-mcp%22%5D%2C%22env%22%3A%7B%22AGENTQL_API_KEY%22%3A%22%24%7Binput%3AapiKey%7D%22%7D%7D&inputs=%5B%7B%22type%22%3A%22promptString%22%2C%22id%22%3A%22apiKey%22%2C%22description%22%3A%22AgentQL+API+Key%22%2C%22password%22%3Atrue%7D%5D&quality=insiders)\n\n#### Manual Installation\n\nClick the install buttons at the top of this section for the quickest installation method. For manual installation, follow these steps:\n\nAdd the following JSON block to your User Settings (JSON) file in VS Code. You can do this by pressing `Ctrl + Shift + P` and typing `Preferences: Open User Settings (JSON)`.\n\n```json\n{\n  \"mcp\": {\n    \"inputs\": [\n      {\n        \"type\": \"promptString\",\n        \"id\": \"apiKey\",\n        \"description\": \"AgentQL API Key\",\n        \"password\": true\n      }\n    ],\n    \"servers\": {\n      \"agentql\": {\n        \"command\": \"npx\",\n        \"args\": [\"-y\", \"agentql-mcp\"],\n        \"env\": {\n          \"AGENTQL_API_KEY\": \"${input:apiKey}\"\n        }\n      }\n    }\n  }\n}\n```\n\nOptionally, you can add it to a file called `.vscode/mcp.json` in your workspace. This will allow you to share the configuration with others.\n\n```json\n{\n  \"inputs\": [\n    {\n      \"type\": \"promptString\",\n      \"id\": \"apiKey\",\n      \"description\": \"AgentQL API Key\",\n      \"password\": true\n    }\n  ],\n  \"servers\": {\n    \"agentql\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"agentql-mcp\"],\n      \"env\": {\n        \"AGENTQL_API_KEY\": \"${input:apiKey}\"\n      }\n    }\n  }\n}\n```\n\n### Configure Cursor\n\n- Open **Cursor Settings**\n- Go to **MCP > MCP Servers**\n- Click **+ Add new MCP Server**\n- Enter the following:\n  - Name: \"agentql\" (or your preferred name)\n  - Type: \"command\"\n  - Command: `env AGENTQL_API_KEY=YOUR_API_KEY npx -y agentql-mcp`\n\nRead more about MCP configuration in Cursor [here](https://docs.cursor.com/context/model-context-protocol).\n\n### Configure Windsurf\n\n- Open **Windsurf: MCP Configuration Panel**\n- Click **Add custom server+**\n- Alternatively you can open `~/.codeium/windsurf/mcp_config.json` directly\n- Add `agentql` server inside `mcpServers` dictionary in the config file\n\n```json title=\"mcp_config.json\"\n{\n  \"mcpServers\": {\n    \"agentql\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"agentql-mcp\"],\n      \"env\": {\n        \"AGENTQL_API_KEY\": \"YOUR_API_KEY\"\n      }\n    }\n  }\n}\n```\n\nRead more about MCP configuration in Windsurf [here](https://docs.codeium.com/windsurf/mcp).\n\n### Validate MCP integration\n\nGive your agent a task that will require extracting data from the web. For example:\n\n```text\nExtract the list of videos from the page https://www.youtube.com/results?search_query=agentql, every video should have a title, an author name, a number of views and a url to the video. Make sure to exclude ads items. Format this as a markdown table.\n```\n\n> [!TIP]\n> In case your agent complains that it can't open urls or load content from the web instead of using AgentQL, try adding \"use tools\" or \"use agentql tool\" hint.\n\n## Development\n\nInstall dependencies:\n\n```bash\nnpm install\n```\n\nBuild the server:\n\n```bash\nnpm run build\n```\n\nFor development with auto-rebuild:\n\n```bash\nnpm run watch\n```\n\nIf you want to try out development version, you can use the following config instead of the default one:\n\n```json\n{\n  \"mcpServers\": {\n    \"agentql\": {\n      \"command\": \"/path/to/agentql-mcp/dist/index.js\",\n      \"env\": {\n        \"AGENTQL_API_KEY\": \"YOUR_API_KEY\"\n      }\n    }\n  }\n}\n```\n\n> [!NOTE]\n> Don't forget to remove the default AgentQL MCP server config to not confuse Claude with two similar servers.\n\n## Debugging\n\nSince MCP servers communicate over stdio, debugging can be challenging. We recommend using the [MCP Inspector](https://github.com/modelcontextprotocol/inspector), which is available as a package script:\n\n```bash\nnpm run inspector\n```\n\nThe Inspector will provide a URL to access debugging tools in your browser.\n",
      "npm_url": "https://www.npmjs.com/package/agentql-mcp",
      "npm_downloads": 7466,
      "keywords": [
        "agentql",
        "tinyfish",
        "io",
        "io agentql",
        "search tinyfish",
        "data web"
      ],
      "category": "web-search"
    },
    "tisDDM--searxng-mcp": {
      "owner": "tisDDM",
      "name": "searxng-mcp",
      "url": "https://github.com/tisDDM/searxng-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/tisDDM.webp",
      "description": "Connect AI assistants to SearXNG, a privacy-focused metasearch engine that supports both public and private instances. Customize search parameters for flexible web searching with no initial configuration required.",
      "stars": 22,
      "forks": 6,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-09-24T16:50:54Z",
      "readme_content": "# SearXNG MCP Server\n\n<p align=\"center\">\n  \n</p>\n\nA Model Context Protocol (MCP) server that enables AI assistants to perform web searches using [SearXNG](https://github.com/searxng/searxng), a privacy-respecting metasearch engine. Works out-of-the-box with zero additional deployment by automatically selecting a random instance from [SearX.space](https://searx.space/), while also supporting private instances with basic authentication.\n\n## Features\n\n- **Zero-configuration setup**: Works immediately by using a random public instance from [SearX.space](https://searx.space/)\n- **Private instance support**: Connect to your own SearXNG instance with optional basic authentication\n- Perform web searches with customizable parameters\n- Support for multiple search engines\n- Privacy-focused search results\n- Markdown-formatted search results\n- Sensible default values for all parameters\n\n**CAVEAT - Public Instances might be unavailabe for this purpose and return \"Request failed with status code 429\"**\n\n## Installation\n\n### Prerequisites\n\n- Node.js (v16 or higher)\n- npm (v7 or higher)\n- Access to a SearXNG instance (self-hosted or public)\n\n\n### Install from source\n\n```bash\n# Clone the repository\ngit clone https://github.com/tisDDM/searxng-mcp.git\ncd searxng-mcp\n\n# Install dependencies\nnpm install\n\n# Build the project\nnpm run build\n```\n\n## Configuration\n\nThe SearXNG MCP server can be configured with the following environment variables:\n\n- `SEARXNG_URL` (optional): The URL of your SearXNG instance (e.g., `https://searx.example.com`). If not provided, a random public instance from [SearX.space](https://searx.space/) will be automatically selected, making the server usable with zero additional deployment.\n- `USE_RANDOM_INSTANCE` (optional): Set to \"false\" to disable random instance selection when no URL is provided. Default is \"true\".\n- `SEARXNG_USERNAME` (optional): Username for basic authentication when connecting to a private instance\n- `SEARXNG_PASSWORD` (optional): Password for basic authentication when connecting to a private instance\n\nYou can set these environment variables in a `.env` file in the root directory of the project:\n\n```\nSEARXNG_URL=https://searx.example.com\nSEARXNG_USERNAME=your_username\nSEARXNG_PASSWORD=your_password\n```\n\n## Usage\n\n### Running the server\n\n```bash\n# If installed globally\nsearxngmcp\n\n# If installed from source\nnode build/index.js\n```\n\n### Integrating with Claude Desktop\n\n1. Open Claude Desktop\n2. Go to Settings > MCP Servers\n3. Add a new MCP server with the following configuration:\n   ```json\n   {\n     \"mcpServers\": {\n       \"searxngmcp\": {\n         \"command\": \"searxngmcp\",\n         \"env\": {\n           // Optional: If not provided, a random public instance will be used\n           \"SEARXNG_URL\": \"https://searx.example.com\",\n           // Optional: Only needed for private instances with authentication\n           \"SEARXNG_USERNAME\": \"your_username\",\n           \"SEARXNG_PASSWORD\": \"your_password\"\n         },\n         \"disabled\": false,\n         \"autoApprove\": []\n       }\n     }\n   }\n   ```\n\n### Integrating with Claude in VSCode\n\n1. Open VSCode\n2. Go to Settings > Extensions > Claude > MCP Settings\n3. Add a new MCP server with the following configuration:\n   ```json\n   {\n     \"mcpServers\": {\n       \"searxngmcp\": {\n         \"command\": \"node\",\n         \"args\": [\"/path/to/searxng-mcp/build/index.js\"],\n         \"env\": {\n           // Optional: If not provided, a random public instance will be used\n           \"SEARXNG_URL\": \"https://searx.example.com\",\n           // Optional: Only needed for private instances with authentication\n           \"SEARXNG_USERNAME\": \"your_username\",\n           \"SEARXNG_PASSWORD\": \"your_password\"\n         },\n         \"disabled\": false,\n         \"autoApprove\": []\n       }\n     }\n   }\n   ```\n\n## Usage with Smolagents\n\nSearXNG MCP can be easily integrated with Smolagents, a lightweight framework for building AI agents. This allows you to create powerful research agents that can search the web and process the results:\n\n```python\nfrom smolagents import CodeAgent, LiteLLMModel, ToolCollection\nfrom mcp import StdioServerParameters\n\n# Configure the SearXNG MCP server\nserver_parameters = StdioServerParameters(\n    command=\"node\",\n    args=[\"path/to/searxng-mcp/build/index.js\"],\n    env={\n        \"SEARXNG_URL\": \"https://your-searxng-instance.com\",\n        \"SEARXNG_USERNAME\": \"your_username\",  # Optional\n        \"SEARXNG_PASSWORD\": \"your_password\"   # Optional\n    }\n)\n\n# Create a tool collection from the MCP server\nwith ToolCollection.from_mcp(server_parameters) as tool_collection:\n    # Initialize your LLM model\n    model = LiteLLMModel(\n        model_id=\"your-model-id\",\n        api_key=\"your-api-key\",\n        temperature=0.7\n    )\n    \n    # Create an agent with the search tools\n    search_agent = CodeAgent(\n        name=\"search_agent\",\n        tools=tool_collection.tools,\n        model=model\n    )\n    \n    # Run the agent with a search prompt\n    result = search_agent.run(\n        \"Perform a search about: 'climate change solutions' and summarize the top 5 results.\"\n    )\n    \n    print(result)\n```\n\n## Available Tools\n\n### searxngsearch\n\nPerform web searches using SearXNG, a privacy-respecting metasearch engine. Returns relevant web content with customizable parameters.\n\n#### Parameters\n\n| Parameter   | Type             | Description                                                                      | Default     | Required |\n|-------------|------------------|----------------------------------------------------------------------------------|-------------|---------|\n| query       | string           | Search query                                                                     | -           | Yes      |\n| language    | string           | Language code for search results (e.g., 'en', 'de', 'fr')                        | 'en'        | No       |\n| time_range  | string           | Time range for search results. Options: 'day', 'week', 'month', 'year'           | null        | No       |\n| categories  | array of strings | Categories to search in (e.g., 'general', 'images', 'news')                      | null        | No       |\n| engines     | array of strings | Specific search engines to use                                                   | null        | No       |\n| safesearch  | number           | Safe search level: 0 (off), 1 (moderate), 2 (strict)                             | 1           | No       |\n| pageno      | number           | Page number for results. Must be minimum 1                                       | 1           | No       |\n| max_results | number           | Maximum number of search results to return. Range: 1-50                          | 10          | No       |\n\n#### Example\n\n```javascript\n// Example request\nconst result = await client.callTool('searxngsearch', {\n  query: 'climate change solutions',\n  language: 'en',\n  time_range: 'year',\n  categories: ['general', 'news'],\n  safesearch: 1,\n  max_results: 5\n});\n```\n\n## Development\n\n### Setup\n\n```bash\n# Clone the repository\ngit clone https://github.com/tisDDM/searxng-mcp.git\ncd searxng-mcp\n\n# Install dependencies\nnpm install\n```\n\n### Build\n\n```bash\nnpm run build\n```\n\n### Watch mode (for development)\n\n```bash\nnpm run watch\n```\n\n### Testing with MCP Inspector\n\n```bash\nnpm run inspector\n```\n\n## License\n\nMIT",
      "npm_url": "https://www.npmjs.com/package/searxng-mcp",
      "npm_downloads": 264,
      "keywords": [
        "metasearch",
        "searxng",
        "searching",
        "searxng privacy",
        "metasearch engine",
        "tisddm searxng"
      ],
      "category": "web-search"
    },
    "tizee--mcp-server-ietf": {
      "owner": "tizee",
      "name": "mcp-server-ietf",
      "url": "https://github.com/tizee/mcp-server-ietf",
      "imageUrl": "/freedevtools/mcp/pfp/tizee.webp",
      "description": "Access and retrieve IETF RFC documents, enabling search by keywords and management of document pagination. Provides standardized access to essential specifications for Large Language Models.",
      "stars": 8,
      "forks": 3,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-27T16:37:54Z",
      "readme_content": "# MCP-Server-IETF\n\nA Model Context Protocol server for fetching IETF documents (RFCs) for Large Language Models.\n\n## Overview\n\nThis project implements a [Model Context Protocol (MCP)](https://modelcontextprotocol.github.io/) server that provides access to IETF RFC documents. It enables Large Language Models to access RFC specifications through a standardized interface.\n\nKey features:\n- Download and cache RFC index and documents\n- Search RFCs by keyword in titles\n- Access RFC documents with pagination support\n- Extract metadata like page numbers from documents\n\n## Installation\n\n### Requirements\n- Python 3.11 or higher\n- Dependencies as listed in `pyproject.toml`\n\n### Install from source\n\n```bash\n# Clone the repository\ngit clone https://github.com/tizee/mcp-server-ietf\ncd mcp-server-ietf\n\n# Install with pip\npip install -e .\n```\n\n## Usage\n\n### Starting the server\n\n```bash\n# Start the server\nmcp-server-ietf\n```\n\nOr use it with the MCP inspector:\n\n```bash\nnpx @modelcontextprotocol/inspector uv run mcp-server-ietf\n```\n\n### Available Tools\n\nWhen connected to the server, the following tools are available:\n\n#### `list_docs_number`\nGet the total number of RFC documents available in the index.\n\n#### `get_doc`\nGet an RFC document by its number with pagination support.\n\nParameters:\n- `number`: The RFC number (e.g., \"1234\")\n- `start_line`: The line number to start from (default: 1)\n- `max_lines`: Maximum number of lines to return (default: 200)\n\n#### `search_rfc_by_keyword`\nSearch for RFC documents by keyword in their titles.\n\nParameters:\n- `keyword`: The search term to look for in RFC titles\n\n## Development\n\n### Setup Development Environment\n\n```bash\n# Install development dependencies\nuv install -e .[dev]\n```\n\nRun inspector with Makefile:\n\n```\nmake dev\n```\n\n### Running Tests\n\n```bash\n# Run tests\nuv run pytest\n```\n\nOr using the Makefile:\n\n```bash\nmake test\n```\n\n### Cache Location\n\nBy default, the server caches RFC documents and the index at `~/.cache/ietf-doc-server`.\n\n### Environment Variables\n\n- `LOG_LEVEL`: Set the logging level (default: \"DEBUG\")\n\n## License\n\nMIT License - See `LICENSE` file for details.\n\n## Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "rfc",
        "search",
        "pagination",
        "ietf rfc",
        "rfc documents",
        "retrieve ietf"
      ],
      "category": "web-search"
    },
    "tokenizin-agency--mcp-npx-fetch": {
      "owner": "tokenizin-agency",
      "name": "mcp-npx-fetch",
      "url": "https://github.com/tokenizin-agency/mcp-npx-fetch",
      "imageUrl": "/freedevtools/mcp/pfp/tokenizin-agency.webp",
      "description": "Fetches and processes web content, transforming it into multiple formats such as HTML, JSON, Markdown, and plain text. It operates using the Model Context Protocol for seamless integration with AI models.",
      "stars": 37,
      "forks": 9,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-09-10T22:50:35Z",
      "readme_content": "# MCP NPX Fetch\n\n<div align=\"center\">\n\n[![npm version](https://img.shields.io/npm/v/@tokenizin/mcp-npx-fetch.svg)](https://www.npmjs.com/package/@tokenizin/mcp-npx-fetch)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![TypeScript](https://img.shields.io/badge/TypeScript-5.7-blue.svg)](https://www.typescriptlang.org/)\n[![Model Context Protocol](https://img.shields.io/badge/MCP-Compatible-green.svg)](https://github.com/modelcontextprotocol)\n\nA powerful MCP server for fetching and transforming web content into various formats (HTML, JSON, Markdown, Plain Text) with ease.\n\n[Installation](#installation) •\n[Features](#features) •\n[Usage](#usage) •\n[Documentation](#documentation) •\n[Contributing](#contributing)\n\n</div>\n\n<a href=\"https://glama.ai/mcp/servers/m2a0ue08n2\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/m2a0ue08n2/badge\" alt=\"NPX Fetch MCP server\" /></a>\n\n---\n\n## 🚀 Features\n\n- 🌐 **Universal Content Fetching**: Supports HTML, JSON, plain text, and Markdown formats\n- 🔒 **Custom Headers Support**: Add authentication and custom headers to your requests\n- 🛠 **Built-in Transformations**: Automatic conversion between formats\n- ⚡ **High Performance**: Built with modern JavaScript features and optimized for speed\n- 🔌 **MCP Compatible**: Seamlessly integrates with Claude Desktop and other MCP clients\n- 🎯 **Type-Safe**: Written in TypeScript with full type definitions\n\n## 📦 Installation\n\n### NPM Global Installation\n\n```bash\nnpm install -g @tokenizin/mcp-npx-fetch\n\n```\n\n### Direct Usage with NPX\n\n```bash\nnpx @tokenizin/mcp-npx-fetch\n```\n\n## 📚 Documentation\n\n### Available Tools\n\n#### `fetch_html`\n\nFetches and returns raw HTML content from any URL.\n\n```typescript\n{\n  url: string;     // Required: Target URL\n  headers?: {      // Optional: Custom request headers\n    [key: string]: string;\n  };\n}\n```\n\n#### `fetch_json`\n\nFetches and parses JSON data from any URL.\n\n```typescript\n{\n  url: string;     // Required: Target URL\n  headers?: {      // Optional: Custom request headers\n    [key: string]: string;\n  };\n}\n```\n\n#### `fetch_txt`\n\nFetches and returns clean plain text content, removing HTML tags and scripts.\n\n```typescript\n{\n  url: string;     // Required: Target URL\n  headers?: {      // Optional: Custom request headers\n    [key: string]: string;\n  };\n}\n```\n\n#### `fetch_markdown`\n\nFetches content and converts it to well-formatted Markdown.\n\n```typescript\n{\n  url: string;     // Required: Target URL\n  headers?: {      // Optional: Custom request headers\n    [key: string]: string;\n  };\n}\n```\n\n## 🔧 Usage\n\n### CLI Usage\n\nStart the MCP server directly:\n\n```bash\nmcp-npx-fetch\n```\n\nOr via npx:\n\n```bash\nnpx @tokenizin/mcp-npx-fetch\n```\n\n### Claude Desktop Integration\n\n1. Locate your Claude Desktop configuration file:\n\n   - macOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\n   - Windows: `%APPDATA%/Claude/claude_desktop_config.json`\n   - Linux: `~/.config/Claude/claude_desktop_config.json`\n\n2. Add the following configuration to your `mcpServers` object:\n\n```json\n{\n  \"mcpServers\": {\n    \"fetch\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@tokenizin/mcp-npx-fetch\"],\n      \"env\": {}\n    }\n  }\n}\n```\n\n## 💻 Local Development\n\n1. Clone the repository:\n\n```bash\ngit clone https://github.com/tokenizin-agency/mcp-npx-fetch.git\ncd mcp-npx-fetch\n```\n\n2. Install dependencies:\n\n```bash\nnpm install\n```\n\n3. Start development mode:\n\n```bash\nnpm run dev\n```\n\n4. Run tests:\n\n```bash\nnpm test\n```\n\n## 🛠 Technical Stack\n\n- [Model Context Protocol SDK](https://github.com/modelcontextprotocol/sdk) - Core MCP functionality\n- [JSDOM](https://github.com/jsdom/jsdom) - HTML parsing and manipulation\n- [Turndown](https://github.com/mixmark-io/turndown) - HTML to Markdown conversion\n- [TypeScript](https://www.typescriptlang.org/) - Type safety and modern JavaScript features\n- [Zod](https://github.com/colinhacks/zod) - Runtime type validation\n\n## 🤝 Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request. For major changes, please open an issue first to discuss what you would like to change.\n\n1. Fork the repository\n2. Create your feature branch (`git checkout -b feature/AmazingFeature`)\n3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)\n4. Push to the branch (`git push origin feature/AmazingFeature`)\n5. Open a Pull Request\n\n## 📄 License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n\n---\n\n<div align=\"center\">\nMade with ❤️ by <a href=\"https://github.com/tokenizin-agency\">PT Tokenizin Technology Agency</a>\n</div>\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "web",
        "npx",
        "tokenizin",
        "search tokenizin",
        "web search",
        "mcp npx"
      ],
      "category": "web-search"
    },
    "tositon--OpenDeepSearch": {
      "owner": "tositon",
      "name": "OpenDeepSearch",
      "url": "https://github.com/tositon/OpenDeepSearch",
      "imageUrl": "/freedevtools/mcp/pfp/tositon.webp",
      "description": "Performs comprehensive, in-depth research on complex topics by breaking down questions into manageable sub-questions and synthesizing findings into detailed reports with citations. Integrates seamlessly with various MCP clients for enhanced research capabilities.",
      "stars": 3,
      "forks": 1,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-06-13T07:49:23Z",
      "readme_content": "# OpenDeepSearch\n\nAn open-source alternative to Perplexity Deep Research using the Model Context Protocol (MCP).\n\n## Overview\n\nOpenDeepSearch is a powerful research tool that performs comprehensive, in-depth research on complex topics. It combines the structured thinking approach of Sequential Thinking with the search capabilities of Brave Search to provide detailed, well-sourced research reports.\n\n## Features\n\n- **Comprehensive Research**: Breaks down complex questions into manageable sub-questions\n- **Iterative Search**: Performs multiple searches to gather diverse information\n- **Intelligent Analysis**: Analyzes search results to extract relevant information\n- **Synthesis**: Combines findings into a coherent, well-structured report\n- **Citations**: Includes sources for all information in the report\n- **MCP Integration**: Seamlessly integrates with Claude Desktop, Cursor, and other MCP clients\n- **WebSockets**: Supports integration with Smithery and other MCP clients\n- **Publication**: Allows publishing the research tool on the Smithery platform for easy access\n\n## Installation\n\n### Prerequisites\n\n- Node.js 16 or higher\n- A Brave Search API key (get one at [https://brave.com/search/api/](https://brave.com/search/api/))\n\n### NPM Installation\n\n```bash\nnpm install -g open-deep-research\n```\n\n### Running with NPX\n\n```bash\nBRAVE_API_KEY=your_api_key npx open-deep-research\n```\n\n### Local Installation\n\n```bash\n# Clone the repository\ngit clone https://github.com/tositon/open-deep-research.git\ncd open-deep-research\n\n# Install dependencies\nnpm install\n\n# Build the project\nnpm run build\n\n# Run with Brave Search API\nBRAVE_API_KEY=your_api_key npm start\n```\n\n### Installation via Smithery\n\n```bash\n# Install for Claude\nnpx @smithery/cli install open-deep-research --client claude\n\n# Install for Cursor\nnpx @smithery/cli install open-deep-research --client cursor\n```\n\nWhen installing via Smithery, you will be prompted to enter a Brave Search API key.\n\n## Usage\n\n### With Claude Desktop\n\nAdd the following to your Claude Desktop configuration:\n\n```json\n{\n  \"mcpServers\": {\n    \"open-deep-research\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"open-deep-research\"\n      ],\n      \"env\": {\n        \"BRAVE_API_KEY\": \"your_api_key_here\"\n      }\n    }\n  }\n}\n```\n\n### With Cursor\n\nIn Cursor, you can add the MCP server with:\n\n```\nclaude mcp add \"open-deep-research\" npx open-deep-research\n```\n\nMake sure to set the `BRAVE_API_KEY` environment variable before running Cursor.\n\n### Example Queries\n\n- \"What are the latest developments in quantum computing?\"\n- \"Compare and contrast different approaches to climate change mitigation\"\n- \"Explain the history and impact of the Renaissance on European art\"\n- \"What are the pros and cons of different renewable energy sources?\"\n\n## How It Works\n\n1. **Question Analysis**: The system analyzes the main question and breaks it down into sub-questions\n2. **Iterative Search**: For each sub-question, the system performs searches using Brave Search API\n3. **Result Analysis**: The system analyzes the search results to extract relevant information\n4. **Synthesis**: The system combines the findings into a coherent report\n5. **Citation**: All information is properly cited with sources\n\n## Development\n\n### Setup\n\n```bash\ngit clone https://github.com/tositon/open-deep-research.git\ncd open-deep-research\nnpm install\n```\n\n### Build\n\n```bash\nnpm run build\n```\n\n### Run in Development Mode\n\n```bash\nBRAVE_API_KEY=your_api_key npm run dev\n```\n\n## Testing\n\n### Testing with MCP Inspector\n\nДля тестирования MCP сервера можно использовать MCP Inspector, который предоставляет удобный интерфейс для взаимодействия с инструментами:\n\n```bash\n# Установка и запуск MCP Inspector\nnpx @modelcontextprotocol/inspector\n\n# Запуск сервера в другом терминале\nBRAVE_API_KEY=your_api_key npm start\n```\n\nПосле запуска Inspector, откройте браузер и перейдите по адресу http://localhost:5173. Подключитесь к WebSocket серверу, используя URL `ws://localhost:3000`.\n\n### Примеры запросов для тестирования инструментов\n\nВ интерфейсе MCP Inspector вы можете выбрать инструмент и настроить параметры запроса:\n\n#### Тестирование Brave Web Search\n\n```json\n{\n  \"query\": \"latest quantum computing advancements\",\n  \"count\": 5\n}\n```\n\n#### Тестирование Sequential Thinking\n\n```json\n{\n  \"thought\": \"Начинаю анализ проблемы глобального потепления\",\n  \"thoughtNumber\": 1,\n  \"totalThoughts\": 5,\n  \"nextThoughtNeeded\": true\n}\n```\n\n#### Тестирование Deep Research\n\n```json\n{\n  \"query\": \"Сравнение различных источников возобновляемой энергии\",\n  \"action\": \"start\",\n  \"maxSubQuestions\": 3\n}\n```\n\n### Testing with Claude or Cursor\n\nПосле установки сервера через Smithery или локально, вы можете использовать его с Claude Desktop или Cursor, выбрав соответствующий MCP сервер в настройках.\n\n## Publishing on Smithery\n\nTo publish the server on the Smithery platform:\n\n1. Ensure the repository is hosted on GitHub and is public\n2. Register on the [Smithery](https://smithery.ai/) platform\n3. Authenticate via GitHub to connect with the repository\n4. Go to the \"Deployments\" tab on the server page\n5. Click the \"Deploy on Smithery\" button\n6. Follow the deployment setup instructions\n\nAfter publishing, users can install the server using the Smithery CLI:\n\n```bash\nnpx @smithery/cli install open-deep-research --client claude\n```\n\n## Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n\n## License\n\nThis project is licensed under the MIT License - see the LICENSE file for details.\n\n## Acknowledgments\n\n- Inspired by Perplexity Deep Research\n- Built on the Model Context Protocol\n- Uses Sequential Thinking approach for structured research\n- Powered by Brave Search API ",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "search",
        "citations",
        "topics",
        "web search",
        "search tositon",
        "research complex"
      ],
      "category": "web-search"
    },
    "traylinx--traylinx-search-engine-mcp-server": {
      "owner": "traylinx",
      "name": "traylinx-search-engine-mcp-server",
      "url": "https://github.com/traylinx/traylinx-search-engine-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/traylinx.webp",
      "description": "Connect to the deployed Agentic Search API to perform web searches that yield detailed and contextually relevant results, including text summaries, HTML, and media content. Integrate seamlessly with MCP clients for enhanced search functionality.",
      "stars": 0,
      "forks": 3,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-04-03T15:04:08Z",
      "readme_content": "# Traylinx Search Engine MCP Server\n\n[![smithery badge](https://smithery.ai/badge/traylinx/traylinx-search-engine-mcp-server)](https://smithery.ai/server/traylinx/traylinx-search-engine-mcp-server)\n\nA Model Context Protocol (MCP) server that acts as a bridge to the deployed **Agentic Search API**. It allows MCP clients like Claude Desktop and Cursor to utilize intelligent search capabilities with both text summaries and structured data (HTML, images, and more).\n\n## Tools\n\n### `search`\nPerform a web search using Traylinx's API, which provides detailed and contextually relevant results with citations. By default, no time filtering is applied to search results.\n\n**Inputs:**\n- `query` (string): The search query to perform.\n- `search_recency_filter` (string, optional): Filter search results by recency. Options: \"month\", \"week\", \"day\", \"hour\". If not specified, no time filtering is applied.\n\n## How it Works\n\n1. You configure this MCP server with your Agentic Search API URL and API Key (via environment variables passed by the client config).\n2. An MCP client (e.g., Claude) sends a tool call to this server with a search query and optional recency filter.\n3. This MCP server makes a request to the Agentic Search API with the query and authorization header.\n4. It parses the rich response (text, HTML, search results, media, news) and returns structured content to the MCP client.\n\n## Installation\n\n### Prerequisites\n\n* Node.js >= 18.0.0\n* An API Key from Traylinx.com\n\n### Step 1: Get an API Key from Traylinx\n\n1. Visit [traylinx.com](https://traylinx.com) and sign up for an account\n2. Navigate to the developer dashboard/API section\n3. Generate your API key for the Agentic Search API\n4. Keep this key secure - you'll need it for configuration\n\n### Step 2: Set Up the MCP Server\n\n```bash\n# Clone the repository\ngit clone https://github.com/traylinx/traylinx-search-engine-mcp-server.git\ncd traylinx-search-engine-mcp-server\n\n# Install dependencies\nnpm install\n\n# Build the project\nnpm run build\n```\n\n### Step 3: Configure Your MCP Client\n\n#### For Claude Desktop\n\nEdit your `claude_desktop_config.json` file:\n\n```json\n{\n  \"mcpServers\": {\n    \"traylinx-search-engine-mcp-server\": {\n      \"command\": \"node\",\n      \"args\": [\"path/to/traylinx-search-engine-mcp-server/dist/index.js\"],\n      \"env\": {\n        \"AGENTIC_SEARCH_API_KEY\": \"sk-lf-xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\",\n        \"AGENTIC_SEARCH_API_URL\": \"https://agentic-search-engines-n3n7u.ondigitalocean.app\",\n        \"LOG_LEVEL\": \"INFO\"\n      }\n    }\n  }\n}\n```\n\nYou can access this file at:\n- macOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\n- Windows: `%APPDATA%\\Claude\\claude_desktop_config.json`\n\n#### For Cursor\n\nEdit your `mcp.json` file:\n\n```json\n{\n  \"traylinx-search-engine-mcp-server\": {\n    \"env\": {\n      \"AGENTIC_SEARCH_API_KEY\": \"sk-lf-xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\",\n      \"AGENTIC_SEARCH_API_URL\": \"https://agentic-search-engines-n3n7u.ondigitalocean.app\",\n      \"LOG_LEVEL\": \"INFO\"\n    },\n    \"command\": \"node\",\n    \"args\": [\"path/to/traylinx-search-engine-mcp-server/dist/index.js\"]\n  }\n}\n```\n\n**IMPORTANT: Replace the placeholder API key with your actual key from Traylinx.com**\n\n## Verification\n\n1. After configuring your MCP client, restart it completely.\n2. Start a new chat and instruct it to use the tool:\n   - \"Use the search tool to find information about quantum computing.\"\n   - \"Search for the latest news about artificial intelligence and filter by last week.\"\n   - \"Extract text and HTML from the URL https://traylinx.com\"\n3. When the client requests permission, grant it.\n4. You should receive a response containing both text content and potentially structured data.\n\n## Advanced Usage\n\nThe Traylinx Search Engine MCP Server supports multiple response types:\n\n* **Text Content**: Standard markdown text summarizing the search results\n* **Embedded HTML**: For URL extractions, the server can return the scraped HTML\n* **Search Items**: Structured search results with title, URL, and snippet\n* **Media Items**: Images, videos, and other media found during the search\n* **News Articles**: Recent news with thumbnails and metadata\n* **Raw API Response**: Complete response data for advanced use cases\n\n### Using the Recency Filter\n\nTo filter search results by recency:\n\n```\n// Example from Claude Desktop\nUse the search tool to find recent news about SpaceX with results from the last day only.\n\n// Example from a custom client\n{\n  \"name\": \"search\",\n  \"arguments\": {\n    \"query\": \"SpaceX launches\",\n    \"search_recency_filter\": \"week\"\n  }\n}\n```\n\n## Features\n\n* **Rich Content Types**: Returns multiple content types beyond just text\n* **Time Filtering**: Filter results by recency (month, week, day, hour)\n* **Secure API Key Handling**: API key stays in environment variables\n* **Configurable Endpoint**: Easily switch between API endpoints if needed\n* **Full MCP Compliance**: Implements all required MCP server methods\n\n## Deployment\n\n### Smithery.ai Deployment\n\nThis MCP server can be deployed to [Smithery.ai](https://smithery.ai):\n\n1. Create/login to your Smithery account\n2. Click \"Deploy a New MCP Server\"\n3. Enter ID: `traylinx-search-engine-mcp-server`  \n4. Use base directory: `.` (dot for root)\n5. Click \"Create\"\n\nOnce deployed, you can reference this server in Claude's web interface by using:\n```\nUse the traylinx-search-engine-mcp-server to search for [your query]\n```\n\n**Note:** You'll need to provide your `AGENTIC_SEARCH_API_KEY` as an environment variable in the Smithery deployment settings.\n\n## Troubleshooting\n\nIf you encounter issues:\n\n1. Check your API key is correctly set in the configuration\n2. Ensure the MCP client has been fully restarted after configuration\n3. Verify network connectivity to the Agentic Search API\n4. Set `LOG_LEVEL` to `DEBUG` for more detailed logs\n\nFor additional support, contact the API provider at support@traylinx.com\n\n## License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details. ",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "searches",
        "search",
        "agentic",
        "agentic search",
        "search traylinx",
        "traylinx search"
      ],
      "category": "web-search"
    },
    "trilogy-group--youtube-summarizer-mcp": {
      "owner": "trilogy-group",
      "name": "youtube-summarizer-mcp",
      "url": "https://github.com/trilogy-group/youtube-summarizer-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/trilogy-group.webp",
      "description": "Integrates Youtube summarization capabilities into AI applications, providing access to APIs that summarize videos directly from YouTube.",
      "stars": 3,
      "forks": 2,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-08-07T05:33:49Z",
      "readme_content": "# MCP Server\n\nMCP Server is created on top of all the APIs from the Youtube-Summarizer. All APIs are exposed as tools in the MCP protocol and available for any AI application to integrate with.\n\n**Note:** Currently MCP only supports local connections, so it doesn't support remote use of these tools.\n\n## Setup\n\n### Docker Setup\nBuild the Docker image:\n```bash\ndocker build -t youtube-summarizer-mcp .\n```\n\nRun the MCP server using Docker:\n```bash\ndocker run -i --rm youtube-summarizer-mcp\n```\n\n### Using the Inspector\nYou can use the MCP Inspector to explore available tools and test them:\n```bash\n./inspector.sh\n```\n\n### Usage with Claude Desktop\n\nAdd the following to your `claude_desktop_config.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"youtube-summarizer\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"-i\",\n        \"--rm\",\n        \"youtube-summarizer-mcp\"\n      ]\n    }\n  }\n}\n```\n\nNow you can use the added mcp tools from server.py in claude desktop\n\n### MCP Client Sample (Without Claude Desktop)\nRun the MCP client locally to try out the Social Toolkit using natural language:\n\n### Setup\n```bash\n./setup.sh\n```\n\n### Run\n```bash\n./run.sh\n```\n\nIt will run both MCP server and client, connected to each other. The terminal will prompt for natural language queries from the user, which then will be translated into MCP tool calls to answer the user query.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "summarizer",
        "summarization",
        "summarize",
        "youtube summarizer",
        "youtube summarization",
        "summarize videos"
      ],
      "category": "web-search"
    },
    "tulong66--mcp-tavily-proxy": {
      "owner": "tulong66",
      "name": "mcp-tavily-proxy",
      "url": "https://github.com/tulong66/mcp-tavily-proxy",
      "imageUrl": "/freedevtools/mcp/pfp/tulong66.webp",
      "description": "Enables AI-powered web searches through proxy servers, providing direct answers and access to recent news articles with AI-extracted relevant content. It supports HTTP/HTTPS proxy configurations and offers robust logging and error handling for proxy issues.",
      "stars": 2,
      "forks": 2,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-07-06T09:57:43Z",
      "readme_content": "# Tavily MCP Server with Proxy Support\n\nA Model Context Protocol server that provides AI-powered web search capabilities using Tavily's search API, with added support for HTTP/HTTPS proxy configurations. This server enables LLMs to perform sophisticated web searches through proxy servers, get direct answers to questions, and search recent news articles with AI-extracted relevant content.\n\n## Features\n\n- All original Tavily MCP Server features\n- HTTP/HTTPS proxy support through environment variables\n- Enhanced logging for proxy configurations\n- Robust error handling for proxy-related issues\n\n## Available Tools\n\n- `tavily_web_search` - Performs comprehensive web searches with AI-powered content extraction.\n    - `query` (string, required): Search query\n    - `max_results` (integer, optional): Maximum number of results to return (default: 5, max: 20)\n    - `search_depth` (string, optional): Either \"basic\" or \"advanced\" search depth (default: \"basic\")\n\n- `tavily_answer_search` - Performs web searches and generates direct answers with supporting evidence.\n    - `query` (string, required): Search query\n    - `max_results` (integer, optional): Maximum number of results to return (default: 5, max: 20)\n    - `search_depth` (string, optional): Either \"basic\" or \"advanced\" search depth (default: \"advanced\")\n\n- `tavily_news_search` - Searches recent news articles with publication dates.\n    - `query` (string, required): Search query\n    - `max_results` (integer, optional): Maximum number of results to return (default: 5, max: 20)\n    - `days` (integer, optional): Number of days back to search (default: 3)\n\n## Installation\n\n### Use `pip`\n\n```bash\npip install mcp-tavily-proxy\n```\n\nor if you have `uv` installed:\n\n```bash\nuv pip install mcp-tavily-proxy\n```\n\n### Build from Source\n\nClone this repository and build and install the program:\n\n```bash\ngit clone https://github.com/tulong66/mcp-tavily-proxy.git\ncd mcp-tavily-proxy\nuv build\nuv pip install .\n```\n\n## Configuration\n\n### API Key and Proxy Settings\n\nThe server requires a Tavily API key and supports proxy configuration through environment variables:\n\n1. Set required environment variables:\n```bash\n# Tavily API Key\nexport TAVILY_API_KEY=your_api_key_here\n\n# Proxy Settings (if needed)\nexport HTTP_PROXY=http://your-proxy:port\nexport HTTPS_PROXY=http://your-proxy:port\n```\n\n2. Or provide API key as a command-line argument:\n```bash\npython -m mcp_server_tavily --api-key=your_api_key_here\n```\n\n### Configure for Claude.app\n\nAdd to your Claude settings:\n\n```json\n{\n  \"mcpServers\": {\n    \"tavily\": {\n      \"command\": \"python\",\n      \"args\": [\"-m\", \"mcp_server_tavily\"]\n    },\n    \"env\": {\n      \"TAVILY_API_KEY\": \"your_api_key_here\",\n      \"HTTP_PROXY\": \"http://your-proxy:port\",\n      \"HTTPS_PROXY\": \"http://your-proxy:port\"\n    }\n  }\n}\n```\n\n## Examples\n\nFor a regular search:\n```\nTell me about Anthropic's newly released MCP protocol\n```\n\nTo generate a report with explicit exclusions:\n```\nTell me about redwood trees. Please use MLA format in markdown syntax and include the URLs in the citations. Exclude Wikipedia sources.\n```\n\nFor news search:\n```\nGive me the top 10 AI-related news in the last 5 days\n```\n\n## Debugging\n\nEnable debug logging to see detailed proxy configuration information:\n\n```bash\nexport TAVILY_LOG_LEVEL=DEBUG\npython -m mcp_server_tavily\n```\n\nYou can also use the MCP inspector:\n\n```bash\nnpx @modelcontextprotocol/inspector python -m mcp_server_tavily\n```\n\n## Contributing\n\nContributions are welcome! Please feel free to submit pull requests to help improve the proxy support or add new features.\n\n## License\n\nThis project is licensed under the MIT License - see the LICENSE file for details.\n\n## Acknowledgments\n\nThis project is based on the original [mcp-tavily](https://github.com/RamXX/mcp-tavily) with added proxy support functionality.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "proxy",
        "searches",
        "http",
        "searches proxy",
        "tavily proxy",
        "proxy enables"
      ],
      "category": "web-search"
    },
    "unlimitbladeworks--awesome-mcp-twikit": {
      "owner": "unlimitbladeworks",
      "name": "awesome-mcp-twikit",
      "url": "https://github.com/unlimitbladeworks/awesome-mcp-twikit",
      "imageUrl": "/freedevtools/mcp/pfp/unlimitbladeworks.webp",
      "description": "Search tweets, analyze sentiment, and retrieve timeline updates from Twitter. Provides tools for social media insights and monitoring directly within LLM-powered applications.",
      "stars": 0,
      "forks": 1,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-05-13T04:02:20Z",
      "readme_content": "# Awesome-MCP-Twikit\n\n[![smithery badge](https://smithery.ai/badge/@unlimitbladeworks/awesome-mcp-twikit)](https://smithery.ai/server/@unlimitbladeworks/awesome-mcp-twikit)\nA Model Context Protocol (MCP) server for interacting with Twitter.\n\n\n## Installation\n\n### Installing via Smithery\n\nTo install Twikit Twitter Search for Claude Desktop automatically via [Smithery](https://smithery.ai/server/mcp-twikit):\n\n```bash\nnpx -y @smithery/cli install mcp-twikit --client claude\n```\n\n### Manual Installation\n```json\n{\n    \"mcpServer\": {\n        \"mcp-twikit-tools\": {\n            \"command\": \"uvx\",\n            \"args\": [\n                \"--from\",\n                \"git+https://github.com/unlimitbladeworks/awesome-mcp-twikit\",\n                \"mcp-twikit-tools\"\n            ],\n            \"env\": {\n                \"TWITTER_USERNAME\": \"@example\",\n                \"TWITTER_EMAIL\": \"me@example.com\",\n                \"TWITTER_PASSWORD\": \"secret\",\n                \"TWITTER_2FA\": \"2FA\",\n                \"ENABLE_PROXY\": \"true\",\n                \"PROXY\": \"http://ip:port\"\n            }\n        }\n    }\n}\n```\n\n## Example usage\n\nUsing [mcp-client-cli](https://github.com/adhikasp/mcp-client-cli)\n\n\n### Comparing sentiments across different twitter accounts\n\n```\n$ llm compare 20 latest tweet directed @IndiHomeCare, @di_cbn, @BiznetHome, @ID_MyRepublic. What are people sentiment to the product? Do 1 search for each account\n\nI'll help you analyze the latest tweets for these internet service providers in Indonesia to understand the public sentiment. I'll use the search_twitter function for each account.\n\nTool Calls:\n  search_twitter\n  Args:\n    query: to:IndiHomeCare\n    count: 20\n    sort_by: Latest\n\nTool Calls:\n  search_twitter\n  Args:\n    query: to:di_cbn\n    count: 20\n    sort_by: Latest\n\nTool Calls:\n  search_twitter\n  Args:\n    query: to:BiznetHome\n    count: 20\n    sort_by: Latest\n\nTool Calls:\n  search_twitter\n  Args:\n    query: to:ID_MyRepublic\n    count: 20\n    sort_by: Latest\n\nSentiment Analysis Summary:\n\n1. IndiHomeCare:\n- Predominantly Negative Sentiment\n- Key Issues:\n  - Unstable internet connection\n  - Slow response times\n  - Intermittent service disruptions\n  - Long waiting times for customer support\n- Specific Complaints:\n  - Network instability\n  - Lagging internet\n  - Unresolved technical issues\n  - Lack of timely communication\n\n2. CBN (di_cbn):\n- Mostly Negative Sentiment\n- Key Issues:\n  - Prolonged internet outages (some reporting 1-2 days of no service)\n  - Slow customer support response\n  - Lack of clear communication about service disruptions\n- Specific Complaints:\n  - LOS (Line of Sight) issues\n  - No timely resolution of technical problems\n  - Delayed email and support responses\n\n3. BiznetHome:\n- Strongly Negative Sentiment\n- Key Issues:\n  - Frequent network interruptions\n  - Slow internet speeds\n  - Poor customer support responsiveness\n- Specific Complaints:\n  - Packet loss\n  - Inconsistent connection\n  - Long download times\n  - Unresolved technical issues\n\n4. MyRepublic (ID_MyRepublic):\n- Extremely Negative Sentiment\n- Key Issues:\n  - Severe and prolonged service disruptions\n  - Complete internet outages\n  - Inability to access online services and games\n  - Extremely poor customer support\n- Specific Complaints:\n  - Multi-day service interruptions\n  - Inability to access websites and online games\n  - Missed technician appointments\n  - Lack of compensation or clear communication\n\nOverall Sentiment Ranking (from Bad to Worst):\n1. IndiHomeCare (Bad)\n2. CBN (Worse)\n3. BiznetHome (Very Bad)\n4. MyRepublic (Extremely Bad)\n\nCommon Themes Across Providers:\n- Poor network stability\n- Slow customer support response\n- Lack of transparent communication\n- Prolonged service interruptions\n- Unresolved technical issues\n\nRecommendation: These Indonesian internet service providers are experiencing significant customer satisfaction issues. Customers are frustrated with unreliable services, poor technical support, and lack of timely problem resolution.\n```\n\n### Getting tweets from your home timeline\n\n```\n$ llm what is happening on my twitter timeline?\n\nI'll help you check your Twitter timeline. I'll retrieve the latest tweets from your timeline:\n\nTool Calls:\n  get_timeline\n  Args:\n\nI'll summarize the key highlights from your Twitter timeline:\n\n1. Professional/Tech Highlights:\n- @ibamarief is preparing to onboard their first intern, noting it's a significant responsibility\n- @papanberjalan shared a critique about web developers not understanding computer performance\n- @pathofexile is running a Twitch Drops promotion for their game\n\n2. Notable Tweets:\n- @elonmusk posted a cryptic \"Hmm\"\n- @realDonaldTrump discussed eliminating Daylight Saving Time\n- @blinkist shared an article about billionaires reading up to 50 books per year\n\n3. Miscellaneous Interesting Tweets:\n- @nealtaparia shared a thread about 11 companies built around a single product\n- @Rixhabh__ posted about creative and iconic ads\n- Several tweets in Indonesian covering various topics from personal stories to social issues\n\nWould you like me to elaborate on any of these tweets or provide more context about any specific post?\n```\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "twitter",
        "tweets",
        "twikit",
        "tweets analyze",
        "search tweets",
        "twitter provides"
      ],
      "category": "web-search"
    },
    "uzaysozen--imdb-mcp-server": {
      "owner": "uzaysozen",
      "name": "imdb-mcp-server",
      "url": "https://github.com/uzaysozen/imdb-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/uzaysozen.webp",
      "description": "Access detailed movie and TV show information from the IMDb API, including data about films, actors, and upcoming releases. Enhance applications with rich media content and insights.",
      "stars": 3,
      "forks": 3,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-06-30T18:43:31Z",
      "readme_content": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/uzaysozen-imdb-mcp-server-badge.png)](https://mseep.ai/app/uzaysozen-imdb-mcp-server)\n# IMDb MCP Server\n\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![Python 3.13](https://img.shields.io/badge/python-3.13-blue.svg)](https://www.python.org/downloads/)\n[![Docker](https://img.shields.io/badge/Docker-Available-blue.svg)](https://www.docker.com/)\n[![RapidAPI](https://img.shields.io/badge/RapidAPI-IMDb-orange.svg)](https://rapidapi.com/octopusteam-octopusteam-default/api/imdb236)\n[![smithery badge](https://smithery.ai/badge/@uzaysozen/imdb-mcp-server)](https://smithery.ai/server/@uzaysozen/imdb-mcp-server)\n\n[![Verified on MseeP](https://mseep.ai/badge.svg)](https://mseep.ai/app/8ed9e57d-d9e7-4a5d-ab94-4113be3ee842)\n\n[Verified on MCP Review](https://mcpreview.com/mcp-servers/uzaysozen/imdb-mcp-server)\n\nA Python server implementing Model Context Protocol (MCP) for movie and TV show information using the IMDb API service.\n\n## Table of Contents\n- [Overview](#overview)\n- [Features](#features)\n- [Configuration](#configuration)\n- [Tools](#tools)\n  - [Search Tools](#search-tools)\n  - [IMDb ID Tools](#imdb-id-tools)\n  - [Configuration Tools](#configuration-tools)\n  - [Movies Tools](#movies-tools)\n  - [TV Shows Tools](#tv-shows-tools)\n  - [Upcoming Releases Tools](#upcoming-releases-tools)\n  - [India Spotlight Tools](#india-spotlight-tools)\n- [Example Prompt and Response](#example-prompt-and-response)\n- [Installation](#installation)\n- [Starting the Server](#starting-the-server)\n- [Technical Details](#technical-details)\n  - [Pagination System](#pagination-system)\n  - [Caching System](#caching-system)\n- [Limitations](#limitations)\n- [Troubleshooting](#troubleshooting)\n- [License](#license)\n\n## Overview\n\nThis server provides a comprehensive set of tools for accessing IMDb data through the IMDb API. It serves as a bridge between agents and the IMDb database, offering detailed information about movies, TV shows, actors, directors, and more.\n\n## Features\n\n- 🎬 Movie and TV show search capabilities\n- 📋 Detailed information about movies and TV shows\n- 👨‍👩‍👧‍👦 Cast and crew information\n- 🏆 Top-rated and popular content lists\n- 💰 Box office data\n- 🌍 Country-specific movie information (with special focus on Indian cinema)\n- 🔜 Upcoming releases\n- 🔄 Efficient response caching system\n\n## Configuration\n\nThis server requires an API key from RapidAPI for the IMDb API service:\n\n1. Create an account on [RapidAPI](https://rapidapi.com/)\n2. Subscribe to the [IMDb API](https://rapidapi.com/octopusteam-octopusteam-default/api/imdb236) on RapidAPI\n3. Set the environment variable:\n   ```\n   RAPID_API_KEY_IMDB=your_api_key_here\n   ```\n\n## Tools\n\n### Search Tools\n\n| Tool | Description | Example |\n|------|-------------|---------|\n| **search_imdb** | Search for movies and TV shows with various filtering options | `search_imdb(primary_title=\"Inception\")` |\n\n### IMDb ID Tools\n\n| Tool | Description | Example |\n|------|-------------|---------|\n| **get_imdb_details** | Retrieve detailed information about a movie or TV show | `get_imdb_details(imdb_id=\"tt1375666\")` |\n| **get_directors** | Retrieve the directors of a movie | `get_directors(imdb_id=\"tt1375666\")` |\n| **get_cast** | Retrieve the cast of a movie | `get_cast(imdb_id=\"tt1375666\")` |\n| **get_writers** | Retrieve the writers of a movie | `get_writers(imdb_id=\"tt1375666\")` |\n\n### Configuration Tools\n\n| Tool | Description | Example |\n|------|-------------|---------|\n| **get_types** | Get all available content types | `get_types()` |\n| **get_genres** | Get all available genres | `get_genres()` |\n| **get_countries** | Get all available countries | `get_countries()` |\n| **get_languages** | Get all available languages | `get_languages()` |\n\n### Movies Tools\n*Paginated (5 results per page)*\n\n| Tool | Description | Example |\n|------|-------------|---------|\n| **get_top_250_movies** | Get the top 250 movies from IMDb | `get_top_250_movies()` |\n| **get_top_box_office_us** | Get the US box office records | `get_top_box_office_us()` |\n| **get_most_popular_movies** | Get the most popular movies | `get_most_popular_movies()` |\n\n### TV Shows Tools\n*Paginated (5 results per page)*\n\n| Tool | Description | Example |\n|------|-------------|---------|\n| **get_top_250_tv_shows** | Get the top 250 TV shows from IMDb | `get_top_250_tv_shows()` |\n| **get_most_popular_tv_shows** | Get the most popular TV shows | `get_most_popular_tv_shows()` |\n\n### Upcoming Releases Tools\n*Paginated (5 results per page)*\n\n| Tool | Description | Example |\n|------|-------------|---------|\n| **get_upcoming_releases** | Get upcoming movie and TV show releases by country | `get_upcoming_releases(country_code=\"US\", type=\"MOVIE\")` |\n| **get_available_country_codes_for_upcoming_releases** | Get available country codes for upcoming releases | `get_available_country_codes_for_upcoming_releases()` |\n\n### India Spotlight Tools (Paginated)\n*Paginated (5 results per page)*\n\n| Tool | Description | Example |\n|------|-------------|---------|\n| **get_top_rated_malayalam_movies** | Get top 50 rated Malayalam movies | `get_top_rated_malayalam_movies()` |\n| **get_upcoming_indian_movies** | Get most anticipated upcoming Indian movies | `get_upcoming_indian_movies()` |\n| **get_trending_tamil_movies** | Get trending Tamil movies | `get_trending_tamil_movies()` |\n| **get_trending_telugu_movies** | Get trending Telugu movies | `get_trending_telugu_movies()` |\n| **get_top_rated_tamil_movies** | Get top 50 rated Tamil movies | `get_top_rated_tamil_movies()` |\n| **get_top_rated_telugu_movies** | Get top 50 rated Telugu movies | `get_top_rated_telugu_movies()` |\n| **get_top_rated_indian_movies** | Get top 250 rated Indian movies | `get_top_rated_indian_movies()` |\n\n## Example Prompt and Response\n\n**What are the 5 action movies that will be released in the nearest future?**\n\nI'll find the upcoming action movies for you.\n\nCalled MCP tool ```get_upcoming_releases```\n\nNow I'll look for the action movies from these upcoming releases:\n\nCalled MCP tool ```get_upcoming_releases```\n\nBased on the search results, here are the 5 upcoming action movies that will be released in the nearest future:\n\n1. **A Minecraft Movie** (April 4, 2025)\n   - Genres: Action, Adventure, Comedy\n   - Description: Four misfits are pulled through a portal into a cubic wonderland and embark on a quest with an expert crafter to get back home.\n\n2. **Freaky Tales** (April 4, 2025)\n   - Genres: Action, Adventure, Comedy\n   - Description: Four interconnected stories set in 1987 Oakland, CA about the love of music, movies, people, places and memories.\n\n3. **Wake Up** (April 4, 2025)\n   - Genres: Action, Horror\n   - Description: Young activists invade a furniture store to protest environmental issues but find themselves trapped with a hunting-obsessed night guard.\n\n4. **Warfare** (April 11, 2025)\n   - Genres: Action, Drama, War\n   - Description: Based on ex-Navy Seal Ray Mendoza's real-life experiences during the Iraq War.\n\n5. **The Amateur** (April 11, 2025)\n   - Genres: Action, Thriller\n   - Description: A CIA cryptographer blackmails his agency into training him to pursue terrorists who killed his wife.\n\n## Installation\n\n### Installing via Smithery\n\nTo install IMDb Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@uzaysozen/imdb-mcp-server):\n\n```bash\nnpx -y @smithery/cli install @uzaysozen/imdb-mcp-server --client claude\n```\n\n### Option 1: Using Docker (Recommended)\n\n1. Clone this repository\n```bash\ngit clone https://github.com/uzaysozen/imdb-mcp-server.git\ncd imdb-mcp-server\n```\n\n2. Install dependencies\n```bash\npip install -r requirements.txt\n```\n\n3. Build the Docker image\n```bash\ndocker build -t imdb_server .\n```\n\n4. Run the Docker container (ensure your API key is passed as an environment variable)\n```bash\ndocker run -d -p 8000:8000 -e RAPID_API_KEY_IMDB=your_api_key_here --name imdb_server imdb_server\n```\n\n5. Add this to your `claude_desktop_config.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"imdb_server\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"exec\",\n        \"-i\",\n        \"imdb_server\",\n        \"imdb-mcp-server\"\n      ],\n      \"env\": {\n        \"RAPID_API_KEY_IMDB\": \"your_api_key_here\"\n      }\n    }\n  }\n}\n```\n\n### Option 2: Direct Python Execution\n\n1. Clone this repository\n```bash\ngit clone https://github.com/uzaysozen/imdb-mcp-server.git\ncd imdb-mcp-server\n```\n\n2. Install dependencies\n```bash\npip install -r requirements.txt\n```\n\n3. Set the API key environment variable\n```bash\nexport RAPID_API_KEY_IMDB=your_api_key_here\n```\n\n4. Add this to your `claude_desktop_config.json`, adjusting the Python path as needed:\n\n```json\n{\n  \"mcpServers\": {\n    \"imdb_server\": {\n      \"command\": \"/path/to/your/python\",\n      \"args\": [\n        \"/path/to/imdb_server.py\"\n      ],\n      \"env\": {\n        \"RAPID_API_KEY_IMDB\": \"your_api_key_here\"\n      }\n    }\n  }\n}\n```\n\n## Starting the Server\n\n```bash\n# Start the server directly\npython imdb_server.py\n\n# Or using MCP CLI\nmcp run imdb_server.py\n\n# Or if using Docker, the server starts automatically with the container\ndocker run -d -p 8000:8000 -e RAPID_API_KEY_IMDB=your_api_key_here --name imdb_server imdb_server\n```\n\nAfter adding your chosen configuration, restart Claude Desktop to load the IMDb server. You'll then be able to use all the movie and TV show data tools in your conversations with Claude.\n\n## Technical Details\n\nThe server is built on:\n- IMDb API via RapidAPI\n- MCP for API interface\n- Requests for API communication\n- FastMCP for server implementation\n- Custom in-memory caching system\n- Smart pagination that limits results to 5 items per request, optimizing for AI agent consumption\n\n### Pagination System\n\nAll data retrieval tools implement pagination to enhance AI agent performance:\n\n#### Purpose\n- **AI-Optimized Responses**: Limits each response to 5 items, preventing overwhelm in AI agents that process the data\n- **Focused Results**: Helps agents provide more relevant and concise information to users\n- **Improved Processing**: Reduces the cognitive load on AI agents when analyzing movie and TV show data\n\n#### Implementation\n- Each paginated endpoint accepts a `start` parameter (default: 0)\n- Results include navigation metadata (totalCount, hasMore, nextStart)\n- Consistent 5-item page size across all collection endpoints\n- Example request with pagination: `get_top_250_movies(start=5)` returns items 6-10\n\n#### Benefits\n- **Better Agent Responses**: Prevents AI agents from receiving too much data at once\n- **Manageable Information**: Creates digestible chunks of data that agents can process effectively\n- **Sequential Access**: Allows structured exploration of large datasets through multiple tool calls\n\n### Caching System\n\nThe server implements an efficient caching system to improve performance and reduce API calls:\n\n#### Features\n\n- **In-memory Cache**: Stores API responses in memory for quick retrieval\n- **Configurable Expiration and Size**: Cache entries expire after a customizable time period (default: 10 minutes) and have a default size of 100 cache keys\n- **Automatic Cache Cleaning**: Periodically (default: 5 minutes) removes expired entries to manage memory usage using a background thread\n- **Cache Keys**: Generated based on the URL and query parameters to ensure uniqueness\n\n#### Benefits\n\n- **Reduced API Usage**: Helps stay within API rate limits by reusing responses\n- **Faster Response Times**: Eliminates network latency for cached queries\n- **Cost Efficiency**: Minimizes the number of API calls, especially for popular or repeated queries\n\n#### Configuration\n\nThe cache size and expiration time can be adjusted in the code:\n\n```python\n# Default are 600 seconds (10 minutes) and 100 cache keys\nresponse_cache = ResponseCache(expiry_seconds=120, max_size=50)\n```\n\n## Limitations\n\n- API rate limits apply based on your RapidAPI subscription\n- Some detailed information may require additional API calls\n- Search results may be limited to a certain number of items per request\n- In-memory cache is lost when server restarts\n- All paginated responses return a maximum of 5 items per page\n\n## Troubleshooting\n\n| Problem | Solution |\n|---------|----------|\n| API key not recognized | Ensure the RAPID_API_KEY_IMDB environment variable is properly set |\n| Rate limit exceeded | Check your RapidAPI subscription tier and limits |\n| Timeout errors | The server has a 30-second timeout; for large requests, try limiting parameters |\n| Empty results | Try broader search terms or check if the content exists in IMDb's database |\n| High memory usage | If running for extended periods with many unique queries, restart the server occasionally to clear the cache |\n\n## License\n\nThis MCP server is available under the MIT License.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "imdb",
        "uzaysozen",
        "films",
        "uzaysozen imdb",
        "imdb api",
        "information imdb"
      ],
      "category": "web-search"
    },
    "vaebe--mcp": {
      "owner": "vaebe",
      "name": "mcp",
      "url": "https://github.com/vaebe/mcp",
      "imageUrl": "/freedevtools/mcp/pfp/vaebe.webp",
      "description": "Leverage the GitHub search API to retrieve relevant data from GitHub repositories, enabling advanced search capabilities for applications. Efficiently integrate and utilize data from various GitHub resources.",
      "stars": 0,
      "forks": 3,
      "license": "Apache License 2.0",
      "language": "JavaScript",
      "updated_at": "2025-04-16T14:21:57Z",
      "readme_content": "# mcp\n\n## 版本更新\n\n```bash\n# 安装 changesets\nnpm install @changesets/cli --save-dev\n\n# 初始化 changesets\nnpx changeset init\n\n# 生成变更日志\npnpm changelog\n\n# 创建一个新的 changeset\nnpx changeset\n\n# 更新版本号和生成发布日志\nnpx changeset version\n\n# 测试发布\npnpm -r publish --access public --dry-run --no-git-checks\n# 发布\npnpm -r publish --access public --no-git-checks\n\n# 移除版本\nnpm unpublish @vaebe/server-github-search@0.2.0 --force\n```\n\n## 测试 mcp\n\n```bash\nnpx @modelcontextprotocol/inspector node mcp/githubSearch.mjs\n```\n\n## docker 打包测试\n\n```bash\ndocker build -f packages/github-search/Dockerfile -t github-search .\n```",
      "npm_url": "https://www.npmjs.com/package/mcp",
      "npm_downloads": 17624,
      "keywords": [
        "github",
        "repositories",
        "search",
        "github search",
        "various github",
        "github resources"
      ],
      "category": "web-search"
    },
    "valyu-network--valyu-mcp-js": {
      "owner": "valyu-network",
      "name": "valyu-mcp-js",
      "url": "https://github.com/valyu-network/valyu-mcp-js",
      "imageUrl": "/freedevtools/mcp/pfp/valyu-network.webp",
      "description": "Access Valyu's knowledge retrieval and feedback APIs to search proprietary and web sources for information and submit transaction feedback. Provides real-time insights and enhances LLM capabilities through dynamic user interaction.",
      "stars": 0,
      "forks": 4,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-03-05T01:01:57Z",
      "readme_content": "# Valyu MCP Server\n\nA Model Context Protocol server that provides access to Valyu's knowledge retrieval and feedback APIs. This server enables LLMs to search proprietary and web sources for information and submit feedback on transactions.\n\n### Available Tools\n\n- `knowledge` - Search proprietary and/or web sources for information\n  - Required arguments:\n    - `query` (string): The question or topic to search for\n    - `search_type` (string): Type of sources to search (\"proprietary\", \"web\", or \"all\")\n    - `max_price` (number): Maximum allowed price per thousand queries (CPM)\n  - Optional arguments:\n    - `data_sources` (string[]): List of index names to search over\n    - `max_num_results` (integer): Number of results returned after reranking\n    - `similarity_threshold` (number): Minimum similarity score for included results\n    - `query_rewrite` (boolean): Whether to rewrite the query for better performance\n\n- `feedback` - Submit user feedback for a transaction\n  - Required arguments:\n    - `tx_id` (string): Transaction ID to provide feedback for\n    - `feedback` (string): User feedback text\n    - `sentiment` (string): Sentiment rating (\"very good\", \"good\", \"bad\", \"very bad\")\n\n## Installation\n\n### Using Docker\n\n```bash\ndocker pull ghcr.io/tiovikram/valyu-mcp-server\ndocker run -i --rm -e VALYU_API_KEY=your-api-key ghcr.io/tiovikram/valyu-mcp-server\n```\n\n## Configuration\n\n### Environment Variables\n\n- `VALYU_API_KEY` (required): Your Valyu API key\n\n### Configure for Claude.app\n\nAdd to your Claude settings:\n\n<summary>Using Docker</summary>\n\n```json\n\"mcpServers\": {\n  \"valyu\": {\n    \"command\": \"docker\",\n    \"args\": [\"run\", \"--pull\", \"--rm\", \"-i\", \"-e\", \"VALYU_API_KEY\", \"ghcr.io/tiovikram/valyu-mcp-server\"],\n    \"env\": {\n      \"VALYU_API_KEY\": \"<your-valyu-api-key>\"\n    }\n  }\n}\n```\n## Example Interactions\n\n1. Knowledge search:\n```json\n{\n  \"name\": \"knowledge\",\n  \"arguments\": {\n    \"query\": \"What is quantum computing?\",\n    \"search_type\": \"all\",\n    \"max_price\": 0.5,\n    \"data_sources\": [\"valyu/valyu-arxiv\", \"valyu/valyu-wikipedia\"],\n    \"max_num_results\": 5\n  }\n}\n```\n\n2. Submit feedback:\n```json\n{\n  \"name\": \"feedback\",\n  \"arguments\": {\n    \"tx_id\": \"12345abcdef\",\n    \"feedback\": \"The information was very helpful and accurate.\",\n    \"sentiment\": \"very good\"\n  }\n}\n```\n\n## Debugging\n\nYou can use the MCP inspector to debug the server:\n\n```bash\nnpx @modelcontextprotocol/inspector node dist/index.js\n```\n\n## Examples of Questions for Claude\n\n1. \"Can you search for information about artificial intelligence in medicine?\"\n2. \"I'd like to learn about sustainable energy solutions. Can you search for that?\"\n3. \"Please help me submit feedback for my transaction with ID TX123456.\"\n4. \"Find me the latest research on climate change adaptation strategies.\"\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "valyu",
        "apis",
        "search",
        "search valyu",
        "valyu knowledge",
        "valyu network"
      ],
      "category": "web-search"
    },
    "vinhphamai23--brave-search": {
      "owner": "vinhphamai23",
      "name": "brave-search",
      "url": "https://github.com/vinhphamai23/brave-search",
      "imageUrl": "/freedevtools/mcp/pfp/vinhphamai23.webp",
      "description": "Integrates the Brave Search API to provide web and local search capabilities, enabling general web queries, news searches, and local business searches with detailed results and filtering options.",
      "stars": 0,
      "forks": 0,
      "license": "No License",
      "language": "JavaScript",
      "updated_at": "2025-05-22T03:56:01Z",
      "readme_content": "# Brave Search MCP Server\n\nAn MCP server implementation that integrates the Brave Search API, providing both web and local search capabilities.\n\n## Features\n\n- **Web Search**: General queries, news, articles, with pagination and freshness controls\n- **Local Search**: Find businesses, restaurants, and services with detailed information\n- **Flexible Filtering**: Control result types, safety levels, and content freshness\n- **Smart Fallbacks**: Local search automatically falls back to web when no results are found\n\n## Tools\n\n- **brave_web_search**\n\n  - Execute web searches with pagination and filtering\n  - Inputs:\n    - `query` (string): Search terms\n    - `count` (number, optional): Results per page (max 20)\n    - `offset` (number, optional): Pagination offset (max 9)\n\n- **brave_local_search**\n  - Search for local businesses and services\n  - Inputs:\n    - `query` (string): Local search terms\n    - `count` (number, optional): Number of results (max 20)\n  - Automatically falls back to web search if no local results found\n\n## Configuration\n\n### Getting an API Key\n\n1. Sign up for a [Brave Search API account](https://brave.com/search/api/)\n2. Choose a plan (Free tier available with 2,000 queries/month)\n3. Generate your API key [from the developer dashboard](https://api-dashboard.search.brave.com/app/keys)\n\n### Usage with Claude Desktop\n\nAdd this to your `claude_desktop_config.json`:\n\n### Docker\n\n```json\n{\n  \"mcpServers\": {\n    \"brave-search\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"-i\",\n        \"--rm\",\n        \"-e\",\n        \"BRAVE_API_KEY\",\n        \"mcp/brave-search\"\n      ],\n      \"env\": {\n        \"BRAVE_API_KEY\": \"YOUR_API_KEY_HERE\"\n      }\n    }\n  }\n}\n```\n\n### NPX\n\n```json\n{\n  \"mcpServers\": {\n    \"brave-search\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"@modelcontextprotocol/server-brave-search\"\n      ],\n      \"env\": {\n        \"BRAVE_API_KEY\": \"YOUR_API_KEY_HERE\"\n      }\n    }\n  }\n}\n```\n\n### Usage with VS Code\n\nFor quick installation, use the one-click installation buttons below...\n\n[![Install with NPX in VS Code](https://img.shields.io/badge/VS_Code-NPM-0098FF?style=flat-square&logo=visualstudiocode&logoColor=white)](https://insiders.vscode.dev/redirect/mcp/install?name=brave&inputs=%5B%7B%22type%22%3A%22promptString%22%2C%22id%22%3A%22apiKey%22%7D%5D&config=%7B%22command%22%3A%22npx%22%2C%22args%22%3A%5B%22-y%22%2C%22%40modelcontextprotocol%2Fserver-brave-search%22%5D%2C%22env%22%3A%7B%22BRAVE_API_KEY%22%3A%22%24%7Binput%3Abrave_api_key%7D%22%7D%7D) [![Install with NPX in VS Code Insiders](https://img.shields.io/badge/VS_Code_Insiders-NPM-24bfa5?style=flat-square&logo=visualstudiocode&logoColor=white)](https://insiders.vscode.dev/redirect/mcp/install?name=brave&inputs=%5B%7B%22type%22%3A%22promptString%22%2C%22id%22%3A%22apiKey%22%7D%5D&config=%7B%22command%22%3A%22npx%22%2C%22args%22%3A%5B%22-y%22%2C%22%40modelcontextprotocol%2Fserver-brave-search%22%5D%2C%22env%22%3A%7B%22BRAVE_API_KEY%22%3A%22%24%7Binput%3Abrave_api_key%7D%22%7D%7D&quality=insiders)\n\n[![Install with Docker in VS Code](https://img.shields.io/badge/VS_Code-Docker-0098FF?style=flat-square&logo=visualstudiocode&logoColor=white)](https://insiders.vscode.dev/redirect/mcp/install?name=brave&inputs=%5B%7B%22type%22%3A%22promptString%22%2C%22id%22%3A%22apiKey%22%7D%5D&config=%7B%22command%22%3A%22docker%22%2C%22args%22%3A%5B%22run%22%2C%22-i%22%2C%22--rm%22%2C%22-e%22%2C%22BRAVE_API_KEY%22%2C%22mcp%2Fbrave-search%22%5D%2C%22env%22%3A%7B%22BRAVE_API_KEY%22%3A%22%24%7Binput%3Abrave_api_key%7D%22%7D%7D) [![Install with Docker in VS Code Insiders](https://img.shields.io/badge/VS_Code_Insiders-Docker-24bfa5?style=flat-square&logo=visualstudiocode&logoColor=white)](https://insiders.vscode.dev/redirect/mcp/install?name=brave&inputs=%5B%7B%22type%22%3A%22promptString%22%2C%22id%22%3A%22apiKey%22%7D%5D&config=%7B%22command%22%3A%22docker%22%2C%22args%22%3A%5B%22run%22%2C%22-i%22%2C%22--rm%22%2C%22-e%22%2C%22BRAVE_API_KEY%22%2C%22mcp%2Fbrave-search%22%5D%2C%22env%22%3A%7B%22BRAVE_API_KEY%22%3A%22%24%7Binput%3Abrave_api_key%7D%22%7D%7D&quality=insiders)\n\nFor manual installation, add the following JSON block to your User Settings (JSON) file in VS Code. You can do this by pressing `Ctrl + Shift + P` and typing `Preferences: Open User Settings (JSON)`.\n\nOptionally, you can add it to a file called `.vscode/mcp.json` in your workspace. This will allow you to share the configuration with others.\n\n> Note that the `mcp` key is not needed in the `.vscode/mcp.json` file.\n\n#### Docker\n\n```json\n{\n  \"mcp\": {\n    \"inputs\": [\n      {\n        \"type\": \"promptString\",\n        \"id\": \"brave_api_key\",\n        \"description\": \"Brave Search API Key\",\n        \"password\": true\n      }\n    ],\n    \"servers\": {\n      \"brave-search\": {\n        \"command\": \"docker\",\n        \"args\": [\n          \"run\",\n          \"-i\",\n          \"--rm\",\n          \"-e\",\n          \"BRAVE_API_KEY\",\n          \"mcp/brave-search\"\n        ],\n        \"env\": {\n          \"BRAVE_API_KEY\": \"${input:brave_api_key}\"\n        }\n      }\n    }\n  }\n}\n```\n\n#### NPX\n\n```json\n{\n  \"mcp\": {\n    \"inputs\": [\n      {\n        \"type\": \"promptString\",\n        \"id\": \"brave_api_key\",\n        \"description\": \"Brave Search API Key\",\n        \"password\": true\n      }\n    ],\n    \"servers\": {\n      \"brave-search\": {\n        \"command\": \"npx\",\n        \"args\": [\"-y\", \"@modelcontextprotocol/server-brave-search\"],\n        \"env\": {\n          \"BRAVE_API_KEY\": \"${input:brave_api_key}\"\n        }\n      }\n    }\n  }\n}\n```\n\n## Build\n\nDocker build:\n\n```bash\ndocker build -t mcp/brave-search:latest -f src/brave-search/Dockerfile .\n```\n\n## License\n\nThis MCP server is licensed under the MIT License. This means you are free to use, modify, and distribute the software, subject to the terms and conditions of the MIT License. For more details, please see the LICENSE file in the project repository.",
      "npm_url": "https://www.npmjs.com/package/brave-search",
      "npm_downloads": 50744,
      "keywords": [
        "searches",
        "api",
        "search",
        "search api",
        "brave search",
        "web search"
      ],
      "category": "web-search"
    },
    "vinsidious--whodis-mcp-server": {
      "owner": "vinsidious",
      "name": "whodis-mcp-server",
      "url": "https://github.com/vinsidious/whodis-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/vinsidious.webp",
      "description": "Check domain availability using WHOIS lookups integrated as an MCP tool, enabling seamless verification of domain registration status within various workflows.",
      "stars": 4,
      "forks": 2,
      "license": "No License",
      "language": "TypeScript",
      "updated_at": "2025-08-02T07:46:32Z",
      "readme_content": "# Whodis MCP Server\n\nThis project provides a Model Context Protocol (MCP) server specifically designed to check the availability of domain names using WHOIS lookups. It allows AI assistants or other tools to integrate domain availability checks into their workflows.\n\n---\n\n# Overview\n\n## What is MCP?\n\nModel Context Protocol (MCP) is an open standard that allows AI systems to securely and contextually connect with external tools and data sources. This server implements the MCP standard to provide domain availability information.\n\n## Features\n\n-   **Domain Availability Checks**: Uses the `whoiser` library to perform WHOIS lookups and determine if domains appear to be available or registered.\n-   **MCP Tool Integration**: Exposes a `check-domain-availability` tool for MCP clients (like AI assistants).\n-   **CLI Interface**: Includes a command-line interface (`whodis-mcp-server check-domain-availability ...`) for direct usage and testing.\n-   **Structured Logging**: Provides detailed logging for debugging and monitoring.\n-   **Configurable**: Supports configuration via environment variables or `.env` files.\n\n---\n\n# Getting Started\n\n## Prerequisites\n\n-   **Node.js** (>=18.x): [Download](https://nodejs.org/)\n-   **Git**: For version control\n\n---\n\n## Step 1: Clone and Install\n\n```bash\n# Clone the repository\ngit clone https://github.com/vinsidious/whodis-mcp-server.git\ncd whodis-mcp-server\n\n# Install dependencies\nnpm install\n```\n\n---\n\n## Step 2: Run Development Server\n\nStart the server in development mode to interact with it via the MCP Inspector:\n\n```bash\nnpm run dev:server\n```\n\nThis starts the MCP server and enables the MCP Inspector at http://localhost:5173, where you can test the `check-domain-availability` tool.\n\n---\n\n## Step 3: Test the Tool via CLI\n\nRun the domain availability checker directly from the command line:\n\n```bash\n# Using CLI in development mode\nnpm run dev:cli -- check-domain-availability example.com non-existent-domain-12345.org\n\n# Or run the built version\nnpm run start:cli -- check-domain-availability google.com my-unique-idea.dev\n```\n\nThe CLI will output a JSON object containing `available` and `unavailable` arrays.\n\n---\n\n# Architecture\n\nThis server follows a layered architecture:\n\n```\nsrc/\n├── cli/              # Command-line interface logic\n├── controllers/      # Business logic for domain checks\n├── services/         # Interaction with the whoiser library\n├── tools/            # MCP tool definition and argument validation\n├── types/            # Shared type definitions\n├── utils/            # Shared utilities (logging, errors, etc.)\n└── index.ts          # Main entry point for server and CLI\n```\n\n---\n\n# Development Guide\n\n## Development Scripts\n\n```bash\n# Start MCP server in development mode (with MCP Inspector)\nnpm run dev:server\n\n# Run CLI commands in development mode\nnpm run dev:cli -- check-domain-availability <domains...>\n\n# Build the project for production\nnpm run build\n\n# Start MCP server in production mode (requires MCP client)\nnpm run start:server\n\n# Run CLI commands using the production build\nnpm run start:cli -- check-domain-availability <domains...>\n```\n\n## Testing\n\n```bash\n# Run all tests\nnpm test\n\n# Generate test coverage report\nnpm run test:coverage\n```\n\n## Code Quality\n\n```bash\n# Lint code\nnpm run lint\n\n# Format code with Prettier\nnpm run format\n```\n\n---\n\n# MCP Tool: `check-domain-availability`\n\n-   **PURPOSE**: Checks the availability of one or more domain names.\n-   **INPUT**: An array of domain names.\n\t```json\n\t{\n\t\t\"domains\": [\"example.com\", \"another-domain.net\"]\n\t}\n\t```\n-   **OUTPUT**: A JSON object containing two arrays: `available` (domains that appear to be unregistered) and `unavailable` (domains that appear to be registered).\n\t```json\n\t{\n\t\t\"available\": [\"likely-available-domain123.xyz\"],\n\t\t\"unavailable\": [\"google.com\"]\n\t}\n\t```\n\t*Note*: Availability checks depend on WHOIS server responses and might not be 100% accurate for all TLDs or due to temporary network issues. Domains where lookup failed are omitted.\n-   **WHEN TO USE**: Use this tool when you need to determine if specific domain names can potentially be registered.\n\n---\n\n# Debugging\n\n## MCP Inspector\n\nAccess the visual MCP Inspector to test the tool and view request/response details:\n\n1.  Run `npm run dev:server`\n2.  Open http://localhost:5173 in your browser\n3.  Use the UI to call the `check-domain-availability` tool.\n\n## Server Logs\n\nEnable debug logs for detailed output:\n\n```bash\n# Set environment variable\nDEBUG=true npm run dev:server\n\n# Or set DEBUG=true in your .env file\n```\n\nLogs are also saved to files in `~/.mcp/data/whodis-mcp-server.*.log`.\n\n---\n\n# Publishing\n\nTo publish updates to npm:\n\n1.  Ensure changes are committed and follow conventional commit messages (e.g., `feat:`, `fix:`, `chore:`).\n2.  Push changes to the `main` branch.\n3.  The `ci-semantic-release.yml` workflow will automatically build, test, version, and publish the package to npm.\n\n---\n\n# License\n\n[ISC License](https://opensource.org/licenses/ISC)",
      "npm_url": "https://www.npmjs.com/package/whodis-mcp-server",
      "npm_downloads": 354,
      "keywords": [
        "whois",
        "domain",
        "lookups",
        "using whois",
        "whois lookups",
        "check domain"
      ],
      "category": "web-search"
    },
    "vitamin3615--uniprot-mcp-server": {
      "owner": "vitamin3615",
      "name": "uniprot-mcp-server",
      "url": "https://github.com/vitamin3615/uniprot-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/vitamin3615.webp",
      "description": "Fetch protein information seamlessly from the UniProt database, including detailed data, sequences, functions, and structures. Supports integration with MCP-compatible AI applications for enhanced protein analysis.",
      "stars": 0,
      "forks": 0,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-04-01T01:49:15Z",
      "readme_content": "# UniProt MCP Server\n\nAn MCP (Model Context Protocol) server that allows language models to fetch protein information from the UniProt database. This server can be integrated with Claude Desktop and other MCP-compatible AI applications.\n\n## Features\n\n- **Search UniProt**: Search for proteins by name, gene, or other criteria\n- **Get Protein Details**: Retrieve detailed information about a protein\n- **Get Protein Sequences**: Fetch amino acid sequences\n- **Get Protein Functions**: Retrieve functional annotations including GO terms\n- **Get Protein Structures**: Find related PDB structures\n\n## Installation\n\n1. Clone this repository:\n   ```bash\n   git clone https://github.com/yourusername/uniprot-mcp-server.git\n   cd uniprot-mcp-server\n   ```\n\n2. Create a virtual environment (optional but recommended):\n   ```bash\n   python -m venv venv\n   source venv/bin/activate  # On Windows: venv\\Scripts\\activate\n   ```\n\n3. Install dependencies:\n   ```bash\n   pip install -r requirements.txt\n   ```\n\n## Local Setup with Claude Desktop\n\n1. Make the server script executable:\n   ```bash\n   chmod +x server.py\n   ```\n\n2. Configure Claude Desktop to use this MCP server:\n\n   ### For macOS/Linux:\n   ```bash\n   code ~/Library/Application\\ Support/Claude/claude_desktop_config.json\n   ```\n\n   ### For Windows:\n   ```powershell\n   code $env:AppData\\Claude\\claude_desktop_config.json\n   ```\n\n3. Add the following configuration (update the path to match your actual file location):\n\n   ```json\n   {\n     \"mcpServers\": {\n       \"uniprot\": {\n         \"command\": \"python\",\n         \"args\": [\n           \"/ABSOLUTE/PATH/TO/uniprot-mcp-server/server.py\"\n         ]\n       }\n     }\n   }\n   ```\n\n4. Restart Claude Desktop\n\n## Usage Examples\n\nOnce set up with Claude Desktop, you can ask questions like:\n\n- \"Find proteins related to Alzheimer's disease\"\n- \"Get the sequence for protein P05067 (Amyloid-beta precursor protein)\"\n- \"What are the functions of the TP53 protein?\"\n- \"Are there any known 3D structures for BRCA1 protein?\"\n\n## Development\n\nTo run the server directly for testing:\n\n```bash\npython server.py\n```\n\n## Testing Utilities\n\nThis project includes two testing utilities to verify functionality:\n\n### 1. Server Health Check\n\nVerifies that the MCP server starts correctly without errors:\n\n```bash\npython server_health_check.py\n```\n\n### 2. UniProt API Test\n\nTests connectivity with the UniProt API and verifies that all API calls work as expected:\n\n```bash\npython test_server.py\n```\n\nRun these tests before using the server to ensure everything is properly configured.\n\n## License\n\nMIT\n\n## Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n\n1. Fork the repository\n2. Create your feature branch (`git checkout -b feature/amazing-feature`)\n3. Commit your changes (`git commit -m 'Add some amazing feature'`)\n4. Push to the branch (`git push origin feature/amazing-feature`)\n5. Open a Pull Request ",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "protein",
        "vitamin3615",
        "uniprot",
        "vitamin3615 uniprot",
        "protein information",
        "uniprot database"
      ],
      "category": "web-search"
    },
    "w-jeon--mcp-brave-search": {
      "owner": "w-jeon",
      "name": "mcp-brave-search",
      "url": "https://github.com/w-jeon/mcp-brave-search",
      "imageUrl": "/freedevtools/mcp/pfp/w-jeon.webp",
      "description": "Integrates the Brave Search API for web and local search capabilities, enabling detailed queries and results filtering. Offers real-time processing with smart fallbacks between local and web search.",
      "stars": 0,
      "forks": 2,
      "license": "No License",
      "language": "JavaScript",
      "updated_at": "2025-03-12T03:34:08Z",
      "readme_content": "# Brave Search MCP Server\n\nAn MCP server implementation that integrates the Brave Search API, providing both web and local search capabilities.\n\n## Features\n\n- **Web Search**: General queries, news, articles, with pagination and freshness controls\n- **Local Search**: Find businesses, restaurants, and services with detailed information\n- **Flexible Filtering**: Control result types, safety levels, and content freshness\n- **Smart Fallbacks**: Local search automatically falls back to web when no results are found\n\n## Tools\n\n- **brave_web_search**\n  - Execute web searches with pagination and filtering\n  - Inputs:\n    - `query` (string): Search terms\n    - `count` (number, optional): Results per page (max 20)\n    - `offset` (number, optional): Pagination offset (max 9)\n\n- **brave_local_search**\n  - Search for local businesses and services\n  - Inputs:\n    - `query` (string): Local search terms\n    - `count` (number, optional): Number of results (max 20)\n  - Automatically falls back to web search if no local results found\n\n\n## Configuration\n\n### Getting an API Key\n1. Sign up for a [Brave Search API account](https://brave.com/search/api/)\n2. Choose a plan (Free tier available with 2,000 queries/month)\n3. Generate your API key [from the developer dashboard](https://api.search.brave.com/app/keys)\n\n### Usage with Claude Desktop\nAdd this to your `claude_desktop_config.json`:\n\n### Docker\n\n```json\n{\n  \"mcpServers\": {\n    \"brave-search\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"-i\",\n        \"--rm\",\n        \"-e\",\n        \"BRAVE_API_KEY\",\n        \"mcp/brave-search\"\n      ],\n      \"env\": {\n        \"BRAVE_API_KEY\": \"YOUR_API_KEY_HERE\"\n      }\n    }\n  }\n}\n```\n\n### NPX\n\n```json\n{\n  \"mcpServers\": {\n    \"brave-search\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"@modelcontextprotocol/server-brave-search\"\n      ],\n      \"env\": {\n        \"BRAVE_API_KEY\": \"YOUR_API_KEY_HERE\"\n      }\n    }\n  }\n}\n```\n\n\n## Build\n\nDocker build:\n\n```bash\ndocker build -t mcp/brave-search:latest -f src/brave-search/Dockerfile .\n```\n\n## License\n\nThis MCP server is licensed under the MIT License. This means you are free to use, modify, and distribute the software, subject to the terms and conditions of the MIT License. For more details, please see the LICENSE file in the project repository.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "search",
        "api",
        "web",
        "brave search",
        "search api",
        "web search"
      ],
      "category": "web-search"
    },
    "waldzellai--waldzell-mcp": {
      "owner": "waldzellai",
      "name": "waldzell-mcp",
      "url": "https://github.com/waldzellai/waldzell-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/waldzellai.webp",
      "description": "Enable natural language interaction with Yelp's business data, allowing users to search for businesses, read reviews, find events, and explore categories through conversational queries.",
      "stars": 174,
      "forks": 28,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-09-30T01:46:50Z",
      "readme_content": "# Waldzell MCP Servers\n\nThis repository contains a set of Model Context Protocol (MCP) servers. Each server lives in its own folder under `servers/` and can be used independently. The project is intentionally lightweight and does not make use of a complex monorepo toolchain.\n\n## Available servers\n\n- [Clear Thought](./servers/server-clear-thought) – Sequential thinking tools inspired by James Clear\n- [Google Styleguide](./servers/server-google-styleguide) – Google TypeScript style guide server\n- [Stochastic Thinking](./servers/server-stochasticthinking) – Stochastic thinking utilities\n- [TypeStyle](./servers/server-typestyle) – TypeScript style guide server\n\n## Getting started\n\n### Prerequisites\n\n- Node.js 18 or higher\n- npm (comes with Node.js)\n\nInstall dependencies for all servers:\n\n```bash\nnpm install\n```\n\nBuild every server:\n\n```bash\nnpm run build --workspaces\n```\n\nRun tests for all servers:\n\n```bash\nnpm test --workspaces\n```\n\nRefer to each server's README for usage instructions and additional scripts.\n\n## Publishing\n\nTo publish the packages defined in this repository:\n\n```bash\nnpm run build --workspaces && changeset publish\n```\n\n## License\n\nAll code in this repository is licensed under the MIT License.\n\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "waldzell",
        "waldzellai",
        "yelp",
        "yelp business",
        "waldzell mcp",
        "search waldzellai"
      ],
      "category": "web-search"
    },
    "webscraping-ai--webscraping-ai-mcp-server": {
      "owner": "webscraping-ai",
      "name": "webscraping-ai-mcp-server",
      "url": "https://github.com/webscraping-ai/webscraping-ai-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/webscraping-ai.webp",
      "description": "Advanced web data extraction capabilities that support question answering about web content, structured data extraction, and HTML retrieval with JavaScript rendering. Manages concurrent requests effectively and offers features like device emulation and proxy rotation for efficient scraping of dynamic web pages.",
      "stars": 31,
      "forks": 11,
      "license": "No License",
      "language": "JavaScript",
      "updated_at": "2025-09-27T23:32:52Z",
      "readme_content": "# WebScraping.AI MCP Server\n\nA Model Context Protocol (MCP) server implementation that integrates with [WebScraping.AI](https://webscraping.ai) for web data extraction capabilities.\n\n## Features\n\n- Question answering about web page content\n- Structured data extraction from web pages\n- HTML content retrieval with JavaScript rendering\n- Plain text extraction from web pages\n- CSS selector-based content extraction\n- Multiple proxy types (datacenter, residential) with country selection\n- JavaScript rendering using headless Chrome/Chromium\n- Concurrent request management with rate limiting\n- Custom JavaScript execution on target pages\n- Device emulation (desktop, mobile, tablet)\n- Account usage monitoring\n\n## Installation\n\n### Running with npx\n\n```bash\nenv WEBSCRAPING_AI_API_KEY=your_api_key npx -y webscraping-ai-mcp\n```\n\n### Manual Installation\n\n```bash\n# Clone the repository\ngit clone https://github.com/webscraping-ai/webscraping-ai-mcp-server.git\ncd webscraping-ai-mcp-server\n\n# Install dependencies\nnpm install\n\n# Run\nnpm start\n```\n\n### Configuring in Cursor\nNote: Requires Cursor version 0.45.6+\n\nThe WebScraping.AI MCP server can be configured in two ways in Cursor:\n\n1. **Project-specific Configuration** (recommended for team projects):\n   Create a `.cursor/mcp.json` file in your project directory:\n   ```json\n   {\n     \"servers\": {\n       \"webscraping-ai\": {\n         \"type\": \"command\",\n         \"command\": \"npx -y webscraping-ai-mcp\",\n         \"env\": {\n           \"WEBSCRAPING_AI_API_KEY\": \"your-api-key\",\n           \"WEBSCRAPING_AI_CONCURRENCY_LIMIT\": \"5\"\n         }\n       }\n     }\n   }\n   ```\n\n2. **Global Configuration** (for personal use across all projects):\n   Create a `~/.cursor/mcp.json` file in your home directory with the same configuration format as above.\n\n> If you are using Windows and are running into issues, try using `cmd /c \"set WEBSCRAPING_AI_API_KEY=your-api-key && npx -y webscraping-ai-mcp\"` as the command.\n\nThis configuration will make the WebScraping.AI tools available to Cursor's AI agent automatically when relevant for web scraping tasks.\n\n### Running on Claude Desktop\n\nAdd this to your `claude_desktop_config.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"mcp-server-webscraping-ai\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"webscraping-ai-mcp\"],\n      \"env\": {\n        \"WEBSCRAPING_AI_API_KEY\": \"YOUR_API_KEY_HERE\",\n        \"WEBSCRAPING_AI_CONCURRENCY_LIMIT\": \"5\"\n      }\n    }\n  }\n}\n```\n\n## Configuration\n\n### Environment Variables\n\n#### Required\n\n- `WEBSCRAPING_AI_API_KEY`: Your WebScraping.AI API key\n  - Required for all operations\n  - Get your API key from [WebScraping.AI](https://webscraping.ai)\n\n#### Optional Configuration\n- `WEBSCRAPING_AI_CONCURRENCY_LIMIT`: Maximum number of concurrent requests (default: `5`)\n- `WEBSCRAPING_AI_DEFAULT_PROXY_TYPE`: Type of proxy to use (default: `residential`)\n- `WEBSCRAPING_AI_DEFAULT_JS_RENDERING`: Enable/disable JavaScript rendering (default: `true`)\n- `WEBSCRAPING_AI_DEFAULT_TIMEOUT`: Maximum web page retrieval time in ms (default: `15000`, max: `30000`)\n- `WEBSCRAPING_AI_DEFAULT_JS_TIMEOUT`: Maximum JavaScript rendering time in ms (default: `2000`)\n\n### Configuration Examples\n\nFor standard usage:\n```bash\n# Required\nexport WEBSCRAPING_AI_API_KEY=your-api-key\n\n# Optional - customize behavior (default values)\nexport WEBSCRAPING_AI_CONCURRENCY_LIMIT=5\nexport WEBSCRAPING_AI_DEFAULT_PROXY_TYPE=residential # datacenter or residential\nexport WEBSCRAPING_AI_DEFAULT_JS_RENDERING=true\nexport WEBSCRAPING_AI_DEFAULT_TIMEOUT=15000\nexport WEBSCRAPING_AI_DEFAULT_JS_TIMEOUT=2000\n```\n\n## Available Tools\n\n### 1. Question Tool (`webscraping_ai_question`)\n\nAsk questions about web page content.\n\n```json\n{\n  \"name\": \"webscraping_ai_question\",\n  \"arguments\": {\n    \"url\": \"https://example.com\",\n    \"question\": \"What is the main topic of this page?\",\n    \"timeout\": 30000,\n    \"js\": true,\n    \"js_timeout\": 2000,\n    \"wait_for\": \".content-loaded\",\n    \"proxy\": \"datacenter\",\n    \"country\": \"us\"\n  }\n}\n```\n\nExample response:\n\n```json\n{\n  \"content\": [\n    {\n      \"type\": \"text\",\n      \"text\": \"The main topic of this page is examples and documentation for HTML and web standards.\"\n    }\n  ],\n  \"isError\": false\n}\n```\n\n### 2. Fields Tool (`webscraping_ai_fields`)\n\nExtract structured data from web pages based on instructions.\n\n```json\n{\n  \"name\": \"webscraping_ai_fields\",\n  \"arguments\": {\n    \"url\": \"https://example.com/product\",\n    \"fields\": {\n      \"title\": \"Extract the product title\",\n      \"price\": \"Extract the product price\",\n      \"description\": \"Extract the product description\"\n    },\n    \"js\": true,\n    \"timeout\": 30000\n  }\n}\n```\n\nExample response:\n\n```json\n{\n  \"content\": [\n    {\n      \"type\": \"text\",\n      \"text\": {\n        \"title\": \"Example Product\",\n        \"price\": \"$99.99\",\n        \"description\": \"This is an example product description.\"\n      }\n    }\n  ],\n  \"isError\": false\n}\n```\n\n### 3. HTML Tool (`webscraping_ai_html`)\n\nGet the full HTML of a web page with JavaScript rendering.\n\n```json\n{\n  \"name\": \"webscraping_ai_html\",\n  \"arguments\": {\n    \"url\": \"https://example.com\",\n    \"js\": true,\n    \"timeout\": 30000,\n    \"wait_for\": \"#content-loaded\"\n  }\n}\n```\n\nExample response:\n\n```json\n{\n  \"content\": [\n    {\n      \"type\": \"text\",\n      \"text\": \"<html>...[full HTML content]...</html>\"\n    }\n  ],\n  \"isError\": false\n}\n```\n\n### 4. Text Tool (`webscraping_ai_text`)\n\nExtract the visible text content from a web page.\n\n```json\n{\n  \"name\": \"webscraping_ai_text\",\n  \"arguments\": {\n    \"url\": \"https://example.com\",\n    \"js\": true,\n    \"timeout\": 30000\n  }\n}\n```\n\nExample response:\n\n```json\n{\n  \"content\": [\n    {\n      \"type\": \"text\",\n      \"text\": \"Example Domain\\nThis domain is for use in illustrative examples in documents...\"\n    }\n  ],\n  \"isError\": false\n}\n```\n\n### 5. Selected Tool (`webscraping_ai_selected`)\n\nExtract content from a specific element using a CSS selector.\n\n```json\n{\n  \"name\": \"webscraping_ai_selected\",\n  \"arguments\": {\n    \"url\": \"https://example.com\",\n    \"selector\": \"div.main-content\",\n    \"js\": true,\n    \"timeout\": 30000\n  }\n}\n```\n\nExample response:\n\n```json\n{\n  \"content\": [\n    {\n      \"type\": \"text\",\n      \"text\": \"<div class=\\\"main-content\\\">This is the main content of the page.</div>\"\n    }\n  ],\n  \"isError\": false\n}\n```\n\n### 6. Selected Multiple Tool (`webscraping_ai_selected_multiple`)\n\nExtract content from multiple elements using CSS selectors.\n\n```json\n{\n  \"name\": \"webscraping_ai_selected_multiple\",\n  \"arguments\": {\n    \"url\": \"https://example.com\",\n    \"selectors\": [\"div.header\", \"div.product-list\", \"div.footer\"],\n    \"js\": true,\n    \"timeout\": 30000\n  }\n}\n```\n\nExample response:\n\n```json\n{\n  \"content\": [\n    {\n      \"type\": \"text\",\n      \"text\": [\n        \"<div class=\\\"header\\\">Header content</div>\",\n        \"<div class=\\\"product-list\\\">Product list content</div>\",\n        \"<div class=\\\"footer\\\">Footer content</div>\"\n      ]\n    }\n  ],\n  \"isError\": false\n}\n```\n\n### 7. Account Tool (`webscraping_ai_account`)\n\nGet information about your WebScraping.AI account.\n\n```json\n{\n  \"name\": \"webscraping_ai_account\",\n  \"arguments\": {}\n}\n```\n\nExample response:\n\n```json\n{\n  \"content\": [\n    {\n      \"type\": \"text\",\n      \"text\": {\n        \"requests\": 5000,\n        \"remaining\": 4500,\n        \"limit\": 10000,\n        \"resets_at\": \"2023-12-31T23:59:59Z\"\n      }\n    }\n  ],\n  \"isError\": false\n}\n```\n\n## Common Options for All Tools\n\nThe following options can be used with all scraping tools:\n\n- `timeout`: Maximum web page retrieval time in ms (15000 by default, maximum is 30000)\n- `js`: Execute on-page JavaScript using a headless browser (true by default)\n- `js_timeout`: Maximum JavaScript rendering time in ms (2000 by default)\n- `wait_for`: CSS selector to wait for before returning the page content\n- `proxy`: Type of proxy, datacenter or residential (residential by default)\n- `country`: Country of the proxy to use (US by default). Supported countries: us, gb, de, it, fr, ca, es, ru, jp, kr, in\n- `custom_proxy`: Your own proxy URL in \"http://user:password@host:port\" format\n- `device`: Type of device emulation. Supported values: desktop, mobile, tablet\n- `error_on_404`: Return error on 404 HTTP status on the target page (false by default)\n- `error_on_redirect`: Return error on redirect on the target page (false by default)\n- `js_script`: Custom JavaScript code to execute on the target page\n\n## Error Handling\n\nThe server provides robust error handling:\n\n- Automatic retries for transient errors\n- Rate limit handling with backoff\n- Detailed error messages\n- Network resilience\n\nExample error response:\n\n```json\n{\n  \"content\": [\n    {\n      \"type\": \"text\",\n      \"text\": \"API Error: 429 Too Many Requests\"\n    }\n  ],\n  \"isError\": true\n}\n```\n\n## Integration with LLMs\n\nThis server implements the [Model Context Protocol](https://github.com/facebookresearch/modelcontextprotocol), making it compatible with any MCP-enabled LLM platforms. You can configure your LLM to use these tools for web scraping tasks.\n\n### Example: Configuring Claude with MCP\n\n```javascript\nconst { Claude } = require('@anthropic-ai/sdk');\nconst { Client } = require('@modelcontextprotocol/sdk/client/index.js');\nconst { StdioClientTransport } = require('@modelcontextprotocol/sdk/client/stdio.js');\n\nconst claude = new Claude({\n  apiKey: process.env.ANTHROPIC_API_KEY\n});\n\nconst transport = new StdioClientTransport({\n  command: 'npx',\n  args: ['-y', 'webscraping-ai-mcp'],\n  env: {\n    WEBSCRAPING_AI_API_KEY: 'your-api-key'\n  }\n});\n\nconst client = new Client({\n  name: 'claude-client',\n  version: '1.0.0'\n});\n\nawait client.connect(transport);\n\n// Now you can use Claude with WebScraping.AI tools\nconst tools = await client.listTools();\nconst response = await claude.complete({\n  prompt: 'What is the main topic of example.com?',\n  tools: tools\n});\n```\n\n## Development\n\n```bash\n# Clone the repository\ngit clone https://github.com/webscraping-ai/webscraping-ai-mcp-server.git\ncd webscraping-ai-mcp-server\n\n# Install dependencies\nnpm install\n\n# Run tests\nnpm test\n\n# Add your .env file\ncp .env.example .env\n\n# Start the inspector\nnpx @modelcontextprotocol/inspector node src/index.js\n```\n\n### Contributing\n\n1. Fork the repository\n2. Create your feature branch\n3. Run tests: `npm test`\n4. Submit a pull request\n\n## License\n\nMIT License - see LICENSE file for details \n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "webscraping",
        "scraping",
        "web",
        "ai webscraping",
        "webscraping ai",
        "search webscraping"
      ],
      "category": "web-search"
    },
    "weidwonder--crawl4ai-mcp-server": {
      "owner": "weidwonder",
      "name": "crawl4ai-mcp-server",
      "url": "https://github.com/weidwonder/crawl4ai-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/weidwonder.webp",
      "description": "Provides powerful search capabilities optimized for LLMs to understand and extract valuable content from web pages using multiple search engines. Efficiently filters non-core content and formats outputs for AI processing.",
      "stars": 118,
      "forks": 23,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-27T04:12:21Z",
      "readme_content": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/weidwonder-crawl4ai-mcp-server-badge.png)](https://mseep.ai/app/weidwonder-crawl4ai-mcp-server)\n\n# Crawl4AI MCP Server\n\n[![smithery badge](https://smithery.ai/badge/@weidwonder/crawl4ai-mcp-server)](https://smithery.ai/server/@weidwonder/crawl4ai-mcp-server)\n\n这是一个基于MCP (Model Context Protocol)的智能信息获取服务器,为AI助手系统提供强大的搜索能力和面向LLM优化的网页内容理解功能。通过多引擎搜索和智能内容提取,帮助AI系统高效获取和理解互联网信息,将网页内容转换为最适合LLM处理的格式。\n\n## 特性\n\n- 🔍 强大的多引擎搜索能力,支持DuckDuckGo和Google\n- 📚 面向LLM优化的网页内容提取,智能过滤非核心内容\n- 🎯 专注信息价值,自动识别和保留关键内容\n- 📝 多种输出格式,支持引用溯源\n- 🚀 基于FastMCP的高性能异步设计\n\n## 安装\n\n### 方式1: 大部分的安装场景\n\n1. 确保您的系统满足以下要求:\n   - Python >= 3.9\n   - 建议使用专门的虚拟环境\n\n2. 克隆仓库:\n```bash\ngit clone https://github.com/yourusername/crawl4ai-mcp-server.git\ncd crawl4ai-mcp-server\n```\n\n3. 创建并激活虚拟环境:\n```bash\npython -m venv crawl4ai_env\nsource crawl4ai_env/bin/activate  # Linux/Mac\n# 或\n.\\crawl4ai_env\\Scripts\\activate  # Windows\n```\n\n4. 安装依赖:\n```bash\npip install -r requirements.txt\n```\n\n5. 安装playwright浏览器:\n```bash\nplaywright install\n```\n\n### 方式2: 安装到Claude桌面客户端 via Smithery\n\n通过 [Smithery](https://smithery.ai/server/@weidwonder/crawl4ai-mcp-server) 将 Crawl4AI MCP 的 Claude 桌面端服务安装自动配置至您本地的 `Claude 伸展中心`:\n\n```bash\nnpx -y @smithery/cli install @weidwonder/crawl4ai-mcp-server --client claude\n```\n\n## 使用方法\n\n服务器提供以下工具:\n\n### search\n强大的网络搜索工具,支持多个搜索引擎:\n\n- DuckDuckGo搜索(默认): 无需API密钥,全面处理AbstractText、Results和RelatedTopics\n- Google搜索: 需要配置API密钥,提供精准搜索结果\n- 支持同时使用多个引擎获取更全面的结果\n\n参数说明:\n- `query`: 搜索查询字符串\n- `num_results`: 返回结果数量(默认10)\n- `engine`: 搜索引擎选择\n  - \"duckduckgo\": DuckDuckGo搜索(默认)\n  - \"google\": Google搜索(需要API密钥)\n  - \"all\": 同时使用所有可用的搜索引擎\n\n示例:\n```python\n# DuckDuckGo搜索(默认)\n{\n    \"query\": \"python programming\",\n    \"num_results\": 5\n}\n\n# 使用所有可用引擎\n{\n    \"query\": \"python programming\",\n    \"num_results\": 5,\n    \"engine\": \"all\"\n}\n```\n\n### read_url\n面向LLM优化的网页内容理解工具,提供智能内容提取和格式转换:\n\n- `markdown_with_citations`: 包含内联引用的Markdown(默认),保持信息溯源\n- `fit_markdown`: 经过LLM优化的精简内容,去除冗余信息\n- `raw_markdown`: 基础HTML→Markdown转换\n- `references_markdown`: 单独的引用/参考文献部分\n- `fit_html`: 生成fit_markdown的过滤后HTML\n- `markdown`: 默认Markdown格式\n\n示例:\n```python\n{\n    \"url\": \"https://example.com\",\n    \"format\": \"markdown_with_citations\"\n}\n```\n\n示例:\n```python\n# DuckDuckGo搜索(默认)\n{\n    \"query\": \"python programming\",\n    \"num_results\": 5\n}\n\n# Google搜索\n{\n    \"query\": \"python programming\",\n    \"num_results\": 5,\n    \"engine\": \"google\"\n}\n```\n\n如需使用Google搜索,需要在config.json中配置API密钥:\n```json\n{\n    \"google\": {\n        \"api_key\": \"your-api-key\",\n        \"cse_id\": \"your-cse-id\"\n    }\n}\n```\n\n## LLM内容优化\n\n服务器采用了一系列针对LLM的内容优化策略:\n\n- 智能内容识别: 自动识别并保留文章主体、关键信息段落\n- 噪音过滤: 自动过滤导航栏、广告、页脚等对理解无帮助的内容\n- 信息完整性: 保留URL引用,支持信息溯源\n- 长度优化: 使用最小词数阈值(10)过滤无效片段\n- 格式优化: 默认输出markdown_with_citations格式,便于LLM理解和引用\n\n## 开发说明\n\n项目结构:\n```\ncrawl4ai_mcp_server/\n├── src/\n│   ├── index.py      # 服务器主实现\n│   └── search.py     # 搜索功能实现\n├── config_demo.json  # 配置文件示例\n├── pyproject.toml    # 项目配置\n├── requirements.txt  # 依赖列表\n└── README.md        # 项目文档\n```\n\n## 配置说明\n\n1. 复制配置示例文件:\n```bash\ncp config_demo.json config.json\n```\n\n2. 如需使用Google搜索,在config.json中配置API密钥:\n```json\n{\n    \"google\": {\n        \"api_key\": \"your-google-api-key\",\n        \"cse_id\": \"your-google-cse-id\"\n    }\n}\n```\n\n## 更新日志\n\n- 2025.02.08: 添加搜索功能,支持DuckDuckGo(默认)和Google搜索\n- 2025.02.07: 重构项目结构,使用FastMCP实现,优化依赖管理\n- 2025.02.07: 优化内容过滤配置,提高token效率并保持URL完整性\n\n## 许可证\n\nMIT License\n\n## 贡献\n\n欢迎提交Issue和Pull Request!\n\n## 作者\n\n- Owner: weidwonder  \n- Coder: Claude Sonnet 3.5 \n    - 100% Code wrote by Claude. Cost: $9 ($2 for code writing, $7 cost for Debuging😭)\n    - 3 hours time cost. 0.5 hours for code writing, 0.5 hours for env preparing, 2 hours for debuging.😭\n\n## 致谢\n\n感谢所有为项目做出贡献的开发者!\n\n特别感谢:\n- [Crawl4ai](https://github.com/crawl4ai/crawl4ai) 项目提供的优秀网页内容提取技术支持",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "search",
        "crawl4ai",
        "web",
        "search engines",
        "crawl4ai mcp",
        "web search"
      ],
      "category": "web-search"
    },
    "wendellbigato--mcp-cep": {
      "owner": "wendellbigato",
      "name": "mcp-cep",
      "url": "https://github.com/wendellbigato/mcp-cep",
      "imageUrl": "/freedevtools/mcp/pfp/wendellbigato.webp",
      "description": "Provides postal code (CEP) address lookups using the ViaCEP API. Integrates with command-line tools for efficient address retrieval tasks.",
      "stars": 3,
      "forks": 3,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-30T12:12:49Z",
      "readme_content": "# MCP-CEP\n\nServidor MCP para consulta de CEPs usando a API pública do [ViaCEP](https://viacep.com.br).  \nCompatível com [Goose](https://block.github.io/goose/) como extensão de linha de comando (Command-line Extension).\n\n---\n\n### 👤 Autor\n\n**Wendell Barreto**  \n[https://github.com/wendellbigato](https://github.com/wendellbigato)  \n\n\n---\n\n## 🚀 Instalação\n\n### 1. Clone este repositório\n\n```bash\ngit clone https://github.com/wendellbigato/mcp-cep.git\ncd mcp-cep\n````\n\n> Altere a URL acima para a real quando publicar.\n\n---\n\n### 2. Crie e ative o ambiente virtual\n\n```bash\npython3.11 -m venv .venv\nsource .venv/bin/activate  # Linux/macOS\n# ou\n.venv\\Scripts\\activate.bat  # Windows\n```\n\n---\n\n### 3. Instale as dependências com `uv` ou `pip`\n\nUsando [uv](https://github.com/astral-sh/uv):\n\n```bash\nuv pip install -e \".[cli]\"\n```\n\nOu com pip normal:\n\n```bash\npip install -e \".[cli]\"\n```\n\n---\n\n## 🧩 Configurando como extensão no Goose\n\n1. Execute:\n\n```bash\ngoose configure\n```\n\n2. Selecione `Add Extension`\n\n3. Escolha `Command-line Extension`\n\n4. Preencha os campos:\n\n| Campo              | Valor                                                                     |\n| ------------------ | ------------------------------------------------------------------------- |\n| **Extension name** | `mcp-cep`                                                                 |\n| **Command to run** | `/caminho/completo/para/uv --directory /caminho/para/mcp-cep run main.py` |\n| **Timeout**        | `300`                                                                     |\n| **Environment**    | *(em branco, ou personalize se necessário)*                               |\n\nUse `which uv` e `pwd` para encontrar os caminhos corretos.\n\n---\n\n## ✅ Como testar no Goose\n\nApós configurar, inicie:\n\n```bash\ngoose \n```\n\nE envie comandos como:\n\n```\nRepita: Olá!\nQual o endereço do CEP 01001000?\n```\n\n---\n\n## 🧰 Ferramentas disponíveis\n\n* `echo(texto: str)`: repete o texto enviado.\n* `buscar_cep(cep: str)`: consulta informações de endereço via ViaCEP.\n\n---\n\n## 📄 Licença\n\nEste projeto está licenciado sob a licença MIT.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "postal",
        "address",
        "lookups",
        "address lookups",
        "cep address",
        "address retrieval"
      ],
      "category": "web-search"
    },
    "wenpingwu001--mcp-browser-use": {
      "owner": "wenpingwu001",
      "name": "mcp-browser-use",
      "url": "https://github.com/wenpingwu001/mcp-browser-use",
      "imageUrl": "/freedevtools/mcp/pfp/wenpingwu001.webp",
      "description": "Automate web browsing tasks using natural language commands to control browsers, conduct research, and generate reports. This server facilitates seamless integration of AI-driven browser automation into various workflows.",
      "stars": 0,
      "forks": 0,
      "license": "MIT License",
      "language": "",
      "updated_at": "2025-04-09T08:04:57Z",
      "readme_content": "<br/>\n\n# browser-use MCP server\n[![Documentation](https://img.shields.io/badge/Documentation-📕-blue)](https://docs.browser-use.com)\n[![License](https://img.shields.io/badge/License-MIT-green)](LICENSE)\n\n> **Project Note**: This MCP server implementation builds upon the [browser-use/web-ui](https://github.com/browser-use/web-ui) foundation. Core browser automation logic and configuration patterns are adapted from the original project.\n\nAI-driven browser automation server implementing the Model Context Protocol (MCP) for natural language browser control and web research.\n\n<a href=\"https://glama.ai/mcp/servers/@Saik0s/mcp-browser-use\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@Saik0s/mcp-browser-use/badge\" alt=\"Browser-Use MCP server\" /></a>\n\n## Features\n\n-   🧠 **MCP Integration** - Full protocol implementation for AI agent communication.\n-   🌐 **Browser Automation** - Page navigation, form filling, element interaction via natural language (`run_browser_agent` tool).\n-   👁️ **Visual Understanding** - Optional screenshot analysis for vision-capable LLMs.\n-   🔄 **State Persistence** - Option to manage a browser session across multiple MCP calls or connect to user's browser.\n-   🔌 **Multi-LLM Support** - Integrates with OpenAI, Anthropic, Azure, DeepSeek, Google, Mistral, Ollama, OpenRouter, Alibaba, Moonshot, Unbound AI.\n-   🔍 **Deep Research Tool** - Dedicated tool for multi-step web research and report generation (`run_deep_search` tool).\n-   ⚙️ **Environment Variable Configuration** - Fully configurable via environment variables.\n-   🔗 **CDP Connection** - Ability to connect to and control a user-launched Chrome/Chromium instance via Chrome DevTools Protocol.\n\n## Quick Start\n\n### Prerequisites\n\n-   Python 3.11 or higher\n-   `uv` (fast Python package installer): `pip install uv`\n-   Chrome/Chromium browser installed\n-   Install Playwright browsers: `uv sync` and then `uv run playwright install`\n\n### Integration with MCP Clients (e.g., Claude Desktop)\n\nYou can configure clients like Claude Desktop to connect to this server. Add the following structure to the client's configuration (e.g., `claude_desktop_config.json`), adjusting the path and environment variables as needed:\n\n```json\n// Example for Claude Desktop config\n\"mcpServers\": {\n    \"browser-use\": {\n      // Option 1: Run installed package\n      // \"command\": \"uvx\",\n      // \"args\": [\"mcp-server-browser-use\"],\n\n      // Option 2: Run from local development source\n      \"command\": \"uvx\",\n      \"args\": [\n        \"mcp-server-browser-use\"\n      ],\n      \"env\": {\n        // --- CRITICAL: Add required API keys here ---\n        \"OPENAI_API_KEY\": \"YOUR_KEY_HERE_IF_USING_OPENAI\",\n        \"ANTHROPIC_API_KEY\": \"YOUR_KEY_HERE_IF_USING_ANTHROPIC\",\n        // ... add other keys based on MCP_MODEL_PROVIDER ...\n\n        // --- Optional Overrides (defaults are usually fine) ---\n        \"MCP_MODEL_PROVIDER\": \"anthropic\", // Default provider\n        \"MCP_MODEL_NAME\": \"claude-3-7-sonnet-20250219\", // Default model\n        \"BROWSER_HEADLESS\": \"true\",    // Default: run browser without UI\n        \"BROWSER_USE_LOGGING_LEVEL\": \"INFO\",\n\n        // --- Example for connecting to your own browser ---\n        // \"MCP_USE_OWN_BROWSER\": \"true\",\n        // \"CHROME_CDP\": \"http://localhost:9222\",\n\n        // Ensure Python uses UTF-8\n        \"PYTHONIOENCODING\": \"utf-8\",\n        \"PYTHONUNBUFFERED\": \"1\",\n        \"PYTHONUTF8\": \"1\"\n      }\n    }\n}\n```\n\n**Important:** Ensure the `command` and `args` correctly point to how you want to run the server (either the installed package or from the source directory). Set the necessary API keys in the `env` section.\n\n## MCP Tools\n\nThis server exposes the following tools via the Model Context Protocol:\n\n### Synchronous Tools (Wait for Completion)\n\n1.  **`run_browser_agent`**\n    *   **Description:** Executes a browser automation task based on natural language instructions and waits for it to complete. Uses settings prefixed with `MCP_` (e.g., `MCP_HEADLESS`, `MCP_MAX_STEPS`).\n    *   **Arguments:**\n        *   `task` (string, required): The primary task or objective.\n        *   `add_infos` (string, optional): Additional context or hints for the agent (used by `custom` agent type).\n    *   **Returns:** (string) The final result extracted by the agent or an error message.\n\n2.  **`run_deep_search`**\n    *   **Description:** Performs in-depth web research on a topic, generates a report, and waits for completion. Uses settings prefixed with `MCP_RESEARCH_` and general `BROWSER_` settings (e.g., `BROWSER_HEADLESS`).\n    *   **Arguments:**\n        *   `research_task` (string, required): The topic or question for the research.\n        *   `max_search_iterations` (integer, optional, default: 10): Max search cycles.\n        *   `max_query_per_iteration` (integer, optional, default: 3): Max search queries per cycle.\n    *   **Returns:** (string) The generated research report in Markdown format, including the file path, or an error message.\n\n## Configuration (Environment Variables)\n\nConfigure the server using environment variables. You can set these in your system or place them in a `.env` file in the project root.\n\n| Variable                        | Description                                                                                             | Required?                      | Default Value                     | Example Value                     |\n| :------------------------------ | :------------------------------------------------------------------------------------------------------ | :----------------------------- | :-------------------------------- | :-------------------------------- |\n| **LLM Settings**                |                                                                                                         |                                |                                   |                                   |\n| `MCP_MODEL_PROVIDER`            | LLM provider to use. See options below.                                                                 | **Yes**                        | `anthropic`                       | `openai`                          |\n| `MCP_MODEL_NAME`                | Specific model name for the chosen provider.                                                            | No                             | `claude-3-7-sonnet-20250219`      | `gpt-4o`                          |\n| `MCP_TEMPERATURE`               | LLM temperature (0.0-2.0). Controls randomness.                                                         | No                             | `0.0`                             | `0.7`                             |\n| `MCP_TOOL_CALLING_METHOD`       | Method for tool invocation ('auto', 'json_schema', 'function_calling'). Affects `run_browser_agent`.    | No                             | `auto`                            | `json_schema`                     |\n| `MCP_MAX_INPUT_TOKENS`          | Max input tokens for LLM context for `run_browser_agent`.                                               | No                             | `128000`                          | `64000`                           |\n| `MCP_BASE_URL`                  | Optional: Generic override for the LLM provider's base URL.                                             | No                             | Provider-specific                 | `http://localhost:8080/v1`        |\n| `MCP_API_KEY`                   | Optional: Generic override for the LLM provider's API key (takes precedence over provider-specific keys). | No                             | -                                 | `sk-...`                          |\n| **Provider API Keys**           | **Required based on `MCP_MODEL_PROVIDER` unless `MCP_API_KEY` is set.**                                 |                                |                                   |                                   |\n| `OPENAI_API_KEY`                | API Key for OpenAI.                                                                                     | If Used                        | -                                 | `sk-...`                          |\n| `ANTHROPIC_API_KEY`             | API Key for Anthropic.                                                                                  | If Used                        | -                                 | `sk-ant-...`                      |\n| `GOOGLE_API_KEY`                | API Key for Google AI (Gemini).                                                                         | If Used                        | -                                 | `AIza...`                         |\n| `AZURE_OPENAI_API_KEY`          | API Key for Azure OpenAI.                                                                               | If Used                        | -                                 | `...`                             |\n| `DEEPSEEK_API_KEY`              | API Key for DeepSeek.                                                                                   | If Used                        | -                                 | `sk-...`                          |\n| `MISTRAL_API_KEY`               | API Key for Mistral AI.                                                                                 | If Used                        | -                                 | `...`                             |\n| `OPENROUTER_API_KEY`            | API Key for OpenRouter.                                                                                 | If Used                        | -                                 | `sk-or-...`                       |\n| `ALIBABA_API_KEY`               | API Key for Alibaba Cloud (DashScope).                                                                  | If Used                        | -                                 | `sk-...`                          |\n| `MOONSHOT_API_KEY`              | API Key for Moonshot AI.                                                                                | If Used                        | -                                 | `sk-...`                          |\n| `UNBOUND_API_KEY`               | API Key for Unbound AI.                                                                                 | If Used                        | -                                 | `...`                             |\n| **Provider Endpoints**          | Optional: Override default API endpoints.                                                               |                                |                                   |                                   |\n| `OPENAI_ENDPOINT`               | OpenAI API endpoint URL.                                                                                | No                             | `https://api.openai.com/v1`       |                                   |\n| `ANTHROPIC_ENDPOINT`            | Anthropic API endpoint URL.                                                                             | No                             | `https://api.anthropic.com`       |                                   |\n| `AZURE_OPENAI_ENDPOINT`         | **Required if using Azure.** Your Azure resource endpoint.                                              | If Used                        | -                                 | `https://res.openai.azure.com/` |\n| `AZURE_OPENAI_API_VERSION`      | Azure API version.                                                                                      | No                             | `2025-01-01-preview`              | `2023-12-01-preview`              |\n| `DEEPSEEK_ENDPOINT`             | DeepSeek API endpoint URL.                                                                              | No                             | `https://api.deepseek.com`        |                                   |\n| `MISTRAL_ENDPOINT`              | Mistral API endpoint URL.                                                                               | No                             | `https://api.mistral.ai/v1`       |                                   |\n| `OLLAMA_ENDPOINT`               | Ollama API endpoint URL.                                                                                | No                             | `http://localhost:11434`          | `http://ollama.local:11434`       |\n| `OPENROUTER_ENDPOINT`           | OpenRouter API endpoint URL.                                                                            | No                             | `https://openrouter.ai/api/v1`    |                                   |\n| `ALIBABA_ENDPOINT`              | Alibaba (DashScope) API endpoint URL.                                                                   | No                             | `https://dashscope...v1`          |                                   |\n| `MOONSHOT_ENDPOINT`             | Moonshot API endpoint URL.                                                                              | No                             | `https://api.moonshot.cn/v1`      |                                   |\n| `UNBOUND_ENDPOINT`              | Unbound AI API endpoint URL.                                                                            | No                             | `https://api.getunbound.ai`       |                                   |\n| **Ollama Specific**             |                                                                                                         |                                |                                   |                                   |\n| `OLLAMA_NUM_CTX`                | Context window size for Ollama models.                                                                  | No                             | `32000`                           | `8192`                            |\n| `OLLAMA_NUM_PREDICT`            | Max tokens to predict for Ollama models.                                                                | No                             | `1024`                            | `2048`                            |\n| **Agent Settings (`run_browser_agent`)** |                                                                                                 |                                |                                   |                                   |\n| `MCP_AGENT_TYPE`                | Agent implementation for `run_browser_agent` ('org' or 'custom').                                       | No                             | `org`                             | `custom`                          |\n| `MCP_MAX_STEPS`                 | Max steps per agent run.                                                                                | No                             | `100`                             | `50`                              |\n| `MCP_USE_VISION`                | Enable vision capabilities (screenshot analysis).                                                       | No                             | `true`                            | `false`                           |\n| `MCP_MAX_ACTIONS_PER_STEP`      | Max actions per agent step.                                                                             | No                             | `5`                               | `10`                              |\n| `MCP_KEEP_BROWSER_OPEN`         | Keep browser managed by server open between `run_browser_agent` calls (if `MCP_USE_OWN_BROWSER=false`). | No                             | `false`                           | `true`                            |\n| `MCP_ENABLE_RECORDING`          | Enable Playwright video recording for `run_browser_agent`.                                              | No                             | `false`                           | `true`                            |\n| `MCP_SAVE_RECORDING_PATH`       | Path to save agent run video recordings (Required if `MCP_ENABLE_RECORDING=true`).                      | If Recording                   | -                                 | `./tmp/recordings`                |\n| `MCP_AGENT_HISTORY_PATH`        | Directory to save agent history JSON files.                                                             | No                             | `./tmp/agent_history`             | `./agent_runs`                    |\n| `MCP_HEADLESS`                  | Run browser without UI specifically for `run_browser_agent` tool.                                       | No                             | `true`                            | `false`                           |\n| `MCP_DISABLE_SECURITY`          | Disable browser security features specifically for `run_browser_agent` tool (use cautiously).           | No                             | `true`                            | `false`                           |\n| **Deep Research Settings (`run_deep_search`)** |                                                                                         |                                |                                   |                                   |\n| `MCP_RESEARCH_MAX_ITERATIONS`   | Max search iterations for deep research.                                                                | No                             | `10`                              | `5`                               |\n| `MCP_RESEARCH_MAX_QUERY`        | Max search queries per iteration.                                                                       | No                             | `3`                               | `5`                               |\n| `MCP_RESEARCH_USE_OWN_BROWSER`  | Use a separate browser instance for research (requires `CHROME_CDP` if `MCP_USE_OWN_BROWSER=true`).     | No                             | `false`                           | `true`                            |\n| `MCP_RESEARCH_SAVE_DIR`         | Directory to save research artifacts (report, results).                                                 | No                             | `./tmp/deep_research/{task_id}`   | `./research_output`               |\n| `MCP_RESEARCH_AGENT_MAX_STEPS`  | Max steps for sub-agents within deep research.                                                          | No                             | `10`                              | `15`                              |\n| **Browser Settings (General & Specific Tool Overrides)** |                                                                                |                                |                                   |                                   |\n| `MCP_USE_OWN_BROWSER`           | Set to true to connect to user's browser via `CHROME_CDP` instead of launching a new one.               | No                             | `false`                           | `true`                            |\n| `CHROME_CDP`                    | Connect to existing Chrome via DevTools Protocol URL. Required if `MCP_USE_OWN_BROWSER=true`.           | If `MCP_USE_OWN_BROWSER=true`  | -                                 | `http://localhost:9222`           |\n| `BROWSER_HEADLESS`              | Run browser without visible UI. Primarily affects `run_deep_search`. See also `MCP_HEADLESS`.           | No                             | `true`                            | `false`                           |\n| `BROWSER_DISABLE_SECURITY`      | General browser security setting. See also `MCP_DISABLE_SECURITY`.                                      | No                             | `false`                           | `true`                            |\n| `CHROME_PATH`                   | Path to Chrome/Chromium executable.                                                                     | No                             | -                                 | `/usr/bin/chromium-browser`       |\n| `CHROME_USER_DATA`              | Path to Chrome user data directory (for persistent sessions, useful with `CHROME_CDP`).                 | No                             | -                                 | `~/.config/google-chrome/Profile 1` |\n| `BROWSER_TRACE_PATH`            | Directory to save Playwright trace files (useful for debugging).                                        | No                             | `./tmp/trace`                     | `./traces`                        |\n| `BROWSER_WINDOW_WIDTH`          | Browser window width (pixels).                                                                          | No                             | `1280`                            | `1920`                            |\n| `BROWSER_WINDOW_HEIGHT`         | Browser window height (pixels).                                                                         | No                             | `720`                             | `1080`                            |\n| **Server & Logging**            |                                                                                                         |                                |                                   |                                   |\n| `LOG_FILE`                      | Path for the server log file.                                                                           | No                             | `mcp_server_browser_use.log`      | `/var/log/mcp_browser.log`        |\n| `BROWSER_USE_LOGGING_LEVEL`     | Logging level (`DEBUG`, `INFO`, `WARNING`, `ERROR`, `CRITICAL`).                                        | No                             | `INFO`                            | `DEBUG`                           |\n| `ANONYMIZED_TELEMETRY`          | Enable/disable anonymized telemetry (`true`/`false`).                                                   | No                             | `true`                            | `false`                           |\n\n**Supported LLM Providers (`MCP_MODEL_PROVIDER`):**\n\n`openai`, `azure_openai`, `anthropic`, `google`, `mistral`, `ollama`, `deepseek`, `openrouter`, `alibaba`, `moonshot`, `unbound`\n\n## Connecting to Your Own Browser (CDP)\n\nInstead of having the server launch and manage its own browser instance, you can connect it to a Chrome/Chromium browser that you launch and manage yourself. This is useful for:\n\n*   Using your existing browser profile (cookies, logins, extensions).\n*   Observing the automation directly in your own browser window.\n*   Debugging complex scenarios.\n\n**Steps:**\n\n1.  **Launch Chrome/Chromium with Remote Debugging Enabled:**\n    Open your terminal or command prompt and run the command appropriate for your operating system. This tells Chrome to listen for connections on a specific port (e.g., 9222).\n\n    *   **macOS:**\n        ```bash\n        /Applications/Google\\ Chrome.app/Contents/MacOS/Google\\ Chrome --remote-debugging-port=9222\n        ```\n        *(Adjust the path if Chrome is installed elsewhere)*\n\n    *   **Linux:**\n        ```bash\n        google-chrome --remote-debugging-port=9222\n        # or\n        chromium-browser --remote-debugging-port=9222\n        ```\n\n    *   **Windows (Command Prompt):**\n        ```cmd\n        \"C:\\Program Files\\Google\\Chrome\\Application\\chrome.exe\" --remote-debugging-port=9222\n        ```\n        *(Adjust the path to your Chrome installation if necessary)*\n\n    *   **Windows (PowerShell):**\n        ```powershell\n        & \"C:\\Program Files\\Google\\Chrome\\Application\\chrome.exe\" --remote-debugging-port=9222\n        ```\n        *(Adjust the path to your Chrome installation if necessary)*\n\n    **Note:** If port 9222 is already in use, choose a different port (e.g., 9223) and use that same port in the `CHROME_CDP` environment variable.\n\n2.  **Configure Environment Variables:**\n    Set the following environment variables in your `.env` file or system environment before starting the MCP server:\n\n    ```dotenv\n    MCP_USE_OWN_BROWSER=true\n    CHROME_CDP=http://localhost:9222 # Use the same port you launched Chrome with\n    ```\n    *   `MCP_USE_OWN_BROWSER=true`: Tells the server to connect to an existing browser instead of launching one.\n    *   `CHROME_CDP`: Specifies the URL where the server can connect to your browser's DevTools Protocol endpoint.\n\n3.  **Run the MCP Server:**\n    Start the server as usual:\n    ```bash\n    uv run mcp-server-browser-use\n    ```\n\nNow, when you use the `run_browser_agent` or `run_deep_search` tools, the server will connect to your running Chrome instance instead of creating a new one.\n\n**Important Considerations:**\n\n*   The browser launched with `--remote-debugging-port` must remain open while the MCP server is running and needs to interact with it.\n*   Ensure the `CHROME_CDP` URL is accessible from where the MCP server is running (usually `http://localhost:PORT` if running on the same machine).\n*   Using your own browser means the server inherits its state (open tabs, logged-in sessions). Be mindful of this during automation.\n*   Settings like `MCP_HEADLESS`, `BROWSER_HEADLESS`, `MCP_KEEP_BROWSER_OPEN` are ignored when `MCP_USE_OWN_BROWSER=true`. Window size is determined by your browser window.\n\n## Development\n\n```bash\n# Install dev dependencies and sync project deps\nuv sync --dev\n\n# Install playwright browsers\nuv run playwright install\n\n# Run with debugger (Example connecting to own browser via CDP)\n# 1. Launch Chrome: google-chrome --remote-debugging-port=9222\n# 2. Run inspector command:\nnpx @modelcontextprotocol/inspector@latest \\\n  -e ANTHROPIC_API_KEY=$ANTHROPIC_API_KEY \\\n  -e MCP_MODEL_PROVIDER=anthropic \\\n  -e MCP_MODEL_NAME=claude-3-7-sonnet-20250219 \\\n  -e MCP_USE_OWN_BROWSER=true \\\n  -e CHROME_CDP=http://localhost:9222 \\\n  uv --directory . run mcp run src/mcp_server_browser_use/server.py\n# Note: Change timeout in inspector's config panel if needed (default is 10 seconds)\n```\n\n## Troubleshooting\n\n-   **Browser Conflicts**: If *not* using `CHROME_CDP` (`MCP_USE_OWN_BROWSER=false`), ensure no other conflicting Chrome instances are running with the same user data directory if `CHROME_USER_DATA` is specified.\n-   **CDP Connection Issues**: If using `MCP_USE_OWN_BROWSER=true`:\n    *   Verify Chrome was launched with the `--remote-debugging-port` flag.\n    *   Ensure the port in `CHROME_CDP` matches the port used when launching Chrome.\n    *   Check for firewall issues blocking the connection to the specified port.\n    *   Make sure the browser is still running.\n-   **API Errors**: Double-check that the correct API key environment variable (`OPENAI_API_KEY`, `ANTHROPIC_API_KEY`, etc.) is set for your chosen `MCP_MODEL_PROVIDER`, or that `MCP_API_KEY` is set. Verify keys and endpoints (`AZURE_OPENAI_ENDPOINT` is required for Azure).\n-   **Vision Issues**: Ensure `MCP_USE_VISION=true` if using vision features and that your selected LLM model supports vision.\n-   **Dependency Problems**: Run `uv sync` to ensure all dependencies are correctly installed. Check `pyproject.toml`.\n-   **Logging**: Check the log file specified by `LOG_FILE` (default: `mcp_server_browser_use.log`) for detailed error messages. Increase `BROWSER_USE_LOGGING_LEVEL` to `DEBUG` for more verbose output.\n\n## License\n\nMIT - See [LICENSE](LICENSE) for details.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "automation",
        "automate",
        "browser",
        "browser automation",
        "automate web",
        "use automate"
      ],
      "category": "web-search"
    },
    "westsideori--cursor-a11y-mcp": {
      "owner": "westsideori",
      "name": "cursor-a11y-mcp",
      "url": "https://github.com/westsideori/cursor-a11y-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/westsideori.webp",
      "description": "Run accessibility tests on web applications to identify compliance issues and enhance digital accessibility. Utilize axe-core and Puppeteer to generate detailed violation reports with information about impact levels and affected elements.",
      "stars": 1,
      "forks": 2,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-08-16T21:34:07Z",
      "readme_content": "# Cursor A11y MCP\n\nA Model Context Protocol (MCP) server that provides accessibility testing capabilities AI agents. This tool helps identify accessibility issues in web applications using axe-core and Puppeteer.\n\n<a href=\"https://glama.ai/mcp/servers/mik2l7a1tw\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/mik2l7a1tw/badge\" alt=\"Cursor A11y MCP server\" />\n</a>\n\n## Features\n\n- Run accessibility tests on any URL or local development server\n- Powered by axe-core for comprehensive accessibility testing\n- Provides detailed violation reports including:\n  - Impact level\n  - Description of the issue\n  - Help text and documentation links\n  - Affected HTML elements\n  - Failure summaries\n\n## Project Structure\n\n- `src/` - Source code for the MCP server and accessibility testing tool\n- `test-site/` - A React application with intentional accessibility issues for testing\n- `build/` - Compiled version of the source code\n\n## Installation\n\n```bash\nnpm install\n```\n\nThen install the test site dependencies:\n\n```bash\ncd test-site\nnpm install\ncd ..\n```\n\n## Usage\n\n### Starting the MCP Server\n\n```bash\nnpm run build\nnpm start\n```\n\n### Running the Test Site\n\n```bash\nnpm run start:test-site\n```\n\nThe test site will be available at `http://localhost:5000`.\n\n### Running Accessibility Tests\n\nThe tool accepts two types of inputs:\n\n1. A full URL to test\n2. A relative path that will be appended to `http://localhost:5000`\n\n## Dependencies\n\n- `@modelcontextprotocol/sdk`: ^1.4.1\n- `puppeteer`: ^24.1.1\n- `zod`: ^3.24.1\n\n### Test Site Dependencies\n\n- `react`: ^18.2.0\n- `react-dom`: ^18.2.0\n- `react-scripts`: 5.0.1\n\n## Development\n\n1. Make changes to the source code in the `src/` directory\n2. Run `npm run build` to compile the changes\n3. Start the server with `npm start`\n\n## Configuring in Cursor\n\nTo add this accessibility testing tool to Cursor's MCP Server settings:\n\n1. Open Cursor's Settings (⌘ + ,)\n2. Navigate to \"Features\" > \"MCP Servers\"\n3. Add a new MCP Server with the following configuration:\n   - Name: `a11y`\n   - Select `command` from the dropdown\n   - Command: `node path/to/cursor-a11y-mcp/index/file/in/build/folder`\n     (Replace `path/to/cursor-a11y-mcp/index/file/in/build/folder` with the absolute path to your index.js file in the build folder.)\n4. Click `Add`\n5. The accessibility testing tool will now be available in Cursor's Composer\n\n## Usage in Composer\n\nTo use the accessibility testing tool in Cursor's Composer:\n\n1. Run in your terminal:\n\n```bash\nnpm run start:test-site\n```\n\nThis will start the test site at `http://localhost:5000`\n\n2. In Cursor's Composer, type `use a11y tool`\n3. Composer will prompt you to run the tool\n4. After running the tool, you will see the accessibility violations in the response, and code actions to fix the violations\n5. The Composer may prompt you to use the tool again to confirm that the violations are fixed\n\n## License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n\n## Version\n\nCurrent version: 2.0.1",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "accessibility",
        "compliance",
        "applications",
        "accessibility tests",
        "accessibility utilize",
        "run accessibility"
      ],
      "category": "web-search"
    },
    "williamvd4--web-search": {
      "owner": "williamvd4",
      "name": "web-search",
      "url": "https://github.com/williamvd4/web-search",
      "imageUrl": "/freedevtools/mcp/pfp/williamvd4.webp",
      "description": "Seamlessly search the web and retrieve structured results including titles, URLs, and descriptions from Google search without requiring API keys. The server is configurable for the number of results returned per search.",
      "stars": 3,
      "forks": 3,
      "license": "No License",
      "language": "",
      "updated_at": "2025-08-23T08:40:13Z",
      "readme_content": "# Web Search MCP Server\n\nA Model Context Protocol (MCP) server that enables free web searching using Google search results, with no API keys required.\n\n## Features\n\n- Search the web using Google search results\n- No API keys or authentication required\n- Returns structured results with titles, URLs, and descriptions\n- Configurable number of results per search\n\n## Installation\n\n1. Clone or download this repository\n2. Install dependencies:\n```bash\nnpm install\n```\n3. Build the server:\n```bash\nnpm run build\n```\n4. Add the server to your MCP configuration:\n\nFor VSCode (Claude Dev Extension):\n```json\n{\n  \"mcpServers\": {\n    \"web-search\": {\n      \"command\": \"node\",\n      \"args\": [\"/path/to/web-search/build/index.js\"]\n    }\n  }\n}\n```\n\nFor Claude Desktop:\n```json\n{\n  \"mcpServers\": {\n    \"web-search\": {\n      \"command\": \"node\",\n      \"args\": [\"/path/to/web-search/build/index.js\"]\n    }\n  }\n}\n```\n\n## Usage\n\nThe server provides a single tool named `search` that accepts the following parameters:\n\n```typescript\n{\n  \"query\": string,    // The search query\n  \"limit\": number     // Optional: Number of results to return (default: 5, max: 10)\n}\n```\n\nExample usage:\n```typescript\nuse_mcp_tool({\n  server_name: \"web-search\",\n  tool_name: \"search\",\n  arguments: {\n    query: \"your search query\",\n    limit: 3  // optional\n  }\n})\n```\n\nExample response:\n```json\n[\n  {\n    \"title\": \"Example Search Result\",\n    \"url\": \"https://example.com\",\n    \"description\": \"Description of the search result...\"\n  }\n]\n```\n\n## Limitations\n\nSince this tool uses web scraping of Google search results, there are some important limitations to be aware of:\n\n1. **Rate Limiting**: Google may temporarily block requests if too many searches are performed in a short time. To avoid this:\n   - Keep searches to a reasonable frequency\n   - Use the limit parameter judiciously\n   - Consider implementing delays between searches if needed\n\n2. **Result Accuracy**: \n   - The tool relies on Google's HTML structure, which may change\n   - Some results might be missing descriptions or other metadata\n   - Complex search operators may not work as expected\n\n3. **Legal Considerations**:\n   - This tool is intended for personal use\n   - Respect Google's terms of service\n   - Consider implementing appropriate rate limiting for your use case\n\n## Contributing\n\nFeel free to submit issues and enhancement requests!\n",
      "npm_url": "https://www.npmjs.com/package/web-search",
      "npm_downloads": 629,
      "keywords": [
        "search",
        "williamvd4",
        "web",
        "search web",
        "williamvd4 web",
        "web search"
      ],
      "category": "web-search"
    },
    "willsmith2099--open-webui": {
      "owner": "willsmith2099",
      "name": "open-webui",
      "url": "https://github.com/willsmith2099/open-webui",
      "imageUrl": "/freedevtools/mcp/pfp/willsmith2099.webp",
      "description": "A self-hosted WebUI for enhancing interactions with language models. Supports features like voice calls and Markdown, functioning entirely offline and compatible with various LLM runners.",
      "stars": 0,
      "forks": 0,
      "license": "MIT License",
      "language": "",
      "updated_at": "2024-11-22T01:49:24Z",
      "readme_content": "# Open WebUI 👋\n\n![GitHub stars](https://img.shields.io/github/stars/open-webui/open-webui?style=social)\n![GitHub forks](https://img.shields.io/github/forks/open-webui/open-webui?style=social)\n![GitHub watchers](https://img.shields.io/github/watchers/open-webui/open-webui?style=social)\n![GitHub repo size](https://img.shields.io/github/repo-size/open-webui/open-webui)\n![GitHub language count](https://img.shields.io/github/languages/count/open-webui/open-webui)\n![GitHub top language](https://img.shields.io/github/languages/top/open-webui/open-webui)\n![GitHub last commit](https://img.shields.io/github/last-commit/open-webui/open-webui?color=red)\n![Hits](https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fgithub.com%2Follama-webui%2Follama-wbui&count_bg=%2379C83D&title_bg=%23555555&icon=&icon_color=%23E7E7E7&title=hits&edge_flat=false)\n[![Discord](https://img.shields.io/badge/Discord-Open_WebUI-blue?logo=discord&logoColor=white)](https://discord.gg/5rJgQTnV4s)\n[![v1_label_Sponsor_message_logo_GitHub_color_fe8e86](https://img.shields.io/static/v1?label=Sponsor&message=%E2%9D%A4&logo=GitHub&color=%23fe8e86)](https://github.com/sponsors/tjbck)\n\nOpen WebUI is an [extensible](https://github.com/open-webui/pipelines), feature-rich, and user-friendly self-hosted WebUI designed to operate entirely offline. It supports various LLM runners, including Ollama and OpenAI-compatible APIs. For more information, be sure to check out our [Open WebUI Documentation](https://docs.openwebui.com/).\n\n\n\n## Key Features of Open WebUI ⭐\n\n- 🚀 **Effortless Setup**: Install seamlessly using Docker or Kubernetes (kubectl, kustomize or helm) for a hassle-free experience with support for both `:ollama` and `:cuda` tagged images.\n\n- 🤝 **Ollama/OpenAI API Integration**: Effortlessly integrate OpenAI-compatible APIs for versatile conversations alongside Ollama models. Customize the OpenAI API URL to link with **LMStudio, GroqCloud, Mistral, OpenRouter, and more**.\n\n- 🛡️ **Granular Permissions and User Groups**: By allowing administrators to create detailed user roles and permissions, we ensure a secure user environment. This granularity not only enhances security but also allows for customized user experiences, fostering a sense of ownership and responsibility amongst users.\n\n- 📱 **Responsive Design**: Enjoy a seamless experience across Desktop PC, Laptop, and Mobile devices.\n\n- 📱 **Progressive Web App (PWA) for Mobile**: Enjoy a native app-like experience on your mobile device with our PWA, providing offline access on localhost and a seamless user interface.\n\n- ✒️🔢 **Full Markdown and LaTeX Support**: Elevate your LLM experience with comprehensive Markdown and LaTeX capabilities for enriched interaction.\n\n- 🎤📹 **Hands-Free Voice/Video Call**: Experience seamless communication with integrated hands-free voice and video call features, allowing for a more dynamic and interactive chat environment.\n\n- 🛠️ **Model Builder**: Easily create Ollama models via the Web UI. Create and add custom characters/agents, customize chat elements, and import models effortlessly through [Open WebUI Community](https://openwebui.com/) integration.\n\n- 🐍 **Native Python Function Calling Tool**: Enhance your LLMs with built-in code editor support in the tools workspace. Bring Your Own Function (BYOF) by simply adding your pure Python functions, enabling seamless integration with LLMs.\n\n- 📚 **Local RAG Integration**: Dive into the future of chat interactions with groundbreaking Retrieval Augmented Generation (RAG) support. This feature seamlessly integrates document interactions into your chat experience. You can load documents directly into the chat or add files to your document library, effortlessly accessing them using the `#` command before a query.\n\n- 🔍 **Web Search for RAG**: Perform web searches using providers like `SearXNG`, `Google PSE`, `Brave Search`, `serpstack`, `serper`, `Serply`, `DuckDuckGo`, `TavilySearch`, `SearchApi` and `Bing` and inject the results directly into your chat experience.\n\n- 🌐 **Web Browsing Capability**: Seamlessly integrate websites into your chat experience using the `#` command followed by a URL. This feature allows you to incorporate web content directly into your conversations, enhancing the richness and depth of your interactions.\n\n- 🎨 **Image Generation Integration**: Seamlessly incorporate image generation capabilities using options such as AUTOMATIC1111 API or ComfyUI (local), and OpenAI's DALL-E (external), enriching your chat experience with dynamic visual content.\n\n- ⚙️ **Many Models Conversations**: Effortlessly engage with various models simultaneously, harnessing their unique strengths for optimal responses. Enhance your experience by leveraging a diverse set of models in parallel.\n\n- 🔐 **Role-Based Access Control (RBAC)**: Ensure secure access with restricted permissions; only authorized individuals can access your Ollama, and exclusive model creation/pulling rights are reserved for administrators.\n\n- 🌐🌍 **Multilingual Support**: Experience Open WebUI in your preferred language with our internationalization (i18n) support. Join us in expanding our supported languages! We're actively seeking contributors!\n\n- 🧩 **Pipelines, Open WebUI Plugin Support**: Seamlessly integrate custom logic and Python libraries into Open WebUI using [Pipelines Plugin Framework](https://github.com/open-webui/pipelines). Launch your Pipelines instance, set the OpenAI URL to the Pipelines URL, and explore endless possibilities. [Examples](https://github.com/open-webui/pipelines/tree/main/examples) include **Function Calling**, User **Rate Limiting** to control access, **Usage Monitoring** with tools like Langfuse, **Live Translation with LibreTranslate** for multilingual support, **Toxic Message Filtering** and much more.\n\n- 🌟 **Continuous Updates**: We are committed to improving Open WebUI with regular updates, fixes, and new features.\n\nWant to learn more about Open WebUI's features? Check out our [Open WebUI documentation](https://docs.openwebui.com/features) for a comprehensive overview!\n\n## 🔗 Also Check Out Open WebUI Community!\n\nDon't forget to explore our sibling project, [Open WebUI Community](https://openwebui.com/), where you can discover, download, and explore customized Modelfiles. Open WebUI Community offers a wide range of exciting possibilities for enhancing your chat interactions with Open WebUI! 🚀\n\n## How to Install 🚀\n\n### Installation via Python pip 🐍\n\nOpen WebUI can be installed using pip, the Python package installer. Before proceeding, ensure you're using **Python 3.11** to avoid compatibility issues.\n\n1. **Install Open WebUI**:\n   Open your terminal and run the following command to install Open WebUI:\n\n   ```bash\n   pip install open-webui\n   ```\n\n2. **Running Open WebUI**:\n   After installation, you can start Open WebUI by executing:\n\n   ```bash\n   open-webui serve\n   ```\n\nThis will start the Open WebUI server, which you can access at [http://localhost:8080](http://localhost:8080)\n\n### Quick Start with Docker 🐳\n\n> [!NOTE]  \n> Please note that for certain Docker environments, additional configurations might be needed. If you encounter any connection issues, our detailed guide on [Open WebUI Documentation](https://docs.openwebui.com/) is ready to assist you.\n\n> [!WARNING]\n> When using Docker to install Open WebUI, make sure to include the `-v open-webui:/app/backend/data` in your Docker command. This step is crucial as it ensures your database is properly mounted and prevents any loss of data.\n\n> [!TIP]  \n> If you wish to utilize Open WebUI with Ollama included or CUDA acceleration, we recommend utilizing our official images tagged with either `:cuda` or `:ollama`. To enable CUDA, you must install the [Nvidia CUDA container toolkit](https://docs.nvidia.com/dgx/nvidia-container-runtime-upgrade/) on your Linux/WSL system.\n\n### Installation with Default Configuration\n\n- **If Ollama is on your computer**, use this command:\n\n  ```bash\n  docker run -d -p 3000:8080 --add-host=host.docker.internal:host-gateway -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main\n  ```\n\n- **If Ollama is on a Different Server**, use this command:\n\n  To connect to Ollama on another server, change the `OLLAMA_BASE_URL` to the server's URL:\n\n  ```bash\n  docker run -d -p 3000:8080 -e OLLAMA_BASE_URL=https://example.com -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main\n  ```\n\n- **To run Open WebUI with Nvidia GPU support**, use this command:\n\n  ```bash\n  docker run -d -p 3000:8080 --gpus all --add-host=host.docker.internal:host-gateway -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:cuda\n  ```\n\n### Installation for OpenAI API Usage Only\n\n- **If you're only using OpenAI API**, use this command:\n\n  ```bash\n  docker run -d -p 3000:8080 -e OPENAI_API_KEY=your_secret_key -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main\n  ```\n\n### Installing Open WebUI with Bundled Ollama Support\n\nThis installation method uses a single container image that bundles Open WebUI with Ollama, allowing for a streamlined setup via a single command. Choose the appropriate command based on your hardware setup:\n\n- **With GPU Support**:\n  Utilize GPU resources by running the following command:\n\n  ```bash\n  docker run -d -p 3000:8080 --gpus=all -v ollama:/root/.ollama -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:ollama\n  ```\n\n- **For CPU Only**:\n  If you're not using a GPU, use this command instead:\n\n  ```bash\n  docker run -d -p 3000:8080 -v ollama:/root/.ollama -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:ollama\n  ```\n\nBoth commands facilitate a built-in, hassle-free installation of both Open WebUI and Ollama, ensuring that you can get everything up and running swiftly.\n\nAfter installation, you can access Open WebUI at [http://localhost:3000](http://localhost:3000). Enjoy! 😄\n\n### Other Installation Methods\n\nWe offer various installation alternatives, including non-Docker native installation methods, Docker Compose, Kustomize, and Helm. Visit our [Open WebUI Documentation](https://docs.openwebui.com/getting-started/) or join our [Discord community](https://discord.gg/5rJgQTnV4s) for comprehensive guidance.\n\n### Troubleshooting\n\nEncountering connection issues? Our [Open WebUI Documentation](https://docs.openwebui.com/troubleshooting/) has got you covered. For further assistance and to join our vibrant community, visit the [Open WebUI Discord](https://discord.gg/5rJgQTnV4s).\n\n#### Open WebUI: Server Connection Error\n\nIf you're experiencing connection issues, it’s often due to the WebUI docker container not being able to reach the Ollama server at 127.0.0.1:11434 (host.docker.internal:11434) inside the container . Use the `--network=host` flag in your docker command to resolve this. Note that the port changes from 3000 to 8080, resulting in the link: `http://localhost:8080`.\n\n**Example Docker Command**:\n\n```bash\ndocker run -d --network=host -v open-webui:/app/backend/data -e OLLAMA_BASE_URL=http://127.0.0.1:11434 --name open-webui --restart always ghcr.io/open-webui/open-webui:main\n```\n\n### Keeping Your Docker Installation Up-to-Date\n\nIn case you want to update your local Docker installation to the latest version, you can do it with [Watchtower](https://containrrr.dev/watchtower/):\n\n```bash\ndocker run --rm --volume /var/run/docker.sock:/var/run/docker.sock containrrr/watchtower --run-once open-webui\n```\n\nIn the last part of the command, replace `open-webui` with your container name if it is different.\n\nCheck our Migration Guide available in our [Open WebUI Documentation](https://docs.openwebui.com/tutorials/migration/).\n\n### Using the Dev Branch 🌙\n\n> [!WARNING]\n> The `:dev` branch contains the latest unstable features and changes. Use it at your own risk as it may have bugs or incomplete features.\n\nIf you want to try out the latest bleeding-edge features and are okay with occasional instability, you can use the `:dev` tag like this:\n\n```bash\ndocker run -d -p 3000:8080 -v open-webui:/app/backend/data --name open-webui --add-host=host.docker.internal:host-gateway --restart always ghcr.io/open-webui/open-webui:dev\n```\n\n## What's Next? 🌟\n\nDiscover upcoming features on our roadmap in the [Open WebUI Documentation](https://docs.openwebui.com/roadmap/).\n\n## License 📜\n\nThis project is licensed under the [MIT License](LICENSE) - see the [LICENSE](LICENSE) file for details. 📄\n\n## Support 💬\n\nIf you have any questions, suggestions, or need assistance, please open an issue or join our\n[Open WebUI Discord community](https://discord.gg/5rJgQTnV4s) to connect with us! 🤝\n\n## Star History\n\n<a href=\"https://star-history.com/#open-webui/open-webui&Date\">\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://api.star-history.com/svg?repos=open-webui/open-webui&type=Date&theme=dark\" />\n    <source media=\"(prefers-color-scheme: light)\" srcset=\"https://api.star-history.com/svg?repos=open-webui/open-webui&type=Date\" />\n    <img alt=\"Star History Chart\" src=\"https://api.star-history.com/svg?repos=open-webui/open-webui&type=Date\" />\n  </picture>\n</a>\n\n---\n\nCreated by [Timothy Jaeryang Baek](https://github.com/tjbck) - Let's make Open WebUI even more amazing together! 💪",
      "npm_url": "https://www.npmjs.com/package/open-webui",
      "npm_downloads": 1679,
      "keywords": [
        "webui",
        "web",
        "willsmith2099",
        "webui self",
        "hosted webui",
        "open webui"
      ],
      "category": "web-search"
    },
    "wiseman--osm-mcp": {
      "owner": "wiseman",
      "name": "osm-mcp",
      "url": "https://github.com/wiseman/osm-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/wiseman.webp",
      "description": "Query and visualize OpenStreetMap data through a web-based interface, utilizing PostgreSQL/PostGIS for backend data management. Features include dynamic map interactions such as adding markers and polygons as well as adjustable view settings.",
      "stars": 30,
      "forks": 8,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-09-24T03:38:17Z",
      "readme_content": "# MCP-OSM: OpenStreetMap Integration for MCP\n\nThis package provides OpenStreetMap integration for MCP, allowing users to query\nand visualize map data through an MCP interface.\n\n[](osm-mcp.webp)\n\n## Features\n\n- Web-based map viewer using Leaflet and OpenStreetMap\n- Server-to-client communication via Server-Sent Events (SSE)\n- MCP tools for map control (adding markers, polygons, setting view, getting view)\n- PostgreSQL/PostGIS query interface for OpenStreetMap data\n\n## Installation\n\nThis is my `claude_desktop_config.json`:\n```json\n{\n  \"mcpServers\": {\n    \"OSM PostgreSQL Server\": {\n      \"command\": \"/Users/wiseman/.local/bin/uv\",\n      \"args\": [\n        \"run\",\n        \"--env-file\",\n        \".env\",\n        \"--with\",\n        \"mcp[cli]\",\n        \"--with\",\n        \"psycopg2\",\n        \"--with-editable\",\n        \"/Users/wiseman/src/mcp-osm\",\n        \"--directory\",\n        \"/Users/wiseman/src/mcp-osm\",\n        \"mcp\",\n        \"run\",\n        \"mcp.py\"\n      ]\n    }\n  }\n}\n```\n\nWhen the MCP server starts it also starts a web server at http://localhost:8889/\nthat has the map interface.\n\n### Environment Variables\n\nThe following environment variables can be used to configure the MCP:\n\n- `FLASK_HOST` - Host for the Flask server (default: 127.0.0.1)\n- `FLASK_PORT` - Port for the Flask server (default: 8889)\n- `PGHOST` - PostgreSQL host (default: localhost)\n- `PGPORT` - PostgreSQL port (default: 5432)\n- `PGDB` - PostgreSQL database name (default: osm)\n- `PGUSER` - PostgreSQL username (default: postgres)\n- `PGPASSWORD` - PostgreSQL password (default: postgres)\n\n### MCP Tools\n\nThe following MCP tools are available:\n\n- `get_map_view` - Get the current map view\n- `set_map_view` - Set the map view to specific coordinates or bounds\n- `set_map_title` - Set the title displayed at the bottom right of the map\n- `add_map_marker` - Add a marker at specific coordinates\n- `add_map_line` - Add a line defined by a set of coordinates\n- `add_map_polygon` - Add a polygon defined by a set of coordinates\n- `query_osm_postgres` - Execute a SQL query against the OpenStreetMap database",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "openstreetmap",
        "osm",
        "postgis",
        "visualize openstreetmap",
        "openstreetmap data",
        "wiseman osm"
      ],
      "category": "web-search"
    },
    "wllcnm--mcp-reddit": {
      "owner": "wllcnm",
      "name": "mcp-reddit",
      "url": "https://github.com/wllcnm/mcp-reddit",
      "imageUrl": "/freedevtools/mcp/pfp/wllcnm.webp",
      "description": "Interact with Reddit content through AI assistance, enabling search, analysis, and retrieval of posts and comments from specific subreddits. Users can browse and analyze popular posts efficiently.",
      "stars": 9,
      "forks": 2,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-17T12:25:33Z",
      "readme_content": "# MCP Reddit Server\n\n[English](README-en.md) | [中文说明](README.md)\n\n<h2 id=\"chinese\">中文说明</h2>\n\n## 1. 项目介绍\n\n这是一个基于MCP（Model Context Protocol）协议的Reddit服务器，专门设计用于与Claude等大语言模型进行交互。通过这个服务，你可以让AI助手帮助你浏览和分析Reddit上的内容。\n\n### 1.1 主要功能\n\n- 搜索特定subreddit中的帖子\n- 获取帖子详细信息和评论\n- 浏览subreddit中的热门帖子\n\n### 1.2 项目结构\n\n```\nnangeAGICode/reddit_chat_claude/\n├── .github/\n│   └── workflows/\n│       └── docker.yml # GitHub Actions工作流配置\n├── src/\n│   ├── init.py\n│   └── server.py # MCP服务器核心代码\n├── .gitignore # Git忽略文件配置\n├── Dockerfile # Docker构建文件\n├── LICENSE # MIT许可证\n├── README.md # 项目说明文档\n└── requirements.txt # Python依赖包列表\n```\n\n## 2. 安装和配置\n\n### 2.1 前提条件\n\n- Docker（必须）\n- Python 3.12+（用于本地开发）\n- Reddit API凭证（必需）\n\n### 2.2 获取Reddit API凭证\n\n1. 访问 https://www.reddit.com/prefs/apps\n2. 点击\"create another app...\"\n3. 选择\"script\"\n4. 填写必要信息\n5. 获取client_id和client_secret\n\n### 2.3 环境变量配置\n\n需要设置以下环境变量：\n\n```bash\nREDDIT_CLIENT_ID=你的client_id\nREDDIT_CLIENT_SECRET=你的client_secret\nREDDIT_USER_AGENT=你的user_agent\n```\n### 2.4 Docker安装\nMAC用户：\n1.访问 Docker 官网：https://www.docker.com/products/docker-desktop\n2.点击 \"Download for Mac\"\n3.选择对应你的 Mac 芯片的版本（Apple Silicon 或 Intel）\n4.下载并安装 .dmg 文件\n\n5.验证安装：\n```bash\n# 检查 Docker 版本\ndocker --version\n\n# 运行测试容器\ndocker run hello-world\n```\n6.确保 Docker 服务正在运行：\n```bash\n# 检查 Docker 服务状态\ndocker ps\n```\n\n\n\n\n\n\n\n## 3. 使用方法\n\n### 3.1 在Claude桌面客户端中使用\n\n在你的`claude_desktop_config.json`中添加以下配置：\n\n```json\n{\n  \"mcpServers\": {\n    \"reddit\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"-i\",\n        \"--rm\",\n        \"-e\", \"REDDIT_CLIENT_ID=你的client_id\",\n        \"-e\", \"REDDIT_CLIENT_SECRET=你的client_secret\",\n        \"-e\", \"REDDIT_USER_AGENT=你的user_agent\",\n        \"ghcr.io/nangeplus/mcp-reddit:latest\"\n      ]\n    }\n  }\n}\n```\n\n### 3.2 可用工具\n\n1. **search_subreddit**: 搜索特定subreddit中的帖子\n   - 参数：\n     - `subreddit`: subreddit名称\n     - `query`: 搜索关键词\n     - `limit`: 最大返回结果数（默认：5）\n\n2. **get_post_details**: 获取特定帖子的详细信息\n   - 参数：\n     - `post_id`: Reddit帖子ID\n     - `comment_limit`: 获取评论的最大数量（默认：10）\n\n3. **get_subreddit_hot**: 获取subreddit中的热门帖子\n   - 参数：\n     - `subreddit`: subreddit名称\n     - `limit`: 返回帖子的最大数量（默认：5）\n\n### 3.3 使用示例\n\n在Claude中，你可以这样使用工具：\n\n```json\n{\n  \"tool\": \"get_subreddit_hot\",\n  \"arguments\": {\n    \"subreddit\": \"Python\",\n    \"limit\": 3\n  }\n}\n```\n\n**示例对话：**\n\n用户：帮我查看Python subreddit中最热门的3个帖子。\n\nClaude：好的，我来帮你查看。我将使用`get_subreddit_hot`工具：\n\n```json\n{\n  \"tool\": \"get_subreddit_hot\",\n  \"arguments\": {\n    \"subreddit\": \"Python\",\n    \"limit\": 3\n  }\n}\n```\n\n[Claude会返回帖子信息]\n\n用户：帮我查看第一个帖子的详细内容和评论。\n\nClaude：我将使用`get_post_details`工具：\n\n```json\n{\n  \"tool\": \"get_post_details\",\n  \"arguments\": {\n    \"post_id\": \"返回的帖子ID\"\n  }\n}\n```\n\n## 4. 本地开发\n\n### 4.1 克隆仓库\n\n```bash\ngit clone https://github.com/nangeplus/mcp-reddit.git\ncd mcp-reddit\n```\n\n### 4.2 安装依赖\n\n```bash\npip install -r requirements.txt\n```\n\n### 4.3 运行服务器\n\n```bash\npython src/server.py\n```\n\n### 4.4 Docker构建\n\n```bash\ndocker build -t mcp-reddit .\ndocker run -i --rm \\\n  -e REDDIT_CLIENT_ID=你的client_id \\\n  -e REDDIT_CLIENT_SECRET=你的client_secret \\\n  -e REDDIT_USER_AGENT=你的user_agent \\\n  mcp-reddit\n```\n\n## 5. 注意事项\n\n1. **安全性**\n   - 请妥善保管你的Reddit API凭证\n   - 不要在公共场合分享你的配置文件\n   - 建议使用环境变量而不是硬编码凭证\n\n2. **使用限制**\n   - Reddit API有调用频率限制\n   - 默认返回的评论数量限制为10条\n   - 搜索结果默认限制为5条\n\n3. **故障排除**\n   - 检查API凭证是否正确\n   - 确保网络连接正常\n   - 查看日志输出了解详细错误信息\n\n## 6. 贡献指南\n\n1. Fork 项目\n2. 创建特性分支 (`git checkout -b feature/AmazingFeature`)\n3. 提交更改 (`git commit -m 'Add some AmazingFeature'`)\n4. 推送到分支 (`git push origin feature/AmazingFeature`)\n5. 开启 Pull Request\n\n## 7. 许可证\n\n本项目采用 MIT 许可证 - 查看 [LICENSE](LICENSE) 文件了解详情\n\n---\n",
      "npm_url": "https://www.npmjs.com/package/mcp-reddit-server",
      "npm_downloads": 90,
      "keywords": [
        "search",
        "retrieval",
        "web",
        "content ai",
        "web search",
        "search wllcnm"
      ],
      "category": "web-search"
    },
    "wolfyy970--docs-fetch-mcp": {
      "owner": "wolfyy970",
      "name": "docs-fetch-mcp",
      "url": "https://github.com/wolfyy970/docs-fetch-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/wolfyy970.webp",
      "description": "Fetch and explore web content autonomously by navigating through documentation and web pages to extract relevant information. It supports recursive exploration and filters navigation links for content-rich pages.",
      "stars": 7,
      "forks": 4,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-07-30T22:42:56Z",
      "readme_content": "# Docs Fetch MCP Server\n\nA Model Context Protocol (MCP) server for fetching web content with recursive exploration capabilities. This server enables LLMs to autonomously explore web pages and documentation to learn about specific topics.\n\n## Overview\n\nThe Docs Fetch MCP Server provides a simple but powerful way for LLMs to retrieve and explore web content. It enables:\n\n- Fetching clean, readable content from any web page\n- Recursive exploration of linked pages up to a specified depth\n- Same-domain link traversal to gather comprehensive information\n- Smart filtering of navigation links to focus on content-rich pages\n\nThis tool is particularly useful when users want an LLM to learn about a specific topic by exploring documentation or web content.\n\n## Features\n\n- **Content Extraction**: Cleanly extracts the main content from web pages, removing distractions like navigation, ads, and irrelevant elements\n- **Link Analysis**: Identifies and extracts links from the page, assessing their relevance\n- **Recursive Exploration**: Follows links to related content within the same domain, up to a specified depth\n- **Parallel Processing**: Efficiently crawls content with concurrent requests and proper error handling\n- **Robust Error Handling**: Gracefully handles network issues, timeouts, and malformed pages\n- **Dual-Strategy Approach**: Uses fast axios requests first with puppeteer as a fallback for more complex pages\n- **Timeout Prevention**: Implements global timeout handling to ensure reliable operation within MCP time limits\n- **Partial Results**: Returns available content even when some pages fail to load completely\n\n## Usage\n\nThe server exposes a single MCP tool:\n\n### `fetch_doc_content`\n\nFetches web page content with the ability to explore linked pages up to a specified depth.\n\n**Parameters:**\n- `url` (string, required): URL of the web page to fetch\n- `depth` (number, optional, default: 1): Maximum depth of directory/link exploration (1-5)\n\n**Returns:**\n```json\n{\n  \"rootUrl\": \"https://example.com/docs\",\n  \"explorationDepth\": 2,\n  \"pagesExplored\": 5,\n  \"content\": [\n    {\n      \"url\": \"https://example.com/docs\",\n      \"title\": \"Documentation\",\n      \"content\": \"Main page content...\",\n      \"links\": [\n        {\n          \"url\": \"https://example.com/docs/topic1\",\n          \"text\": \"Topic 1\"\n        },\n        ...\n      ]\n    },\n    ...\n  ]\n}\n```\n\n## Installation\n\n1. Clone this repository:\n```bash\ngit clone https://github.com/wolfyy970/docs-fetch-mcp.git\ncd docs-fetch-mcp\n```\n\n2. Install dependencies:\n```bash\nnpm install\n```\n\n3. Build the project:\n```bash\nnpm run build\n```\n\n4. Configure your MCP settings in your Claude Client:\n```json\n{\n  \"mcpServers\": {\n    \"docs-fetch\": {\n      \"command\": \"node\",\n      \"args\": [\n        \"/path/to/docs-fetch-mcp/build/index.js\"\n      ],\n      \"env\": {\n        \"MCP_TRANSPORT\": \"pipe\"\n      }\n    }\n  }\n}\n```\n\n## Dependencies\n\n- `@modelcontextprotocol/sdk`: MCP server SDK\n- `puppeteer`: Headless browser for web page interaction\n- `axios`: HTTP client for making requests\n\n## Development\n\nTo run the server in development mode:\n\n```bash\nnpm run dev\n```\n\n## License\n\nMIT\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "search",
        "web",
        "fetch",
        "web search",
        "explore web",
        "docs fetch"
      ],
      "category": "web-search"
    },
    "wong2--mcp-jina-reader": {
      "owner": "wong2",
      "name": "mcp-jina-reader",
      "url": "https://github.com/wong2/mcp-jina-reader",
      "imageUrl": "/freedevtools/mcp/pfp/wong2.webp",
      "description": "Fetches the content of a remote URL and converts it into Markdown format using Jina Reader.",
      "stars": 45,
      "forks": 10,
      "license": "No License",
      "language": "TypeScript",
      "updated_at": "2025-09-15T11:38:55Z",
      "readme_content": "# Jina Reader MCP Server\n\nFetch the content of a remote URL as Markdown with Jina Reader\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "markdown",
        "jina",
        "web",
        "jina reader",
        "mcp jina",
        "using jina"
      ],
      "category": "web-search"
    },
    "wukan1986--mcp_query_table": {
      "owner": "wukan1986",
      "name": "mcp_query_table",
      "url": "https://github.com/wukan1986/mcp_query_table",
      "imageUrl": "/freedevtools/mcp/pfp/wukan1986.webp",
      "description": "Extracts and queries data from various financial websites while allowing seamless switching between sources in case of outages. Utilizes browser automation for accurate data retrieval without needing manual logins or complex request handling.",
      "stars": 36,
      "forks": 9,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-30T02:20:45Z",
      "readme_content": "# mcp_query_table\n\n1. 基于`playwright`实现的财经网页表格爬虫，支持`Model Context Protocol (MCP) `。目前可查询来源为\n\n    - [同花顺问财](http://iwencai.com/)\n    - [通达信问小达](https://wenda.tdx.com.cn/)\n    - [东方财富条件选股](https://xuangu.eastmoney.com/)\n\n   实盘时，如果某网站宕机或改版，可以立即切换到其他网站。(注意：不同网站的表格结构不同，需要提前做适配)\n\n2. 基于`playwright`实现的大语言模型调用爬虫。目前可用来源为\n    - [纳米搜索](https://www.n.cn/)\n    - [腾讯元宝](https://yuanbao.tencent.com/)\n    - [百度AI搜索](https://chat.baidu.com/)\n\n   `RooCode`提供了`Human Reply`功能。但发现`纳米搜索`网页版复制时格式破坏，所以研发了此功能\n\n## 安装\n\n```commandline\npip install -i https://pypi.org/simple --upgrade mcp_query_table\npip install -i https://pypi.tuna.tsinghua.edu.cn/simple --upgrade mcp_query_table\n```\n\n## 使用\n\n```python\nimport asyncio\n\nfrom mcp_query_table import *\n\n\nasync def main() -> None:\n    async with BrowserManager(endpoint=\"http://127.0.0.1:9222\", executable_path=None, devtools=True) as bm:\n        # 问财需要保证浏览器宽度>768，防止界面变成适应手机\n        page = await bm.get_page()\n        df = await query(page, '收益最好的200只ETF', query_type=QueryType.ETF, max_page=1, site=Site.THS)\n        print(df.to_markdown())\n        df = await query(page, '年初至今收益率前50', query_type=QueryType.Fund, max_page=1, site=Site.TDX)\n        print(df.to_csv())\n        df = await query(page, '流通市值前10的行业板块', query_type=QueryType.Index, max_page=1, site=Site.TDX)\n        print(df.to_csv())\n        # TODO 东财翻页要提前登录\n        df = await query(page, '今日涨幅前5的概念板块;', query_type=QueryType.Board, max_page=3, site=Site.EastMoney)\n        print(df)\n\n        output = await chat(page, \"1+2等于多少？\", provider=Provider.YuanBao)\n        print(output)\n        output = await chat(page, \"3+4等于多少？\", provider=Provider.YuanBao, create=True)\n        print(output)\n\n        print('done')\n        bm.release_page(page)\n        await page.wait_for_timeout(2000)\n\n\nif __name__ == '__main__':\n    asyncio.run(main())\n\n```\n\n## 注意事项\n\n1. 浏览器最好是`Chrome`。如一定要使用`Edge`,除了关闭`Edge`所有窗口外，还要在任务管理器关闭`Microsoft Edge`\n   的所有进程，即`taskkill /f /im msedge.exe`\n2. 浏览器要保证窗口宽度，防止部分网站自动适配成手机版，导致表格查询失败\n3. 如有网站账号，请提前登录。此工具无自动登录功能\n4. 不同网站的表格结构不同，同条件返回股票数量也不同。需要查询后做适配\n\n## 工作原理\n\n不同于`requests`，`playwright`是基于浏览器的，模拟用户在浏览器中的操作。\n\n1. 不需要解决登录问题\n2. 不需要解决请求构造、响应解析\n3. 可以直接获取表格数据，所见即所得\n4. 运行速度慢于`requests`，但开发效率高\n\n数据的获取有：\n\n1. 直接解析HTML表格\n    1. 数字文本化了，不利于后期研究\n    2. 适用性最强\n2. 截获请求，获取返回的`json`数据\n    1. 类似于`requests`，需要做响应解析\n    2. 灵活性差点，网站改版后，需要重新做适配\n\n此项目采用的是模拟点击浏览器来发送请求，使用截获响应并解析的方法来获取数据。\n\n后期会根据不同的网站改版情况，使用更适合的方法。\n\n## 无头模式\n\n无头模式运行速度更快，但部分网站需要提前登录，所以，无头模式一定要指定`user_data_dir`，否则会出现需要登录的情况。\n\n- `endpoint=None`时，`headless=True`可无头启动新浏览器实例。指定`executable_path`和`user_data_dir`，才能确保无头模式下正常运行。\n- `endpoint`以`http://`开头，连接`CDP`模式启动的有头浏览器，参数必有`--remote-debugging-port`。`executable_path`为本地浏览器路径。\n- `endpoint`以`ws://`开头，连接远程`Playwright Server`。也是无头模式，但无法指定`user_data_dir`，所以使用受限\n    - 参考：https://playwright.dev/python/docs/docker#running-the-playwright-server\n\n`Chrome`新版的安全策略使用默认`user_data_dir`时将无法创建`CDP`服务，建议重新复制配置目录到其他地方\n\n## MCP支持\n\n确保可以在控制台中执行`python -m mcp_query_table -h`。如果不能，可能要先`pip install mcp_query_table`\n\n在`Cline`中可以配置如下。其中`command`是`python`的绝对路径，`timeout`是超时时间，单位为秒。 在各`AI`\n平台中由于返回时间常需1分钟以上，所以需要设置大的超时时间。\n\n### STDIO方式\n\n```json\n{\n  \"mcpServers\": {\n    \"mcp_query_table\": {\n      \"timeout\": 300,\n      \"command\": \"D:\\\\Users\\\\Kan\\\\miniconda3\\\\envs\\\\py312\\\\python.exe\",\n      \"args\": [\n        \"-m\",\n        \"mcp_query_table\",\n        \"--format\",\n        \"markdown\",\n        \"--endpoint\",\n        \"http://127.0.0.1:9222\",\n        \"--executable_path\",\n        \"C:\\\\Program Files\\\\Google\\\\Chrome\\\\Application\\\\chrome.exe\"\n      ]\n    }\n  }\n}\n```\n\n### SSE方式\n\n先在控制台中执行如下命令，启动`MCP`服务\n\n```commandline\npython -m mcp_query_table --format markdown --transport sse --port 8000 --endpoint http://127.0.0.1:9222  --user_data_dir \"D:\\user-data-dir\"\n```\n\n然后就可以连接到`MCP`服务了\n\n```json\n{\n  \"mcpServers\": {\n    \"mcp_query_table\": {\n      \"timeout\": 300,\n      \"url\": \"http://127.0.0.1:8000/sse\"\n    }\n  }\n}\n```\n\n### Streamable HTTP方式\n\n```commandline\npython -m mcp_query_table --format markdown --transport streamable-http --port 8000 --endpoint http://127.0.0.1:9222  --user_data_dir \"D:\\user-data-dir\"\n```\n\n连接的地址是`http://127.0.0.1:8000/mcp`\n\n## 使用`MCP Inspector`进行调试\n\n```commandline\nnpx @modelcontextprotocol/inspector python -m mcp_query_table --format markdown --endpoint http://127.0.0.1:9222\n```\n\n打开浏览器并翻页是一个比较耗时的操作，会导致`MCP Inspector`页面超时，可以`http://localhost:5173/?timeout=300000`\n表示超时时间为300秒\n\n第一次尝试编写`MCP`项目，可能会有各种问题，欢迎大家交流。\n\n## `MCP`使用技巧\n\n1. 2024年涨幅最大的100只股票按2024年12月31日总市值排名。三个网站的结果都不一样\n    - 同花顺：显示了2201只股票。前5个是工商银行、农业银行、中国移动、中国石油、建设银行\n    - 通达信：显示了100只股票，前5个是寒武纪、正丹股份，汇金科技、万丰奥威、艾融软件\n    - 东方财富：显示了100只股票，前5个是海光信息、寒武纪、光启技术、润泽科技、新易盛\n\n2. 大语言模型对问题拆分能力弱，所以要能合理的提问，保证查询条件不会被改动。以下推荐第2、3种\n    - 2024年涨幅最大的100只股票按2024年12月31日总市值排名\n      > 大语言模型非常有可能拆分这句，导致一步查询被分成了多步查询\n    - 向东方财富查询“2024年涨幅最大的100只股票按2024年12月31日总市值排名”\n      > 用引号括起来，避免被拆分\n    - 向东方财富板块查询 “去年涨的最差的行业板块”，再查询此板块中去年涨的最好的5只股票\n      > 分成两步查询，先查询板块，再查询股票。但最好不要全自动，因为第一步的结果它不理解“今日涨幅”和“区间涨幅”,需要交互修正\n\n## 支持`Streamlit`\n\n实现在同一页面中查询金融数据，并手工输入到`AI`中进行深度分析。参考`streamlit`目录下的`README.md`文件。\n\n\n\n## 参考\n\n- [Selenium webdriver无法附加到edge实例，edge的--remote-debugging-port选项无效](https://blog.csdn.net/qq_30576521/article/details/142370538)\n- https://github.com/AtuboDad/playwright_stealth/issues/31\n- https://github.com/browser-use/browser-use/issues/1520",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "queries",
        "retrieval",
        "mcp_query_table",
        "wukan1986 mcp_query_table",
        "web search",
        "queries data"
      ],
      "category": "web-search"
    },
    "wysh3--perplexity-mcp-zerver": {
      "owner": "wysh3",
      "name": "perplexity-mcp-zerver",
      "url": "https://github.com/wysh3/perplexity-mcp-zerver",
      "imageUrl": "/freedevtools/mcp/pfp/wysh3.webp",
      "description": "Leverage AI-powered research capabilities by performing web searches, retrieving documentation, and analyzing code through a modular tool architecture. The server facilitates interactions with the Perplexity website without requiring an API key, utilizing browser automation for efficient data retrieval.",
      "stars": 59,
      "forks": 15,
      "license": "GNU General Public License v3.0",
      "language": "TypeScript",
      "updated_at": "2025-09-29T08:26:10Z",
      "readme_content": "# Perplexity MCP Zerver\n\nA minimalist research server implementing the Model Context Protocol (MCP) to deliver AI-powered research capabilities through Perplexity's web interface.\n\n[![MCP Compatible](https://img.shields.io/badge/MCP-Compatible-333)]()\n[![TypeScript Codebase](https://img.shields.io/badge/TypeScript-Codebase-333)]()\n[![Tests Passing](https://img.shields.io/badge/Tests-Passing-333)]()\n[![Bun Runtime](https://img.shields.io/badge/Runtime-Bun-333)]()\n\n## Research Capabilities\n\n- **Intelligent Web Research**: Search and summarize content without API limits\n- **Persistent Conversations**: Maintain context with local SQLite chat storage\n- **Content Extraction**: Clean article extraction with GitHub repository parsing\n- **Developer Tooling**: Documentation retrieval, API discovery, code analysis\n- **Keyless Operation**: Browser automation replaces API key requirements\n\n---\n\n## Available Tools\n\n### Search (`search`)\nPerform research queries with configurable depth  \n*Returns raw text results*\n\n### Get Documentation (`get_documentation`)\nRetrieve technical documentation with examples  \n*Returns structured documentation*\n\n### Find APIs (`find_apis`)\nDiscover relevant APIs for development needs  \n*Returns API listings and descriptions*\n\n### Check Deprecated Code (`check_deprecated_code`)\nAnalyze code snippets for outdated patterns  \n*Returns analysis report*\n\n### Extract URL Content (`extract_url_content`)\nParse web content with automatic GitHub handling  \n*Returns structured content metadata*\n\n### Chat (`chat_perplexity`)\nPersistent conversations with context history  \n*Returns conversation state in JSON format*\n\n---\n\n## Getting Started\n\n### Prerequisites\n- Bun runtime\n- Node.js 18+ (for TypeScript compilation)\n\n### Installation\n```bash\ngit clone https://github.com/wysh3/perplexity-mcp-zerver.git\ncd perplexity-mcp-zerver\nbun install\nbun run build\n```\n\n### Configuration\nAdd to your MCP configuration file:\n```json\n{\n  \"mcpServers\": {\n    \"perplexity-server\": {\n      \"command\": \"bun\",\n      \"args\": [\"/absolute/path/to/build/main.js\"],\n      \"timeout\": 300\n    }\n  }\n}\n```\n\n### Usage\nInitiate commands through your MCP client:\n- \"Use perplexity to research quantum computing advancements\"\n- \"Ask perplexity-server for React 18 documentation\"\n- \"Begin conversation with perplexity about neural networks\"\n\n---\n\n## Technical Comparison\n\n| Feature              | This Implementation | Traditional APIs |\n|----------------------|---------------------|------------------|\n| Authentication       | None required       | API keys         |\n| Cost                 | Free                | Usage-based      |\n| Data Privacy         | Local processing    | Remote servers   |\n| GitHub Integration   | Native support      | Limited          |\n| History Persistence  | SQLite storage      | Session-based    |\n\n---\n\n## Troubleshooting\n\n**Server Connection Issues**\n1. Verify absolute path in configuration\n2. Confirm Node.js installation with `node -v`\n3. Ensure build completed successfully\n\n**Content Extraction**\n- GitHub paths must use full repository URLs\n- Adjust link recursion depth in source configuration\n\n---\n\n## Origins & License\n \nbased on - [DaInfernalCoder/perplexity-researcher-mcp](https://github.com/DaInfernalCoder/perplexity-researcher-mcp)  \nrefactored from - [sm-moshi/docshunter](https://github.com/sm-moshi/docshunter)  \n\nLicensed under **GNU GPL v3.0** - [View License](LICENSE)\n\n---\n\n> This project interfaces with Perplexity via browser automation. Use responsibly and ethically. Stability depends on Perplexity's website consistency. Educational use only.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "searches",
        "search",
        "web",
        "search wysh3",
        "perplexity website",
        "wysh3 perplexity"
      ],
      "category": "web-search"
    },
    "wzlnzx--GeminiProChat": {
      "owner": "wzlnzx",
      "name": "GeminiProChat",
      "url": "https://github.com/wzlnzx/GeminiProChat",
      "imageUrl": "/freedevtools/mcp/pfp/wzlnzx.webp",
      "description": "Gemini Pro Chat provides a minimal web interface for interacting with the Gemini Pro AI model. Users can deploy their own instances and customize the integration using an API key for secure and seamless operation.",
      "stars": 0,
      "forks": 0,
      "license": "MIT License",
      "language": "",
      "updated_at": "2024-04-13T09:35:03Z",
      "readme_content": "# GeminiProChat\n\nEnglish | [中文](README_cn.md) | [Italiano](README_it.md) | [日本語](README_jp.md)\n\nMinimal web UI for Gemini Pro Chat.\n\nLive demo: [Gemini Pro Chat](https://www.geminiprochat.com)\n\n[![image](https://github.com/babaohuang/GeminiProChat/assets/559171/d02fd440-401a-410d-a112-4b10935624c6)](https://www.geminiprochat.com)\n\n## Deploy\n\n### Deploy With Vercel(Recommended)\n\n[![Deploy with Vercel](https://vercel.com/button)](https://vercel.com/new/clone?repository-url=https://github.com/babaohuang/GeminiProChat&env=GEMINI_API_KEY&envDescription=Google%20API%20Key%20for%20GeminiProChat&envLink=https://makersuite.google.com/app/apikey&project-name=gemini-pro-chat&repository-name=gemini-pro-chat&demo-title=Gemini%20Pro%20Chat&demo-description=Minimal%20web%20UI%20for%20Gemini%20Pro.&demo-url=https%3A%2F%2Fgeminiprochat.com&demo-image=https%3A%2F%2Fgeminiprochat.com%2Ficon.svg)\n\nJust click the button above and follow the instructions to deploy your own copy of the app.\n\n> [!NOTE]\n> #### Solution for \"User location is not supported for the API use\"\n> If you encounter the issue **\"User location is not supported for the API use\"**, follow these steps to resolve it:\n>\n> 1. Go to this [**palm-netlify-proxy**](https://github.com/antergone/palm-netlify-proxy) repo and click **\"Deploy With Netlify\"**.\n> 2. Once the deployment is complete, you will receive a domain name assigned by Netlify (e.g., `https://xxx.netlify.app`).\n> 3. In your **Gemini Pro Chat** project, set an environment variable named `API_BASE_URL` with the value being the domain you got from deploying the palm proxy (`https://xxx.netlify.app`).\n> 4. Redeploy your **Gemini Pro Chat** project to finalize the configuration. This should resolve the issue.\n>\n> Thanks to [**antergone**](https://github.com/antergone/palm-netlify-proxy) for providing this solution.\n\n### Deploy on Railway\n\n[![Deploy on Railway](https://railway.app/button.svg)](https://railway.app/template/v9QL5u?referralCode=tSzmIe)\n\nJust click the button above and follow the instructions to deploy on Railway.\n\n### Deploy on Zeabur\n\n[![Deploy on Zeabur](https://zeabur.com/button.svg)](https://zeabur.com/templates/1103PJ)\n\nJust click the button above and follow the instructions to deploy on Zeabur.\n\n### Deploy With Docker\n\nTo deploy with Docker, you can use the following command:\n\n```bash\ndocker run --name geminiprochat \\\n--restart always \\\n-p 3000:3000 \\\n-itd \\\n-e GEMINI_API_KEY=your_api_key_here \\\nbabaohuang/geminiprochat:latest\n```\nPlease make sure to replace `your_api_key_here` with your own GEMINI API key.\n\nThis will start the **geminiprochat** service, accessible at `http://localhost:3000`. \n\n## Environment Variables\n\nYou can control the website through environment variables.\n\n| Name | Description | Required |\n| --- | --- | --- |\n| `GEMINI_API_KEY` | Your API Key for GEMINI. You can get it from [here](https://makersuite.google.com/app/apikey).| **✔** |\n| `API_BASE_URL` | Custom base url for GEMINI API. Click [here](https://github.com/babaohuang/GeminiProChat?tab=readme-ov-file#solution-for-user-location-is-not-supported-for-the-api-use) to see when to use this. | ❌ |\n| `HEAD_SCRIPTS` | Inject analytics or other scripts before `</head>` of the page | ❌ |\n| `PUBLIC_SECRET_KEY` | Secret string for the project. Use for generating signatures for API calls | ❌ |\n| `SITE_PASSWORD` | Set password for site, support multiple password separated by comma. If not set, site will be public | ❌ |\n\n## Running Locally\n\n### Pre environment\n1. **Node**: Check that both your development environment and deployment environment are using `Node v18` or later. You can use [nvm](https://github.com/nvm-sh/nvm) to manage multiple `node` versions locally.\n\n   ```bash\n    node -v\n   ```\n\n2. **PNPM**: We recommend using [pnpm](https://pnpm.io/) to manage dependencies. If you have never installed pnpm, you can install it with the following command:\n\n   ```bash\n    npm i -g pnpm\n   ```\n\n3. **GEMINI_API_KEY**: Before running this application, you need to obtain the API key from Google. You can register the API key at [https://makersuite.google.com/app/apikey](https://makersuite.google.com/app/apikey).\n\n### Getting Started\n\n1. Install dependencies\n\n   ```bash\n    pnpm install\n   ```\n\n2. Copy the `.env.example` file, then rename it to `.env`, and add your [`GEMINI_API_KEY`](https://makersuite.google.com/app/apikey) to the `.env` file.\n\n   ```bash\n    GEMINI_API_KEY=AIzaSy...\n   ```\n\n3. Run the application, the local project runs on `http://localhost:3000/`.\n\n   ```bash\n    pnpm run dev\n   ```\n\n## Acknowledgements\n\nThis project is inspired by and based on the following open-source project:\n\n- [ChatGPT-Demo](https://github.com/anse-app/chatgpt-demo) - For the foundational codebase and features.\n\n## Star History\n\n[![Star History Chart](https://api.star-history.com/svg?repos=babaohuang/geminiprochat&type=Timeline)](https://star-history.com/#babaohuang/geminiprochat&Timeline)\n\n## Buy me a coffee\n\nIf this repo is helpful to you, buy me a coffee,thank you very much!😄\n\n<a href=\"https://www.buymeacoffee.com/babaohuang\" target=\"_blank\"><img src=\"https://cdn.buymeacoffee.com/buttons/default-orange.png\" alt=\"Buy Me A Coffee\" height=\"41\" width=\"174\"></a>\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "geminiprochat",
        "gemini",
        "wzlnzx",
        "wzlnzx geminiprochat",
        "geminiprochat gemini",
        "gemini pro"
      ],
      "category": "web-search"
    },
    "xiyuefox--firecrawl": {
      "owner": "xiyuefox",
      "name": "firecrawl",
      "url": "https://github.com/xiyuefox/firecrawl",
      "imageUrl": "/freedevtools/mcp/pfp/xiyuefox.webp",
      "description": "Provides advanced scraping, crawling, and data extraction capabilities to convert web content into structured formats. Facilitates the collection of clean data from any website for AI applications.",
      "stars": 0,
      "forks": 0,
      "license": "GNU Affero General Public License v3.0",
      "language": "",
      "updated_at": "2025-03-21T03:52:51Z",
      "readme_content": "<h3 align=\"center\">\n  <a name=\"readme-top\"></a>\n  <img alt=\"firecrawl_logo\"\n    src=\"https://raw.githubusercontent.com/mendableai/firecrawl/main/img/firecrawl_logo.png\"\n    height=\"200\"\n  >\n</h3>\n<div align=\"center\">\n    <a href=\"https://github.com/mendableai/firecrawl/blob/main/LICENSE\">\n  <img src=\"https://img.shields.io/github/license/mendableai/firecrawl\" alt=\"License\">\n</a>\n    <a href=\"https://pepy.tech/project/firecrawl-py\">\n  <img src=\"https://static.pepy.tech/badge/firecrawl-py\" alt=\"Downloads\">\n</a>\n<a href=\"https://GitHub.com/mendableai/firecrawl/graphs/contributors\">\n  <img src=\"https://img.shields.io/github/contributors/mendableai/firecrawl.svg\" alt=\"GitHub Contributors\">\n</a>\n<a href=\"https://firecrawl.dev\">\n  <img src=\"https://img.shields.io/badge/Visit-firecrawl.dev-orange\" alt=\"Visit firecrawl.dev\">\n</a>\n</div>\n<div>\n  <p align=\"center\">\n    <a href=\"https://twitter.com/firecrawl_dev\">\n      <img src=\"https://img.shields.io/badge/Follow%20on%20X-000000?style=for-the-badge&logo=x&logoColor=white\" alt=\"Follow on X\" />\n    </a>\n    <a href=\"https://www.linkedin.com/company/104100957\">\n      <img src=\"https://img.shields.io/badge/Follow%20on%20LinkedIn-0077B5?style=for-the-badge&logo=linkedin&logoColor=white\" alt=\"Follow on LinkedIn\" />\n    </a>\n    <a href=\"https://discord.com/invite/gSmWdAkdwd\">\n      <img src=\"https://img.shields.io/badge/Join%20our%20Discord-5865F2?style=for-the-badge&logo=discord&logoColor=white\" alt=\"Join our Discord\" />\n    </a>\n  </p>\n</div>\n\n# 🔥 Firecrawl\n\nEmpower your AI apps with clean data from any website. Featuring advanced scraping, crawling, and data extraction capabilities.\n\n_This repository is in development, and we’re still integrating custom modules into the mono repo. It's not fully ready for self-hosted deployment yet, but you can run it locally._\n\n## What is Firecrawl?\n\n[Firecrawl](https://firecrawl.dev?ref=github) is an API service that takes a URL, crawls it, and converts it into clean markdown or structured data. We crawl all accessible subpages and give you clean data for each. No sitemap required. Check out our [documentation](https://docs.firecrawl.dev).\n\n_Pst. hey, you, join our stargazers :)_\n\n<a href=\"https://github.com/mendableai/firecrawl\">\n  <img src=\"https://img.shields.io/github/stars/mendableai/firecrawl.svg?style=social&label=Star&maxAge=2592000\" alt=\"GitHub stars\">\n</a>\n\n## How to use it?\n\nWe provide an easy to use API with our hosted version. You can find the playground and documentation [here](https://firecrawl.dev/playground). You can also self host the backend if you'd like.\n\nCheck out the following resources to get started:\n- [x] **API**: [Documentation](https://docs.firecrawl.dev/api-reference/introduction)\n- [x] **SDKs**: [Python](https://docs.firecrawl.dev/sdks/python), [Node](https://docs.firecrawl.dev/sdks/node), [Go](https://docs.firecrawl.dev/sdks/go), [Rust](https://docs.firecrawl.dev/sdks/rust)\n- [x] **LLM Frameworks**: [Langchain (python)](https://python.langchain.com/docs/integrations/document_loaders/firecrawl/), [Langchain (js)](https://js.langchain.com/docs/integrations/document_loaders/web_loaders/firecrawl), [Llama Index](https://docs.llamaindex.ai/en/latest/examples/data_connectors/WebPageDemo/#using-firecrawl-reader), [Crew.ai](https://docs.crewai.com/), [Composio](https://composio.dev/tools/firecrawl/all), [PraisonAI](https://docs.praison.ai/firecrawl/), [Superinterface](https://superinterface.ai/docs/assistants/functions/firecrawl), [Vectorize](https://docs.vectorize.io/integrations/source-connectors/firecrawl)\n- [x] **Low-code Frameworks**: [Dify](https://dify.ai/blog/dify-ai-blog-integrated-with-firecrawl), [Langflow](https://docs.langflow.org/), [Flowise AI](https://docs.flowiseai.com/integrations/langchain/document-loaders/firecrawl), [Cargo](https://docs.getcargo.io/integration/firecrawl), [Pipedream](https://pipedream.com/apps/firecrawl/)\n- [x] **Others**: [Zapier](https://zapier.com/apps/firecrawl/integrations), [Pabbly Connect](https://www.pabbly.com/connect/integrations/firecrawl/)\n- [ ] Want an SDK or Integration? Let us know by opening an issue.\n\nTo run locally, refer to guide [here](https://github.com/mendableai/firecrawl/blob/main/CONTRIBUTING.md).\n\n### API Key\n\nTo use the API, you need to sign up on [Firecrawl](https://firecrawl.dev) and get an API key.\n\n### Features\n\n- [**Scrape**](#scraping): scrapes a URL and get its content in LLM-ready format (markdown, structured data via [LLM Extract](#llm-extraction-beta), screenshot, html)\n- [**Crawl**](#crawling): scrapes all the URLs of a web page and return content in LLM-ready format\n- [**Map**](#map-alpha): input a website and get all the website urls - extremely fast\n- [**Extract**](#extract): get structured data from single page, multiple pages or entire websites with AI.\n\n### Powerful Capabilities\n- **LLM-ready formats**: markdown, structured data, screenshot, HTML, links, metadata\n- **The hard stuff**: proxies, anti-bot mechanisms, dynamic content (js-rendered), output parsing, orchestration\n- **Customizability**: exclude tags, crawl behind auth walls with custom headers, max crawl depth, etc...\n- **Media parsing**: pdfs, docx, images\n- **Reliability first**: designed to get the data you need - no matter how hard it is\n- **Actions**: click, scroll, input, wait and more before extracting data\n- **Batching (New)**: scrape thousands of URLs at the same time with a new async endpoint.\n\nYou can find all of Firecrawl's capabilities and how to use them in our [documentation](https://docs.firecrawl.dev)\n\n### Crawling\n\nUsed to crawl a URL and all accessible subpages. This submits a crawl job and returns a job ID to check the status of the crawl.\n\n```bash\ncurl -X POST https://api.firecrawl.dev/v1/crawl \\\n    -H 'Content-Type: application/json' \\\n    -H 'Authorization: Bearer fc-YOUR_API_KEY' \\\n    -d '{\n      \"url\": \"https://docs.firecrawl.dev\",\n      \"limit\": 10,\n      \"scrapeOptions\": {\n        \"formats\": [\"markdown\", \"html\"]\n      }\n    }'\n```\n\nReturns a crawl job id and the url to check the status of the crawl.\n\n```json\n{\n  \"success\": true,\n  \"id\": \"123-456-789\",\n  \"url\": \"https://api.firecrawl.dev/v1/crawl/123-456-789\"\n}\n```\n\n### Check Crawl Job\n\nUsed to check the status of a crawl job and get its result.\n\n```bash\ncurl -X GET https://api.firecrawl.dev/v1/crawl/123-456-789 \\\n  -H 'Content-Type: application/json' \\\n  -H 'Authorization: Bearer YOUR_API_KEY'\n```\n\n```json\n{\n  \"status\": \"completed\",\n  \"total\": 36,\n  \"creditsUsed\": 36,\n  \"expiresAt\": \"2024-00-00T00:00:00.000Z\",\n  \"data\": [\n    {\n      \"markdown\": \"[Firecrawl Docs home page![light logo](https://mintlify.s3-us-west-1.amazonaws.com/firecrawl/logo/light.svg)!...\",\n      \"html\": \"<!DOCTYPE html><html lang=\\\"en\\\" class=\\\"js-focus-visible lg:[--scroll-mt:9.5rem]\\\" data-js-focus-visible=\\\"\\\">...\",\n      \"metadata\": {\n        \"title\": \"Build a 'Chat with website' using Groq Llama 3 | Firecrawl\",\n        \"language\": \"en\",\n        \"sourceURL\": \"https://docs.firecrawl.dev/learn/rag-llama3\",\n        \"description\": \"Learn how to use Firecrawl, Groq Llama 3, and Langchain to build a 'Chat with your website' bot.\",\n        \"ogLocaleAlternate\": [],\n        \"statusCode\": 200\n      }\n    }\n  ]\n}\n```\n\n### Scraping\n\nUsed to scrape a URL and get its content in the specified formats.\n\n```bash\ncurl -X POST https://api.firecrawl.dev/v1/scrape \\\n    -H 'Content-Type: application/json' \\\n    -H 'Authorization: Bearer YOUR_API_KEY' \\\n    -d '{\n      \"url\": \"https://docs.firecrawl.dev\",\n      \"formats\" : [\"markdown\", \"html\"]\n    }'\n```\n\nResponse:\n\n```json\n{\n  \"success\": true,\n  \"data\": {\n    \"markdown\": \"Launch Week I is here! [See our Day 2 Release 🚀](https://www.firecrawl.dev/blog/launch-week-i-day-2-doubled-rate-limits)[💥 Get 2 months free...\",\n    \"html\": \"<!DOCTYPE html><html lang=\\\"en\\\" class=\\\"light\\\" style=\\\"color-scheme: light;\\\"><body class=\\\"__variable_36bd41 __variable_d7dc5d font-inter ...\",\n    \"metadata\": {\n      \"title\": \"Home - Firecrawl\",\n      \"description\": \"Firecrawl crawls and converts any website into clean markdown.\",\n      \"language\": \"en\",\n      \"keywords\": \"Firecrawl,Markdown,Data,Mendable,Langchain\",\n      \"robots\": \"follow, index\",\n      \"ogTitle\": \"Firecrawl\",\n      \"ogDescription\": \"Turn any website into LLM-ready data.\",\n      \"ogUrl\": \"https://www.firecrawl.dev/\",\n      \"ogImage\": \"https://www.firecrawl.dev/og.png?123\",\n      \"ogLocaleAlternate\": [],\n      \"ogSiteName\": \"Firecrawl\",\n      \"sourceURL\": \"https://firecrawl.dev\",\n      \"statusCode\": 200\n    }\n  }\n}\n```\n\n### Map (Alpha)\n\nUsed to map a URL and get urls of the website. This returns most links present on the website.\n\n```bash cURL\ncurl -X POST https://api.firecrawl.dev/v1/map \\\n    -H 'Content-Type: application/json' \\\n    -H 'Authorization: Bearer YOUR_API_KEY' \\\n    -d '{\n      \"url\": \"https://firecrawl.dev\"\n    }'\n```\n\nResponse:\n\n```json\n{\n  \"status\": \"success\",\n  \"links\": [\n    \"https://firecrawl.dev\",\n    \"https://www.firecrawl.dev/pricing\",\n    \"https://www.firecrawl.dev/blog\",\n    \"https://www.firecrawl.dev/playground\",\n    \"https://www.firecrawl.dev/smart-crawl\",\n  ]\n}\n```\n\n#### Map with search\n\nMap with `search` param allows you to search for specific urls inside a website.\n\n```bash cURL\ncurl -X POST https://api.firecrawl.dev/v1/map \\\n    -H 'Content-Type: application/json' \\\n    -H 'Authorization: Bearer YOUR_API_KEY' \\\n    -d '{\n      \"url\": \"https://firecrawl.dev\",\n      \"search\": \"docs\"\n    }'\n```\n\nResponse will be an ordered list from the most relevant to the least relevant.\n\n```json\n{\n  \"status\": \"success\",\n  \"links\": [\n    \"https://docs.firecrawl.dev\",\n    \"https://docs.firecrawl.dev/sdks/python\",\n    \"https://docs.firecrawl.dev/learn/rag-llama3\",\n  ]\n}\n```\n\n### Extract\n\nGet structured data from entire websites with a prompt and/or a schema.\n\nYou can extract structured data from one or multiple URLs, including wildcards:\n\nSingle Page:\nExample: https://firecrawl.dev/some-page\n\nMultiple Pages / Full Domain\nExample: https://firecrawl.dev/*\n\nWhen you use /*, Firecrawl will automatically crawl and parse all URLs it can discover in that domain, then extract the requested data.\n\n```bash\ncurl -X POST https://api.firecrawl.dev/v1/extract \\\n    -H 'Content-Type: application/json' \\\n    -H 'Authorization: Bearer YOUR_API_KEY' \\\n    -d '{\n      \"urls\": [\n        \"https://firecrawl.dev/*\", \n        \"https://docs.firecrawl.dev/\", \n        \"https://www.ycombinator.com/companies\"\n      ],\n      \"prompt\": \"Extract the company mission, whether it is open source, and whether it is in Y Combinator from the page.\",\n      \"schema\": {\n        \"type\": \"object\",\n        \"properties\": {\n          \"company_mission\": {\n            \"type\": \"string\"\n          },\n          \"is_open_source\": {\n            \"type\": \"boolean\"\n          },\n          \"is_in_yc\": {\n            \"type\": \"boolean\"\n          }\n        },\n        \"required\": [\n          \"company_mission\",\n          \"is_open_source\",\n          \"is_in_yc\"\n        ]\n      }\n    }'\n```\n\n```json\n{\n  \"success\": true,\n  \"id\": \"44aa536d-f1cb-4706-ab87-ed0386685740\",\n  \"urlTrace\": []\n}\n```\n\nIf you are using the sdks, it will auto pull the response for you:\n\n```json\n{\n  \"success\": true,\n  \"data\": {\n    \"company_mission\": \"Firecrawl is the easiest way to extract data from the web. Developers use us to reliably convert URLs into LLM-ready markdown or structured data with a single API call.\",\n    \"supports_sso\": false,\n    \"is_open_source\": true,\n    \"is_in_yc\": true\n  }\n}\n```\n\n### LLM Extraction (Beta)\n\nUsed to extract structured data from scraped pages.\n\n```bash\ncurl -X POST https://api.firecrawl.dev/v1/scrape \\\n    -H 'Content-Type: application/json' \\\n    -H 'Authorization: Bearer YOUR_API_KEY' \\\n    -d '{\n      \"url\": \"https://www.mendable.ai/\",\n      \"formats\": [\"json\"],\n      \"jsonOptions\": {\n        \"schema\": {\n          \"type\": \"object\",\n          \"properties\": {\n            \"company_mission\": {\n                      \"type\": \"string\"\n            },\n            \"supports_sso\": {\n                      \"type\": \"boolean\"\n            },\n            \"is_open_source\": {\n                      \"type\": \"boolean\"\n            },\n            \"is_in_yc\": {\n                      \"type\": \"boolean\"\n            }\n          },\n          \"required\": [\n            \"company_mission\",\n            \"supports_sso\",\n            \"is_open_source\",\n            \"is_in_yc\"\n          ]\n        }\n      }\n    }'\n```\n\n```json\n{\n  \"success\": true,\n  \"data\": {\n    \"content\": \"Raw Content\",\n    \"metadata\": {\n      \"title\": \"Mendable\",\n      \"description\": \"Mendable allows you to easily build AI chat applications. Ingest, customize, then deploy with one line of code anywhere you want. Brought to you by SideGuide\",\n      \"robots\": \"follow, index\",\n      \"ogTitle\": \"Mendable\",\n      \"ogDescription\": \"Mendable allows you to easily build AI chat applications. Ingest, customize, then deploy with one line of code anywhere you want. Brought to you by SideGuide\",\n      \"ogUrl\": \"https://mendable.ai/\",\n      \"ogImage\": \"https://mendable.ai/mendable_new_og1.png\",\n      \"ogLocaleAlternate\": [],\n      \"ogSiteName\": \"Mendable\",\n      \"sourceURL\": \"https://mendable.ai/\"\n    },\n    \"json\": {\n      \"company_mission\": \"Train a secure AI on your technical resources that answers customer and employee questions so your team doesn't have to\",\n      \"supports_sso\": true,\n      \"is_open_source\": false,\n      \"is_in_yc\": true\n    }\n  }\n}\n```\n\n### Extracting without a schema (New)\n\nYou can now extract without a schema by just passing a `prompt` to the endpoint. The llm chooses the structure of the data.\n\n```bash\ncurl -X POST https://api.firecrawl.dev/v1/scrape \\\n    -H 'Content-Type: application/json' \\\n    -H 'Authorization: Bearer YOUR_API_KEY' \\\n    -d '{\n      \"url\": \"https://docs.firecrawl.dev/\",\n      \"formats\": [\"json\"],\n      \"jsonOptions\": {\n        \"prompt\": \"Extract the company mission from the page.\"\n      }\n    }'\n```\n\n### Interacting with the page with Actions (Cloud-only)\n\nFirecrawl allows you to perform various actions on a web page before scraping its content. This is particularly useful for interacting with dynamic content, navigating through pages, or accessing content that requires user interaction.\n\nHere is an example of how to use actions to navigate to google.com, search for Firecrawl, click on the first result, and take a screenshot.\n\n```bash\ncurl -X POST https://api.firecrawl.dev/v1/scrape \\\n    -H 'Content-Type: application/json' \\\n    -H 'Authorization: Bearer YOUR_API_KEY' \\\n    -d '{\n        \"url\": \"google.com\",\n        \"formats\": [\"markdown\"],\n        \"actions\": [\n            {\"type\": \"wait\", \"milliseconds\": 2000},\n            {\"type\": \"click\", \"selector\": \"textarea[title=\\\"Search\\\"]\"},\n            {\"type\": \"wait\", \"milliseconds\": 2000},\n            {\"type\": \"write\", \"text\": \"firecrawl\"},\n            {\"type\": \"wait\", \"milliseconds\": 2000},\n            {\"type\": \"press\", \"key\": \"ENTER\"},\n            {\"type\": \"wait\", \"milliseconds\": 3000},\n            {\"type\": \"click\", \"selector\": \"h3\"},\n            {\"type\": \"wait\", \"milliseconds\": 3000},\n            {\"type\": \"screenshot\"}\n        ]\n    }'\n```\n\n### Batch Scraping Multiple URLs (New)\n\nYou can now batch scrape multiple URLs at the same time. It is very similar to how the /crawl endpoint works. It submits a batch scrape job and returns a job ID to check the status of the batch scrape.\n\n```bash\ncurl -X POST https://api.firecrawl.dev/v1/batch/scrape \\\n    -H 'Content-Type: application/json' \\\n    -H 'Authorization: Bearer YOUR_API_KEY' \\\n    -d '{\n      \"urls\": [\"https://docs.firecrawl.dev\", \"https://docs.firecrawl.dev/sdks/overview\"],\n      \"formats\" : [\"markdown\", \"html\"]\n    }'\n```\n\n### Search\n\nThe search endpoint combines web search with Firecrawl’s scraping capabilities to return full page content for any query.\n\nInclude `scrapeOptions` with `formats: [\"markdown\"]` to get complete markdown content for each search result otherwise it defaults to getting SERP results (url, title, description).\n\n```bash\ncurl -X POST https://api.firecrawl.dev/v1/search \\\n    -H 'Content-Type: application/json' \\\n    -H 'Authorization: Bearer YOUR_API_KEY' \\\n    -d '{\n      \"query\": \"What is Mendable?\"\n    }'\n```\n\n```json\n{\n  \"success\": true,\n  \"data\": [\n    {\n      \"url\": \"https://mendable.ai\",\n      \"title\": \"Mendable | AI for CX and Sales\",\n      \"description\": \"AI for CX and Sales\"\n    }\n  ]\n}\n```\n\n## Using Python SDK\n\n### Installing Python SDK\n\n```bash\npip install firecrawl-py\n```\n\n### Crawl a website\n\n```python\nfrom firecrawl.firecrawl import FirecrawlApp\n\napp = FirecrawlApp(api_key=\"fc-YOUR_API_KEY\")\n\n# Scrape a website:\nscrape_status = app.scrape_url(\n  'https://firecrawl.dev', \n  params={'formats': ['markdown', 'html']}\n)\nprint(scrape_status)\n\n# Crawl a website:\ncrawl_status = app.crawl_url(\n  'https://firecrawl.dev', \n  params={\n    'limit': 100, \n    'scrapeOptions': {'formats': ['markdown', 'html']}\n  },\n  poll_interval=30\n)\nprint(crawl_status)\n```\n\n### Extracting structured data from a URL\n\nWith LLM extraction, you can easily extract structured data from any URL. We support pydantic schemas to make it easier for you too. Here is how you to use it:\n\n```python\n\nfrom firecrawl.firecrawl import FirecrawlApp\n\napp = FirecrawlApp(api_key=\"fc-YOUR_API_KEY\")\n\nclass ArticleSchema(BaseModel):\n    title: str\n    points: int\n    by: str\n    commentsURL: str\n\nclass TopArticlesSchema(BaseModel):\n    top: List[ArticleSchema] = Field(..., max_items=5, description=\"Top 5 stories\")\n\ndata = app.scrape_url('https://news.ycombinator.com', {\n    'formats': ['json'],\n    'jsonOptions': {\n        'schema': TopArticlesSchema.model_json_schema()\n    }\n})\nprint(data[\"json\"])\n```\n\n## Using the Node SDK\n\n### Installation\n\nTo install the Firecrawl Node SDK, you can use npm:\n\n```bash\nnpm install @mendable/firecrawl-js\n```\n\n### Usage\n\n1. Get an API key from [firecrawl.dev](https://firecrawl.dev)\n2. Set the API key as an environment variable named `FIRECRAWL_API_KEY` or pass it as a parameter to the `FirecrawlApp` class.\n\n```js\nimport FirecrawlApp, { CrawlParams, CrawlStatusResponse } from '@mendable/firecrawl-js';\n\nconst app = new FirecrawlApp({apiKey: \"fc-YOUR_API_KEY\"});\n\n// Scrape a website\nconst scrapeResponse = await app.scrapeUrl('https://firecrawl.dev', {\n  formats: ['markdown', 'html'],\n});\n\nif (scrapeResponse) {\n  console.log(scrapeResponse)\n}\n\n// Crawl a website\nconst crawlResponse = await app.crawlUrl('https://firecrawl.dev', {\n  limit: 100,\n  scrapeOptions: {\n    formats: ['markdown', 'html'],\n  }\n} satisfies CrawlParams, true, 30) satisfies CrawlStatusResponse;\n\nif (crawlResponse) {\n  console.log(crawlResponse)\n}\n```\n\n\n### Extracting structured data from a URL\n\nWith LLM extraction, you can easily extract structured data from any URL. We support zod schema to make it easier for you too. Here is how to use it:\n\n```js\nimport FirecrawlApp from \"@mendable/firecrawl-js\";\nimport { z } from \"zod\";\n\nconst app = new FirecrawlApp({\n  apiKey: \"fc-YOUR_API_KEY\"\n});\n\n// Define schema to extract contents into\nconst schema = z.object({\n  top: z\n    .array(\n      z.object({\n        title: z.string(),\n        points: z.number(),\n        by: z.string(),\n        commentsURL: z.string(),\n      })\n    )\n    .length(5)\n    .describe(\"Top 5 stories on Hacker News\"),\n});\n\nconst scrapeResult = await app.scrapeUrl(\"https://news.ycombinator.com\", {\n  jsonOptions: { extractionSchema: schema },\n});\n\nconsole.log(scrapeResult.data[\"json\"]);\n```\n\n## Open Source vs Cloud Offering\n\nFirecrawl is open source available under the AGPL-3.0 license. \n\nTo deliver the best possible product, we offer a hosted version of Firecrawl alongside our open-source offering. The cloud solution allows us to continuously innovate and maintain a high-quality, sustainable service for all users.\n\nFirecrawl Cloud is available at [firecrawl.dev](https://firecrawl.dev) and offers a range of features that are not available in the open source version:\n\n![Open Source vs Cloud Offering](https://raw.githubusercontent.com/mendableai/firecrawl/main/img/open-source-cloud.png)\n\n\n## Contributing\n\nWe love contributions! Please read our [contributing guide](CONTRIBUTING.md) before submitting a pull request. If you'd like to self-host, refer to the [self-hosting guide](SELF_HOST.md).\n\n_It is the sole responsibility of the end users to respect websites' policies when scraping, searching and crawling with Firecrawl. Users are advised to adhere to the applicable privacy policies and terms of use of the websites prior to initiating any scraping activities. By default, Firecrawl respects the directives specified in the websites' robots.txt files when crawling. By utilizing Firecrawl, you expressly agree to comply with these conditions._\n\n## Contributors\n\n<a href=\"https://github.com/mendableai/firecrawl/graphs/contributors\">\n  <img alt=\"contributors\" src=\"https://contrib.rocks/image?repo=mendableai/firecrawl\"/>\n</a>\n\n## License Disclaimer\n\nThis project is primarily licensed under the GNU Affero General Public License v3.0 (AGPL-3.0), as specified in the LICENSE file in the root directory of this repository. However, certain components of this project are licensed under the MIT License. Refer to the LICENSE files in these specific directories for details.\n\nPlease note:\n\n- The AGPL-3.0 license applies to all parts of the project unless otherwise specified.\n- The SDKs and some UI components are licensed under the MIT License. Refer to the LICENSE files in these specific directories for details.\n- When using or contributing to this project, ensure you comply with the appropriate license terms for the specific component you are working with.\n\nFor more details on the licensing of specific components, please refer to the LICENSE files in the respective directories or contact the project maintainers.\n\n\n<p align=\"right\" style=\"font-size: 14px; color: #555; margin-top: 20px;\">\n    <a href=\"#readme-top\" style=\"text-decoration: none; color: #007bff; font-weight: bold;\">\n        ↑ Back to Top ↑\n    </a>\n</p>\n",
      "npm_url": "https://www.npmjs.com/package/firecrawl",
      "npm_downloads": 84977,
      "keywords": [
        "scraping",
        "firecrawl",
        "xiyuefox",
        "advanced scraping",
        "scraping crawling",
        "crawling data"
      ],
      "category": "web-search"
    },
    "xiyuefox--mcp-server-perplexity": {
      "owner": "xiyuefox",
      "name": "mcp-server-perplexity",
      "url": "https://github.com/xiyuefox/mcp-server-perplexity",
      "imageUrl": "/freedevtools/mcp/pfp/xiyuefox.webp",
      "description": "Request chat completions with citations from Perplexity, providing efficient and informative interactions through the Perplexity API. It is designed for seamless integration into applications, allowing users to enhance conversational capabilities.",
      "stars": 0,
      "forks": 0,
      "license": "MIT License",
      "language": "",
      "updated_at": "2025-03-09T12:41:16Z",
      "readme_content": "# Perplexity MCP Server\n\n[![smithery badge](https://smithery.ai/badge/mcp-server-perplexity)](https://smithery.ai/server/mcp-server-perplexity)\n\nMCP Server for the Perplexity API.\n\n> :warning: **Limitations:**\n> - The Claude Desktop client may timeout if Perplexity processing takes too long\n> - This issue might be resolved if Claude Desktop implements support for long running operations and progress reporting in the future\n> - Implementation updates to handle these features will be made if they become available\n\n<a href=\"https://glama.ai/mcp/servers/hchfq9bydq\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/hchfq9bydq/badge\" alt=\"Perplexity Server MCP server\" /></a>\n\n## Components\n\n### Tools\n\n- **ask_perplexity**: Request chat completion with citations from Perplexity  \n\n## Quickstart\n\n### Install\n\n#### Claude Desktop\n\n- On macOS: `~/Library/Application\\ Support/Claude/claude_desktop_config.json`  \n- On Windows: `%APPDATA%/Claude/claude_desktop_config.json`\n\n```\n\"mcpServers\": {\n  \"Perplexity\": {\n    \"command\": \"uvx\",\n    \"args\": [\n      \"mcp-server-perplexity\"\n    ],\n    \"env\": {\n      \"PERPLEXITY_API_KEY\": \"your-perplexity-api-key\"\n    }\n  }\n}\n```\n",
      "npm_url": "https://www.npmjs.com/package/mcp-server-perplexity",
      "npm_downloads": 934,
      "keywords": [
        "chat",
        "xiyuefox",
        "perplexity",
        "chat completions",
        "perplexity api",
        "perplexity request"
      ],
      "category": "web-search"
    },
    "xytangme--neodb-mcp": {
      "owner": "xytangme",
      "name": "neodb-mcp",
      "url": "https://github.com/xytangme/neodb-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/xytangme.webp",
      "description": "Interact with a social book cataloging service to fetch user information, search for books, and retrieve detailed book information through its API.",
      "stars": 1,
      "forks": 6,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-10-03T04:12:33Z",
      "readme_content": "# NeoDB MCP Server\n\nA Message Control Protocol (MCP) server implementation for interacting with [NeoDB](https://neodb.social/), a social book cataloging service. This server provides tools to fetch user information, search books, and retrieve detailed book information through NeoDB's API.\n\n<a href=\"https://glama.ai/mcp/servers/1any3eeaza\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/1any3eeaza/badge\" alt=\"NeoDB Server MCP server\" /></a>\n\n## Setup\n\n### Install UV\nFirst, install UV package installer:\n\n```bash\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n```\n\n### Create Virtual Environment\nCreate and activate a Python virtual environment using UV:\n\n```bash\nuv venv\nsource .venv/bin/activate  # On Unix/macOS\n# or\n.venv\\Scripts\\activate     # On Windows\n```\n\n### Install Dependencies\nInstall project dependencies using UV:\n\n```bash\nuv pip install .\n```\n\n## Available Tools\n\nThe server provides the following tools:\n\n1. **get-user-info**\n   - Gets current user's basic information\n   - No parameters required\n\n2. **search-books**\n   - Searches items in the catalog\n   - Parameters:\n     - `query` (string): Search query for books\n\n3. **get-book**\n   - Gets detailed information about a specific book\n   - Parameters:\n     - `book_id` (string): The ID of the book to retrieve\n\n## Usage with Claude Desktop\n\n### Get Access Token\n\nThere are two ways to get your access token:\n\n1. Using the official guide: Follow the [official documentation](https://neodb.net/api/) to obtain your access token.\n\n2. Using automated script: You can use the [neodb-get-access-token](https://github.com/xytangme/neodb-get-access-token) script which provides a simplified way to get your access token.\n\n### Update Config `claude_desktop_config.json`\n\n```json\n{\n  \"mcpServers\": {\n    \"neodb\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"--directory\",\n        \"<PATH_TO_PROJECT_DIR>\",\n        \"run\",\n        \"<PATH_TO_SCRIPT>\",\n        \"<API_BASE> e.g. https://neodb.social\",\n        \"<ACCESS_TOKEN>\"\n      ]\n    }\n  }\n}\n```\n\nWhere:\n- `<API_BASE>`: The base URL for the NeoDB API\n- `<ACCESS_TOKEN>`: Your NeoDB API access token\n\n## License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "neodb",
        "cataloging",
        "xytangme",
        "xytangme neodb",
        "search xytangme",
        "neodb mcp"
      ],
      "category": "web-search"
    },
    "y7ut--mcp-tavily-search": {
      "owner": "y7ut",
      "name": "mcp-tavily-search",
      "url": "https://github.com/y7ut/mcp-tavily-search",
      "imageUrl": "/freedevtools/mcp/pfp/y7ut.webp",
      "description": "This server provides search capabilities utilizing the Tavily platform, allowing users to perform search queries within the specified context. It integrates with the Model Context Protocol to facilitate search operations through a command-line interface or Docker.",
      "stars": 0,
      "forks": 2,
      "license": "MIT License",
      "language": "Go",
      "updated_at": "2025-04-09T06:10:03Z",
      "readme_content": "# MCP TAVILY SEARCH\n\nA Model Context Protocol (MCP) server that provide search by tavily.\n\n## Quick start\n\ninstall\n\n```sh\ngo install github.com/y7ut/mcp-tavily-search@latest\n```\n\nadd config to mcp config file.\n\n```json\n{\n  \"mcpServers\": {\n    \"tavily\": {\n      \"command\": \"mcp-tavily-search\",\n      \"args\": [\n        \"run\",\n        \"tvly-*******************\"\n      ]\n    }\n  }\n}\n```\n\n```json\n{\n  \"mcpServers\": {\n    \"tavily\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"--rm\",\n        \"-i\",\n        \"docker.ijiwei.com/mcp/mcp-tavily-search:latest\",\n        \"run\",\n        \"tvly-*******************\"\n      ]\n    }\n  }\n}\n```\n\nor debug\n\n```sh\nnpx @modelcontextprotocol/inspector mcp-tavily-search run tvly-xxxxxxxxxx\n\nnpx --no-cache @modelcontextprotocol/inspector docker run --rm -i mcp-tavily-search:latest run tvly-xxxxx\n```\n\n## Tools\n\n### search_news\n\n| **Parameter**   | **Type**   | **Default Value** | **Description**                                                                                                                                           | **Required** |\n|------------------|------------|-------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------|--------------|\n| `keyword`        | `string`   | N/A               | The keyword to search for.                                                                                                                                | Yes          |\n| `days`           | `number`   | `7`               | Number of days to search within. Default is 7 days.                                                                                                       | No           |\n| `limit`          | `number`   | `5`               | Number of news articles to return. Default is 5.                                                                                                          | No           |\n| `search_depth`   | `string`   | `\"basic\"`         | The depth of the search. It can be `\"basic\"` or `\"advanced\"`. Default is `\"basic\"`.                                                                       | No           |\n| `topic`          | `string`   | `\"news\"`          | The topic of the search. Options are `\"general\"` (unprocessed pages) or `\"news\"` (high-quality news). Default is `\"news\"`.                                 | No           |\n",
      "npm_url": "https://www.npmjs.com/package/mcp-tavily-search",
      "npm_downloads": 5604,
      "keywords": [
        "search",
        "docker",
        "tavily",
        "tavily search",
        "search server",
        "provides search"
      ],
      "category": "web-search"
    },
    "yanjunz--mcp_search_images": {
      "owner": "yanjunz",
      "name": "mcp_search_images",
      "url": "https://github.com/yanjunz/mcp_search_images",
      "imageUrl": "/freedevtools/mcp/pfp/yanjunz.webp",
      "description": "Search for high-quality images from sources like Unsplash, Pexels, and Pixabay, and generate custom icons based on text descriptions, facilitating visual enhancements for projects.",
      "stars": 10,
      "forks": 1,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-15T13:46:13Z",
      "readme_content": "# MCP 图像搜索与图标生成服务\n\n基于多个图片API的搜索服务和图标生成功能，专门设计用于与 Cursor MCP 服务集成。支持图片搜索、下载和AI生成图标。\n\n\n\n## 工作原理\n\n本工具通过MCP (Model Control Protocol) 为Cursor IDE提供图像搜索和图标生成功能：\n\n1. **搜索图片**: 连接Unsplash、Pexels和Pixabay等图片源，根据关键词搜索高质量图片\n2. **下载图片**: 将搜索到的图片下载到指定位置，方便直接在项目中使用\n3. **生成图标**: 基于文本描述生成自定义图标，满足项目UI需求\n\n### 系统工作流程\n\n```\n用户 (在Cursor中) → 向Claude/大模型提问 → 大模型调用MCP工具 → 工具处理请求 → 返回结果 → 大模型展示结果\n```\n\n比如，你可以在Cursor中向Claude询问\"帮我找5张关于太空的图片\"，Claude会通过MCP工具搜索并展示图片，然后你可以进一步要求下载或生成特定图标。\n\n## 功能特点\n\n* 支持多个图片源搜索 (Unsplash, Pexels, Pixabay)\n* 高质量图标生成 (基于Together AI)\n* 简单易用的API\n* 完整的错误处理\n* 自定义保存路径和文件名\n* 可调整图片尺寸\n\n## 环境准备\n\n### 1. Python 环境\n\n* Python 3.10+\n* 下载地址： https://www.python.org/downloads/\n* 推荐使用 pyenv 管理 Python 版本：\n\n```bash\n# macOS 安装 pyenv\nbrew install pyenv\n\n# 安装 Python\npyenv install 3.13.2\npyenv global 3.13.2\n```\n\n### 2. uv 包管理工具\n\nuv 是一个快速的 Python 包管理器，需要先安装：\n\n```bash\n# macOS 安装 uv\nbrew install uv\n\n# 或者使用 pip 安装\npip install uv\n```\n\n### 3. 图片API密钥\n\n#### Unsplash API 密钥\n1. 访问 [Unsplash Developers](https://unsplash.com/developers)\n2. 注册/登录账号\n3. 创建新的应用程序\n4. 获取 Access Key\n\n#### Pexels API 密钥\n1. 访问 [Pexels API](https://www.pexels.com/api/)\n2. 注册/登录账号\n3. 请求API密钥\n\n#### Pixabay API 密钥\n1. 访问 [Pixabay API](https://pixabay.com/api/docs/)\n2. 注册/登录账号\n3. 获取API密钥\n\n#### Together AI API 密钥\n1. 访问 [Together AI API Keys](https://api.together.xyz/keys)\n2. 注册/登录账号\n3. 创建新的 API 密钥\n\n### 4. Cursor\n\n* 下载并安装 [Cursor IDE](https://cursor.sh/)\n* 确保 Cursor 已正确配置 Python 环境\n\n## 安装配置\n\n1. 克隆项目：\n\n```bash\ngit clone https://github.com/yanjunz/mcp_search_images.git\n```\n\n2. 安装依赖：\n\n```bash\npython3 -m pip install fastmcp requests\n```\n\n出现证书问题可以使用：\n\n```bash\npython3 -m pip install fastmcp requests --trusted-host pypi.org --trusted-host files.pythonhosted.org --upgrade --force-reinstall --no-cache-dir\n```\n\n3. 配置 API 密钥：\n\n从模板创建配置文件：\n\n```bash\n# 复制模板文件作为配置文件\ncp config.json.template config.json\n\n# 编辑配置文件，设置API密钥\nnano config.json  # 或使用其他编辑器\n```\n\n在 `config.json` 中修改以下配置：\n\n```json\n{\n    \"api\": {\n        \"unsplash_access_key\": \"你的Unsplash访问密钥\",\n        \"pexels_api_key\": \"你的Pexels API密钥\",\n        \"pixabay_api_key\": \"你的Pixabay API密钥\",\n        \"together_api_key\": \"你的Together API密钥\",\n        \"timeout\": 30,\n        \"max_retries\": 3,\n        \"retry_delay\": 5\n    },\n    // ...其他配置...\n}\n```\n\n> **注意**：请确保不要将包含API密钥的配置文件提交到版本控制系统中。\n> 项目中的 `.gitignore` 文件已配置为忽略 `config.json`，但保留 `config.json.template`。\n\n## 运行服务\n\n### 方法一：直接使用Python运行\n\n这是最简单的方式，直接使用Python运行服务：\n\n```bash\npython3.11 main.py\n```\n\n服务启动后会显示以下信息:\n```\n启动图片搜索服务 - 端口: 5173\n提供的工具: search_images, download_image, generate_icon\nINFO:     Started server process [xxxxx]\nINFO:     Waiting for application startup.\nINFO:     Application startup complete.\nINFO:     Uvicorn running on http://0.0.0.0:5173 (Press CTRL+C to quit)\n```\n\n### 方法二：使用fastmcp命令运行\n\n如果您安装了fastmcp包，也可以使用fastmcp命令运行：\n\n1. 开发模式运行（带调试界面）：\n\n```bash\nfastmcp dev main.py\n```\n\n2. 生产模式运行：\n\n```bash\nfastmcp run main.py\n```\n\n3. 如果端口被占用，可以指定其他端口：\n\n```bash\nPORT=5174 fastmcp dev main.py\n```\n\n### 方法三：使用uv运行\n\n如果您使用uv作为包管理器：\n\n```bash\nuv run --with fastmcp fastmcp run main.py\n```\n\n或者在开发模式下：\n\n```bash\nuv run --with fastmcp fastmcp dev main.py\n```\n\n### Cursor与MCP的工作原理\n\n为了更好地理解和解决连接问题，以下是Cursor与MCP服务交互的基本工作原理：\n\n1. **MCP服务启动流程**：\n   * 当运行`python3.11 main.py`时，服务初始化并创建SSE（Server-Sent Events）应用\n   * 服务在指定端口（默认5173）开始监听请求\n   * 服务注册工具函数（search_images, download_image, generate_icon）\n   * 对于使用ServerLink方式的连接，服务需要在`/sse`路径上正确处理SSE请求\n\n2. **Cursor连接流程**：\n   * 当在Cursor设置中添加MCP工具时，Cursor尝试与提供的URL建立连接\n   * Cursor发送初始化请求，检查服务是否正常响应\n   * 服务需要返回正确的MCP协议响应，包括可用工具列表\n   * 连接成功后，Cursor会将该工具添加到可用工具列表\n   \n3. **诊断连接问题**：\n   * 检查服务是否在运行：`lsof -i :5173`\n   * 检查网络连接：`curl http://localhost:5173`\n   * 检查服务是否正确实现MCP协议：服务启动日志应显示注册的工具\n   * 检查防火墙和网络权限：本地服务有时可能被防火墙阻止\n   \n4. **完整的测试流程**：\n   ```bash\n   # 1. 停止任何可能正在运行的服务\n   pkill -f \"python.*main.py\"\n   \n   # 2. 启动服务（在前台运行以查看日志）\n   python3.11 main.py\n   \n   # 3. 在新的终端窗口中，测试连接\n   curl http://localhost:5173\n   \n   # 4. 测试SSE端点（用于ServerLink方式）\n   curl http://localhost:5173/sse\n   \n   # 5. 在Cursor中添加MCP工具并测试\n   ```\n\n如果按照以上步骤操作后仍然无法连接，可能需要检查Python版本兼容性或依赖包是否正确安装。有时重新安装依赖包也有帮助：\n\n```bash\npython3.11 -m pip uninstall fastmcp mcp uvicorn starlette -y\npython3.11 -m pip install fastmcp mcp uvicorn starlette\n```\n\n## 使用说明\n\n### 在 Cursor IDE 中使用\n\n1. 确保服务正在运行\n   ```bash\n   # 直接运行Python脚本\n   python3.11 main.py\n   ```\n   服务启动后会显示以下信息:\n   ```\n   启动图片搜索服务 - 端口: 5173\n   提供的工具: search_images, download_image, generate_icon\n   INFO:     Started server process [xxxxx]\n   INFO:     Waiting for application startup.\n   INFO:     Application startup complete.\n   INFO:     Uvicorn running on http://0.0.0.0:5173 (Press CTRL+C to quit)\n   ```\n\n2. 在Cursor中添加MCP服务:\n   * 打开Cursor IDE\n   * 点击左下角的齿轮图标，打开设置\n   * 选择\"AI & Copilot\"设置\n   * 在\"MCP工具\"部分点击\"添加MCP工具\"\n   * 填写以下信息:\n     - 名称: 图片搜索服务\n     - 类型: SSE (Server-Sent Events)\n     - URL: http://localhost:5173\n     - 点击\"保存\"\n     \n   **备选配置方法**:\n   * 某些版本的Cursor可能需要使用ServerLink配置:\n     - 名称: 图片搜索服务\n     - 类型: sse\n     - ServerLink: http://localhost:5173/sse\n     - 点击\"保存\"\n\n   > **注意**: 如果出现\"Fail to create client\"错误，请检查以下几点:\n   > 1. 确认服务正在运行 (通过`lsof -i :5173`检查端口是否被监听)\n   > 2. 尝试在浏览器中访问`http://localhost:5173`测试连接性\n   > 3. 确保URL没有多余的斜杠或空格\n   > 4. 对于ServerLink方式，确保使用正确的端点路径`/sse`\n   > 5. 重启服务后再次尝试添加\n   > 6. 有时需要重启Cursor IDE以清除之前的连接缓存\n\n3. 开始使用MCP工具:\n   * 在Cursor中打开包含Claude或其他支持工具调用的大模型对话窗口\n   * 当服务正在运行时，大模型可以自动发现并使用该工具\n   * 如果大模型未自动发现工具，可以提示它:\"请使用图片搜索服务来查找图片\"\n\n4. 在开发过程中随时使用:\n   * 编写代码时需要图标素材，可以直接向大模型描述需求\n   * 例如:\"帮我找一些适合作为登录按钮的图标\"\n   * 大模型会调用MCP工具搜索图片并展示结果\n   * 你可以进一步要求下载或生成自定义图标\n\n5. 查看图标保存位置:\n   * 默认情况下，图标会保存在项目根目录下的`icons`文件夹中\n   * 可以通过以下命令查看已保存的图标:\n     ```bash\n     ls -la icons\n     ```\n\n### 功能使用示例\n\n#### 搜索图片\n\n可以直接向大模型描述需求:\n```\n搜索关键词为\"technology\"的图片\n```\n或更具体的描述:\n```\n请在Unsplash上搜索5张关于\"artificial intelligence\"的图片\n```\n\n#### 下载图片\n\n当大模型显示搜索结果后，你可以要求下载特定图片:\n```\n下载第2张图片并保存为tech-icon.png\n```\n或者指定保存路径:\n```\n将第3张图片下载到/Users/username/Desktop/，文件名为ai-image.jpg\n```\n\n#### 生成图标\n\n可以提供详细的描述来生成符合需求的图标:\n```\n生成一个蓝色科技风格的图标，保存为blue-tech.png\n```\n或者更详细的描述:\n```\n请创建一个扁平化设计的邮件图标，红色轮廓，白色背景，图标尺寸为256x256，保存为email-icon.png\n```\n\n### 实际对话示例\n\n查看[示例对话](examples/dialog_example.md)了解如何在实际使用中与Claude/大模型交互来搜索和生成图标。\n\n### 集成到项目工作流\n\n1. 在项目初始阶段批量生成图标:\n   * 创建设计系统时，可以一次性生成多个相关图标\n   * 例如:\"帮我生成一套包含主页、设置、用户、消息通知的应用图标\"\n\n2. 开发过程中按需搜索:\n   * 在编写代码时随时查找所需图片资源\n   * 例如:\"我正在开发一个天气应用，需要几个天气相关的图标\"\n\n3. 项目完善阶段定制图标:\n   * 根据应用风格统一优化图标\n   * 例如:\"生成一组与我当前应用风格一致的社交媒体分享图标\"\n\n### 最佳实践\n\n1. **使用明确的关键词**: 搜索时使用具体、明确的关键词获得更精确的结果\n2. **指定图片源**: 根据需求选择合适的图片源（Unsplash适合自然风光，Pixabay适合商业图片等）\n3. **保存结构化命名**: 为图标使用结构化命名，如`category-name-size.png`\n4. **批量操作**: 一次性请求多个相关图标而不是逐个请求\n5. **与代码结合**: 在实际开发中提及代码上下文，大模型可以更准确地理解你的需求\n\n## 错误排查\n\n### Cursor MCP连接错误\n\n如果在Cursor中添加MCP服务时遇到\"Fail to create client\"错误，请尝试以下解决方法：\n\n1. **检查服务状态**：\n   ```bash\n   # 检查服务是否正在运行\n   lsof -i :5173\n   # 如果没有输出，表示服务未运行，请启动服务\n   python3.11 main.py\n   ```\n\n2. **测试连接**：\n   ```bash\n   # 使用curl测试API连接\n   curl -v http://localhost:5173\n   ```\n\n3. **修改连接设置**：\n   * 确保选择了正确的连接类型：SSE\n   * 尝试使用IP地址代替localhost：`http://127.0.0.1:5173`\n   * 确保URL不含额外斜杠：使用`http://localhost:5173`而非`http://localhost:5173/`\n   * 尝试使用ServerLink方式配置：\n     - 类型: sse\n     - ServerLink: http://localhost:5173/sse\n   * 有些版本的Cursor可能对URL格式有特定要求，两种方式都值得尝试\n\n4. **重启组件**：\n   * 停止并重启MCP服务\n   * 重启Cursor IDE\n   * 如果使用macOS，检查防火墙设置是否阻止了连接\n\n5. **检查日志**：\n   * 观察服务启动时的日志输出\n   * 当尝试从Cursor连接时，查看服务端有无新的日志输出\n\n6. **尝试其他端口**：\n   * 修改代码中的端口（如改为5174）并重启服务：\n   ```python\n   uvicorn.run(sse_app, host=\"0.0.0.0\", port=5174)\n   ```\n\n### 其他常见问题\n\n如果遇到问题，请检查：\n\n1. 服务是否正常运行\n2. 保存路径是否正确\n3. 目录权限是否正确\n4. 网络连接是否正常\n5. API 密钥是否有效\n6. Python 环境是否正确配置\n7. uv 是否正确安装\n8. 依赖包是否完整安装\n\n## 贡献\n\n欢迎提交问题和拉取请求来改进项目。\n\n## 许可\n\n[MIT License](LICENSE)",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "mcp_search_images",
        "icons",
        "images",
        "yanjunz mcp_search_images",
        "mcp_search_images search",
        "images sources"
      ],
      "category": "web-search"
    },
    "yap-audio--tiktok-mcp": {
      "owner": "yap-audio",
      "name": "tiktok-mcp",
      "url": "https://github.com/yap-audio/tiktok-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/yap-audio.webp",
      "description": "Discover and extract metadata from TikTok videos by searching for trending content using hashtags. This service includes built-in error handling, anti-detection measures, and configurable options for video retrieval and API session management.",
      "stars": 56,
      "forks": 6,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-10-02T22:17:13Z",
      "readme_content": "# TikTok MCP Service\n\nA Model Context Protocol service for TikTok video discovery and metadata extraction. This service provides a robust interface for searching TikTok videos by hashtags and retrieving trending content, with built-in anti-detection measures and error handling.\n\n## Features\n\n- Search videos by hashtags\n- Configurable video count per search (default: 30)\n- Anti-bot detection measures\n- Proxy support\n- Automatic API session management\n- Rate limiting and error handling\n- Health status monitoring\n\n## Configuration\n\nThe service uses environment variables for configuration. Create a `.env` file with:\n\n```env\nms_token=your_tiktok_ms_token  # Optional but recommended to avoid bot detection\nTIKTOK_PROXY=your_proxy_url    # Optional proxy configuration\n```\n\n## Installation and Setup\n\n```bash\n# Install dependencies\npoetry install\n\n# Install browser automation dependencies\npoetry run python -m playwright install\n\n# Start the service\npoetry run python -m tiktok_mcp_service.main\n```\n\n## Claude Desktop Integration\n\nOnce your service is running, you can integrate it with Claude Desktop. Since we're using Poetry for dependency management, make sure to run the MCP CLI commands through Poetry:\n\n```bash\n# Navigate to the project directory\ncd /path/to/tiktok-mcp-service\n\n# Install the service in Claude Desktop with Poetry in editable mode\npoetry run mcp install tiktok_mcp_service/main.py --with-editable . -f .env\n\n# Optional: Install with a custom name\npoetry run mcp install tiktok_mcp_service/main.py --name \"TikTok Video Search\" --with-editable . -f .env\n```\n\nAfter installation, the service will be available in Claude Desktop and will run using Poetry for proper dependency management.\n\n## API Endpoints\n\n### Health Check\n- `GET /health` - Check service health and API initialization status\n  ```json\n  {\n    \"status\": \"running\",\n    \"api_initialized\": true,\n    \"service\": {\n      \"name\": \"TikTok MCP Service\",\n      \"version\": \"0.1.0\",\n      \"description\": \"A Model Context Protocol service for searching TikTok videos\"\n    }\n  }\n  ```\n\n### Search Videos\n- `POST /search` - Search for videos with hashtags\n  ```json\n  {\n    \"search_terms\": [\"python\", \"coding\"],\n    \"count\": 30  // Optional, defaults to 30\n  }\n  ```\n  Response includes video URLs, descriptions, and engagement statistics (views, likes, shares, comments).\n\n### Resource Management\n- `POST /cleanup` - Clean up resources and API sessions\n\n## Error Handling\n\nThe service includes comprehensive error handling for:\n- API initialization failures\n- Bot detection issues\n- Network errors\n- Rate limiting\n- Invalid search terms\n\n## Development\n\nBuilt with:\n- TikTokApi\n- FastMCP\n- Poetry for dependency management\n- Playwright for browser automation\n\n## License\n\nMIT",
      "npm_url": "https://www.npmjs.com/package/tiktok-mcp",
      "npm_downloads": 134,
      "keywords": [
        "tiktok",
        "searching",
        "hashtags",
        "tiktok videos",
        "metadata tiktok",
        "audio tiktok"
      ],
      "category": "web-search"
    },
    "yiye--MCP-Finder-Server": {
      "owner": "yiye",
      "name": "MCP-Finder-Server",
      "url": "https://github.com/yiye/MCP-Finder-Server",
      "imageUrl": "/freedevtools/mcp/pfp/yiye.webp",
      "description": "Enables searching for MCP server implementations through the filtering of results by name, description, or tags. Facilitates streamlined exploration of available MCP servers by allowing users to limit search results.",
      "stars": 0,
      "forks": 1,
      "license": "No License",
      "language": "TypeScript",
      "updated_at": "2025-04-18T01:41:30Z",
      "readme_content": "# MCP Finder Server\n\n[![smithery badge](https://smithery.ai/badge/@yiye/mcp-finder-server)](https://smithery.ai/server/@yiye/mcp-finder-server)\n\nA Model Context Protocol (MCP) server that allows AI models to search for other MCP server implementations listed in the [awesome-mcp-servers](https://github.com/punkpeye/awesome-mcp-servers) repository.\n\n## Features\n\n- Search for MCP servers by name, description, or tags\n- Filter MCP servers by specific tags (e.g., programming language, OS compatibility)\n- Limit the number of results returned\n\n## Installation\n\n### Installing via Smithery\n\nTo install mcp-finder-server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@yiye/mcp-finder-server):\n\n```bash\nnpx -y @smithery/cli install @yiye/mcp-finder-server --client claude\n```\n\n### Manual Installation\n1. Clone this repository\n2. Install dependencies:\n```bash\nnpm install\n```\n\n## Usage\n\n### Build the server\n```bash\nnpm run build\n```\n\n### Run the server\n```bash\nnpm start\n```\n\n### Development mode\n```bash\nnpm run dev\n```\n\n## Tool: find-mcp-servers\n\nThe server provides a single tool called `find-mcp-servers` with the following parameters:\n\n- `search` (optional): Text to search for in server names, descriptions, or tags\n- `tag` (optional): Filter servers by tag (e.g., 'Databases', '🐍', '🏠')\n- `limit` (optional, default 10): Maximum number of results to return\n\n## Tag Meanings\n\n- 🐍 – Python codebase\n- 📇 – TypeScript codebase\n- 🏎️ – Go codebase\n- 🦀 – Rust codebase\n- #️⃣ - C# Codebase\n- ☕ - Java codebase\n- ☁️ - Cloud Service\n- 🏠 - Local Service\n- 📟 - Embedded Systems\n- 🍎 – For macOS\n- 🪟 – For Windows\n- 🐧 - For Linux\n\n## Example Response\n\n```\nFound 3 MCP servers:\n\nName: firebase/genkit\nURL: https://github.com/firebase/genkit\nTags: 📇, Frameworks\nDescription: – Provides integration between [Genkit](https://github.com/firebase/genkit/tree/main) and the Model Context Protocol (MCP).\n---\nName: lastmile-ai/mcp-agent\nURL: https://github.com/lastmile-ai/mcp-agent\nTags: 🤖, 🔌, Frameworks\nDescription: - Build effective agents with MCP servers using simple, composable patterns.\n---\nName: LiteMCP\nURL: https://github.com/wong2/litemcp\nTags: 📇, Frameworks\nDescription: - A high-level framework for building MCP servers in JavaScript/TypeScript\n---\n```\n\n## License\n\nISC \n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "searching",
        "search",
        "mcp",
        "searching mcp",
        "mcp finder",
        "mcp servers"
      ],
      "category": "web-search"
    },
    "yokingma--one-search-mcp": {
      "owner": "yokingma",
      "name": "one-search-mcp",
      "url": "https://github.com/yokingma/one-search-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/yokingma.webp",
      "description": "Enables web search, scraping, and content extraction from various websites using multiple search engines and scrapers. It supports local browser searches and integrates with tools like SearXNG, DuckDuckGo, and Bing for enhanced data retrieval.",
      "stars": 58,
      "forks": 12,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-10-01T15:37:12Z",
      "readme_content": "# 🚀 OneSearch MCP Server: Web Search & Crawl & Scraper & Extract\n\nA Model Context Protocol (MCP) server implementation that integrates with Searxng/Tavily/DuckDuckGo/Bing for web search, local browser search, and scraping capabilities with Firecrawl.\n\n## Features\n\n- Web Search, scrape, crawl and extract content from websites.\n- Support multiple search engines and web scrapers: **SearXNG**, **Firecrawl**, **Tavily**, **DuckDuckGo**, **Bing**, etc.\n- **Local web search** (browser search), support multiple search engines: **Bing**, **Google**, **Baidu**, **Sogou**, etc.\n  - Use `puppeteer-core` to scrape content from websites.\n  - You should have a local browser installed, such as `Chromium`, `Google Chrome`, `Google Chrome Canary`, etc.\n  - Free, no keys required.\n- **Enabled tools:** `one_search`, `one_scrape`, `one_map`\n- Support for self-hosted: SearXNG, Firecrawl, etc. (see [Deploy](./deploy/README.md))\n\n## Installation\n\n### Installing via Smithery\n\nTo install OneSearch for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@yokingma/one-search):\n\n```bash\nnpx -y @smithery/cli install @yokingma/one-search --client claude\n```\n\n### Manual Installation\n\n```shell\n# Manually install (Optional)\nnpm install -g one-search-mcp\n```\n\n```shell\n# using npx\nenv SEARCH_API_URL=http://127.0.0.1:8080 FIRECRAWL_API_URL=http://127.0.0.1:3002 npx -y one-search-mcp\n```\n\n## Environment Variables\n\n**Search Engine:**\n\n- **SEARCH_PROVIDER** (Optional): The search provider to use, supports `searxng`, `duckduckgo`, `bing`, `tavily`, `local`, default is `local`.\n- **SEARCH_API_URL** (Optional): The URL of the SearxNG API, required for `searxng`.\n- **SEARCH_API_KEY** (Optional): The API key for the search provider, required for `tavily`, `bing`.\n\n```ts\n// supported search providers\nexport type SearchProvider = 'searxng' | 'duckduckgo' | 'bing' | 'tavily' | 'local';\n```\n\n**Firecrawl:**\n\n- FIRECRAWL_API_URL (Optional): The URL of the Firecrawl API, required for `firecrawl`.\n- FIRECRAWL_API_KEY (Optional): The API key for the Firecrawl API, required for `firecrawl` if using cloud service.\n\n## Running on Cursor\n\nYour `mcp.json` file will look like this:\n\n```json\n{\n  \"mcpServers\": {\n    \"one-search-mcp\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"one-search-mcp\"],\n      \"env\": {\n        \"SEARCH_PROVIDER\": \"searxng\",\n        \"SEARCH_API_URL\": \"http://127.0.0.1:8080\",\n        \"SEARCH_API_KEY\": \"YOUR_API_KEY\",\n        \"FIRECRAWL_API_URL\": \"http://127.0.0.1:3002\",\n        \"FIRECRAWL_API_KEY\": \"YOUR_API_KEY\"\n      }\n    }\n  }\n}\n```\n\n## Running on Windsurf\n\nAdd this to your `./codeium/windsurf/model_config.json` file:\n\n```json\n{\n  \"mcpServers\": {\n    \"one-search-mcp\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"one-search-mcp\"],\n      \"env\": {\n        \"SEARCH_PROVIDER\": \"searxng\",\n        \"SEARCH_API_URL\": \"http://127.0.0.1:8080\",\n        \"SEARCH_API_KEY\": \"YOUR_API_KEY\",\n        \"FIRECRAWL_API_URL\": \"http://127.0.0.1:3002\",\n        \"FIRECRAWL_API_KEY\": \"YOUR_API_KEY\"\n      }\n    }\n  }\n}\n```\n\n## Self-host\n\nLocal deployment of SearXNG and Firecrawl, please refer to [Deploy](./deploy/README.md)\n\n## Troubleshooting\n\n- [ReferenceError]: __name is not defined: This is because Puppeteer has problems with `tsx`, [esbuild#1031](https://github.com/evanw/esbuild/issues/1031)\n\n## License\n\nMIT License - see [LICENSE](./LICENSE) file for details.\n",
      "npm_url": "https://www.npmjs.com/package/one-search-mcp",
      "npm_downloads": 8857,
      "keywords": [
        "searches",
        "scraping",
        "search",
        "search scraping",
        "web search",
        "search engines"
      ],
      "category": "web-search"
    },
    "yuna0x0--anilist-mcp": {
      "owner": "yuna0x0",
      "name": "anilist-mcp",
      "url": "https://github.com/yuna0x0/anilist-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/yuna0x0.webp",
      "description": "Access and interact with detailed anime and manga data, including information about shows, characters, and user profiles. Supports searching and filtering of various media types from AniList.",
      "stars": 55,
      "forks": 8,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-10-02T17:09:05Z",
      "readme_content": "# AniList MCP Server\n\nA Model Context Protocol (MCP) server that interfaces with the AniList API, allowing LLM clients to access and interact with anime, manga, character, staff, and user data from AniList.\n\n## Features\n\n- Search for anime, manga, characters, staff, and studios\n- Get detailed information about specific anime, manga, characters, and staff members\n- Access user profiles and lists\n- Support for advanced filtering options\n- Retrieve genres and media tags\n- **Dual transport support**: Both HTTP and STDIO transports\n- **Cloud deployment ready**: Support Smithery and other platforms\n\n## Requirements\n\n- Node.js 18+\n\n## Local Installation (STDIO Transport)\n\n1. Add this server to your `mcp.json` / `claude_desktop_config.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"anilist\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"anilist-mcp\"],\n      \"env\": {\n        \"ANILIST_TOKEN\": \"your_api_token\"\n      }\n    }\n  }\n}\n```\n\nYou may remove the `env` object entirely, if you are not planning to use the AniList Token for operations that require login.\n\n2. Restart your MCP client (e.g., Claude Desktop)\n3. Use the tools to interact with AniList\n\n## Server Deployment (HTTP Transport)\n\n### Self-Hosting\nFollow the [Local Development](#local-development) instructions to set up the project locally, then run:\n```bash\npnpm run start:http\n```\nThis will start the server on port 8081 by default. You can change the port by setting the `PORT` environment variable.\n\n### Cloud Deployment\n\nYou can deploy this MCP server to any cloud platform that supports Node.js server applications.\n\nYou can also deploy via MCP platforms like [Smithery](https://smithery.ai/server/@yuna0x0/anilist-mcp).\n\n## Configuration\n### Environment Variables (STDIO Transport and HTTP Transport server where host provides the config)\n\nWhen using the STDIO transport or hosting the HTTP transport server, you can pass configuration via environment variables:\n- `ANILIST_TOKEN`: (Optional) AniList API Token (Only needed for operations that require login)\n\n> [!CAUTION]\n> If you are hosting the HTTP transport server with token pre-configured, you should protect your endpoint and implement authentication before allowing users to access it. Otherwise, anyone can access your MCP server while using your AniList token.\n\n### HTTP Headers (HTTP Transport where user provides the config)\n\nWhen using the HTTP transport, user can pass configuration via HTTP headers:\n- `Anilist-Token`: (Optional) AniList API Token (Only needed for operations that require login)\n\nIf the user provides the token in the header, while the server also has `ANILIST_TOKEN` set, the header value will take precedence.\n\n### Get an AniList API Token (Optional)\n\nTo get an API token, follow these steps:\n\n1. Go to [AniList settings](https://anilist.co/settings/developer).\n2. Click on \"Create New Client\".\n3. Use this URL as your client's \"Redirect URL\":\n```\nhttps://anilist.co/api/v2/oauth/pin\n```\n\n4. Click \"Save\"\n5. Then go to https://anilist.co/api/v2/oauth/authorize?client_id={clientID}&response_type=token, replace the `{clientID}` with the client ID you get. It will ask you to log in and then provide you with the token to use.\n6. Copy the generated token and use it in your `.env` file or environment variables.\n\n## Available Tools\n\n### Misc Tools\n- **get_genres**: Get all available genres on AniList\n- **get_media_tags**: Get all available media tags on AniList\n- **get_site_statistics**: Get AniList site statistics over the last seven days\n- **get_studio**: Get information about a studio by its AniList ID or name\n- **favourite_studio**: [Requires Login] Favourite or unfavourite a studio by its ID\n\n### Activity Tools\n- **delete_activity**: [Requires Login] Delete the current authorized user's activity post\n- **get_activity**: Get a specific AniList activity by its ID\n- **get_user_activity**: Fetch activities from a user\n- **post_message_activity**: [Requires Login] Post a new message activity or update an existing one\n- **post_text_activity**: [Requires Login] Post a new text activity or update an existing one\n\n### List Tools\n- **get_user_anime_list**: Get a user's anime list\n- **get_user_manga_list**: Get a user's manga list\n- **add_list_entry**: [Requires Login] Add an entry to the authorized user's list\n- **remove_list_entry**: [Requires Login] Remove an entry from the authorized user's list\n- **update_list_entry**: [Requires Login] Update an entry on the authorized user's list\n\n### Media Tools\n- **get_anime**: Get detailed information about an anime by its AniList ID\n- **get_manga**: Get detailed information about a manga by its AniList ID\n- **favourite_anime**: [Requires Login] Favourite or unfavourite an anime by its ID\n- **favourite_manga**: [Requires Login] Favourite or unfavourite a manga by its ID\n\n### People Tools\n- **get_character**: Get information about a character by their AniList ID\n- **get_staff**: Get information about staff member by their AniList ID\n- **favourite_character**: [Requires Login] Favourite or unfavourite a character by its ID\n- **favourite_staff**: [Requires Login] Favourite or unfavourite a staff member by their ID\n- **get_todays_birthday_characters**: Get all characters whose birthday is today\n- **get_todays_birthday_staff**: Get all staff members whose birthday is today\n\n### Recommendation Tools\n- **get_recommendation**: Get an AniList recommendation by its ID\n- **get_recommendations_for_media**: Get AniList recommendations for a specific media\n\n### Search Tools\n- **search_activity**: Search for activities on AniList\n- **search_anime**: Search for anime with query term and filters\n- **search_manga**: Search for manga with query term and filters\n- **search_character**: Search for characters based on a query term\n- **search_staff**: Search for staff members based on a query term\n- **search_studio**: Search for studios based on a query term\n- **search_user**: Search for users on AniList\n\n### Thread Tools\n- **get_thread**: Get a specific thread by its AniList ID\n- **get_thread_comments**: Get comments for a specific thread\n- **delete_thread**: [Requires Login] Delete a thread by its ID\n\n### User Tools\n- **get_user_profile**: Get a user's AniList profile\n- **get_user_stats**: Get a user's AniList statistics\n- **get_full_user_info**: Get a user's complete profile and stats information\n- **get_user_recent_activity**: Get recent activity from a user\n- **get_authorized_user**: [Requires Login] Get profile information of the currently authorized user\n- **follow_user**: [Requires Login] Follow or unfollow a user by their ID\n- **update_user**: [Requires Login] Update user settings\n\n## Example Usage\n\n### Basic Anime Search\n\n```\nCan you search for anime similar to \"Bocchi the Rock!\"?\n```\n\n### Get Character Info\n\n```\nCan you tell me about the character Hitori Gotou? Use the AniList tools to find information.\n```\n\n### Compare Studio Works\n\n```\nWhat anime has Studio Ghibli produced? Can you list their most popular works?\n```\n\n## Local Development\n\nThis project uses [pnpm](https://pnpm.io) as its package manager.\n\nClone the repository and install dependencies:\n\n```bash\ngit clone https://github.com/yuna0x0/anilist-mcp.git\ncd anilist-mcp\npnpm install\n```\n\n### Configuration (Optional)\n\n1. Create a `.env` file by copying the example:\n```bash\ncp env.example .env\n```\n\n2. Edit the `.env` file and add your AniList API token:\n```\nANILIST_TOKEN=your_api_token\n```\n\n## Debugging with MCP Inspector\n\nYou can use the MCP Inspector to test and debug the AniList MCP server:\n\n```bash\nnpx @modelcontextprotocol/inspector -e ANILIST_TOKEN=your_api_token npx anilist-mcp\n\n# Use this instead when Local Development\npnpm run inspector\n```\n\nThen open your browser to the provided URL (usually http://localhost:6274) to access the MCP Inspector interface. From there, you can:\n\n1. Connect to your running AniList MCP server\n2. Browse available tools\n3. Run tools with custom parameters\n4. View the responses\n\nThis is particularly useful for testing your setup before connecting it to MCP clients like Claude Desktop.\n\n## Docker\n\nPull from GitHub Container Registry:\n```bash\ndocker pull ghcr.io/yuna0x0/anilist-mcp\n```\n\nDocker build (Local Development):\n```bash\ndocker build -t ghcr.io/yuna0x0/anilist-mcp .\n```\n\nDocker multi-platform build (Local Development):\n```bash\ndocker buildx build --platform linux/amd64,linux/arm64 -t ghcr.io/yuna0x0/anilist-mcp .\n```\n\n## MCP Bundles (MCPB)\n\nTo create an MCP Bundle for this server, run:\n```bash\npnpm run pack:mcpb\n```\n\n## Security Notice\n\nThis MCP server accepts your AniList API token in the .env file, environment variable or HTTP header. Keep this information secure and never commit it to version control.\n\n## License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n",
      "npm_url": "https://www.npmjs.com/package/anilist-mcp",
      "npm_downloads": 28778,
      "keywords": [
        "manga",
        "search",
        "searching",
        "manga data",
        "search yuna0x0",
        "yuna0x0 anilist"
      ],
      "category": "web-search"
    },
    "yutakobayashidev--webforai-mcp-server": {
      "owner": "yutakobayashidev",
      "name": "webforai-mcp-server",
      "url": "https://github.com/yutakobayashidev/webforai-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/yutakobayashidev.webp",
      "description": "Extract plain text from any web page URL and convert it to clean, well-formatted Markdown output. Provides robust error handling and is optimized for AI consumption through seamless integration with MCP clients.",
      "stars": 2,
      "forks": 1,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-05-03T08:09:50Z",
      "readme_content": "# WebforAI Text Extractor - MCP Server\n\nA Cloudflare Workers-based Model Context Protocol (MCP) server that extracts plain text from web pages using [WebforAI](https://webforai.dev/).\n\n## 🌟 What is WebforAI?\n\n[WebforAI](https://webforai.dev/) is a powerful library designed to make web content accessible to AI models. It provides tools to:\n\n- Convert HTML to clean, structured Markdown\n- Extract meaningful content from web pages\n- Process tables, links, and images intelligently\n- Prepare web content for AI consumption\n\nThis MCP server leverages WebforAI's capabilities to extract plain text from any web page URL, making it easy to feed web content into AI models through the Model Context Protocol.\n\n## 📋 Features\n\n- **Simple API**: Extract text from any web page with a single API call\n- **Clean Output**: Receive well-formatted Markdown text without HTML noise\n- **Error Handling**: Robust error handling for failed requests\n- **Cloudflare Workers**: Serverless deployment with global distribution\n- **MCP Compatible**: Works with any MCP client like Claude Desktop or Cloudflare AI Playground\n\n## 🚀 Getting Started\n\n### Deploy to Cloudflare Workers\n\n[![Deploy to Workers](https://deploy.workers.cloudflare.com/button)](https://deploy.workers.cloudflare.com/?url=https://github.com/yutakobayashidev/webforai-mcp-server)\n\nThis will deploy your MCP server to a URL like: `webforai-mcp-server.<your-account>.workers.dev/sse`\n\n### Local Development\n\n1. Clone this repository:\n   ```bash\n   git clone https://github.com/yutakobayashidev/webforai-mcp-server.git\n   cd webforai-mcp-server\n   ```\n\n2. Install dependencies:\n   ```bash\n   pnpm install\n   ```\n\n3. Start the development server:\n   ```bash\n   pnpm dev\n   ```\n\n4. Your server will be available at `http://localhost:8787`\n\n## 🔧 Using the Text Extraction Tool\n\nThe `extractWebPageText` tool accepts a URL to a web page and returns the extracted text content in markdown format:\n\n```json\n{\n  \"url\": \"https://example.com/page\"\n}\n```\n\nThe response will contain the extracted text in Markdown format, with:\n- Links converted to plain text\n- Tables converted to plain text\n- Images hidden\n\n## 🔌 Connecting to MCP Clients\n\n### Cloudflare AI Playground\n\n1. Go to [Cloudflare AI Playground](https://playground.ai.cloudflare.com/)\n2. Enter your deployed MCP server URL (`webforai-mcp-server.<your-account>.workers.dev/sse`)\n3. You can now use your text extraction tool directly from the playground!\n\n### Claude Desktop\n\nTo connect to your MCP server from Claude Desktop:\n\n1. Follow [Anthropic's Quickstart](https://modelcontextprotocol.io/quickstart/user)\n2. In Claude Desktop go to Settings > Developer > Edit Config\n3. Update with this configuration:\n\n```json\n{\n  \"mcpServers\": {\n    \"webforaiExtractor\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"mcp-remote\",\n        \"http://localhost:8787/sse\"  // or webforai-mcp-server.your-account.workers.dev/sse\n      ]\n    }\n  }\n}\n```\n\n4. Restart Claude and you should see the text extraction tool become available\n\n## 📚 Learn More\n\n- [WebforAI Documentation](https://webforai.dev/)\n- [Model Context Protocol](https://modelcontextprotocol.io/)\n- [Cloudflare Workers](https://developers.cloudflare.com/workers/)\n- [Cloudflare AI](https://developers.cloudflare.com/ai/)\n\n## 📄 License\n\nMIT       \n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "webforai",
        "yutakobayashidev",
        "markdown",
        "webforai mcp",
        "yutakobayashidev webforai",
        "search yutakobayashidev"
      ],
      "category": "web-search"
    },
    "ywwAHU--mcp-scholarly": {
      "owner": "ywwAHU",
      "name": "mcp-scholarly",
      "url": "https://github.com/ywwAHU/mcp-scholarly",
      "imageUrl": "/freedevtools/mcp/pfp/ywwAHU.webp",
      "description": "Search for academic articles using the MCP server by querying a dedicated tool for relevant papers based on keywords. Access trusted academic sources for streamlined research.",
      "stars": 0,
      "forks": 1,
      "license": "MIT License",
      "language": "",
      "updated_at": "2025-04-15T07:27:21Z",
      "readme_content": "# mcp-scholarly MCP server\n[![smithery badge](https://smithery.ai/badge/mcp-scholarly)](https://smithery.ai/server/mcp-scholarly)\n\nA MCP server to search for accurate academic articles. More scholarly vendors will be added soon.\n\n\n\n![image](https://github.com/user-attachments/assets/13202184-bc12-4530-b7c1-2ee698f3e1cc)\n\n<a href=\"https://glama.ai/mcp/servers/aq05b2p0ql\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/aq05b2p0ql/badge\" alt=\"Scholarly Server MCP server\" /></a>\n\n![star-history-2025323](https://github.com/user-attachments/assets/b73c916d-194c-429e-a7f9-0ff7d0db0b02)\n\n\n## Components\n\n### Tools\n\nThe server implements one tool:\n- search-arxiv: Search arxiv for articles related to the given keyword.\n  - Takes \"keyword\" as required string arguments\n\n## Quickstart\n\n### Install\n\n#### Claude Desktop\n\nOn MacOS: `~/Library/Application\\ Support/Claude/claude_desktop_config.json`\nOn Windows: `%APPDATA%/Claude/claude_desktop_config.json`\n\n<details>\n  <summary>Development/Unpublished Servers Configuration</summary>\n  ```\n  \"mcpServers\": {\n    \"mcp-scholarly\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"--directory\",\n        \"/Users/adityakarnam/PycharmProjects/mcp-scholarly/mcp-scholarly\",\n        \"run\",\n        \"mcp-scholarly\"\n      ]\n    }\n  }\n  ```\n</details>\n\n<details>\n  <summary>Published Servers Configuration</summary>\n  ```\n  \"mcpServers\": {\n    \"mcp-scholarly\": {\n      \"command\": \"uvx\",\n      \"args\": [\n        \"mcp-scholarly\"\n      ]\n    }\n  }\n  ```\n</details>\n\nor if you are using Docker\n\n<details>\n  <summary>Published Docker Servers Configuration</summary>\n  ```\n  \"mcpServers\": {\n    \"mcp-scholarly\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\", \"--rm\", \"-i\",\n        \"mcp/scholarly\"\n      ]\n    }\n  }\n  ```\n</details>\n\n### Installing via Smithery\n\nTo install mcp-scholarly for Claude Desktop automatically via [Smithery](https://smithery.ai/server/mcp-scholarly):\n\n```bash\nnpx -y @smithery/cli install mcp-scholarly --client claude\n```\n\n## Development\n\n### Building and Publishing\n\nTo prepare the package for distribution:\n\n1. Sync dependencies and update lockfile:\n```bash\nuv sync\n```\n\n2. Build package distributions:\n```bash\nuv build\n```\n\nThis will create source and wheel distributions in the `dist/` directory.\n\n3. Publish to PyPI:\n```bash\nuv publish\n```\n\nNote: You'll need to set PyPI credentials via environment variables or command flags:\n- Token: `--token` or `UV_PUBLISH_TOKEN`\n- Or username/password: `--username`/`UV_PUBLISH_USERNAME` and `--password`/`UV_PUBLISH_PASSWORD`\n\n### Debugging\n\nSince MCP servers run over stdio, debugging can be challenging. For the best debugging\nexperience, we strongly recommend using the [MCP Inspector](https://github.com/modelcontextprotocol/inspector).\n\n\nYou can launch the MCP Inspector via [`npm`](https://docs.npmjs.com/downloading-and-installing-node-js-and-npm) with this command:\n\n```bash\nnpx @modelcontextprotocol/inspector uv --directory /Users/adityakarnam/PycharmProjects/mcp-scholarly/mcp-scholarly run mcp-scholarly\n```\n\n\nUpon launching, the Inspector will display a URL that you can access in your browser to begin debugging.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "scholarly",
        "search",
        "research",
        "scholarly search",
        "mcp scholarly",
        "search academic"
      ],
      "category": "web-search"
    },
    "zbkm--mmnt-mcp-server": {
      "owner": "zbkm",
      "name": "mmnt-mcp-server",
      "url": "https://github.com/zbkm/mmnt-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/zbkm.webp",
      "description": "Utilizes the Mamont search engine for executing search queries and retrieving cached web pages. Provides a standardized interface for accessing real-time search results and text-based content efficiently.",
      "stars": 0,
      "forks": 1,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-04-03T15:37:47Z",
      "readme_content": "# mmnt-mcp-server\nMCP server for the [Mamont](https://www.mmnt.ru/) search engine.\n\n### Tools:\n- mmnt_search\n    * search query on search engine\n    * inputs\n        * query - query string\n        * page - page number\n- mmnt_cache\n    * retrieve page from search page cache\n    * inputs\n        * id - unique cache id\n        * onlyText - should the result be text only (no html)\n\n### Install\n```json\n{\n  \"mcpServers\": {\n    \"mmnt\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"mmnt-mcp-server\"]\n    }\n  }\n}\n```\n",
      "npm_url": "https://www.npmjs.com/package/mmnt-mcp-server",
      "npm_downloads": 351,
      "keywords": [
        "search",
        "mamont",
        "mmnt",
        "mamont search",
        "web search",
        "search engine"
      ],
      "category": "web-search"
    },
    "zcaceres--fetch-mcp": {
      "owner": "zcaceres",
      "name": "fetch-mcp",
      "url": "https://github.com/zcaceres/fetch-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/zcaceres.webp",
      "description": "Fetches web content in various formats such as HTML, JSON, plain text, and Markdown from specified URLs. It can return raw HTML content and JSON data from various online sources.",
      "stars": 611,
      "forks": 100,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-10-01T17:28:25Z",
      "readme_content": "# Fetch MCP Server\n\n\n\nThis MCP server provides functionality to fetch web content in various formats, including HTML, JSON, plain text, and Markdown.\n\n[Available on NPM](https://www.npmjs.com/package/mcp-fetch-server)\n\n<a href=\"https://glama.ai/mcp/servers/nu09wf23ao\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/nu09wf23ao/badge\" alt=\"Fetch Server MCP server\" />\n</a>\n\n## Components\n\n### Tools\n\n- **fetch_html**\n  - Fetch a website and return the content as HTML\n  - Input:\n    - `url` (string, required): URL of the website to fetch\n    - `headers` (object, optional): Custom headers to include in the request\n    - `max_length` (number, optional): Maximum length to fetch (default 5000, can change via environment variable)\n    - `start_index` (number, optional): Used together with max_length to retrieve contents piece by piece, 0 by default\n  - Returns the raw HTML content of the webpage\n\n- **fetch_json**\n  - Fetch a JSON file from a URL\n  - Input:\n    - `url` (string, required): URL of the JSON to fetch\n    - `headers` (object, optional): Custom headers to include in the request\n    - `max_length` (number, optional): Maximum length to fetch (default 5000, can change via environment variable)\n    - `start_index` (number, optional): Used together with max_length to retrieve contents piece by piece, 0 by default\n  - Returns the parsed JSON content\n\n- **fetch_txt**\n  - Fetch a website and return the content as plain text (no HTML)\n  - Input:\n    - `url` (string, required): URL of the website to fetch\n    - `headers` (object, optional): Custom headers to include in the request\n    - `max_length` (number, optional): Maximum length to fetch (default 5000, can change via environment variable)\n    - `start_index` (number, optional): Used together with max_length to retrieve contents piece by piece, 0 by default\n  - Returns the text content of the webpage with HTML tags, scripts, and styles removed\n\n- **fetch_markdown**\n  - Fetch a website and return the content as Markdown\n  - Input:\n    - `url` (string, required): URL of the website to fetch\n    - `headers` (object, optional): Custom headers to include in the request\n    - `max_length` (number, optional): Maximum length to fetch (default 5000, can change via environment variable)\n    - `start_index` (number, optional): Used together with max_length to retrieve contents piece by piece, 0 by default\n  - Returns the content of the webpage converted to Markdown format\n\n### Resources\n\nThis server does not provide any persistent resources. It's designed to fetch and transform web content on demand.\n\n## Getting started\n\n1. Clone the repository\n2. Install dependencies: `npm install`\n3. Build the server: `npm run build`\n\n### Usage\n\nTo use the server, you can run it directly:\n\n```bash\nnpm start\n```\n\nThis will start the Fetch MCP Server running on stdio.\n\n### Environment variables\n\n- **DEFAULT_LIMIT** - sets the default size limit for the fetch (0 = no limit)\n\n### Usage with Desktop App\n\nTo integrate this server with a desktop app, add the following to your app's server configuration:\n\n```json\n{\n  \"mcpServers\": {\n    \"fetch\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"mcp-fetch-server\"\n      ], \n      \"env\": {\n        \"DEFAULT_LIMIT\": \"50000\" // optionally change default limit\n      }\n    }\n  }\n}\n```\n\n## Features\n\n- Fetches web content using modern fetch API\n- Supports custom headers for requests\n- Provides content in multiple formats: HTML, JSON, plain text, and Markdown\n- Uses JSDOM for HTML parsing and text extraction\n- Uses TurndownService for HTML to Markdown conversion\n\n## Development\n\n- Run `npm run dev` to start the TypeScript compiler in watch mode\n- Use `npm test` to run the test suite\n\n## License\n\nThis project is licensed under the MIT License.",
      "npm_url": "https://www.npmjs.com/package/fetch-mcp",
      "npm_downloads": 36810,
      "keywords": [
        "zcaceres",
        "fetches",
        "fetch",
        "zcaceres fetch",
        "search zcaceres",
        "fetches web"
      ],
      "category": "web-search"
    },
    "zhangzhongnan928--mcp-pa-ai-agent": {
      "owner": "zhangzhongnan928",
      "name": "mcp-pa-ai-agent",
      "url": "https://github.com/zhangzhongnan928/mcp-pa-ai-agent",
      "imageUrl": "/freedevtools/mcp/pfp/zhangzhongnan928.webp",
      "description": "Manage calendar events, track tasks and to-dos, read and send emails, search the web for information, and control smart home devices.",
      "stars": 19,
      "forks": 5,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-09-29T06:11:15Z",
      "readme_content": "# MCP Personal Assistant Agent\n\nA versatile personal assistant AI agent built with the Model Context Protocol (MCP) that helps with calendar, tasks, emails, and more.\n\n## Overview\n\nThis project is a Model Context Protocol (MCP) server that provides a set of tools for a personal assistant agent. It can be integrated with MCP clients like Claude for Desktop to give AI assistants the ability to:\n\n- Manage calendar events\n- Track tasks and to-dos\n- Read and send emails\n- Search the web and retrieve information\n- Control smart home devices\n\n## Requirements\n\n⚠️ **IMPORTANT:** Python 3.10 or higher is required for the MCP SDK. The server will not work with earlier Python versions.\n\n- Python 3.10+ \n- MCP SDK 1.2.0+\n- Required Python packages (see requirements.txt)\n\n## Installation\n\n1. Clone the repository:\n```bash\ngit clone https://github.com/yourusername/mcp-pa-ai-agent.git\ncd mcp-pa-ai-agent\n```\n\n2. Ensure you have Python 3.10+:\n```bash\npython --version\n```\n\n3. If your system Python is older than 3.10, set up a compatible environment:\n```bash\n# Using conda\nconda create -n mcp-env python=3.10\nconda activate mcp-env\n\n# OR using venv (if Python 3.10+ is installed elsewhere)\npython3.10 -m venv venv\nsource venv/bin/activate  # On Windows: venv\\Scripts\\activate\n```\n\n4. Install dependencies:\n```bash\npip install -r requirements.txt\n```\n\n5. Configure environment variables by copying the example file:\n```bash\ncp .env.example .env\n```\n\n6. Edit the `.env` file with your API credentials and settings.\n\n## Running the Server\n\nStart the MCP server with:\n\n```bash\npython mcp_server.py\n```\n\nThe server will start and listen for MCP client connections.\n\n## Connecting to Claude for Desktop\n\n1. Install [Claude for Desktop](https://claude.ai/desktop)\n\n2. Configure Claude for Desktop to use this MCP server by editing the configuration file at:\n   - MacOS/Linux: `~/Library/Application Support/Claude/claude_desktop_config.json`\n   - Windows: `%APPDATA%\\Claude\\claude_desktop_config.json`\n\n3. Add the following configuration:\n```json\n{\n  \"mcpServers\": {\n    \"personal-assistant\": {\n      \"command\": \"/path/to/python\",\n      \"args\": [\n        \"/absolute/path/to/mcp-pa-ai-agent/mcp_server.py\"\n      ]\n    }\n  }\n}\n```\n\nIf you're using a virtual environment, make sure to point to the Python executable in that environment.\n\n4. Restart Claude for Desktop\n\n## Available Tools\n\n### Calendar\n- `get_events`: Retrieve upcoming calendar events\n- `create_event`: Schedule a new calendar event\n\n### Tasks\n- `list_tasks`: View all tasks or filter by status\n- `add_task`: Create a new task\n- `update_task_status`: Mark tasks as pending, in-progress, or completed\n\n### Email\n- `get_emails`: List recent emails from your inbox\n- `read_email`: View the full content of a specific email\n- `send_email`: Compose and send a new email\n\n### Knowledge\n- `web_search`: Search the web for information\n- `get_weather`: Get current weather information\n- `get_news`: Retrieve latest news articles\n\n### Smart Home\n- `list_devices`: View all smart home devices\n- `control_device`: Control smart home devices (lights, thermostats, etc.)\n- `get_device_state`: Get detailed information about a device's current state\n\n## Configuration\n\nThe server requires various API keys and credentials to access different services:\n\n- **Google API**: For calendar and email functionality (OAuth2 credentials)\n- **Weather API**: For weather information\n- **News API**: For news retrieval\n- **Home Assistant**: For smart home control\n\nRefer to the `.env.example` file for all configurable options.\n\n## Troubleshooting\n\n### Python Version Issues\n\nIf you see an error like:\n```\nError: Python 3.10 or higher is required for the MCP server.\n```\n\nYou need to upgrade your Python version or use a virtual environment with Python 3.10+.\n\n### MCP SDK Installation Issues\n\nIf you encounter problems installing the MCP SDK:\n```\nERROR: Could not find a version that satisfies the requirement mcp>=1.2.0\n```\n\nMake sure you're using Python 3.10+ and pip is updated:\n```bash\npip install --upgrade pip\n```\n\n## Development\n\nTo add new functionality to the server, you can:\n\n1. Create a new module in the `modules/` directory\n2. Implement functions with the `@mcp.tool()` decorator\n3. Import your module in `mcp_server.py`\n\n## License\n\nMIT\n\n## Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "mcp",
        "agent",
        "ai",
        "agent manage",
        "mcp pa",
        "ai agent"
      ],
      "category": "web-search"
    },
    "zhsama--duckduckgo-mcp-server": {
      "owner": "zhsama",
      "name": "duckduckgo-mcp-server",
      "url": "https://github.com/zhsama/duckduckgo-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/zhsama.webp",
      "description": "Provides DuckDuckGo search functionality with a simple interface for performing web searches. Supports rate limiting and error handling while integrating with the DuckDuckGo API.",
      "stars": 63,
      "forks": 7,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-10-02T01:13:04Z",
      "readme_content": "# duckduckgo-search MCP Server\n\nEnglish | [中文](README_zh.md)\n\nA Model Context Protocol server for DuckDuckGo Search\n\nThis is a TypeScript-based MCP server that provides DuckDuckGo search functionality. It demonstrates core MCP concepts through:\n\n- Integration with DuckDuckGo Search\n- Easy-to-use search tool interface\n- Rate limiting and error handling support\n\n<a href=\"https://glama.ai/mcp/servers/34fhy9xb9w\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/34fhy9xb9w/badge\" alt=\"DuckDuckGo Server MCP server\" />\n</a>\n\n## Features\n\n### Search Tool\n\n- `duckduckgo_search` - Perform web searches using DuckDuckGo API\n  - Required parameter: `query` (search query, max 400 characters)\n  - Optional parameter: `count` (number of results, 1-20, default 10)\n  - Optional parameter: `safeSearch` (safety level: strict/moderate/off, default moderate)\n  - Returns formatted Markdown search results\n\n### Rate Limits\n\n- Maximum 1 request per second\n- Maximum 15000 requests per month\n\n## Development\n\n### Prerequisites\n\n- Node.js >= 18\n- pnpm >= 8.0.0\n\n### Installation\n\n```bash\n# Install pnpm if not already installed\nnpm install -g pnpm\n\n# Install project dependencies\npnpm install\n```\n\n### Build and Run\n\nBuild the server:\n\n```bash\npnpm run build\n```\n\nFor development with auto-rebuild:\n\n```bash\npnpm run watch\n```\n\n## Setup in Claude Desktop\n\nTo use with Claude Desktop, add the server config:\n\nOn MacOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\nOn Windows: `%APPDATA%/Claude/claude_desktop_config.json`\n\n```json\n# online\n{\n  \"mcpServers\": {\n    \"duckduckgo-search\": {\n        \"command\": \"npx\",\n        \"args\": [\n          \"-y\",\n          \"duckduckgo-mcp-server\"\n        ]\n    }\n  }\n}\n\n# local\n{\n  \"mcpServers\": {\n    \"duckduckgo-search\": {\n      \"command\": \"node\",\n      \"args\": [\n        \"/path/to/duckduckgo-search/build/index.js\"\n      ]\n    }\n  }\n}\n```\n![image](https://github.com/user-attachments/assets/6906e280-9dbb-4bb5-a537-d9e45e666084)\n![image](https://github.com/user-attachments/assets/867a70ae-082f-45ab-a623-869bfd6c31eb)\n\n### Debugging\n\nSince MCP servers communicate over stdio, debugging can be challenging. We recommend using the [MCP Inspector](https://github.com/modelcontextprotocol/inspector), which is available as a package script:\n\n```bash\npnpm run inspector\n```\n\nThe Inspector will provide a URL to access debugging tools in your browser.",
      "npm_url": "https://www.npmjs.com/package/duckduckgo-mcp-server",
      "npm_downloads": 10136,
      "keywords": [
        "duckduckgo",
        "searches",
        "search",
        "duckduckgo search",
        "duckduckgo api",
        "provides duckduckgo"
      ],
      "category": "web-search"
    },
    "zhushengxiao--jianshu": {
      "owner": "zhushengxiao",
      "name": "jianshu",
      "url": "https://github.com/zhushengxiao/jianshu",
      "imageUrl": "/freedevtools/mcp/pfp/zhushengxiao.webp",
      "description": "Manage and organize writing projects with a user-friendly interface, providing features tailored specifically for writers to enhance their workflow.",
      "stars": 0,
      "forks": 0,
      "license": "No License",
      "language": "JavaScript",
      "updated_at": "2020-06-12T16:04:49Z",
      "readme_content": "This project was bootstrapped with [Create React App](https://github.com/facebook/create-react-app).\n\n## Available Scripts\n\nIn the project directory, you can run:\n\n### `yarn start`\n\nRuns the app in the development mode.<br />\nOpen [http://localhost:3000](http://localhost:3000) to view it in the browser.\n\nThe page will reload if you make edits.<br />\nYou will also see any lint errors in the console.\n\n### `yarn test`\n\nLaunches the test runner in the interactive watch mode.<br />\nSee the section about [running tests](https://facebook.github.io/create-react-app/docs/running-tests) for more information.\n\n### `yarn build`\n\nBuilds the app for production to the `build` folder.<br />\nIt correctly bundles React in production mode and optimizes the build for the best performance.\n\nThe build is minified and the filenames include the hashes.<br />\nYour app is ready to be deployed!\n\nSee the section about [deployment](https://facebook.github.io/create-react-app/docs/deployment) for more information.\n\n### `yarn eject`\n\n**Note: this is a one-way operation. Once you `eject`, you can’t go back!**\n\nIf you aren’t satisfied with the build tool and configuration choices, you can `eject` at any time. This command will remove the single build dependency from your project.\n\nInstead, it will copy all the configuration files and the transitive dependencies (webpack, Babel, ESLint, etc) right into your project so you have full control over them. All of the commands except `eject` will still work, but they will point to the copied scripts so you can tweak them. At this point you’re on your own.\n\nYou don’t have to ever use `eject`. The curated feature set is suitable for small and middle deployments, and you shouldn’t feel obligated to use this feature. However we understand that this tool wouldn’t be useful if you couldn’t customize it when you are ready for it.\n\n## Learn More\n\nYou can learn more in the [Create React App documentation](https://facebook.github.io/create-react-app/docs/getting-started).\n\nTo learn React, check out the [React documentation](https://reactjs.org/).\n\n### Code Splitting\n\nThis section has moved here: https://facebook.github.io/create-react-app/docs/code-splitting\n\n### Analyzing the Bundle Size\n\nThis section has moved here: https://facebook.github.io/create-react-app/docs/analyzing-the-bundle-size\n\n### Making a Progressive Web App\n\nThis section has moved here: https://facebook.github.io/create-react-app/docs/making-a-progressive-web-app\n\n### Advanced Configuration\n\nThis section has moved here: https://facebook.github.io/create-react-app/docs/advanced-configuration\n\n### Deployment\n\nThis section has moved here: https://facebook.github.io/create-react-app/docs/deployment\n\n### `yarn build` fails to minify\n\nThis section has moved here: https://facebook.github.io/create-react-app/docs/troubleshooting#npm-run-build-fails-to-minify\n#\u0000 \u0000j\u0000i\u0000a\u0000n\u0000s\u0000h\u0000u\u0000\r\u0000\n\u0000",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "zhushengxiao",
        "writing",
        "writers",
        "organize writing",
        "search zhushengxiao",
        "zhushengxiao jianshu"
      ],
      "category": "web-search"
    },
    "zoomeye-ai--mcp_zoomeye": {
      "owner": "zoomeye-ai",
      "name": "mcp_zoomeye",
      "url": "https://github.com/zoomeye-ai/mcp_zoomeye",
      "imageUrl": "/freedevtools/mcp/pfp/zoomeye-ai.webp",
      "description": "Query network asset information using ZoomEye dorks, while benefiting from a caching mechanism and advanced error handling to enhance data retrieval capabilities.",
      "stars": 48,
      "forks": 13,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-10-03T03:44:56Z",
      "readme_content": "# 🚀 ZoomEye MCP Server\n\nA Model Context Protocol (MCP) server that provides network asset information based on query conditions. This server allows Large Language Models (LLMs) to obtain network asset information by querying ZoomEye using dorks and other search parameters.\n\n## 🔔 Announcement\n\n🎉 We are excited to announce the official open-source release of **ZoomEye MCP Server** — a powerful Model Context Protocol (MCP) server that brings real-time cyber asset intelligence to AI assistants and development environments.\n\n🚀 Free Trial: 7-Day FREE Access to ZoomEye MCP!\nExperience ZoomEye MCP — the AI-powered cyberspace asset search engine — absolutely free for 7 days!\n\n🔍 Search global internet assets, track real-time changes, and unlock AI-driven insights — all in one place.\n\n👉 How to claim:\n\n1. Follow us on Twitter: [@zoomeye_team](https://x.com/zoomeye_team)\n2. DM us \"MCP\" and your MCP setup screenshot\n3. Get instant access to your 7-day membership\n\n🎁 Limited-time free trial — explore the power of AI asset search today!\n\n💡 Provide insightful feedback that gets officially adopted, and you'll unlock **even more rewards**!\n\n🔧 Fully compatible with leading MCP environments:\n\n- Claude Desktop\n- Cursor\n- Windsurf\n- Cline\n- Continue\n- Zed\n- Cherry Studio\n- Chatbox\n\n🔗 Explore ZoomEye MCP Server on:\n\n- GitHub: [zoomeye-ai/mcp_zoomeye](https://github.com/zoomeye-ai/mcp_zoomeye)\n- MCP.so: [mcp.so/server/mcp_zoomeye](https://mcp.so/server/mcp_zoomeye/zoomeye-ai)\n- Smithery: [smithery.ai/server/@zoomeye-ai/mcp_zoomeye](https://smithery.ai/server/@zoomeye-ai/mcp_zoomeye)\n- Cursor Directory: [cursor.directory/mcp/zoomeye](https://cursor.directory/mcp/zoomeye)\n- Pulse MCP: [pulsemcp.com/servers/zoomeye](https://www.pulsemcp.com/servers/zoomeye)\n- Glama MCP: [glama.ai/mcp/servers](https://glama.ai/mcp/servers)\n\nWe welcome everyone to use, explore, and contribute!\n\n## 🔑 How can I get a ZoomEye API key?\n\nTo use this MCP server, you’ll need a ZoomEye API key.\n\n1. Go to https://www.zoomeye.ai\n2. Register or log in\n3. Click your avatar → **Profile**\n4. Copy your **API-KEY**\n5. Set the environment variable:\n   \n   `export ZOOMEYE_API_KEY=\"your_api_key_here\"`\n\n\n\n\n\n## Features\n\n- Query ZoomEye for network asset information using dorks\n- Caching mechanism to improve performance and reduce API calls\n- Automatic retry mechanism for failed API requests\n- Comprehensive error handling and logging\n\n## Available Tools\n\n- `zoomeye_search` - Get network asset information based on query conditions.\n  - Required parameters:\n    - `qbase64` (string): Base64 encoded query string for ZoomEye search\n  - Optional parameters:\n    - `page` (integer): View asset page number, default is 1\n    - `pagesize` (integer): Number of records per page, default is 10, maximum is 1000\n    - `fields` (string): The fields to return, separated by commas\n    - `sub_type` (string): Data type, supports v4, v6, and web. Default is v4\n    - `facets` (string): Statistical items, separated by commas if there are multiple\n    - `ignore_cache` (boolean): Whether to ignore the cache\n\n## Usage Guide\n\n### Basic Usage\n\nOnce the server is running, you can interact with it through your AI assistant or development environment. Here's how to use it:\n\n1. **Start the server** using one of the installation methods above\n2. **Configure your AI assistant** (Claude Desktop, Cursor, Windsurf, Cline, Continue, Zed, etc.) to use the server\n3. **Query network information** using natural language\n\n\n\n### Search Syntax Guide\n\n- Search Scope covers devices (IPv4, IPv6) and websites (domains).\n- When entering a search string, the system will match keywords in \"global\" mode, including content from various\n  protocols such as HTTP, SSH, FTP, etc. (e.g., HTTP/HTTPS protocol headers, body, SSL, title, and other protocol\n  banners).\n- Search strings are case-insensitive and will be segmented for matching (the search results page provides a \"\n  segmentation\" test feature). When using == for search, it enforces exact case-sensitive matching with strict syntax.\n- Please use quotes for search strings (e.g., \"Cisco System\" or 'Cisco System'). If the search string contains quotes,\n  use the escape character, e.g.,\"a\\\"b\". If the search string contains parentheses, use the escape character, e.g.,\n  portinfo\\(\\).\n\nYou can see more detailed search syntax rules in [prompts.py](./src/mcp_server_zoomeye/prompts.py).\n\nFor more information on the ZoomEye Search API, refer to the [ZoomEye API v2 documentation](https://www.zoomeye.ai/doc).\n\n## Getting Started\n\n### Prerequisites\n\n1. **ZoomEye API Key**\n   \n   - Register for an account at [ZoomEye](https://www.zoomeye.ai/)\n   - Obtain your API key from your account settings\n   - The API key will be used to authenticate your requests to the ZoomEye API\n2. **Python Environment**\n   \n   - Python 3.10 or higher is required\n   - Alternatively, you can use Docker to run the server without installing Python\n\n## Installation\n\n### Using PIP\n\nAlternatively, you can install `mcp-server-zoomeye` via pip:\n\n```bash\npip install mcp-server-zoomeye\n```\n\nAfter installation, you can run it as a script using the following command:\n\n```bash\npython -m mcp_server_zoomeye\n```\n\n### Using Docker\n\nYou can also run the ZoomEye MCP server using Docker:\n\n#### Pull from Docker Hub\n\n```bash\n# Pull the latest image\ndocker pull zoomeyeteam/mcp-server-zoomeye:latest\n\n# Run the container with your API key\ndocker run -i --rm -e ZOOMEYE_API_KEY=your_api_key_here zoomeyeteam/mcp-server-zoomeye:latest\n```\n\n> **Note**: We provide multi-architecture Docker images that support `linux/amd64` and `linux/arm64` platforms and can run on Intel/AMD and ARM (such as Apple Silicon) processors.\n\n#### Build from Source\n\nAlternatively, you can build the Docker image from source:\n\n```bash\n# Clone the repository\ngit clone https://github.com/zoomeye-ai/mcp_zoomeye.git\ncd mcp_zoomeye\n\n# Build the Docker image\ndocker build -t zoomeyeteam/mcp-server-zoomeye:local .\n\n# Run the container\ndocker run -i --rm -e ZOOMEYE_API_KEY=your_api_key_here zoomeyeteam/mcp-server-zoomeye:local\n```\n\n### Using uv\n\n[`uv`](https://docs.astral.sh/uv/) is a fast Python package installer and resolver written in Rust. It's a modern alternative to pip that offers significant performance improvements.\n\n#### Installation of uv\n\n```bash\n# Install uv using curl (macOS/Linux)\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n\n# Or using PowerShell (Windows)\nirm https://astral.sh/uv/install.ps1 | iex\n\n# Or using Homebrew (macOS)\nbrew install uv\n```\n\n#### Using uvx to run mcp-server-zoomeye\n\nNo specific installation is required when using [`uvx`](https://docs.astral.sh/uv/guides/tools/), which allows you to run Python packages directly:\n\n#### Installing with uv\n\nAlternatively, you can install the package using uv:\n\n```bash\n# Install in the current environment\nuv pip install mcp-server-zoomeye\n\n# Or create and install in a new virtual environment\nuv venv\nuv pip install mcp-server-zoomeye\n```\n\n## Configuration\n\n### Environment Variables\n\nThe ZoomEye MCP server requires the following environment variable:\n\n- `ZOOMEYE_API_KEY`: Your ZoomEye API key for authentication\n\nYou can set this environment variable in several ways:\n\n1. **Export in your shell session**:\n   \n   ```bash\n   export ZOOMEYE_API_KEY=\"your_api_key_here\"\n   ```\n2. **Pass directly when running the container** (for Docker):\n   \n   ```bash\n   docker run -i --rm -e ZOOMEYE_API_KEY=your_api_key_here zoomeyeteam/mcp-server-zoomeye:latest\n   ```\n\n### Configure Claude.app\n\nAdd the following in Claude settings:\n\n<details>\n<summary>Using uvx</summary>\n\n```json\n\"mcpServers\": {\n  \"zoomeye\": {\n    \"command\": \"uvx\",\n    \"args\": [\"mcp-server-zoomeye\"],\n    \"env\": {\n        \"ZOOMEYE_API_KEY\": \"your_api_key_here\"\n    }\n  }\n}\n```\n\n</details>\n\n<details>\n<summary>Using docker</summary>\n\n```json\n\"mcpServers\": {\n  \"zoomeye\": {\n    \"command\": \"docker\",\n    \"args\": [\"run\", \"-i\", \"--rm\", \"-e\", \"ZOOMEYE_API_KEY=your_api_key_here\", \"zoomeyeteam/mcp-server-zoomeye:latest\"],\n    \"env\": {\n      \"ZOOMEYE_API_KEY\": \"your_api_key_here\"\n    }\n  }\n}\n```\n\n</details>\n\n<details>\n<summary>Installed via pip</summary>\n\n```json\n\"mcpServers\": {\n  \"zoomeye\": {\n    \"command\": \"python\",\n    \"args\": [\"-m\", \"mcp_server_zoomeye\"],\n    \"env\": {\n        \"ZOOMEYE_API_KEY\": \"your_api_key_here\"\n    }\n  }\n}\n```\n\n</details>\n\n### Configure Zed\n\nAdd the following in Zed's settings.json:\n\n<details>\n<summary>Using uvx</summary>\n\n```json\n\"context_servers\": [\n  \"mcp-server-zoomeye\": {\n    \"command\": \"uvx\",\n    \"args\": [\"mcp-server-zoomeye\"],\n    \"env\": {\n        \"ZOOMEYE_API_KEY\": \"your_api_key_here\"\n    }\n  }\n],\n```\n\n</details>\n\n<details>\n<summary>Installed via pip</summary>\n\n```json\n\"context_servers\": {\n  \"mcp-server-zoomeye\": {\n    \"command\": \"python\",\n    \"args\": [\"-m\", \"mcp_server_zoomeye\"],\n    \"env\": {\n        \"ZOOMEYE_API_KEY\": \"your_api_key_here\"\n    }\n  }\n},\n```\n\n</details>\n\n## Example Interactions\n\n### Example 1: Retrieve global Apache Tomcat assets\n\n```json\n{\n  \"name\": \"zoomeye_search\",\n  \"arguments\": {\n    \"qbase64\": \"app=\\\"Apache Tomcat\\\"\"\n  }\n}\n```\n\nResponse:\n\n```json\n{\n  \"code\": 60000,\n  \"message\": \"success\",\n  \"total\": 163139107,\n  \"query\": \"app=\\\"Apache Tomcat\\\"\",\n  \"data\": [\n    {\n      \"url\": \"https://1.1.1.1:443\",\n      \"ssl.jarm\": \"29d29d15d29d29d00029d29d29d29dea0f89a2e5fb09e4d8e099befed92cfa\",\n      \"ssl.ja3s\": \"45094d08156d110d8ee97b204143db14\",\n      \"iconhash_md5\": \"f3418a443e7d841097c714d69ec4bcb8\",\n      \"robots_md5\": \"0b5ce08db7fb8fffe4e14d05588d49d9\",\n      \"security_md5\": \"0b5ce08db7fb8fffe4e14d05588d49d9\",\n      \"ip\": \"1.1.1.1\",\n      \"domain\": \"www.google.com\",\n      \"hostname\": \"SPACEX\",\n      \"os\": \"windows\",\n      \"port\": 443,\n      \"service\": \"https\",\n      \"title\": [\"GoogleGoogle appsGoogle Search\"],\n      \"version\": \"1.1.0\",\n      \"device\": \"webcam\",\n      \"rdns\": \"c01031-001.cust.wallcloud.ch\",\n      \"product\": \"OpenSSD\",\n      \"header\": \"HTTP/1.1 302 Found Location: https://www.google.com/?gws_rd=ssl Cache-Control: private...\",\n      \"header_hash\": \"27f9973fe57298c3b63919259877a84d\",\n      \"body\": \"HTTP/1.1 302 Found Location: https://www.google.com/?gws_rd=ssl Cache-Control: private...\",\n      \"body_hash\": \"84a18166fde3ee7e7c974b8d1e7e21b4\",\n      \"banner\": \"SSH-2.0-OpenSSH_7.6p1 Ubuntu-4ubuntu0.3\",\n      \"update_time\": \"2024-07-03T14:34:10\",\n      \"header.server.name\": \"nginx\",\n      \"header.server.version\": \"1.8.1\",\n      \"continent.name\": \"Europe\",\n      \"country.name\": \"Germany\",\n      \"province.name\": \"Hesse\",\n      \"city.name\": \"Frankfurt\",\n      \"lon\": \"118.753262\",\n      \"lat\": \"32.064838\",\n      \"isp.name\": \"aviel.ru\",\n      \"organization.name\": \"SERVISFIRST BANK\",\n      \"zipcode\": \"210003\",\n      \"idc\": 0,\n      \"honeypot\": 0,\n      \"asn\": 4837,\n      \"protocol\": \"tcp\",\n      \"ssl\": \"SSL Certificate Version: TLS 1.2 CipherSuit: TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256...\",\n      \"primary_industry\": \"Finance\",\n      \"sub_industry\": \"bank\",\n      \"rank\": 60\n    }\n  ]\n}\n```\n\n## Debugging and Troubleshooting\n\n### Using MCP Inspector\n\nThe Model Context Protocol Inspector is a tool that helps debug MCP servers by simulating client interactions. You can use it to test your ZoomEye MCP server:\n\n```bash\n# For uvx installation\nnpx @modelcontextprotocol/inspector uvx mcp-server-zoomeye\n\n# If developing locally\ncd path/to/servers/src/mcp_server_zoomeye\nnpx @modelcontextprotocol/inspector uv run mcp-server-zoomeye\n```\n\n### Common Issues\n\n1. **Authentication Errors**\n   \n   - Ensure your ZoomEye API key is correct and properly set as an environment variable\n   - Check that your API key has not expired or been revoked\n2. **Connection Issues**\n   \n   - Verify your internet connection\n   - Check if the ZoomEye API is experiencing downtime\n3. **No Results**\n   \n   - Your query might be too specific or contain syntax errors\n   - Try simplifying your query or using different search terms\n4. **Rate Limiting**\n   \n   - ZoomEye API has rate limits based on your account type\n   - Space out your requests or upgrade your account for higher limits\n\n## Advanced Usage\n\n### Caching\n\nThe ZoomEye MCP server implements caching to improve performance and reduce API calls:\n\n- Responses are cached based on the query parameters\n- Cache duration is configurable (default: 1 hour)\n- You can bypass the cache by setting `ignore_cache` to `true` in your query\n\n### Custom Fields\n\nYou can request specific fields in your query results by using the `fields` parameter:\n\n```json\n{\n  \"name\": \"zoomeye_search\",\n  \"arguments\": {\n    \"qbase64\": \"app=\\\"Apache\\\"\",\n    \"fields\": \"ip,port,domain,service,os,country,city\"\n  }\n}\n```\n\n### Pagination\n\nFor queries that return many results, you can paginate through them:\n\n```json\n{\n  \"name\": \"zoomeye_search\",\n  \"arguments\": {\n    \"qbase64\": \"app=\\\"Apache\\\"\",\n    \"page\": 2,\n    \"pagesize\": 20\n  }\n}\n```\n\n## Contributing\n\nWe encourage contributions to mcp-server-zoomeye to help expand and improve its functionality. Whether it's adding new related tools, enhancing existing features, or improving documentation, your input is valuable.\n\nFor examples of other MCP servers and implementation patterns, see:\nhttps://github.com/modelcontextprotocol/servers\n\nPull requests are welcome! Feel free to contribute new ideas, bug fixes, or enhancements to make mcp-server-zoomeye more robust and practical.\n\n## License\n\nmcp-server-zoomeye is licensed under the MIT License. This means you are free to use, modify, and distribute the software, subject to the terms and conditions of the MIT License. For more information, see the LICENSE file in the project repository.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "zoomeye",
        "mcp_zoomeye",
        "retrieval",
        "search zoomeye",
        "mcp_zoomeye query",
        "ai mcp_zoomeye"
      ],
      "category": "web-search"
    },
    "zueai--webdev-mcp": {
      "owner": "zueai",
      "name": "webdev-mcp",
      "url": "https://github.com/zueai/webdev-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/zueai.webp",
      "description": "Provides web development tools for capturing screenshots and listing available screens as base64 encoded strings for further processing or analysis.",
      "stars": 10,
      "forks": 5,
      "license": "No License",
      "language": "TypeScript",
      "updated_at": "2025-09-20T06:57:23Z",
      "readme_content": "# webdev-mcp\n\nAn MCP server that provides useful web development tools.\n\n## Usage\n\n### Cursor\n\n- To install in a project, add the MCP server to your `.cursor/mcp.json`:\n\n```json\n{\n\t\"mcpServers\": {\n\t\t\"webdev\": {\n\t\t\t\"command\": \"npx\",\n\t\t\t\"args\": [\"webdev-mcp\"],\n\n\t\t}\n\t}\n}\n```\n\n- To install globally, add this command to your Cursor settings:\n\n```bash\nnpx webdev-mcp\n```\n\n### Windsurf\n\n- Add the MCP server to your `~/.codeium/windsurf/mcp_config.json` file:\n\n```json\n{\n\t\"mcpServers\": {\n\t\t\"webdev\": {\n\t\t\t\"command\": \"npx\",\n\t\t\t\"args\": [\"webdev-mcp\"]\n\t\t}\n\t}\n}\n```\n\n## Tools\n\nCurrently, the only 2 tools are `takeScreenshot` and `listScreens`. Your agent can use the list screens tool to get the screen id of the screen it wants to screenshot.\n\nThe tool will return the screenshot as a base64 encoded string.\n\n\n\n## Tips\n\nMake sure YOLO mode is on and MCP tools protection is off in your Cursor settings for the best experience. You might have to allow Cursor to record your screen on MacOS.",
      "npm_url": "https://www.npmjs.com/package/webdev-mcp",
      "npm_downloads": 3530,
      "keywords": [
        "webdev",
        "zueai",
        "screenshots",
        "zueai webdev",
        "webdev mcp",
        "capturing screenshots"
      ],
      "category": "web-search"
    },
    "zxsimple--mcp-sample": {
      "owner": "zxsimple",
      "name": "mcp-sample",
      "url": "https://github.com/zxsimple/mcp-sample",
      "imageUrl": "/freedevtools/mcp/pfp/zxsimple.webp",
      "description": "Fetch website content effortlessly for data retrieval in applications. Connect with an LLM client to automate web interactions.",
      "stars": 0,
      "forks": 0,
      "license": "MIT License",
      "language": "",
      "updated_at": "2025-04-02T02:45:50Z",
      "readme_content": "# MCP Python SDK\n\n<div align=\"center\">\n\n<strong>Python implementation of the Model Context Protocol (MCP)</strong>\n\n[![PyPI][pypi-badge]][pypi-url]\n[![MIT licensed][mit-badge]][mit-url]\n[![Python Version][python-badge]][python-url]\n[![Documentation][docs-badge]][docs-url]\n[![Specification][spec-badge]][spec-url]\n[![GitHub Discussions][discussions-badge]][discussions-url]\n\n</div>\n\n<!-- omit in toc -->\n## Table of Contents\n\n- [MCP Python SDK](#mcp-python-sdk)\n  - [Overview](#overview)\n  - [Installation](#installation)\n    - [Adding MCP to your python project](#adding-mcp-to-your-python-project)\n    - [Running the standalone MCP development tools](#running-the-standalone-mcp-development-tools)\n  - [Quickstart](#quickstart)\n  - [What is MCP?](#what-is-mcp)\n  - [Core Concepts](#core-concepts)\n    - [Server](#server)\n    - [Resources](#resources)\n    - [Tools](#tools)\n    - [Prompts](#prompts)\n    - [Images](#images)\n    - [Context](#context)\n  - [Running Your Server](#running-your-server)\n    - [Development Mode](#development-mode)\n    - [Claude Desktop Integration](#claude-desktop-integration)\n    - [Direct Execution](#direct-execution)\n    - [Mounting to an Existing ASGI Server](#mounting-to-an-existing-asgi-server)\n  - [Examples](#examples)\n    - [Echo Server](#echo-server)\n    - [SQLite Explorer](#sqlite-explorer)\n  - [Advanced Usage](#advanced-usage)\n    - [Low-Level Server](#low-level-server)\n    - [Writing MCP Clients](#writing-mcp-clients)\n    - [MCP Primitives](#mcp-primitives)\n    - [Server Capabilities](#server-capabilities)\n  - [Documentation](#documentation)\n  - [Contributing](#contributing)\n  - [License](#license)\n\n[pypi-badge]: https://img.shields.io/pypi/v/mcp.svg\n[pypi-url]: https://pypi.org/project/mcp/\n[mit-badge]: https://img.shields.io/pypi/l/mcp.svg\n[mit-url]: https://github.com/modelcontextprotocol/python-sdk/blob/main/LICENSE\n[python-badge]: https://img.shields.io/pypi/pyversions/mcp.svg\n[python-url]: https://www.python.org/downloads/\n[docs-badge]: https://img.shields.io/badge/docs-modelcontextprotocol.io-blue.svg\n[docs-url]: https://modelcontextprotocol.io\n[spec-badge]: https://img.shields.io/badge/spec-spec.modelcontextprotocol.io-blue.svg\n[spec-url]: https://spec.modelcontextprotocol.io\n[discussions-badge]: https://img.shields.io/github/discussions/modelcontextprotocol/python-sdk\n[discussions-url]: https://github.com/modelcontextprotocol/python-sdk/discussions\n\n## Overview\n\nThe Model Context Protocol allows applications to provide context for LLMs in a standardized way, separating the concerns of providing context from the actual LLM interaction. This Python SDK implements the full MCP specification, making it easy to:\n\n- Build MCP clients that can connect to any MCP server\n- Create MCP servers that expose resources, prompts and tools\n- Use standard transports like stdio and SSE\n- Handle all MCP protocol messages and lifecycle events\n\n## Installation\n\n### Adding MCP to your python project\n\nWe recommend using [uv](https://docs.astral.sh/uv/) to manage your Python projects. In a uv managed python project, add mcp to dependencies by:\n\n```bash\nuv add \"mcp[cli]\"\n```\n\nAlternatively, for projects using pip for dependencies:\n```bash\npip install \"mcp[cli]\"\n```\n\n### Running the standalone MCP development tools\n\nTo run the mcp command with uv:\n\n```bash\nuv run mcp\n```\n\n## Quickstart\n\nLet's create a simple MCP server that exposes a calculator tool and some data:\n\n```python\n# server.py\nfrom mcp.server.fastmcp import FastMCP\n\n# Create an MCP server\nmcp = FastMCP(\"Demo\")\n\n\n# Add an addition tool\n@mcp.tool()\ndef add(a: int, b: int) -> int:\n    \"\"\"Add two numbers\"\"\"\n    return a + b\n\n\n# Add a dynamic greeting resource\n@mcp.resource(\"greeting://{name}\")\ndef get_greeting(name: str) -> str:\n    \"\"\"Get a personalized greeting\"\"\"\n    return f\"Hello, {name}!\"\n```\n\nYou can install this server in [Claude Desktop](https://claude.ai/download) and interact with it right away by running:\n```bash\nmcp install server.py\n```\n\nAlternatively, you can test it with the MCP Inspector:\n```bash\nmcp dev server.py\n```\n\n## What is MCP?\n\nThe [Model Context Protocol (MCP)](https://modelcontextprotocol.io) lets you build servers that expose data and functionality to LLM applications in a secure, standardized way. Think of it like a web API, but specifically designed for LLM interactions. MCP servers can:\n\n- Expose data through **Resources** (think of these sort of like GET endpoints; they are used to load information into the LLM's context)\n- Provide functionality through **Tools** (sort of like POST endpoints; they are used to execute code or otherwise produce a side effect)\n- Define interaction patterns through **Prompts** (reusable templates for LLM interactions)\n- And more!\n\n## Core Concepts\n\n### Server\n\nThe FastMCP server is your core interface to the MCP protocol. It handles connection management, protocol compliance, and message routing:\n\n```python\n# Add lifespan support for startup/shutdown with strong typing\nfrom contextlib import asynccontextmanager\nfrom collections.abc import AsyncIterator\nfrom dataclasses import dataclass\n\nfrom fake_database import Database  # Replace with your actual DB type\n\nfrom mcp.server.fastmcp import Context, FastMCP\n\n# Create a named server\nmcp = FastMCP(\"My App\")\n\n# Specify dependencies for deployment and development\nmcp = FastMCP(\"My App\", dependencies=[\"pandas\", \"numpy\"])\n\n\n@dataclass\nclass AppContext:\n    db: Database\n\n\n@asynccontextmanager\nasync def app_lifespan(server: FastMCP) -> AsyncIterator[AppContext]:\n    \"\"\"Manage application lifecycle with type-safe context\"\"\"\n    # Initialize on startup\n    db = await Database.connect()\n    try:\n        yield AppContext(db=db)\n    finally:\n        # Cleanup on shutdown\n        await db.disconnect()\n\n\n# Pass lifespan to server\nmcp = FastMCP(\"My App\", lifespan=app_lifespan)\n\n\n# Access type-safe lifespan context in tools\n@mcp.tool()\ndef query_db(ctx: Context) -> str:\n    \"\"\"Tool that uses initialized resources\"\"\"\n    db = ctx.request_context.lifespan_context[\"db\"]\n    return db.query()\n```\n\n### Resources\n\nResources are how you expose data to LLMs. They're similar to GET endpoints in a REST API - they provide data but shouldn't perform significant computation or have side effects:\n\n```python\nfrom mcp.server.fastmcp import FastMCP\n\nmcp = FastMCP(\"My App\")\n\n\n@mcp.resource(\"config://app\")\ndef get_config() -> str:\n    \"\"\"Static configuration data\"\"\"\n    return \"App configuration here\"\n\n\n@mcp.resource(\"users://{user_id}/profile\")\ndef get_user_profile(user_id: str) -> str:\n    \"\"\"Dynamic user data\"\"\"\n    return f\"Profile data for user {user_id}\"\n```\n\n### Tools\n\nTools let LLMs take actions through your server. Unlike resources, tools are expected to perform computation and have side effects:\n\n```python\nimport httpx\nfrom mcp.server.fastmcp import FastMCP\n\nmcp = FastMCP(\"My App\")\n\n\n@mcp.tool()\ndef calculate_bmi(weight_kg: float, height_m: float) -> float:\n    \"\"\"Calculate BMI given weight in kg and height in meters\"\"\"\n    return weight_kg / (height_m**2)\n\n\n@mcp.tool()\nasync def fetch_weather(city: str) -> str:\n    \"\"\"Fetch current weather for a city\"\"\"\n    async with httpx.AsyncClient() as client:\n        response = await client.get(f\"https://api.weather.com/{city}\")\n        return response.text\n```\n\n### Prompts\n\nPrompts are reusable templates that help LLMs interact with your server effectively:\n\n```python\nfrom mcp.server.fastmcp import FastMCP\nfrom mcp.server.fastmcp.prompts import base\n\nmcp = FastMCP(\"My App\")\n\n\n@mcp.prompt()\ndef review_code(code: str) -> str:\n    return f\"Please review this code:\\n\\n{code}\"\n\n\n@mcp.prompt()\ndef debug_error(error: str) -> list[base.Message]:\n    return [\n        base.UserMessage(\"I'm seeing this error:\"),\n        base.UserMessage(error),\n        base.AssistantMessage(\"I'll help debug that. What have you tried so far?\"),\n    ]\n```\n\n### Images\n\nFastMCP provides an `Image` class that automatically handles image data:\n\n```python\nfrom mcp.server.fastmcp import FastMCP, Image\nfrom PIL import Image as PILImage\n\nmcp = FastMCP(\"My App\")\n\n\n@mcp.tool()\ndef create_thumbnail(image_path: str) -> Image:\n    \"\"\"Create a thumbnail from an image\"\"\"\n    img = PILImage.open(image_path)\n    img.thumbnail((100, 100))\n    return Image(data=img.tobytes(), format=\"png\")\n```\n\n### Context\n\nThe Context object gives your tools and resources access to MCP capabilities:\n\n```python\nfrom mcp.server.fastmcp import FastMCP, Context\n\nmcp = FastMCP(\"My App\")\n\n\n@mcp.tool()\nasync def long_task(files: list[str], ctx: Context) -> str:\n    \"\"\"Process multiple files with progress tracking\"\"\"\n    for i, file in enumerate(files):\n        ctx.info(f\"Processing {file}\")\n        await ctx.report_progress(i, len(files))\n        data, mime_type = await ctx.read_resource(f\"file://{file}\")\n    return \"Processing complete\"\n```\n\n## Running Your Server\n\n### Development Mode\n\nThe fastest way to test and debug your server is with the MCP Inspector:\n\n```bash\nmcp dev server.py\n\n# Add dependencies\nmcp dev server.py --with pandas --with numpy\n\n# Mount local code\nmcp dev server.py --with-editable .\n```\n\n### Claude Desktop Integration\n\nOnce your server is ready, install it in Claude Desktop:\n\n```bash\nmcp install server.py\n\n# Custom name\nmcp install server.py --name \"My Analytics Server\"\n\n# Environment variables\nmcp install server.py -v API_KEY=abc123 -v DB_URL=postgres://...\nmcp install server.py -f .env\n```\n\n### Direct Execution\n\nFor advanced scenarios like custom deployments:\n\n```python\nfrom mcp.server.fastmcp import FastMCP\n\nmcp = FastMCP(\"My App\")\n\nif __name__ == \"__main__\":\n    mcp.run()\n```\n\nRun it with:\n```bash\npython server.py\n# or\nmcp run server.py\n```\n\n### Mounting to an Existing ASGI Server\n\nYou can mount the SSE server to an existing ASGI server using the `sse_app` method. This allows you to integrate the SSE server with other ASGI applications.\n\n```python\nfrom starlette.applications import Starlette\nfrom starlette.routing import Mount, Host\nfrom mcp.server.fastmcp import FastMCP\n\n\nmcp = FastMCP(\"My App\")\n\n# Mount the SSE server to the existing ASGI server\napp = Starlette(\n    routes=[\n        Mount('/', app=mcp.sse_app()),\n    ]\n)\n\n# or dynamically mount as host\napp.router.routes.append(Host('mcp.acme.corp', app=mcp.sse_app()))\n```\n\nFor more information on mounting applications in Starlette, see the [Starlette documentation](https://www.starlette.io/routing/#submounting-routes).\n\n## Examples\n\n### Echo Server\n\nA simple server demonstrating resources, tools, and prompts:\n\n```python\nfrom mcp.server.fastmcp import FastMCP\n\nmcp = FastMCP(\"Echo\")\n\n\n@mcp.resource(\"echo://{message}\")\ndef echo_resource(message: str) -> str:\n    \"\"\"Echo a message as a resource\"\"\"\n    return f\"Resource echo: {message}\"\n\n\n@mcp.tool()\ndef echo_tool(message: str) -> str:\n    \"\"\"Echo a message as a tool\"\"\"\n    return f\"Tool echo: {message}\"\n\n\n@mcp.prompt()\ndef echo_prompt(message: str) -> str:\n    \"\"\"Create an echo prompt\"\"\"\n    return f\"Please process this message: {message}\"\n```\n\n### SQLite Explorer\n\nA more complex example showing database integration:\n\n```python\nimport sqlite3\n\nfrom mcp.server.fastmcp import FastMCP\n\nmcp = FastMCP(\"SQLite Explorer\")\n\n\n@mcp.resource(\"schema://main\")\ndef get_schema() -> str:\n    \"\"\"Provide the database schema as a resource\"\"\"\n    conn = sqlite3.connect(\"database.db\")\n    schema = conn.execute(\"SELECT sql FROM sqlite_master WHERE type='table'\").fetchall()\n    return \"\\n\".join(sql[0] for sql in schema if sql[0])\n\n\n@mcp.tool()\ndef query_data(sql: str) -> str:\n    \"\"\"Execute SQL queries safely\"\"\"\n    conn = sqlite3.connect(\"database.db\")\n    try:\n        result = conn.execute(sql).fetchall()\n        return \"\\n\".join(str(row) for row in result)\n    except Exception as e:\n        return f\"Error: {str(e)}\"\n```\n\n## Advanced Usage\n\n### Low-Level Server\n\nFor more control, you can use the low-level server implementation directly. This gives you full access to the protocol and allows you to customize every aspect of your server, including lifecycle management through the lifespan API:\n\n```python\nfrom contextlib import asynccontextmanager\nfrom collections.abc import AsyncIterator\n\nfrom fake_database import Database  # Replace with your actual DB type\n\nfrom mcp.server import Server\n\n\n@asynccontextmanager\nasync def server_lifespan(server: Server) -> AsyncIterator[dict]:\n    \"\"\"Manage server startup and shutdown lifecycle.\"\"\"\n    # Initialize resources on startup\n    db = await Database.connect()\n    try:\n        yield {\"db\": db}\n    finally:\n        # Clean up on shutdown\n        await db.disconnect()\n\n\n# Pass lifespan to server\nserver = Server(\"example-server\", lifespan=server_lifespan)\n\n\n# Access lifespan context in handlers\n@server.call_tool()\nasync def query_db(name: str, arguments: dict) -> list:\n    ctx = server.request_context\n    db = ctx.lifespan_context[\"db\"]\n    return await db.query(arguments[\"query\"])\n```\n\nThe lifespan API provides:\n- A way to initialize resources when the server starts and clean them up when it stops\n- Access to initialized resources through the request context in handlers\n- Type-safe context passing between lifespan and request handlers\n\n```python\nimport mcp.server.stdio\nimport mcp.types as types\nfrom mcp.server.lowlevel import NotificationOptions, Server\nfrom mcp.server.models import InitializationOptions\n\n# Create a server instance\nserver = Server(\"example-server\")\n\n\n@server.list_prompts()\nasync def handle_list_prompts() -> list[types.Prompt]:\n    return [\n        types.Prompt(\n            name=\"example-prompt\",\n            description=\"An example prompt template\",\n            arguments=[\n                types.PromptArgument(\n                    name=\"arg1\", description=\"Example argument\", required=True\n                )\n            ],\n        )\n    ]\n\n\n@server.get_prompt()\nasync def handle_get_prompt(\n    name: str, arguments: dict[str, str] | None\n) -> types.GetPromptResult:\n    if name != \"example-prompt\":\n        raise ValueError(f\"Unknown prompt: {name}\")\n\n    return types.GetPromptResult(\n        description=\"Example prompt\",\n        messages=[\n            types.PromptMessage(\n                role=\"user\",\n                content=types.TextContent(type=\"text\", text=\"Example prompt text\"),\n            )\n        ],\n    )\n\n\nasync def run():\n    async with mcp.server.stdio.stdio_server() as (read_stream, write_stream):\n        await server.run(\n            read_stream,\n            write_stream,\n            InitializationOptions(\n                server_name=\"example\",\n                server_version=\"0.1.0\",\n                capabilities=server.get_capabilities(\n                    notification_options=NotificationOptions(),\n                    experimental_capabilities={},\n                ),\n            ),\n        )\n\n\nif __name__ == \"__main__\":\n    import asyncio\n\n    asyncio.run(run())\n```\n\n### Writing MCP Clients\n\nThe SDK provides a high-level client interface for connecting to MCP servers:\n\n```python\nfrom mcp import ClientSession, StdioServerParameters, types\nfrom mcp.client.stdio import stdio_client\n\n# Create server parameters for stdio connection\nserver_params = StdioServerParameters(\n    command=\"python\",  # Executable\n    args=[\"example_server.py\"],  # Optional command line arguments\n    env=None,  # Optional environment variables\n)\n\n\n# Optional: create a sampling callback\nasync def handle_sampling_message(\n    message: types.CreateMessageRequestParams,\n) -> types.CreateMessageResult:\n    return types.CreateMessageResult(\n        role=\"assistant\",\n        content=types.TextContent(\n            type=\"text\",\n            text=\"Hello, world! from model\",\n        ),\n        model=\"gpt-3.5-turbo\",\n        stopReason=\"endTurn\",\n    )\n\n\nasync def run():\n    async with stdio_client(server_params) as (read, write):\n        async with ClientSession(\n            read, write, sampling_callback=handle_sampling_message\n        ) as session:\n            # Initialize the connection\n            await session.initialize()\n\n            # List available prompts\n            prompts = await session.list_prompts()\n\n            # Get a prompt\n            prompt = await session.get_prompt(\n                \"example-prompt\", arguments={\"arg1\": \"value\"}\n            )\n\n            # List available resources\n            resources = await session.list_resources()\n\n            # List available tools\n            tools = await session.list_tools()\n\n            # Read a resource\n            content, mime_type = await session.read_resource(\"file://some/path\")\n\n            # Call a tool\n            result = await session.call_tool(\"tool-name\", arguments={\"arg1\": \"value\"})\n\n\nif __name__ == \"__main__\":\n    import asyncio\n\n    asyncio.run(run())\n```\n\n### MCP Primitives\n\nThe MCP protocol defines three core primitives that servers can implement:\n\n| Primitive | Control               | Description                                         | Example Use                  |\n|-----------|-----------------------|-----------------------------------------------------|------------------------------|\n| Prompts   | User-controlled       | Interactive templates invoked by user choice        | Slash commands, menu options |\n| Resources | Application-controlled| Contextual data managed by the client application   | File contents, API responses |\n| Tools     | Model-controlled      | Functions exposed to the LLM to take actions        | API calls, data updates      |\n\n### Server Capabilities\n\nMCP servers declare capabilities during initialization:\n\n| Capability  | Feature Flag                 | Description                        |\n|-------------|------------------------------|------------------------------------|\n| `prompts`   | `listChanged`                | Prompt template management         |\n| `resources` | `subscribe`<br/>`listChanged`| Resource exposure and updates      |\n| `tools`     | `listChanged`                | Tool discovery and execution       |\n| `logging`   | -                            | Server logging configuration       |\n| `completion`| -                            | Argument completion suggestions    |\n\n## Documentation\n\n- [Model Context Protocol documentation](https://modelcontextprotocol.io)\n- [Model Context Protocol specification](https://spec.modelcontextprotocol.io)\n- [Officially supported servers](https://github.com/modelcontextprotocol/servers)\n\n## Contributing\n\nWe are passionate about supporting contributors of all levels of experience and would love to see you get involved in the project. See the [contributing guide](CONTRIBUTING.md) to get started.\n\n## License\n\nThis project is licensed under the MIT License - see the LICENSE file for details.\n",
      "npm_url": "https://www.npmjs.com/package/mcp-sample-server",
      "npm_downloads": 77,
      "keywords": [
        "retrieval",
        "web",
        "search",
        "web search",
        "automate web",
        "data retrieval"
      ],
      "category": "web-search"
    },
    "zxsimple--reference-servers": {
      "owner": "zxsimple",
      "name": "reference-servers",
      "url": "https://github.com/zxsimple/reference-servers",
      "imageUrl": "/freedevtools/mcp/pfp/zxsimple.webp",
      "description": "Retrieve and process web content from URLs, converting HTML to markdown format for easier extraction and consumption by language models. It supports content truncation and allows for customized starting points for content extraction.",
      "stars": 0,
      "forks": 0,
      "license": "MIT License",
      "language": "",
      "updated_at": "2025-04-02T02:58:42Z",
      "readme_content": "# Model Context Protocol servers\n\nThis repository is a collection of *reference implementations* for the [Model Context Protocol](https://modelcontextprotocol.io/) (MCP), as well as references\nto community built servers and additional resources.\n\nThe servers in this repository showcase the versatility and extensibility of MCP, demonstrating how it can be used to give Large Language Models (LLMs) secure, controlled access to tools and data sources.\nEach MCP server is implemented with either the [Typescript MCP SDK](https://github.com/modelcontextprotocol/typescript-sdk) or [Python MCP SDK](https://github.com/modelcontextprotocol/python-sdk).\n\n> Note: Lists in this README are maintained in alphabetical order to minimize merge conflicts when adding new items.\n\n## 🌟 Reference Servers\n\nThese servers aim to demonstrate MCP features and the Typescript and Python SDK.\n\n- **[AWS KB Retrieval](src/aws-kb-retrieval-server)** - Retrieval from AWS Knowledge Base using Bedrock Agent Runtime\n- **[Brave Search](src/brave-search)** - Web and local search using Brave's Search API\n- **[EverArt](src/everart)** - AI image generation using various models\n- **[Everything](src/everything)** - Reference / test server with prompts, resources, and tools\n- **[Fetch](src/fetch)** - Web content fetching and conversion for efficient LLM usage\n- **[Filesystem](src/filesystem)** - Secure file operations with configurable access controls\n- **[Git](src/git)** - Tools to read, search, and manipulate Git repositories\n- **[GitHub](src/github)** - Repository management, file operations, and GitHub API integration\n- **[GitLab](src/gitlab)** - GitLab API, enabling project management\n- **[Google Drive](src/gdrive)** - File access and search capabilities for Google Drive\n- **[Google Maps](src/google-maps)** - Location services, directions, and place details\n- **[Memory](src/memory)** - Knowledge graph-based persistent memory system\n- **[PostgreSQL](src/postgres)** - Read-only database access with schema inspection\n- **[Puppeteer](src/puppeteer)** - Browser automation and web scraping\n- **[Sentry](src/sentry)** - Retrieving and analyzing issues from Sentry.io\n- **[Sequential Thinking](src/sequentialthinking)** - Dynamic and reflective problem-solving through thought sequences\n- **[Slack](src/slack)** - Channel management and messaging capabilities\n- **[Sqlite](src/sqlite)** - Database interaction and business intelligence capabilities\n- **[Time](src/time)** - Time and timezone conversion capabilities\n\n## 🤝 Third-Party Servers\n\n### 🎖️ Official Integrations\n\nOfficial integrations are maintained by companies building production ready MCP servers for their platforms.\n\n- <img height=\"12\" width=\"12\" src=\"https://axiom.co/favicon.ico\" alt=\"Axiom Logo\" /> **[Axiom](https://github.com/axiomhq/mcp-server-axiom)** - Query and analyze your Axiom logs, traces, and all other event data in natural language\n- <img height=\"12\" width=\"12\" src=\"https://browserbase.com/favicon.ico\" alt=\"Browserbase Logo\" /> **[Browserbase](https://github.com/browserbase/mcp-server-browserbase)** - Automate browser interactions in the cloud (e.g. web navigation, data extraction, form filling, and more)\n- <img alt=\"cloudflare\" height=\"12\" width=\"12\" src=\"https://cdn.simpleicons.org/cloudflare\" /> **[Cloudflare](https://github.com/cloudflare/mcp-server-cloudflare)** - Deploy, configure & interrogate your resources on the Cloudflare developer platform (e.g. Workers/KV/R2/D1)\n- <img height=\"12\" width=\"12\" src=\"https://e2b.dev/favicon.ico\" alt=\"E2B Logo\" /> **[E2B](https://github.com/e2b-dev/mcp-server)** - Run code in secure sandboxes hosted by [E2B](https://e2b.dev)\n- <img height=\"12\" width=\"12\" src=\"https://exa.ai/images/favicon-32x32.png\" alt=\"Exa Logo\" /> **[Exa](https://github.com/exa-labs/exa-mcp-server)** - Search Engine made for AIs by [Exa](https://exa.ai)\n- <img height=\"12\" width=\"12\" src=\"https://fireproof.storage/favicon.ico\" alt=\"Fireproof Logo\" /> **[Fireproof](https://github.com/fireproof-storage/mcp-database-server)** - Immutable ledger database with live synchronization\n- <img alt=\"jetbrains\" height=\"12\" width=\"12\" src=\"https://cdn.simpleicons.org/jetbrains\" /> **[JetBrains](https://github.com/JetBrains/mcp-jetbrains)** – Work on your code with JetBrains IDEs\n- <img height=\"12\" width=\"12\" src=\"https://kagi.com/favicon.ico\" alt=\"Kagi Logo\" /> **[Kagi Search](https://github.com/kagisearch/kagimcp)** - Search the web using Kagi's search API\n- <img height=\"12\" width=\"12\" src=\"https://www.meilisearch.com/favicon.ico\" alt=\"Meilisearch Logo\" /> **[Meilisearch](https://github.com/meilisearch/meilisearch-mcp)** - Interact & query with Meilisearch (Full-text & semantic search API)\n-  **[Metoro](https://github.com/metoro-io/metoro-mcp-server)** - Query and interact with kubernetes environments monitored by Metoro\n- <img height=\"12\" width=\"12\" src=\"https://www.motherduck.com/favicon.ico\" alt=\"MotherDuck Logo\" /> **[MotherDuck](https://github.com/motherduckdb/mcp-server-motherduck)** - Query and analyze data with MotherDuck and local DuckDB\n- <img height=\"12\" width=\"12\" src=\"https://needle-ai.com/images/needle-logo-orange-2-rounded.png\" alt=\"Needle AI Logo\" /> **[Needle](https://github.com/needle-ai/needle-mcp)** - Production-ready RAG out of the box to search and retrieve data from your own documents.\n- <img height=\"12\" width=\"12\" src=\"https://neo4j.com/favicon.ico\" alt=\"Neo4j Logo\" /> **[Neo4j](https://github.com/neo4j-contrib/mcp-neo4j/)** - Neo4j graph database server (schema + read/write-cypher) and separate graph database backed memory\n- **[Neon](https://github.com/neondatabase/mcp-server-neon)** - Interact with the Neon serverless Postgres platform\n- <img alt=\"logomark\" height=\"12\" width=\"12\" src=\"https://qdrant.tech/img/brand-resources-logos/logomark.svg\" /> **[Qdrant](https://github.com/qdrant/mcp-server-qdrant/)** - Implement semantic memory layer on top of the Qdrant vector search engine\n- **[Raygun](https://github.com/MindscapeHQ/mcp-server-raygun)** - Interact with your crash reporting and real using monitoring data on your Raygun account\n- <img alt=\"56912e614b35093426c515860f9f2234\" height=\"12\" width=\"12\" src=\"https://pics.fatwang2.com/56912e614b35093426c515860f9f2234.svg\" /> [Search1API](https://github.com/fatwang2/search1api-mcp) - One API for Search, Crawling, and Sitemaps\n- <img height=\"12\" width=\"12\" src=\"https://www.tinybird.co/favicon.ico\" alt=\"Tinybird Logo\" /> **[Tinybird](https://github.com/tinybirdco/mcp-tinybird)** - Interact with Tinybird serverless ClickHouse platform\n\n### 🌎 Community Servers\n\nA growing set of community-developed and maintained servers demonstrates various applications of MCP across different domains.\n\n> **Note:** Community servers are **untested** and should be used at **your own risk**. They are not affiliated with or endorsed by Anthropic.\n\n- **[AWS S3](https://github.com/aws-samples/sample-mcp-server-s3)** - A sample MCP server for AWS S3 that flexibly fetches objects from S3 such as PDF documents\n- **[AWS](https://github.com/rishikavikondala/mcp-server-aws)** - Perform operations on your AWS resources using an LLM\n- **[Airtable](https://github.com/domdomegg/airtable-mcp-server)** - Read and write access to [Airtable](https://airtable.com/) databases, with schema inspection.\n- **[Airtable](https://github.com/felores/airtable-mcp)** - Airtable Model Context Protocol Server.\n- **[AlphaVantage](https://github.com/calvernaz/alphavantage)** - MCP server for stock market data API [AlphaVantage](https://www.alphavantage.co)\n- **[Anki](https://github.com/scorzeth/anki-mcp-server)** - An MCP server for interacting with your [Anki](https://apps.ankiweb.net) decks and cards.\n- **[Any Chat Completions](https://github.com/pyroprompts/any-chat-completions-mcp)** - Interact with any OpenAI SDK Compatible Chat Completions API like OpenAI, Perplexity, Groq, xAI and many more.\n- **[Atlassian](https://github.com/sooperset/mcp-atlassian)** - Interact with Atlassian Cloud products (Confluence and Jira) including searching/reading Confluence spaces/pages, accessing Jira issues, and project metadata.\n- **[BigQuery](https://github.com/LucasHild/mcp-server-bigquery)** (by LucasHild) - This server enables LLMs to inspect database schemas and execute queries on BigQuery.\n- **[BigQuery](https://github.com/ergut/mcp-bigquery-server)** (by ergut) - Server implementation for Google BigQuery integration that enables direct BigQuery database access and querying capabilities\n- **[ChatMCP](https://github.com/AI-QL/chat-mcp)** – An Open Source Cross-platform GUI Desktop application compatible with Linux, macOS, and Windows, enabling seamless interaction with MCP servers across dynamically selectable LLMs, by **[AIQL](https://github.com/AI-QL)**\n- **[ChatSum](https://github.com/mcpso/mcp-server-chatsum)** - Query and Summarize chat messages with LLM. by [mcpso](https://mcp.so)\n- **[Chroma](https://github.com/privetin/chroma)** - Vector database server for semantic document search and metadata filtering, built on Chroma\n- **[Cloudinary](https://github.com/felores/cloudinary-mcp-server)** - Cloudinary Model Context Protocol Server to upload media to Cloudinary and get back the media link and details.\n- **[cognee-mcp](https://github.com/topoteretes/cognee-mcp-server)** - GraphRAG memory server with customizable ingestion, data processing and search\n- **[coin_api_mcp](https://github.com/longmans/coin_api_mcp)** - Provides access to [coinmarketcap](https://coinmarketcap.com/) cryptocurrency data.\n- **[Contentful-mcp](https://github.com/ivo-toby/contentful-mcp)** - Read, update, delete, publish content in your [Contentful](https://contentful.com) space(s) from this MCP Server.\n- **[Data Exploration](https://github.com/reading-plus-ai/mcp-server-data-exploration)** - MCP server for autonomous data exploration on .csv-based datasets, providing intelligent insights with minimal effort. NOTE: Will execute arbitrary Python code on your machine, please use with caution!\n- **[Dataset Viewer](https://github.com/privetin/dataset-viewer)** - Browse and analyze Hugging Face datasets with features like search, filtering, statistics, and data export\n- **[DevRev](https://github.com/kpsunil97/devrev-mcp-server)** - An MCP server to integrate with DevRev APIs to search through your DevRev Knowledge Graph where objects can be imported from diff. sources listed [here](https://devrev.ai/docs/import#available-sources).\n- **[Dify](https://github.com/YanxingLiu/dify-mcp-server)** - A simple implementation of an MCP server for dify workflows.\n- **[Docker](https://github.com/ckreiling/mcp-server-docker)** - Integrate with Docker to manage containers, images, volumes, and networks.\n- **[Drupal](https://github.com/Omedia/mcp-server-drupal)** - Server for interacting with [Drupal](https://www.drupal.org/project/mcp) using STDIO transport layer.\n- **[Elasticsearch](https://github.com/cr7258/elasticsearch-mcp-server)** - MCP server implementation that provides Elasticsearch interaction.\n- **[Fetch](https://github.com/zcaceres/fetch-mcp)** - A server that flexibly fetches HTML, JSON, Markdown, or plaintext.\n- **[FireCrawl](https://github.com/vrknetha/mcp-server-firecrawl)** - Advanced web scraping with JavaScript rendering, PDF support, and smart rate limiting\n- **[FlightRadar24](https://github.com/sunsetcoder/flightradar24-mcp-server)** - A Claude Desktop MCP server that helps you track flights in real-time using Flightradar24 data.\n- **[Glean](https://github.com/longyi1207/glean-mcp-server)** - A server that uses Glean API to search and chat.\n- **[Google Calendar](https://github.com/v-3/google-calendar)** - Integration with Google Calendar to check schedules, find time, and add/delete events\n- **[Google Tasks](https://github.com/zcaceres/gtasks-mcp)** - Google Tasks API Model Context Protocol Server.\n- **[Home Assistant](https://github.com/tevonsb/homeassistant-mcp)** - Interact with [Home Assistant](https://www.home-assistant.io/) including viewing and controlling lights, switches, sensors, and all other Home Assistant entities.\n- **[HuggingFace Spaces](https://github.com/evalstate/mcp-hfspace)** - Server for using HuggingFace Spaces, supporting Open Source Image, Audio, Text Models and more. Claude Desktop mode for easy integration.\n- **[Inoyu](https://github.com/sergehuber/inoyu-mcp-unomi-server)** - Interact with an Apache Unomi CDP customer data platform to retrieve and update customer profiles\n- **[Keycloak MCP](https://github.com/ChristophEnglisch/keycloak-model-context-protocol)** - This MCP server enables natural language interaction with Keycloak for user and realm management including creating, deleting, and listing users and realms.\n- **[Kubernetes](https://github.com/Flux159/mcp-server-kubernetes)** - Connect to Kubernetes cluster and manage pods, deployments, and services.\n- **[Linear](https://github.com/jerhadf/linear-mcp-server)** - Allows LLM to interact with Linear's API for project management, including searching, creating, and updating issues.\n- **[LlamaCloud](https://github.com/run-llama/mcp-server-llamacloud)** (by marcusschiesser) - Integrate the data stored in a managed index on [LlamaCloud](https://cloud.llamaindex.ai/)\n- **[MCP Installer](https://github.com/anaisbetts/mcp-installer)** - This server is a server that installs other MCP servers for you.\n- **[mcp-k8s-go](https://github.com/strowk/mcp-k8s-go)** - Golang-based Kubernetes server for MCP to browse pods and their logs, events, namespaces and more. Built to be extensible.\n- **[MSSQL](https://github.com/aekanun2020/mcp-server/)** - MSSQL database integration with configurable access controls and schema inspection\n- **[Markdownify](https://github.com/zcaceres/mcp-markdownify-server)** - MCP to convert almost anything to Markdown (PPTX, HTML, PDF, Youtube Transcripts and more)\n- **[Minima](https://github.com/dmayboroda/minima)** - MCP server for RAG on local files\n- **[MongoDB](https://github.com/kiliczsh/mcp-mongo-server)** - A Model Context Protocol Server for MongoDB.\n- **[MySQL](https://github.com/benborla/mcp-server-mysql)** (by benborla) - MySQL database integration in NodeJS with configurable access controls and schema inspection\n- **[MySQL](https://github.com/designcomputer/mysql_mcp_server)** (by DesignComputer) - MySQL database integration in Python with configurable access controls and schema inspection\n- **[NS Travel Information](https://github.com/r-huijts/ns-mcp-server)** - Access Dutch Railways (NS) real-time train travel information and disruptions through the official NS API.\n- **[Notion](https://github.com/suekou/mcp-notion-server)** (by suekou) - Interact with Notion API.\n- **[Notion](https://github.com/v-3/notion-server)** (by v-3) - Notion MCP integration. Search, Read, Update, and Create pages through Claude chat.\n- **[oatpp-mcp](https://github.com/oatpp/oatpp-mcp)** - C++ MCP integration for Oat++. Use [Oat++](https://oatpp.io) to build MCP servers.\n- **[Obsidian Markdown Notes](https://github.com/calclavia/mcp-obsidian)** - Read and search through your Obsidian vault or any directory containing Markdown notes\n- **[OpenAPI](https://github.com/snaggle-ai/openapi-mcp-server)** - Interact with [OpenAPI](https://www.openapis.org/) APIs.\n- **[OpenCTI](https://github.com/Spathodea-Network/opencti-mcp)** - Interact with OpenCTI platform to retrieve threat intelligence data including reports, indicators, malware and threat actors.\n- **[OpenRPC](https://github.com/shanejonas/openrpc-mpc-server)** - Interact with and discover JSON-RPC APIs via [OpenRPC](https://open-rpc.org).\n- **[Pandoc](https://github.com/vivekVells/mcp-pandoc)** - MCP server for seamless document format conversion using Pandoc, supporting Markdown, HTML, and plain text, with other formats like PDF, csv and docx in development.\n- **[Pinecone](https://github.com/sirmews/mcp-pinecone)** - MCP server for searching and uploading records to Pinecone. Allows for simple RAG features, leveraging Pinecone's Inference API.\n- **[Placid.app](https://github.com/felores/placid-mcp-server)** - Generate image and video creatives using Placid.app templates\n- **[Playwright](https://github.com/executeautomation/mcp-playwright)** - This MCP Server will help you run browser automation and webscraping using Playwright\n- **[Postman](https://github.com/shannonlal/mcp-postman)** - MCP server for running Postman Collections locally via Newman. Allows for simple execution of Postman Server and returns the results of whether the collection passed all the tests.\n- **[RAG Web Browser](https://github.com/apify/mcp-server-rag-web-browser)** An MCP server for Apify's RAG Web Browser Actor to perform web searches, scrape URLs, and return content in Markdown.\n- **[Rememberizer AI](https://github.com/skydeckai/mcp-server-rememberizer)** - An MCP server designed for interacting with the Rememberizer data source, facilitating enhanced knowledge retrieval.\n- **[Salesforce MCP](https://github.com/smn2gnt/MCP-Salesforce)** - Interact with Salesforce Data and Metadata\n- **[Scholarly](https://github.com/adityak74/mcp-scholarly)** - A MCP server to search for scholarly and academic articles.\n- **[Snowflake](https://github.com/isaacwasserman/mcp-snowflake-server)** - This MCP server enables LLMs to interact with Snowflake databases, allowing for secure and controlled data operations.\n- **[Spotify](https://github.com/varunneal/spotify-mcp)** - This MCP allows an LLM to play and use Spotify.\n- **[TMDB](https://github.com/Laksh-star/mcp-server-tmdb)** - This MCP server integrates with The Movie Database (TMDB) API to provide movie information, search capabilities, and recommendations.\n- **[Tavily search](https://github.com/RamXX/mcp-tavily)** - An MCP server for Tavily's search & news API, with explicit site inclusions/exclusions\n- **[Todoist](https://github.com/abhiz123/todoist-mcp-server)** - Interact with Todoist to manage your tasks.\n- **[Vega-Lite](https://github.com/isaacwasserman/mcp-vegalite-server)** - Generate visualizations from fetched data using the VegaLite format and renderer.\n- **[Windows CLI](https://github.com/SimonB97/win-cli-mcp-server)** - MCP server for secure command-line interactions on Windows systems, enabling controlled access to PowerShell, CMD, and Git Bash shells.\n- **[X (Twitter)](https://github.com/EnesCinr/twitter-mcp)** (by EnesCinr) - Interact with twitter API. Post tweets and search for tweets by query.\n- **[X (Twitter)](https://github.com/vidhupv/x-mcp)** (by vidhupv) - Create, manage and publish X/Twitter posts directly through Claude chat.\n- **[XMind](https://github.com/apeyroux/mcp-xmind)** - Read and search through your XMind directory containing XMind files.\n\n## 📚 Frameworks\n\nThese are high-level frameworks that make it easier to build MCP servers.\n\n* [EasyMCP](https://github.com/zcaceres/easy-mcp/) (TypeScript)\n* [FastMCP](https://github.com/punkpeye/fastmcp) (TypeScript)\n\n## 📚 Resources\n\nAdditional resources on MCP.\n\n- **[AiMCP](https://www.aimcp.info)** - A collection of MCP clients&servers to find the right mcp tools by **[Hekmon](https://github.com/hekmon8)**\n- **[Awesome Crypto MCP Servers by badkk](https://github.com/badkk/awesome-crypto-mcp-servers)** - A curated list of MCP servers by **[Luke Fan](https://github.com/badkk)**\n- **[Awesome MCP Servers by appcypher](https://github.com/appcypher/awesome-mcp-servers)** - A curated list of MCP servers by **[Stephen Akinyemi](https://github.com/appcypher)**\n- **[Awesome MCP Servers by punkpeye](https://github.com/punkpeye/awesome-mcp-servers)** (**[website](https://glama.ai/mcp/servers)**) - A curated list of MCP servers by **[Frank Fiegel](https://github.com/punkpeye)**\n- **[Awesome MCP Servers by wong2](https://github.com/wong2/awesome-mcp-servers)** (**[website](https://mcpservers.org)**) - A curated list of MCP servers by **[wong2](https://github.com/wong2)**\n- **[Discord Server](https://glama.ai/mcp/discord)** – A community discord server dedicated to MCP by **[Frank Fiegel](https://github.com/punkpeye)**\n- **[MCP Badges](https://github.com/mcpx-dev/mcp-badges)** – Quickly highlight your MCP project with clear, eye-catching badges, by **[Ironben](https://github.com/nanbingxyz)**\n- **[MCP X Community](https://x.com/i/communities/1861891349609603310)** – A X community for MCP by **[Xiaoyi](https://x.com/chxy)**\n- **[mcp-cli](https://github.com/wong2/mcp-cli)** - A CLI inspector for the Model Context Protocol by **[wong2](https://github.com/wong2)**\n- **[mcp-get](https://mcp-get.com)** - Command line tool for installing and managing MCP servers by **[Michael Latman](https://github.com/michaellatman)**\n- **[mcp-manager](https://github.com/zueai/mcp-manager)** - Simple Web UI to install and manage MCP servers for Claude Desktop by **[Zue](https://github.com/zueai)**\n- **[MCPHub](https://github.com/Jeamee/MCPHub-Desktop)** – An Open Source MacOS & Windows GUI Desktop app for discovering, installing and managing MCP servers by **[Jeamee](https://github.com/jeamee)**\n- **[mcp.run](https://mcp.run)** - A hosted registry and control plane to install & run secure + portable MCP Servers.\n- **[Open-Sourced MCP Servers Directory](https://github.com/chatmcp/mcp-directory)** - A curated list of MCP servers by **[mcpso](https://mcp.so)**\n- **[PulseMCP](https://www.pulsemcp.com)** ([API](https://www.pulsemcp.com/api)) - Community hub & weekly newsletter for discovering MCP servers, clients, articles, and news by **[Tadas Antanavicius](https://github.com/tadasant)**, **[Mike Coughlin](https://github.com/macoughl)**, and **[Ravina Patel](https://github.com/ravinahp)**\n- **[r/mcp](https://www.reddit.com/r/mcp)** – A Reddit community dedicated to MCP by **[Frank Fiegel](https://github.com/punkpeye)**\n- **[Smithery](https://smithery.ai/)** - A registry of MCP servers to find the right tools for your LLM agents by **[Henry Mao](https://github.com/calclavia)**\n\n## 🚀 Getting Started\n\n### Using MCP Servers in this Repository\nTypescript-based servers in this repository can be used directly with `npx`.\n\nFor example, this will start the [Memory](src/memory) server:\n```sh\nnpx -y @modelcontextprotocol/server-memory\n```\n\nPython-based servers in this repository can be used directly with [`uvx`](https://docs.astral.sh/uv/concepts/tools/) or [`pip`](https://pypi.org/project/pip/). `uvx` is recommended for ease of use and setup.\n\nFor example, this will start the [Git](src/git) server:\n```sh\n# With uvx\nuvx mcp-server-git\n\n# With pip\npip install mcp-server-git\npython -m mcp_server_git\n```\n\nFollow [these](https://docs.astral.sh/uv/getting-started/installation/) instructions to install `uv` / `uvx` and [these](https://pip.pypa.io/en/stable/installation/) to install `pip`.\n\n### Using an MCP Client\nHowever, running a server on its own isn't very useful, and should instead be configured into an MCP client. For example, here's the Claude Desktop configuration to use the above server:\n\n```json\n{\n  \"mcpServers\": {\n    \"memory\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@modelcontextprotocol/server-memory\"]\n    }\n  }\n}\n```\n\nAdditional examples of using the Claude Desktop as an MCP client might look like:\n\n```json\n{\n  \"mcpServers\": {\n    \"filesystem\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@modelcontextprotocol/server-filesystem\", \"/path/to/allowed/files\"]\n    },\n    \"git\": {\n      \"command\": \"uvx\",\n      \"args\": [\"mcp-server-git\", \"--repository\", \"path/to/git/repo\"]\n    },\n    \"github\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@modelcontextprotocol/server-github\"],\n      \"env\": {\n        \"GITHUB_PERSONAL_ACCESS_TOKEN\": \"<YOUR_TOKEN>\"\n      }\n    },\n    \"postgres\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@modelcontextprotocol/server-postgres\", \"postgresql://localhost/mydb\"]\n    }\n  }\n}\n```\n\n## 🛠️ Creating Your Own Server\n\nInterested in creating your own MCP server? Visit the official documentation at [modelcontextprotocol.io](https://modelcontextprotocol.io/introduction) for comprehensive guides, best practices, and technical details on implementing MCP servers.\n\n## 🤝 Contributing\n\nSee [CONTRIBUTING.md](CONTRIBUTING.md) for information about contributing to this repository.\n\n## 🔒 Security\n\nSee [SECURITY.md](SECURITY.md) for reporting security vulnerabilities.\n\n## 📜 License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n\n## 💬 Community\n\n- [GitHub Discussions](https://github.com/orgs/modelcontextprotocol/discussions)\n\n## ⭐ Support\n\nIf you find MCP servers useful, please consider starring the repository and contributing new servers or improvements!\n\n---\n\nManaged by Anthropic, but built together with the community. The Model Context Protocol is open source and we encourage everyone to contribute their own servers and improvements!",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "web",
        "zxsimple",
        "html",
        "search zxsimple",
        "web content",
        "content extraction"
      ],
      "category": "web-search"
    },
    "zym9863--pixabay-mcp": {
      "owner": "zym9863",
      "name": "pixabay-mcp",
      "url": "https://github.com/zym9863/pixabay-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/zym9863.webp",
      "description": "Connect to the Pixabay API to search for images and retrieve formatted results that include image URLs and metadata. Handle errors seamlessly during API interactions for reliable performance.",
      "stars": 4,
      "forks": 1,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-09-29T07:41:49Z",
      "readme_content": "# pixabay-mcp MCP Server\n\n[中文版](README_zh.md)\n\nA Model Context Protocol (MCP) server for Pixabay image and video search with structured results & runtime validation.\n\n<a href=\"https://glama.ai/mcp/servers/@zym9863/pixabay-mcp\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@zym9863/pixabay-mcp/badge\" alt=\"Pixabay Server MCP server\" />\n</a>\n\nThis TypeScript MCP server exposes Pixabay search tools over stdio so AI assistants / agents can retrieve media safely and reliably.\n\nHighlights:\n- Image & video search tools (Pixabay official API)\n- Runtime argument validation (enums, ranges, semantic checks)\n- Consistent error logging without leaking sensitive keys\n- Planned structured JSON payloads for easier downstream automation (see Roadmap)\n\n## Features\n\n### Tools\n`search_pixabay_images`\n  - Required: `query` (string)\n  - Optional: `image_type` (all|photo|illustration|vector), `orientation` (all|horizontal|vertical), `per_page` (3-200)\n  - Returns: human-readable text block (current) + (planned) structured JSON array of hits\n\n`search_pixabay_videos`\n  - Required: `query`\n  - Optional: `video_type` (all|film|animation), `orientation`, `per_page` (3-200), `min_duration`, `max_duration`\n  - Returns: human-readable text block + (planned) structured JSON with duration & URLs\n\n### Configuration\nEnvironment variables:\n| Name | Required | Default | Description |\n| ---- | -------- | ------- | ----------- |\n| `PIXABAY_API_KEY` | Yes | - | Your Pixabay API key (images & videos) |\n| `PIXABAY_TIMEOUT_MS` | No | 10000 (planned) | Request timeout once feature lands |\n| `PIXABAY_RETRY` | No | 0 (planned) | Number of retry attempts for transient network errors |\n\nNotes:\n- Safe search is enabled by default.\n- Keys are never echoed back in structured errors or logs.\n\n## Usage Examples\n\nCurrent (text only response excerpt):\n```\nFound 120 images for \"cat\":\n- cat, pet, animal (User: Alice): https://.../medium1.jpg\n- kitten, cute (User: Bob): https://.../medium2.jpg\n```\n\nPlanned structured result (Roadmap v0.4+):\n```jsonc\n{\n  \"content\": [\n    { \"type\": \"text\", \"text\": \"Found 120 images for \\\"cat\\\":\\n- ...\" },\n    {\n      \"type\": \"json\",\n      \"data\": {\n        \"query\": \"cat\",\n        \"totalHits\": 120,\n        \"page\": 1,\n        \"perPage\": 20,\n        \"hits\": [\n          { \"id\": 123, \"tags\": [\"cat\",\"animal\"], \"user\": \"Alice\", \"previewURL\": \"...\", \"webformatURL\": \"...\", \"largeImageURL\": \"...\" }\n        ]\n      }\n    }\n  ]\n}\n```\n\nError response (planned shape):\n```json\n{\n  \"content\": [{ \"type\": \"text\", \"text\": \"Pixabay API error: 400 ...\" }],\n  \"isError\": true,\n  \"metadata\": { \"status\": 400, \"code\": \"UPSTREAM_BAD_REQUEST\", \"hint\": \"Check API key or parameters\" }\n}\n```\n\n## Development\n\nInstall dependencies:\n```bash\nnpm install\n```\n\nBuild the server:\n```bash\nnpm run build\n```\n\nWatch mode:\n```bash\nnpm run watch\n```\n\n## Installation\n\n### Option 1: Using npx (Recommended)\n\nAdd this to your Claude Desktop configuration:\n\nOn MacOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\nOn Windows: `%APPDATA%/Claude/claude_desktop_config.json`\n\n```json\n{\n  \"mcpServers\": {\n    \"pixabay-mcp\": {\n      \"command\": \"npx\",\n      \"args\": [\"pixabay-mcp@latest\"],\n      \"env\": {\n        \"PIXABAY_API_KEY\": \"your_api_key_here\"\n      }\n    }\n  }\n}\n```\n\n### Option 2: Local Installation\n\n1. Clone and build the project:\n\n```bash\ngit clone https://github.com/zym9863/pixabay-mcp.git\ncd pixabay-mcp\nnpm install\nnpm run build\n```\n\n2. Add the server config:\n\n```json\n{\n  \"mcpServers\": {\n    \"pixabay-mcp\": {\n      \"command\": \"/path/to/pixabay-mcp/build/index.js\",\n      \"env\": {\n        \"PIXABAY_API_KEY\": \"your_api_key_here\"\n      }\n    }\n  }\n}\n```\n\n### API Key Setup\n\nGet your Pixabay API key from [https://pixabay.com/api/docs/](https://pixabay.com/api/docs/) and set it in the configuration above. The same key grants access to both image and video endpoints.\n\n### Debugging\n\nSince MCP servers communicate over stdio, debugging can be challenging. We recommend using the [MCP Inspector](https://github.com/modelcontextprotocol/inspector), which is available as a package script:\n\n```bash\nnpm run inspector\n```\n\nThe Inspector will provide a URL to access debugging tools in your browser.\n\n## Roadmap (Condensed)\n| Version | Focus | Key Items |\n| ------- | ----- | --------- |\n| v0.4 | Structured & Reliability | JSON payload, timeout, structured errors |\n| v0.5 | UX & Pagination | page/order params, limited retry, modular refactor, tests |\n| v0.6 | Multi-source Exploration | Evaluate integrating Unsplash/Pexels abstraction |\n\nSee `product.md` for full backlog & prioritization.\n\n## Contributing\nPlanned contributions welcome once tests & module split land (v0.5 target). Feel free to open issues for API shape / schema suggestions.\n\n## License\nMIT\n\n## Disclaimer\nThis project is not affiliated with Pixabay. Respect Pixabay's Terms of Service and rate limits.\n",
      "npm_url": "https://www.npmjs.com/package/pixabay-mcp",
      "npm_downloads": 174,
      "keywords": [
        "pixabay",
        "search",
        "mcp",
        "pixabay api",
        "pixabay mcp",
        "connect pixabay"
      ],
      "category": "web-search"
    }
  }
}