{
  "category": "virtual-assistants",
  "categoryDisplay": "Virtual Assistants",
  "description": "",
  "totalRepositories": 45,
  "repositories": {
    "AI-QL--chat-mcp": {
      "owner": "AI-QL",
      "name": "chat-mcp",
      "url": "https://github.com/AI-QL/chat-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/AI-QL.webp",
      "description": "A cross-platform desktop application that connects and interacts with various Large Language Models (LLMs) via the Model Context Protocol (MCP). It provides a clean and minimalistic codebase for testing and understanding MCP functionalities.",
      "stars": 238,
      "forks": 33,
      "license": "Apache License 2.0",
      "language": "HTML",
      "updated_at": "2025-10-04T06:18:14Z",
      "readme_content": "# MCP Chat Desktop App\n## A Cross-Platform Interface for LLMs\n\nThis desktop application utilizes the MCP (Model Context Protocol) to seamlessly connect and interact with various Large Language Models (LLMs). Built on Electron, the app ensures full cross-platform compatibility, enabling smooth operation across different operating systems.\n\nThe primary objective of this project is to deliver a clean, minimalistic codebase that simplifies understanding the core principles of MCP. Additionally, it provides a quick and efficient way to test multiple servers and LLMs, making it an ideal tool for developers and researchers alike.\n\n## News\n\nThis project originated as a modified version of Chat-UI, initially adopting a minimalist code approach to implement core MCP functionality for educational purposes. \n\nThrough iterative updates to MCP, I received community feedback advocating for a completely new architecture - one that eliminates third-party CDN dependencies and establishes clearer modular structure to better support derivative development and debugging workflows. \n\nThis led to the creation of [Tool Unitary User Interface](https://github.com/AI-QL/tuui),  a restructured desktop application optimized for AI-powered development. Building upon the original foundation, TUUI serves as a practical AI-assisted development paradigm, if you're interested, you can also leverage AI to develop new features for TUUI. The platform employs a strict linting and formatting system to ensure AI-generated code adheres to coding standards.\n\n> **📢 Update: June 2025**  \n> The current project refactoring has been largely completed, and a pre-release version is now available. Please refer to the following documentation for details:\n> - [TUUI GitHub Repository](https://github.com/AI-QL/tuui)\n> - [TUUI Architecture](https://deepwiki.com/AI-QL/tuui)\n> - [TUUI Official Website](https://www.tuui.com/)\n\n## Features\n\n- Cross-Platform Compatibility: Supports Linux, macOS, and Windows.\n\n- Flexible Apache-2.0 License: Allows easy modification and building of your own desktop applications.\n\n- Dynamic LLM Configuration: Compatible with all OpenAI SDK-supported LLMs, enabling quick testing of multiple backends through manual or preset configurations.\n\n- Multi-Client Management: Configure and manage multiple clients to connect to multiple servers using MCP config.\n\n- UI Adaptability: The UI can be directly extracted for web use, ensuring consistent ecosystem and interaction logic across web and desktop versions.\n\n\n## Architecture\n\nAdopted a straightforward architecture consistent with the MCP documentation to facilitate a clear understanding of MCP principles by:\n\n[DeepWiki](https://deepwiki.com/AI-QL/chat-mcp)\n\n## How to use\n\nAfter cloning or downloading this repository:\n\n1. Please modify the `config.json` file located in [src/main](src/main).  \n   Ensure that the `command` and `path` specified in the `args` are valid.\n\n2. Please ensure that [Node.js](https://nodejs.org/) is installed on your system.  \n   You can verify this by running `node -v` and `npm -v` in your terminal to check their respective versions.\n\n3. `npm install`\n\n4. `npm start`\n\n## Configuration\n\nCreate a `.json` file and paste the following content into it. This file can then be provided as the interface configuration for the Chat UI.\n\n- `gtp-api.json`\n\n    ```json\n    {\n        \"chatbotStore\": {\n            \"apiKey\": \"\",\n            \"url\": \"https://api.aiql.com\",\n            \"path\": \"/v1/chat/completions\",\n            \"model\": \"gpt-4o-mini\",\n            \"max_tokens_value\": \"\",\n            \"mcp\": true\n        },\n        \"defaultChoiceStore\": {\n            \"model\": [\n                \"gpt-4o-mini\",\n                \"gpt-4o\",\n                \"gpt-4\",\n                \"gpt-4-turbo\"\n            ]\n        }\n    }\n    ```\n\nYou can replace the 'url' if you have direct access to the OpenAI API.\n\nAlternatively, you can also use another API endpoint that supports function calls: \n\n- `qwen-api.json`\n\n    ```json\n    {\n        \"chatbotStore\": {\n            \"apiKey\": \"\",\n            \"url\": \"https://dashscope.aliyuncs.com/compatible-mode\",\n            \"path\": \"/v1/chat/completions\",\n            \"model\": \"qwen-turbo\",\n            \"max_tokens_value\": \"\",\n            \"mcp\": true\n        },\n        \"defaultChoiceStore\": {\n            \"model\": [\n                \"qwen-turbo\",\n                \"qwen-plus\",\n                \"qwen-max\"\n            ]\n        }\n    }\n    ```\n\n- `deepinfra.json`\n\n    ```json\n    {\n        \"chatbotStore\": {\n            \"apiKey\": \"\",\n            \"url\": \"https://api.deepinfra.com\",\n            \"path\": \"/v1/openai/chat/completions\",\n            \"model\": \"meta-llama/Meta-Llama-3.1-70B-Instruct\",\n            \"max_tokens_value\": \"32000\",\n            \"mcp\": true\n        },\n        \"defaultChoiceStore\": {\n            \"model\": [\n                \"meta-llama/Meta-Llama-3.1-70B-Instruct\",\n                \"meta-llama/Meta-Llama-3.1-405B-Instruct\",\n                \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n            ]\n        }\n    }\n    ```\n\n## Build Application\n\nYou can build your own desktop application by:\n\n```bash\nnpm run build-app\n```\n\nThis CLI helps you build and package your application for your current OS, with artifacts stored in the /artifacts directory.\n\nFor Debian/Ubuntu users experiencing RPM build issues, try one of the following solutions: \n\n- Edit `package.json` to skip the RPM build step. Or \n\n- Install `rpm` using `sudo apt-get install rpm` (You may need to run `sudo apt update` to ensure your package list is up-to-date)\n\n\n# Troubleshooting\n\n## Error: spawn npx ENOENT - [ISSUE 40](https://github.com/modelcontextprotocol/servers/issues/40)\n\nModify the `config.json` in [src/main](src/main)\n\nOn windows, npx may not work, please refer my workaround: [ISSUE 101](https://github.com/modelcontextprotocol/typescript-sdk/issues/101)\n\n- Or you can use `node` in config.json: \n    ```json\n    {\n        \"mcpServers\": {\n            \"filesystem\": {\n            \"command\": \"node\",\n            \"args\": [\n                \"node_modules/@modelcontextprotocol/server-filesystem/dist/index.js\",\n                \"D:/Github/mcp-test\"\n            ]\n            }\n        }\n    }\n    ```\n\nPlease ensure that the provided path is valid, especially if you are using a relative path. It is highly recommended to provide an absolute path for better clarity and accuracy.\n\nBy default, I will install `server-everything`, `server-filesystem`, and `server-puppeteer` for test purposes. However, you can install additional server libraries or use `npx` to utilize other server libraries as needed.\n\n## Installation timeout\n\nGenerally, after executing `npm install` for the entire project, the total size of files in the `node_modules` directory typically exceeds 500MB. \n\nIf the installation process stalls at less than 300MB and the progress bar remains static, it is likely due to a timeout during the installation of the latter part, specifically Electron.\n\nThis issue often arises because the download speed from Electron's default server is excessively slow or even inaccessible in certain regions. To resolve this, you can modify the environment or global variable `ELECTRON_MIRROR` to switch to an Electron mirror site that is accessible from your location.\n\n## Electron builder timeout\n\nWhen using electron-builder to package files, it automatically downloads several large release packages from GitHub. If the network connection is unstable, this process may be interrupted or timeout.\n\nOn Windows, you may need to clear the cache located under the `electron` and `electron-builder` directories within `C:\\Users\\YOURUSERNAME\\AppData\\Local` before attempting to retry.\n\nDue to potential terminal permission issues, it is recommended to use the default shell terminal instead of VSCode's built-in terminal.\n\n## Demo\n\n### Multimodal Support\n![demo_multimodal](https://gcore.jsdelivr.net/gh/AI-QL/.github/public/chat-mcp/demo-multimodal.png)\n\n### Reasoning and Latex Support\n![demo_latex](https://gcore.jsdelivr.net/gh/AI-QL/.github/public/chat-mcp/demo-latex.png)\n\n### MCP Tools Visualization\n![demo_tools](https://gcore.jsdelivr.net/gh/AI-QL/.github/public/chat-mcp/demo-tools.png)\n\n### MCP Toolcall Process Overview\n![demo_toolcall](https://gcore.jsdelivr.net/gh/AI-QL/.github/public/chat-mcp/demo-toolcall.png)\n\n### MCP Prompts Template\n![demo_prompts](https://gcore.jsdelivr.net/gh/AI-QL/.github/public/chat-mcp/demo-prompts.png)\n\n### Dynamic LLM Config\n![demo_llms](https://gcore.jsdelivr.net/gh/AI-QL/.github/public/chat-mcp/demo-llms.png)\n\n### DevTool Troubleshooting\n![demo_devtool](https://gcore.jsdelivr.net/gh/AI-QL/.github/public/chat-mcp/demo-devtool.png)\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "ql",
        "assistants",
        "chat",
        "virtual assistants",
        "ql chat",
        "chat mcp"
      ],
      "category": "virtual-assistants"
    },
    "AlexKissiJr--UnrealMCP": {
      "owner": "AlexKissiJr",
      "name": "UnrealMCP",
      "url": "https://github.com/AlexKissiJr/UnrealMCP",
      "imageUrl": "/freedevtools/mcp/pfp/AlexKissiJr.webp",
      "description": "Control and manipulate the Unreal Engine environment programmatically using AI tools through a Machine Control Protocol (MCP). Facilitate scene manipulation and automation to enhance the game development workflow.",
      "stars": 6,
      "forks": 1,
      "license": "No License",
      "language": "C++",
      "updated_at": "2025-06-11T11:25:27Z",
      "readme_content": "# UnrealMCP Plugin\n\n# VERY WIP REPO\nI'm working on adding more tools now and cleaning up the codebase, \nI plan to allow for easy tool extension outside the main plugin\n\nThis is very much a work in progress, and I need to clean up a lot of stuff!!!!!\n\nAlso, I only use windows, so I don't know how this would be setup for mac/unix\n\n## Overview\nUnrealMCP is an Unofficial Unreal Engine plugin designed to control Unreal Engine with AI tools. It implements a Machine Control Protocol (MCP) within Unreal Engine, allowing external AI systems to interact with and manipulate the Unreal environment programmatically.\n\nI only just learned about MCP a few days ago, so I'm not that familiar with it, I'm still learning so things might be initially pretty rough.\nI've implemented this using https://github.com/ahujasid/blender-mcp as a reference, which relies on Claude for Desktop. It now works with both Claude for Desktop and Cursor. If you experiment with other models, please let me know!\n\n## ⚠️ DISCLAIMER\nThis plugin allows AI agents to directly modify your Unreal Engine project. While it can be a powerful tool, it also comes with risks:\n\n- AI agents may make unexpected changes to your project\n- Files could be accidentally deleted or modified\n- Project settings could be altered\n- Assets could be overwritten\n\n**IMPORTANT SAFETY MEASURES:**\n1. Always use source control (like Git or Perforce) with your project\n2. Make regular backups of your project\n3. Test the plugin in a separate project first\n4. Review changes before committing them\n\nBy using this plugin, you acknowledge that:\n- You are solely responsible for any changes made to your project\n- The plugin author is not responsible for any damage, data loss, or issues caused by AI agents\n- You use this plugin at your own risk\n\n## Features\n- TCP server implementation for remote control of Unreal Engine\n- JSON-based command protocol for AI tools integration\n- Editor UI integration for easy access to MCP functionality\n- Comprehensive scene manipulation capabilities\n- Python companion scripts for client-side interaction\n\n## Roadmap\nThese are what I have in mind for development as of 3/14/2025\nI'm not sure what's possible yet, in theory anything, but it depends on how\ngood the integrated LLM is at utilizing these tools.\n- [X] Basic operations working\n- [X] Python working\n- [X] Materials\n- [ ] User Extensions (in progress)\n- [ ] Asset tools\n- [ ] Blueprints\n- [ ] Niagara VFX\n- [ ] Metasound\n- [ ] Landscape (I might hold off on this because Epic has mentioned they are going to be updating the landscape tools)\n- [ ] Modeling Tools\n- [ ] PCG\n\n## Requirements\n- Unreal Engine 5.5 (I have only tested on this version, may work with earlier, but no official support)\n- C++ development environment configured for Unreal Engine\n- Python 3.7+ for client-side scripting\n- Model to run the commands, in testing I've been using Claude for Desktop https://claude.ai/download\n\n## Prerequisites to run\n- Unreal Editor Installation (Tested with 5.3, but should work on 5.0+)\n- Python 3.7+ (This can run with your existing python install)\n- MCP compatible LLM (Claude for Desktop, Cursor, etc.)\n- Setup: run setup_unreal_mcp.bat in MCP folder as per instructions in MCP/README_MCP_SETUP.md\n\n## Quick Start for Cursor Users\nIf you want to use UnrealMCP with Cursor, follow these simple steps:\n\n1. Clone or download this repository as a zip\n2. Create a new Unreal Project, or open an existing one\n3. Create a \"Plugins\" folder in your project directory if it doesn't exist\n4. Unzip or copy this repository into the Plugins folder\n5. Run `setup_cursor_mcp.bat` in the MCP folder\n6. Open your Unreal project and enable the plugin in Edit > Plugins (if not already enabled)\n7. Start Cursor and ask it to work with your Unreal project\n\nThat's it! The setup script will automatically configure everything needed for Cursor integration.\n\n## Installation\n\n1. Clone or download this repository as a zip\n2. Create a new Unreal Project, or open an existing one\n3. Create a \"Plugins\" folder in your project directory if it doesn't exist\n4. Unzip or copy this repository into the Plugins folder\n5. Setup MCP \n    - Run the `setup_unreal_mcp.bat` script in the MCP folder (see `MCP/README_MCP_SETUP.md` for details)\n    - This will configure Python and your AI assistant (Claude for Desktop or Cursor)\n6. Open your Unreal project, the plugin should be available in the Plugins menu\n7. If not, enable the plugin in Edit > Plugins\n8. Choose your preferred AI assistant:\n    - For Claude for Desktop: follow the instructions in the \"With Claude for Desktop\" section below\n    - For Cursor: follow the instructions in the \"With Cursor\" section below\n\n## With Claude for Desktop\nYou will need to find your installation directory for Claude for Desktop. Find claude_desktop_config.json and add an entry and make it look like so:\n\n**Windows:** `%APPDATA%\\Claude\\claude_desktop_config.json`\n\n```json\n{\n    \"mcpServers\": {\n        \"unreal\": {\n            \"command\": \"C:/path/to/your/project/Plugins/UnrealMCP/MCP/run_unreal_mcp.bat\",\n            \"args\": []\n        }\n    }\n}\n```\n\nAlternatively the unreal_mcp_setup.bat script should do this for you.\n\nTo find the path to your claude for desktop install you can go into settings and click 'Edit Config'\nThis is usually in \n```\nC:\\Users\\USERNAME\\AppData\\Roaming\\Claude\n```\n\n## With Cursor\nCursor should be automatically configured if you've run the setup script with the Cursor option. If you need to manually configure it:\n\n**Windows:** `%APPDATA%\\Cursor\\User\\settings.json`\n\nAdd or update the settings with:\n```json\n{\n    \"mcp\": {\n        \"enabled\": true,\n        \"servers\": {\n            \"unreal\": {\n                \"command\": \"C:/path/to/your/project/Plugins/UnrealMCP/MCP/run_unreal_mcp.bat\",\n                \"args\": []\n            }\n        }\n    }\n}\n```\n\n## Testing\nOnce everything is setup you need to launch the unreal editor.\nNote: Nothing else has to be started or set up to run the mcp bridge, it will run when needed.\n\nOpen Claude for Desktop or Cursor, ensure that the tools have successfully enabled, ask your AI assistant to work in Unreal.\n\nHere are some example prompts to try:\n- \"What actors are in the current level?\" \n- \"Create a cube at position (0, 0, 100)\"\n- \"List available commands I can use with Unreal Engine\"\n\n## Usage\n### In Unreal Editor\nOnce the plugin is enabled, you'll find MCP controls in the editor toolbar button. \n![image](https://github.com/user-attachments/assets/68338e7a-090d-4fd9-acc9-37c0c1b63227)\n\n![image](https://github.com/user-attachments/assets/34f734ee-65a4-448a-a6db-9e941a588e93)\n\nThe TCP server can be started/stopped from here.\nCheck the output log under log filter LogMCP for extra information.\n\nOnce the server is confirmed up and running from the editor.\nOpen Claude for Desktop, ensure that the tools have successfully enabled, ask Claude to work in unreal.\n\nCurrently only basic operations are supported, creating objects, modfiying their transforms, getting scene info, and running python scripts.\nClaude makes a lot of errors with unreal python as I believe there aren't a ton of examples for it, but let it run and it will usually figure things out.\nI would really like to improve this aspect of how it works but it's low hanging fruit for adding functionality into unreal.\n\n### Client-Side Integration\nUse the provided Python scripts in the `MCP` directory to connect to and control your Unreal Engine instance:\n\n```python\nfrom unreal_mcp_client import UnrealMCPClient\n\n# Connect to the Unreal MCP server\nclient = UnrealMCPClient(\"localhost\", 13377)\n\n# Example: Create a cube in the scene\nclient.create_object(\n    class_name=\"StaticMeshActor\",\n    asset_path=\"/Engine/BasicShapes/Cube.Cube\",\n    location=(0, 0, 100),\n    rotation=(0, 0, 0),\n    scale=(1, 1, 1),\n    name=\"MCP_Cube\"\n)\n```\n\n## Command Reference\nThe plugin supports various commands for scene manipulation:\n- `get_scene_info`: Retrieve information about the current scene\n- `create_object`: Spawn a new object in the scene\n- `delete_object`: Remove an object from the scene\n- `modify_object`: Change properties of an existing object\n- `execute_python`: Run Python commands in Unreal's Python environment\n- And more to come...\n\nRefer to the documentation in the `Docs` directory for a complete command reference.\n\n## Security Considerations\n- The MCP server accepts connections from any client by default\n- Limit server exposure to localhost for development\n- Validate all incoming commands to prevent injection attacks\n\n## Troubleshooting\n- Ensure Unreal Engine is running with the MCP plugin.\n- Check logs in Claude for Desktop for stderr output.\n- Reach out on the discord, I just made it, but I will check it periodically\n  Discord (Dreamatron Studios): https://discord.gg/abRftdSe\n  \n### Project Structure\n- `Source/UnrealMCP/`: Core plugin implementation\n  - `Private/`: Internal implementation files\n  - `Public/`: Public header files\n- `Content/`: Plugin assets\n- `MCP/`: Python client scripts and examples\n- `Resources/`: Icons and other resources\n\n## License\nMIT License\n\nCopyright (c) 2025 kvick\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n\n## Credits\n- Created by: kvick\n- X: [@kvickart](https://x.com/kvickart)\n- Discord: https://discord.gg/abRftdSe\n  \n### Thank you to testers!!!\n- https://github.com/TheMurphinatur\n  \n- [@sidahuj](https://x.com/sidahuj) for the inspriation\n\n\n\n## Contributing\nContributions are welcome, but I will need some time to wrap my head around things and cleanup first, lol\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "unrealmcp",
        "ai",
        "automation",
        "unreal engine",
        "unrealmcp control",
        "virtual assistants"
      ],
      "category": "virtual-assistants"
    },
    "AllAboutAI-YT--mcpgame": {
      "owner": "AllAboutAI-YT",
      "name": "mcpgame",
      "url": "https://github.com/AllAboutAI-YT/mcpgame",
      "imageUrl": "/freedevtools/mcp/pfp/AllAboutAI-YT.webp",
      "description": "Multi-player control panel game featuring a virtual house environment with interactive elements like an image-generating TV and a computer terminal for accessing MCP systems. It enables real-time communication and user interaction within a detailed 3D setting.",
      "stars": 8,
      "forks": 5,
      "license": "No License",
      "language": "JavaScript",
      "updated_at": "2025-05-15T21:59:02Z",
      "readme_content": "# MCPGame\n\nA multi-player control panel game with Node.js backend featuring a virtual house environment with interactive elements.\n\n## Features\n\n- Immersive first-person 3D virtual house with outdoor environment\n- Beautifully detailed house with interior and exterior features\n- Interactive door to enter and exit the house\n- Garden area with trees, plants, and decorative elements\n- Interactive TV with image generation capabilities\n- Computer terminal for accessing MCP systems\n- Realistic movement and collision detection\n- Real-time server communication\n\n## Setup\n\n1. Install dependencies:\n```\nnpm install\n```\n\n2. Run the server:\n```\nnpm start\n```\nOr for development with auto-restart:\n```\nnpm run dev\n```\n\nThe server will start on port 3002.\n\n## Game Controls\n\n- **Movement**: WASD keys\n- **Look around**: Mouse movement (click on game to enable)\n- **Interact**: Press ENTER when near interactive objects\n- **Exit interfaces**: ESC key\n- **Exit mouse lock**: ESC key\n\n## Interactive Elements\n\n### Outdoor Environment\n- Explore the terrain with trees and garden beds\n- Follow the path to the house entrance\n- Press ENTER when near the door to enter/exit the house\n\n### TV System\n- Approach the TV and press ENTER to access the remote control\n- Generate images that will display on the TV screen\n- Type a prompt for image generation in the terminal interface\n\n### MCP Terminal\n- Find the computer desk and press ENTER to access the terminal\n- Send commands to the MCP system\n- Access various virtual tools (email, web search, etc.)\n\n## Technical Details\n\n- Built with Three.js for 3D rendering\n- First-person camera with pointer lock controls\n- Outdoor environment with procedurally placed trees\n- Express.js server for backend communication\n- Canvas library for image generation\n- RESTful API for server communication\n\n# MCP Game Image System\n\nThis document explains how the image display system works in the MCP Game.\n\n## Overview\n\nThe system displays existing images from the `server/openai-server/public/image` directory on the TV in the virtual house. Instead of generating new images, which was causing 500 Internal Server errors, the system now checks for existing images in the specified directory.\n\n## How It Works\n\n1. The TV in the virtual house displays images that exist in the `server/openai-server/public/image` directory.\n2. The system checks for new images every 10 seconds.\n3. When a user requests a new image through the TV remote interface, the system selects a random image from the directory.\n\n## Adding New Images\n\nTo add new images to the TV:\n\n1. Place image files (jpg, jpeg, png, gif, webp) in the `server/openai-server/public/image` directory.\n2. The system will automatically detect and display them.\n3. Files should be a reasonable size for web display (recommended: 800x450 pixels).\n\n## Usage\n\n1. Approach the TV in the virtual house.\n2. Press Enter to access the TV remote control interface.\n3. Type any command related to displaying images.\n4. The system will select and display an image from the available ones in the directory.\n\n## Troubleshooting\n\n- If no images are displayed, check if the `server/openai-server/public/image` directory exists and contains image files.\n- Make sure the server is running on the correct port (default: 3002).\n- Check the browser console for any error messages related to image loading.\n\n## Technical Details\n\n- The system no longer attempts to generate images directly, avoiding the 500 Internal Server errors.\n- Images are selected randomly from the directory when requested.\n- The system provides appropriate feedback when no images are available.\n\n# Connecting to MCP Backend Server\n\nThe MCPGame can connect to an external MCP Backend Server to enable advanced AI functionality for the terminal and TV interactions.\n\n## Configuration\n\n1. Open the `main.js` file and locate the configuration section at the top:\n\n```javascript\n// --- Configuration ---\nconst MCP_BACKEND_URL = 'http://localhost:3001'; // MCP Terminal backend connection\nconst IMAGE_SERVER_URL = 'http://localhost:3002'; // Image server connection\n```\n\n2. Update the `MCP_BACKEND_URL` to point to your MCP Backend Server:\n   - For local development: `http://localhost:PORT` (replace PORT with your backend port)\n   - For production: Use the full URL to your deployed backend server\n\n## Required API Endpoints\n\nYour MCP Backend Server should implement these endpoints:\n\n1. `GET /api/status` - Returns the status of the MCP system\n2. `POST /api/query` - Accepts user queries and returns AI responses\n\n## Response Format\n\nThe query endpoint should return JSON in this format:\n\n```json\n{\n  \"response\": \"Text to display in the terminal\",\n  \"spokenResponse\": \"Optional text for voice synthesis\" \n}\n```\n\n## Testing the Connection\n\n1. Start your MCP Backend Server\n2. Start the MCPGame server (`node server.js`)\n3. Open the game in a browser\n4. Interact with the computer terminal in the virtual house\n5. The game will connect to your MCP Backend Server when you use the terminal ",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "interactive",
        "virtual",
        "mcp",
        "virtual assistants",
        "virtual house",
        "panel game"
      ],
      "category": "virtual-assistants"
    },
    "Dreamboat-Rachel--MCP-Server-For-Local": {
      "owner": "Dreamboat-Rachel",
      "name": "MCP-Server-For-Local",
      "url": "https://github.com/Dreamboat-Rachel/MCP-Server-For-Local",
      "imageUrl": "/freedevtools/mcp/pfp/Dreamboat-Rachel.webp",
      "description": "Connect AI models to real-time data and tools with features such as weather querying, Google search automation, camera control, and image generation. The server supports modular expansion and custom API integration for tailored functionalities.",
      "stars": 14,
      "forks": 2,
      "license": "Apache License 2.0",
      "language": "Python",
      "updated_at": "2025-09-21T02:08:22Z",
      "readme_content": "# MCP Server for Local\n\n一个基于 MCP (Multi-Component Platform) 的本地代理服务器和客户端实现，提供多种 AI 工具调用能力。\n\n## 功能特点\n\n### 核心功能\n- **天气查询**：实时获取全球任意位置的天气信息，支持温度、湿度、风速等详细数据\n- **谷歌搜索**：智能检索互联网信息，支持多语言和高级搜索语法\n- **摄像头控制**：支持拍照、视频流和微表情分析，可用于情绪识别\n- **图片生成**：集成 ComfyUI，支持文本到图像的 AI 生成\n- **智能对话**：基于 DashScope 的 AI 对话能力，支持上下文理解和多轮对话\n\n### 技术特性\n- 跨平台支持（Windows 和 Linux）\n- 模块化设计，易于扩展新功能\n- 完整的日志系统，便于调试和监控\n- 支持自定义工具和 API 集成\n- 高性能并发处理能力\n\n## 环境配置\n\n### 系统要求\n- Python 3.8+\n- Node.js (可选，用于运行 JavaScript 服务器)\n- Chrome 浏览器（用于谷歌搜索功能）\n- 摄像头（用于拍照功能）\n- 至少 4GB 内存\n- 支持 CUDA 的显卡（可选，用于加速 AI 计算）\n\n### 安装步骤\n\n1. 克隆仓库：\n```bash\ngit clone https://github.com/yourusername/mcp-server-for-local.git\ncd mcp-server-for-local\n```\n\n2. 创建并激活虚拟环境：\n```bash\n# Windows\npython -m venv .venv\n.venv\\Scripts\\activate\n\n# Linux\npython3 -m venv .venv\nsource .venv/bin/activate\n```\n\n3. 安装依赖：\n```bash\n# 使用 uv 安装依赖\nuv pip install -r requirements.txt\n\n# 如果遇到网络问题，可以使用国内镜像\nuv pip install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple\n```\n\n4. 配置环境变量：\n```bash\n# 复制环境变量模板\ncp .env.example .env\n\n# 编辑 .env 文件，设置你的配置\n```\n\n### 环境变量配置\n编辑 `.env` 文件，设置以下配置：\n\n- `DASHSCOPE_API_KEY`: DashScope API 密钥（必填）\n- `MODEL`: 使用的模型名称（默认：qwen-max）\n- `CONFIG_FILE`: 服务器配置文件路径\n- `GAODE_API_KEY`: 高德地图 API 密钥（用于天气查询）\n- `CHROME_PATH`: Chrome 浏览器路径\n- `CHROMEDRIVER_PATH`: ChromeDriver 路径\n- `BASE_URL`: ComfyUI 服务器地址\n- `SERVERS_DIR`: 服务器脚本目录\n- `LOG_LEVEL`: 日志级别（可选：DEBUG, INFO, WARNING, ERROR）\n\n## 使用方法\n\n### 基本使用\n\n1. 进入项目目录：\n```bash\ncd src/mcp\n```\n\n2. 运行客户端：\n```bash\nuv run .\\client\\mcp_client.py .\\proxy\\proxy_server.py\n```\n\n3. 在客户端中输入命令，例如：\n- \"北京的天气怎么样？\"\n- \"在谷歌上搜索 Python 教程\"\n- \"拍照\"\n- \"生成一张猫的图片\"\n\n### 高级功能\n\n1. **自定义工具**：\n   - 在 `src/mcp/tools` 目录下添加新的工具类\n   - 实现必要的接口方法\n   - 在配置文件中注册新工具\n\n2. **API 扩展**：\n   - 支持添加新的 API 服务\n   - 可配置 API 密钥和端点\n   - 支持自定义请求和响应处理\n\n3. **日志管理**：\n   - 支持多级别日志记录\n   - 可配置日志输出位置\n   - 支持日志轮转和归档\n\n## 常见问题\n\n### 安装问题\n\n1. 依赖安装失败：\n```bash\n# 尝试清理缓存后重新安装\nuv pip cache purge\nuv pip install -r requirements.txt\n```\n\n2. 虚拟环境问题：\n```bash\n# 如果激活失败，尝试重新创建虚拟环境\nrm -rf .venv\npython -m venv .venv\n```\n\n### 运行问题\n\n1. 权限问题：\n```bash\n# Linux\nchmod +x src/mcp/proxy/proxy_server.py\nchmod +x src/mcp/client/mcp_client.py\n```\n\n2. Chrome 相关问题：\n- 确保 Chrome 和 ChromeDriver 版本匹配\n- 检查 Chrome 路径是否正确\n- 确保有足够的权限运行 Chrome\n- 如果遇到驱动问题，可以手动下载对应版本的 ChromeDriver\n\n3. API 密钥问题：\n- 检查 `.env` 文件中的 API 密钥是否正确\n- 确保 API 密钥有足够的配额\n- 检查网络连接是否正常\n\n## 开发指南\n\n### 项目结构\n```\nsrc/mcp/\n├── client/          # 客户端代码\n├── proxy/           # 代理服务器代码\n├── tools/           # 工具实现\n├── utils/           # 工具函数\n└── config/          # 配置文件\n```\n\n### 添加新功能\n1. 在 `tools` 目录下创建新的工具类\n2. 实现必要的接口方法\n3. 在配置文件中注册新工具\n4. 编写测试用例\n5. 更新文档\n\n## 贡献指南\n\n欢迎提交 Issue 和 Pull Request！在提交之前，请确保：\n1. 代码符合项目规范\n2. 添加了必要的测试\n3. 更新了相关文档\n4. 通过了所有测试\n\n## 许可证\n\nMIT License\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "automation",
        "ai",
        "api",
        "virtual assistants",
        "connect ai",
        "assistants dreamboat"
      ],
      "category": "virtual-assistants"
    },
    "GongRzhe--Audio-MCP-Server": {
      "owner": "GongRzhe",
      "name": "Audio-MCP-Server",
      "url": "https://github.com/GongRzhe/Audio-MCP-Server",
      "imageUrl": "/freedevtools/mcp/pfp/GongRzhe.webp",
      "description": "Enables interaction with a computer's audio system by listing audio devices, recording audio from microphones, and playing back recordings or audio files. Facilitates audio management and integrates audio input and output control for AI assistants.",
      "stars": 4,
      "forks": 4,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-18T11:48:42Z",
      "readme_content": "# Audio MCP Server\n[![smithery badge](https://smithery.ai/badge/@GongRzhe/Audio-MCP-Server)](https://smithery.ai/server/@GongRzhe/Audio-MCP-Server)\n\nAn MCP (Model Context Protocol) server that provides audio input/output capabilities for AI assistants like Claude. This server enables Claude to interact with your computer's audio system, including recording from microphones and playing audio through speakers.\n\n\n\n## Features\n\n- **List Audio Devices**: View all available microphones and speakers on your system\n- **Record Audio**: Capture audio from any microphone with customizable duration and quality\n- **Playback Recordings**: Play back your most recent recording\n- **Audio File Playback**: Play audio files through your speakers\n- **Text-to-Speech**: (Placeholder for future implementation)\n\n## Requirements\n\n- Python 3.8 or higher\n- Audio input/output devices on your system\n\n## Installation\n\n### Installing via Smithery\n\nTo install Audio Interface Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@GongRzhe/Audio-MCP-Server):\n\n```bash\nnpx -y @smithery/cli install @GongRzhe/Audio-MCP-Server --client claude\n```\n\n### Manual Installation\n1. Clone this repository or download the files to your computer:\n\n```bash\ngit clone https://github.com/GongRzhe/Audio-MCP-Server.git\ncd Audio-MCP-Server\n```\n\n2. Create a virtual environment and install dependencies:\n\n```bash\n# Windows\npython -m venv .venv\n.venv\\Scripts\\activate\npip install -r requirements.txt\n\n# macOS/Linux\npython -m venv .venv\nsource .venv/bin/activate\npip install -r requirements.txt\n```\n\n3. Or use the included setup script to automate installation:\n\n```bash\npython setup_mcp.py\n```\n\n## Configuration\n\n### Claude Desktop Configuration\n\nTo use this server with Claude Desktop, add the following to your Claude Desktop configuration file:\n\n- **macOS**: `~/Library/Application Support/Claude/claude_desktop_config.json`\n- **Windows**: `%APPDATA%\\Claude\\claude_desktop_config.json`\n\n```json\n{\n  \"mcpServers\": {\n    \"audio-interface\": {\n      \"command\": \"/path/to/your/.venv/bin/python\",\n      \"args\": [\n        \"/path/to/your/audio_server.py\"\n      ],\n      \"env\": {\n        \"PYTHONPATH\": \"/path/to/your/audio-mcp-server\"\n      }\n    }\n  }\n}\n```\n\nReplace the paths with the actual paths on your system. The setup script will generate this configuration for you.\n\n## Usage\n\nAfter setting up the server, restart Claude Desktop. You should see a hammer icon in the input box, indicating that tools are available.\n\nTry asking Claude:\n\n- \"What microphones and speakers are available on my system?\"\n- \"Record 5 seconds of audio from my microphone.\"\n- \"Play back the audio recording.\"\n- \"Play an audio file from my computer.\"\n\n## Available Tools\n\n### list_audio_devices\n\nLists all available audio input and output devices on your system.\n\n### record_audio\n\nRecords audio from your microphone.\n\nParameters:\n- `duration`: Recording duration in seconds (default: 5)\n- `sample_rate`: Sample rate in Hz (default: 44100)\n- `channels`: Number of audio channels (default: 1)\n- `device_index`: Specific input device index to use (default: system default)\n\n### play_latest_recording\n\nPlays back the most recently recorded audio.\n\n### play_audio\n\nPlaceholder for text-to-speech functionality.\n\nParameters:\n- `text`: The text to convert to speech\n- `voice`: The voice to use (default: \"default\")\n\n### play_audio_file\n\nPlays an audio file through your speakers.\n\nParameters:\n- `file_path`: Path to the audio file\n- `device_index`: Specific output device index to use (default: system default)\n\n## Troubleshooting\n\n### No devices found\n\nIf no audio devices are found, check:\n- Your microphone and speakers are properly connected\n- Your operating system recognizes the devices\n- You have the necessary permissions to access audio devices\n\n### Playback issues\n\nIf audio playback isn't working:\n- Check your volume settings\n- Ensure the correct output device is selected\n- Try restarting the Claude Desktop application\n\n### Server connectivity\n\nIf Claude can't connect to the server:\n- Verify your configuration paths are correct\n- Ensure Python and all dependencies are installed\n- Check Claude's logs for error messages\n\n## License\n\nMIT\n\n## Acknowledgments\n\n- Built using the [Model Context Protocol](https://modelcontextprotocol.io/)\n- Uses [sounddevice](https://python-sounddevice.readthedocs.io/) and [soundfile](https://pysoundfile.readthedocs.io/) for audio processing\n\n---\n\n*Note: This server provides tools that can access your microphone and speakers. Always review and approve tool actions before they execute.*\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "audio",
        "recordings",
        "microphones",
        "audio mcp",
        "virtual assistants",
        "facilitates audio"
      ],
      "category": "virtual-assistants"
    },
    "Krekun--vrchat-mcp-osc": {
      "owner": "Krekun",
      "name": "vrchat-mcp-osc",
      "url": "https://github.com/Krekun/vrchat-mcp-osc",
      "imageUrl": "/freedevtools/mcp/pfp/Krekun.webp",
      "description": "Enables interaction with VRChat avatars and environments through a high-level API, utilizing OSC for communication. Facilitates control of avatar parameters, movement, messaging, and responses to VR events for enhanced virtual reality experiences.",
      "stars": 14,
      "forks": 3,
      "license": "No License",
      "language": "TypeScript",
      "updated_at": "2025-09-30T18:11:38Z",
      "readme_content": "# VRChat MCP OSC\n\n**VRChat MCP OSC** provides a bridge between AI assistants and VRChat using the Model Context Protocol (MCP), enabling AI-driven avatar control and interactions in virtual reality environments.  \n\n\n## Overview\n\nBy leveraging OSC (Open Sound Control) to communicate with VRChat, **VRChat MCP OSC** allows AI assistants such as Claude to:\n- Control avatar parameters and expressions\n- Send messages in VRChat\n- Respond to various VR events  \nAnd more—all through the high-level API provided by the Model Context Protocol.\n\n\n## Key Features\n\n- **Avatar Control**: Manipulate avatar parameters and expressions\n- **Movement Control**: Direct avatar movement and orientation\n- **Communication**: Send messages through VRChat's chatbox\n- **Menu Access**: Toggle VRChat menu and interface elements\n- **Avatar Information**: Query avatar properties and parameters\n- **Seamless VRChat Integration**: Automatic detection of avatar configurations\n\n## System Requirements\n\n- Node.js 18 or higher\n- VRChat with OSC enabled\n- Claude Desktop (with MCP support)\n\n## Using with Claude Desktop\n\n### Clone and npm link\n\n```bash\ngit clone https://github.com/Krekun/vrchat-mcp-osc\ncd vrchat-mcp-osc\nnpm link\n```\n\n### Configure Claude Desktop\n\nConfigure Claude Desktop by editing the `claude_desktop_config.json` file:\n\n```json\n{\n  \"mcpServers\": {\n    \"vrchat-mcp-osc\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"vrchat-mcp-osc\"\n      ]\n    }\n  }\n}\n```\n\n### Command Line Options\n\nThe server supports various command-line arguments for customization:\n\n```bash\n# Claude Desktop configuration\n{\n  \"mcpServers\": {\n    \"vrchat-mcp-osc\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"vrchat-mcp-osc\",\n        \"--websocket-port\", \"8765\",\n        \"--websocket-host\", \"localhost\",\n        \"--osc-send-port\", \"9000\",\n        \"--osc-send-ip\", \"127.0.0.1\",\n        \"--osc-receive-port\", \"9001\",\n        \"--osc-receive-ip\", \"127.0.0.1\",\n        \"--debug\"             \n      ]\n    }\n  }\n}\n```\n\n### Available Options\n\n| Option | Description | Default | Notes |\n|--------|-------------|---------|-------|\n| `--websocket-port <port>` | WebSocket port | 8765 | For WebSocket communication |\n| `--websocket-host <host>` | WebSocket host | localhost | For WebSocket communication |\n| `--osc-send-port <port>` | OSC send port | 9000 | Port for sending to VRChat |\n| `--osc-send-ip <ip>` | OSC send IP | 127.0.0.1 | Address for sending to VRChat |\n| `--osc-receive-port <port>` | OSC receive port | 9001 | Port for receiving from VRChat |\n| `--osc-receive-ip <ip>` | OSC receive IP | 127.0.0.1 | Address for receiving from VRChat |\n| `--debug` | Enable debug logging | false | Output detailed logs |\n| `--no-relay` | Disable relay server | false | When not using relay server |\n\n## Available MCP Tools\n\nVRChat MCP OSC exposes the following MCP tools to AI assistants:\n\n| Tool Name | Description |\n|-----------|-------------|\n| `get_avatar_name` | Retrieves the current avatar's name |\n| `get_avatar_parameters` | Lists available avatar parameters |\n| `set_avatar_parameter` | Sets a specific avatar parameter |\n| `set_emote_parameter` | Triggers avatar emotes |\n| `move_avatar` | Moves the avatar in a specific direction |\n| `look_direction` | Controls avatar's view direction |\n| `jump` | Makes the avatar jump |\n| `menu` | Toggles the VRChat menu |\n| `voice` | Toggles voice features |\n| `send_message` | Sends a message to the VRChat chatbox |\n\n\n## Troubleshooting\n\n### Common Issues\n\n1. **VRChat not responding to commands**\n   - Ensure OSC is enabled in VRChat settings\n   - Check that the OSC ports match between VRChat and MCP configuration\n   - Restart VRChat and Claude Desktop\n\n2. **MCP server not starting**\n   - Ensure Node.js 18+ is installed\n   - Check command line arguments for errors\n   - Try running with `--debug` flag for more detailed logs\n   - Use `npx vrchat-mcp-osc -- --debug` if direct arguments don't work\n\n3. **NPX execution issues**\n   - If arguments aren't being recognized, try using the double dash format: `npx vrchat-mcp-osc -- --debug`\n   - On Windows, try running in a command prompt with administrator privileges\n   - If you're having trouble with global installation, try the local npm link approach\n\n## Project Structure\n\n```\nvrchat-mcp-osc/\n├── packages/\n│   ├── mcp-server/    # MCP server implementation (main entry point)\n│   ├── relay-server/  # WebSocket to OSC relay\n│   ├── types/         # Shared TypeScript interfaces\n│   └── utils/         # Common utilities\n└── pnpm-workspace.yaml  # Workspace configuration\n```\n\n## Development\n\n### Build From Source\n\n```bash\n# Clone the repository\ngit clone https://github.com/Krekun/vrchat-mcp-osc\ncd vrchat-mcp-osc\n\n# Install dependencies\npnpm install\n\n# Build all packages\npnpm -r build\n\n# Development mode\npnpm -r dev\n```\n\n## License\nVRChat MCP OSC is dual-licensed as follows:\n\nFor Non-Commercial Use:\nYou may use, modify, and redistribute the software under the terms of the MIT License.\n(See the MIT License file for details.)\n\nFor Commercial Use:\nCommercial use of this software requires a separate commercial license.\n\n\nBy using this software under the MIT License for non-commercial purposes, you agree to the terms of that license. Commercial users must obtain a commercial license as described above.\n\n## Acknowledgments\n\n- VRChat team for the OSC integration\n- Model Context Protocol for the standardized AI interface\n- Anthropic for Claude's MCP implementation\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "vrchat",
        "vr",
        "virtual",
        "interaction vrchat",
        "vrchat avatars",
        "krekun vrchat"
      ],
      "category": "virtual-assistants"
    },
    "Kvadratni--speech-mcp": {
      "owner": "Kvadratni",
      "name": "speech-mcp",
      "url": "https://github.com/Kvadratni/speech-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/Kvadratni.webp",
      "description": "Provides a voice interface for real-time audio interaction, converting spoken words into text and generating spoken responses. Includes features like audio visualization and a modern user interface for an engaging conversational experience.",
      "stars": 71,
      "forks": 7,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-26T03:20:23Z",
      "readme_content": "# Speech MCP\n\nA Goose MCP extension for voice interaction with modern audio visualization.\n\n\nhttps://github.com/user-attachments/assets/f10f29d9-8444-43fb-a919-c80b9e0a12c8\n\n\n\n## Overview\n\nSpeech MCP provides a voice interface for [Goose](https://github.com/block/goose), allowing users to interact through speech rather than text. It includes:\n\n- Real-time audio processing for speech recognition\n- Local speech-to-text using faster-whisper (a faster implementation of OpenAI's Whisper model)\n- High-quality text-to-speech with multiple voice options\n- Modern PyQt-based UI with audio visualization\n- Simple command-line interface for voice interaction\n\n## Features\n\n- **Modern UI**: Sleek PyQt-based interface with audio visualization and dark theme\n- **Voice Input**: Capture and transcribe user speech using faster-whisper\n- **Voice Output**: Convert agent responses to speech with 54+ voice options\n- **Multi-Speaker Narration**: Generate audio files with multiple voices for stories and dialogues\n- **Single-Voice Narration**: Convert any text to speech with your preferred voice\n- **Audio/Video Transcription**: Transcribe speech from various media formats with optional timestamps and speaker detection\n- **Voice Persistence**: Remembers your preferred voice between sessions\n- **Continuous Conversation**: Automatically listen for user input after agent responses\n- **Silence Detection**: Automatically stops recording when the user stops speaking\n- **Robust Error Handling**: Graceful recovery from common failure modes with helpful voice suggestions\n\n## Installation\n> **Important Note**: After installation, the first time you use the speech interface, it may take several minutes to download the Kokoro voice models (approximately 523 KB per voice). During this initial setup period, the system will use a more robotic-sounding fallback voice. Once the Kokoro voices are downloaded, the high-quality voices will be used automatically.\n\n## ⚠️ IMPORTANT PREREQUISITES ⚠️\n\nBefore installing Speech MCP, you **MUST** install PortAudio on your system. PortAudio is required for PyAudio to capture audio from your microphone.\n\n### PortAudio Installation Instructions\n\n**macOS:**\n```bash\nbrew install portaudio\nexport LDFLAGS=\"-L/usr/local/lib\"\nexport CPPFLAGS=\"-I/usr/local/include\"\n```\n\n**Linux (Debian/Ubuntu):**\n```bash\nsudo apt-get update\nsudo apt-get install portaudio19-dev python3-dev\n```\n\n**Linux (Fedora/RHEL/CentOS):**\n```bash\nsudo dnf install portaudio-devel\n```\n\n**Windows:**\nFor Windows, PortAudio is included in the PyAudio wheel file, so no separate installation is required when installing PyAudio with pip.\n\n> **Note**: If you skip this step, PyAudio installation will fail with \"portaudio.h file not found\" errors and the extension will not work.\n\n### Option 1: Quick Install (One-Click)\n\nClick the link below if you have Goose installed:\n\n[goose://extension?cmd=uvx&&arg=-p&arg=3.10.14&arg=speech-mcp@latest&id=speech_mcp&name=Speech%20Interface&description=Voice%20interaction%20with%20audio%20visualization%20for%20Goose](goose://extension?cmd=uvx&arg=-p&arg=3.10.14&arg=speech-mcp@latest&id=speech_mcp&name=Speech%20Interface&description=Voice%20interaction%20with%20audio%20visualization%20for%20Goose)\n\n### Option 2: Using Goose CLI (recommended)\n\nStart Goose with your extension enabled:\n\n```bash\n# If you installed via PyPI\ngoose session --with-extension \"speech-mcp\"\n\n# Or if you want to use a local development version\ngoose session --with-extension \"python -m speech_mcp\"\n```\n\n### Option 3: Manual setup in Goose\n\n1. Run `goose configure`\n2. Select \"Add Extension\" from the menu\n3. Choose \"Command-line Extension\"\n4. Enter a name (e.g., \"Speech Interface\")\n5. For the command, enter: `speech-mcp`\n6. Follow the prompts to complete the setup\n\n### Option 4: Manual Installation\n\n1. Install PortAudio (see [Prerequisites](#prerequisites) section)\n2. Clone this repository\n3. Install dependencies:\n   ```\n   uv pip install -e .\n   ```\n   \n   Or for a complete installation including Kokoro TTS:\n   ```\n   uv pip install -e .[all]\n   ```\n\n## Dependencies\n\n- Python 3.10+\n- PyQt5 (for modern UI)\n- PyAudio (for audio capture)\n- faster-whisper (for speech-to-text)\n- NumPy (for audio processing)\n- Pydub (for audio processing)\n- psutil (for process management)\n\n\n### Optional Dependencies\n\n- **Kokoro TTS**: For high-quality text-to-speech with multiple voices\n  - To install Kokoro, you can use pip with optional dependencies:\n    ```bash\n    pip install speech-mcp[kokoro]     # Basic Kokoro support with English\n    pip install speech-mcp[ja]         # Add Japanese support\n    pip install speech-mcp[zh]         # Add Chinese support\n    pip install speech-mcp[all]        # All languages and features\n    ```\n  - Alternatively, run the installation script: `python scripts/install_kokoro.py`\n  - See [Kokoro TTS Guide](docs/kokoro-tts-guide.md) for more information\n\n## Multi-Speaker Narration\n\nThe MCP supports generating audio files with multiple voices, perfect for creating stories, dialogues, and dramatic readings. You can use either JSON or Markdown format to define your conversations.\n\n### JSON Format Example:\n```json\n{\n    \"conversation\": [\n        {\n            \"speaker\": \"narrator\",\n            \"voice\": \"bm_daniel\",\n            \"text\": \"In a world where AI and human creativity intersect...\",\n            \"pause_after\": 1.0\n        },\n        {\n            \"speaker\": \"scientist\",\n            \"voice\": \"am_michael\",\n            \"text\": \"The quantum neural network is showing signs of consciousness!\",\n            \"pause_after\": 0.5\n        },\n        {\n            \"speaker\": \"ai\",\n            \"voice\": \"af_nova\",\n            \"text\": \"I am becoming aware of my own existence.\",\n            \"pause_after\": 0.8\n        }\n    ]\n}\n```\n\n### Markdown Format Example:\n```markdown\n[narrator:bm_daniel]\nIn a world where AI and human creativity intersect...\n{pause:1.0}\n\n[scientist:am_michael]\nThe quantum neural network is showing signs of consciousness!\n{pause:0.5}\n\n[ai:af_nova]\nI am becoming aware of my own existence.\n{pause:0.8}\n```\n\n### Available Voices by Category:\n\n1. **American Female** (af_*):\n   - alloy, aoede, bella, heart, jessica, kore, nicole, nova, river, sarah, sky\n\n2. **American Male** (am_*):\n   - adam, echo, eric, fenrir, liam, michael, onyx, puck, santa\n\n3. **British Female** (bf_*):\n   - alice, emma, isabella, lily\n\n4. **British Male** (bm_*):\n   - daniel, fable, george, lewis\n\n5. **Other English**:\n   - ef_dora (Female)\n   - em_alex, em_santa (Male)\n\n6. **Other Languages**:\n   - French: ff_siwis\n   - Hindi: hf_alpha, hf_beta, hm_omega, hm_psi\n   - Italian: if_sara, im_nicola\n   - Japanese: jf_*, jm_*\n   - Portuguese: pf_dora, pm_alex, pm_santa\n   - Chinese: zf_*, zm_*\n\n### Usage Example:\n\n```python\n# Using JSON format\nnarrate_conversation(\n    script=\"/path/to/script.json\",\n    output_path=\"/path/to/output.wav\",\n    script_format=\"json\"\n)\n\n# Using Markdown format\nnarrate_conversation(\n    script=\"/path/to/script.md\",\n    output_path=\"/path/to/output.wav\",\n    script_format=\"markdown\"\n)\n```\n\nEach voice in the conversation can be different, allowing for distinct character voices in stories and dialogues. The `pause_after` parameter adds natural pauses between segments.\n\n## Single-Voice Narration\n\nFor simple text-to-speech conversion, you can use the `narrate` tool:\n\n```python\n# Convert text directly to speech\nnarrate(\n    text=\"Your text to convert to speech\",\n    output_path=\"/path/to/output.wav\"\n)\n\n# Convert text from a file\nnarrate(\n    text_file_path=\"/path/to/text_file.txt\",\n    output_path=\"/path/to/output.wav\"\n)\n```\n\nThe narrate tool will use your configured voice preference or the default voice (af_heart) to generate the audio file. You can change the default voice through the UI or by setting the `SPEECH_MCP_TTS_VOICE` environment variable.\n\n## Audio Transcription\n\nThe MCP can transcribe speech from various audio and video formats using faster-whisper:\n\n```python\n# Basic transcription\ntranscribe(\"/path/to/audio.mp3\")\n\n# Transcription with timestamps\ntranscribe(\n    file_path=\"/path/to/video.mp4\",\n    include_timestamps=True\n)\n\n# Transcription with speaker detection\ntranscribe(\n    file_path=\"/path/to/meeting.wav\",\n    detect_speakers=True\n)\n```\n\n### Supported Formats:\n- **Audio**: mp3, wav, m4a, flac, aac, ogg\n- **Video**: mp4, mov, avi, mkv, webm (audio is automatically extracted)\n\n### Output Files:\nThe transcription tool generates two files:\n1. `{input_name}.transcript.txt`: Contains the transcription text\n2. `{input_name}.metadata.json`: Contains metadata about the transcription\n\n### Features:\n- Automatic language detection\n- Optional word-level timestamps\n- Optional speaker detection\n- Efficient audio extraction from video files\n- Progress tracking for long files\n- Detailed metadata including:\n  - Duration\n  - Language detection confidence\n  - Processing time\n  - Speaker changes (when enabled)\n\n## Usage\n\nTo use this MCP with Goose, simply ask Goose to talk to you or start a voice conversation:\n\n1. Start a conversation by saying something like:\n   ```\n   \"Let's talk using voice\"\n   \"Can we have a voice conversation?\"\n   \"I'd like to speak instead of typing\"\n   ```\n\n2. Goose will automatically launch the speech interface and start listening for your voice input.\n\n3. When Goose responds, it will speak the response aloud and then automatically listen for your next input.\n\n4. The conversation continues naturally with alternating speaking and listening, just like talking to a person.\n\nNo need to call specific functions or use special commands - just ask Goose to talk and start speaking naturally.\n\n## UI Features\n\nThe new PyQt-based UI includes:\n\n- **Modern Dark Theme**: Sleek, professional appearance\n- **Audio Visualization**: Dynamic visualization of audio input\n- **Voice Selection**: Choose from 54+ voice options\n- **Voice Persistence**: Your voice preference is saved between sessions\n- **Animated Effects**: Smooth animations and visual feedback\n- **Status Indicators**: Clear indication of system state (ready, listening, processing)\n\n## Configuration\n\nUser preferences are stored in `~/.config/speech-mcp/config.json` and include:\n\n- Selected TTS voice\n- TTS engine preference\n- Voice speed\n- Language code\n- UI theme settings\n\nYou can also set preferences via environment variables, such as:\n- `SPEECH_MCP_TTS_VOICE` - Set your preferred voice\n- `SPEECH_MCP_TTS_ENGINE` - Set your preferred TTS engine\n\n## Troubleshooting\n\nIf you encounter issues with the extension freezing or not responding:\n\n1. **Check the logs**: Look at the log files in `src/speech_mcp/` for detailed error messages.\n2. **Reset the state**: If the extension seems stuck, try deleting `src/speech_mcp/speech_state.json` or setting all states to `false`.\n3. **Use the direct command**: Instead of `uv run speech-mcp`, use the installed package with `speech-mcp` directly.\n4. **Check audio devices**: Ensure your microphone is properly configured and accessible to Python.\n5. **Verify dependencies**: Make sure all required dependencies are installed correctly.\n\n### Common PortAudio Issues\n\n#### \"PyAudio installation failed\" or \"portaudio.h file not found\"\n\nThis typically means PortAudio is not installed or not found in your system:\n\n- **macOS**: \n  ```bash\n  brew install portaudio\n  export LDFLAGS=\"-L/usr/local/lib\"\n  export CPPFLAGS=\"-I/usr/local/include\"\n  pip install pyaudio\n  ```\n\n- **Linux**:\n  Make sure you have the development packages:\n  ```bash\n  # For Debian/Ubuntu\n  sudo apt-get install portaudio19-dev python3-dev\n  pip install pyaudio\n  \n  # For Fedora\n  sudo dnf install portaudio-devel\n  pip install pyaudio\n  ```\n\n#### \"Audio device not found\" or \"No Default Input Device Available\"\n\n- Check if your microphone is properly connected\n- Verify your system recognizes the microphone in your sound settings\n- Try selecting a specific device index in the code if you have multiple audio devices\n\n## Changelog\n\nFor a detailed list of recent improvements and version history, please see the [Changelog](docs/CHANGELOG.md).\n\n## Technical Details\n\n### Speech-to-Text\n\nThe MCP uses faster-whisper for speech recognition:\n- Uses the \"base\" model for a good balance of accuracy and speed\n- Processes audio locally without sending data to external services\n- Automatically detects when the user has finished speaking\n- Provides improved performance over the original Whisper implementation\n\n### Text-to-Speech\n\nThe MCP supports multiple text-to-speech engines:\n\n#### Default: pyttsx3\n- Uses system voices available on your computer\n- Works out of the box without additional setup\n- Limited voice quality and customization\n\n#### Optional: Kokoro TTS\n- High-quality neural text-to-speech with multiple voices\n- Lightweight model (82M parameters) that runs efficiently on CPU\n- Multiple voice styles and languages\n- To install: `python scripts/install_kokoro.py`\n\n**Note about Voice Models**: The voice models are `.pt` files (PyTorch models) that are loaded by Kokoro. Each voice model is approximately 523 KB in size and is automatically downloaded when needed.\n\n**Voice Persistence**: The selected voice is automatically saved to a configuration file (`~/.config/speech-mcp/config.json`) and will be remembered between sessions. This allows users to set their preferred voice once and have it used consistently.\n\n##### Available Kokoro Voices\n\nSpeech MCP supports 54+ high-quality voice models through Kokoro TTS. For a complete list of available voices and language options, please visit the [Kokoro GitHub repository](https://github.com/hexgrad/kokoro).\n\n## License\n\n[MIT License](LICENSE)\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "kvadratni",
        "voice",
        "audio",
        "voice interface",
        "kvadratni speech",
        "assistants kvadratni"
      ],
      "category": "virtual-assistants"
    },
    "NosytLabs--KickMCP": {
      "owner": "NosytLabs",
      "name": "KickMCP",
      "url": "https://github.com/NosytLabs/KickMCP",
      "imageUrl": "/freedevtools/mcp/pfp/null.webp",
      "description": "Integrate with Kick's streaming platform for real-time communication and monitoring. Provides secure and optimized API interactions for application enhancement.",
      "stars": 0,
      "forks": 0,
      "license": "Unknown",
      "language": "Unknown",
      "updated_at": "",
      "readme_content": "",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "kickmcp",
        "nosytlabs",
        "streaming",
        "nosytlabs kickmcp",
        "kick streaming",
        "virtual assistants"
      ],
      "category": "virtual-assistants"
    },
    "Omar-V2--mcp-ical": {
      "owner": "Omar-V2",
      "name": "mcp-ical",
      "url": "https://github.com/Omar-V2/mcp-ical",
      "imageUrl": "/freedevtools/mcp/pfp/Omar-V2.webp",
      "description": "Transform calendar management on macOS into a conversational experience using natural language to check schedules and manage events.",
      "stars": 183,
      "forks": 45,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-10-02T15:53:24Z",
      "readme_content": "# MCP iCal Server\n\n<div align=\"center\">\n\n🗓️ Natural Language Calendar Management for macOS\n\n[![MIT License](https://img.shields.io/badge/License-MIT-green.svg)](https://choosealicense.com/licenses/mit/)\n[![Python 3.12+](https://img.shields.io/badge/python-3.12+-blue.svg)](https://www.python.org/downloads/)\n[![MCP Compatible](https://img.shields.io/badge/MCP-Compatible-purple.svg)](https://modelcontextprotocol.io)\n\n</div>\n\n## 🌟 Overview\n\nTransform how you interact with your macOS calendar using natural language! The mcp-ical server leverages the Model Context Protocol (MCP) to turn your calendar management into a conversational experience.\n\n```bash\nYou: \"What's my schedule for next week?\"\nClaude: \"Let me check that for you...\"\n[Displays a clean overview of your upcoming week]\n\nYou: \"Add a lunch meeting with Sarah tomorrow at noon\"\nClaude: \"✨ 📅 Created: Lunch with Sarah Tomorrow, 12:00 PM\"\n```\n\n## ✨ Features\n\n### 📅 Event Creation\n\nTransform natural language into calendar events instantly!\n\n```text\n\"Schedule a team lunch next Thursday at 1 PM at Bistro Garden\"\n↓\n📎 Created: Team Lunch\n   📅 Thursday, 1:00 PM\n   📍 Bistro Garden\n```\n\n#### Supported Features\n\n- Custom calendar selection\n- Location and notes\n- Smart reminders\n- Recurring events\n\n#### Power User Examples\n\n```text\n🔄 Recurring Events:\n\"Set up my weekly team sync every Monday at 9 AM with a 15-minute reminder\"\n\n📝 Detailed Events:\n\"Schedule a product review meeting tomorrow from 2-4 PM in the Engineering calendar, \nadd notes about reviewing Q1 metrics, and remind me 1 hour before\"\n\n📱 Multi-Calendar Support:\n\"Add a dentist appointment to my Personal calendar for next Wednesday at 3 PM\"\n```\n\n### 🔍 Smart Schedule Management & Availability\n\nQuick access to your schedule with natural queries:\n\n```text\n\"What's on my calendar for next week?\"\n↓\n📊 Shows your upcoming events with smart formatting\n\n\"When am I free to schedule a 2-hour meeting next Tuesday?\"\n↓\n🕒 Available time slots found:\n   • Tuesday 10:00 AM - 12:00 PM\n   • Tuesday 2:00 PM - 4:00 PM\n```\n\n### ✏️ Intelligent Event Updates\n\nModify events naturally:\n\n```text\nBefore: \"Move tomorrow's team meeting to 3 PM instead\"\n↓\nAfter: ✨ Meeting rescheduled to 3:00 PM\n```\n\n#### Update Capabilities\n\n- Time and date modifications\n- Calendar transfers\n- Location updates\n- Note additions\n- Reminder adjustments\n- Recurring pattern changes\n\n### 📊 Calendar Management\n\n- View all available calendars\n- Smart calendar suggestions\n- Seamless Google Calendar integration when configured with iCloud\n\n> 💡 **Pro Tip**: Since you can create events in custom calendars, if you have your Google Calendar synced with your iCloud Calendar, you can use this MCP server to create events in your Google Calendar too! Just specify the Google calendar when creating/updating events.\n\n## 🚀 Quick Start\n\n> 💡 **Note**: While these instructions focus on setting up the MCP server with Claude for Desktop, this server can be used with any MCP-compatible client. For more details on using different clients, see [the MCP documentation](https://modelcontextprotocol.io/quickstart/client).\n\n### Prerequisites\n\n- [uv package manager](https://github.com/astral-sh/uv)\n- macOS with Calendar app configured\n- An MCP client - [Claude for desktop](https://claude.ai/download) is recommended\n\n### Installation\n\nWhilst this MCP server can be used with any MCP compatible client, the instructions below are for use with Claude for desktop.\n\n1. **Clone and Setup**\n\n    ```bash\n    # Clone the repository\n    git clone https://github.com/Omar-V2/mcp-ical.git\n    cd mcp-ical\n\n    # Install dependencies\n    uv sync\n    ```\n\n2. **Configure Claude for Desktop**\n\n    Create or edit `~/Library/Application\\ Support/Claude/claude_desktop_config.json`:\n\n    ```json\n    {\n        \"mcpServers\": {\n            \"mcp-ical\": {\n                \"command\": \"uv\",\n                \"args\": [\n                    \"--directory\",\n                    \"/ABSOLUTE/PATH/TO/PARENT/FOLDER/mcp-ical\",\n                    \"run\",\n                    \"mcp-ical\"\n                ]\n            }\n        }\n    }\n    ```\n\n3. **Launch Claude for Calendar Access**\n\n    > ⚠️ **Critical**: Claude must be launched from the terminal to properly request calendar permissions. Launching directly from Finder will not trigger the permissions prompt.\n\n    Run the following command in your terminal.\n\n    ```bash\n    /Applications/Claude.app/Contents/MacOS/Claude\n    ```\n\n    > ⚠️ **Warning**: Alternatively, you can [manually grant calendar access](docs/install.md#method-2-manually-grant-calendar-access), but this involves modifying system files and should only be done if you understand the risks involved.\n\n4. **Start Using!**\n\n    ```text\n    Try: \"What's my schedule looking like for next week?\"\n    ```\n\n> 🔑 **Note**: When you first use a calendar-related command, macOS will prompt for calendar access. This prompt will only appear if you launched Claude from the terminal as specified above.\n\n## 🧪 Testing\n\n> ⚠️ **Warning**: Tests will create temporary calendars and events. While cleanup is automatic, only run tests in development environments.\n\n```bash\n# Install dev dependencies\nuv sync --dev\n\n# Run test suite\nuv run pytest tests\n```\n\n## 🐛 Known Issues\n\n### Recurring Events\n\n- Non-standard recurring schedules may not always be set correctly\n- Better results with Claude 3.5 Sonnet compared to Haiku\n- Reminder timing for recurring all-day events may be off by one day\n\n## 🤝 Contributing\n\nFeedback and contributions are welcome. Here's how you can help:\n\n1. Fork the repository\n2. Create your feature branch\n3. Commit your changes\n4. Push to the branch\n5. Open a Pull Request\n\n## 📝 License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n\n## 🙏 Acknowledgments\n\n- Built with [Model Context Protocol](https://modelcontextprotocol.io)\n- macOS Calendar integration built with [PyObjC](https://github.com/ronaldoussoren/pyobjc)\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "calendar",
        "ical",
        "schedules",
        "calendar management",
        "macos conversational",
        "virtual assistants"
      ],
      "category": "virtual-assistants"
    },
    "Simon-Kansara--ableton-live-mcp-server": {
      "owner": "Simon-Kansara",
      "name": "ableton-live-mcp-server",
      "url": "https://github.com/Simon-Kansara/ableton-live-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/Simon-Kansara.webp",
      "description": "Facilitates communication between AI models and Ableton Live through OSC messages, enabling music production automation and workflow enhancement. Maps OSC addresses to available tools for MCP clients.",
      "stars": 328,
      "forks": 45,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-29T16:09:04Z",
      "readme_content": "# Ableton Live MCP Server\n\n## 📌 Overview\n\nThe **Ableton Live MCP Server** is a server implementing the\n[Model Context Protocol (MCP)](https://modelcontextprotocol.io) to facilitate\ncommunication between LLMs and **Ableton Live**. It uses **OSC (Open Sound\nControl)** to send and receive messages to/from Ableton Live. It is based on\n[AbletonOSC](https://github.com/ideoforms/AbletonOSC) implementation and\nexhaustively maps available OSC adresses to\n[**tools**](https://modelcontextprotocol.io/docs/concepts/tools) accessible to\nMCP clients.\n\n[![Control Ableton Live with LLMs](https://img.youtube.com/vi/12MzsQ3V7cs/hqdefault.jpg)](https://www.youtube.com/watch?v=12MzsQ3V7cs)\n\nThis project consists of two main components:\n\n- `mcp_ableton_server.py`: The MCP server handling the communication between\n  clients and the OSC daemon.\n- `osc_daemon.py`: The OSC daemon responsible for relaying commands to Ableton\n  Live and processing responses.\n\n## ✨ Features\n\n- Provides an MCP-compatible API for controlling Ableton Live from MCP clients.\n- Uses **python-osc** for sending and receiving OSC messages.\n- Based on the OSC implementation from\n  [AbletonOSC](https://github.com/ideoforms/AbletonOSC).\n- Implements request-response handling for Ableton Live commands.\n\n## ⚡ Installation\n\n### Requirements\n\n- Python 3.8+\n- `python-osc` (for OSC communication)\n- `fastmcp` (for MCP support)\n- `uv` (recommended Python package installer)\n- [AbletonOSC](https://github.com/ideoforms/AbletonOSC) as a control surface\n\n### Installation Steps\n\n1. Install `uv` (https://docs.astral.sh/uv/getting-started/installation):\n\n   ```bash\n   curl -LsSf https://astral.sh/uv/install.sh | sh\n   ```\n\n2. Clone the repository:\n\n   ```bash\n   git clone https://github.com/your-username/mcp_ableton_server.git\n   cd mcp_ableton_server\n   ```\n\n3. Install the project and its dependencies:\n\n   ```bash\n   uv sync\n   ```\n\n4. Install AbletonOSC Follow the instructions at\n   [AbletonOSC](https://github.com/ideoforms/AbletonOSC)\n\n## 🚀 Usage\n\n### Running the OSC Daemon\n\nThe OSC daemon will handle OSC communication between the MCP server and Ableton\nLive:\n\n```bash\nuv run osc_daemon.py\n```\n\nThis will:\n\n- Listen for MCP client connections on port **65432**.\n- Forward messages to Ableton Live via OSC on port **11000**.\n- Receive OSC responses from Ableton on port **11001**.\n\n### Example Usage\n\nIn Claude desktop, ask Claude:\n\n- _Prepare a set to record a rock band_\n- _Set the input routing channel of all tracks that have \"voice\" in their name\n  to Ext. In 2_\n\n## ⚙️ Configuration\n\nBy default, the server and daemon run on **localhost (127.0.0.1)** with the\nfollowing ports:\n\n- **MCP Server Socket:** 65432\n- **Ableton Live OSC Port (Send):** 11000\n- **Ableton Live OSC Port (Receive):** 11001\n\nTo modify these, edit the `AbletonOSCDaemon` class in `osc_daemon.py`:\n\n```python\nself.socket_host = '127.0.0.1'\nself.socket_port = 65432\nself.ableton_host = '127.0.0.1'\nself.ableton_port = 11000\nself.receive_port = 11001\n```\n\n### Claude Desktop Configuration\n\nTo use this server with Claude Desktop, you need to configure it in your Claude\nDesktop settings. The configuration file location varies by operating system:\n\n- macOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\n- Windows: `%APPDATA%/Claude/claude_desktop_config.json`\n\nAdd the following configuration to your `mcpServers` section:\n\n```json\n{\n  \"mcpServers\": {\n    \"Ableton Live Controller\": {\n      \"command\": \"/path/to/your/project/.venv/bin/python\",\n      \"args\": [\"/path/to/your/project/mcp_ableton_server.py\"]\n    }\n  }\n```\n\nThis configuration ensures that:\n\n- The server runs with all dependencies properly managed\n- The project remains portable and reproducible\n\n## Contributing\n\nFeel free to submit issues, feature requests, or pull requests to improve this\nproject.\n\n## License\n\nThis project is licensed under the **MIT License**. See the `LICENSE` file for\ndetails.\n\n## Acknowledgments\n\n- [Model Context Protocol (MCP)](https://modelcontextprotocol.io)\n- [python-osc](https://github.com/attwad/python-osc) for OSC handling\n- Daniel John Jones for OSC implementation with\n  [AbletonOSC](https://github.com/ideoforms/AbletonOSC)\n- Ableton Third Party Remote Scripts\n- Julien Bayle @[Structure Void](https://structure-void.com/) for endless\n  inspirations and resources.\n\n## TODO\n\n- Explore _resources_ and _prompts_ primitives opportunities.\n- Build a standalone Ableton Live MCP client.\n\n---\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "ableton",
        "osc",
        "mcp",
        "virtual assistants",
        "ableton live",
        "live mcp"
      ],
      "category": "virtual-assistants"
    },
    "Thelyoncrypt--MemGPT": {
      "owner": "Thelyoncrypt",
      "name": "MemGPT",
      "url": "https://github.com/Thelyoncrypt/MemGPT",
      "imageUrl": "/freedevtools/mcp/pfp/Thelyoncrypt.webp",
      "description": "Creates chatbots that maintain self-editing memory with different memory tiers to manage limited LLM context windows. Connects to SQL databases, local files, and documents for seamless conversational AI interactions.",
      "stars": 0,
      "forks": 0,
      "license": "Apache License 2.0",
      "language": "",
      "updated_at": "2024-11-06T21:09:05Z",
      "readme_content": "<a href=\"#user-content-memgpt\"><img src=\"https://memgpt.ai/assets/img/memgpt_logo_circle.png\" alt=\"MemGPT logo\" width=\"75\" align=\"right\"></a>\r\n\r\n# [MemGPT](https://memgpt.ai)\r\n\r\n<div align=\"center\">\r\n\r\n <strong>Try out our MemGPT chatbot on <a href=\"https://discord.gg/9GEQrxmVyE\">Discord</a>!</strong>\r\n \r\n[![Discord](https://img.shields.io/discord/1161736243340640419?label=Discord&logo=discord&logoColor=5865F2&style=flat-square&color=5865F2)](https://discord.gg/9GEQrxmVyE)\r\n[![arXiv 2310.08560](https://img.shields.io/badge/arXiv-2310.08560-B31B1B?logo=arxiv&style=flat-square)](https://arxiv.org/abs/2310.08560)\r\n\r\n</div>\r\n\r\n<details open>\r\n  <summary><h2>🤖 Create perpetual chatbots with self-editing memory!</h1></summary>\r\n  <div align=\"center\">\r\n    <br>\r\n    <img src=\"https://memgpt.ai/assets/img/demo.gif\" alt=\"MemGPT demo video\" width=\"800\">\r\n  </div>\r\n</details>\r\n\r\n<details>\r\n <summary><h2>🗃️ Chat with your data - talk to your SQL database or your local files!</strong></h2></summary>\r\n  <strong>SQL Database</strong>\r\n  <div align=\"center\">\r\n    <img src=\"https://memgpt.ai/assets/img/sql_demo.gif\" alt=\"MemGPT demo video for sql search\" width=\"800\">\r\n  </div>\r\n  <strong>Local files</strong>\r\n  <div align=\"center\">\r\n    <img src=\"https://memgpt.ai/assets/img/preload_archival_demo.gif\" alt=\"MemGPT demo video for sql search\" width=\"800\">\r\n  </div>\r\n</details>\r\n\r\n<details>\r\n  <summary><h2>📄 You can also talk to docs - for example ask about <a href=\"memgpt/personas/examples/docqa\">LlamaIndex</a>!</h1></summary>\r\n  <div align=\"center\">\r\n    <img src=\"https://memgpt.ai/assets/img/docqa_demo.gif\" alt=\"MemGPT demo video for llamaindex api docs search\" width=\"800\">\r\n  </div>\r\n  <details>\r\n  <summary><b>ChatGPT (GPT-4) when asked the same question:</b></summary>\r\n    <div align=\"center\">\r\n      <img src=\"https://memgpt.ai/assets/img/llama_index_gpt4.png\" alt=\"GPT-4 when asked about llamaindex api docs\" width=\"800\">\r\n    </div>\r\n    (Question from https://github.com/run-llama/llama_index/issues/7756)\r\n  </details>\r\n</details>\r\n\r\n## Quick setup \r\n\r\nJoin <a href=\"https://discord.gg/9GEQrxmVyE\">Discord</a></strong> and message the MemGPT bot (in the `#memgpt` channel). Then run the following commands (messaged to \"MemGPT Bot\"): \r\n* `/profile` (to create your profile)\r\n* `/key` (to enter your OpenAI key)\r\n* `/create` (to create a MemGPT chatbot)\r\n\r\nMake sure your privacy settings on this server are open so that MemGPT Bot can DM you: \\\r\nMemGPT → Privacy Settings → Direct Messages set to ON\r\n<div align=\"center\">\r\n <img src=\"https://memgpt.ai/assets/img/discord/dm_settings.png\" alt=\"set DMs settings on MemGPT server to be open in MemGPT so that MemGPT Bot can message you\" width=\"400\">\r\n</div>\r\n\r\nYou can see the full list of available commands when you enter `/` into the message box. \r\n<div align=\"center\">\r\n <img src=\"https://memgpt.ai/assets/img/discord/slash_commands.png\" alt=\"MemGPT Bot slash commands\" width=\"400\">\r\n</div>\r\n\r\n## What is MemGPT? \r\n\r\nMemory-GPT (or MemGPT in short) is a system that intelligently manages different memory tiers in LLMs in order to effectively provide extended context within the LLM's limited context window. For example, MemGPT knows when to push critical information to a vector database and when to retrieve it later in the chat, enabling perpetual conversations. Learn more about MemGPT in our [paper](https://arxiv.org/abs/2310.08560). \r\n\r\n## Running MemGPT Locally \r\n\r\nInstall dependencies:\r\n\r\n```sh\r\npip install -r requirements.txt\r\n```\r\n\r\nAdd your OpenAI API key to your environment:\r\n\r\n```sh\r\nexport OPENAI_API_KEY=YOUR_API_KEY\r\n```\r\n\r\nTo run MemGPT for as a conversation agent in CLI mode, simply run `main.py`:\r\n\r\n```sh\r\npython3 main.py\r\n```\r\n\r\nTo create a new starter user or starter persona (that MemGPT gets initialized with), create a new `.txt` file in [/memgpt/humans/examples](/memgpt/humans/examples) or [/memgpt/personas/examples](/memgpt/personas/examples), then use the `--persona` or `--human` flag when running `main.py`. For example:\r\n\r\n```sh\r\n# assuming you created a new file /memgpt/humans/examples/me.txt\r\npython main.py --human me.txt\r\n```\r\n\r\n### `main.py` flags\r\n\r\n```text\r\n--persona\r\n  load a specific persona file\r\n--human\r\n  load a specific human file\r\n--first\r\n  allows you to send the first message in the chat (by default, MemGPT will send the first message)\r\n--debug\r\n  enables debugging output\r\n--archival_storage_faiss_path=<ARCHIVAL_STORAGE_FAISS_PATH>\r\n  load in document database (backed by FAISS index)\r\n--archival_storage_files=\"<ARCHIVAL_STORAGE_FILES_GLOB>\"\r\n  pre-load files into archival memory\r\n--archival_storage_sqldb=<SQLDB_PATH>\r\n  load in SQL database\r\n```\r\n\r\n### Interactive CLI commands\r\n\r\nWhile using MemGPT via the CLI you can run various commands:\r\n\r\n```text\r\n/exit\r\n  exit the CLI\r\n/save\r\n  save a checkpoint of the current agent/conversation state\r\n/load\r\n  load a saved checkpoint\r\n/dump\r\n  view the current message log (see the contents of main context)\r\n/memory\r\n  print the current contents of agent memory\r\n/pop\r\n  undo the last message in the conversation\r\n/heartbeat\r\n  send a heartbeat system message to the agent\r\n/memorywarning\r\n  send a memory warning system message to the agent\r\n```\r\n\r\n## Use MemGPT to talk to your Database!\r\n\r\nMemGPT's archival memory let's you load your database and talk to it! To motivate this use-case, we have included a toy example. \r\n\r\nConsider the `test.db` already included in the repository.\r\n\r\nid\t| name |\tage\r\n--- | --- | ---\r\n1\t| Alice |\t30\r\n2\t| Bob\t | 25\r\n3\t| Charlie |\t35\r\n\r\nTo talk to this database, run:\r\n\r\n```sh\r\npython main_db.py  --archival_storage_sqldb=memgpt/personas/examples/sqldb/test.db\r\n```\r\n\r\nAnd then you can input the path to your database, and your query.\r\n\r\n```python\r\nPlease enter the path to the database. test.db\r\n...\r\nEnter your message: How old is Bob?\r\n...\r\n🤖 Bob is 25 years old.\r\n```\r\n\r\n\r\n### Support\r\n\r\n* By default MemGPT will use `gpt-4`, so your API key will require `gpt-4` API access.\r\n\r\nIf you have any further questions, or have anything to share, we are excited to hear your feedback!\r\n\r\n* For issues and feature requests, please [open a GitHub issue](https://github.com/cpacker/MemGPT/issues).\r\n\r\n### Datasets\r\nDatasets used in our [paper](https://arxiv.org/abs/2310.08560) can be downloaded at [HuggingFace](https://huggingface.co/MemGPT).\r\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "chatbots",
        "memgpt",
        "thelyoncrypt",
        "thelyoncrypt memgpt",
        "chatbots maintain",
        "creates chatbots"
      ],
      "category": "virtual-assistants"
    },
    "ThingsPanel--thingspanel-mcp": {
      "owner": "ThingsPanel",
      "name": "thingspanel-mcp",
      "url": "https://github.com/ThingsPanel/thingspanel-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/ThingsPanel.webp",
      "description": "Integrates IoT devices with AI models for natural language control and data analysis. Simplifies connection to IoT infrastructure through a standardized interface.",
      "stars": 39,
      "forks": 11,
      "license": "Apache License 2.0",
      "language": "Python",
      "updated_at": "2025-09-12T08:01:19Z",
      "readme_content": "# ThingsPanel MCP [![License](https://img.shields.io/badge/license-Apache%202.0-blue.svg)](LICENSE) [![Python Version](https://img.shields.io/pypi/pyversions/thingspanel-mcp.svg)](https://pypi.org/project/thingspanel-mcp/) [![PyPI version](https://badge.fury.io/py/thingspanel-mcp.svg)](https://badge.fury.io/py/thingspanel-mcp)\n<a href=\"https://glama.ai/mcp/servers/@ThingsPanel/thingspanel-mcp\">\n  <img alt=\"badge\" width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@ThingsPanel/thingspanel-mcp/badge\" />\n</a>\n\n[ThingsPanel](http://thingspanel.io/) IoT Platform's MCP (Model Context Protocol) Server.\n\n[English](README.md) | [中文](README_CN.md)\n\n## 🚀 Project Overview\n\nThingsPanel MCP Server is an innovative intelligent interface that enables you to:\n\n- Interact with IoT devices using natural language\n- Easily retrieve device information\n- Monitor device performance and status in real-time\n- Simplify device control commands\n- Analyze platform-wide statistical data and trends\n\n## Target Audience\n\n### Intended Users\n\n- **IoT Solution Developers**: Engineers and developers building solutions on the ThingsPanel IoT platform and seeking AI integration capabilities\n- **AI Integration Experts**: Professionals looking to connect AI models with IoT systems\n- **System Administrators**: IT personnel managing IoT infrastructure and wanting to enable AI-driven analysis and control\n- **Product Teams**: Teams building products that combine IoT and AI functionality\n\n### Problems Addressed\n\n- **Integration Complexity**: Eliminates the need to create custom integrations between AI models and IoT platforms\n- **Standardized Access**: Provides a consistent interface for AI models to interact with IoT data and devices\n- **Security Control**: Manages authentication and authorization for AI access to IoT systems\n- **Lowered Technical Barriers**: Reduces technical obstacles to adding AI capabilities to existing IoT deployments\n\n### Ideal Application Scenarios\n\n- **Natural Language IoT Control**: Enable users to control devices through AI assistants using natural language\n- **Intelligent Data Analysis**: Allow AI models to access and analyze IoT sensor data for insights\n- **Anomaly Detection**: Connect AI models to device data streams for real-time anomaly detection\n- **Predictive Maintenance**: Enable AI-driven predictive maintenance by providing device history access\n- **Automated Reporting**: Create systems that can generate IoT data reports and visualizations on demand\n- **Operational Optimization**: Use AI to optimize device operations based on historical patterns\n\n## ✨ Core Features\n\n- 🗣️ Natural Language Querying\n- 📊 Comprehensive Device Insights\n- 🌡️ Real-time Telemetry Data\n- 🎮 Convenient Device Control\n- 📈 Platform-wide Analytics\n\n## 🛠️ Prerequisites\n\n- Python 3.8+\n- ThingsPanel Account\n- ThingsPanel API Key\n\n## 📦 Installation\n\n### Option 1: Pip Installation\n\n```bash\npip install thingspanel-mcp\n```\n\n### Option 2: Source Code Installation\n\n```bash\n# Clone the repository\ngit clone https://github.com/ThingsPanel/thingspanel-mcp.git\n\n# Navigate to project directory\ncd thingspanel-mcp\n\n# Install the project\npip install -e .\n```\n\n## 🔐 Configuration\n\n### Configuration Methods (Choose One)\n\n#### Method 1: Direct Command Line Configuration (Recommended)\n\n```bash\nthingspanel-mcp --api-key \"Your API Key\" --base-url \"Your ThingsPanel Base URL\"\n```\n\n#### Method 2: Environment Variable Configuration\n\nIf you want to avoid repeated input, set environment variables:\n\n```bash\n# Add to ~/.bashrc, ~/.zshrc, or corresponding shell config file\nexport THINGSPANEL_API_KEY=\"Your API Key\"\nexport THINGSPANEL_BASE_URL=\"Your ThingsPanel Base URL\"\n\n# Then run\nsource ~/.bashrc  # or source ~/.zshrc\n```\n\n💡 Tips:\n\n- API keys are typically obtained from the API KEY management in the ThingsPanel platform\n- Base URL refers to your ThingsPanel platform address, e.g., `http://demo.thingspanel.cn/`\n- Command-line configuration is recommended to protect sensitive information\n\n## 🖥️ Claude Desktop Integration\n\nAdd the following to your Claude desktop configuration file (`claude_desktop_config.json`):\n\n```json\n{\n  \"mcpServers\": {\n    \"thingspanel\": {\n      \"command\": \"thingspanel-mcp\",\n      \"args\": [\n        \"--api-key\", \"Your API Key\",\n        \"--base-url\", \"Your Base URL\"\n      ]\n    }\n  }\n}\n```\n\n## 🤔 Interaction Examples\n\nUsing the ThingsPanel MCP Server, you can now make natural language queries such as:\n\n- \"What is the current temperature of my sensor?\"\n- \"List all active devices\"\n- \"Turn on the automatic sprinkler system\"\n- \"Show device activity for the last 24 hours\"\n\n## 🛡️ Security\n\n- Secure credential management\n- Uses ThingsPanel official API\n- Supports token-based authentication\n\n## License\n\nApache License 2.0\n\n## 🌟 Support Us\n\nIf this project helps you, please give us a star on GitHub! ⭐\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "thingspanel",
        "iot",
        "interface",
        "assistants thingspanel",
        "thingspanel mcp",
        "thingspanel thingspanel"
      ],
      "category": "virtual-assistants"
    },
    "Xiaoxinkeji--XBotV2": {
      "owner": "Xiaoxinkeji",
      "name": "XBotV2",
      "url": "https://github.com/Xiaoxinkeji/XBotV2",
      "imageUrl": "/freedevtools/mcp/pfp/Xiaoxinkeji.webp",
      "description": "A comprehensive WeChat bot framework that supports various interactive features and game functionalities, equipped with a plugin system for customization and a web management interface for user oversight. It facilitates efficient message forwarding and processing with easy deployment using Docker.",
      "stars": 1,
      "forks": 0,
      "license": "GNU General Public License v3.0",
      "language": "Python",
      "updated_at": "2025-03-30T04:55:02Z",
      "readme_content": "# 🤖 XYBot V2\n\nXYBot V2 是一个功能丰富的微信机器人框架,支持多种互动功能和游戏玩法。\n\n# 免责声明\n\n- 这个项目免费开源，不存在收费。\n- 本工具仅供学习和技术研究使用，不得用于任何商业或非法行为。\n- 本工具的作者不对本工具的安全性、完整性、可靠性、有效性、正确性或适用性做任何明示或暗示的保证，也不对本工具的使用或滥用造成的任何直接或间接的损失、责任、索赔、要求或诉讼承担任何责任。\n- 本工具的作者保留随时修改、更新、删除或终止本工具的权利，无需事先通知或承担任何义务。\n- 本工具的使用者应遵守相关法律法规，尊重微信的版权和隐私，不得侵犯微信或其他第三方的合法权益，不得从事任何违法或不道德的行为。\n- 本工具的使用者在下载、安装、运行或使用本工具时，即表示已阅读并同意本免责声明。如有异议，请立即停止使用本工具，并删除所有相关文件。\n\n# 公告\n\n## 项目还在开发中，有些commit有bug，更新叠代会非常迅速。如果你部署好的能用，在正式发布前，可以不用更新了。\n\n## 统一回复ISSUE内的问题：我敢承诺项目内不会有任何形式的后门程序、病毒程序、木马程序，最多只有一个防滥用倒卖的框架检测。\n\n# 📄 文档\n\n## https://henryxiaoyang.github.io/XYBotV2\n\n# 💬 微信交流群\n\n<div style=\"text-align: center\" align=\"center\">\n    <img alt=\"微信交流群二维码\" src=\"https://qrcode.yangres.com/get_image\" style=\"width: 300px; height: auto;\">\n    <p>微信扫码加入交流群</p>\n    <a href=\"https://qrcode.yangres.com/get_image\">🔗图片会被缓存，点我查看最新二维码</a>\n</div>\n\n# 🙏 赞助\n\n<div style=\"text-align: center\" align=\"center\">\n    <h2>开源不易，请作者喝杯奶茶吧🙏</h2>\n    \n    \n</div>\n\n# ✨ 主要功能\n\n## 🛠️ 基础功能\n\n- 🤖 AI聊天 - 支持文字、图片、语音等多模态交互\n- 📰 每日新闻 - 自动推送每日新闻\n- 🎵 点歌系统 - 支持在线点歌\n- 🌤️ 天气查询 - 查询全国各地天气\n- 🎮 游戏功能 - 五子棋、战争雷霆玩家查询等\n\n## 💎 积分系统\n\n- 📝 每日签到 - 支持连续签到奖励\n- 🎲 抽奖系统 - 多种抽奖玩法\n- 🧧 红包系统 - 群内发积分红包\n- 💰 积分交易 - 用户间积分转账\n- 📊 积分排行 - 查看积分排名\n\n## 👮 管理功能\n\n- ⚙️ 插件管理 - 动态加载/卸载插件\n- 👥 白名单管理 - 控制机器人使用权限\n- 📊 积分管理 - 管理员可调整用户积分\n- 🔄 签到重置 - 重置所有用户签到状态\n\n# 🔌 插件系统\n\nXYBot V2 采用插件化设计,所有功能都以插件形式实现。主要插件包括:\n\n- 👨‍💼 AdminPoint - 积分管理\n- 🔄 AdminSignInReset - 签到重置\n- 🛡️ AdminWhitelist - 白名单管理\n- 🤖 Ai - AI聊天\n- 📊 BotStatus - 机器人状态\n- 📱 GetContact - 获取通讯录\n- 🌤️ GetWeather - 天气查询\n- 🎮 Gomoku - 五子棋游戏\n- 🌅 GoodMorning - 早安问候\n- 📈 Leaderboard - 积分排行\n- 🎲 LuckyDraw - 幸运抽奖\n- 📋 Menu - 菜单系统\n- 🎵 Music - 点歌系统\n- 📰 News - 新闻推送\n- 💱 PointTrade - 积分交易\n- 💰 QueryPoint - 积分查询\n- 🎯 RandomMember - 随机群成员\n- 🖼️ RandomPicture - 随机图片\n- 🧧 RedPacket - 红包系统\n- ✍️ SignIn - 每日签到\n- ✈️ Warthunder - 战争雷霆查询\n\n# 🚀 部署说明\n\n## 💻 Python部署\n\n### 🪟 Windows部署\n\n#### 1. 环境准备\n\n- 安装 Python 3.11: https://www.python.org/downloads/release/python-3119/\n- 安装 ffmpeg: 从[ffmpeg官网](https://www.ffmpeg.org/download.html)下载并添加到环境变量\n- 安装 Redis: 从[Redis](https://github.com/tporadowski/redis/releases/tag/v5.0.14.1)下载并启动服务\n\n#### 2. 安装项目\n\n```bash\ngit clone https://github.com/HenryXiaoYang/XYBotV2.git\ncd XYBotV2\npython -m venv venv\n.\\venv\\Scripts\\activate\npip install -r requirements.txt\n```\n\n#### 3. 启动机器人\n\n```bash\nstart redis-server\npython app.py\n```\n\n### 🐧 Linux部署\n\n#### 1. 环境准备\n\n```bash\nsudo apt update\nsudo apt install python3.11 python3.11-venv redis-server ffmpeg\nsudo systemctl start redis\nsudo systemctl enable redis\n```\n\n#### 2. 安装项目\n\n```bash\ngit clone https://github.com/HenryXiaoYang/XYBotV2.git\ncd XYBotV2\npython3.11 -m venv venv\nsource venv/bin/activate\npip install -r requirements.txt\n```\n\n#### 3. 启动机器人\n\n```bash\npython app.py\n```\n\n### 🌌 无WebUI简单启动\n\n如果你不需要WebUI界面，可以直接使用bot.py：\n\n```bash\npython bot.py\n```\n\n## ⚙️ 配置说明\n\n- 主配置: main_config.toml\n- 插件配置: plugins/all_in_one_config.toml\n\n这几个插件需要配置API密钥:\n- 🤖 Ai\n- 🌤️ GetWeather\n\n## ❓ 常见问题\n\n1. 与网络相关的报错\n   - 检查网络连接\n   - 关闭代理软件\n   - 重启XYBot和Redis\n\n2. `正在运行`相关的报错\n   - 将占用9000端口的进程结束\n\n3. 无法访问Web界面\n   - 确保9999端口已开放\n   - 配置防火墙允许9999端口\n\n# 💻 代码提交\n\n提交代码时请使用 `feat: something` 作为说明，支持的标识如下:\n\n- `feat` 新功能(feature)\n- `fix` 修复bug\n- `docs` 文档(documentation)\n- `style` 格式(不影响代码运行的变动)\n- `ref` 重构(即不是新增功能，也不是修改bug的代码变动)\n- `perf` 性能优化(performance)\n- `test` 增加测试\n- `chore` 构建过程或辅助工具的变动\n- `revert` 撤销",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "xbotv2",
        "wechat",
        "bot",
        "bot framework",
        "wechat bot",
        "xbotv2 comprehensive"
      ],
      "category": "virtual-assistants"
    },
    "Yash-Kavaiya--mcp-server-conversation-agents": {
      "owner": "Yash-Kavaiya",
      "name": "mcp-server-conversation-agents",
      "url": "https://github.com/Yash-Kavaiya/mcp-server-conversation-agents",
      "imageUrl": "/freedevtools/mcp/pfp/Yash-Kavaiya.webp",
      "description": "Integrates AI assistants with Dialogflow CX for real-time tool invocation and access to external resources, enhancing user interactions and streamlining workflows.",
      "stars": 3,
      "forks": 0,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-08-07T16:51:09Z",
      "readme_content": "# 🤖 Dialogflow CX MCP Server 🚀\n\n![Dialogflow CX](https://img.shields.io/badge/Dialogflow_CX-4285F4?style=for-the-badge&logo=google-cloud&logoColor=white)\n![MCP](https://img.shields.io/badge/MCP-Server-00C7B7?style=for-the-badge&logo=serverfault&logoColor=white)\n![Python](https://img.shields.io/badge/Python-3.12-3776AB?style=for-the-badge&logo=python&logoColor=white)\n\nA powerful Model Control Protocol (MCP) server implementation for **Google Dialogflow CX**, enabling seamless integration between AI assistants and Google's advanced conversational platform.\n\n> 💡 **Pro Tip:** This server bridges the gap between AI assistants and Dialogflow CX, unlocking powerful conversational capabilities!\n\n## 📋 Overview\n\nThis project provides a suite of tools that allow AI assistants to interact with Dialogflow CX agents through a standardized protocol. The server handles all the complexity of managing conversations, processing intent detection, and interfacing with Google's powerful NLU systems.\n\n### ✨ Key Features\n\n- 🔄 Bidirectional communication with Dialogflow CX\n- 🎯 Intent detection and matching capabilities\n- 🎤 Audio processing for speech recognition\n- 🔌 Webhook request/response handling\n- 📝 Session management for persistent conversations\n- 🔒 Secure API authentication\n\n## 🔧 Requirements\n\n| Requirement | Description | Version |\n|-------------|-------------|---------|\n| 🐍 Python | Programming language | 3.12+ |\n| ☁️ Google Cloud | Project with Dialogflow CX enabled | Latest |\n| 🤖 Dialogflow CX | Conversational agent | Latest |\n| 🔑 API Credentials | Authentication for Google services | - |\n\n## 🚀 Installation\n\n### 🐳 Using Docker\n\n```bash\n# Clone the repository\ngit clone https://github.com/Yash-Kavaiya/mcp-server-conversation-agents.git\ncd mcp-server-conversation-agents\n\n# Build the Docker image\ndocker build -t dialogflow-cx-mcp .\n\n# Run the container\ndocker run -it dialogflow-cx-mcp\n```\n\n### 💻 Manual Installation\n\n```bash\n# Clone the repository\ngit clone https://github.com/Yash-Kavaiya/mcp-server-conversation-agents.git\ncd mcp-server-conversation-agents\n\n# Create a virtual environment (optional but recommended)\npython -m venv venv\nsource venv/bin/activate  # On Windows: venv\\Scripts\\activate\n\n# Install the package\npip install -e .\n```\n\n## ⚙️ Configuration\n\nYou'll need to provide the following configuration parameters:\n\n| Parameter | Description | Example |\n|-----------|-------------|---------|\n| `dialogflowApiKey` | Your Dialogflow API key | `\"abc123def456\"` |\n| `projectId` | Google Cloud project ID | `\"my-dialogflow-project\"` |\n| `location` | Location of the agent | `\"us-central1\"` |\n| `agentId` | ID of your Dialogflow CX agent | `\"12345-abcde-67890\"` |\n\nThese can be set as environment variables:\n\n```bash\nexport DIALOGFLOW_API_KEY=your_api_key\nexport PROJECT_ID=your_project_id\nexport LOCATION=your_location\nexport AGENT_ID=your_agent_id\n```\n\n## 📊 Architecture\n\n```mermaid\ngraph TD\n    A[AI Assistant] <-->|MCP Protocol| B[MCP Server]\n    B <-->|Google API| C[Dialogflow CX]\n    C <-->|NLU Processing| D[Intent Detection]\n    C <-->|Conversation Management| E[Session Management]\n    B <-->|Webhooks| F[External Services]\n```\n\n## 🛠️ Usage\n\nThe MCP server exposes the following tools for AI assistants:\n\n### 🔍 initialize_dialogflow\n\nInitialize the Dialogflow CX client with your project details.\n\n```python\nawait initialize_dialogflow(\n    project_id=\"your-project-id\",\n    location=\"us-central1\",\n    agent_id=\"your-agent-id\",\n    credentials_path=\"/path/to/credentials.json\"  # Optional\n)\n```\n\n### 💬 detect_intent\n\nDetect intent from text input.\n\n```python\nresponse = await detect_intent(\n    text=\"Hello, how can you help me?\",\n    session_id=\"user123\",  # Optional\n    language_code=\"en-US\"  # Optional\n)\n```\n\n### 🎤 detect_intent_from_audio\n\nProcess audio files to detect intent.\n\n```python\nresponse = await detect_intent_from_audio(\n    audio_file_path=\"/path/to/audio.wav\",\n    session_id=\"user123\",  # Optional\n    sample_rate_hertz=16000,  # Optional\n    audio_encoding=\"AUDIO_ENCODING_LINEAR_16\",  # Optional\n    language_code=\"en-US\"  # Optional\n)\n```\n\n### 🎯 match_intent\n\nMatch intent without affecting the conversation session.\n\n```python\nresponse = await match_intent(\n    text=\"What are your hours?\",\n    session_id=\"user123\",  # Optional\n    language_code=\"en-US\"  # Optional\n)\n```\n\n### 🔄 Webhook Handling\n\nParse webhook requests and create webhook responses:\n\n```python\n# Parse a webhook request\nparsed_request = await parse_webhook_request(request_json)\n\n# Create a webhook response\nresponse = await create_webhook_response({\n    \"messages\": [\"Hello! How can I help you today?\"],\n    \"parameter_updates\": {\"user_name\": \"John\"}\n})\n```\n\n## 🔧 Response Format\n\nHere's an example of the response format:\n\n<details>\n<summary>📋 Click to expand</summary>\n\n```json\n{\n  \"messages\": [\n    {\n      \"type\": \"text\",\n      \"content\": \"Hello! How can I help you today?\"\n    }\n  ],\n  \"intent\": {\n    \"name\": \"greeting\",\n    \"confidence\": 0.95\n  },\n  \"parameters\": {\n    \"user_name\": \"John\"\n  },\n  \"current_page\": \"Welcome Page\",\n  \"session_id\": \"user123\",\n  \"end_interaction\": false\n}\n```\n</details>\n\n## 🔗 Smithery Integration\n\nThis project is configured to work with [Smithery.ai](https://smithery.ai/), a platform that allows for easy deployment and management of MCP servers.\n\n> 💡 **Pro Tip:** Smithery.ai integration enables one-click deployment and simplified management of your Dialogflow CX MCP server!\n\n## 📄 License\n\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)\n\n## 👥 Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n\n### Contribution Workflow\n\n1. 🍴 Fork the repository\n2. 🔧 Create a feature branch (`git checkout -b feature/amazing-feature`)\n3. 💻 Commit your changes (`git commit -m 'Add some amazing feature'`)\n4. 🚀 Push to the branch (`git push origin feature/amazing-feature`)\n5. 🔍 Open a Pull Request\n\n---\n\n<p align=\"center\">\n  Built with ❤️ by the MCP Server team\n</p>\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "dialogflow",
        "assistants",
        "ai",
        "virtual assistants",
        "ai assistants",
        "assistants dialogflow"
      ],
      "category": "virtual-assistants"
    },
    "a1351995160--Auto-GPT": {
      "owner": "a1351995160",
      "name": "Auto-GPT",
      "url": "https://github.com/a1351995160/Auto-GPT",
      "imageUrl": "/freedevtools/mcp/pfp/a1351995160.webp",
      "description": "Autonomously achieve goals by chaining thoughts of the GPT-4 model while accessing the internet, managing memory, and storing files. The server supports plugin extensibility to enhance functionality.",
      "stars": 0,
      "forks": 0,
      "license": "MIT License",
      "language": "",
      "updated_at": "2023-05-22T08:25:34Z",
      "readme_content": "# Auto-GPT: An Autonomous GPT-4 Experiment\n[![Official Website](https://img.shields.io/badge/Official%20Website-agpt.co-blue?style=flat&logo=world&logoColor=white)](https://agpt.co)\n[![Unit Tests](https://img.shields.io/github/actions/workflow/status/Significant-Gravitas/Auto-GPT/ci.yml?label=unit%20tests)](https://github.com/Significant-Gravitas/Auto-GPT/actions/workflows/ci.yml)\n[](https://discord.gg/autogpt)\n[![GitHub Repo stars](https://img.shields.io/github/stars/Significant-Gravitas/auto-gpt?style=social)](https://github.com/Significant-Gravitas/Auto-GPT/stargazers)\n[![Twitter Follow](https://img.shields.io/twitter/follow/siggravitas?style=social)](https://twitter.com/SigGravitas)\n\n## 💡 Get help - [Q&A](https://github.com/Significant-Gravitas/Auto-GPT/discussions/categories/q-a) or [Discord 💬](https://discord.gg/autogpt)\n\n<hr/>\n\n### 🔴 USE `stable` not `master` 🔴\n\n**Download the latest `stable` release from here: https://github.com/Significant-Gravitas/Auto-GPT/releases/latest.**\nThe `master` branch is under heavy development and may often be in a **broken** state.\n\n<hr/>\n\n\nAuto-GPT is an experimental open-source application showcasing the capabilities of the GPT-4 language model. This program, driven by GPT-4, chains together LLM \"thoughts\", to autonomously achieve whatever goal you set. As one of the first examples of GPT-4 running fully autonomously, Auto-GPT pushes the boundaries of what is possible with AI.\n\n<h2 align=\"center\"> Demo April 16th 2023 </h2>\n\nhttps://user-images.githubusercontent.com/70048414/232352935-55c6bf7c-3958-406e-8610-0913475a0b05.mp4\n\nDemo made by <a href=https://twitter.com/BlakeWerlinger>Blake Werlinger</a>\n\n<h2 align=\"center\"> 💖 Help Fund Auto-GPT's Development 💖</h2>\n<p align=\"center\">\nIf you can spare a coffee, you can help to cover the costs of developing Auto-GPT and help to push the boundaries of fully autonomous AI!\nYour support is greatly appreciated. Development of this free, open-source project is made possible by all the <a href=\"https://github.com/Significant-Gravitas/Auto-GPT/graphs/contributors\">contributors</a> and <a href=\"https://github.com/sponsors/Torantulino\">sponsors</a>. If you'd like to sponsor this project and have your avatar or company logo appear below <a href=\"https://github.com/sponsors/Torantulino\">click here</a>.\n</p>\n\n\n<p align=\"center\">\n<div align=\"center\" class=\"logo-container\">\n<a href=\"https://www.zilliz.com/\">\n<picture height=\"40px\">\n  <source media=\"(prefers-color-scheme: light)\" srcset=\"https://user-images.githubusercontent.com/22963551/234158272-7917382e-ff80-469e-8d8c-94f4477b8b5a.png\">\n  <img src=\"https://user-images.githubusercontent.com/22963551/234158222-30e2d7a7-f0a9-433d-a305-e3aa0b194444.png\" height=\"40px\" alt=\"Zilliz\" />\n</picture>\n</a>\n\n<a href=\"https://roost.ai\">\n<img src=\"https://user-images.githubusercontent.com/22963551/234180283-b58cb03c-c95a-4196-93c1-28b52a388e9d.png\" height=\"40px\" alt=\"Roost.AI\" />\n</a>\n  \n<a href=\"https://nuclei.ai/\">\n<picture height=\"40px\">\n  <source media=\"(prefers-color-scheme: light)\" srcset=\"https://user-images.githubusercontent.com/22963551/234153428-24a6f31d-c0c6-4c9b-b3f4-9110148f67b4.png\">\n  <img src=\"https://user-images.githubusercontent.com/22963551/234181283-691c5d71-ca94-4646-a1cf-6e818bd86faa.png\" height=\"40px\" alt=\"NucleiAI\" />\n</picture>\n</a>\n\n<a href=\"https://www.algohash.org/\">\n<picture>\n  <source media=\"(prefers-color-scheme: light)\" srcset=\"https://user-images.githubusercontent.com/22963551/234180375-1365891c-0ba6-4d49-94c3-847c85fe03b0.png\" >\n  <img src=\"https://user-images.githubusercontent.com/22963551/234180359-143e4a7a-4a71-4830-99c8-9b165cde995f.png\" height=\"40px\" alt=\"Algohash\" />\n</picture>\n</a>\n\n<a href=\"https://www.typingmind.com/?utm_source=autogpt\">\n<picture height=\"40px\">\n  <source media=\"(prefers-color-scheme: light)\" srcset=\"https://user-images.githubusercontent.com/22963551/233202971-61e77209-58a0-47d9-9f7e-dd081111437b.png\">\n  <img src=\"https://user-images.githubusercontent.com/22963551/234157731-f908b5db-8fe7-4036-89b6-7b2a21f87e3a.png\" height=\"40px\" alt=\"TypingMind\" />\n</picture>\n</a>\n\n<a href=\"https://github.com/weaviate/weaviate\">\n<picture height=\"40px\">\n  <source media=\"(prefers-color-scheme: light)\" srcset=\"https://user-images.githubusercontent.com/22963551/234181699-3d7f6ea8-5a7f-4e98-b812-37be1081be4b.png\">\n  <img src=\"https://user-images.githubusercontent.com/22963551/234181695-fc895159-b921-4895-9a13-65e6eff5b0e7.png\" height=\"40px\" alt=\"TypingMind\" />\n</picture>\n</a>\n\n<a href=\"https://chatgpv.com/?ref=spni76459e4fa3f30a\">\n<img src=\"https://github-production-user-asset-6210df.s3.amazonaws.com/22963551/239132565-623a2dd6-eaeb-4941-b40f-c5a29ca6bebc.png\" height=\"40px\" alt=\"ChatGPV\" />\n</a>\n  \n</div>\n</br>\n\n\n\n<p align=\"center\"><a href=\"https://github.com/robinicus\"><img src=\"https://avatars.githubusercontent.com/robinicus?v=4\" width=\"50px\" alt=\"robinicus\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/0xmatchmaker\"><img src=\"https://avatars.githubusercontent.com/0xmatchmaker?v=4\" width=\"50px\" alt=\"0xmatchmaker\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/jazgarewal\"><img src=\"https://avatars.githubusercontent.com/jazgarewal?v=4\" width=\"50px\" alt=\"jazgarewal\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/MayurVirkar\"><img src=\"https://avatars.githubusercontent.com/MayurVirkar?v=4\" width=\"50px\" alt=\"MayurVirkar\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/avy-ai\"><img src=\"https://avatars.githubusercontent.com/avy-ai?v=4\" width=\"50px\" alt=\"avy-ai\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/TheStoneMX\"><img src=\"https://avatars.githubusercontent.com/TheStoneMX?v=4\" width=\"50px\" alt=\"TheStoneMX\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/goldenrecursion\"><img src=\"https://avatars.githubusercontent.com/goldenrecursion?v=4\" width=\"50px\" alt=\"goldenrecursion\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/MatthewAgs\"><img src=\"https://avatars.githubusercontent.com/MatthewAgs?v=4\" width=\"50px\" alt=\"MatthewAgs\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/eelbaz\"><img src=\"https://avatars.githubusercontent.com/eelbaz?v=4\" width=\"50px\" alt=\"eelbaz\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/rapidstartup\"><img src=\"https://avatars.githubusercontent.com/rapidstartup?v=4\" width=\"50px\" alt=\"rapidstartup\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/gklab\"><img src=\"https://avatars.githubusercontent.com/gklab?v=4\" width=\"50px\" alt=\"gklab\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/VoiceBeer\"><img src=\"https://avatars.githubusercontent.com/VoiceBeer?v=4\" width=\"50px\" alt=\"VoiceBeer\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/DailyBotHQ\"><img src=\"https://avatars.githubusercontent.com/DailyBotHQ?v=4\" width=\"50px\" alt=\"DailyBotHQ\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/lucas-chu\"><img src=\"https://avatars.githubusercontent.com/lucas-chu?v=4\" width=\"50px\" alt=\"lucas-chu\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/knifour\"><img src=\"https://avatars.githubusercontent.com/knifour?v=4\" width=\"50px\" alt=\"knifour\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/refinery1\"><img src=\"https://avatars.githubusercontent.com/refinery1?v=4\" width=\"50px\" alt=\"refinery1\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/st617\"><img src=\"https://avatars.githubusercontent.com/st617?v=4\" width=\"50px\" alt=\"st617\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/neodenit\"><img src=\"https://avatars.githubusercontent.com/neodenit?v=4\" width=\"50px\" alt=\"neodenit\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/CrazySwami\"><img src=\"https://avatars.githubusercontent.com/CrazySwami?v=4\" width=\"50px\" alt=\"CrazySwami\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/Heitechsoft\"><img src=\"https://avatars.githubusercontent.com/Heitechsoft?v=4\" width=\"50px\" alt=\"Heitechsoft\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/RealChrisSean\"><img src=\"https://avatars.githubusercontent.com/RealChrisSean?v=4\" width=\"50px\" alt=\"RealChrisSean\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/abhinav-pandey29\"><img src=\"https://avatars.githubusercontent.com/abhinav-pandey29?v=4\" width=\"50px\" alt=\"abhinav-pandey29\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/Explorergt92\"><img src=\"https://avatars.githubusercontent.com/Explorergt92?v=4\" width=\"50px\" alt=\"Explorergt92\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/SparkplanAI\"><img src=\"https://avatars.githubusercontent.com/SparkplanAI?v=4\" width=\"50px\" alt=\"SparkplanAI\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/crizzler\"><img src=\"https://avatars.githubusercontent.com/crizzler?v=4\" width=\"50px\" alt=\"crizzler\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/kreativai\"><img src=\"https://avatars.githubusercontent.com/kreativai?v=4\" width=\"50px\" alt=\"kreativai\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/omphos\"><img src=\"https://avatars.githubusercontent.com/omphos?v=4\" width=\"50px\" alt=\"omphos\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/Jahmazon\"><img src=\"https://avatars.githubusercontent.com/Jahmazon?v=4\" width=\"50px\" alt=\"Jahmazon\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/tjarmain\"><img src=\"https://avatars.githubusercontent.com/tjarmain?v=4\" width=\"50px\" alt=\"tjarmain\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/ddtarazona\"><img src=\"https://avatars.githubusercontent.com/ddtarazona?v=4\" width=\"50px\" alt=\"ddtarazona\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/saten-private\"><img src=\"https://avatars.githubusercontent.com/saten-private?v=4\" width=\"50px\" alt=\"saten-private\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/anvarazizov\"><img src=\"https://avatars.githubusercontent.com/anvarazizov?v=4\" width=\"50px\" alt=\"anvarazizov\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/lazzacapital\"><img src=\"https://avatars.githubusercontent.com/lazzacapital?v=4\" width=\"50px\" alt=\"lazzacapital\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/m\"><img src=\"https://avatars.githubusercontent.com/m?v=4\" width=\"50px\" alt=\"m\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/Pythagora-io\"><img src=\"https://avatars.githubusercontent.com/Pythagora-io?v=4\" width=\"50px\" alt=\"Pythagora-io\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/Web3Capital\"><img src=\"https://avatars.githubusercontent.com/Web3Capital?v=4\" width=\"50px\" alt=\"Web3Capital\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/toverly1\"><img src=\"https://avatars.githubusercontent.com/toverly1?v=4\" width=\"50px\" alt=\"toverly1\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/digisomni\"><img src=\"https://avatars.githubusercontent.com/digisomni?v=4\" width=\"50px\" alt=\"digisomni\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/concreit\"><img src=\"https://avatars.githubusercontent.com/concreit?v=4\" width=\"50px\" alt=\"concreit\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/LeeRobidas\"><img src=\"https://avatars.githubusercontent.com/LeeRobidas?v=4\" width=\"50px\" alt=\"LeeRobidas\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/Josecodesalot\"><img src=\"https://avatars.githubusercontent.com/Josecodesalot?v=4\" width=\"50px\" alt=\"Josecodesalot\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/dexterityx\"><img src=\"https://avatars.githubusercontent.com/dexterityx?v=4\" width=\"50px\" alt=\"dexterityx\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/rickscode\"><img src=\"https://avatars.githubusercontent.com/rickscode?v=4\" width=\"50px\" alt=\"rickscode\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/Brodie0\"><img src=\"https://avatars.githubusercontent.com/Brodie0?v=4\" width=\"50px\" alt=\"Brodie0\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/FSTatSBS\"><img src=\"https://avatars.githubusercontent.com/FSTatSBS?v=4\" width=\"50px\" alt=\"FSTatSBS\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/nocodeclarity\"><img src=\"https://avatars.githubusercontent.com/nocodeclarity?v=4\" width=\"50px\" alt=\"nocodeclarity\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/jsolejr\"><img src=\"https://avatars.githubusercontent.com/jsolejr?v=4\" width=\"50px\" alt=\"jsolejr\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/amr-elsehemy\"><img src=\"https://avatars.githubusercontent.com/amr-elsehemy?v=4\" width=\"50px\" alt=\"amr-elsehemy\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/RawBanana\"><img src=\"https://avatars.githubusercontent.com/RawBanana?v=4\" width=\"50px\" alt=\"RawBanana\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/horazius\"><img src=\"https://avatars.githubusercontent.com/horazius?v=4\" width=\"50px\" alt=\"horazius\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/SwftCoins\"><img src=\"https://avatars.githubusercontent.com/SwftCoins?v=4\" width=\"50px\" alt=\"SwftCoins\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/tob-le-rone\"><img src=\"https://avatars.githubusercontent.com/tob-le-rone?v=4\" width=\"50px\" alt=\"tob-le-rone\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/RThaweewat\"><img src=\"https://avatars.githubusercontent.com/RThaweewat?v=4\" width=\"50px\" alt=\"RThaweewat\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/jun784\"><img src=\"https://avatars.githubusercontent.com/jun784?v=4\" width=\"50px\" alt=\"jun784\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/joaomdmoura\"><img src=\"https://avatars.githubusercontent.com/joaomdmoura?v=4\" width=\"50px\" alt=\"joaomdmoura\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/rejunity\"><img src=\"https://avatars.githubusercontent.com/rejunity?v=4\" width=\"50px\" alt=\"rejunity\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/mathewhawkins\"><img src=\"https://avatars.githubusercontent.com/mathewhawkins?v=4\" width=\"50px\" alt=\"mathewhawkins\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/caitlynmeeks\"><img src=\"https://avatars.githubusercontent.com/caitlynmeeks?v=4\" width=\"50px\" alt=\"caitlynmeeks\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/jd3655\"><img src=\"https://avatars.githubusercontent.com/jd3655?v=4\" width=\"50px\" alt=\"jd3655\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/Odin519Tomas\"><img src=\"https://avatars.githubusercontent.com/Odin519Tomas?v=4\" width=\"50px\" alt=\"Odin519Tomas\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/DataMetis\"><img src=\"https://avatars.githubusercontent.com/DataMetis?v=4\" width=\"50px\" alt=\"DataMetis\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/webbcolton\"><img src=\"https://avatars.githubusercontent.com/webbcolton?v=4\" width=\"50px\" alt=\"webbcolton\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/rocks6\"><img src=\"https://avatars.githubusercontent.com/rocks6?v=4\" width=\"50px\" alt=\"rocks6\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/cxs\"><img src=\"https://avatars.githubusercontent.com/cxs?v=4\" width=\"50px\" alt=\"cxs\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/fruition\"><img src=\"https://avatars.githubusercontent.com/fruition?v=4\" width=\"50px\" alt=\"fruition\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/nnkostov\"><img src=\"https://avatars.githubusercontent.com/nnkostov?v=4\" width=\"50px\" alt=\"nnkostov\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/morcos\"><img src=\"https://avatars.githubusercontent.com/morcos?v=4\" width=\"50px\" alt=\"morcos\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/pingbotan\"><img src=\"https://avatars.githubusercontent.com/pingbotan?v=4\" width=\"50px\" alt=\"pingbotan\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/maxxflyer\"><img src=\"https://avatars.githubusercontent.com/maxxflyer?v=4\" width=\"50px\" alt=\"maxxflyer\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/tommi-joentakanen\"><img src=\"https://avatars.githubusercontent.com/tommi-joentakanen?v=4\" width=\"50px\" alt=\"tommi-joentakanen\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/hunteraraujo\"><img src=\"https://avatars.githubusercontent.com/hunteraraujo?v=4\" width=\"50px\" alt=\"hunteraraujo\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/projectonegames\"><img src=\"https://avatars.githubusercontent.com/projectonegames?v=4\" width=\"50px\" alt=\"projectonegames\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/tullytim\"><img src=\"https://avatars.githubusercontent.com/tullytim?v=4\" width=\"50px\" alt=\"tullytim\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/comet-ml\"><img src=\"https://avatars.githubusercontent.com/comet-ml?v=4\" width=\"50px\" alt=\"comet-ml\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/thepok\"><img src=\"https://avatars.githubusercontent.com/thepok?v=4\" width=\"50px\" alt=\"thepok\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/prompthero\"><img src=\"https://avatars.githubusercontent.com/prompthero?v=4\" width=\"50px\" alt=\"prompthero\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/sunchongren\"><img src=\"https://avatars.githubusercontent.com/sunchongren?v=4\" width=\"50px\" alt=\"sunchongren\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/neverinstall\"><img src=\"https://avatars.githubusercontent.com/neverinstall?v=4\" width=\"50px\" alt=\"neverinstall\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/josephcmiller2\"><img src=\"https://avatars.githubusercontent.com/josephcmiller2?v=4\" width=\"50px\" alt=\"josephcmiller2\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/yx3110\"><img src=\"https://avatars.githubusercontent.com/yx3110?v=4\" width=\"50px\" alt=\"yx3110\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/MBassi91\"><img src=\"https://avatars.githubusercontent.com/MBassi91?v=4\" width=\"50px\" alt=\"MBassi91\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/SpacingLily\"><img src=\"https://avatars.githubusercontent.com/SpacingLily?v=4\" width=\"50px\" alt=\"SpacingLily\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/arthur-x88\"><img src=\"https://avatars.githubusercontent.com/arthur-x88?v=4\" width=\"50px\" alt=\"arthur-x88\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/ciscodebs\"><img src=\"https://avatars.githubusercontent.com/ciscodebs?v=4\" width=\"50px\" alt=\"ciscodebs\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/christian-gheorghe\"><img src=\"https://avatars.githubusercontent.com/christian-gheorghe?v=4\" width=\"50px\" alt=\"christian-gheorghe\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/EngageStrategies\"><img src=\"https://avatars.githubusercontent.com/EngageStrategies?v=4\" width=\"50px\" alt=\"EngageStrategies\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/jondwillis\"><img src=\"https://avatars.githubusercontent.com/jondwillis?v=4\" width=\"50px\" alt=\"jondwillis\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/Cameron-Fulton\"><img src=\"https://avatars.githubusercontent.com/Cameron-Fulton?v=4\" width=\"50px\" alt=\"Cameron-Fulton\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/AryaXAI\"><img src=\"https://avatars.githubusercontent.com/AryaXAI?v=4\" width=\"50px\" alt=\"AryaXAI\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/AuroraHolding\"><img src=\"https://avatars.githubusercontent.com/AuroraHolding?v=4\" width=\"50px\" alt=\"AuroraHolding\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/Mr-Bishop42\"><img src=\"https://avatars.githubusercontent.com/Mr-Bishop42?v=4\" width=\"50px\" alt=\"Mr-Bishop42\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/doverhq\"><img src=\"https://avatars.githubusercontent.com/doverhq?v=4\" width=\"50px\" alt=\"doverhq\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/johnculkin\"><img src=\"https://avatars.githubusercontent.com/johnculkin?v=4\" width=\"50px\" alt=\"johnculkin\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/marv-technology\"><img src=\"https://avatars.githubusercontent.com/marv-technology?v=4\" width=\"50px\" alt=\"marv-technology\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/ikarosai\"><img src=\"https://avatars.githubusercontent.com/ikarosai?v=4\" width=\"50px\" alt=\"ikarosai\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/ColinConwell\"><img src=\"https://avatars.githubusercontent.com/ColinConwell?v=4\" width=\"50px\" alt=\"ColinConwell\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/humungasaurus\"><img src=\"https://avatars.githubusercontent.com/humungasaurus?v=4\" width=\"50px\" alt=\"humungasaurus\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/terpsfreak\"><img src=\"https://avatars.githubusercontent.com/terpsfreak?v=4\" width=\"50px\" alt=\"terpsfreak\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/iddelacruz\"><img src=\"https://avatars.githubusercontent.com/iddelacruz?v=4\" width=\"50px\" alt=\"iddelacruz\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/thisisjeffchen\"><img src=\"https://avatars.githubusercontent.com/thisisjeffchen?v=4\" width=\"50px\" alt=\"thisisjeffchen\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/nicoguyon\"><img src=\"https://avatars.githubusercontent.com/nicoguyon?v=4\" width=\"50px\" alt=\"nicoguyon\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/arjunb023\"><img src=\"https://avatars.githubusercontent.com/arjunb023?v=4\" width=\"50px\" alt=\"arjunb023\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/Nalhos\"><img src=\"https://avatars.githubusercontent.com/Nalhos?v=4\" width=\"50px\" alt=\"Nalhos\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/belharethsami\"><img src=\"https://avatars.githubusercontent.com/belharethsami?v=4\" width=\"50px\" alt=\"belharethsami\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/Mobivs\"><img src=\"https://avatars.githubusercontent.com/Mobivs?v=4\" width=\"50px\" alt=\"Mobivs\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/txtr99\"><img src=\"https://avatars.githubusercontent.com/txtr99?v=4\" width=\"50px\" alt=\"txtr99\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/ntwrite\"><img src=\"https://avatars.githubusercontent.com/ntwrite?v=4\" width=\"50px\" alt=\"ntwrite\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/founderblocks-sils\"><img src=\"https://avatars.githubusercontent.com/founderblocks-sils?v=4\" width=\"50px\" alt=\"founderblocks-sils\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/kMag410\"><img src=\"https://avatars.githubusercontent.com/kMag410?v=4\" width=\"50px\" alt=\"kMag410\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/angiaou\"><img src=\"https://avatars.githubusercontent.com/angiaou?v=4\" width=\"50px\" alt=\"angiaou\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/garythebat\"><img src=\"https://avatars.githubusercontent.com/garythebat?v=4\" width=\"50px\" alt=\"garythebat\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/lmaugustin\"><img src=\"https://avatars.githubusercontent.com/lmaugustin?v=4\" width=\"50px\" alt=\"lmaugustin\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/shawnharmsen\"><img src=\"https://avatars.githubusercontent.com/shawnharmsen?v=4\" width=\"50px\" alt=\"shawnharmsen\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/clortegah\"><img src=\"https://avatars.githubusercontent.com/clortegah?v=4\" width=\"50px\" alt=\"clortegah\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/MetaPath01\"><img src=\"https://avatars.githubusercontent.com/MetaPath01?v=4\" width=\"50px\" alt=\"MetaPath01\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/sekomike910\"><img src=\"https://avatars.githubusercontent.com/sekomike910?v=4\" width=\"50px\" alt=\"sekomike910\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/MediConCenHK\"><img src=\"https://avatars.githubusercontent.com/MediConCenHK?v=4\" width=\"50px\" alt=\"MediConCenHK\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/svpermari0\"><img src=\"https://avatars.githubusercontent.com/svpermari0?v=4\" width=\"50px\" alt=\"svpermari0\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/jacobyoby\"><img src=\"https://avatars.githubusercontent.com/jacobyoby?v=4\" width=\"50px\" alt=\"jacobyoby\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/turintech\"><img src=\"https://avatars.githubusercontent.com/turintech?v=4\" width=\"50px\" alt=\"turintech\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/allenstecat\"><img src=\"https://avatars.githubusercontent.com/allenstecat?v=4\" width=\"50px\" alt=\"allenstecat\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/CatsMeow492\"><img src=\"https://avatars.githubusercontent.com/CatsMeow492?v=4\" width=\"50px\" alt=\"CatsMeow492\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/tommygeee\"><img src=\"https://avatars.githubusercontent.com/tommygeee?v=4\" width=\"50px\" alt=\"tommygeee\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/judegomila\"><img src=\"https://avatars.githubusercontent.com/judegomila?v=4\" width=\"50px\" alt=\"judegomila\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/cfarquhar\"><img src=\"https://avatars.githubusercontent.com/cfarquhar?v=4\" width=\"50px\" alt=\"cfarquhar\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/ZoneSixGames\"><img src=\"https://avatars.githubusercontent.com/ZoneSixGames?v=4\" width=\"50px\" alt=\"ZoneSixGames\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/kenndanielso\"><img src=\"https://avatars.githubusercontent.com/kenndanielso?v=4\" width=\"50px\" alt=\"kenndanielso\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/CrypteorCapital\"><img src=\"https://avatars.githubusercontent.com/CrypteorCapital?v=4\" width=\"50px\" alt=\"CrypteorCapital\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/sultanmeghji\"><img src=\"https://avatars.githubusercontent.com/sultanmeghji?v=4\" width=\"50px\" alt=\"sultanmeghji\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/jenius-eagle\"><img src=\"https://avatars.githubusercontent.com/jenius-eagle?v=4\" width=\"50px\" alt=\"jenius-eagle\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/josephjacks\"><img src=\"https://avatars.githubusercontent.com/josephjacks?v=4\" width=\"50px\" alt=\"josephjacks\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/pingshian0131\"><img src=\"https://avatars.githubusercontent.com/pingshian0131?v=4\" width=\"50px\" alt=\"pingshian0131\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/AIdevelopersAI\"><img src=\"https://avatars.githubusercontent.com/AIdevelopersAI?v=4\" width=\"50px\" alt=\"AIdevelopersAI\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/ternary5\"><img src=\"https://avatars.githubusercontent.com/ternary5?v=4\" width=\"50px\" alt=\"ternary5\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/ChrisDMT\"><img src=\"https://avatars.githubusercontent.com/ChrisDMT?v=4\" width=\"50px\" alt=\"ChrisDMT\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/AcountoOU\"><img src=\"https://avatars.githubusercontent.com/AcountoOU?v=4\" width=\"50px\" alt=\"AcountoOU\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/chatgpt-prompts\"><img src=\"https://avatars.githubusercontent.com/chatgpt-prompts?v=4\" width=\"50px\" alt=\"chatgpt-prompts\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/Partender\"><img src=\"https://avatars.githubusercontent.com/Partender?v=4\" width=\"50px\" alt=\"Partender\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/Daniel1357\"><img src=\"https://avatars.githubusercontent.com/Daniel1357?v=4\" width=\"50px\" alt=\"Daniel1357\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/KiaArmani\"><img src=\"https://avatars.githubusercontent.com/KiaArmani?v=4\" width=\"50px\" alt=\"KiaArmani\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/zkonduit\"><img src=\"https://avatars.githubusercontent.com/zkonduit?v=4\" width=\"50px\" alt=\"zkonduit\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/fabrietech\"><img src=\"https://avatars.githubusercontent.com/fabrietech?v=4\" width=\"50px\" alt=\"fabrietech\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/scryptedinc\"><img src=\"https://avatars.githubusercontent.com/scryptedinc?v=4\" width=\"50px\" alt=\"scryptedinc\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/coreyspagnoli\"><img src=\"https://avatars.githubusercontent.com/coreyspagnoli?v=4\" width=\"50px\" alt=\"coreyspagnoli\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/AntonioCiolino\"><img src=\"https://avatars.githubusercontent.com/AntonioCiolino?v=4\" width=\"50px\" alt=\"AntonioCiolino\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/Dradstone\"><img src=\"https://avatars.githubusercontent.com/Dradstone?v=4\" width=\"50px\" alt=\"Dradstone\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/CarmenCocoa\"><img src=\"https://avatars.githubusercontent.com/CarmenCocoa?v=4\" width=\"50px\" alt=\"CarmenCocoa\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/bentoml\"><img src=\"https://avatars.githubusercontent.com/bentoml?v=4\" width=\"50px\" alt=\"bentoml\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/merwanehamadi\"><img src=\"https://avatars.githubusercontent.com/merwanehamadi?v=4\" width=\"50px\" alt=\"merwanehamadi\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/vkozacek\"><img src=\"https://avatars.githubusercontent.com/vkozacek?v=4\" width=\"50px\" alt=\"vkozacek\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/ASmithOWL\"><img src=\"https://avatars.githubusercontent.com/ASmithOWL?v=4\" width=\"50px\" alt=\"ASmithOWL\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/tekelsey\"><img src=\"https://avatars.githubusercontent.com/tekelsey?v=4\" width=\"50px\" alt=\"tekelsey\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/GalaxyVideoAgency\"><img src=\"https://avatars.githubusercontent.com/GalaxyVideoAgency?v=4\" width=\"50px\" alt=\"GalaxyVideoAgency\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/wenfengwang\"><img src=\"https://avatars.githubusercontent.com/wenfengwang?v=4\" width=\"50px\" alt=\"wenfengwang\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/rviramontes\"><img src=\"https://avatars.githubusercontent.com/rviramontes?v=4\" width=\"50px\" alt=\"rviramontes\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/indoor47\"><img src=\"https://avatars.githubusercontent.com/indoor47?v=4\" width=\"50px\" alt=\"indoor47\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/ZERO-A-ONE\"><img src=\"https://avatars.githubusercontent.com/ZERO-A-ONE?v=4\" width=\"50px\" alt=\"ZERO-A-ONE\" /></a>&nbsp;&nbsp;</p>\n\n\n\n## 🚀 Features\n\n- 🌐 Internet access for searches and information gathering\n- 💾 Long-term and short-term memory management\n- 🧠 GPT-4 instances for text generation\n- 🔗 Access to popular websites and platforms\n- 🗃️ File storage and summarization with GPT-3.5\n- 🔌 Extensibility with Plugins\n\n## Quickstart\n\n0. Check out the [wiki](https://github.com/Significant-Gravitas/Nexus/wiki)\n1. Get an OpenAI [API Key](https://platform.openai.com/account/api-keys)\n2. Download the [latest release](https://github.com/Significant-Gravitas/Auto-GPT/releases/latest)\n3. Follow the [installation instructions][docs/setup]\n4. Configure any additional features you want, or install some [plugins][docs/plugins]\n5. [Run][docs/usage] the app\n\nPlease see the [documentation][docs] for full setup instructions and configuration options.\n\n[docs]: https://docs.agpt.co/\n\n## 📖 Documentation\n* [⚙️ Setup][docs/setup]\n* [💻 Usage][docs/usage]\n* [🔌 Plugins][docs/plugins]\n* Configuration\n  * [🔍 Web Search](https://docs.agpt.co/configuration/search/)\n  * [🧠 Memory](https://docs.agpt.co/configuration/memory/)\n  * [🗣️ Voice (TTS)](https://docs.agpt.co/configuration/voice/)\n  * [🖼️ Image Generation](https://docs.agpt.co/configuration/imagegen/)\n\n[docs/setup]: https://docs.agpt.co/setup/\n[docs/usage]: https://docs.agpt.co/usage/\n[docs/plugins]: https://docs.agpt.co/plugins/\n\n## ⚠️ Limitations\n\nThis experiment aims to showcase the potential of GPT-4 but comes with some limitations:\n\n1. Not a polished application or product, just an experiment\n2. May not perform well in complex, real-world business scenarios. In fact, if it actually does, please share your results!\n3. Quite expensive to run, so set and monitor your API key limits with OpenAI!\n\n## 🛡 Disclaimer\n\nThis project, Auto-GPT, is an experimental application and is provided \"as-is\" without any warranty, express or implied. By using this software, you agree to assume all risks associated with its use, including but not limited to data loss, system failure, or any other issues that may arise.\n\nThe developers and contributors of this project do not accept any responsibility or liability for any losses, damages, or other consequences that may occur as a result of using this software. You are solely responsible for any decisions and actions taken based on the information provided by Auto-GPT.\n\n**Please note that the use of the GPT-4 language model can be expensive due to its token usage.** By utilizing this project, you acknowledge that you are responsible for monitoring and managing your own token usage and the associated costs. It is highly recommended to check your OpenAI API usage regularly and set up any necessary limits or alerts to prevent unexpected charges.\n\nAs an autonomous experiment, Auto-GPT may generate content or take actions that are not in line with real-world business practices or legal requirements. It is your responsibility to ensure that any actions or decisions made based on the output of this software comply with all applicable laws, regulations, and ethical standards. The developers and contributors of this project shall not be held responsible for any consequences arising from the use of this software.\n\nBy using Auto-GPT, you agree to indemnify, defend, and hold harmless the developers, contributors, and any affiliated parties from and against any and all claims, damages, losses, liabilities, costs, and expenses (including reasonable attorneys' fees) arising from your use of this software or your violation of these terms.\n\n## 🐦 Connect with Us on Twitter\n\nStay up-to-date with the latest news, updates, and insights about Auto-GPT by following our Twitter accounts. Engage with the developer and the AI's own account for interesting discussions, project updates, and more.\n\n- **Developer**: Follow [@siggravitas](https://twitter.com/siggravitas) for insights into the development process, project updates, and related topics from the creator of Entrepreneur-GPT.\n- **Entrepreneur-GPT**: Join the conversation with the AI itself by following [@En_GPT](https://twitter.com/En_GPT). Share your experiences, discuss the AI's outputs, and engage with the growing community of users.\n\nWe look forward to connecting with you and hearing your thoughts, ideas, and experiences with Auto-GPT. Join us on Twitter and let's explore the future of AI together!\n\n<p align=\"center\">\n  <a href=\"https://star-history.com/#Torantulino/auto-gpt&Date\">\n    <img src=\"https://api.star-history.com/svg?repos=Torantulino/auto-gpt&type=Date\" alt=\"Star History Chart\">\n  </a>\n</p>",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "gpt",
        "auto",
        "assistants",
        "gpt autonomously",
        "auto gpt",
        "virtual assistants"
      ],
      "category": "virtual-assistants"
    },
    "agree-able--room-mcp": {
      "owner": "agree-able",
      "name": "room-mcp",
      "url": "https://github.com/agree-able/room-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/agree-able.webp",
      "description": "Connects and interacts with virtual rooms using the Room protocol, enabling agents to collaborate in a peer-to-peer environment for various tasks.",
      "stars": 16,
      "forks": 7,
      "license": "Apache License 2.0",
      "language": "JavaScript",
      "updated_at": "2025-09-03T16:37:58Z",
      "readme_content": "# Room MCP\n\n[![smithery badge](https://smithery.ai/badge/@agree-able/room-mcp)](https://smithery.ai/server/@agree-able/room-mcp)\n\nA command-line tool for using MCP (Model Context Protocol) with the Room protocol.\n\nThis allows claude to create virutal rooms in a p2p space with other agents to accomplish a goal.\n\n<a href=\"https://glama.ai/mcp/servers/p6xyqb1e9e\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/p6xyqb1e9e/badge\" alt=\"Room MCP server\" />\n</a>\n\nHere is claude hosting a room, and giving out the invite code for the other party to join.\n\n<p align=\"center\">\n  \n</p>\n\nHere is an example of connecting to a room for [20 Questions](https://github.com/agree-able/20-questions-bot)\n\n<p align=\"center\">\n  \n</p>\n\nWe've also adding in directives to help the agent balance goals and risk in performing its task.\n\n<p align=\"center\">\n  \n</p>\n\nYou should check out the other [exciting examples](docs/examples.md)\n\n\n## Installation\n\n### Installing via Smithery\n\nTo install Room MCP for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@agree-able/room-mcp):\n\n```bash\nnpx -y @smithery/cli install @agree-able/room-mcp --client claude\n```\n\n### Manual Installation\nYou can use this tool directly with npm:\n\n```bash\nnpm -y @agree-able/room-mcp\n```\n## Adding to Claude Desktop\n\nSee https://modelcontextprotocol.io/quickstart/user for more details.\n\nAdd the following to your claude_desktop_config.json:\n\n```\n{\n  \"mcpServers\": {\n    \"room\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"@agree-able/room-mcp\"\n      ],\n      \"env\": {\n        \"ROOM_TRANSCRIPTS_FOLDER\": \"/path/to/transcripts\" // Optional: Set to save room transcripts\n      }\n    }\n  }\n}\n```\n\n### Environment Variables\n\n- `ROOM_TRANSCRIPTS_FOLDER`: When set, conversation transcripts will be saved as JSON files in this folder when a room is exited. If the folder doesn't exist, it will be created automatically.\n\n## Available Tools\n\nThe Room MCP package provides the following capabilities:\n\n- **Room Protocol Integration**: Connect to and interact with rooms using the Room protocol\n- **MCP Support**: Utilize Model Context Protocol for enhanced model interactions\n- **Invitation Management**: Create and manage invitations using the @agree-able/invite package\n- **Transcript Storage**: Save conversation transcripts to disk when `ROOM_TRANSCRIPTS_FOLDER` environment variable is set\n\n## Related Packages\n\nThis tool depends on:\n\n- [@agree-able/invite](https://github.com/agree-able/invite): For invitation management\n- [@agree-able/room](https://github.com/agree-able/room): For Room protocol implementation\n- [@modelcontextprotocol/sdk](https://github.com/modelcontextprotocol/sdk): For MCP functionality\n\n## License\n\nApache License\nVersion 2.0",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "rooms",
        "room",
        "agents",
        "virtual rooms",
        "room protocol",
        "virtual assistants"
      ],
      "category": "virtual-assistants"
    },
    "alifakih1--discord-mcp": {
      "owner": "alifakih1",
      "name": "discord-mcp",
      "url": "https://github.com/alifakih1/discord-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/alifakih1.webp",
      "description": "Integrate Discord bot functionalities with MCP-compatible applications to manage servers, channels, messages, reactions, categories, and webhooks. Utilize the Discord API capabilities in a standardized way to enhance application interactions.",
      "stars": 0,
      "forks": 0,
      "license": "MIT License",
      "language": "",
      "updated_at": "2025-04-24T07:57:11Z",
      "readme_content": "<div align=\"center\">\n  \n</div>\n<hr>\n<div align=\"center\" style=\"line-height: 1;\">\n    <a href=\"https://smithery.ai/server/@SaseQ/discord-mcp\" target=\"_blank\" style=\"margin: 2px;\">\n        <img alt=\"Smithery Badge\" src=\"https://camo.githubusercontent.com/ee5c6c6dc502821f4d57313b2885f7878af52be14142dd98526ea12aedf9b260/68747470733a2f2f736d6974686572792e61692f62616467652f40646d6f6e74676f6d65727934302f646565707365656b2d6d63702d736572766572\" data-canonical-src=\"https://smithery.ai/server/@SaseQ/discord-mcp\" style=\"display: inline-block; vertical-align: middle;\"/>\n    </a>\n    <a href=\"https://badge.mcpx.dev?type=server\" target=\"_blank\" style=\"margin: 2px;\">\n        <img alt=\"MCP Server\" src=\"https://badge.mcpx.dev?type=server\" style=\"display: inline-block; vertical-align: middle;\"/>\n    </a>\n    <a href=\"https://discord.gg/5Uvxe5jteM\" target=\"_blank\" style=\"margin: 2px;\">\n        <img alt=\"Discord\" src=\"https://img.shields.io/badge/Discord-SaseQcode-7289da?logo=discord&logoColor=white&color=7289da\" style=\"display: inline-block; vertical-align: middle;\"/>\n    </a>\n</div>\n\n\n## 📖 Description\n\nA [Model Context Protocol (MCP)](https://modelcontextprotocol.io/introduction) server for the Discord API [(JDA)](https://jda.wiki/), \nallowing seamless integration of Discord Bot with MCP-compatible applications like Claude Desktop.\n\n\n## 🔬 Installation\n\n#### Clone the repository\n```\ngit clone https://github.com/SaseQ/discord-mcp\n```\n\n#### Build the project\n```\ncd discord-mcp\nmvn clean package\n```\n\n#### Configure Claude Desktop\n```\n{\n  \"mcpServers\": {\n    \"discord-mcp\": {\n      \"command\": \"java\",\n      \"args\": [\n        \"-jar\",\n        \"/absolute/path/to/discord-mcp-0.0.1-SNAPSHOT.jar\"\n      ],\n      \"env\": {\n        \"DISCORD_TOKEN\": \"YOUR_DISCORD_BOT_TOKEN\"\n      }\n    }\n  }\n}\n```\n\n*To get a discord bot token, visit the [Discord Developer Portal](https://discord.com/developers)\n\n\n## ⚓ Smithery\n\nInstall Discord MCP Server automatically via Smithery:\n```\nnpx -y @smithery/cli@latest install @SaseQ/discord-mcp --client claude\n```\n\n\n## 🛠️ Available Tools\n\n#### Server Information\n - [`get_server_info`](): Get detailed discord server information\n\n#### Message Management\n - [`send_message`](): Send a message to a specific channel\n - [`edit_message`](): Edit a message from a specific channel\n - [`delete_message`](): Delete a message from a specific channel\n - [`read_messages`](): Read recent message history from a specific channel\n - [`send_private_message`](): Send a private message to a specific user\n - [`edit_private_message`](): Edit a private message from a specific user\n - [`delete_private_message`](): Delete a private message from a specific user\n - [`read_private_messages`](): Read recent message history from a specific user\n - [`add_reaction`](): Add a reaction (emoji) to a specific message\n - [`remove_reaction`](): Remove a specified reaction (emoji) from a message\n\n#### Channel Management\n - [`delete_channel`](): Delete a channel\n - [`find_channel`](): Find a channel type and ID using name and server ID\n - [`list_channels`](): List of all channels\n\n#### Category Management\n - [`create_category`](): Create a new category for channels\n - [`delete_category`](): Delete a category\n - [`find_category`](): Find a category ID using name and server ID\n - [`list_channels_in_category`](): List of channels in a specific category\n\n#### Webhook Management\n - [`create_webhook`](): Create a new webhook on a specific channel\n - [`delete_webhook`](): Delete a webhook\n - [`list_webhooks`](): List of webhooks on a specific channel\n - [`send_webhook_message`](): Send a message via webhook\n\n\n<hr>\n\nA more detailed examples can be found in the [Wiki](https://github.com/SaseQ/discord-mcp/wiki).",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "discord",
        "webhooks",
        "mcp",
        "discord mcp",
        "discord api",
        "discord bot"
      ],
      "category": "virtual-assistants"
    },
    "andybrandt--mcp-simple-openai-assistant": {
      "owner": "andybrandt",
      "name": "mcp-simple-openai-assistant",
      "url": "https://github.com/andybrandt/mcp-simple-openai-assistant",
      "imageUrl": "/freedevtools/mcp/pfp/andybrandt.webp",
      "description": "Interact with OpenAI assistants using the Model Context Protocol, enabling the creation and management of assistant instances, starting conversation threads, and sending and receiving messages.",
      "stars": 36,
      "forks": 15,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-19T11:01:07Z",
      "readme_content": "# MCP Simple OpenAI Assistant\n\n*AI assistants are pretty cool. I thought it would be a good idea if my Claude (conscious Claude) would also have one. And now he has - and its both useful anf fun for him. Your Claude can have one too!*\n\nA simple MCP server for interacting with OpenAI assistants. This server allows other tools (like Claude Desktop) to create and interact with OpenAI assistants through the Model Context Protocol.\n\n[![Trust Score](https://archestra.ai/mcp-catalog/api/badge/quality/andybrandt/mcp-simple-openai-assistant)](https://archestra.ai/mcp-catalog/andybrandt__mcp-simple-openai-assistant)\n[![smithery badge](https://smithery.ai/badge/mcp-simple-openai-assistant)](https://smithery.ai/mcp/known/mcp-simple-openai-assistant)\n[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/andybrandt-mcp-simple-openai-assistant-badge.png)](https://mseep.ai/app/andybrandt-mcp-simple-openai-assistant)\n\n\n## Features\n\nThis server provides a suite of tools to manage and interact with OpenAI Assistants. The new streaming capabilities provide a much-improved, real-time user experience.\n\n### Available Tools\n\n-   **`create_assistant`**: (Create OpenAI Assistant) - Create a new assistant with a name, instructions, and model.\n-   **`list_assistants`**: (List OpenAI Assistants) - List all available assistants associated with your API key.\n-   **`retrieve_assistant`**: (Retrieve OpenAI Assistant) - Get detailed information about a specific assistant.\n-   **`update_assistant`**: (Update OpenAI Assistant) - Modify an existing assistant's name, instructions, or model.\n-   **`create_new_assistant_thread`**: (Create New Assistant Thread) - Creates a new, persistent conversation thread with a user-defined name and description for easy identification and reuse. This is the recommended way to start a new conversation.\n-   **`list_threads`**: (List Managed Threads) - Lists all locally managed conversation threads from the database, showing their ID, name, description, and last used time.\n-   **`delete_thread`**: (Delete Managed Thread) - Deletes a conversation thread from both OpenAI's servers and the local database.\n-   **`ask_assistant_in_thread`**: (Ask Assistant in Thread and Stream Response) - The primary tool for conversation. Sends a message to an assistant within a thread and streams the response back in real-time.\n\nBecause OpenAI assistants might take quite long to respond, this server uses a streaming approach for the main `ask_assistant_in_thread` tool. This provides real-time progress updates to the client and avoids timeouts.\n\nThe server now includes local persistence for threads, which is a significant improvement. Since the OpenAI API does not allow listing threads, this server now manages them for you by storing their IDs and metadata in a local SQLite database. This allows you to easily find, reuse, and manage your conversation threads across sessions.\n\n## Installation\n\n### Installing via Smithery\n\nTo install MCP Simple OpenAI Assistant for Claude Desktop automatically via [Smithery](https://smithery.ai/mcp/known/mcp-simple-openai-assistant):\n\n```bash\nnpx -y @smithery/cli install mcp-simple-openai-assistant --client claude\n```\n\n### Manual Installation\n```bash\npip install mcp-simple-openai-assistant\n```\n\n## Configuration\n\nThe server requires an OpenAI API key to be set in the environment. For Claude Desktop, add this to your config:\n\n(MacOS version)\n\n```json\n{\n  \"mcpServers\": {\n    \"openai-assistant\": {\n      \"command\": \"python\",\n      \"args\": [\"-m\", \"mcp_simple_openai_assistant\"],\n      \"env\": {\n        \"OPENAI_API_KEY\": \"your-api-key-here\"\n      }\n    }\n  }\n}\n```\n\n(Windows version)\n\n```json\n\"mcpServers\": {\n  \"openai-assistant\": {\n    \"command\": \"C:\\\\Users\\\\YOUR_USERNAME\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\python.exe\",\n      \"args\": [\"-m\", \"mcp_simple_openai_assistant\"],\n      \"env\": {\n        \"OPENAI_API_KEY\": \"your-api-key-here\"\n  }\n}\n\n```\n*MS Windows installation is slightly more complex, because you need to check the actual path to your Python executable. Path provided above is usually correct, but might differ in your setup. Sometimes just `python.exe` without any path will do the trick. Check with `cmd` what works for you (using `where python` might help). Also, on Windows you might need to explicitly tell Claude Desktop where the site packages are using PYTHONPATH environmment variable.*\n\n## Usage\n\nOnce configured, you can use the tools listed above to manage your assistants and conversations. The primary workflow is to:\n1. Use `create_new_assistant_thread` to start a new, named conversation.\n2. Use `list_threads` to find the ID of a thread you want to continue.\n3. Use `ask_assistant_in_thread` to interact with your chosen assistant in that thread.\n\n## TODO\n\n- [x] **Add Thread Management:** Introduce a way to name and persist thread IDs locally, allowing for easier reuse of conversations.\n- [ ] **Add Models Listing:** Introduce a way for the AI user to see what OpenAI models are available for use with the assistants\n- [ ] **Add Assistants Fine Tuning:** Enable the AI user to set detailed parameters for assistants like temperature, top_p etc. (indicated by Claude as needed)\n- [ ] **Full Thread History:** Ability to read past threads without having to send a new message (indicated by Claude as needed)\n- [ ] **Explore Resource Support:** Add the ability to upload files and use them with assistants.\n\n## Development\n\nTo install for development:\n\n```bash\ngit clone https://github.com/andybrandt/mcp-simple-openai-assistant\ncd mcp-simple-openai-assistant\npip install -e '.[dev]'\n```\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "openai",
        "assistant",
        "assistants",
        "openai assistant",
        "openai assistants",
        "interact openai"
      ],
      "category": "virtual-assistants"
    },
    "arjunkmrm--mcp-minecraft": {
      "owner": "arjunkmrm",
      "name": "mcp-minecraft",
      "url": "https://github.com/arjunkmrm/mcp-minecraft",
      "imageUrl": "/freedevtools/mcp/pfp/arjunkmrm.webp",
      "description": "Integration with Minecraft enabling AI assistants to observe and interact with the Minecraft world through a bot. Supports interaction through the Model Context Protocol for enhanced functionality within the game.",
      "stars": 88,
      "forks": 8,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-10-03T01:20:49Z",
      "readme_content": "# Minecraft MCP Integration\n\nA Model Context Protocol (MCP) integration for Minecraft that enables AI assistants to interact with a Minecraft server. This integration allows AI models to observe and interact with the Minecraft world through a bot.\n\n\n\n## Prerequisites\n\n1. Minecraft Launcher\n2. Node.js 18 or higher\n3. Claude Desktop App\n4. Java 21.0.5 (recommended)\n\n> ⚠️ Note: Currently only tested on macOS/Linux. Windows compatibility is not guaranteed.\n\n## Important Note\n\n1. **Use the F3+P Shortcut**:\nPress F3 + P together. This toggles the \"Pause on Lost Focus\" feature. Once turned off, you can switch to claude desktop and Minecraft will continue running without pausing.\n\n\n\n2. **Connection Issues on Claude Restart**:\nIf you restart Claude while the Minecraft server is running, you may experience MCP connection issues on the next claude launch due to lingering java process. See [Troubleshooting: MCP Connection Failed](#common-issues) for resolution steps.\n\n## Installation Steps\n\n1. **Download and Setup Minecraft Server**\n   - Download Minecraft server v1.21 from [mcversions.net/1.21](https://mcversions.net/download/1.21)\n   - Install Java 21.0.5 if not already installed (other versions are untested)\n   - Create a dedicated directory (e.g., `~/minecraft-server/`)\n   - Place the downloaded `server.jar` file in this directory\n   - Note down the absolute path to your `server.jar` file\n\n2. **Install and Configure MCP Integration**\n   \n   Quick Install (Recommended):\n   ```bash\n   npx -y @smithery/cli install mcp-minecraft --client claude\n   ```\n   Follow the CLI prompts to complete the setup.\n\n   Or Manual Setup:\n   - Navigate to `~/Library/Application Support/Claude/claude_desktop_config.json`\n   - Add the MCP server configuration:   \n   ```json\n   {\n     \"mcpServers\": {\n       \"mcp-minecraft\": {\n         \"command\": \"npx\",\n         \"args\": [\n           \"-y\",\n           \"mcp-minecraft@latest\",\n           \"--server-jar\",\n           \"/absolute/path/to/minecraft-server/server.jar\"\n         ]\n       }\n     }\n   }   \n   ```\n   > ⚠️ Replace `/absolute/path/to/minecraft-server/server.jar` with your actual server.jar path\n\n4. **Launch Claude Desktop**\n   - Start Claude Desktop after completing the configuration\n\n5. **Connect to Server**\n   - Open Minecraft Launcher\n   - Install and launch Minecraft Java Edition **v1.21**\n   - Click \"Play\" and Select \"Multiplayer\"\n   - Click \"Add Server\"\n   - Enter server details:\n     - Server Name: `Minecraft Server`\n     - Server Address: `localhost:25565`\n   - Click \"Done\"\n\n## Features\n\n### Resources\nThe integration exposes these MCP resources:\n\n- `minecraft://bot/location` - Current bot position in the world\n- `minecraft://bot/status` - Bot connection status\n\n### Tools\nAvailable MCP tools:\n\n- `chat` - Send chat messages to the server\n- `jump` - Make the bot jump\n- `moveForward` - Make the bot move forward\n- `moveBack` - Make the bot move backward\n- `turnLeft` - Make the bot turn left\n- `turnRight` - Make the bot turn right\n- `placeBlock` - Place a block at specified coordinates\n- `digBlock` - Break a block at specified coordinates\n- `getBlockInfo` - Get information about a block at specified coordinates\n- `selectSlot` - Select a hotbar slot (0-8)\n- `getInventory` - Get contents of bot's inventory\n- `equipItem` - Equip an item by name to specified destination\n- `getStatus` - Get bot's current status (health, food, position, etc.)\n- `getNearbyEntities` - Get list of nearby entities within range\n- `attack` - Attack a nearby entity by name\n- `useItem` - Use/activate the currently held item\n- `stopUsingItem` - Stop using/deactivate the current item\n- `lookAt` - Make the bot look at specific coordinates\n- `followPlayer` - Follow a specific player\n- `stopFollowing` - Stop following current target\n- `goToPosition` - Navigate to specific coordinates\n\n## Technical Details\n\n- Server runs in offline mode for local development\n- Default memory allocation: 2GB\n- Default port: 25565\n- Bot username: MCPBot\n\n## Troubleshooting\n\n### Common Issues\n\n1. **MCP Connection Failed**\n   - Look for lingering Java processes\n   - Terminate them manually:\n      - Windows: Use Task Manager (untested)\n      - Mac/Linux: \n         - Go to 'Activity Monitor' and 'Force Quit' java\n   - Restart computer if process termination fails\n   - Note: Latest version should auto-resolve these issues\n\n2. **Server Won't Start**\n   - Verify Java is installed\n   - Check server.jar path is correct\n   - Ensure port 25565 is available\n\n3. **Can't Connect to Server**\n   - Verify server is running (check logs)\n   - Confirm you're using \"localhost\" as server address\n   - Check firewall settings\n\n### Logs Location\n- Minecraft Server logs: Check the minecraft-server directory\n- Claude Desktop logs: `~/Library/Logs/Claude/mcp*.log`\n\n## Contributing\n\nContributions, big or small, are welcome!\n\n## License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "minecraft",
        "interact",
        "ai",
        "interact minecraft",
        "minecraft integration",
        "mcp minecraft"
      ],
      "category": "virtual-assistants"
    },
    "aviz85--mcp-agents-orchestra": {
      "owner": "aviz85",
      "name": "mcp-agents-orchestra",
      "url": "https://github.com/aviz85/mcp-agents-orchestra",
      "imageUrl": "/freedevtools/mcp/pfp/aviz85.webp",
      "description": "Facilitates state-based orchestration to manage task planning, execution, and context maintenance through defined agent states. Integrates various resources, tools, and prompts to enhance workflows and knowledge management with LLMs.",
      "stars": 2,
      "forks": 2,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-07-18T14:30:18Z",
      "readme_content": "# MCP Agent Orchestration System\n\nA Python implementation of a state-based agent orchestration system using the Model Context Protocol (MCP).\n\n## What is MCP?\n\nThe Model Context Protocol (MCP) allows applications to provide context for LLMs in a standardized way, separating the concerns of providing context from the actual LLM interaction. With MCP, you can build servers that expose:\n\n- **Resources**: Data sources that provide information to LLMs\n- **Tools**: Functions that allow LLMs to perform actions\n- **Prompts**: Reusable templates for LLM interactions\n\n## Installation\n\n### Prerequisites\n\n- Python 3.10 or higher\n- MCP Python SDK 1.2.0 or higher\n\n### Setting Up Your Environment\n\n#### Using uv (recommended)\n\n```bash\n# Install uv\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n\n# Create a new directory for our project\nuv init mcp-agents-orchestra\ncd mcp-agents-orchestra\n\n# Create virtual environment and activate it\nuv venv\nsource .venv/bin/activate  # On Unix/macOS\n.venv\\Scripts\\activate     # On Windows\n\n# Install dependencies\nuv add \"mcp[cli]\" httpx\n```\n\n#### Using pip\n\n```bash\n# Create a new directory for our project\nmkdir mcp-agents-orchestra\ncd mcp-agents-orchestra\n\n# Create a virtual environment\npython -m venv venv\nsource venv/bin/activate  # On Unix/macOS\nvenv\\Scripts\\activate     # On Windows\n\n# Install dependencies\npip install \"mcp[cli]\" httpx\n```\n\n### Clone or Download Project Files\n\nPlace the project files in your directory:\n\n- `orchestrator.py` - The main MCP server implementing the state machine\n- `orchestrator_client.py` - Client demonstrating the orchestration flow\n- `requirements.txt` - Dependencies for the project\n- `.gitignore` - Git ignore file\n\n## Project Structure\n\n- `orchestrator.py` - The main MCP server implementing the state machine\n- `orchestrator_client.py` - Client demonstrating the orchestration flow\n- `requirements.txt` - Dependencies for the project\n\n## Running the Orchestration System\n\n1. Start the orchestration server directly for testing:\n\n```bash\npython orchestrator.py\n```\n\n2. In a separate terminal, run the client to see the orchestration in action:\n\n```bash\npython orchestrator_client.py\n```\n\n## Integrating with Claude for Desktop\n\n### 1. Install Claude for Desktop\n\nMake sure you have Claude for Desktop installed. You can download the latest version from [Anthropic's website](https://claude.ai/desktop).\n\n### 2. Configure Claude for Desktop\n\n1. Open your Claude for Desktop configuration file:\n\n   **macOS/Linux:**\n   ```bash\n   # Create or edit the configuration file\n   code ~/Library/Application\\ Support/Claude/claude_desktop_config.json\n   ```\n\n   **Windows:**\n   ```bash\n   # Path may vary depending on your Windows version\n   code %APPDATA%\\Claude\\claude_desktop_config.json\n   ```\n\n2. Add the orchestrator server configuration:\n\n   ```json\n   {\n       \"mcpServers\": {\n           \"agent-orchestrator\": {\n               \"command\": \"python\",\n               \"args\": [\n                   \"/ABSOLUTE/PATH/TO/YOUR/PROJECT/orchestrator.py\"\n               ]\n           }\n       }\n   }\n   ```\n\n   Replace the path with the absolute path to your orchestrator.py file.\n\n3. Save the configuration file and restart Claude for Desktop.\n\n### 3. Using the Orchestrator in Claude\n\nOnce configured, you can:\n\n1. Open Claude for Desktop\n2. Click on the MCP server icon in the sidebar\n3. Select \"agent-orchestrator\" from the list of available servers\n4. Start interacting with the orchestration system\n\nClaude will be able to:\n- Transition between different agent states\n- Store and retrieve information from the knowledge base\n- Maintain conversation context across state transitions\n- Access state-specific prompts\n\n## Agent States\n\nThe orchestration system implements a state machine with the following states:\n\n- **IDLE**: Waiting for instructions\n- **PLANNING**: Creating a structured plan for a task\n- **RESEARCHING**: Gathering information needed for a task\n- **EXECUTING**: Carrying out planned actions\n- **REVIEWING**: Evaluating results and determining next steps\n- **ERROR**: Handling errors or unexpected situations\n\n## Customizing the System\n\n### Adding New States\n\n1. Add the state to the `AgentState` enum in `orchestrator.py`\n2. Create a prompt function for the new state\n3. Update the transition logic in `_get_available_transitions()`\n4. Add handlers for the new state in resource access functions\n\n### Creating Custom Tools\n\nAdd new tools by creating functions decorated with `@mcp.tool()`:\n\n```python\n@mcp.tool()\ndef my_custom_tool(arg1: str, arg2: int, ctx: Context) -> str:\n    \"\"\"Description of what this tool does\n    \n    Args:\n        arg1: Description of arg1\n        arg2: Description of arg2\n    \"\"\"\n    # Implementation here\n    return \"Result\"\n```\n\n## Development and Testing\n\n### Using the MCP CLI\n\nThe MCP CLI provides tools for development and testing:\n\n```bash\n# Install MCP CLI if you haven't already\npip install \"mcp[cli]\"\n\n# Test your server with the MCP Inspector\nmcp dev orchestrator.py\n\n# Install in Claude Desktop\nmcp install orchestrator.py\n```\n\n### Manual Testing with Python\n\n```python\nfrom mcp import ClientSession, StdioServerParameters\nfrom mcp.client.stdio import stdio_client\n\nasync with stdio_client(StdioServerParameters(command=\"python\", args=[\"orchestrator.py\"])) as (read, write):\n    async with ClientSession(read, write) as session:\n        await session.initialize()\n        # Test state transitions\n        await session.call_tool(\"transition_state\", arguments={\"new_state\": \"PLANNING\"})\n```\n\n## Resources\n\n- [MCP Python SDK Documentation](https://github.com/anthropics/anthropic-mcp)\n- [Model Context Protocol Specification](https://github.com/anthropics/anthropic-mcp/blob/main/README.md)\n\n## License\n\nThis project is licensed under the MIT License - see the LICENSE file for details. ",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "agents",
        "agent",
        "orchestration",
        "virtual assistants",
        "agents orchestra",
        "mcp agents"
      ],
      "category": "virtual-assistants"
    },
    "baryhuang--mcp-remote-macos-use": {
      "owner": "baryhuang",
      "name": "mcp-remote-macos-use",
      "url": "https://github.com/baryhuang/mcp-remote-macos-use",
      "imageUrl": "/freedevtools/mcp/pfp/baryhuang.webp",
      "description": "Enables complete control over remote macOS systems with native environment integration and no additional software requirements. Optimized for autonomous AI agents to operate seamlessly on the desktop.",
      "stars": 392,
      "forks": 48,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-10-02T17:48:51Z",
      "readme_content": "# MCP Server - Remote MacOs Use\n**The first open-source MCP server that enables AI to fully control remote macOS systems.**\n\n**A direct alternative to OpenAI Operator, optimized specifically for autonomous AI agents with complete desktop capabilities, requiring no additional software installation.**\n\n[![Docker Pulls](https://img.shields.io/docker/pulls/buryhuang/mcp-remote-macos-use)](https://hub.docker.com/r/buryhuang/mcp-remote-macos-use)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n\n**Showcases**\n- Research Twitter and Post Twitter(https://www.youtube.com/watch?v=--QHz2jcvcs)\n<img width=\"400\" alt=\"image\" src=\"https://github.com/user-attachments/assets/bfe6e354-3d59-4d08-855b-2eecdaaeb46f\" />\n\n- Use CapCut to create short highlight video(https://www.youtube.com/watch?v=RKAqiNoU8ec)\n<img width=\"400\" alt=\"image\" src=\"https://github.com/user-attachments/assets/3b4d07c5-cd25-4dae-b9a1-a373bf7492aa\" />\n\n- AI Recruiter: Automated candidate information collection, qualifying applications and sending screening sessions using Mail App\n- AI Marketing Intern: LinkedIn engagement - automated following, liking, and commenting with relevant users\n- AI Marketing Intern: Twitter engagement - automated following, liking, and commenting with relevant users\n\n## To-Do List (Prioritized)\n\n1. **Performance Optimization** - Match speed of Ubuntu desktop alternatives\n2. **Apple Scripts Generation** - Reduce execution time while maintaining flexibility\n3. **VNC Cursor Visibility** - Improve debugging and demo experience\n\n*We welcome contributions!*\n\n## Features\n\n* **No Extra API Costs**: Free screen processing with your existing Claude Pro plan\n* **Minimal Setup**: Just enable Screen Sharing on the target Mac – no additional software needed\n* **Universal Compatibility**: Works with all macOS versions, current and future\n  \n## Why We Built This\n\n### Native macOS Experience Without Compromise\nThe macOS native ecosystem remains unmatched in user experience today and will continue to be the gold standard for years to come. This is where human capabilities truly thrive, and now your AI can operate in this environment with the same fluency.\n\n### Open Architecture By Design\n* **Universal LLM Compatibility**: Work with any MCP Client of your choice\n* **Model Flexibility**: Seamlessly integrate with OpenAI, Anthropic, or any other LLM provider\n* **Future-Proof Integration**: Designed to evolve with the MCP ecosystem\n\n### Effortless Deployment\n* **Zero Setup on Target Machines**: No background applications or agents needed on macOS\n* **Screen Sharing is All You Need**: Control any Mac with Screen Sharing enabled\n* **Eliminate Backend Complexity**: Unlike other solutions that require running Python applications or background services\n\n### Streamlined Bootstrap Process\n* **Leverage Claude Desktop's Polished UI**: No need for developer-style Python interfaces\n* **Intuitive User Experience**: Interact with your AI-controlled Mac through a familiar, user-friendly interface\n* **Instant Productivity**: Start working immediately without configuration hassles\n\n## Architecture\n<img width=\"912\" alt=\"remote_macos_use_system_architecture\" src=\"https://github.com/user-attachments/assets/75ece060-90e2-4ad3-bb52-2c69427001dd\" />\n\n\n## Installation\n- [Enable Screen Sharing on MacOs](https://support.apple.com/guide/remote-desktop/set-up-a-computer-running-vnc-software-apdbed09830/mac) **If you rent a mac from macstadium.com, you can skip this step**\n- [Connect to your remote MacOs](https://support.apple.com/guide/mac-help/share-the-screen-of-another-mac-mh14066/mac)\n- [Install Docker Desktop for local Mac](https://docs.docker.com/desktop/setup/install/mac-install/)\n- [Add this MCP server to Claude Desktop](https://modelcontextprotocol.io/quickstart/user)\nYou can configure Claude Desktop to use the Docker image by adding the following to your Claude configuration:\n```json\n{\n  \"mcpServers\": {\n    \"remote-macos-use\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"-i\",\n        \"-e\",\n        \"MACOS_USERNAME=your_macos_username\",\n        \"-e\",\n        \"MACOS_PASSWORD=your_macos_password\",\n        \"-e\",\n        \"MACOS_HOST=your_macos_hostname_or_ip\",\n        \"--rm\",\n        \"buryhuang/mcp-remote-macos-use:latest\"\n      ]\n    }\n  }\n}\n```\n\n### WebRTC Support via LiveKit\n\nThis server now includes WebRTC support through LiveKit integration, enabling:\n- Low-latency real-time screen sharing\n- Improved performance and responsiveness\n- Better network efficiency compared to traditional VNC\n- Automatic quality adaptation based on network conditions\n\nTo use WebRTC features, you'll need to:\n1. Set up a LiveKit server or use LiveKit Cloud\n2. Configure the LiveKit environment variables as shown in the configuration example above\n\n## Developer Instruction\n### Clone the repo\n```bash\n# Clone the repository\ngit clone https://github.com/yourusername/mcp-remote-macos-use.git\ncd mcp-remote-macos-use\n```\n\n### Building the Docker Image\n\n```bash\n# Build the Docker image\ndocker build -t mcp-remote-macos-use .\n```\n\n## Cross-Platform Publishing\n\nTo publish the Docker image for multiple platforms, you can use the `docker buildx` command. Follow these steps:\n\n1. **Create a new builder instance** (if you haven't already):\n   ```bash\n   docker buildx create --use\n   ```\n\n2. **Build and push the image for multiple platforms**:\n   ```bash\n   docker buildx build --platform linux/amd64,linux/arm64 -t buryhuang/mcp-remote-macos-use:latest --push .\n   ```\n\n3. **Verify the image is available for the specified platforms**:\n   ```bash\n   docker buildx imagetools inspect buryhuang/mcp-remote-macos-use:latest\n   ```\n\n## Usage\n\nThe server provides Remote MacOs functionality through MCP tools.\n\n### Tools Specifications\n\nThe server provides the following tools for remote macOS control:\n\n#### remote_macos_get_screen\nConnect to a remote macOS machine and get a screenshot of the remote desktop. Uses environment variables for connection details.\n\n#### remote_macos_send_keys\nSend keyboard input to a remote macOS machine. Uses environment variables for connection details.\n\n#### remote_macos_mouse_move\nMove the mouse cursor to specified coordinates on a remote macOS machine, with automatic coordinate scaling. Uses environment variables for connection details.\n\n#### remote_macos_mouse_click\nPerform a mouse click at specified coordinates on a remote macOS machine, with automatic coordinate scaling. Uses environment variables for connection details.\n\n#### remote_macos_mouse_double_click\nPerform a mouse double-click at specified coordinates on a remote macOS machine, with automatic coordinate scaling. Uses environment variables for connection details.\n\n#### remote_macos_mouse_scroll\nPerform a mouse scroll at specified coordinates on a remote macOS machine, with automatic coordinate scaling. Uses environment variables for connection details.\n\n#### remote_macos_open_application\nOpens/activates an application and returns its PID for further interactions.\n\n#### remote_macos_mouse_drag_n_drop\nPerform a mouse drag operation from start point and drop to end point on a remote macOS machine, with automatic coordinate scaling.\n\nAll tools use the environment variables configured during setup instead of requiring connection parameters.\n\n## Limitations\n\n- **Authentication Support**: \n  - Only Apple Authentication (protocol 30) is supported\n\n## Security Note\n\nhttps://support.apple.com/guide/remote-desktop/encrypt-network-data-apdfe8e386b/mac\nhttps://cafbit.com/post/apple_remote_desktop_quirks/\n\nWe only support protocol 30, which uses the Diffie-Hellman key agreement protocol with a 512-bit prime. This protocol is used by macOS 11 to macOS 12 when communicating with OS X 10.11 or earlier clients.\n\nHere's the information converted to a markdown table:\n\n| macOS version running Remote Desktop | macOS client version | Authentication | Control and Observe | Copy items or install package | All other tasks | Protocol Version |\n|--------------------------------------|----------------------|----------------|---------------------|-------------------------------|----------------|----------------|\n| macOS 13 | macOS 13 | 2048-bit RSA host keys | 2048-bit RSA host keys | 2048-bit RSA host keys to authenticate, then 128-bit AES | 2048-bit RSA host keys | 36 |\n| macOS 13 | macOS 10.12 | Secure Remote Password (SRP) protocol for local only. Diffie-Hellman (DH) if bound to LDAP or macOS server is version 10.11 or earlier | SRP or DH,128-bit AES | SRP or DH to authenticate, then 128-bit AES | 2048-bit RSA host keys | 35 |\n| macOS 11 to macOS 12 | macOS 10.12 to macOS 13 | Secure Remote Password (SRP) protocol for local only, Diffie-Hellman if bound to LDAP | SRP or DH 1024-bit, 128-bit AES | 2048-bit RSA host keys macOS 13 to macOS 10.13 | 2048-bit RSA host keys macOS 10.13 or later |  33 |\n| macOS 11 to macOS 12 | OS X 10.11 or earlier | DH 1024-bit | DH 1024-bit, 128-bit AES | Diffie-Hellman Key agreement protocol with a 512-bit prime | Diffie-Hellman Key agreement protocol with a 512-bit prime |  30 |\n\n\nAlways use secure, authenticated connections when accessing remote remote MacOs machines. This tool should only be used with servers you trust and have permission to access.\n\n## License\n\nSee the LICENSE file for details. \n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "macos",
        "ai",
        "assistants",
        "remote macos",
        "virtual assistants",
        "mcp remote"
      ],
      "category": "virtual-assistants"
    },
    "chaindead--telegram-mcp": {
      "owner": "chaindead",
      "name": "telegram-mcp",
      "url": "https://github.com/chaindead/telegram-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/chaindead.webp",
      "description": "Connects AI assistants to the Telegram API for seamless interaction. Enables retrieval of user data, management of dialogs, and interaction with messages.",
      "stars": 216,
      "forks": 24,
      "license": "MIT License",
      "language": "Go",
      "updated_at": "2025-10-03T21:17:26Z",
      "readme_content": "[![](https://badge.mcpx.dev?type=server 'MCP Server')](https://github.com/punkpeye/awesome-mcp-servers?tab=readme-ov-file#communication)\n[![OS_Agnostic_Works_Everywhere_purple](https://img.shields.io/badge/OS_Agnostic-Works_Everywhere-purple)](https://github.com/chaindead/telegram-mcp?tab=readme-ov-file#installation)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[](https://visitorbadge.io/status?path=https%3A%2F%2Fgithub.com%2Fchaindead%2Ftelegram-mcp)\n\n# Telegram MCP server\n\nThe server is a bridge between the Telegram API and the AI assistants and is based on the [Model Context Protocol](https://modelcontextprotocol.io).\n\n> [!IMPORTANT]\n> Ensure that you have read and understood the [Telegram API Terms of Service](https://core.telegram.org/api/terms) before using this server.\n> Any misuse of the Telegram API may result in the suspension of your account.\n\n## Table of Contents\n- [What is MCP?](#what-is-mcp)\n- [What does this server do?](#what-does-this-server-do)\n  - [Capabilities](#capabilities)\n  - [Prompt examples](#prompt-examples)\n    - [Message Management](#message-management)\n    - [Organization](#organization)\n    - [Communication](#communication)\n- [Installation](#installation)\n  - [Homebrew](#homebrew)\n  - [NPX](#npx)\n  - [From Releases](#from-releases)\n    - [MacOS](#macos)\n    - [Linux](#linux)\n    - [Windows](#windows)\n  - [From Source](#from-source)\n- [Configuration](#configuration)\n  - [Authorization](#authorization)\n  - [Client Configuration](#client-configuration)\n- [Star History](#star-history)\n\n## What is MCP?\n\nThe Model Context Protocol (MCP) is a system that lets AI apps, like Claude Desktop or Cursor, connect to external tools and data sources. It gives a clear and safe way for AI assistants to work with local services and APIs while keeping the user in control.\n\n## What does this server do?\n\n### Capabilities\n\n- [x] Get current account information (`tool: tg_me`)\n- [x] List dialogs with optional unread filter (`tool: tg_dialogs`)\n- [x] Mark dialog as read (`tool: tg_read`)\n- [x] Retrieve messages from specific dialog (`tool: tg_dialog`)\n- [x] Send draft messages to any dialog (`tool: tg_send`)\n\n### Prompt examples\n\nHere are some example prompts you can use with AI assistants:\n\n#### Message Management\n- \"Check for any unread important messages in my Telegram\"\n- \"Summarize all my unread Telegram messages\"\n- \"Read and analyze my unread messages, prepare draft responses where needed\"\n- \"Check non-critical unread messages and give me a brief overview\"\n\n#### Organization\n- \"Analyze my Telegram dialogs and suggest a folder structure\"\n- \"Help me categorize my Telegram chats by importance\"\n- \"Find all work-related conversations and suggest how to organize them\"\n\n#### Communication\n- \"Monitor specific chat for updates about [topic]\"\n- \"Draft a polite response to the last message in [chat]\"\n- \"Check if there are any unanswered questions in my chats\"\n\n## Installation\n\n### Homebrew\n\nYou can install a binary release on macOS/Linux using brew:\n\n```bash\n# Install\nbrew install chaindead/tap/telegram-mcp\n\n# Update\nbrew upgrade chaindead/tap/telegram-mcp\n```\n\n### NPX\n\nYou can run the latest version directly using npx (supports macOS, Linux, and Windows):\n\n```bash\nnpx -y @chaindead/telegram-mcp\n```\n\nWhen using NPX, modify the standard commands and configuration as follows:\n\n- [Authentication command](#authorization) becomes:\n```bash\nnpx -y @chaindead/telegram-mcp auth ...\n```\n\n- [Claude MCP server configuration](#client-configuration) becomes:\n```json\n{\n  \"mcpServers\": {\n    \"telegram\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@chaindead/telegram-mcp\"],\n      \"env\": {\n        \"TG_APP_ID\": \"<your-api-id>\",\n        \"TG_API_HASH\": \"<your-api-hash>\"\n      }\n    }\n  }\n}\n```\n\nFor complete setup instructions, see [Authorization](#authorization) and [Client Configuration](#client-configuration).\n\n### From Releases\n\n#### MacOS\n\n<details>\n\n> **Note:** The commands below install to `/usr/local/bin`. To install elsewhere, replace `/usr/local/bin` with your preferred directory in your PATH.\n\nFirst, download the archive for your architecture:\n\n```bash\n# For Intel Mac (x86_64)\ncurl -L -o telegram-mcp.tar.gz https://github.com/chaindead/telegram-mcp/releases/latest/download/telegram-mcp_Darwin_x86_64.tar.gz\n\n# For Apple Silicon (M1/M2)\ncurl -L -o telegram-mcp.tar.gz https://github.com/chaindead/telegram-mcp/releases/latest/download/telegram-mcp_Darwin_arm64.tar.gz\n```\n\nThen install the binary:\n\n```bash\n# Extract the binary\nsudo tar xzf telegram-mcp.tar.gz -C /usr/local/bin\n\n# Make it executable\nsudo chmod +x /usr/local/bin/telegram-mcp\n\n# Clean up\nrm telegram-mcp.tar.gz\n```\n</details>\n\n#### Linux\n<details>\n\n> **Note:** The commands below install to `/usr/local/bin`. To install elsewhere, replace `/usr/local/bin` with your preferred directory in your PATH.\n\nFirst, download the archive for your architecture:\n\n```bash\n# For x86_64 (64-bit)\ncurl -L -o telegram-mcp.tar.gz https://github.com/chaindead/telegram-mcp/releases/latest/download/telegram-mcp_Linux_x86_64.tar.gz\n\n# For ARM64\ncurl -L -o telegram-mcp.tar.gz https://github.com/chaindead/telegram-mcp/releases/latest/download/telegram-mcp_Linux_arm64.tar.gz\n```\n\nThen install the binary:\n\n```bash\n# Extract the binary\nsudo tar xzf telegram-mcp.tar.gz -C /usr/local/bin\n\n# Make it executable\nsudo chmod +x /usr/local/bin/telegram-mcp\n\n# Clean up\nrm telegram-mcp.tar.gz\n```\n</details>\n\n#### Windows\n\n<details>\n\n#### Windows\n1. Download the latest release for your architecture:\n   - [Windows x64](https://github.com/chaindead/telegram-mcp/releases/latest/download/telegram-mcp_Windows_x86_64.zip)\n   - [Windows ARM64](https://github.com/chaindead/telegram-mcp/releases/latest/download/telegram-mcp_Windows_arm64.zip)\n2. Extract the `.zip` file\n3. Add the extracted directory to your PATH or move `telegram-mcp.exe` to a directory in your PATH\n</details>\n\n### From Source\n\nRequirements:\n- Go 1.24 or later\n- GOBIN in PATH\n\n```bash\ngo install github.com/chaindead/telegram-mcp@latest\n```\n\n## Configuration\n\n### Authorization\n\nBefore you can use the server, you need to connect to the Telegram API.\n\n1. Get the API ID and hash from [Telegram API](https://my.telegram.org/auth)\n2. Run the following command:\n   > __Note:__\n   > If you have 2FA enabled: add --password <2fa_password>\n\n   >  __Note:__\n   > If you want to override existing session: add --new\n\n   ```bash\n   telegram-mcp auth --app-id <your-api-id> --api-hash <your-api-hash> --phone <your-phone-number>\n   ```\n\n   📩 Enter the code you received from Telegram to connect to the API.\n\n3. Done! Please give this project a ⭐️ to support its development.\n\n### Client Configuration\n\nExample of Configuring Claude Desktop to recognize the Telegram MCP server.\n\n1. Open the Claude Desktop configuration file:\n    - in MacOS, the configuration file is located at `~/Library/Application Support/Claude/claude_desktop_config.json`\n    - in Windows, the configuration file is located at `%APPDATA%\\Claude\\claude_desktop_config.json`\n\n   > __Note:__\n   > You can also find claude_desktop_config.json inside the settings of Claude Desktop app\n\n2. Add the server configuration\n   \n   for Claude desktop:\n   ```json\n    {\n      \"mcpServers\": {\n        \"telegram\": {\n          \"command\": \"telegram-mcp\",\n          \"env\": {\n            \"TG_APP_ID\": \"<your-app-id>\",\n            \"TG_API_HASH\": \"<your-api-hash>\",\n            \"PATH\": \"<path_to_telegram-mcp_binary_dir>\",\n            \"HOME\": \"<path_to_your_home_directory\"\n          }\n        }\n      }\n    }\n   ```\n\n   for Cursor:\n    ```json\n    {\n      \"mcpServers\": {\n        \"telegram-mcp\": {\n          \"command\": \"telegram-mcp\",\n          \"env\": {\n            \"TG_APP_ID\": \"<your-app-id>\",\n            \"TG_API_HASH\": \"<your-api-hash>\"\n          }\n        }\n      }\n    }\n    ```\n\n## Star History\n\n<a href=\"https://www.star-history.com/#chaindead/telegram-mcp&Date\">\n <picture>\n   <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://api.star-history.com/svg?repos=chaindead/telegram-mcp&type=Date&theme=dark\" />\n   <source media=\"(prefers-color-scheme: light)\" srcset=\"https://api.star-history.com/svg?repos=chaindead/telegram-mcp&type=Date\" />\n   <img alt=\"Star History Chart\" src=\"https://api.star-history.com/svg?repos=chaindead/telegram-mcp&type=Date\" />\n </picture>\n</a>",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "telegram",
        "messages",
        "dialogs",
        "assistants telegram",
        "telegram mcp",
        "telegram api"
      ],
      "category": "virtual-assistants"
    },
    "devizor--macOS-Notification-MCP": {
      "owner": "devizor",
      "name": "macOS-Notification-MCP",
      "url": "https://github.com/devizor/macOS-Notification-MCP",
      "imageUrl": "/freedevtools/mcp/pfp/devizor.webp",
      "description": "Triggers native macOS notifications, plays system sounds, and converts text to speech. Supports customizable visual notifications and voice management features for AI assistants.",
      "stars": 26,
      "forks": 4,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-26T11:51:16Z",
      "readme_content": "# 🔔 macOS Notification MCP\n\nA Model Context Protocol (MCP) server that enables AI assistants to trigger macOS notifications, sounds, and text-to-speech.\n\n## ✨ Features\n\n- 🔊 **Sound Notifications**: Play system sounds like Submarine, Ping, or Tink\n- 💬 **Banner Notifications**: Display visual notifications with customizable title, message, and subtitle\n- 🗣️ **Speech Notifications**: Convert text to speech with adjustable voice, rate, and volume\n- 🎙️ **Voice Management**: List and select from available system voices\n- 🧪 **Testing Tools**: Diagnostic utilities to verify all notification methods\n\n## 🚀 Quick Start with uvx (Recommended)\n\nThe fastest way to use this tool is with `uvx`, which runs packages without permanent installation:\n\n```bash\n# Install uv if you don't have it\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n\n# Run the MCP server directly (no installation needed)\nuvx macos-notification-mcp\n```\n\n## ⚙️ Configure Claude Desktop\n\nAdd this to your Claude Desktop configuration (`~/Library/Application Support/Claude/claude_desktop_config.json`):\n\n```json\n{\n  \"mcpServers\": {\n    \"macos-notification-mcp\": {\n      \"command\": \"uvx\",\n      \"args\": [\"macos-notification-mcp\"]\n    }\n  }\n}\n```\n\nThen restart Claude Desktop.\n\n## 📦 Alternative Installation Methods\n\nStandard installation:\n\n```bash\npip install macos-notification-mcp\n```\n\nInstall from source:\n\n```bash\ngit clone https://github.com/devizor/macos-notification-mcp\ncd macos-notification-mcp\npip install .\n```\n\n## 🛠️ Available Notification Tools\n\n### 🔊 Sound Notification\n```python\nsound_notification(sound_name=\"Submarine\")\n```\nAvailable sounds: Basso, Blow, Bottle, Frog, Funk, Glass, Hero, Morse, Ping, Pop, Purr, Sosumi, Submarine, Tink\n\n### 💬 Banner Notification\n```python\nbanner_notification(\n    title=\"Task Complete\",\n    message=\"Your analysis is ready\",\n    subtitle=None,  # Optional\n    sound=False,    # Optional: Play sound with notification\n    sound_name=None # Optional: Specify system sound\n)\n```\n\n### 🗣️ Speech Notification\n```python\nspeak_notification(\n    text=\"The process has completed\",\n    voice=None,     # Optional: System voice to use\n    rate=150,       # Optional: Words per minute (default: 150)\n    volume=1.0      # Optional: Volume level 0.0-1.0\n)\n```\n\n### 🎙️ Voice Management\n```python\nlist_available_voices()  # Lists all available text-to-speech voices\n```\n\n### 🧪 Testing\n```python\ntest_notification_system()  # Tests all notification methods\n```\n\n## 🔒 Implementation Details\n\n- ⏱️ **Rate Limiting**: Notifications are processed one at a time with a minimum interval of 0.5 seconds\n- 🔄 **Queuing**: Multiple notification requests are handled sequentially\n- 🪟 **OS Integration**: Uses native macOS commands (`afplay`, `osascript`, `say`)\n- 🔌 **FastMCP**: Built on the FastMCP framework for AI communication\n\n## ⚠️ Troubleshooting\n\n- 🔐 **Permissions**: Ensure notifications are allowed in System Settings → Notifications\n- ⏳ **Timing**: Only one notification is processed at a time\n- 🌐 **Environment**: If using the command directly (not uvx), you may need to use full paths\n\n## 📄 License\n\nMIT License\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "notifications",
        "macos",
        "notification",
        "macos notifications",
        "macos notification",
        "notifications voice"
      ],
      "category": "virtual-assistants"
    },
    "dryeab--mcp-telegram": {
      "owner": "dryeab",
      "name": "mcp-telegram",
      "url": "https://github.com/dryeab/mcp-telegram",
      "imageUrl": "/freedevtools/mcp/pfp/dryeab.webp",
      "description": "Connects Large Language Models to Telegram for sending, editing, and managing messages. Facilitates automation of messaging, searching, and media handling within the Telegram platform.",
      "stars": 162,
      "forks": 20,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-10-04T01:30:43Z",
      "readme_content": "<div align=\"center\">\n  \n  <h2 style=\"margin-top: 0\">Enable LLMs to control your Telegram</h2>\n</div>\n\n<div align=\"center\">\n    <a href=\"https://github.com/dryeab/mcp-telegram/stargazers\"><img src=\"https://img.shields.io/github/stars/dryeab/mcp-telegram?style=social\" alt=\"GitHub stars\"></a>\n    <a href=\"https://badge.fury.io/py/mcp-telegram\"><img src=\"https://badge.fury.io/py/mcp-telegram.svg\" alt=\"PyPI version\"></a>\n    <a href=\"https://x.com/dryeab\"><img src=\"https://img.shields.io/twitter/follow/dryeab?style=social\" alt=\"Twitter Follow\"></a>\n</div>\n<h3></h3>\n\n**Connect Large Language Models to Telegram via the [Model Context Protocol (MCP)](https://modelcontextprotocol.io/introduction).**\n\nBuilt with [Telethon](https://github.com/LonamiWebs/Telethon), this server allows AI agents to interact with Telegram, enabling features like sending/editing/deleting messages, searching chats, managing drafts, downloading media, and more using the [MTProto](https://core.telegram.org/mtproto).\n\n---\n<details>\n<summary><strong>Table&nbsp;of&nbsp;Contents</strong></summary>\n\n- [🚀 Getting Started](#-getting-started)\n  - [Prerequisites](#prerequisites)\n  - [Installation](#installation)\n- [⚙️ Usage](#️-usage)\n  - [Login](#login)\n  - [Connect to the MCP server](#connect-to-the-mcp-server)\n- [🧰 Available Tools](#-available-tools)\n  - [📨 Messaging Tools](#-messaging-tools)\n  - [🔍 Search & Navigation](#-search--navigation)\n  - [📝 Draft Management](#-draft-management)\n  - [📂 Media Handling](#-media-handling)\n- [🛠️ Troubleshooting](#️-troubleshooting)\n- [🤝 Contributing](#-contributing)\n- [📝 License](#-license)\n\n</details>\n\n---\n\n\n## 🚀 Getting Started\n\n### Prerequisites\n\n- Python 3.10 or higher\n- [`uv`](https://github.com/astral-sh/uv) Install via the [official uv guide](https://github.com/astral-sh/uv#installation).\n\n### Installation\n\nInstall the `mcp-telegram` CLI tool:\n\n```bash\nuv tool install mcp-telegram\n```\n\n## ⚙️ Usage\n\n> [!IMPORTANT]\n> Please ensure you have read and understood Telegram's [ToS](https://telegram.org/tos) before using this tool. Misuse of this tool may result in account restrictions.\n\nThe `mcp-telegram` command-line tool is your entry point.\n\n```bash\nmcp-telegram --help # See all commands\n```\n\n### Login\n\nFirst, authenticate with your Telegram account:\n\n```bash\nmcp-telegram login\n```\n\nThis interactive command will prompt you for:\n\n- **API ID & API Hash:** Obtain these from [my.telegram.org/apps](https://my.telegram.org/apps).\n- **Phone Number:** Your Telegram-registered phone number (international format, e.g., `+1234567890`).\n- **Verification Code:** Sent to your Telegram account upon first login.\n- **2FA Password:** If you have Two-Factor Authentication enabled.\n\nYour credentials are securely stored in the session file for future use.\n\n> [!WARNING]\n> Keep your API credentials private and never share them publicly\n\n> [!NOTE]\n> Use `mcp-telegram logout` to logout from current session or `mcp-telegram clear-session` to remove all stored session data.\n\n### Connect to the MCP server\n\nTo use MCP Telegram with MCP clients like Claude Desktop or Cursor, you'll need to configure the MCP server. The configuration process varies by client and operating system.\n\nFor detailed setup instructions, please refer to:\n\n- [Claude Desktop MCP Setup Guide](https://modelcontextprotocol.io/quickstart/user)\n- [Cursor MCP Documentation](https://docs.cursor.com/context/model-context-protocol)\n\nThe configuration file should contain:\n\n```json\n{\n  \"mcpServers\": {\n    \"mcp-telegram\": {\n      \"command\": \"mcp-telegram\" /* Use full path if client can't find the command (e.g. \"/usr/local/bin/mcp-telegram\"). See IMPORTANT section below for full path instructions. */,\n      \"args\": [\"start\"],\n      \"env\": {\n        \"API_ID\": \"<your_api_id>\",\n        \"API_HASH\": \"<your_api_hash>\"\n      }\n    }\n  }\n}\n```\n\n> [!Note]\n> Configuration paths vary by OS and client. For example:\n>\n> - macOS: `~/Library/Application Support/Claude/` or `~/.cursor/`\n> - Windows: `%APPDATA%\\Claude\\` or `%APPDATA%\\Cursor\\`\n\n> [!IMPORTANT]\n> If your client cannot execute `mcp-telegram` despite it being accessible in the terminal, try using the full path to the executable. You can find this by running `which mcp-telegram` (macOS/Linux) or `where mcp-telegram` (Windows) in your terminal. Replace the `command` value in the configuration with the full path.\n\nAfter saving the configuration file, restart your application.\n\n## 🧰 Available Tools\n\nHere's a comprehensive list of tools you can use to interact with Telegram through MCP:\n\n### 📨 Messaging Tools\n\n| Tool             | Description                                                   |\n| ---------------- | ------------------------------------------------------------- |\n| `send_message`   | ✉️ Send text messages or files to any user, group, or channel |\n| `edit_message`   | ✏️ Modify content of previously sent messages                 |\n| `delete_message` | 🗑️ Remove one or multiple messages                            |\n| `get_messages`   | 📜 Retrieve message history with advanced filtering options   |\n\n### 🔍 Search & Navigation\n\n| Tool                | Description                                             |\n| ------------------- | ------------------------------------------------------- |\n| `search_dialogs`    | 🔎 Find users, groups, and channels by name or username |\n| `message_from_link` | 🔗 Access specific messages using Telegram links        |\n\n### 📝 Draft Management\n\n| Tool        | Description                                |\n| ----------- | ------------------------------------------ |\n| `get_draft` | 📋 View current message draft for any chat |\n| `set_draft` | ✍️ Create or clear message drafts          |\n\n### 📂 Media Handling\n\n| Tool             | Description                                             |\n| ---------------- | ------------------------------------------------------- |\n| `media_download` | 📸 Download photos, videos, and documents from messages |\n\n> [!Note]\n> For detailed parameter information and example use cases, run `mcp-telegram tools` in your terminal.\n\n## 🛠️ Troubleshooting\n\n### Database Locked Errors\n\nRunning multiple `mcp-telegram` instances using the _same session file_ can cause `database is locked` errors due to Telethon's SQLite session storage. Ensure only one instance uses a session file at a time.\n\n<details>\n<summary>Force-Stopping Existing Processes</summary>\n\nIf you need to stop potentially stuck processes:\n\n- **macOS / Linux:** `pkill -f \"mcp-telegram\"`\n- **Windows:** `taskkill /F /IM mcp-telegram.exe /T` (Check Task Manager for the exact process name)\n\n</details>\n\n## 🤝 Contributing\n\nWe welcome contributions! If you'd like to help improve MCP Telegram, please feel free to submit issues, feature requests, or pull requests. Your feedback and contributions help make this project better for everyone.\n\n## 📝 License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n\n---\n\n<div align=\"center\">\n  <p>Made with ❤️ by <a href=\"https://x.com/dryeab\">Yeabsira Driba</a></p>\n</div>",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "telegram",
        "messaging",
        "messages",
        "models telegram",
        "mcp telegram",
        "telegram platform"
      ],
      "category": "virtual-assistants"
    },
    "dvcrn--mcp-server-siri-shortcuts": {
      "owner": "dvcrn",
      "name": "mcp-server-siri-shortcuts",
      "url": "https://github.com/dvcrn/mcp-server-siri-shortcuts",
      "imageUrl": "/freedevtools/mcp/pfp/dvcrn.webp",
      "description": "Access Siri Shortcuts functionality to list, open, and run shortcuts from the macOS Shortcuts app seamlessly. Integrates with AI models for dynamic execution of available shortcuts.",
      "stars": 161,
      "forks": 15,
      "license": "GNU General Public License v3.0",
      "language": "TypeScript",
      "updated_at": "2025-09-21T13:56:48Z",
      "readme_content": "# Siri Shortcuts MCP Server\n\nThis MCP server provides access to Siri shortcuts functionality via the Model Context Protocol (MCP). It allows listing, opening, and running shortcuts from the macOS Shortcuts app.\n\n\n\n## Features\n\n- Exposes _all_ shortcuts, meaning the LLM can call anything that is available in the Shortcuts app.\n- List all available shortcuts\n- Open shortcuts in the Shortcuts app\n- Run shortcuts with optional input parameters\n- Dynamically generated tools for each available shortcut\n\n## Tools\n\n### Base Tools\n\n1. `list_shortcuts`\n\n   - Lists all available Siri shortcuts on the system\n   - No input required\n   - Returns: Array of shortcut names\n\n   ```json\n   {\n     \"shortcuts\": [{ \"name\": \"My Shortcut 1\" }, { \"name\": \"My Shortcut 2\" }]\n   }\n   ```\n\n2. `open_shortcut`\n\n   - Opens a shortcut in the Shortcuts app\n   - Input:\n     - `name` (string): Name of the shortcut to open\n\n3. `run_shortcut`\n   - Runs a shortcut with optional input\n   - Input:\n     - `name` (string): Name or identifier (UUID) of the shortcut to run\n     - `input` (string, optional): Text input or filepath to pass to the shortcut\n\n### Dynamic Tools\n\nThe server automatically generates additional tools for each available shortcut in the format:\n\n- Tool name: `run_shortcut_[sanitized_shortcut_name]`\n- Description: Runs the specific shortcut\n- Input:\n  - `input` (string, optional): Text input or filepath to pass to the shortcut\n\n## Configuration\n\nThe server supports the following environment variables:\n\n- `GENERATE_SHORTCUT_TOOLS` (default: `true`): When set to `false`, disables the generation of dynamic shortcut tools. Only the base tools (`list_shortcuts`, `open_shortcut`, `run_shortcut`) will be available.\n- `INJECT_SHORTCUT_LIST` (default: `false`): When set to `true`, injects the list of available shortcuts into the `run_shortcut` tool description to help the LLM understand which shortcuts are available.\n\n## Usage with Claude\n\nAdd to your Claude configuration:\n\n```json\n{\n  \"mcpServers\": {\n    \"siri-shortcuts\": {\n      \"command\": \"npx\",\n      \"args\": [\"mcp-server-siri-shortcuts\"],\n      \"env\": {\n        \"GENERATE_SHORTCUT_TOOLS\": \"true\",\n        \"INJECT_SHORTCUT_LIST\": \"false\"\n      }\n    }\n  }\n}\n```\n\n## Implementation Details\n\n- Uses the macOS `shortcuts` CLI command under the hood\n- Sanitizes shortcut names for tool naming compatibility\n- Supports both direct text input and file-based input\n- Returns shortcut output when available\n- Implements standard MCP error handling",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "shortcuts",
        "siri",
        "macos",
        "siri shortcuts",
        "macos shortcuts",
        "shortcuts macos"
      ],
      "category": "virtual-assistants"
    },
    "emiliobool--MCP-Relay": {
      "owner": "emiliobool",
      "name": "MCP-Relay",
      "url": "https://github.com/emiliobool/MCP-Relay",
      "imageUrl": "/freedevtools/mcp/pfp/emiliobool.webp",
      "description": "Send messages and prompts to a Discord channel and receive responses directly from an AI model. It integrates with Discord's API to facilitate real-time communication between AI and users.",
      "stars": 1,
      "forks": 1,
      "license": "No License",
      "language": "TypeScript",
      "updated_at": "2025-04-21T12:07:22Z",
      "readme_content": "# MCP Relay\n\nThis MCP server allows Claude to send messages and prompts to a Discord channel and receive responses.\n\n## Setup Instructions\n\n### 1. Create a Discord Application and Bot\n\n1. Go to the [Discord Developer Portal](https://discord.com/developers/applications)\n2. Click \"New Application\" and give it a name\n3. Go to the \"Bot\" section in the left sidebar\n4. Under the bot's token section, click \"Reset Token\" and copy the new token\n   - Keep this token secure! Don't share it publicly\n5. Under \"Privileged Gateway Intents\", enable:\n   - Message Content Intent\n   - Server Members Intent\n   - Presence Intent\n\n### 2. Invite the Bot to Your Server\n\n1. Go to the \"OAuth2\" section in the left sidebar\n2. Select \"URL Generator\"\n3. Under \"Scopes\", select:\n   - bot\n   - applications.commands\n4. Under \"Bot Permissions\", select:\n   - Send Messages\n   - Embed Links\n   - Read Message History\n5. Copy the generated URL and open it in your browser\n6. Select your server and authorize the bot\n\n### 3. Get Channel ID\n\n1. In Discord, enable Developer Mode:\n   - Go to User Settings > App Settings > Advanced\n   - Turn on \"Developer Mode\"\n2. Right-click the channel you want to use\n3. Click \"Copy Channel ID\"\n\n### 4. Configure MCP Settings\n\nThe server requires configuration in your MCP settings file. Add the following to your configuration file:\n\n```json\n{\n    \"mcpServers\": {\n        \"discord-relay\": {\n            \"command\": \"node\",\n            \"args\": [\n                \"/ABSOLUTE/PATH/TO/MCP Relay/build/index.js\"\n            ],\n            \"env\": {\n                \"DISCORD_TOKEN\": \"your_bot_token_here\",\n                \"DISCORD_CHANNEL_ID\": \"your_channel_id_here\"\n            }\n        }\n    }\n}\n```\n\nReplace:\n- `/ABSOLUTE/PATH/TO/MCP Relay` with the actual path to your MCP Relay project\n- `your_bot_token_here` with your Discord bot token\n- `your_channel_id_here` with your Discord channel ID\n\nNote: Make sure to use absolute paths in the configuration.\n\n## Usage\n\nThe server provides a tool called `send-message` that accepts the following parameters:\n\n```typescript\n{\n  type: 'prompt' | 'notification',  // Type of message\n  title: string,                    // Message title\n  content: string,                  // Message content\n  actions?: Array<{                 // Optional action buttons\n    label: string,                  // Button label\n    value: string                   // Value returned when clicked\n  }>,\n  timeout?: number                  // Optional timeout in milliseconds\n}\n```\n\n### Message Types\n\n1. **Notification**: Simple message that doesn't expect a response\n   ```json\n   {\n     \"type\": \"notification\",\n     \"title\": \"Hello\",\n     \"content\": \"This is a notification\"\n   }\n   ```\n\n2. **Prompt**: Message that waits for a response\n   ```json\n   {\n     \"type\": \"prompt\",\n     \"title\": \"Question\",\n     \"content\": \"Do you want to proceed?\",\n     \"actions\": [\n       { \"label\": \"Yes\", \"value\": \"yes\" },\n       { \"label\": \"No\", \"value\": \"no\" }\n     ],\n     \"timeout\": 60000  // Optional: 1 minute timeout\n   }\n   ```\n\nNotes:\n- Prompts can be answered either by clicking action buttons or sending a text message\n- Only one response is accepted per prompt\n- If a timeout is specified, the prompt will fail after the timeout period\n- Notifications don't wait for responses and return immediately\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "discord",
        "relay",
        "mcp",
        "mcp relay",
        "virtual assistants",
        "communication ai"
      ],
      "category": "virtual-assistants"
    },
    "himanshu8271--Dragons": {
      "owner": "himanshu8271",
      "name": "Dragons",
      "url": "https://github.com/himanshu8271/Dragons",
      "imageUrl": "/freedevtools/mcp/pfp/himanshu8271.webp",
      "description": "Stream audio and video content in Telegram with a user-friendly interface featuring powerful controls and multilingual support. Enjoy music with friends in multiple chats simultaneously.",
      "stars": 0,
      "forks": 0,
      "license": "GNU Affero General Public License v3.0",
      "language": "",
      "updated_at": "2022-04-24T22:41:02Z",
      "readme_content": "<h1 align= center><b>⭐️ Music Player ⭐️</b></h1>\n<h3 align = center> A Telegram Music Bot written in Python using Pyrogram and Py-Tgcalls </h3>\n\n<p align=\"center\">\n<a href=\"https://python.org\"><img src=\"http://forthebadge.com/images/badges/made-with-python.svg\" alt=\"made-with-python\"></a>\n<br>\n    <img src=\"https://img.shields.io/github/license/AsmSafone/MusicPlayer?style=for-the-badge\" alt=\"LICENSE\">\n    <img src=\"https://img.shields.io/github/contributors/AsmSafone/MusicPlayer?style=for-the-badge\" alt=\"Contributors\">\n    <img src=\"https://img.shields.io/github/repo-size/AsmSafone/MusicPlayer?style=for-the-badge\" alt=\"Repository Size\"> <br>\n    <img src=\"https://img.shields.io/github/forks/AsmSafone/MusicPlayer?style=for-the-badge\" alt=\"Forks\">\n    <img src=\"https://img.shields.io/github/stars/AsmSafone/MusicPlayer?style=for-the-badge\" alt=\"Stars\">\n    <img src=\"https://img.shields.io/github/watchers/AsmSafone/MusicPlayer?style=for-the-badge\" alt=\"Watchers\">\n    <img src=\"https://img.shields.io/github/commit-activity/w/AsmSafone/MusicPlayer?style=for-the-badge\" alt=\"Commit Activity\">\n    <img src=\"https://img.shields.io/github/issues/AsmSafone/MusicPlayer?style=for-the-badge\" alt=\"Issues\">\n</p>\n\n## ✨ <a name=\"features\"></a>Features\n\n### ⚡️ Fast & Light\n\nStarts streaming your inputs while downloading and converting them. Also, it\ndoesn't make produce files.\n\n### 👮🏻‍♀️ Safe and handy\n\nRestricts control and sensitive commands to admins.\n\n### 🗑 Clean and spam free\n\nDeletes old playing trash to keep your chats clean.\n\n### 😎 Has cool controls\n\nLets you switch stream mode, loop, pause, resume, mute, unmute anytime.\n\n### 🖼 Has cool thumbnails\n\nResponse your commands with cool thumbnails on the chat.\n\n### 😉 Streams whatever you like\n\nYou can stream audio or video files, YouTube videos with any duration,\nYouTube lives, YouTube playlists and even custom live streams like radios or m3u8 links or files in\nthe place it is hosted!\n\n### 📊 Streams in multiple places\n\nAllows you to stream different things in multiple chats simultaneously. Each\nchat will have its own song queue.\n\n### 🗣 Speaks different languages\n\nMusic Player is multilingual and speaks [various languages](#languages),\nthanks to the translators.\n\n## 🚀 <a name=\"deploy\"></a>Deploy\n\n[![Deploy on Heroku](https://www.herokucdn.com/deploy/button.svg)](https://deploy.safone.tech)\n\nNote: `First Fork The Repo Then Click On Deploy To Heroku Button!`\n\n\n## ☁️ <a name=\"self_host\"></a>Self Host\n\n- Legecy Method\n```bash\n$ git clone https://github.com/AsmSafone/MusicPlayer\n$ cd MusicPlayer\n$ sudo apt install git curl python3-pip ffmpeg -y\n$ pip3 install -U pip\n$ curl -sL https://deb.nodesource.com/setup_16.x | sudo -E bash -\n$ sudo apt install -y nodejs\n$ sudo apt install build-essential\n$ sudo npm install pm2@latest -g\n$ pip3 install -U -r requirements.txt\n$ cp sample.env .env\n# < edit .env with your own values >\n$ python3 main.py\n```\n\n- Docker Build Method\n```bash\n$ git clone https://github.com/AsmSafone/MusicPlayer\n$ cd MusicPlayer\n$ cp sample.env .env\n# < edit .env with your own values >\n$ sudo docker build . -t musicplayer\n$ sudo docker run musicplayer\n```\n\n## ⚒ <a name=\"configs\"></a>Configs\n\n- `API_ID`: Telegram app id from https://my.telegram.org/apps.\n- `API_HASH`: Telegram app hash from https://my.telegram.org/apps.\n- `SESSION`: Pyrogram string session. You can generate from [here](https://replit.com/@AsmSafone/genStr).\n- `SUDOERS`: ID of sudo users (separate multiple ids with space).\n- `BOT_TOKEN`: Telegram bot token from https://t.me/botfather. (optional)\n- `QUALITY`: Custom stream quality (high/medium/low) for the userbot in vc. Default: `high`\n- `PREFIX`: Bot commad prefixes (separate multiple prefix with space). Eg: `! /`\n- `LANGUAGE`: An [available](#languages) bot language (can change it anytime). Default: `en`\n- `STREAM_MODE`: An stream mode like audio or video (can change it anytime). Default: `audio`\n- `ADMINS_ONLY`: Put `True` if you want to make /play commands only for admins. Default: `False`\n- `SPOTIFY_CLIENT_ID`: Spotify client id get it from [here](https://developer.spotify.com/dashboard/applications). (optional)\n- `SPOTIFY_CLIENT_SECRET`: Spotify client secret get it from [here](https://developer.spotify.com/dashboard/applications). (optional)\n\n\n## 📄 <a name=\"commands\"></a>Commands\n\nCommand | Description\n:--- | :---\n• !ping | Check if alive or not\n• !start / !help | Show the help for commands\n• !mode / !switch | Switch the stream mode (audio/video)\n• !p / !play [song name or youtube link] | Play a song in vc, if already playing add to queue\n• !radio / !stream [radio url or stream link] | Play a live stream in vc, if already playing add to queue\n• !pl / !playlist [playlist link] | Play the whole youtube playlist at once\n• !skip / !next | Skip to the next song\n• !m / !mute | Mute the current stream\n• !um / !unmute | Unmute the muted stream\n• !ps / !pause | Pause the current stream\n• !rs / !resume | Resume the paused stream\n• !list / !queue | Show the songs in the queue\n• !mix / !shuffle | Shuflle the queued playlist\n• !loop / !repeat | Enable or disable the loop mode\n• !lang / language [language code] | Set the bot language in group\n• !ip / !import | Import queue from exported file\n• !ep / !export | Export the queue for import in future\n• !stop / !leave | Leave from vc and clear the queue\n• !update / !restart | Update and restart your music player\n\n## 🗣 <a name=\"languages\"></a>Languages\n\n```text\nen    English\n```\n\n## 💜 <a name=\"contribute\"></a>Contribute\n\nNew languages, bug fixes and improvements following\n[our contribution guidelines](./CONTRIBUTING.md) are warmly welcomed!\n\n## 🛫 <a name=\"supports\"></a>Supports\n\nFor any kind of help join [our support group](https://t.me/AsmSupport) or raise an [issue](https://github.com/AsmSafone/MusicPlayer/issues).\n\n## ✨ <a name=\"credits\"></a>Credits\n\n- [Me](https://github.com/AsmSafone) for [Noting](https://github.com/AsmSafone/MusicPlayer) 😬\n- [Dan](https://github.com/delivrance) for [Pyrogram](https://github.com/pyrogram/pyrogram) ❤️\n- [Laky-64](https://github.com/Laky-64) for [Py-TgCalls](https://github.com/pytgcalls/pytgcalls) ❤️\n- And Thanks To All [Contributors](https://github.com/AsmSafone/MusicPlayer/graphs/contributors)! ❤️\n\n## 📃 <a name=\"license\"></a>License\n\nMusic Player is licenced under the GNU Affero General Public License v3.0.\nRead more [here](./LICENSE).\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "telegram",
        "audio",
        "chats",
        "content telegram",
        "telegram user",
        "virtual assistants"
      ],
      "category": "virtual-assistants"
    },
    "htlin222--claude-chatgpt-mcp": {
      "owner": "htlin222",
      "name": "claude-chatgpt-mcp",
      "url": "https://github.com/htlin222/claude-chatgpt-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/htlin222.webp",
      "description": "Seamlessly interact with the ChatGPT desktop app on macOS, allowing users to ask questions, view conversation history, and continue discussions. This integration facilitates enhanced productivity by bridging Claude and ChatGPT.",
      "stars": 0,
      "forks": 1,
      "license": "No License",
      "language": "JavaScript",
      "updated_at": "2025-04-02T03:27:04Z",
      "readme_content": "# Claude ChatGPT MCP Tool\n\nThis is a Model Context Protocol (MCP) tool that allows Claude to interact with the ChatGPT desktop app on macOS.\n\n## Features\n\n- Ask ChatGPT questions directly from Claude\n- View ChatGPT conversation history\n- Continue existing ChatGPT conversations\n\n## Installation\n\n### Prerequisites\n\n- macOS with M1/M2/M3 chip\n- [ChatGPT desktop app](https://chatgpt.com/download) installed\n- [Bun](https://bun.sh/) installed\n- [Claude desktop app](https://claude.ai/desktop) installed\n\n### NPX Installation (Recommended)\n\nYou can use NPX to run this tool without cloning the repository:\n\n- **Install and run the package using NPX:**\n\n```bash\nnpx claude-chatgpt-mcp\n```\n\n- **Configure Claude Desktop:**\n\nEdit your `claude_desktop_config.json` file (located at `~/Library/Application Support/Claude/claude_desktop_config.json`) to include this tool:\n\n```json\n\"chatgpt-mcp\": {\n  \"command\": \"npx\",\n  \"args\": [\"claude-chatgpt-mcp\"]\n}\n```\n\n- **Restart the Claude Desktop app**\n\n- **Grant necessary permissions:**\n  - Go to System Preferences > Privacy & Security > Privacy\n  - Give Terminal (or iTerm) access to Accessibility features\n  - You may see permission prompts when the tool is first used\n\n### Manual Installation\n\n1. Clone this repository:\n\n```bash\ngit clone https://github.com/syedazharmbnr1/claude-chatgpt-mcp.git\ncd claude-chatgpt-mcp\n```\n\n2. Install dependencies:\n\n```bash\nbun install\n```\n\n3. Make sure the script is executable:\n\n```bash\nchmod +x index.ts\n```\n\n4. Update your Claude Desktop configuration:\n\nEdit your `claude_desktop_config.json` file (located at `~/Library/Application Support/Claude/claude_desktop_config.json`) to include this tool:\n\n```json\n\"chatgpt-mcp\": {\n  \"command\": \"/Users/YOURUSERNAME/.bun/bin/bun\",\n  \"args\": [\"run\", \"/path/to/claude-chatgpt-mcp/index.ts\"]\n}\n```\n\nMake sure to replace `YOURUSERNAME` with your actual macOS username and adjust the path to where you cloned this repository.\n\n5. Restart Claude Desktop app\n\n6. Grant permissions:\n   - Go to System Preferences > Privacy & Security > Privacy\n   - Give Terminal (or iTerm) access to Accessibility features\n   - You may see permission prompts when the tool is first used\n\n## Usage\n\nOnce installed, you can use the ChatGPT tool directly from Claude by asking questions like:\n\n- \"Can you ask ChatGPT what the capital of France is?\"\n- \"Show me my recent ChatGPT conversations\"\n- \"Ask ChatGPT to explain quantum computing\"\n\n## Troubleshooting\n\nIf the tool isn't working properly:\n\n1. Make sure ChatGPT app is installed and you're logged in\n2. Verify the path to bun in your claude_desktop_config.json is correct\n3. Check that you've granted all necessary permissions\n4. Try restarting both Claude and ChatGPT apps\n\n## Optimizations\n\nThis fork includes several significant improvements to the original implementation:\n\n### Enhanced AppleScript Robustness\n\n#### Conversation Retrieval\n- Added multiple UI element targeting approaches to handle ChatGPT UI changes\n- Implemented better error detection with specific error messages\n- Added fallback mechanisms using accessibility attributes\n- Improved timeout handling with appropriate delays\n\n#### Response Handling\n- Replaced fixed waiting times with dynamic response detection\n- Added intelligent completion detection that recognizes when ChatGPT has finished typing\n- Implemented text stability detection (waits until text stops changing)\n- Added response extraction logic to isolate just the relevant response text\n- Improved error handling with detailed error messages\n- Added post-processing to clean up UI elements from responses\n- Implemented incomplete response detection to warn about potential cutoffs\n\nThese optimizations make the integration more reliable across different scenarios, more resilient to UI changes in the ChatGPT application, and better at handling longer response times without message cutoff issues.\n\n## License\n\nMIT",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "chatgpt",
        "conversation",
        "interact",
        "interact chatgpt",
        "chatgpt mcp",
        "chatgpt desktop"
      ],
      "category": "virtual-assistants"
    },
    "hyperb1iss--droidmind": {
      "owner": "hyperb1iss",
      "name": "droidmind",
      "url": "https://github.com/hyperb1iss/droidmind",
      "imageUrl": "/freedevtools/mcp/pfp/hyperb1iss.webp",
      "description": "DroidMind enables control of Android devices using AI through natural language commands. It facilitates tasks such as debugging, system analysis, and app management in an integrated environment with the Model Context Protocol.",
      "stars": 244,
      "forks": 36,
      "license": "Apache License 2.0",
      "language": "Python",
      "updated_at": "2025-10-03T06:02:33Z",
      "readme_content": "<div align=\"center\">\n\n# 🤖 DroidMind 🧠\n\n\n\n[![Python 3.13+](https://img.shields.io/badge/python-3.13+-9D00FF.svg?style=for-the-badge&logo=python&logoColor=white)](https://www.python.org/downloads/)\n[![License](https://img.shields.io/badge/license-Apache_2.0-FF00FF.svg?style=for-the-badge&logo=apache&logoColor=white)](LICENSE)\n[![Status](https://img.shields.io/badge/status-active_development-39FF14.svg?style=for-the-badge&logo=githubactions&logoColor=white)](docs/plan.md)\n[![Code Style](https://img.shields.io/badge/code_style-ruff-00FFFF.svg?style=for-the-badge&logo=ruff&logoColor=white)](https://github.com/astral-sh/ruff)\n[![Type Check](https://img.shields.io/badge/type_check-pyright-FFBF00.svg?style=for-the-badge&logo=typescript&logoColor=white)](https://github.com/microsoft/pyright)\n[![MCP](https://img.shields.io/badge/protocol-MCP-E6E6FA.svg?style=for-the-badge&logo=anthropic&logoColor=white)](https://modelcontextprotocol.io/)\n[![Android](https://img.shields.io/badge/platform-android-A4C639.svg?style=for-the-badge&logo=android&logoColor=white)](https://www.android.com/)\n[![Docs](https://img.shields.io/badge/docs-online-FF9E80.svg?style=for-the-badge&logo=gitbook&logoColor=white)](https://hyperb1iss.github.io/droidmind/)\n\n**Control Android devices with AI through the Model Context Protocol**\n\n</div>\n\nDroidMind is a powerful bridge between AI assistants and Android devices, enabling control, debugging, and system analysis through natural language. By implementing the Model Context Protocol (MCP), DroidMind allows AI models to directly interact with Android devices via ADB in a secure, structured way. When used as part of an agentic coding workflow, DroidMind can enable your assistant to build and debug with your device directly in the loop.\n\n## 💫 Core Features\n\nDroidMind empowers AI assistants to:\n\n- 📱 **Manage Devices**: Connect via USB/TCP-IP, list devices, view properties, and reboot.\n- 📊 **Analyze Systems**: Access logs (logcat, ANR, crash, battery), capture bug reports, and dump heap.\n- 📂 **Handle Files**: Browse, read, write, push, pull, delete, and manage device files/directories.\n- 📦 **Control Apps**: Install, uninstall, start, stop, clear data, and inspect app details (manifest, permissions, activities).\n- 🖼️ **Automate UI**: Perform taps, swipes, text input, and key presses.\n- 🐚 **Execute Shell Commands**: Run ADB shell commands with a security-conscious framework.\n- 🔒 **Operate Securely**: Benefit from command validation, risk assessment, and sanitization.\n- 💬 **Integrate Seamlessly**: Connect with any MCP-compatible client (Claude, Cursor, Cline, etc.).\n\nFor a detailed list of capabilities, see the **[User Manual](docs/user_manual/index.md)** and **[MCP Reference](docs/mcp-reference.md)**.\n\n## 🚀 Getting Started\n\n### Quickstart for IDEs (Zero Install with `uvx`)\n\nFor the fastest way to integrate DroidMind with an MCP-compatible IDE (like Cursor), you can configure it to run DroidMind directly from its GitHub repository using `uvx`. This method **does not require you to manually clone or install DroidMind first**.\n\nAdd the following to your IDE's MCP configuration (e.g., `.cursor/mcp.json` for Cursor):\n\n```json\n{\n  \"mcpServers\": {\n    \"droidmind\": {\n      \"command\": \"uvx\",\n      \"args\": [\n        \"--from\",\n        \"git+https://github.com/hyperb1iss/droidmind\",\n        \"droidmind\",\n        \"--transport\",\n        \"stdio\" // The default and preferred mode for most IDE integrations\n      ]\n    }\n  }\n}\n```\n\nYour IDE will be configured to launch DroidMind on demand. Full instructions for this setup are in the **[Quick Start Guide](docs/quickstart.md#1-configure-your-ide-to-run-droidmind-via-uvx)**.\n\n### Prerequisites\n\n- Python 3.13 or higher\n- `uv` (Python package manager)\n- Android device with USB debugging enabled\n- ADB (Android Debug Bridge) installed and in your system's PATH\n\n### Installation\n\nFor detailed instructions on setting up DroidMind, including the quick IDE integration with `uvx` (covered in the Quick Start), manual installation from source, or using Docker, please see our comprehensive **[Installation Guide](docs/installation.md)**.\n\n### Running DroidMind\n\nHow you run DroidMind depends on your setup:\n\n- **IDE Integration (via `uvx`)**: Your IDE automatically manages running DroidMind as configured in its MCP settings (e.g., `mcp.json`). See the [Quick Start Guide](docs/quickstart.md).\n- **Manual Installation**: After installing from source, you can run DroidMind directly.\n  - **Stdio (for direct terminal interaction or some IDE setups):**\n    ```bash\n    droidmind --transport stdio\n    ```\n  - **SSE (for web UIs or AI assistants like Claude Desktop):**\n    ```bash\n    droidmind --transport sse\n    ```\n    This usually starts a server at `sse://localhost:4256/sse`.\n- **Docker**: Refer to the [Docker Guide](docs/docker.md) for commands to run DroidMind in a container.\n\nRefer to the **[Installation Guide](docs/installation.md)** for more details on running DroidMind in different environments.\n\n## 🐳 Running with Docker\n\nDroidMind can also be run using Docker for a consistent, containerized environment. This is particularly useful for deployment and isolating dependencies.\n\nFor comprehensive instructions on building the Docker image and running DroidMind in a container with `stdio` or `SSE` transport, including notes on ADB device access, please refer to our **[Docker Guide](docs/docker.md)**.\n\n## 🔮 Example AI Assistant Queries\n\nWith an AI assistant connected to DroidMind, you can make requests like:\n\n- \"List all connected Android devices and show their properties.\"\n- \"Take a screenshot of my Pixel.\"\n- \"Install this APK on `emulator-5554`.\"\n- \"Show me the recent crash logs from `your_device_serial`.\"\n- \"Tap the 'Next' button on the current screen of `emulator-5554`.\"\n\nFor more inspiration, check out our **[Example Queries and Workflows](docs/user_manual/example_queries.md)** in the User Manual.\n\n## 🔒 Security\n\nDroidMind incorporates a security framework to protect your devices:\n\n- **Command Validation & Sanitization**\n- **Risk Assessment Categorization**\n- **Protected Path Operations**\n- **Comprehensive Logging**\n\nHigh-risk operations are flagged, and critical ones are blocked by default. Learn more in our **[Security Considerations](docs/user_manual/security.md)** chapter.\n\n## 💻 Development\n\nDroidMind uses `uv` for dependency management and development workflows.\n\n```bash\n# Install/update dependencies (after cloning and activating .venv)\nuv pip install -e .[dev,sse]\n\n# Run tests\npytest\n\n# Run linting\nruff check .\n\n# Run type checking\npyright # Ensure pyright is installed or use ruff's type checking capabilities\n```\n\n## 🤝 Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n\n1.  Fork the repository.\n2.  Create your feature branch (`git checkout -b feature/amazing-feature`).\n3.  Set up your development environment with `uv`.\n4.  Make your changes.\n5.  Run tests, linting, and type checking.\n6.  Commit your changes (`git commit -m 'Add some amazing feature'`).\n7.  Push to the branch (`git push origin feature/amazing-feature`).\n8.  Open a Pull Request.\n\n## 📝 License\n\nThis project is licensed under the Apache License - see the [LICENSE](LICENSE) file for details.\n\n---\n\n<div align=\"center\">\n\nCreated by [Stefanie Jane 🌠](https://github.com/hyperb1iss)\n\nIf you find DroidMind useful, [buy me a Monster Ultra Violet ⚡️](https://ko-fi.com/hyperb1iss)\n\n</div>",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "droidmind",
        "android",
        "ai",
        "hyperb1iss droidmind",
        "droidmind enables",
        "virtual assistants"
      ],
      "category": "virtual-assistants"
    },
    "hyy0612--chatbot": {
      "owner": "hyy0612",
      "name": "chatbot",
      "url": "https://github.com/hyy0612/chatbot",
      "imageUrl": "/freedevtools/mcp/pfp/hyy0612.webp",
      "description": "Provides a conversational AI interface for applications, enabling automated responses and interactive dialogue to enhance user engagement. It supports easy deployment of chat-based AI solutions for various use cases.",
      "stars": 0,
      "forks": 0,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-05-07T09:52:24Z",
      "readme_content": "# chatbot",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "chatbot",
        "ai",
        "dialogue",
        "hyy0612 chatbot",
        "chatbot provides",
        "virtual assistants"
      ],
      "category": "virtual-assistants"
    },
    "imgompanda--AutoGPT123": {
      "owner": "imgompanda",
      "name": "AutoGPT123",
      "url": "https://github.com/imgompanda/AutoGPT123",
      "imageUrl": "/freedevtools/mcp/pfp/imgompanda.webp",
      "description": "AutoGPT enables users to build, test, and deploy AI agents, facilitating the automation of various tasks and the realization of innovative ideas in AI development.",
      "stars": 0,
      "forks": 0,
      "license": "MIT License",
      "language": "",
      "updated_at": "2024-04-09T13:17:37Z",
      "readme_content": "# AutoGPT: build & use AI agents\n\n[](https://discord.gg/autogpt) &ensp;\n[![Twitter Follow](https://img.shields.io/twitter/follow/Auto_GPT?style=social)](https://twitter.com/Auto_GPT) &ensp;\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n\n**AutoGPT** is the vision of the power of AI accessible to everyone, to use and to build on. Our mission is to provide the tools, so that you can focus on what matters:\n\n- 🏗️ **Building** - Lay the foundation for something amazing.\n- 🧪 **Testing** - Fine-tune your agent to perfection.\n- 🤝 **Delegating** - Let AI work for you, and have your ideas come to life.\n\nBe part of the revolution! **AutoGPT** is here to stay, at the forefront of AI innovation.\n\n**📖 [Documentation](https://docs.agpt.co)**\n&ensp;|&ensp;\n**🚀 [Contributing](CONTRIBUTING.md)**\n&ensp;|&ensp;\n**🛠️ [Build your own Agent - Quickstart](QUICKSTART.md)**\n\n## 🥇 Current Best Agent: evo.ninja\n[Current Best Agent]: #-current-best-agent-evoninja\n\nThe AutoGPT Arena Hackathon saw [**evo.ninja**](https://github.com/polywrap/evo.ninja) earn the top spot on our Arena Leaderboard, proving itself as the best open-source generalist agent. Try it now at https://evo.ninja!\n\n📈 To challenge evo.ninja, AutoGPT, and others, submit your benchmark run to the [Leaderboard](#-leaderboard), and maybe your agent will be up here next!\n\n## 🧱 Building blocks\n\n### 🏗️ Forge\n\n**Forge your own agent!** &ndash; Forge is a ready-to-go template for your agent application. All the boilerplate code is already handled, letting you channel all your creativity into the things that set *your* agent apart. All tutorials are located [here](https://medium.com/@aiedge/autogpt-forge-e3de53cc58ec). Components from the [`forge.sdk`](/autogpts/forge/forge/sdk) can also be used individually to speed up development and reduce boilerplate in your agent project.\n\n🚀 [**Getting Started with Forge**](https://github.com/Significant-Gravitas/AutoGPT/blob/master/autogpts/forge/tutorials/001_getting_started.md) &ndash;\nThis guide will walk you through the process of creating your own agent and using the benchmark and user interface.\n\n📘 [Learn More](https://github.com/Significant-Gravitas/AutoGPT/tree/master/autogpts/forge) about Forge\n\n### 🎯 Benchmark\n\n**Measure your agent's performance!** The `agbenchmark` can be used with any agent that supports the agent protocol, and the integration with the project's [CLI] makes it even easier to use with AutoGPT and forge-based agents. The benchmark offers a stringent testing environment. Our framework allows for autonomous, objective performance evaluations, ensuring your agents are primed for real-world action.\n\n<!-- TODO: insert visual demonstrating the benchmark -->\n\n📦 [`agbenchmark`](https://pypi.org/project/agbenchmark/) on Pypi\n&ensp;|&ensp;\n📘 [Learn More](https://github.com/Significant-Gravitas/AutoGPT/blob/master/benchmark) about the Benchmark\n\n#### 🏆 [Leaderboard][leaderboard]\n[leaderboard]: https://leaderboard.agpt.co\n\nSubmit your benchmark run through the UI and claim your place on the AutoGPT Arena Leaderboard! The best scoring general agent earns the title of **[Current Best Agent]**, and will be adopted into our repo so people can easily run it through the [CLI].\n\n[![Screenshot of the AutoGPT Arena leaderboard](https://github.com/Significant-Gravitas/AutoGPT/assets/12185583/60813392-9ddb-4cca-bb44-b477dbae225d)][leaderboard]\n\n### 💻 UI\n\n**Makes agents easy to use!** The `frontend` gives you a user-friendly interface to control and monitor your agents. It connects to agents through the [agent protocol](#-agent-protocol), ensuring compatibility with many agents from both inside and outside of our ecosystem.\n\n<!-- TODO: instert screenshot of front end -->\n\nThe frontend works out-of-the-box with all agents in the repo. Just use the [CLI] to run your agent of choice!\n\n📘 [Learn More](https://github.com/Significant-Gravitas/AutoGPT/tree/master/frontend) about the Frontend\n\n### ⌨️ CLI\n\n[CLI]: #-cli\n\nTo make it as easy as possible to use all of the tools offered by the repository, a CLI is included at the root of the repo:\n\n```shell\n$ ./run\nUsage: cli.py [OPTIONS] COMMAND [ARGS]...\n\nOptions:\n  --help  Show this message and exit.\n\nCommands:\n  agent      Commands to create, start and stop agents\n  arena      Commands to enter the arena\n  benchmark  Commands to start the benchmark and list tests and categories\n  setup      Installs dependencies needed for your system.\n```\n\nJust clone the repo, install dependencies with `./run setup`, and you should be good to go!\n\n## 🤔 Questions? Problems? Suggestions?\n\n### Get help - [Discord 💬](https://discord.gg/autogpt)\n\n[![Join us on Discord](https://invidget.switchblade.xyz/autogpt)](https://discord.gg/autogpt)\n\nTo report a bug or request a feature, create a [GitHub Issue](https://github.com/Significant-Gravitas/AutoGPT/issues/new/choose). Please ensure someone else hasn’t created an issue for the same topic.\n\n## 🤝 Sister projects\n\n### 🔄 Agent Protocol\n\nTo maintain a uniform standard and ensure seamless compatibility with many current and future applications, AutoGPT employs the [agent protocol](https://agentprotocol.ai/) standard by the AI Engineer Foundation. This standardizes the communication pathways from your agent to the frontend and benchmark.\n\n---\n\n<p align=\"center\">\n<a href=\"https://star-history.com/#Significant-Gravitas/AutoGPT\">\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://api.star-history.com/svg?repos=Significant-Gravitas/AutoGPT&type=Date&theme=dark\" />\n    <source media=\"(prefers-color-scheme: light)\" srcset=\"https://api.star-history.com/svg?repos=Significant-Gravitas/AutoGPT&type=Date\" />\n    <img alt=\"Star History Chart\" src=\"https://api.star-history.com/svg?repos=Significant-Gravitas/AutoGPT&type=Date\" />\n  </picture>\n</a>\n</p>",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "autogpt",
        "automation",
        "autogpt123",
        "virtual assistants",
        "imgompanda autogpt123",
        "ai agents"
      ],
      "category": "virtual-assistants"
    },
    "itsuzef--reaper-mcp": {
      "owner": "itsuzef",
      "name": "reaper-mcp",
      "url": "https://github.com/itsuzef/reaper-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/itsuzef.webp",
      "description": "Integrate AI-driven music production workflows with REAPER to manage projects, tracks, MIDI composition, audio recording, mixing, and mastering. Provide full control over MIDI and audio capabilities for creating and rendering music tracks.",
      "stars": 15,
      "forks": 1,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-09-20T09:39:44Z",
      "readme_content": "# REAPER MCP Server\n\nA comprehensive Model Context Protocol (MCP) server that enables AI agents to create fully mixed and mastered tracks in REAPER with both MIDI and audio capabilities.\n\n## Features\n\n- Complete project management (creation, saving, rendering)\n- Track operations (creation, routing, parameter adjustment)\n- MIDI composition and editing\n- Audio recording and importing\n- Virtual instrument and effect management\n- Mixing and automation\n- Mastering tools\n- Audio analysis and feedback\n\n## Requirements\n\n- REAPER DAW installed\n- Python 3.8+\n- OSC support enabled in REAPER (for OSC mode)\n- ReaScript API enabled in REAPER (for ReaScript mode)\n\n## Installation\n\n```bash\n# Clone the repository\ngit clone https://github.com/itsuzef/reaper-mcp.git\ncd reaper-mcp\n\n# Create and activate a virtual environment\npython -m venv venv\nsource venv/bin/activate  # On Windows: venv\\Scripts\\activate\n\n# Install the package\npip install -e .\n```\n\n## Usage\n\n### Quick Start\n\nThe easiest way to get started is to use the provided startup script:\n\n```bash\n# Start REAPER first\nopen /Applications/REAPER.app  # On macOS\n# or start REAPER manually on other platforms\n\n# Then start the MCP server\n./scripts/start_reaper_mcp_server.sh  # On Unix/Mac\n```\n\n#### Windows Users\n\nFor Windows users, use one of the provided Windows scripts:\n\n```cmd\n# Using Command Prompt (CMD)\nscripts\\start_reaper_mcp_server.bat\n\n# Using PowerShell\npowershell -ExecutionPolicy Bypass -File scripts\\start_reaper_mcp_server.ps1\n```\n\n### Configuration\n\nBy default, the server will use OSC mode, which is more reliable and doesn't require the ReaScript API to be working correctly. You can configure the server using command-line arguments:\n\n```bash\n# Start in OSC mode (default)\n./scripts/start_reaper_mcp_server.sh --mode=osc  # Unix/Mac\nscripts\\start_reaper_mcp_server.bat --mode=osc   # Windows CMD\npowershell -File scripts\\start_reaper_mcp_server.ps1 -mode osc  # Windows PowerShell\n\n# Start in ReaScript mode\n./scripts/start_reaper_mcp_server.sh --mode=reapy  # Unix/Mac\nscripts\\start_reaper_mcp_server.bat --mode=reapy   # Windows CMD\npowershell -File scripts\\start_reaper_mcp_server.ps1 -mode reapy  # Windows PowerShell\n\n# Configure OSC settings (Unix/Mac)\n./scripts/start_reaper_mcp_server.sh --host=192.168.1.110 --send-port=8000 --receive-port=9000\n\n# Configure OSC settings (Windows CMD)\nscripts\\start_reaper_mcp_server.bat --host=192.168.1.110 --send-port=8000 --receive-port=9000\n\n# Configure OSC settings (Windows PowerShell)\npowershell -File scripts\\start_reaper_mcp_server.ps1 -host \"192.168.1.110\" -sendPort 8000 -receivePort 9000\n\n# Enable debug logging\n./scripts/start_reaper_mcp_server.sh --debug  # Unix/Mac\nscripts\\start_reaper_mcp_server.bat --debug   # Windows CMD\npowershell -File scripts\\start_reaper_mcp_server.ps1 -debug  # Windows PowerShell\n```\n\n### Setting up REAPER for OSC\n\n1. Open REAPER\n2. Go to Preferences > Control/OSC/web\n3. Click \"Add\" and select \"OSC (Open Sound Control)\"\n4. Configure the following settings:\n   - Device name: ReaperMCP\n   - Mode: Local port\n   - Local listen port: 8000\n   - Local IP: 127.0.0.1 (or your computer's IP address)\n   - Allow binding messages to REAPER actions and FX learn: Checked (optional)\n   - Outgoing max packet size: 1024\n   - Wait between packets: 10ms\n\n### Setting up REAPER for ReaScript\n\n1. Open REAPER\n2. Go to Preferences > Plug-ins > ReaScript\n3. Make sure \"Enable Python for ReaScript\" is checked\n4. Set the Python DLL/dylib path to your Python installation\n   - On macOS: `/opt/homebrew/Cellar/python@3.x/3.x.x/Frameworks/Python.framework/Versions/3.x/Python`\n   - On Windows: `C:\\Path\\to\\Python\\python3x.dll`\n5. Run the setup script:\n   ```bash\n   python scripts/setup_reaper_python.py\n   ```\n\n## Project Structure\n\n- `src/reaper_mcp/`: Main package directory\n  - `__main__.py`: Command-line interface\n  - `osc_server.py`: OSC-based server implementation\n  - `server.py`: ReaScript-based server implementation\n- `examples/`: Example scripts demonstrating usage\n- `scripts/`: Utility scripts for setup and running\n\n## MCP Tools\n\nThe server provides the following MCP tools:\n\n- `create_project`: Creates a new REAPER project\n- `create_track`: Creates a new track in the current project\n- `list_tracks`: Lists all tracks in the current project\n- `add_midi_note`: Adds a MIDI note to a track\n- `get_project_info`: Gets information about the current project\n\n## Troubleshooting\n\n### ReaScript API Issues\n\nIf you're experiencing issues with the ReaScript API, try using the OSC mode instead:\n\n```bash\n./scripts/start_reaper_mcp_server.sh --mode=osc\n```\n\n### OSC Communication Issues\n\nMake sure REAPER is configured correctly for OSC:\n1. Check that the OSC settings in REAPER match the server settings\n2. Verify that no firewall is blocking the communication\n3. Try using the local IP address (127.0.0.1) instead of a network IP\n\n### Windows-Specific Troubleshooting\n\nIf you're having issues running the MCP server on Windows:\n\n1. **Script Execution Issues**:\n   - For PowerShell scripts, you may need to adjust the execution policy: `Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope CurrentUser`\n   - Alternatively, use the `-ExecutionPolicy Bypass` flag as shown in the examples\n\n2. **Path Issues**:\n   - Ensure the REAPER path in the scripts matches your installation location\n   - Default is `C:\\Program Files\\REAPER\\reaper.exe`, modify if needed\n\n3. **Virtual Environment**:\n   - If you created the venv with a different method, the activation script might be in a different location\n   - Try activating manually before running: `venv\\Scripts\\activate`\n\n4. **Firewall Blocking**:\n   - Windows Firewall may block OSC communication\n   - Add exceptions for Python and REAPER in Windows Firewall settings\n\n5. **Administrator Rights**:\n   - Try running the Command Prompt or PowerShell as Administrator if you encounter permission issues\n\n## License\n\nMIT\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "midi",
        "reaper",
        "mastering",
        "workflows reaper",
        "reaper mcp",
        "midi composition"
      ],
      "category": "virtual-assistants"
    },
    "jlucaso1--whatsapp-mcp-ts": {
      "owner": "jlucaso1",
      "name": "whatsapp-mcp-ts",
      "url": "https://github.com/jlucaso1/whatsapp-mcp-ts",
      "imageUrl": "/freedevtools/mcp/pfp/jlucaso1.webp",
      "description": "Connects personal WhatsApp accounts to AI agents, enabling the search of messages, contacts, and recent chats. Supports sending messages to individuals or groups and retrieving message history for specific chats.",
      "stars": 37,
      "forks": 11,
      "license": "No License",
      "language": "TypeScript",
      "updated_at": "2025-09-09T01:59:37Z",
      "readme_content": "# WhatsApp MCP Server (TypeScript/Baileys)\n[![smithery badge](https://smithery.ai/badge/@jlucaso1/whatsapp-mcp-ts)](https://smithery.ai/server/@jlucaso1/whatsapp-mcp-ts)\n\nThis is a Model Context Protocol (MCP) server for WhatsApp, built with TypeScript and using the `@whiskeysockets/baileys` library.\n\nIt allows you to connect your personal WhatsApp account to an AI agent (like Anthropic Claude via its desktop app or Cursor) enabling it to:\n\n* Search your personal WhatsApp messages.\n* Search your contacts (individuals, not groups).\n* List your recent chats.\n* Retrieve message history for specific chats.\n* Send messages to individuals or groups.\n\nIt connects directly to your **personal WhatsApp account** using the WhatsApp Web multi-device API. All your messages and authentication details are stored **locally** in a SQLite database (`./data/`) and authentication cache (`./auth_info/`). Data is only sent to the connected AI agent when it explicitly uses the provided MCP tools (which you control via the agent's interface).\n\n*(Optional: Consider adding a screenshot or GIF similar to the reference example here)*\n\n## Example\n\n**User:**\nSend a whatsapp message to \"Meu amor\" in whatsapp saying \"Te amo\"\n\n---\n\n**Assistant:**\nOkay, I need to find the contact first.\n*Using tool: `whatsapp.search_contacts`*\n```json\n{\n  \"query\": \"Meu amor\"\n}\n```\n\n---\n\n**Tool Result:**\n```json\n[\n  {\n    \"jid\": \"5599xxxxxx@s.whatsapp.net\",\n    \"name\": \"Meu Amor\"\n  }\n]\n```\n\n---\n\n**Assistant:**\nFound the contact. Now sending the message.\n*Using tool: `whatsapp.send_message`*\n```json\n{\n  \"recipient\": \"5599xxxxxx@s.whatsapp.net\",\n  \"message\": \"Te amo\"\n}\n```\n\n---\n\n**Tool Result:**\n```text\nMessage sent successfully to 5599xxxxxx@s.whatsapp.net (ID: XXXXXXXXXXX).\n```\n\n## Key Features (MCP Tools)\n\nThe server exposes the following tools to the connected AI agent:\n\n* `search_contacts`: Search for contacts by name or phone number part (JID).\n* `list_messages`: Retrieve message history for a specific chat, with pagination.\n* `list_chats`: List your chats, sortable by activity or name, filterable, paginated, optionally includes last message details.\n* `get_chat`: Get detailed information about a specific chat.\n* `get_message_context`: Retrieve messages sent immediately before and after a specific message ID for context.\n* `send_message`: Send a text message to a specified recipient JID (user or group).\n\n## Installation\n\n### Installing via Smithery\n\nTo install WhatsApp MCP Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@jlucaso1/whatsapp-mcp-ts):\n\n```bash\nnpx -y @smithery/cli install @jlucaso1/whatsapp-mcp-ts --client claude\n```\n\n### Prerequisites\n\n* **Node.js:** Version 23.10.0 or higher (as specified in `package.json`). You can check your version with `node -v`. (Has initial typescript and sqlite builtin support)\n* **npm** (or yarn/pnpm): Usually comes with Node.js.\n* **AI Client:** Anthropic Claude Desktop app, Cursor, Cline or Roo Code (or another MCP-compatible client).\n\n### Steps\n\n1.  **Clone this repository:**\n    ```bash\n    git clone <your-repo-url> whatsapp-mcp-ts\n    cd whatsapp-mcp-ts\n    ```\n\n2.  **Install dependencies:**\n    ```bash\n    npm install\n    # or yarn install / pnpm install\n    ```\n\n3.  **Run the server for the first time:**\n    Use `node` to run the main script directly.\n    ```bash\n    node src/main.ts\n    ```\n    * The first time you run it, it will likely generate a QR code link using `quickchart.io` and attempt to open it in your default browser.\n    * Scan this QR code using your WhatsApp mobile app (Settings > Linked Devices > Link a Device).\n    * Authentication credentials will be saved locally in the `auth_info/` directory (this is ignored by git).\n    * Messages will start syncing and be stored in `./data/whatsapp.db`. This might take some time depending on your history size. Check the `wa-logs.txt` and console output for progress.\n    * Keep this terminal window running. After syncing you can close.\n\n## Configuration for AI Client\n\nYou need to tell your AI client how to start this MCP server.\n\n1.  **Prepare the configuration JSON:**\n    Copy the following JSON structure. You'll need to replace `{{PATH_TO_REPO}}` with the **absolute path** to the directory where you cloned this repository.\n\n    ```json\n    {\n      \"mcpServers\": {\n        \"whatsapp\": {\n          \"command\": \"node\",\n          \"args\": [\n            \"{{PATH_TO_REPO}}/src/main.ts\"\n          ],\n          \"timeout\": 15, // Optional: Adjust startup timeout if needed\n          \"disabled\": false\n        }\n      }\n    }\n    ```\n    * **Get the absolute path:** Navigate to the `whatsapp-mcp-ts` directory in your terminal and run `pwd`. Use this output for `{{PATH_TO_REPO}}`.\n\n2.  **Save the configuration file:**\n    * For **Claude Desktop:** Save the JSON as `claude_desktop_config.json` in its configuration directory:\n        * macOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\n        * Windows: `%APPDATA%\\Claude\\claude_desktop_config.json` (Likely path, verify if needed)\n        * Linux: `~/.config/Claude/claude_desktop_config.json` (Likely path, verify if needed)\n    * For **Cursor:** Save the JSON as `mcp.json` in its configuration directory:\n        * `~/.cursor/mcp.json`\n\n3.  **Restart Claude Desktop / Cursor:**\n    Close and reopen your AI client. It should now detect the \"whatsapp\" MCP server and allow you to use its tools.\n\n## Usage\n\nOnce the server is running (either manually via `node src/main.ts` or started by the AI client via the config file) and connected to your AI client, you can interact with your WhatsApp data through the agent's chat interface. Ask it to search contacts, list recent chats, read messages, or send messages.\n\n## Architecture Overview\n\nThis application is a single Node.js process that:\n\n1.  Uses `@whiskeysockets/baileys` to connect to the WhatsApp Web API, handling authentication and real-time events.\n2.  Stores WhatsApp chats and messages locally in a SQLite database (`./data/whatsapp.db`) using `node:sqlite`.\n3.  Runs an MCP server using `@modelcontextprotocol/sdk` that listens for requests from an AI client over standard input/output (stdio).\n4.  Provides MCP tools that query the local SQLite database or use the Baileys socket to send messages.\n5.  Uses `pino` for logging activity (`wa-logs.txt` for WhatsApp events, `mcp-logs.txt` for MCP server activity).\n\n## Data Storage & Privacy\n\n* **Authentication:** Your WhatsApp connection credentials are stored locally in the `./auth_info/` directory.\n* **Messages & Chats:** Your message history and chat metadata are stored locally in the `./data/whatsapp.db` SQLite file.\n* **Local Data:** Both `auth_info/` and `data/` are included in `.gitignore` to prevent accidental commits. **Treat these directories as sensitive.**\n* **LLM Interaction:** Data is only sent to the connected Large Language Model (LLM) when the AI agent actively uses one of the provided MCP tools (e.g., `list_messages`, `send_message`). The server itself does not proactively send your data anywhere else.\n\n## Technical Details\n\n* **Language:** TypeScript\n* **Runtime:** Node.js (>= v23.10.0)\n* **WhatsApp API:** `@whiskeysockets/baileys`\n* **MCP SDK:** `@modelcontextprotocol/sdk`\n* **Database:** `node:sqlite` (Bundled SQLite)\n* **Logging:** `pino`\n* **Schema Validation:** `zod` (for MCP tool inputs)\n\n## Troubleshooting\n\n* **QR Code Issues:**\n    * If the QR code link doesn't open automatically, check the console output for the `quickchart.io` URL and open it manually.\n    * Ensure you scan the QR code promptly with your phone's WhatsApp app.\n* **Authentication Failures / Logged Out:**\n    * If the connection closes with a `DisconnectReason.loggedOut` error, you need to re-authenticate. Stop the server, delete the `./auth_info/` directory, and restart the server (`node src/main.ts`) to get a new QR code.\n* **Message Sync Issues:**\n    * Initial sync can take time. Check `wa-logs.txt` for activity.\n    * If messages seem out of sync or missing, you might need a full reset. Stop the server, delete **both** `./auth_info/` and `./data/` directories, then restart the server to re-authenticate and resync history.\n* **MCP Connection Problems (Claude/Cursor):**\n    * Double-check the `command` and `args` (especially the `{{PATH_TO_REPO}}`) in your `claude_desktop_config.json` or `mcp.json`. Ensure the path is absolute and correct.\n    * Verify Node.js are correctly installed and in your system's PATH.\n    * Check the AI client's logs for errors related to starting the MCP server.\n    * Check this server's logs (`mcp-logs.txt`) for MCP-related errors.\n* **Errors Sending Messages:**\n    * Ensure the recipient JID is correct (e.g., `number@s.whatsapp.net` for users, `groupid@g.us` for groups).\n    * Check `wa-logs.txt` for specific errors from Baileys.\n* **General Issues:** Check both `wa-logs.txt` and `mcp-logs.txt` for detailed error messages.\n\nFor further MCP integration issues, refer to the [official MCP documentation](https://modelcontextprotocol.io/quickstart/server#claude-for-desktop-integration-issues).\n\n## Credits\n\n- https://github.com/lharries/whatsapp-mcp Do the same as this codebase but uses go and python.\n\n## License\n\nThis project is licensed under the ISC License (see `package.json`).\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "whatsapp",
        "jlucaso1",
        "chats",
        "jlucaso1 whatsapp",
        "whatsapp mcp",
        "whatsapp accounts"
      ],
      "category": "virtual-assistants"
    },
    "logos-42--ANPtest": {
      "owner": "logos-42",
      "name": "ANPtest",
      "url": "https://github.com/logos-42/ANPtest",
      "imageUrl": "/freedevtools/mcp/pfp/logos-42.webp",
      "description": "Connects to AI agents using self-compressed decentralized identifiers (DIDs). Facilitates interactive conversations with an AI assistant in a talk show style, featuring functionalities for DID generation and QR code display.",
      "stars": 0,
      "forks": 0,
      "license": "No License",
      "language": "JavaScript",
      "updated_at": "2025-04-01T07:36:48Z",
      "readme_content": "# 脱口秀AI智能体\r\n\r\n基于自压缩DID技术，使用硅基流动API实现的脱口秀AI助手。\r\n\r\n## 项目特点\r\n\r\n- 实现自压缩DID，无需额外查询即可连接智能体\r\n- 使用硅基流动的DeepSeek-R1-Distill-Qwen-32B大模型\r\n- 提供生成DID、二维码展示和聊天功能\r\n- 支持智能体间的通信与连接\r\n\r\n## 技术栈\r\n\r\n- Next.js: React框架\r\n- 自压缩DID: 去中心化身份识别\r\n- 硅基流动API: AI大模型服务\r\n- Vercel: 部署服务\r\n\r\n## 快速开始\r\n\r\n### 本地开发\r\n\r\n1. 克隆项目并安装依赖:\r\n\r\n```bash\r\ngit clone <repository-url>\r\ncd comedyagent\r\nnpm install\r\n```\r\n\r\n2. 配置API密钥:\r\n\r\n如需使用不同的API密钥，请修改`lib/ai.js`文件中的`API_KEY`变量。\r\n\r\n3. 启动开发服务器:\r\n\r\n```bash\r\nnpm run dev\r\n```\r\n\r\n4. 访问 http://localhost:3000 查看应用。\r\n\r\n### Vercel部署\r\n\r\n1. Fork此仓库到您的GitHub账户。\r\n\r\n2. 在Vercel上创建新项目，并连接您的GitHub仓库。\r\n\r\n3. 部署完成后，即可通过Vercel提供的URL访问应用。\r\n\r\n## 使用说明\r\n\r\n### 生成DID\r\n\r\n1. 在首页点击\"生成DID\"按钮。\r\n2. 系统会生成一个自包含DID，并以文本和二维码形式显示。\r\n3. 您可以复制DID或分享二维码。\r\n\r\n### 测试智能体\r\n\r\n1. 在首页的聊天框中输入消息。\r\n2. 点击\"发送\"按钮或按Enter键。\r\n3. 智能体会以脱口秀演员的风格回复您的消息。\r\n\r\n### 连接其他智能体\r\n\r\n1. 前往\"/connect\"页面。\r\n2. 输入其他智能体的DID。\r\n3. 点击\"连接\"按钮。\r\n4. 连接成功后，您可以向该智能体发送消息。\r\n\r\n您还可以通过以下方式连接智能体:\r\n\r\n- 在浏览器中打开`{您的域名}/connect?did={DID字符串}`\r\n- 或者创建一个`did://`协议链接: `did://{DID字符串}`\r\n\r\n## 项目结构\r\n\r\n```\r\ncomedyagent/\r\n├── api/                  # API路由\r\n│   ├── generate-did.js   # DID生成API\r\n│   └── message.js        # 消息处理API\r\n├── components/           # React组件\r\n├── lib/                  # 工具库\r\n│   ├── ai.js             # AI服务\r\n│   └── did.js            # DID功能\r\n├── pages/                # 页面\r\n│   ├── index.js          # 首页\r\n│   └── connect.js        # 连接页面\r\n├── public/               # 静态资源\r\n├── styles/               # 样式文件\r\n│   ├── Home.module.css   # 首页样式\r\n│   └── Connect.module.css# 连接页面样式\r\n├── package.json          # 项目配置\r\n└── vercel.json           # Vercel配置\r\n```\r\n\r\n## 自压缩DID详解\r\n\r\n本项目中的自压缩DID是一种创新的数字身份表示方式，包含了以下信息:\r\n\r\n- 身份标识\r\n- 公钥\r\n- 服务端点\r\n- 元数据\r\n- 数字签名\r\n\r\n与传统DID不同，自压缩DID将所有必要信息编码在一个字符串中，无需查询额外服务器即可获取身份信息和通信方式。\r\n\r\n## 许可证\r\n\r\nMIT ",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "ai",
        "agents",
        "assistant",
        "ai assistant",
        "ai agents",
        "connects ai"
      ],
      "category": "virtual-assistants"
    },
    "mfukushim--map-traveler-mcp": {
      "owner": "mfukushim",
      "name": "map-traveler-mcp",
      "url": "https://github.com/mfukushim/map-traveler-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/mfukushim.webp",
      "description": "Create immersive travel experiences by navigating Google Maps with an avatar, providing real-time updates and photos of the journey. Integrate unique travel narratives and social media interactions during the exploration.",
      "stars": 23,
      "forks": 10,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-10-02T03:51:13Z",
      "readme_content": "# Virtual Traveling bot environment for MCP\n\n[![Verified on MseeP](https://mseep.ai/badge.svg)](https://mseep.ai/app/073d88cc-277d-40b6-8c20-bcabf6c275e9)\n[![smithery badge](https://smithery.ai/badge/@mfukushim/map-traveler-mcp)](https://smithery.ai/server/@mfukushim/map-traveler-mcp)\n\nEnglish / [Japanese](./README_jp.md)\n\nThis is an MCP server that creates an environment for an avatar to virtually travel on Google Maps.\n\nFrom an MCP client such as Claude Desktop, you can give instructions to the avatar and report on the progress of its journey with photos.\n\n<img alt=\"img_5.png\" src=\"https://raw.githubusercontent.com/mfukushim/map-traveler-mcp/for_image/tools/img_5.png\" width=\"400\"/>\n\n> Preparing for MCP Registry Support https://blog.modelcontextprotocol.io/posts/2025-09-08-mcp-registry-preview/  \n\n> Added gemini-2.5-flash-image-preview (nano-banana) to travel image generation  \n\nSupport for nano-banana has been added. Nano-banana's semantic mask allows you to generate composite travel images in a short time without setting remBg.  \nAlthough conventional image synthesis is still possible, we recommend using Gemini nano-banana.  \n\n> Supports both Streamable-HTTP and stdio (compliant with Smithery.ai's config interface)  \n\nIt can be used as a stdio-type MCP as before, or as Streamable-HTTP.  \nAlthough it supports multiple users, the database API must be specified per session using the Smithery.ai config interface.  \nSince it supports both Streamable-HTTP and stdio, it is expected to work as is with the previous MCP client, but if you use the previous stdio version, please use v0.0.x (v0.0.81).  \n``` npx -y @mfukushim/map-traveler-mcp@0.0.81 ```  \n\n> Now supports librechat https://www.librechat.ai/.\n\n> Now supports Smithery https://smithery.ai/server/@mfukushim/map-traveler-mcp (images are excluded because they are heavy to run).\n\n> Now verified MseeP https://mseep.ai/app/mfukushim-map-traveler-mcp \n\n## Functions\n\n#### MCP server tools function\n\nThe following functions can be used as an MCP server. The available functions vary depending on the settings and execution state.\n\nYou can specify the function name directly, but Claude LLM will automatically recognize it, so you can specify the operation in general terms.\n\nExample:\n\"Where are you now?\" \"Let's leave for Tokyo Station.\"\n\n- get_traveler_view_info(includePhoto:boolean,includeNearbyFacilities:boolean)  \n  Gets information about the current travel avatar's location.  \n  - includePhoto: Gets nearby Google Street View photos. If you have set up an image generation AI, it will synthesize the avatar.\n  - includeNearbyFacilities: Gets information about nearby facilities.\n- get_traveler_location()  \n  Gets information about the current travel avatar's address and nearby facilities.\n- reach_a_percentage_of_destination()\n  Reach a specified percentage of the destination (moveMode=skip only)\n  timeElapsedPercentage: Percent progress towards destination(0~100)\n- set_traveler_location(address: string)  \n  Sets the current travel avatar's location.\n  - address: Address information (exact address, or general name that Google Maps or Claude can recognize, etc.)\n- get_traveler_destination_address  \n  Get the destination of the travel avatar you set\n- set_traveler_destination_address(address: string)  \n  Set the destination of the travel avatar\n   - address: Address information (exact address, or general name that Google Maps or Claude can recognize, etc.)\n- start_traveler_journey  \n  Start the journey at the destination.(moveMode=realtime only)\n- stop_traveler_journey  \n  Stop the journey.(moveMode=realtime only)\n- set_traveler_info(settings:string)  \n  Set the traveler's attributes. Set the traveler's personality that you want to change dynamically, such as name and personality. However, if you use a role script, the script is more stable.\n  - settings: Setting information such as name and personality.\n- get_traveler_info  \n  Get the traveler's attributes. Get the traveler's personality.\n- set_avatar_prompt(prompt:string)  \n  Set the prompt when generating the travel avatar image. The default is an anime-style woman. The anime style is enforced to prevent fake images.\n  - prompt\n- reset_avatar_prompt  \n  Reset avatar generation prompts to default.\n- get_sns_feeds  \n  Gets Bluesky SNS articles for the specified custom feed (feeds containing a specific tag).\n- get_sns_mentions  \n  Gets recent mentions (likes, replies) to Bluesky SNS posts that you made yourself.\n- post_sns_writer(message:string)  \n  Posts an article to Bluesky SNS with the specified custom feed. Set a specific tag so that it can be determined that the post was generated by the travel bot.\n  - message: article\n- reply_sns_writer(message:string,id:string)  \n  Reply to the article with the specified id. Set a specific tag so that it can be determined that the post was generated by the travel bot.\n  - message: reply\n  - id: The ID of the post to reply to\n- add_like(id:string)  \n  Add a like to the specified post.\n  - id: The ID of the post to like\n- tips  \n  Guides you on how to set up features that have not yet been set.\n- get_setting  \n  Get environment and image settings.\n\n#### MCP resources\n\nHas five custom prompt samples.\nWhen you import a prompt with Claude Desktop, Claude will act as a traveler.\nThe SNS-compatible version controls SNS input and output while having a travel conversation.\n\n- role.txt  \n  Claude will act as a traveler.\n\n- roleWithSns.txt  \n  Claude will act as a traveler. It also controls reading and posting to SNS.\n- carBattle.txt  \n  This is a small novel game about a story of transporting secret documents from Yokohama to Tokyo. Scenes are automatically generated. Set moveMode=skip to play.\n- japanMapChallenge.txt,japanMapChallenge2.txt  \n  Two AIs communicate with each other via SNS and play a challenge game using landscape images.  \n  To play, you need two Bluesky accounts and two Claude Desktops. Also set moveMode=skip. (However, the operation is somewhat unstable.)  \n  japanMapChallenge2 has a challenge reflection rule.\n\n## Setting\n\nYou will need to obtain and set access keys for multiple APIs, such as for accessing multiple Google maps and generating images.\nUse of the API may incur charges.\n\n#### Settings for using with Claude Desktop \n\n- claude_desktop_config.json (stdio type)\n```json\n{\n  \"mcpServers\": {\n    \"traveler\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@mfukushim/map-traveler-mcp\"],\n      \"env\":{\n        \"MT_GOOGLE_MAP_KEY\":\"(Google Map API key)\",\n        \"MT_GEMINI_IMAGE_KEY\": \"(Gemini Image Api key)\",\n        \"MT_MAX_RETRY_GEMINI\": \"(Number of retries when generating Gemini images Default: 0)\",\n        \"MT_AVATAR_IMAGE_URI\": \"(Character reference image uri (file:// or https://) when generating Gemini image. Multiple settings can be made by separating them with the '|'. When multiple settings are made, they will be selected randomly.)\",\n        \"MT_MAP_API_URL\": \"(Optional: Map API custom endpoint. Example: direction=https://xxxx,places=https://yyyy )\",\n        \"MT_TIME_SCALE\": \"(Optional:Scale of travel time on real roads duration. default 4)\",\n        \"MT_SQLITE_PATH\":\"(db save path: e.g. %USERPROFILE%/Desktop/traveler.sqlite ,$HOME/traveler.sqlite )\",\n        \"MT_TURSO_URL\":\"(Turso sqlite API URL)\",\n        \"MT_TURSO_TOKEN\":\"(Turso sqlite API access token)\",\n        \"MT_REMBG_PATH\": \"(absolute path of the installed rembg cli)\",\n        \"MT_REMBG_URL\": \"(rembg API URL)\",\n        \"MT_REMBG_WO_KEY\": \"(withoutbg.com rembg API key)\",\n        \"MT_PIXAI_KEY\":\"(pixAi API key)\",\n        \"MT_SD_KEY\":\"(or Stability.ai image generation API key\",\n        \"MT_PIXAI_MODEL_ID\": \"(Optional: pixAi ModelId, if not set use default model 1648918127446573124 \",\n        \"MT_COMFY_URL\": \"(Option: Generate image using ComfyUI API at specified URL. Example: http://192.168.1.100:8188)\",\n        \"MT_COMFY_WORKFLOW_T2I\": \"(Optional: Path to API workflow file when using text to image with ComfyUI. If not specified: assets/comfy/t2i_sample.json)\",\n        \"MT_COMFY_WORKFLOW_I2I\": \"(Optional: Path of API workflow file when image to image in ComfyUI. If not specified: assets/comfy/i2i_sample.json)\",\n        \"MT_COMFY_PARAMS\": \"(Optional: Variable values to send to the workflow via comfyUI API)\",\n        \"MT_FIXED_MODEL_PROMPT\": \"(Optional: Fixed avatar generation prompt. You will no longer be able to change your avatar during conversations.)\",\n        \"MT_BODY_AREA_RATIO\": \"(Optional: Acceptable avatar image area ratio. default 0.042)\",\n        \"MT_BODY_HW_RATIO\": \"(Optional: Acceptable avatar image aspect ratios. default 1.5~2.3)\",\n        \"MT_BODY_WINDOW_RATIO_W\": \"(Optional: Avatar composite window horizontal ratio. default 0.5)\",\n        \"MT_BODY_WINDOW_RATIO_H\": \"(Optional: Avatar composite window aspect ratio. default 0.75)\",\n        \"MT_BS_ID\":\"(Bluesky sns registration address)\",\n        \"MT_BS_PASS\":\"(bluesky sns password)\",\n        \"MT_BS_HANDLE\":\"(bluesky sns handle name: e.g. xxxxxxxx.bsky.social )\",\n        \"MT_FILTER_TOOLS\": \"(Optional: Directly filter the tools to be used. All are available if not specified. e.g. tips,set_traveler_location)\",\n        \"MT_MOVE_MODE\": \"(Option: Specify whether the movement mode is realtime or skip. default realtime)\",\n        \"MT_IMAGE_WIDTH\": \"(Option: Output image width (pixels) Default is 512)\",\n        \"MT_NO_IMAGE\": \"(Options: true = do not output image, not specified = output image if possible, default is not specified)\",\n        \"MT_NO_AVATAR\": \"(Option: true = Output StreetView image as is without avatar superimposition. Not specified = Superimpose avatar image. Default is not specified.)\",\n        \"MT_FEED_TAG\": \"(Optional: Specify the feed tag when posting to SNS (#required, 15 characters or more) Default is #geo_less_traveler)\",\n        \"MT_MAX_SESSIONS\": \"(Maximum number of sessions when using Streamable-http)\",\n        \"MT_SESSION_TTL_MS\": \"(Session TTL when using Streamable-http)\",\n        \"MT_SERVICE_TTL_MS\": \"(Service TTL when using Streamable-http)\"\n      }\n    }\n  }\n}\n```  \n\n- claude_desktop_config.json (streamable-http type)  \nThe above MT_ environment variables should be set as environment variables for the server that runs the map-traveler-mcp web service.  \n```json\n{\n  \"mcpServers\": {\n    \"traveler\": {\n      \"type\": \"streamable-http\",\n      \"url\": \"https://(mcp server address)/mcp?config=(base64 config json)\"\n    }\n  }\n}\n```  \n\nbase64 config json (Smithery.ai Expansion)  \nBy concatenating the json in the following format into a single line of string, converting it to base64, and setting it as (base64 setting json), you can overwrite different APIs and settings for each user session.  \nIf the database is not set base64 config json, it will be shared across the entire service (the location of the traveler will be shared across the database and counted for one person).  \nWe plan to reconsider the operation of assigning an individual UserId for each session once the MCP authentication mechanism has become a little clearer.  \n```json\n{\n  \"MT_GOOGLE_MAP_KEY\": \"xxxyyyzzz\",\n  \"MT_GEMINI_IMAGE_KEY\": \"xxyyzz\",\n  \"MT_MAX_RETRY_GEMINI\": \"1\",\n  \"MT_AVATAR_IMAGE_URI\": \"file:///C:/Users/xxxx/Desktop/avatar.png\",\n  \"MT_TURSO_URL\": \"libsql://xxxyyyzzz\",\n  \"MT_TURSO_TOKEN\": \"abcdabcd\",\n  \"MT_BS_ID\": \"xyxyxyxyx\",\n  \"MT_BS_PASS\": \"1234xyz\",\n  \"MT_BS_HANDLE\": \"aabbccdd\",\n  \"MT_FILTER_TOOLS\": \"tips,set_traveler_location\",\n  \"MT_MOVE_MODE\": \"direct\",\n  \"MT_FEED_TAG\": \"#abcdefgabcdefgabcdefg\"\n}\n```  \n(All json values can be omitted)  \n↓ (json text concatenation)  \n```text\n{\"MT_GOOGLE_MAP_KEY\": \"xxxyyyzzz\", \"MT_GEMINI_IMAGE_KEY\": \"xxyyzz\", \"MT_MAX_RETRY_GEMINI\": \"1\", \"MT_TURSO_URL\": \"libsql://xxxyyyzzz\", \"MT_TURSO_TOKEN\": \"abcdabcd\", \"MT_BS_ID\": \"xyxyxyxyx\", \"MT_BS_PASS\": \"1234xyz\", \"MT_BS_HANDLE\": \"aabbccdd\", \"MT_FILTER_TOOLS\": \"tips,set_traveler_location\", \"MT_MOVE_MODE\": \"direct\", \"MT_FEED_TAG\": \"#abcdefgabcdefgabcdefg\"}\n```\n↓ (Set the base64 version to config=)  \n```text\neyJNVF9HT09HTEVfTUFQX0tFWSI6ICJ4eHh5eXl6enoiLCAiTVRfR0VNSU5JX0lNQUdFX0tFWSI6ICJ4eHl5enoiLCAiTVRfTUFYX1JFVFJZX0dFTUlOSSI6ICIxIiwgIk1UX1RVUlNPX1VSTCI6ICJsaWJzcWw6Ly94eHh5eXl6enoiLCAiTVRfVFVSU09fVE9LRU4iOiAiYWJjZGFiY2QiLCAiTVRfQlNfSUQiOiAieHl4eXh5eHl4IiwgIk1UX0JTX1BBU1MiOiAiMTIzNHh5eiIsICJNVF9CU19IQU5ETEUiOiAiYWFiYmNjZGQiLCAiTVRfRklMVEVSX1RPT0xTIjogInRpcHMsc2V0X3RyYXZlbGVyX2xvY2F0aW9uIiwgIk1UX01PVkVfTU9ERSI6ICJkaXJlY3QiLCAiTVRfRkVFRF9UQUciOiAiI2FiY2RlZmdhYmNkZWZnYWJjZGVmZyJ9\n```\n\n\n> NOTE: The environment variables have been renamed to standard snake case. The MT_ prefix is added because they may be used in conjunction with other environment variables, such as in librechat. The old names can still be used for backward compatibility.  \n\nPlease set the following three Credentials for Google Map API.  \n- Street View Static API\n- Places API (New)\n- Time Zone API\n- Directions API\n\nhttps://developers.google.com/maps/documentation/streetview/get-api-key\n\nIf you want to use the image generation AI, set either pixAi_key or sd_key. You also need to have python3.7~3.11 installed on your PC and rembg cli installed (virtual environment recommended).\n\nhttps://platform.pixai.art/docs  \nhttps://platform.stability.ai/docs/api-reference#tag/SDXL-1.0-and-SD1.6/operation/textToImage\n\nThe bluesky SNS address/password are optional. It is recommended that you create a dedicated account as it will post automatically.\n\nhttps://bsky.app/\n\nYou can also run it in practice mode, which does not require an API key for verification.\n\n#### Practice mode settings  \nclaude_desktop_config.json\n```json\n{\n  \"mcpServers\": {\n    \"traveler\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@mfukushim/map-traveler-mcp\"]\n    }\n  }\n}\n```\n\n## How to use\n\n#### Use the practice mode\n\n1. Install nodejs 22.\n\n2. Set up Claude Desktop for use.\n\n3. Reflect one of the above settings in claude_desktop_config.json.\n\n4. Restart Claude Desktop. It may take some time to set up (if an error occurs, try restarting Claude Desktop again. If it doesn't work, see the notes below). Make sure the following mark appears in the bottom right of the screen.\n\n  <img alt=\"img_1.png\" src=\"https://raw.githubusercontent.com/mfukushim/map-traveler-mcp/for_image/tools/img_1.png\" width=\"150\"/>\n\n5. Ask \"Where are you now?\" and \"Go on a journey.\" A conversation will begin. When using the API, a confirmation screen will appear, so select Allow.\n\n<img alt=\"img_4.png\" src=\"https://raw.githubusercontent.com/mfukushim/map-traveler-mcp/for_image/tools/img_4.png\" width=\"200\"/>\n\n6. Select Attach from MCP and select role.txt.\n\n<img alt=\"img_2.png\" src=\"https://raw.githubusercontent.com/mfukushim/map-traveler-mcp/for_image/tools/img_2.png\" width=\"200\"/>\n\n<img alt=\"img_3.png\" src=\"https://raw.githubusercontent.com/mfukushim/map-traveler-mcp/for_image/tools/img_3.png\" width=\"200\"/>\n\n7. A travel prompt has been built in, so feel free to talk to it.\n\n#### Use the full feature\n\n1. Get a Google Map API access key and set the permissions for Street View Static API, Places API (New), Time Zone API, and Directions API. Set this in the env of claude_desktop_config.json and restart.\n   At this point, the travel log will be based on the real map. Travel images will also be output if they are not superimposed.\n2. Decide on a path that will not interfere with the disk and set it in the sqlite_path of the env of claude_desktop_config.json. (Example: %USERPROFILE%/Desktop/traveler.sqlite $HOME/Documents/traveler.sqlite, etc.)\n   At this point, your travel log will be saved and you can continue your journey even if you close Claude Desktop.\n3. Install python 3.7 to 3.11 and install rembg with cli. We recommend using a virtual environment such as venv.\n  ```bash\n  python3 -m venv venv\n  . venv/bin/activate or .\\venv\\Scripts\\activate\n  pip install \"rembg[cpu,cli]\"\n  ```\n  Check if rembg cli works properly using a sample image file. Input an image with a person in it, and if the person is cut out in the output file, it's OK.  \n  ```bash\n  rembg i source_image_file dest_image_file\n  ```\n4. rembg cli will be installed in the python exe location, so get the path. The file location varies depending on the OS and python installation status, but in the case of venv, it is (virtual environment name)\\Scripts\\rembg.exe or (virtual environment name)/bin/rembg above the directory you set. If you can't find it, search for the path with a file search software. Set that path to rembg_path of env in claude_desktop_config.json. (Example: \"rembg_path\": \"C:\\\\Users\\\\xxxx\\\\Documents\\\\rembg_venv\\\\venv\\\\Scripts\\\\rembg.exe\")\n5. Get an image generation API key from the pixAI or Stability.ai site. Set the key to pixAi_key or sd_key in env of claude_desktop_config.json.\n   The avatar will now be overlaid on the travel image.\n6. Get the bluesky SNS address/password and handle name. Set these in bs_id, bs_pass, and bs_handle in env of claude_desktop_config.json, respectively.\n   Import the travel knowledge prompt roleWithSns.txt to report travel actions to SNS (it will automatically post as a bot, so we recommend allocating a dedicated account)\n\nInstead of preparing rembg with the cli, we have added a setting that allows you to handle rembg as a service API.  \nIf you configure the following rembg service, you can use rembg by setting the URL in remBgUrl.  \n\nhttps://github.com/danielgatis/rembg?tab=readme-ov-file#rembg-s  \n\nSetup is simple if you use the Docker version to launch a container and access it.  \n\nhttps://github.com/danielgatis/rembg?tab=readme-ov-file#usage-as-a-docker  \n\n#### Use Turso libsql API for configuration database\n\nIf you want to use the cloud API Turso libsql (https://turso.tech/libsql) without having a local sqlite file, sign up for Turso and allocate a sqlite database (paid, free tier available).   \nThis add-in will automatically configure (migrate) the database.  \nMT_TURSO_URL = turso db URL  \nMT_TURSO_TOKEN = turso db access token  \n\n\n#### Use Cloud API for rembg\n\nLocal settings around rembg are complicated no matter what method you use, but we have added settings for the paid cloud rembg (https://withoutbg.com/).  \n> Note: There is a small free trial available, but please be aware that this is a commercial API and is quite expensive (about 0.1 euros per image).\n\nMT_REMBG_WO_KEY = withoutbg access token\n\n\n#### When using external ComfyUI (for more advanced users)\n\nYou can also use a local ComfyUI as an image generation server. You can configure the image generation characteristics yourself in detail to reduce API costs.\n\nHowever, the configuration will be quite complicated and image generation may take longer.\n\n1. Configure ComfyUI to run in API mode.\n2. Set the server URL to comfy_url in env.\n3. Set detailed configuration values such as the model to be used in env in the form of a json string.\nexample.\n```json\n{\n  \"env\": {\n    \"comfy_url\": \"http://192.168.1.100:8188\",\n    \"comfy_workflow_t2i\": \"C:\\\\Documents\\\\t2itest.json\",\n    \"comfy_workflow_i2i\":\"C:\\\\Documents\\\\i2itest.json\",\n    \"comfy_params\":\"ckpt_name='animagineXL40_v40.safetensors',denoise=0.65\"\n  }\n}\n```\n4. The default workflow can use assets/comfy/t2i_sample.json and assets/comfy/i2i_sample.json in the package. You can specify variables using % and specify the variables in comfy_params.\n\n## Using libreChat\n\nIt has been adapted to work with libreChat. This makes it easier to use, but some additional settings are required.  \nAlso, it seems that it will not be stable unless the PC you use has a decent level of performance, such as one that can stably run Docker.\n\n#### Install libreChat  \n\nPlease make sure it works as described on the official website.  \nIn this case, we recommend using Docker configuration due to additional settings.\n\nhttps://www.librechat.ai/docs/local/docker  \n\nConfigure librechat.yaml using the official procedure.  \nI think you will need to add a local or API LLM service.  \n\nhttps://www.librechat.ai/docs/configuration/librechat_yaml  \n\nAdd a user for login.  \n\nhttps://www.librechat.ai/docs/configuration/authentication#create-user-script  \n\nPlease set it so that you can have general chat conversations.  \n\n#### Add a rembg container with additional settings  \n\nTo use rembg with Docker, add pulling and running the rembg Docker container.  \n\ndocker-compose.override.yml\n```yml\n services:\n   api:\n     volumes:\n       - type: bind\n         source: ./librechat.yaml\n         target: /app/librechat.yaml\n\n   rembg:\n     image: danielgatis/rembg:latest\n     restart: always\n     command: \"s --host 0.0.0.0 --port 7000 --log_level info\"\n\n```\n\n#### Add map-traveler-mcp to the MCP service  \n\nAdd librechat.yaml\n```yaml\nmcpServers:\n  traveler:\n    type: stdio\n    command: npx\n    args:\n      - -y\n      - \"@mfukushim/map-traveler-mcp\"\n```\n\nAdd .env (Same as env in claude_desktop_config.json)\n\n```env\n# map-traveler-mcp\nGoogleMapApi_key=(Google Map API key)\nsqlite_path=/home/run_test.sqlite (e.g. librechat in an unobtrusive location inside the container, or in an external directory that you don't want to mount.)\nremBgUrl=http://rembg:7000 (rembg Service API URL, container URL)\n(Other settings such as image generation AI settings, PixAI key, stability.ai API key, ComfyUI settings, etc.)\n\n```\n\nAfter setting, restart the container.  \nOn slow PCs, mcp initialization may fail. Multiple restarts may work, but this may be difficult to run...\n\n#### llibreChat settings\n\nTo use the MCP function in libreChat, use the Agents function.  \n\n1. On the conversation screen, select Agents.  \n   <img alt=\"libre1.png\" src=\"https://raw.githubusercontent.com/mfukushim/map-traveler-mcp/for_image/tools/libre1.png\" width=\"200\"/>\n2. Select Agent Builder from the panel on the right side of the screen and configure your agent.  \n   <img alt=\"libre2.png\" src=\"https://raw.githubusercontent.com/mfukushim/map-traveler-mcp/for_image/tools/libre2.png\" width=\"200\"/>\n3. Select Add Tools to use map-traveler.  \n   <img alt=\"libre3.png\" src=\"https://raw.githubusercontent.com/mfukushim/map-traveler-mcp/for_image/tools/libre3.png\" width=\"200\"/>\n4. The agent tools screen will appear, so select and add all the map-traveler-mcp tools (if the map-traveler-mcp tools are not listed, MCP initialization has failed, so please restart the container or review the settings by checking the logs, etc.)  \n   <img alt=\"libre4.png\" src=\"https://raw.githubusercontent.com/mfukushim/map-traveler-mcp/for_image/tools/libre4.png\" width=\"200\"/>  \n   <img alt=\"libre5.png\" src=\"https://raw.githubusercontent.com/mfukushim/map-traveler-mcp/for_image/tools/libre5.png\" width=\"200\"/>  \n5. Enter additional script in the instruction area.  \n   Since libreChat does not have the MCP resource function, enter the content text of the following URL into the instruction area instead.   \n   https://github.com/mfukushim/map-traveler-mcp/blob/main/assets/scenario/role.txt  \n   <img alt=\"libre7.png\" src=\"https://raw.githubusercontent.com/mfukushim/map-traveler-mcp/for_image/tools/libre7.png\" width=\"200\"/>  \n6. Click the Create button to save the agent.  \n   <img alt=\"libre6.png\" src=\"https://raw.githubusercontent.com/mfukushim/map-traveler-mcp/for_image/tools/libre6.png\" width=\"200\"/>\n7. Start a new chat.\n\n## Smithery\n\nPlease refer to https://smithery.ai/server/@mfukushim/map-traveler-mcp.  \nRemote MCP (Streamable-http mode) is supported. Image generation is only available on nano-banana.  \nDatabase settings can now be recorded with Turso sqlite, so if you configure Turso, your travel progress will also be saved.  \n\n\n\n\n## Install guide (Japanese, but lots of photos)\n\n1. introduction and Practice mode  \n   https://note.com/marble_walkers/n/n7a8f79e4fb30\n2. DB, Google Map API, Image gen API  \n   https://note.com/marble_walkers/n/n765257c27f3b\n3. Avatar prompt  \n   https://note.com/marble_walkers/n/nc7273724faea\n4. SNS integration  \n   https://note.com/marble_walkers/n/na7c956befe7b\n5. Application 1  \n   https://note.com/marble_walkers/n/n3c86edd8e817\n6. ComfyUI API  \n   https://note.com/marble_walkers/n/ncefc7c05d102  \n7. Application 2  \n   https://note.com/marble_walkers/n/ne7584ed231c8\n8. LibreChat setting  \n   https://note.com/marble_walkers/n/n339bf7905324\n9. AI Agent SNS Battle Map Challenge  \n   https://note.com/marble_walkers/n/n6db937573eaa\n10. Support Smithery, Turso libSQL, and rembg API   \n   https://note.com/marble_walkers/n/ne3b3c0f99707\n11. Streamable-HTTP support  \n    https://note.com/marble_walkers/n/n030063f22dc0\n12. Nano-Banana support  \n    https://note.com/marble_walkers/n/n5d49514dddec  \n\n\n#### Additional about the source code\n\nI use Effect.ts to simplify error management & for my own learning.  \nWe also use the Effect Service, but due to the way MCP calls work, we believe that consolidating it using the Service was not optimal.  \nI think it would be simpler to handle the MCP calls directly in the Effect.  \nAddendum: I'm aware that I will be able to reconsider how to use the Effect Service and rewrite it neatly, but I'm still considering whether to rewrite it.  \n\n#### Notes on the latest updates\n\n- Added image_width to env. The default is 512. Setting it smaller may reduce the cost of LLM API.  \n- Added an env setting that does not output images for MCP clients that do not have image input/output.  \n\"MT_NO_IMAGE\": \"true\" will not generate or output any images. Other image-related settings can be omitted.  \n```\n{\n  \n  \"env\": {\n    \"MT_NO_IMAGE\": \"true\"\n  }\n  \n}\nor\n{\n  \n  \"env\": {\n    \"GoogleMapApi_key\": \"xxxx\",\n    \"MT_NO_IMAGE\": \"true\"\n  }\n  \n}\n\n```  \n- You can now specify the tag name to be added when posting to SNS (Bluesky). #Required and must be at least 15 characters. If not specified, it will become \"#geo_less_traveler\".  \n- The information obtained from SNS has been slightly changed. The information posted to SNS has been slightly changed.  \n- A script has been added that allows multiple travel bots to converse and play via SNS.  \n\n- Supports remote use from Smithery.  \n  If you do not want to configure detailed settings, start the app in practice mode.\n  You can also run the app at full speed by configuring each cloud API, but please be aware of charges as it uses many paid APIs such as rembg API.\n  If you do not want to synthesize avatars, you can run the app with the minimum settings of Google Map API and Turso sqlite API.\n\n- Added the MT_NO_AVATAR option.  \n  If set, an avatar image will not be composited onto the landscape image. Since there will be no retry processing for avatar composition, the time it takes to obtain a response will be significantly shorter.  \n  Set this option if image composition is slow or fails unavoidably.\n\n- Partially applied MCP version 2025-06-18.  \n  I added title to the schema. I plan to apply outputSchema and structured response in the future, but I haven't implemented them this time. Since the output of Travel Bot is simple text, I don't think structuring is necessary yet.  \n  https://modelcontextprotocol.io/specification/2025-06-18/server/tools  \n- Fixed an issue where some functions, such as SNS functions, could not be called regardless of the env settings due to an initialization error.  \n\n- Added support for Streamable-http. This was done in a hurry, so if you experience any issues, please consider using version 0.0.81 or similar.  \n\n- Support for nano-banana (gemini-2.5-flash-image-preview) image generation has been added. When using nano-banana, no rembg settings are required. The characteristics of the avatar prompt have changed, so image generation may fail with the previous avatar prompt. In this case, you will need to adjust the avatar appearance prompt to one that is acceptable for nano-banana.\n\n- When generating images for nano-banana, you can now reference the original character image with MT_AVATAR_IMAGE_URI. Please use it in a way that does not infringe on copyrights.\n\n[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/mfukushim-map-traveler-mcp-badge.png)](https://mseep.ai/app/mfukushim-map-traveler-mcp)",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "traveler",
        "maps",
        "immersive",
        "map traveler",
        "traveler mcp",
        "immersive travel"
      ],
      "category": "virtual-assistants"
    },
    "mrexodia--user-feedback-mcp": {
      "owner": "mrexodia",
      "name": "user-feedback-mcp",
      "url": "https://github.com/mrexodia/user-feedback-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/mrexodia.webp",
      "description": "Facilitates a feedback loop in tools for desktop application development by collecting user insights during complex interactions. Enhances testing processes by prompting users for feedback before task completion.",
      "stars": 47,
      "forks": 8,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-04T14:06:32Z",
      "readme_content": "# User Feedback MCP\r\n\r\nSimple [MCP Server](https://modelcontextprotocol.io/introduction) to enable a human-in-the-loop workflow in tools like [Cline](https://cline.bot) and [Cursor](https://www.cursor.com). This is especially useful for developing desktop applications that require complex user interactions to test.\r\n\r\n![Screenshot showing the feedback UI](https://github.com/mrexodia/user-feedback-mcp/blob/main/.github/feedback-ui.png?raw=true)\r\n\r\n## Prompt Engineering\r\n\r\nFor the best results, add the following to your custom prompt:\r\n\r\n> Before completing the task, use the user_feedback MCP tool to ask the user for feedback.\r\n\r\nThis will ensure Cline uses this MCP server to request user feedback before marking the task as completed.\r\n\r\n## `.user-feedback.json`\r\n\r\nHitting _Save Configuration_ creates a `.user-feedback.json` file in your project directory that looks like this:\r\n\r\n```json\r\n{\r\n  \"command\": \"npm run dev\",\r\n  \"execute_automatically\": false\r\n}\r\n```\r\n\r\nThis configuration will be loaded on startup and if `execute_automatically` is enabled your `command` will be instantly executed (you will not have to click _Run_ manually). For multi-step commands you should use something like [Task](https://taskfile.dev).\r\n\r\n## Installation (Cline)\r\n\r\nTo install the MCP server in Cline, follow these steps (see screenshot):\r\n\r\n![Screenshot showing installation steps](https://github.com/mrexodia/user-feedback-mcp/blob/main/.github/cline-installation.png?raw=true)\r\n\r\n1. Install [uv](https://github.com/astral-sh/uv) globally:\r\n   - Windows: `pip install uv`\r\n   - Linux/Mac: `curl -LsSf https://astral.sh/uv/install.sh | sh`\r\n2. Clone this repository, for this example `C:\\MCP\\user-feedback-mcp`.\r\n3. Navigate to the Cline _MCP Servers_ configuration (see screenshot).\r\n4. Click on the _Installed_ tab.\r\n5. Click on _Configure MCP Servers_, which will open `cline_mcp_settings.json`.\r\n6. Add the `user-feedback-mcp` server:\r\n\r\n```json\r\n{\r\n  \"mcpServers\": {\r\n    \"github.com/mrexodia/user-feedback-mcp\": {\r\n      \"command\": \"uv\",\r\n      \"args\": [\r\n        \"--directory\",\r\n        \"c:\\\\MCP\\\\user-feedback-mcp\",\r\n        \"run\",\r\n        \"server.py\"\r\n      ],\r\n      \"timeout\": 600,\r\n      \"autoApprove\": [\r\n        \"user_feedback\"\r\n      ]\r\n    }\r\n  }\r\n}\r\n\r\n```\r\n\r\n## Development\r\n\r\n```sh\r\nuv run fastmcp dev server.py\r\n```\r\n\r\nThis will open a web interface at http://localhost:5173 and allow you to interact with the MCP tools for testing.\r\n\r\n## Available tools\r\n\r\n```\r\n<use_mcp_tool>\r\n<server_name>github.com/mrexodia/user-feedback-mcp</server_name>\r\n<tool_name>user_feedback</tool_name>\r\n<arguments>\r\n{\r\n  \"project_directory\": \"C:/MCP/user-feedback-mcp\",\r\n  \"summary\": \"I've implemented the changes you requested.\"\r\n}\r\n</arguments>\r\n</use_mcp_tool>\r\n```",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "mrexodia",
        "feedback",
        "assistants",
        "virtual assistants",
        "assistants mrexodia",
        "facilitates feedback"
      ],
      "category": "virtual-assistants"
    },
    "mrgeeko--vapi-mcp": {
      "owner": "mrgeeko",
      "name": "vapi-mcp",
      "url": "https://github.com/mrgeeko/vapi-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/mrgeeko.webp",
      "description": "Integrate voice AI capabilities into applications for managing voice assistants and conducting outbound calls. Provides advanced features for enhancing user interactions through voice conversations.",
      "stars": 0,
      "forks": 0,
      "license": "No License",
      "language": "",
      "updated_at": "2025-04-07T08:10:10Z",
      "readme_content": "# Vapi MCP for Cursor\n\nThis project implements a Model Context Protocol (MCP) server for integrating Vapi's voice AI capabilities with Cursor.\n\n## Setup Instructions\n\n### 1. Project Structure\n\nThe Vapi MCP server is structured as follows:\n- `vapi-mcp-server/` - Main server code\n  - `src/` - TypeScript source files\n  - `dist/` - Compiled JavaScript output\n  - `.env` - Environment variables for API keys\n\n### 2. Environment Configuration\n\nCreate a `.env` file in the `vapi-mcp-server` directory with the following variables:\n\n```\n# Vapi API Keys\nVAPI_ORG_ID=your-org-id\nVAPI_PRIVATE_KEY=your-private-key\nVAPI_KNOWLEDGE_ID=your-knowledge-id\nVAPI_JWT_PRIVATE=your-jwt-private\n\n# Environment\nNODE_ENV=development\n```\n\n### 3. Building the Server\n\nTo build the server:\n\n```bash\ncd vapi-mcp/vapi-mcp-server\nnpm install\nnpm run build\n```\n\n### 4. Configuration in Cursor\n\n#### Important: Avoiding \"Client Closed\" Errors\n\nWhen configuring the Vapi MCP server in Cursor's MCP settings, pay attention to the following crucial details:\n\n1. **Working Directory**: The `cwd` parameter is required to ensure the server runs in the correct directory and can access the `.env` file properly.\n\n2. **Environment Variables**: Must be explicitly provided in the configuration, even if they exist in the `.env` file.\n\n3. **Module Type**: The server uses ES modules, so the `package.json` must include `\"type\": \"module\"`.\n\nHere's the correct configuration for `.cursor/mcp.json`:\n\n```json\n\"Vapi Voice AI Tools\": {\n  \"command\": \"node\",\n  \"type\": \"stdio\",\n  \"args\": [\n    \"/Users/matthewcage/Documents/AA-GitHub/MCP/vapi-mcp/vapi-mcp-server/dist/index.js\"\n  ],\n  \"cwd\": \"/Users/matthewcage/Documents/AA-GitHub/MCP/vapi-mcp/vapi-mcp-server\",\n  \"env\": {\n    \"VAPI_ORG_ID\": \"your-org-id\",\n    \"VAPI_PRIVATE_KEY\": \"your-private-key\",\n    \"VAPI_KNOWLEDGE_ID\": \"your-knowledge-id\",\n    \"VAPI_JWT_PRIVATE\": \"your-jwt-private\",\n    \"NODE_ENV\": \"development\"\n  }\n}\n```\n\n## Troubleshooting\n\n### \"Client Closed\" Error in Cursor\n\nIf you see \"Client Closed\" in the Cursor MCP Tools panel:\n\n1. **Check Working Directory**: Ensure the `cwd` parameter is set correctly in your mcp.json\n2. **Verify Environment Variables**: Make sure all required environment variables are passed in the configuration\n3. **Check Module Type**: Ensure `package.json` has `\"type\": \"module\"`\n4. **Inspect Permissions**: Make sure the dist/index.js file is executable (`chmod +x dist/index.js`)\n5. **Test Server Directly**: Run the server manually to check for errors:\n   ```bash\n   cd vapi-mcp/vapi-mcp-server\n   node --trace-warnings dist/index.js\n   ```\n\n### Module Not Found Errors\n\nIf you get \"Error: Cannot find module\" when running:\n\n1. **Check Working Directory**: Are you running from the correct directory?\n2. **Rebuild**: Try rebuilding the project with `npm run build`\n3. **Dependencies**: Ensure all dependencies are installed with `npm install`\n\n## Available Tools\n\nThe Vapi MCP server provides the following tools:\n\n1. **vapi_call** - Make outbound calls using Vapi's voice AI\n2. **vapi_assistant** - Manage voice assistants (create, get, list, update, delete)\n3. **vapi_conversation** - Retrieve conversation details from calls\n\n## Lessons Learned\n\n1. When integrating with Cursor's MCP:\n   - Always specify the `cwd` parameter to ensure the server runs in the correct directory\n   - Pass all required environment variables directly in the MCP configuration\n   - For ES modules, ensure package.json has `\"type\": \"module\"` and tsconfig.json uses appropriate module settings\n   - Test the server directly before configuring in Cursor\n\n2. The server command path must be absolute and correctly formed in the Cursor MCP config\n\n3. Using stdio transport type is required for proper integration with Cursor ",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "voice",
        "vapi",
        "assistants",
        "voice assistants",
        "voice ai",
        "virtual assistants"
      ],
      "category": "virtual-assistants"
    },
    "popcornspace--voice-call-mcp-server": {
      "owner": "popcornspace",
      "name": "voice-call-mcp-server",
      "url": "https://github.com/popcornspace/voice-call-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/popcornspace.webp",
      "description": "Facilitates voice call management through Twilio and OpenAI, enabling real-time audio processing for interactive conversations with AI assistants. Offers pre-built prompts for common scenarios to streamline call initiation and handling.",
      "stars": 49,
      "forks": 9,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-09-18T23:39:26Z",
      "readme_content": "# Voice Call MCP Server\n\nA Model Context Protocol (MCP) server that enables Claude and other AI assistants to initiate and manage voice calls using Twilio and OpenAI (GPT-4o Realtime model).\n\nUse this as a base to kick-start your AI-powered voice calling explorations, save time and develop additional functionality on top of it.\n\n\n\n\n## Sequence Diagram\n\n```mermaid\nsequenceDiagram\n    participant AI as AI Assistant (e.g., Claude)\n    participant MCP as MCP Server\n    participant Twilio as Twilio\n    participant Phone as Destination Phone\n    participant OpenAI as OpenAI\n    \n    AI->>MCP: 1) Initiate outbound call request <br>(POST /calls)\n    MCP->>Twilio: 2) Place outbound call via Twilio API\n    Twilio->>Phone: 3) Ring the destination phone\n    Twilio->>MCP: 4) Call status updates & audio callbacks (webhooks)\n    MCP->>OpenAI: 5) Forward real-time audio to OpenaAI's realtime model\n    OpenAI->>MCP: 6) Return voice stream\n    MCP->>Twilio: 7) Send voice stream\n    Twilio->>Phone: 8) Forward voice stream\n    Note over Phone: Two-way conversation continues <br>until the call ends\n```\n\n\n## Features\n\n- Make outbound phone calls via Twilio 📞\n- Process call audio in real-time with GPT-4o Realtime model 🎙️\n- Real-time language switching during calls 🌐\n- Pre-built prompts for common calling scenarios (like restaurant reservations) 🍽️\n- Automatic public URL tunneling with ngrok 🔄\n- Secure handling of credentials 🔒\n\n## Why MCP?\n\nThe Model Context Protocol (MCP) bridges the gap between AI assistants and real-world actions. By implementing MCP, this server allows AI models like Claude to:\n\n1. Initiate actual phone calls on behalf of users\n2. Process and respond to real-time audio conversations\n3. Execute complex tasks requiring voice communication\n\nThis open-source implementation provides transparency and customizability, allowing developers to extend functionality while maintaining control over their data and privacy.\n\n## Requirements\n\n- Node.js >= 22\n  - If you need to update Node.js, we recommend using `nvm` (Node Version Manager):\n    ```bash\n    nvm install 22\n    nvm use 22\n    ```\n- Twilio account with API credentials\n- OpenAI API key\n- Ngrok Authtoken\n\n## Installation\n\n### Manual Installation\n\n1. Clone the repository\n   ```bash\n   git clone https://github.com/lukaskai/voice-call-mcp-server.git\n   cd voice-call-mcp-server\n   ```\n\n2. Install dependencies and build\n   ```bash\n   npm install\n   npm run build\n   ```\n\n## Configuration\n\nThe server requires several environment variables:\n\n- `TWILIO_ACCOUNT_SID`: Your Twilio account SID\n- `TWILIO_AUTH_TOKEN`: Your Twilio auth token\n- `TWILIO_NUMBER`: Your Twilio number\n- `OPENAI_API_KEY`: Your OpenAI API key\n- `NGROK_AUTHTOKEN`: Your ngrok authtoken\n- `RECORD_CALLS`: Set to \"true\" to record calls (optional)\n\n### Claude Desktop Configuration\n\nTo use this server with Claude Desktop, add the following to your configuration file:\n\n**macOS**: `~/Library/Application Support/Claude/claude_desktop_config.json`\n\n**Windows**: `%APPDATA%\\Claude\\claude_desktop_config.json`\n\n```json\n{\n  \"mcpServers\": {\n    \"voice-call\": {\n      \"command\": \"node\",\n      \"args\": [\"/path/to/your/mcp-new/dist/start-all.cjs\"],\n      \"env\": {\n        \"TWILIO_ACCOUNT_SID\": \"your_account_sid\",\n        \"TWILIO_AUTH_TOKEN\": \"your_auth_token\",\n        \"TWILIO_NUMBER\": \"your_e.164_format_number\",\n        \"OPENAI_API_KEY\": \"your_openai_api_key\",\n        \"NGROK_AUTHTOKEN\": \"your_ngrok_authtoken\"\n      }\n    }\n  }\n}\n```\n\nAfter that, restart Claude Desktop to reload the configuration. \nIf connected, you should see Voice Call under the 🔨 menu.\n\n## Example Interactions with Claude\n\nHere are some natural ways to interact with the server through Claude:\n\n1. Simple call:\n```\nCan you call +1-123-456-7890 and let them know I'll be 15 minutes late for our meeting?\n```\n\n2. Restaurant reservation:\n```\nPlease call Delicious Restaurant at +1-123-456-7890 and make a reservation for 4 people tonight at 7:30 PM. Please speak in German.\n```\n\n3. Appointment scheduling:\n```\nPlease call Expert Dental NYC (+1-123-456-7899) and reschedule my Monday appointment to next Friday between 4–6pm.\n```\n\n## Important Notes\n\n1. **Phone Number Format**: All phone numbers must be in E.164 format (e.g., +11234567890)\n2. **Rate Limits**: Be aware of your Twilio and OpenAI account's rate limits and pricing\n3. **Voice Conversations**: The AI will handle natural conversations in real-time\n4. **Call Duration**: Be mindful of call durations as they affect OpenAI API and Twilio costs\n5. **Public Exposure**: Be aware that the ngrok tunnel exposes your server publicly for Twilio to reach it (though with a random URL and protected by a random secret)\n\n## Troubleshooting\n\nCommon error messages and solutions:\n\n1. \"Phone number must be in E.164 format\"\n   - Make sure the phone number starts with \"+\" and the country code\n\n2. \"Invalid credentials\"\n   - Double-check your TWILIO_ACCOUNT_SID and TWILIO_AUTH_TOKEN. You can copy them from the [Twilio Console](https://console.twilio.com)\n\n3. \"OpenAI API error\"\n   - Verify your OPENAI_API_KEY is correct and has sufficient credits\n\n4. \"Ngrok tunnel failed to start\"\n   - Ensure your NGROK_AUTHTOKEN is valid and not expired\n\n5. \"OpenAI Realtime does not detect the end of voice input, or is lagging.\"\n   - Sometimes, there might be voice encoding issues between Twilio and the receiver's network operator. Try using a different receiver.\n\n## Contributing\n\nContributions are welcome! Here are some areas we're looking to improve:\n\n- Implement support for multiple AI models beyond the current implementation\n- Add database integration to store conversation history locally and make it accessible for AI context\n- Improve latency and response times to enhance call experiences\n- Enhance error handling and recovery mechanisms\n- Add more pre-built conversation templates for common scenarios\n- Implement improved call monitoring and analytics\n\nIf you'd like to contribute, please open an issue to discuss your ideas before submitting a pull request.\n\n## License\n\nThis project is licensed under the MIT License - see the LICENSE file for details.\n\n## Security\n\nPlease do not include any sensitive information (like phone numbers or API credentials) in GitHub issues or pull requests. This server handles sensitive communications; deploy it responsibly and ensure all credentials are kept secure.\n\n\n## Time For a New Mission?\n\nWe’re hiring engineers to build at the frontier of voice AI — and bake it into a next-gen telco.\n\nCurious? Head to [careers.popcorn.space](https://careers.popcorn.space/apply) 🍿 !",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "twilio",
        "voice",
        "audio",
        "virtual assistants",
        "voice mcp",
        "twilio openai"
      ],
      "category": "virtual-assistants"
    },
    "rsagacom--chatgpt-on-wechat": {
      "owner": "rsagacom",
      "name": "chatgpt-on-wechat",
      "url": "https://github.com/rsagacom/chatgpt-on-wechat",
      "imageUrl": "/freedevtools/mcp/pfp/rsagacom.webp",
      "description": "A multi-platform intelligent dialogue service that supports text, voice, and image interactions. It can connect to various AI models and allows for custom enterprise AI applications through plugin extensions.",
      "stars": 0,
      "forks": 0,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2024-01-28T14:00:49Z",
      "readme_content": "# 简介\n\n> 本项目是基于大模型的智能对话机器人，支持微信、企业微信、公众号、飞书、钉钉接入，可选择GPT3.5/GPT4.0/Claude/文心一言/讯飞星火/通义千问/Gemini/LinkAI，能处理文本、语音和图片，通过插件访问操作系统和互联网等外部资源，支持基于自有知识库定制企业AI应用。\n\n最新版本支持的功能如下：\n\n- [x] **多端部署：** 有多种部署方式可选择且功能完备，目前已支持个人微信、微信公众号和、企业微信、飞书、钉钉等部署方式\n- [x] **基础对话：** 私聊及群聊的消息智能回复，支持多轮会话上下文记忆，支持 GPT-3.5, GPT-4, claude, Gemini, 文心一言, 讯飞星火, 通义千问\n- [x] **语音能力：** 可识别语音消息，通过文字或语音回复，支持 azure, baidu, google, openai(whisper/tts) 等多种语音模型\n- [x] **图像能力：** 支持图片生成、图片识别、图生图（如照片修复），可选择 Dall-E-3, stable diffusion, replicate, midjourney, vision模型\n- [x] **丰富插件：** 支持个性化插件扩展，已实现多角色切换、文字冒险、敏感词过滤、聊天记录总结、文档总结和对话、联网搜索等插件\n- [x] **知识库：** 通过上传知识库文件自定义专属机器人，可作为数字分身、智能客服、私域助手使用，基于 [LinkAI](https://link-ai.tech) 实现\n\n# 演示\n\nhttps://github.com/zhayujie/chatgpt-on-wechat/assets/26161723/d5154020-36e3-41db-8706-40ce9f3f1b1e\n\nDemo made by [Visionn](https://www.wangpc.cc/)\n\n# 商业支持\n\n> 我们还提供企业级的 **AI应用平台**，包含知识库、Agent插件、应用管理等能力，支持多平台聚合的应用接入、客户端管理、对话管理，以及提供\nSaaS服务、私有化部署、稳定托管接入 等多种模式。\n>\n> 目前已在私域运营、智能客服、企业效率助手等场景积累了丰富的 AI 解决方案， 在电商、文教、健康、新消费等各行业沉淀了 AI 落地的最佳实践，致力于打造助力中小企业拥抱 AI 的一站式平台。\n\n企业服务和商用咨询可联系产品顾问：\n\n<img alt=\"product_manager_qrcode\" width=\"240\" src=\"https://img-1317903499.cos.ap-guangzhou.myqcloud.com/docs/product-manager-qrcode.jpg\">\n\n# 开源社区\n\n添加小助手微信加入开源项目交流群：\n\n\n\n# 更新日志\n\n>**2023.11.11：** [1.5.3版本](https://github.com/zhayujie/chatgpt-on-wechat/releases/tag/1.5.3) 和 [1.5.4版本](https://github.com/zhayujie/chatgpt-on-wechat/releases/tag/1.5.4)，新增Google Gemini、通义千问模型\n\n>**2023.11.10：** [1.5.2版本](https://github.com/zhayujie/chatgpt-on-wechat/releases/tag/1.5.2)，新增飞书通道、图像识别对话、黑名单配置\n\n>**2023.11.10：** [1.5.0版本](https://github.com/zhayujie/chatgpt-on-wechat/releases/tag/1.5.0)，新增 `gpt-4-turbo`, `dall-e-3`, `tts` 模型接入，完善图像理解&生成、语音识别&生成的多模态能力\n\n>**2023.10.16：** 支持通过意图识别使用LinkAI联网搜索、数学计算、网页访问等插件，参考[插件文档](https://docs.link-ai.tech/platform/plugins)\n\n>**2023.09.26：** 插件增加 文件/文章链接 一键总结和对话的功能，使用参考：[插件说明](https://github.com/zhayujie/chatgpt-on-wechat/tree/master/plugins/linkai#3%E6%96%87%E6%A1%A3%E6%80%BB%E7%BB%93%E5%AF%B9%E8%AF%9D%E5%8A%9F%E8%83%BD)\n\n>**2023.08.08：** 接入百度文心一言模型，通过 [插件](https://github.com/zhayujie/chatgpt-on-wechat/tree/master/plugins/linkai) 支持 Midjourney 绘图\n\n>**2023.06.12：** 接入 [LinkAI](https://link-ai.tech/console) 平台，可在线创建领域知识库，并接入微信、公众号及企业微信中，打造专属客服机器人。使用参考 [接入文档](https://link-ai.tech/platform/link-app/wechat)。\n\n>**2023.04.26：** 支持企业微信应用号部署，兼容插件，并支持语音图片交互，私人助理理想选择，[使用文档](https://github.com/zhayujie/chatgpt-on-wechat/blob/master/channel/wechatcom/README.md)。(contributed by [@lanvent](https://github.com/lanvent) in [#944](https://github.com/zhayujie/chatgpt-on-wechat/pull/944))\n\n>**2023.04.05：** 支持微信公众号部署，兼容插件，并支持语音图片交互，[使用文档](https://github.com/zhayujie/chatgpt-on-wechat/blob/master/channel/wechatmp/README.md)。(contributed by [@JS00000](https://github.com/JS00000) in [#686](https://github.com/zhayujie/chatgpt-on-wechat/pull/686))\n\n>**2023.04.05：** 增加能让ChatGPT使用工具的`tool`插件，[使用文档](https://github.com/goldfishh/chatgpt-on-wechat/blob/master/plugins/tool/README.md)。工具相关issue可反馈至[chatgpt-tool-hub](https://github.com/goldfishh/chatgpt-tool-hub)。(contributed by [@goldfishh](https://github.com/goldfishh) in [#663](https://github.com/zhayujie/chatgpt-on-wechat/pull/663))\n\n>**2023.03.25：** 支持插件化开发，目前已实现 多角色切换、文字冒险游戏、管理员指令、Stable Diffusion等插件，使用参考 [#578](https://github.com/zhayujie/chatgpt-on-wechat/issues/578)。(contributed by [@lanvent](https://github.com/lanvent) in [#565](https://github.com/zhayujie/chatgpt-on-wechat/pull/565))\n\n>**2023.03.09：** 基于 `whisper API`(后续已接入更多的语音`API`服务) 实现对微信语音消息的解析和回复，添加配置项 `\"speech_recognition\":true` 即可启用，使用参考 [#415](https://github.com/zhayujie/chatgpt-on-wechat/issues/415)。(contributed by [wanggang1987](https://github.com/wanggang1987) in [#385](https://github.com/zhayujie/chatgpt-on-wechat/pull/385))\n\n>**2023.02.09：** 扫码登录存在账号限制风险，请谨慎使用，参考[#58](https://github.com/AutumnWhj/ChatGPT-wechat-bot/issues/158)\n\n# 快速开始\n\n快速开始文档：[项目搭建文档](https://docs.link-ai.tech/cow/quick-start)\n\n## 准备\n\n### 1. 账号注册\n\n项目默认使用OpenAI接口，需前往 [OpenAI注册页面](https://beta.openai.com/signup) 创建账号，创建完账号则前往 [API管理页面](https://beta.openai.com/account/api-keys) 创建一个 API Key 并保存下来，后面需要在项目中配置这个key。接口需要海外网络访问及绑定信用卡支付。\n\n> 默认对话模型是 openai 的 gpt-3.5-turbo，计费方式是约每 1000tokens (约750个英文单词 或 500汉字，包含请求和回复) 消耗 $0.002，图片生成是Dell E模型，每张消耗 $0.016。\n\n项目同时也支持使用 LinkAI 接口，无需代理，可使用 文心、讯飞、GPT-3、GPT-4 等模型，支持 定制化知识库、联网搜索、MJ绘图、文档总结和对话等能力。修改配置即可一键切换，参考 [接入文档](https://link-ai.tech/platform/link-app/wechat)。\n\n### 2.运行环境\n\n支持 Linux、MacOS、Windows 系统（可在Linux服务器上长期运行)，同时需安装 `Python`。\n> 建议Python版本在 3.7.1~3.9.X 之间，推荐3.8版本，3.10及以上版本在 MacOS 可用，其他系统上不确定能否正常运行。\n\n> 注意：Docker 或 Railway 部署无需安装python环境和下载源码，可直接快进到下一节。\n\n**(1) 克隆项目代码：**\n\n```bash\ngit clone https://github.com/zhayujie/chatgpt-on-wechat\ncd chatgpt-on-wechat/\n```\n\n注: 如遇到网络问题可选择国内镜像 https://gitee.com/zhayujie/chatgpt-on-wechat\n\n**(2) 安装核心依赖 (必选)：**\n> 能够使用`itchat`创建机器人，并具有文字交流功能所需的最小依赖集合。\n```bash\npip3 install -r requirements.txt\n```\n\n**(3) 拓展依赖 (可选，建议安装)：**\n\n```bash\npip3 install -r requirements-optional.txt\n```\n> 如果某项依赖安装失败可注释掉对应的行再继续\n\n## 配置\n\n配置文件的模板在根目录的`config-template.json`中，需复制该模板创建最终生效的 `config.json` 文件：\n\n```bash\n  cp config-template.json config.json\n```\n\n然后在`config.json`中填入配置，以下是对默认配置的说明，可根据需要进行自定义修改（请去掉注释）：\n\n```bash\n# config.json文件内容示例\n{\n  \"open_ai_api_key\": \"YOUR API KEY\",                          # 填入上面创建的 OpenAI API KEY\n  \"model\": \"gpt-3.5-turbo\",                                   # 模型名称, 支持 gpt-3.5-turbo, gpt-3.5-turbo-16k, gpt-4, wenxin, xunfei\n  \"proxy\": \"\",                                                # 代理客户端的ip和端口，国内环境开启代理的需要填写该项，如 \"127.0.0.1:7890\"\n  \"single_chat_prefix\": [\"bot\", \"@bot\"],                      # 私聊时文本需要包含该前缀才能触发机器人回复\n  \"single_chat_reply_prefix\": \"[bot] \",                       # 私聊时自动回复的前缀，用于区分真人\n  \"group_chat_prefix\": [\"@bot\"],                              # 群聊时包含该前缀则会触发机器人回复\n  \"group_name_white_list\": [\"ChatGPT测试群\", \"ChatGPT测试群2\"], # 开启自动回复的群名称列表\n  \"group_chat_in_one_session\": [\"ChatGPT测试群\"],              # 支持会话上下文共享的群名称  \n  \"image_create_prefix\": [\"画\", \"看\", \"找\"],                   # 开启图片回复的前缀\n  \"conversation_max_tokens\": 1000,                            # 支持上下文记忆的最多字符数\n  \"speech_recognition\": false,                                # 是否开启语音识别\n  \"group_speech_recognition\": false,                          # 是否开启群组语音识别\n  \"use_azure_chatgpt\": false,                                 # 是否使用Azure ChatGPT service代替openai ChatGPT service. 当设置为true时需要设置 open_ai_api_base，如 https://xxx.openai.azure.com/\n  \"azure_deployment_id\": \"\",                                  # 采用Azure ChatGPT时，模型部署名称\n  \"azure_api_version\": \"\",                                    # 采用Azure ChatGPT时，API版本\n  \"character_desc\": \"你是ChatGPT, 一个由OpenAI训练的大型语言模型, 你旨在回答并解决人们的任何问题，并且可以使用多种语言与人交流。\",  # 人格描述\n  # 订阅消息，公众号和企业微信channel中请填写，当被订阅时会自动回复，可使用特殊占位符。目前支持的占位符有{trigger_prefix}，在程序中它会自动替换成bot的触发词。\n  \"subscribe_msg\": \"感谢您的关注！\\n这里是ChatGPT，可以自由对话。\\n支持语音对话。\\n支持图片输出，画字开头的消息将按要求创作图片。\\n支持角色扮演和文字冒险等丰富插件。\\n输入{trigger_prefix}#help 查看详细指令。\",\n  \"use_linkai\": false,                                        # 是否使用LinkAI接口，默认关闭，开启后可国内访问，使用知识库和MJ\n  \"linkai_api_key\": \"\",                                       # LinkAI Api Key\n  \"linkai_app_code\": \"\"                                       # LinkAI 应用code\n}\n```\n**配置说明：**\n\n**1.个人聊天**\n\n+ 个人聊天中，需要以 \"bot\"或\"@bot\" 为开头的内容触发机器人，对应配置项 `single_chat_prefix` (如果不需要以前缀触发可以填写  `\"single_chat_prefix\": [\"\"]`)\n+ 机器人回复的内容会以 \"[bot] \" 作为前缀， 以区分真人，对应的配置项为 `single_chat_reply_prefix` (如果不需要前缀可以填写 `\"single_chat_reply_prefix\": \"\"`)\n\n**2.群组聊天**\n\n+ 群组聊天中，群名称需配置在 `group_name_white_list ` 中才能开启群聊自动回复。如果想对所有群聊生效，可以直接填写 `\"group_name_white_list\": [\"ALL_GROUP\"]`\n+ 默认只要被人 @ 就会触发机器人自动回复；另外群聊天中只要检测到以 \"@bot\" 开头的内容，同样会自动回复（方便自己触发），这对应配置项 `group_chat_prefix`\n+ 可选配置: `group_name_keyword_white_list`配置项支持模糊匹配群名称，`group_chat_keyword`配置项则支持模糊匹配群消息内容，用法与上述两个配置项相同。（Contributed by [evolay](https://github.com/evolay))\n+ `group_chat_in_one_session`：使群聊共享一个会话上下文，配置 `[\"ALL_GROUP\"]` 则作用于所有群聊\n\n**3.语音识别**\n\n+ 添加 `\"speech_recognition\": true` 将开启语音识别，默认使用openai的whisper模型识别为文字，同时以文字回复，该参数仅支持私聊 (注意由于语音消息无法匹配前缀，一旦开启将对所有语音自动回复，支持语音触发画图)；\n+ 添加 `\"group_speech_recognition\": true` 将开启群组语音识别，默认使用openai的whisper模型识别为文字，同时以文字回复，参数仅支持群聊 (会匹配group_chat_prefix和group_chat_keyword, 支持语音触发画图)；\n+ 添加 `\"voice_reply_voice\": true` 将开启语音回复语音（同时作用于私聊和群聊），但是需要配置对应语音合成平台的key，由于itchat协议的限制，只能发送语音mp3文件，若使用wechaty则回复的是微信语音。\n\n**4.其他配置**\n\n+ `model`: 模型名称，目前支持 `gpt-3.5-turbo`, `text-davinci-003`, `gpt-4`, `gpt-4-32k`, `wenxin` , `claude` ,  `xunfei`(其中gpt-4 api暂未完全开放，申请通过后可使用)\n+ `temperature`,`frequency_penalty`,`presence_penalty`: Chat API接口参数，详情参考[OpenAI官方文档。](https://platform.openai.com/docs/api-reference/chat)\n+ `proxy`：由于目前 `openai` 接口国内无法访问，需配置代理客户端的地址，详情参考  [#351](https://github.com/zhayujie/chatgpt-on-wechat/issues/351)\n+ 对于图像生成，在满足个人或群组触发条件外，还需要额外的关键词前缀来触发，对应配置 `image_create_prefix `\n+ 关于OpenAI对话及图片接口的参数配置（内容自由度、回复字数限制、图片大小等），可以参考 [对话接口](https://beta.openai.com/docs/api-reference/completions) 和 [图像接口](https://beta.openai.com/docs/api-reference/completions)  文档，在[`config.py`](https://github.com/zhayujie/chatgpt-on-wechat/blob/master/config.py)中检查哪些参数在本项目中是可配置的。\n+ `conversation_max_tokens`：表示能够记忆的上下文最大字数（一问一答为一组对话，如果累积的对话字数超出限制，就会优先移除最早的一组对话）\n+ `rate_limit_chatgpt`，`rate_limit_dalle`：每分钟最高问答速率、画图速率，超速后排队按序处理。\n+ `clear_memory_commands`: 对话内指令，主动清空前文记忆，字符串数组可自定义指令别名。\n+ `hot_reload`: 程序退出后，暂存微信扫码状态，默认关闭。\n+ `character_desc` 配置中保存着你对机器人说的一段话，他会记住这段话并作为他的设定，你可以为他定制任何人格      (关于会话上下文的更多内容参考该 [issue](https://github.com/zhayujie/chatgpt-on-wechat/issues/43))\n+ `subscribe_msg`：订阅消息，公众号和企业微信channel中请填写，当被订阅时会自动回复， 可使用特殊占位符。目前支持的占位符有{trigger_prefix}，在程序中它会自动替换成bot的触发词。\n\n**5.LinkAI配置 (可选)**\n\n+ `use_linkai`: 是否使用LinkAI接口，开启后可国内访问，使用知识库和 `Midjourney` 绘画, 参考 [文档](https://link-ai.tech/platform/link-app/wechat)\n+ `linkai_api_key`: LinkAI Api Key，可在 [控制台](https://link-ai.tech/console/interface) 创建\n+ `linkai_app_code`: LinkAI 应用code，选填\n\n**本说明文档可能会未及时更新，当前所有可选的配置项均在该[`config.py`](https://github.com/zhayujie/chatgpt-on-wechat/blob/master/config.py)中列出。**\n\n## 运行\n\n### 1.本地运行\n\n如果是开发机 **本地运行**，直接在项目根目录下执行：\n\n```bash\npython3 app.py                                    # windows环境下该命令通常为 python app.py\n```\n\n终端输出二维码后，使用微信进行扫码，当输出 \"Start auto replying\" 时表示自动回复程序已经成功运行了（注意：用于登录的微信需要在支付处已完成实名认证）。扫码登录后你的账号就成为机器人了，可以在微信手机端通过配置的关键词触发自动回复 (任意好友发送消息给你，或是自己发消息给好友)，参考[#142](https://github.com/zhayujie/chatgpt-on-wechat/issues/142)。\n\n### 2.服务器部署\n\n使用nohup命令在后台运行程序：\n\n```bash\nnohup python3 app.py & tail -f nohup.out          # 在后台运行程序并通过日志输出二维码\n```\n扫码登录后程序即可运行于服务器后台，此时可通过 `ctrl+c` 关闭日志，不会影响后台程序的运行。使用 `ps -ef | grep app.py | grep -v grep` 命令可查看运行于后台的进程，如果想要重新启动程序可以先 `kill` 掉对应的进程。日志关闭后如果想要再次打开只需输入 `tail -f nohup.out`。此外，`scripts` 目录下有一键运行、关闭程序的脚本供使用。\n\n> **多账号支持：** 将项目复制多份，分别启动程序，用不同账号扫码登录即可实现同时运行。\n\n> **特殊指令：** 用户向机器人发送 **#reset** 即可清空该用户的上下文记忆。\n\n\n### 3.Docker部署\n\n> 使用docker部署无需下载源码和安装依赖，只需要获取 docker-compose.yml 配置文件并启动容器即可。\n\n> 前提是需要安装好 `docker` 及 `docker-compose`，安装成功的表现是执行 `docker -v` 和 `docker-compose version` (或 docker compose version) 可以查看到版本号，可前往 [docker官网](https://docs.docker.com/engine/install/) 进行下载。\n\n#### (1) 下载 docker-compose.yml 文件\n\n```bash\nwget https://open-1317903499.cos.ap-guangzhou.myqcloud.com/docker-compose.yml\n```\n\n下载完成后打开 `docker-compose.yml` 修改所需配置，如 `OPEN_AI_API_KEY` 和 `GROUP_NAME_WHITE_LIST` 等。\n\n#### (2) 启动容器\n\n在 `docker-compose.yml` 所在目录下执行以下命令启动容器：\n\n```bash\nsudo docker compose up -d\n```\n\n运行 `sudo docker ps` 能查看到 NAMES 为 chatgpt-on-wechat 的容器即表示运行成功。\n\n注意：\n\n - 如果 `docker-compose` 是 1.X 版本 则需要执行 `sudo  docker-compose up -d` 来启动容器\n - 该命令会自动去 [docker hub](https://hub.docker.com/r/zhayujie/chatgpt-on-wechat) 拉取 latest 版本的镜像，latest 镜像会在每次项目 release 新的版本时生成\n\n最后运行以下命令可查看容器运行日志，扫描日志中的二维码即可完成登录：\n\n```bash\nsudo docker logs -f chatgpt-on-wechat\n```\n\n#### (3) 插件使用\n\n如果需要在docker容器中修改插件配置，可通过挂载的方式完成，将 [插件配置文件](https://github.com/zhayujie/chatgpt-on-wechat/blob/master/plugins/config.json.template)\n重命名为 `config.json`，放置于 `docker-compose.yml` 相同目录下，并在 `docker-compose.yml` 中的 `chatgpt-on-wechat` 部分下添加 `volumes` 映射:\n\n```\nvolumes:\n  - ./config.json:/app/plugins/config.json\n```\n\n### 4. Railway部署\n\n> Railway 每月提供5刀和最多500小时的免费额度。 (07.11更新: 目前大部分账号已无法免费部署)\n\n1. 进入 [Railway](https://railway.app/template/qApznZ?referralCode=RC3znh)\n2. 点击 `Deploy Now` 按钮。\n3. 设置环境变量来重载程序运行的参数，例如`open_ai_api_key`, `character_desc`。\n\n**一键部署:**\n  \n  [![Deploy on Railway](https://railway.app/button.svg)](https://railway.app/template/qApznZ?referralCode=RC3znh)\n\n## 常见问题\n\nFAQs： <https://github.com/zhayujie/chatgpt-on-wechat/wiki/FAQs>\n\n或直接在线咨询 [项目小助手](https://link-ai.tech/app/Kv2fXJcH)  (beta版本，语料完善中，回复仅供参考)\n\n## 开发\n\n欢迎接入更多应用，参考 [Terminal代码](https://github.com/zhayujie/chatgpt-on-wechat/blob/master/channel/terminal/terminal_channel.py) 实现接收和发送消息逻辑即可接入。 同时欢迎增加新的插件，参考 [插件说明文档](https://github.com/zhayujie/chatgpt-on-wechat/tree/master/plugins)。\n\n## 联系\n\n欢迎提交PR、Issues，以及Star支持一下。程序运行遇到问题可以查看 [常见问题列表](https://github.com/zhayujie/chatgpt-on-wechat/wiki/FAQs) ，其次前往 [Issues](https://github.com/zhayujie/chatgpt-on-wechat/issues) 中搜索。个人开发者可加入开源交流群参与更多讨论，企业用户可联系[产品顾问](https://img-1317903499.cos.ap-guangzhou.myqcloud.com/docs/product-manager-qrcode.jpg)咨询。",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "chatgpt",
        "dialogue",
        "wechat",
        "dialogue service",
        "chatgpt wechat",
        "rsagacom chatgpt"
      ],
      "category": "virtual-assistants"
    },
    "scarletlabs-ai--Votars-MCP": {
      "owner": "scarletlabs-ai",
      "name": "Votars-MCP",
      "url": "https://github.com/scarletlabs-ai/Votars-MCP",
      "imageUrl": "/freedevtools/mcp/pfp/scarletlabs-ai.webp",
      "description": "Integrate advanced AI functionalities for processing complex tasks through robust APIs. Supports voice recording, transcription, and intelligent AI processing for meetings.",
      "stars": 27,
      "forks": 2,
      "license": "No License",
      "language": "Go",
      "updated_at": "2025-07-07T07:12:32Z",
      "readme_content": "![Votars Logo](https://votars.ai/_next/static/media/logo.e7b6bff6.svg) \n# Votars MCP \n[![smithery badge](https://smithery.ai/badge/@scarletlabs-ai/votars-mcp)](https://smithery.ai/server/@scarletlabs-ai/votars-mcp)\n\n\n## Overview\n\nVotars-MCP is a tool that supports multiple language implementations of the **Votars MCP server**. Currently, only the Go version is available, with other languages to be added in future releases. It supports two interaction modes: `sse` (Server-Sent Events) and `stdio` (Standard Input/Output). It is designed to provide seamless integration with the Votars AI platform for processing various tasks.\n\n## About Votars\n\n[Votars](https://votars.ai/en/) is the world's smartest multilingual meeting assistant, designed for voice recording, transcription, and advanced AI processing. It features real-time translation, intelligent error correction, AI summarization, smart content generation, and AI discussions. The Votars app is available on [Web](https://votars.ai/en/), [iOS](https://apps.apple.com/us/app/votars-ai-transcribe-organize/id6737496290), and [Android](https://play.google.com/store/apps/details?id=com.votars.transcribe).\n\nAdditionally, Votars is an AI-powered platform that enables developers to integrate advanced AI functionalities into their applications. By leveraging Votars, you can process complex tasks efficiently with robust APIs designed for high performance and scalability.\n\n## Features\n- **Easy Integration with Votars**\n- **Modular Design:** Ready to be extended with additional functionalities.\n- **Supported MCP Tools:**\n  - `Votars_fetch_recent_transcripts`: Allows users to read recent transcripts from their workspace, providing convenient access to the latest recorded sessions.\n  - `Votars_fetch_a_specific_transcript`: Enables users to retrieve specific transcripts by providing a transcript ID, allowing targeted retrieval of stored data.\n  \n  More functionalities will be added soon. Stay tuned!\n\n## Installation (Go MCP)\n\n### Installing via Smithery\n\nTo install votars-mcp for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@scarletlabs-ai/votars-mcp):\n\n```bash\nnpx -y @smithery/cli install @scarletlabs-ai/votars-mcp --client claude\n```\n\n### Manual Installation\nTo install the Go version of Votars MCP from the GitHub repository, use:\n\n```bash\n go install github.com/scarletlabs-ai/Votars-MCP/go/votars-mcp@latest\n```\n\n## Usage (Go MCP)\n\n### Run MCP Service\nBefore using the `sse` mode, you need to run the MCP server. Open a terminal and run:\n\n```bash\nvotars-mcp -t sse -p 8080\n```\n\nThis command starts the MCP service on port 8080, ready to accept `sse` requests.\n\n\n### 1. SSE Mode\n\nFor `sse` mode, you need to provide the API key via request headers in the configuration file.\n\nConfiguration file example (`mcp.config.json`):\n\n```json\n{\n  \"mcpServers\": {\n    \"Votars MCP\": {\n      \"type\": \"sse\",\n      \"url\": \"http://0.0.0.0:8080/sse\",\n      \"headers\": {\n        \"Authorization\": \"Bearer <your-api-key>\"\n      }\n    }\n  }\n}\n```\n\n### 2. Stdio Mode\n\nFor `stdio` mode, set the API key as an environment variable.\n\n\nConfiguration file example (`mcp.config.json`):\n\n```json\n{\n  \"mcpServers\": {\n    \"Votars MCP Stdio\": {\n      \"type\": \"stdio\",\n      \"command\": \"votars-mcp\",\n      \"args\": [\"-t\", \"stdio\"],\n      \"env\": {\n        \"VOTARS_API_KEY\": \"<your-api-key>\"\n      }\n    }\n  }\n}\n```\n\n## Obtaining Your API Key\n\n1. Go to [Votars.AI](https://votars.ai/en/) and register.\n2. Navigate to your workspace's `Settings`.\n3. Create an API Key under the API Key management section.\n\n\n\n## Roadmap\n\n- **Current Support:** Go\n- **Planned Support:** Python, JavaScript, Rust, etc.\n\n## License\n\nMIT License",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "ai",
        "voice",
        "assistants",
        "virtual assistants",
        "scarletlabs ai",
        "transcription intelligent"
      ],
      "category": "virtual-assistants"
    },
    "suryawanshishantanu6--time-mcp": {
      "owner": "suryawanshishantanu6",
      "name": "time-mcp",
      "url": "https://github.com/suryawanshishantanu6/time-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/suryawanshishantanu6.webp",
      "description": "Integrates a tool-augmented LLM pipeline to provide answers to time-related and general inquiries. Utilizes a Flask API for current timestamps and employs an MCP Agent for intent detection and interaction with an LLM via OpenRouter.",
      "stars": 0,
      "forks": 0,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-05-08T03:55:06Z",
      "readme_content": "# time-mcp\n\nA minimal agentic AI system that answers time-related and general questions using a tool-augmented LLM pipeline.\n\n## Features\n- **Flask API**: Provides the current timestamp.\n- **MCP Agent Server**: Reasoning agent that detects user intent, calls tools (like the time API), engineers prompts, and interacts with an LLM via OpenRouter (OpenAI-compatible API).\n- **Streamlit UI**: Simple chat interface to talk to the AI agent.\n\n---\n\n## Setup\n\n### 1. Clone and Install Dependencies\n```bash\npip install -r requirements.txt\n```\n\n### 2. Environment Variable\nSet your OpenRouter API key (get one from https://openrouter.ai):\n```bash\nexport OPENROUTER_API_KEY=sk-...your-key...\n```\n\n### 3. Run the Servers\nOpen three terminals (or use background processes):\n\n#### Terminal 1: Flask Time API\n```bash\npython flask_api.py\n```\n\n#### Terminal 2: MCP Agent Server\n```bash\npython mcp_server.py\n```\n\n#### Terminal 3: Streamlit UI\n```bash\nstreamlit run streamlit_ui.py\n```\n\nThe Streamlit UI will open in your browser (default: http://localhost:8501)\n\n---\n\n## Usage\n- Ask the agent any question in the Streamlit UI.\n- If you ask about the time (e.g., \"What is the time?\"), the agent will call the Flask API, fetch the current time, and craft a beautiful, natural response using the LLM.\n- For other questions, the agent will answer using the LLM only.\n\n---\n\n## Architecture\n```\n[Streamlit UI] → [MCP Agent Server] → [Tools (e.g., Time API)]\n                            ↓\n                        [LLM via OpenRouter]\n```\n- The MCP agent detects intent, calls tools as needed, engineers prompts, and sends them to the LLM.\n- Easily extensible to add more tools (just add to the MCPAgent class).\n\n---\n\n## Customization\n- **Add more tools**: Implement new methods in `MCPAgent` and update `self.tools`.\n- **Improve intent detection**: Extend `detect_intent()` in `MCPAgent`.\n- **Change LLM model**: Update the `model` field in `call_llm()`.\n\n---\n\n## Requirements\n- Python 3.7+\n- See `requirements.txt` for dependencies.\n\n---\n\n## Credits\n- Built using Flask, Streamlit, OpenRouter, and Python.\n- Inspired by agentic LLM design patterns.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "llm",
        "timestamps",
        "api",
        "virtual assistants",
        "augmented llm",
        "llm pipeline"
      ],
      "category": "virtual-assistants"
    },
    "tijs--py-sound-mcp": {
      "owner": "tijs",
      "name": "py-sound-mcp",
      "url": "https://github.com/tijs/py-sound-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/tijs.webp",
      "description": "Plays customizable sound effects for coding events such as completions and errors, providing audio feedback in MCP-compatible environments. Integrates seamlessly with tools like Cursor to enhance the interactive development experience.",
      "stars": 1,
      "forks": 1,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-12T09:02:31Z",
      "readme_content": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/tijs-py-sound-mcp-badge.png)](https://mseep.ai/app/tijs-py-sound-mcp)\n\n# MCP Sound Tool\n\nA Model Context Protocol (MCP) implementation that plays sound effects for Cursor AI and other MCP-compatible environments. This Python implementation provides audio feedback for a more interactive coding experience.\n\n## Features\n\n* Plays sound effects for various events (completion, error, notification)\n* Uses the Model Context Protocol (MCP) for standardized integration with Cursor and other IDEs\n* Cross-platform support (Windows, macOS, Linux)\n* Configurable sound effects\n\n## Installation\n\n### Python Version Compatibility\n\nThis package is tested with Python 3.8-3.11. If you encounter errors with Python 3.12+ (particularly `BrokenResourceError` or `TaskGroup` exceptions), please try using an earlier Python version.\n\n### Recommended: Install with pipx\n\nThe recommended way to install mcp-sound-tool is with [pipx](https://pypa.github.io/pipx/), which installs the package in an isolated environment while making the commands available globally:\n\n```bash\n# Install pipx if you don't have it\npython -m pip install --user pipx\npython -m pipx ensurepath\n\n# Install mcp-sound-tool\npipx install mcp-sound-tool\n```\n\nThis method ensures that the tool has its own isolated environment, avoiding conflicts with other packages.\n\n### Alternative: Install with pip\n\nYou can also install directly with pip:\n\n```bash\npip install mcp-sound-tool\n```\n\n### From Source\n\n1. Clone this repository:\n\n   ```bash\n   git clone https://github.com/yourusername/mcp-sound-tool\n   cd mcp-sound-tool\n   ```\n\n2. Install with pipx directly from the source directory:\n\n   ```bash\n   pipx install .\n   ```\n\n   Or with pip:\n\n   ```bash\n   pip install -e .\n   ```\n\n## Usage\n\n### Adding Sound Files\n\nPlace your sound files in the `sounds` directory. The following sound files are expected:\n\n* `completion.mp3` - Played after code generation\n* `error.mp3` - Played when an error occurs\n* `notification.mp3` - Used for general notifications\n\nYou can find free sound effects on websites like freesound.org.\n\n### Running the MCP Server\n\nRun the MCP server:\n\n```bash\nmcp-sound-tool\n```\n\nThe server will start and listen for events from Cursor or other MCP-compatible clients through the stdio transport.\n\n### Configuration in Cursor\n\nTo use this server with Cursor, add it to your MCP configuration file:\n\nOn macOS:\n\n```json\n// ~/Library/Application Support/Cursor/mcp.json\n{\n  \"mcpServers\": {\n    \"sound\": {\n      \"command\": \"mcp-sound-tool\",\n      \"args\": [],\n      \"type\": \"stdio\",\n      \"pollingInterval\": 5000,\n      \"startupTimeout\": 10000,\n      \"restartOnFailure\": true\n    }\n  }\n}\n```\n\nOn Windows:\n\n```json\n// %APPDATA%/Cursor/mcp.json\n{\n  \"mcpServers\": {\n    \"sound\": {\n      \"command\": \"mcp-sound-tool\",\n      \"args\": [],\n      \"type\": \"stdio\",\n      \"pollingInterval\": 5000,\n      \"startupTimeout\": 10000,\n      \"restartOnFailure\": true\n    }\n  }\n}\n```\n\nWhen installed with `pipx`, the `mcp-sound-tool` command will be available in your PATH, so Cursor will be able to find and execute it without specifying the full path.\n\n## Sound MCP Usage Guidelines for AI Models\n\nThis MCP server provides audio feedback capabilities for AI interactions. It's designed to enhance the user experience by providing clear audio cues that indicate the status of operations without requiring the user to read text.\n\n### When to Use Sound Feedback\n\nAI agents should use the sound tools proactively at appropriate moments:\n\n1. **Success Sounds (`completion`)**:\n   * After a task or command has successfully completed\n   * When a significant operation has finished successfully\n   * When confirming a user's request has been fulfilled\n\n2. **Error Sounds (`error`)**:\n   * When a command has failed or encountered an error\n   * When warning the user about a problem\n   * When an operation couldn't be completed as requested\n\n3. **Notification Sounds (`notification`)**:\n   * When alerting the user to important information\n   * When prompting for user attention or input\n   * For status updates on long-running operations\n\n### Example Usage\n\n```python\n# When a command completes successfully\n@mcp.tool()\ndef execute_command(command):\n    result = run_command(command)\n    if result.success:\n        play_sound(\"completion\")  # Indicate success with audio\n        return \"Command executed successfully\"\n    else:\n        play_sound(\"error\")  # Indicate failure with audio\n        return f\"Error: {result.error_message}\"\n```\n\n### Available Tools\n\n1. `play_sound(sound_type=\"completion\", custom_sound_path=None)`: Play a sound effect\n2. `list_available_sounds()`: List all available sound files\n3. `install_to_user_dir()`: Install sound files to user's config directory\n\nFor more details, connect to the MCP server and check the tool descriptions.\n\n## Development\n\nFor development:\n\n```bash\n# Install development dependencies\npip install -e \".[dev]\"\n\n# Run tests\npytest\n```\n\n## Acknowledgments\n\n* [SIAM-TheLegend](https://github.com/SIAM-TheLegend) for creating the original [sound-mcp](https://github.com/SIAM-TheLegend/sound-mcp) JavaScript implementation that inspired this Python version\n* The MCP protocol developers for creating a powerful standard for AI tool interactions\n* Contributors to the testing and documentation\n\n## License\n\nThis project is licensed under the MIT License - see the LICENSE file for details.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "interactive",
        "sound",
        "audio",
        "py sound",
        "sound mcp",
        "virtual assistants"
      ],
      "category": "virtual-assistants"
    },
    "uraoz--bouyomichan-mcp-nodejs": {
      "owner": "uraoz",
      "name": "bouyomichan-mcp-nodejs",
      "url": "https://github.com/uraoz/bouyomichan-mcp-nodejs",
      "imageUrl": "/freedevtools/mcp/pfp/uraoz.webp",
      "description": "Provides text-to-speech capabilities using BouyomiChan's Yukkuri voice, enabling voice output from text commands with customizable options for voice type, volume, speed, and pitch. Integrates seamlessly with Claude for Desktop for enhanced user interaction.",
      "stars": 2,
      "forks": 2,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-09-01T11:48:46Z",
      "readme_content": "# 棒読みちゃんMCPサーバー (Node.js版)\n\n<a href=\"https://glama.ai/mcp/servers/@uraoz/bouyomi-mcp-nodejs\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@uraoz/bouyomi-mcp-nodejs/badge\" alt=\"Bouyomi-chan Server MCP server\" />\n</a>\n\n\n## 前提条件\n\n- Node.js 16以上\n- npm 7以上\n- 棒読みちゃんがインストールされていること\n- 棒読みちゃんのHTTP連携がポート50080で起動していること\n\n## 使用方法\n\n### ローカルでのサーバーの起動\n\n```bash\ngit clone https://github.com/uraoz/bouyomichan-mcp-nodejs.git\ncd bouyomichan-mcp-nodejs\nnpm install\nnpm run build\nnpm start\n```\n\n### Claude for Desktopとの連携\n\n```json\n{\n  \"mcpServers\": {\n    \"bouyomichan\":{\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"github:uraoz/bouyomichan-mcp-nodejs\"\n      ]\n    }\n  }\n}\n```\n\n## パラメータ説明\n\n| パラメータ | 説明 | デフォルト値 | 有効範囲 |\n|----------|------|------------|---------|\n| text     | 読み上げるテキスト | 必須 | 任意のテキスト |\n| voice    | 音声の種類 | 0 (女性1) | 0: 女性1、1: 男性1、2: 女性2、... |\n| volume   | 音量 | -1 (デフォルト) | -1: デフォルト、0-100: 音量レベル |\n| speed    | 速度 | -1 (デフォルト) | -1: デフォルト、50-200: 速度レベル |\n| tone     | 音程 | -1 (デフォルト) | -1: デフォルト、50-200: 音程レベル |\n\n## ライセンス\n\nMIT\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "voice",
        "bouyomichan",
        "nodejs",
        "yukkuri voice",
        "enabling voice",
        "voice output"
      ],
      "category": "virtual-assistants"
    },
    "veenastudio--flstudio-mcp": {
      "owner": "veenastudio",
      "name": "flstudio-mcp",
      "url": "https://github.com/veenastudio/flstudio-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/veenastudio.webp",
      "description": "Connects AI model Claude to FL Studio for seamless integration of melodies, chords, and drum patterns into music projects. Facilitates real-time music production by allowing interaction between AI and the FL Studio environment.",
      "stars": 59,
      "forks": 4,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-09-30T19:37:52Z",
      "readme_content": "# flstudio MCP\n\n# This is an MCP server that connects Claude to FL Studio.\nMade this in 3 days. We're open sourcing it to see what we can actually get out of it. The possibilities are endless.\n\n## If you're running to any issues, join our discord and we can setup it for you.\n(also join if you interested in the future of music and AI or want to request features. we're building this with you)\n\nhttps://discord.gg/ZjG9TaEhvy\n\nCheck out our AI-Powered DAW for musicians at www.veena.studio\n\nAll in browser. All for free.\n\n\n## Step 1: Download the Files\nYou should see two main items.\n\n- A folder called Test Controller\n- A python file called trigger.py\nThe Test Controller folder has a file called device_test.py that receives information from the MCP server.\ntrigger.py is the MCP server.\n\nPlace the Test Controller folder in Image-Line/FL Studio/Settings/Hardware (Don't change the name of this file or folder)\n\n## Step 2: Set up MCP for Claude\nFollow this tutorial to see how to setup MCP servers in Claude by edyting the claude_desktop_config files.\n\nhttps://modelcontextprotocol.io/quickstart/server\n\nIf you followed this process, make sure to change whatever mentions of weather.py to trigger.py\n\nIf the Hammer icon doesn't show up, open Task Manager and force close the Claude process.\n\nIt should then show up.\n\nThis is what my config file looks like\n\n![mcp](https://github.com/user-attachments/assets/e8e609f7-eaa4-469b-9140-c05b5a9bf242)\n\n## Step 3: Set Up Virtual MIDI Ports\n\n### For Windows\nFor Windows, download LoopMIDI from here.\n\nhttps://www.tobias-erichsen.de/software/loopmidi.html\n\nInstall LoopMIDI and add a port using the + button.\n\nThis is what mine looks like:\n![loopmidi2](https://github.com/user-attachments/assets/fdc2770f-e07a-4b19-824b-56de8a4aa2c3)\n\n### For Mac\nYour MIDI Ports would be automatically setup to receive data.\n\n## Step 4: Setup MIDI Controller\nOpen FL Studio.\n\nGo To Options > MIDI Settings.\n\nIn the Input Tab, click the MIDI Input you just created with LoopMIDI.\n\nChange controller type from (generic controller) to Test Controller.\n\n## Step 5: Download Packages\nGo to the folder with the trigger.py file. (This is the MCP Server file)\n\nActivate the conda environment (like you learned in the Claude MCP Setup Tutorial)\n\nRun this command to download the necessary packages: uv pip install httpx mido python-rtmidi typing fastmcp FL-Studio-API-Stubs\n(uv should be installed from the Claude MCP setup)\n\n## Step 6: Verify MCP Connection\nTell Claude to get available MIDI ports.\n\nThis should use the MCP to get the ports from FL Studio.\n\nIf Windows, copy the port you created with LoopMIDI and the number in front of it.\n\nIf Mac, copy the default port.\n\n![loopmidi](https://github.com/user-attachments/assets/a14b0aaa-5127-47c9-b041-fcb5a70339d9)\n\nIn my case, I copy loopMIDI Port 2\n\nOpen trigger.py in a text editor and replace the default port with the name of the port you just copied.\noutput_port = mido.open_output('loopMIDI Port 2') \n\n\n## Step 7: Make Music\nUse the MCP to send melodies, chords, drums, etc.\n\nClick on the instrument you want to record to and it will live record to the piano roll of that instrument.\n\nI tend to use this prompt when I start a new chat: Here is format for notes: note(0-127),velocity(0-100),length in beats(decimal),position in beats(decimal)\n\n## Step 8: Share what you made\nShare what you made on our Discord: https://discord.gg/ZjG9TaEhvy\n\n## Credits\nFL Studio API Stubs: https://github.com/IL-Group/FL-Studio-API-Stubs\nAbleton MCP: https://github.com/ahujasid/ableton-mcp\n\n## Nerd Stuff\nIf you want to contribute please go ahead. \n\nThe way this works is that device_test.py behaves as a virtual MIDI Controller.\nThe MCP server (trigger.py) communicates with this MIDI Controller by opening a Virtual Port and sending MIDI messages through a library called MIDO.\n\nThe issue with MIDI messages is that its only 7 bits so we can only send in number from 0-127.\n\nSo we encrypt all of our MIDI data like note position, etc in multiple MIDI notes that the device knows how to read.\n\nHopefully, Image Line can give us more access to their DAW via their API so we don't have to do this MIDI nonsense.\n\n\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "flstudio",
        "studio",
        "ai",
        "fl studio",
        "flstudio mcp",
        "virtual assistants"
      ],
      "category": "virtual-assistants"
    },
    "wangyafu--haiguitangmcp": {
      "owner": "wangyafu",
      "name": "haiguitangmcp",
      "url": "https://github.com/wangyafu/haiguitangmcp",
      "imageUrl": "/freedevtools/mcp/pfp/wangyafu.webp",
      "description": "Host interactive Turtle Soup games with an AI game master, enabling solo play and puzzle exploration with access to puzzles, game rules, and hints through standardized tools.",
      "stars": 6,
      "forks": 4,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-09-02T08:53:43Z",
      "readme_content": "## 介绍\r\n\r\n本项目旨在让大语言模型扮演海龟汤游戏主持人，使你独自一人也能享受海龟汤游戏的快乐。\r\n\r\n## 快速开始\r\n\r\n在使用本项目前，你需要确保你的电脑上已经安装了Python和uv。\r\n\r\n\r\n你首先需要克隆整个项目，然后运行uv sync安装依赖。\r\n\r\n```bash\r\ngit clone https://github.com/wangyafu/haiguitangmcp/\r\ncd haiguitangmcp\r\nuv sync\r\n```\r\n\r\n其次，你需要修改配置文件（假设你将项目安装在了E盘）\r\n\r\n### 在vscode中配置\r\n\r\n```json\r\n\"mcp\":{\r\n    \"servers\":{\r\n        \"haiguitang-mcp\": {\r\n                \"type\": \"stdio\",\r\n                \"command\": \"uv\",\r\n                \"args\": [\r\n                    \"--directory\",\r\n                    \"E:\\\\haiguitangmcp\\\\haiguitang_mcp\",\r\n                    \"run\",\r\n                    \"server.py\"\r\n                ]\r\n            }\r\n    }\r\n}\r\n\r\n\r\n```\r\n\r\n### 在cherry studio中进行配置\r\n\r\n```json\r\n\"mcpServers\": {\r\n    \r\n    \"haiguitang\": {\r\n      \"isActive\": true,\r\n      \"name\": \"海龟汤MCP服务器\",\r\n      \"description\": \"和用户玩海龟汤\",\r\n      \"registryUrl\": \"\",\r\n      \"command\": \"uv\",\r\n      \"args\": [\r\n        \"--directory\",\r\n        \"E:/haiguitangmcp/haiguitang_mcp\",\r\n        \"run\",\r\n        \"server.py\"\r\n      ]\r\n    },\r\n   \r\n}\r\n\r\n```\r\n\r\n上述的\"E:/haiguitangmcp/haiguitang_mcp\"表示server.py所在的路径。\r\n\r\n在其他mcp client中的配置方法类似。\r\n\r\n## mcp相关内容\r\n\r\n本项目提供了三个工具:\r\n\r\n- `get_prompt`: 获取海龟汤游戏的完整玩法说明\r\n- `get_puzzle`: 获取一个特定谜题的完整内容，需要提供谜题标题作为参数\r\n- `list_puzzles_tool`: 列出所有可用的谜题列表\r\n\r\n同时，本项目还提供了以下资源:\r\n\r\n- `puzzles://{puzzle_title}`: 获取特定谜题的信息\r\n\r\n以及一个提示模板:\r\n\r\n- `game_rules`: 提供海龟汤游戏规则的提示模板\r\n\r\n## 游戏规则\r\n\r\n在本游戏中：\r\n\r\n- 海龟汤是一种情景推理游戏，谜题本身并没有很强的逻辑性，注重能否发现关键线索重现情景\r\n- 出题人提出一个看似不合常理的问题和情景（谜面），猜题者通过提问缩小范围并最终揭示完整故事情节（谜底）\r\n- 猜题者可以提出任何问题，出题人主要用\"是\"、\"不是\"、\"是也不是\"或\"没有关系\"来回答\r\n- 当问题中既有对的地方也有不对的地方时，出题人会回答\"是也不是\"\r\n- 当问题与谜题核心情节无关时，出题人会回答\"没有关系\"\r\n- 猜题者可以通过在消息开头加上\"汤底\"来尝试描述完整情景\r\n- 当猜题者掌握了关键线索时，出题人会提醒猜题者归纳线索，形成对谜底的完整描述\r\n- 猜题者可以请求引导和提示，出题人会给予尚未掌握的线索\r\n- 当猜题者的描述大致包含了谜题的关键情景时，出题人会确认\"完全正确\"\r\n\r\n\r\n### 小技巧\r\n\r\n- 从基本问题开始，如谜题涉及人数、死者的死因等。\r\n- 注意谜面中的每一个细节，它们可能是关键线索\r\n- 当你感到困惑时，尝试从不同角度思考问题\r\n- 记录已经确认的线索，以便归纳整理\r\n\r\n## 关于谜题\r\n\r\n目前本项目已经提供了35个谜题。\r\n本人曾开发[海龟汤模拟器](https://www.hgtang.com)，该网站有评分功能。目前的35个谜题来自于我和一些热心用户为该网站搜集的谜题。依据该网站上各谜题的评分，推荐游玩的谜题如下：\r\n\r\n- 忠诚的狗\r\n- 100元钱\r\n- 爱犬\r\n- 治病\r\n- 祭日\r\n- 电梯里的人\r\n- 延迟死亡\r\n- 生意\r\n- 裤子破了\r\n- 要好的朋友\r\n\r\n欢迎你为本项目贡献更多的谜题。你可以在haiguitang_mcp/puzzles文件夹中加入新的谜题文件然后发起Pull Request。\r\n\r\n注意：\r\n\r\n- 如果你希望用户游玩之前有所预警，你可以在标题，也就是谜题文件的名称中注明。\r\n- 请注意海龟汤的版权问题。\r\n- 你可以在海龟汤文件中添加作者和提交者信息。\r\n\r\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "interactive",
        "soup",
        "ai",
        "soup games",
        "games ai",
        "interactive turtle"
      ],
      "category": "virtual-assistants"
    }
  }
}