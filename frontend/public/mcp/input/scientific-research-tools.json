{
  "category": "scientific-research-tools",
  "categoryDisplay": "Scientific Research Tools",
  "description": "",
  "totalRepositories": 94,
  "repositories": {
    "54yyyu--zotero-mcp": {
      "owner": "54yyyu",
      "name": "zotero-mcp",
      "url": "https://github.com/54yyyu/zotero-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/54yyyu.webp",
      "description": "Zotero MCP allows you to link your Zotero research library with AI assistants, enabling you to discuss academic papers, receive summaries, and analyze citations for better research productivity.",
      "stars": 565,
      "forks": 42,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-10-04T10:21:36Z",
      "readme_content": "# Zotero MCP: Chat with your Research Library—Local or Web—in Claude, ChatGPT, and more.\n\n<p align=\"center\">\n  <a href=\"https://www.zotero.org/\">\n    <img src=\"https://img.shields.io/badge/Zotero-CC2936?style=for-the-badge&logo=zotero&logoColor=white\" alt=\"Zotero\">\n  </a>\n  <a href=\"https://www.anthropic.com/claude\">\n    <img src=\"https://img.shields.io/badge/Claude-6849C3?style=for-the-badge&logo=anthropic&logoColor=white\" alt=\"Claude\">\n  </a>\n  <a href=\"https://chatgpt.com/\">\n    <img src=\"https://img.shields.io/badge/ChatGPT-74AA9C?style=for-the-badge&logo=openai&logoColor=white\" alt=\"ChatGPT\">\n  </a>\n  <a href=\"https://modelcontextprotocol.io/introduction\">\n    <img src=\"https://img.shields.io/badge/MCP-0175C2?style=for-the-badge&logoColor=white\" alt=\"MCP\">\n  </a>\n</p>\n\n**Zotero MCP** seamlessly connects your [Zotero](https://www.zotero.org/) research library with [ChatGPT](https://openai.com), [Claude](https://www.anthropic.com/claude), and other AI assistants (e.g., [Cherry Studio](https://cherry-ai.com/), [Chorus](https://chorus.sh), [Cursor](https://www.cursor.com/)) via the [Model Context Protocol](https://modelcontextprotocol.io/introduction). Review papers, get summaries, analyze citations, extract PDF annotations, and more!\n\n## ✨ Features\n\n### 🧠 AI-Powered Semantic Search\n- **Vector-based similarity search** over your entire research library\n- **Multiple embedding models**: Default (free), OpenAI, and Gemini options\n- **Intelligent results** with similarity scores and contextual matching\n- **Auto-updating database** with configurable sync schedules\n\n### 🔍 Search Your Library\n- Find papers, articles, and books by title, author, or content\n- Perform complex searches with multiple criteria\n- Browse collections, tags, and recent additions\n- **NEW**: Semantic search for conceptual and topic-based discovery\n\n### 📚 Access Your Content\n- Retrieve detailed metadata for any item\n- Get full text content (when available)\n- Access attachments, notes, and child items\n\n### 📝 Work with Annotations\n- Extract and search PDF annotations directly\n- Access Zotero's native annotations\n- Create and update notes and annotations\n\n### 🔄 Easy Updates\n- **Smart update system** that detects your installation method (uv, pip, conda, pipx)\n- **Configuration preservation** - all settings maintained during updates\n- **Version checking** and automatic update notifications\n\n### 🌐 Flexible Access Methods\n- Local method for offline access (no API key needed)\n- Web API for cloud library access\n- Perfect for both local research and remote collaboration\n\n## 🚀 Quick Install\n\n### Default Installation\n\n#### Installing via uv\n\n```bash\nuv tool install \"git+https://github.com/54yyyu/zotero-mcp.git\"\nzotero-mcp setup  # Auto-configure (Claude Desktop supported)\n```\n\n#### Installing via pip\n\n```bash\npip install git+https://github.com/54yyyu/zotero-mcp.git\nzotero-mcp setup  # Auto-configure (Claude Desktop supported)\n```\n\n### Installing via Smithery\n\nTo install Zotero MCP via [Smithery](https://smithery.ai/server/@54yyyu/zotero-mcp) for Claude Desktop:\n\n```bash\nnpx -y @smithery/cli install @54yyyu/zotero-mcp --client claude\n```\n\n#### Updating Your Installation\n\nKeep zotero-mcp up to date with the smart update command:\n\n```bash\n# Check for updates\nzotero-mcp update --check-only\n\n# Update to latest version (preserves all configurations)\nzotero-mcp update\n```\n\n## 🧠 Semantic Search\n\nZotero MCP now includes powerful AI-powered semantic search capabilities that let you find research based on concepts and meaning, not just keywords.\n\n### Setup Semantic Search\n\nDuring setup or separately, configure semantic search:\n\n```bash\n# Configure during initial setup (recommended)\nzotero-mcp setup\n\n# Or configure semantic search separately\nzotero-mcp setup --semantic-config-only\n```\n\n**Available Embedding Models:**\n- **Default (all-MiniLM-L6-v2)**: Free, runs locally, good for most use cases\n- **OpenAI**: Better quality, requires API key (`text-embedding-3-small` or `text-embedding-3-large`)\n- **Gemini**: Better quality, requires API key (`models/text-embedding-004` or experimental models)\n\n**Update Frequency Options:**\n- **Manual**: Update only when you run `zotero-mcp update-db`\n- **Auto on startup**: Update database every time the server starts\n- **Daily**: Update once per day automatically\n- **Every N days**: Set custom interval\n\n### Using Semantic Search\n\nAfter setup, initialize your search database:\n\n```bash\n# Build the semantic search database (fast, metadata-only)\nzotero-mcp update-db\n\n# Build with full-text extraction (slower, more comprehensive)\nzotero-mcp update-db --fulltext\n\n# Check database status\nzotero-mcp db-status\n```\n\n**Example Semantic Queries in your AI assistant:**\n- *\"Find research similar to machine learning concepts in neuroscience\"*\n- *\"Papers that discuss climate change impacts on agriculture\"*\n- *\"Research related to quantum computing applications\"*\n- *\"Studies about social media influence on mental health\"*\n- *\"Find papers conceptually similar to this abstract: [paste abstract]\"*\n\nThe semantic search provides similarity scores and finds papers based on conceptual understanding, not just keyword matching.\n\n## 🖥️ Setup & Usage\n\nFull documentation is available at [Zotero MCP docs](https://stevenyuyy.us/zotero-mcp/).\n\n**Requirements**\n- Python 3.10+\n- Zotero 7+ (for local API with full-text access)\n- An MCP-compatible client (e.g., Claude Desktop, ChatGPT Developer Mode, Cherry Studio, Chorus)\n\n**For ChatGPT setup: see the [Getting Started guide](./docs/getting-started.md).**\n\n### For Claude Desktop (example MCP client)\n\n#### Configuration\nAfter installation, either:\n\n1. **Auto-configure** (recommended):\n   ```bash\n   zotero-mcp setup\n   ```\n\n2. **Manual configuration**:\n   Add to your `claude_desktop_config.json`:\n   ```json\n   {\n     \"mcpServers\": {\n       \"zotero\": {\n         \"command\": \"zotero-mcp\",\n         \"env\": {\n           \"ZOTERO_LOCAL\": \"true\"\n         }\n       }\n     }\n   }\n   ```\n\n#### Usage\n\n1. Start Zotero desktop (make sure local API is enabled in preferences)\n2. Launch Claude Desktop\n3. Access the Zotero-MCP tool through Claude Desktop's tools interface\n\nExample prompts:\n- \"Search my library for papers on machine learning\"\n- \"Find recent articles I've added about climate change\"\n- \"Summarize the key findings from my paper on quantum computing\"\n- \"Extract all PDF annotations from my paper on neural networks\"\n- \"Search my notes and annotations for mentions of 'reinforcement learning'\"\n- \"Show me papers tagged '#Arm' excluding those with '#Crypt' in my library\"\n- \"Search for papers on operating system with tag '#Arm'\"\n- \"Export the BibTeX citation for papers on machine learning\"\n- **\"Find papers conceptually similar to deep learning in computer vision\"** *(semantic search)*\n- **\"Research that relates to the intersection of AI and healthcare\"** *(semantic search)*\n- **\"Papers that discuss topics similar to this abstract: [paste text]\"** *(semantic search)*\n\n### For Cherry Studio\n\n#### Configuration\nGo to Settings -> MCP Servers -> Edit MCP Configuration, and add the following:\n\n```json\n{\n  \"mcpServers\": {\n    \"zotero\": {\n      \"name\": \"zotero\",\n      \"type\": \"stdio\",\n      \"isActive\": true,\n      \"command\": \"zotero-mcp\",\n      \"args\": [],\n      \"env\": {\n        \"ZOTERO_LOCAL\": \"true\"\n      }\n    }\n  }\n}\n```\nThen click \"Save\".\n\nCherry Studio also provides a visual configuration method for general settings and tools selection.\n\n## 🔧 Advanced Configuration\n\n### Using Web API Instead of Local API\n\nFor accessing your Zotero library via the web API (useful for remote setups):\n\n```bash\nzotero-mcp setup --no-local --api-key YOUR_API_KEY --library-id YOUR_LIBRARY_ID\n```\n\n### Environment Variables\n\n**Zotero Connection:**\n- `ZOTERO_LOCAL=true`: Use the local Zotero API (default: false)\n- `ZOTERO_API_KEY`: Your Zotero API key (for web API)\n- `ZOTERO_LIBRARY_ID`: Your Zotero library ID (for web API)\n- `ZOTERO_LIBRARY_TYPE`: The type of library (user or group, default: user)\n\n**Semantic Search:**\n- `ZOTERO_EMBEDDING_MODEL`: Embedding model to use (default, openai, gemini)\n- `OPENAI_API_KEY`: Your OpenAI API key (for OpenAI embeddings)\n- `OPENAI_EMBEDDING_MODEL`: OpenAI model name (text-embedding-3-small, text-embedding-3-large)\n- `GEMINI_API_KEY`: Your Gemini API key (for Gemini embeddings)\n- `GEMINI_EMBEDDING_MODEL`: Gemini model name (models/text-embedding-004, etc.)\n\n### Command-Line Options\n\n```bash\n# Run the server directly\nzotero-mcp serve\n\n# Specify transport method\nzotero-mcp serve --transport stdio|streamable-http|sse\n\n# Setup and configuration\nzotero-mcp setup --help                    # Get help on setup options\nzotero-mcp setup --semantic-config-only    # Configure only semantic search\nzotero-mcp setup-info                      # Show installation path and config info for MCP clients\n\n# Updates and maintenance\nzotero-mcp update                          # Update to latest version\nzotero-mcp update --check-only             # Check for updates without installing\nzotero-mcp update --force                  # Force update even if up to date\n\n# Semantic search database management\nzotero-mcp update-db                       # Update semantic search database (fast, metadata-only)\nzotero-mcp update-db --fulltext             # Update with full-text extraction (comprehensive but slower)\nzotero-mcp update-db --force-rebuild       # Force complete database rebuild\nzotero-mcp update-db --fulltext --force-rebuild  # Rebuild with full-text extraction\nzotero-mcp db-status                       # Show database status and info\n\n# General\nzotero-mcp version                         # Show current version\n```\n\n## 📑 PDF Annotation Extraction\n\nZotero MCP includes advanced PDF annotation extraction capabilities:\n\n- **Direct PDF Processing**: Extract annotations directly from PDF files, even if they're not yet indexed by Zotero\n- **Enhanced Search**: Search through PDF annotations and comments \n- **Image Annotation Support**: Extract image annotations from PDFs\n- **Seamless Integration**: Works alongside Zotero's native annotation system\n\nFor optimal annotation extraction, it is **highly recommended** to install the [Better BibTeX plugin](https://retorque.re/zotero-better-bibtex/installation/) for Zotero. The annotation-related functions have been primarily tested with this plugin and provide enhanced functionality when it's available.\n\n\nThe first time you use PDF annotation features, the necessary tools will be automatically downloaded.\n\n## 📚 Available Tools\n\n### 🧠 Semantic Search Tools\n- `zotero_semantic_search`: AI-powered similarity search with embedding models\n- `zotero_update_search_database`: Manually update the semantic search database\n- `zotero_get_search_database_status`: Check database status and configuration\n\n### 🔍 Search Tools\n- `zotero_search_items`: Search your library by keywords\n- `zotero_advanced_search`: Perform complex searches with multiple criteria\n- `zotero_get_collections`: List collections\n- `zotero_get_collection_items`: Get items in a collection\n- `zotero_get_tags`: List all tags\n- `zotero_get_recent`: Get recently added items\n- `zotero_search_by_tag`: Search your library using custom tag filters\n\n### 📚 Content Tools\n- `zotero_get_item_metadata`: Get detailed metadata (supports BibTeX export via `format=\"bibtex\"`)\n- `zotero_get_item_fulltext`: Get full text content\n- `zotero_get_item_children`: Get attachments and notes\n\n### 📝 Annotation & Notes Tools\n- `zotero_get_annotations`: Get annotations (including direct PDF extraction)\n- `zotero_get_notes`: Retrieve notes from your Zotero library\n- `zotero_search_notes`: Search in notes and annotations (including PDF-extracted)\n- `zotero_create_note`: Create a new note for an item (beta feature)\n\n## 🔍 Troubleshooting\n\n### General Issues\n- **No results found**: Ensure Zotero is running and the local API is enabled. You need to toggle on `Allow other applications on this computer to communicate with Zotero` in Zotero preferences.\n- **Can't connect to library**: Check your API key and library ID if using web API\n- **Full text not available**: Make sure you're using Zotero 7+ for local full-text access\n- **Local library limitations**: Some functionality (tagging, library modifications) may not work with local JS API. Consider using web library setup for full functionality. (See the [docs](docs/getting-started.md#local-library-limitations) for more info.)\n- **Installation/search option switching issues**: Database problems from changing install methods or search options can often be resolved with `zotero-mcp update-db --force-rebuild`\n\n### Semantic Search Issues\n- **\"Missing required environment variables\" when running update-db**: Run `zotero-mcp setup` to configure your environment, or the CLI will automatically load settings from your MCP client config (e.g., Claude Desktop)\n- **ChromaDB warnings**: Update to the latest version - deprecation warnings have been fixed\n- **Database update takes long**: By default, `update-db` is fast (metadata-only). For comprehensive indexing with full-text, use `--fulltext` flag. Use `--limit` parameter for testing: `zotero-mcp update-db --limit 100`\n- **Semantic search returns no results**: Ensure the database is initialized with `zotero-mcp update-db` and check status with `zotero-mcp db-status`\n- **Limited search quality**: For better semantic search results, use `zotero-mcp update-db --fulltext` to index full-text content (requires local Zotero setup)\n- **OpenAI/Gemini API errors**: Verify your API keys are correctly set and have sufficient credits/quota\n\n### Update Issues  \n- **Update command fails**: Check your internet connection and try `zotero-mcp update --force`\n- **Configuration lost after update**: The update process preserves configs automatically, but check `~/.config/zotero-mcp/` for backup files\n\n## 📄 License\n\nMIT\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "zotero",
        "research",
        "mcp",
        "mcp zotero",
        "zotero research",
        "zotero mcp"
      ],
      "category": "scientific-research-tools"
    },
    "BioContext--PubChem-MCP": {
      "owner": "BioContext",
      "name": "PubChem-MCP",
      "url": "https://github.com/BioContext/PubChem-MCP",
      "imageUrl": "/freedevtools/mcp/pfp/BioContext.webp",
      "description": "Provides access to PubChem chemical and bioassay data through a standardized MCP interface, allowing users to search for compounds, retrieve detailed chemical and bioassay information, and query molecular properties.",
      "stars": 0,
      "forks": 0,
      "license": "Other",
      "language": "Python",
      "updated_at": "2025-04-17T20:18:58Z",
      "readme_content": "# PubChem-MCP\n\nAn MCP (Model Context Protocol) server for accessing PubChem data. This server provides tools for querying the PubChem database for compounds, substances, bioassays, and related information.\n\n## Features\n\n* Search for compounds by name, structure, or identifier\n* Retrieve detailed information about compounds and substances\n* Access bioassay data and molecular properties\n* Query chemical classifications and cross-references\n* Compatible with any MCP client, including Claude for Desktop\n\n## Installation\n\n### Option 1: From GitHub\n\n```bash\n# Clone the repository\ngit clone https://github.com/BioContext/PubChem-MCP.git\ncd PubChem-MCP\n\n# Create a virtual environment\nuv venv\nsource .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\n\n# Install dependencies\npip install -r requirements.txt\n```\n\n### Option 2: Using pip (Once published)\n\n```bash\npip install pubchem-mcp\n```\n\n## Usage\n\n### Running as a standalone server\n\n```bash\n# From source\npython -m mcp_server\n\n# If installed via pip\npubchem-mcp\n```\n\n### Using with Claude for Desktop\n\n1. Install Claude for Desktop\n2. Configure Claude for Desktop to use this server by editing `~/Library/Application Support/Claude/claude_desktop_config.json` (macOS/Linux) or `%AppData%\\Claude\\claude_desktop_config.json` (Windows):\n\n```json\n{\n    \"mcpServers\": {\n        \"pubchem\": {\n            \"command\": \"python\",\n            \"args\": [\n                \"-m\",\n                \"mcp_server\"\n            ]\n        }\n    }\n}\n```\n\n3. Restart Claude for Desktop\n4. You can now use the PubChem tools in Claude\n\n### Example queries for Claude\n\n* \"Find information about aspirin in PubChem\"\n* \"What are the properties of compound CID 2244?\"\n* \"Show me the structure of paracetamol\"\n\n## Available Tools\n\n* `search_compound`: Search for compounds by name or identifier\n* `get_compound_details`: Get detailed information about a compound\n* `get_compound_properties`: Get physical and chemical properties of a compound\n* `search_bioassay`: Search for bioassays\n* `get_substance_details`: Get detailed information about a substance\n* And more...\n\n## Development\n\n```bash\n# Clone the repository\ngit clone https://github.com/BioContext/PubChem-MCP.git\ncd PubChem-MCP\n\n# Create a virtual environment\nuv venv\nsource .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\n\n# Install development dependencies\npip install -e \".[dev]\"\n\n# Run tests\npytest\n```\n\n## License\n\nMIT License\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "biocontext",
        "pubchem",
        "bioassay",
        "biocontext pubchem",
        "tools biocontext",
        "pubchem mcp"
      ],
      "category": "scientific-research-tools"
    },
    "DMontgomery40--mcp-server-birdstats": {
      "owner": "DMontgomery40",
      "name": "mcp-server-birdstats",
      "url": "https://github.com/DMontgomery40/mcp-server-birdstats",
      "imageUrl": "/freedevtools/mcp/pfp/DMontgomery40.webp",
      "description": "Analyze BirdNET-Pi data and eBird observations through a natural language interface while providing visual representations of the data.",
      "stars": 1,
      "forks": 7,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2024-12-13T00:47:19Z",
      "readme_content": "# mcp-server-birdstats\n\n> Cross-reference your BirdNET-Pi data with eBird observations using natural language\n\n## Overview\n\nThis MCP plugin allows you to analyze BirdNET-Pi/BirdWeather detections by connecting them with eBird's extensive database. \n\n## Features\n\n- Natural language interface for querying birdnet-pi / birdweather, and eBird data\n- Cross-reference BirdWeather detections with eBird observations\n- Data-driven visualizations with React artifacts\n\n## Installation\n\n1. Clone the repository:\n```bash\ngit clone https://github.com/yourusername/mcp-server-birdstats.git\n```\n\n2. Install dependencies:\n```bash\nnpm install\n```\n## Claude Config\n```\n\"mcp-birdstats\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"mcp-birdstats\"]\n    },\n```\n\n\n## Usage\n\nThe plugin processes natural language queries about your bird detection data. Example queries:\n\n- \"My BirdNET heard a Swainson's Thrush - is that unusual?\"\n- \"What's my station's rarest detection this week?\"\n- \"Graph my detections by time of day\"\n- \"Are others seeing these warblers on eBird?\"\n\n## Data Integrity\n\nThis plugin strictly adheres to data integrity principles:\n- Only uses actual API response data\n- Never creates example or estimated data\n- Always shows observation counts with percentages\n- Clear indication when data is unavailable\n\n## Core Logic\n\nThe core functionality is contained in a single text file under `src/index.txt` which includes:\n- BirdWeather API interactions\n- eBird API interactions\n- Data analysis functions\n- Type definitions\n- Helper utilities\n\n## API Integration\n\n### BirdWeather API\n- Station statistics\n- Species lists\n- Detection records\n\n### eBird API\n- Recent observations\n- Notable sightings\n- Hotspot data\n- Species information\n\n## License\n\nMIT\n\n## Support\n\nFor issues and feature requests, please open an issue on the GitHub repository.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "birdstats",
        "birdnet",
        "ebird",
        "birdstats analyze",
        "analyze birdnet",
        "server birdstats"
      ],
      "category": "scientific-research-tools"
    },
    "Garoth--wolframalpha-llm-mcp": {
      "owner": "Garoth",
      "name": "wolframalpha-llm-mcp",
      "url": "https://github.com/Garoth/wolframalpha-llm-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/Garoth.webp",
      "description": "Provides access to WolframAlpha's LLM API for querying complex information in natural language, allowing for structured responses on topics like science, mathematics, and history.",
      "stars": 39,
      "forks": 12,
      "license": "No License",
      "language": "TypeScript",
      "updated_at": "2025-09-24T02:59:15Z",
      "readme_content": "# WolframAlpha LLM MCP Server\n\n\n\nA Model Context Protocol (MCP) server that provides access to WolframAlpha's LLM API. https://products.wolframalpha.com/llm-api/documentation\n\n<div>\n  <br/><br/>\n  \n</div>\n\n## Features\n\n- Query WolframAlpha's LLM API with natural language questions\n- Answer complicated mathematical questions\n- Query facts about science, physics, history, geography, and more\n- Get structured responses optimized for LLM consumption\n- Support for simplified answers and detailed responses with sections\n\n## Available Tools\n\n- `ask_llm`: Ask WolframAlpha a question and get a structured llm-friendly response\n- `get_simple_answer`: Get a simplified answer\n- `validate_key`: Validate the WolframAlpha API key\n\n## Installation\n\n```bash\ngit clone https://github.com/Garoth/wolframalpha-llm-mcp.git\nnpm install\n```\n\n## Configuration\n\n1. Get your WolframAlpha API key from [developer.wolframalpha.com](https://developer.wolframalpha.com/)\n\n2. Add it to your Cline MCP settings file inside VSCode's settings (ex. ~/.config/Code/User/globalStorage/saoudrizwan.claude-dev/settings/cline_mcp_settings.json):\n\n```json\n{\n  \"mcpServers\": {\n    \"wolframalpha\": {\n      \"command\": \"node\",\n      \"args\": [\"/path/to/wolframalpha-mcp-server/build/index.js\"],\n      \"env\": {\n        \"WOLFRAM_LLM_APP_ID\": \"your-api-key-here\"\n      },\n      \"disabled\": false,\n      \"autoApprove\": [\n        \"ask_llm\",\n        \"get_simple_answer\",\n        \"validate_key\"\n      ]\n    }\n  }\n}\n```\n\n## Development\n\n### Setting Up Tests\n\nThe tests use real API calls to ensure accurate responses. To run the tests:\n\n1. Copy the example environment file:\n   ```bash\n   cp .env.example .env\n   ```\n\n2. Edit `.env` and add your WolframAlpha API key:\n   ```\n   WOLFRAM_LLM_APP_ID=your-api-key-here\n   ```\n   Note: The `.env` file is gitignored to prevent committing sensitive information.\n\n3. Run the tests:\n   ```bash\n   npm test\n   ```\n\n### Building\n\n```bash\nnpm run build\n```\n\n## License\n\nMIT",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "wolframalpha",
        "scientific",
        "tools",
        "research tools",
        "wolframalpha llm",
        "llm api"
      ],
      "category": "scientific-research-tools"
    },
    "HarshJ23--deepseek-claude-MCP-server": {
      "owner": "HarshJ23",
      "name": "deepseek-claude-MCP-server",
      "url": "https://github.com/HarshJ23/deepseek-claude-MCP-server",
      "imageUrl": "/freedevtools/mcp/pfp/HarshJ23.webp",
      "description": "Integrates DeepSeek R1's advanced reasoning engine with Claude to enhance reasoning capabilities for complex tasks. Supports intricate multi-step reasoning for improved precision and efficiency.",
      "stars": 50,
      "forks": 11,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-13T00:27:41Z",
      "readme_content": "# DeepSeek-Claude MCP Server\n[![smithery badge](https://smithery.ai/badge/@HarshJ23/deepseek-claude-MCP-server)](https://smithery.ai/server/@HarshJ23/deepseek-claude-MCP-server)\n\n**Enhance Claude's reasoning capabilities** with the integration of DeepSeek R1's advanced reasoning engine. This server enables Claude to tackle complex reasoning tasks by leveraging the reasoning capabilites of deepseek r1 model.\n\n<a href=\"https://glama.ai/mcp/servers/w3q7v1srsw\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/w3q7v1srsw/badge\" alt=\"DeepSeek-Claude Server MCP server\" /></a>\n\n---\n\n## 🚀 Features\n\n### **Advanced Reasoning Capabilities**\n- Seamlessly integrates DeepSeek R1's reasoning with Claude.\n- Supports intricate multi-step reasoning tasks.\n- Designed for precision and efficiency in generating thoughtful responses.\n\n\n\n\n\n\n---\n\n## Complete Setup guide\n\n### Installing via Smithery\n\nTo install DeepSeek-Claude for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@HarshJ23/deepseek-claude-MCP-server):\n\n```bash\nnpx -y @smithery/cli install @HarshJ23/deepseek-claude-MCP-server --client claude\n```\n\n### Prerequisites\n- Python 3.12 or higher\n- `uv` package manager\n- DeepSeek API key (Sign up at [DeepSeek Platform](https://platform.deepseek.com))\n\n\n\n1. **Clone the Repository**\n   ```bash\n   git clone https://github.com/harshj23/deepseek-claude-MCP-server.git\n   cd deepseek-claude-MCP-server\n   ```\n\n2. **Ensure UV is Set Up**\n   - **Windows**: Run the following in PowerShell:\n     ```powershell\n     powershell -ExecutionPolicy ByPass -c \"irm https://astral.sh/uv/install.ps1 | iex\"\n     ```\n   - **Mac**: Run the following:\n     ```bash\n     curl -LsSf https://astral.sh/uv/install.sh | sh\n     ```\n\n3. **Create Virtual Environment**\n   ```bash\n   uv venv\n   source .venv/bin/activate\n   ```\n\n4. **Install Dependencies**\n   ```bash\n   uv add \"mcp[cli]\" httpx\n   ```\n\n5. **Set Up API Key**\n   ```bash\n   Obtain your api key from here : https://platform.deepseek.com/api_keys\n   ```\n\n6. **Configure MCP Server**\n   Edit the `claude_desktop_config.json` file to include the following configuration:\n   \n\n   ```json\n   {\n       \"mcpServers\": {\n           \"deepseek-claude\": {\n               \"command\": \"uv\",\n               \"args\": [\n                   \"--directory\",\n                   \"C:\\\\ABSOLUTE\\\\PATH\\\\TO\\\\PARENT\\\\FOLDER\\\\deepseek-claude\",\n                   \"run\",\n                   \"server.py\"\n               ]\n           }\n       }\n   }\n   ```\n\n7. **Run the Server**\n   ```bash\n   uv run server.py\n   ```\n\n8. **Test Setup**\n   - ##### Restart Claude Desktop.\n   - Verify the tools icon is visible in the interface.\n   \n\n\n   - If the server isn’t visible, consult the [troubleshooting guide](https://modelcontextprotocol.io/quickstart/server#troubleshooting).\n\n---\n\n## 🛠 Usage\n\n### Starting the Server\nThe server automatically starts when used with Claude Desktop. Ensure Claude Desktop is configured to detect the MCP server.\n\n### Example Workflow\n1. Claude receives a query requiring advanced reasoning.\n2. The query is forwarded to DeepSeek R1 for processing.\n3. DeepSeek R1 returns structured reasoning wrapped in `<ant_thinking>` tags.\n4. Claude integrates the reasoning into its final response.\n\n---\n\n\n## 📄 License\n\nThis project is licensed under the MIT License. See the [LICENSE](LICENSE) file for details.\n\n---",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "deepseek",
        "reasoning",
        "advanced",
        "reasoning engine",
        "deepseek r1",
        "integrates deepseek"
      ],
      "category": "scientific-research-tools"
    },
    "Jacck--mcp-reasoner": {
      "owner": "Jacck",
      "name": "mcp-reasoner",
      "url": "https://github.com/Jacck/mcp-reasoner",
      "imageUrl": "/freedevtools/mcp/pfp/Jacck.webp",
      "description": "Systematic reasoning server implementation for Claude Desktop, supporting Beam Search and Monte Carlo Tree Search (MCTS) for enhanced complex problem-solving. Features experimental algorithms for policy simulation and adaptive exploration.",
      "stars": 254,
      "forks": 28,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-10-03T12:29:08Z",
      "readme_content": "# MCP Reasoner\nA reasoning implementation for Claude Desktop that lets you use both Beam Search and Monte Carlo Tree Search (MCTS). tbh this started as a way to see if we could make Claude even better at complex problem-solving... turns out we definitely can.\n\n### Current Version:\n**v2.0.0**\n\n#### What's New:\n\n> Added 2 Experimental Reasoning Algorithms:\n>\n>     - `mcts-002-alpha`\n>\n>         - Uses the A* Search Method along with an early *alpha* implementation of a Policy Simulation Layer\n>\n>         - Also includes an early *alpha* implementation of Adaptive Exploration Simulator & Outcome Based Reasoning Simulator\n>\n>     *NOTE* the implementation of these alpha simulators is not complete and is subject to change\n>\n>     - `mcts-002alt-alpha`\n>\n>         - Uses the Bidirectional Search Method along with an early *alpha* implementation of a Policy Simulation Layer\n>\n>         - Also includes an early *alpha* implementation of Adaptive Exploration Simulator & Outcome Based Reasoning Simulator\n>\n>     *NOTE* the implementation of these alpha simulators is not complete and is subject to change\n\n\nWhat happened to `mcts-001-alpha` and `mcts-001alt-alpha`?\n> Quite simply: It was useless and near similar to the base `mcts` method. After initial testing the results yielded in basic thought processes was near similar showing that simply adding policy simulation may not have an effect.\n\nSo why add Polciy Simulation Layer now?\n> Well i think its important to incorporate Policy AND Search in tandem as that is how most of the algorithms implement them.\n\n#### Previous Versions:\n**v1.1.0**\n\n> Added model control over search parameters:\n>\n> beamWidth - lets Claude adjust how many paths to track (1-10)\n>\n> numSimulations - fine-tune MCTS simulation count (1-150)\n\n## Features\n- Two search strategies that you can switch between:\n   - Beam search (good for straightforward stuff)\n   - MCTS (when stuff gets complex) with alpha variations (see above)\n- Tracks how good different reasoning paths are\n- Maps out all the different ways Claude thinks through problems\n- Analyzes how the reasoning process went\n- Follows the MCP protocol (obviously)\n\n## Installation\n```\ngit clone https://github.com/frgmt0/mcp-reasoner.git \n\nOR clone the original:\n\ngit clone https://github.com/Jacck/mcp-reasoner.git\n\ncd mcp-reasoner\nnpm install\nnpm run build\n```\n\n## Configuration\nAdd to Claude Desktop config:\n```\n{\n  \"mcpServers\": {\n    \"mcp-reasoner\": {\n      \"command\": \"node\",\n      \"args\": [\"path/to/mcp-reasoner/dist/index.js\"],\n    }\n  }\n}\n```\n\n## Testing\n\n[More Testing Coming Soon]\n\n## Benchmarks\n\n[Benchmarking will be added soon]\n\nKey Benchmarks to test against:\n\n- MATH500\n\n- GPQA-Diamond\n\n- GMSK8\n\n- Maybe Polyglot &/or SWE-Bench\n\n## License\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "mcp",
        "reasoner",
        "mcts",
        "systematic reasoning",
        "mcp reasoner",
        "reasoning server"
      ],
      "category": "scientific-research-tools"
    },
    "JackKuo666--Crossref-MCP-Server": {
      "owner": "JackKuo666",
      "name": "Crossref-MCP-Server",
      "url": "https://github.com/JackKuo666/Crossref-MCP-Server",
      "imageUrl": "/freedevtools/mcp/pfp/JackKuo666.webp",
      "description": "Connect to Crossref's database to search and access academic paper metadata, including metadata retrieval for specific papers, journals, and funding organizations using various queries.",
      "stars": 3,
      "forks": 1,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-07-28T16:51:46Z",
      "readme_content": "# Crossref MCP Server\n\n🔍 Enable AI assistants to search and access academic paper metadata through Crossref using a simple MCP interface.\n\nThe Crossref MCP Server provides a bridge between AI assistants and Crossref's database of academic literature through the Model Context Protocol (MCP). It allows AI models to search for scientific articles by DOI, title, or keywords, access their metadata, and retrieve journal and funder information in a programmatic way.\n\n## ✨ Core Features\n\n- 🔎 Work Search by Query: Find papers using keywords, titles, or authors ✅\n- 📊 Metadata Access: Retrieve detailed metadata for specific papers by DOI ✅\n- 📚 Journal Search: Find journals in the Crossref database ✅\n- 💰 Funder Search: Discover funding organizations and their supported research ✅\n\n## 🚀 Quick Start\n\n### Prerequisites\n\n- Python 3.10+\n- FastMCP library\n\n### Installation\n\n1. Clone the repository:\n   ```\n   git clone https://github.com/JackKuo666/Crossref-MCP-Server.git\n   cd Crossref-MCP-Server\n   ```\n\n2. Install the required dependencies:\n   ```\n   pip install -r requirements.txt\n   ```\n\n## 📊 Usage\n\nStart the MCP server:\n\n```bash\npython crossref_server.py\n```\n\n## Usage with Claude Desktop or Cline\n\nAdd this configuration to your `cline_mcp_settings.json` or `claude_desktop_config.json`:\n\n(Mac OS)\n\n```json\n{\n  \"mcpServers\": {\n    \"crossref\": {\n      \"command\": \"python\",\n      \"args\": [\"-m\", \"crossref_server.py\"]\n      }\n  }\n}\n```\n\n(Windows version):\n\n```json\n{\n  \"mcpServers\": {\n    \"crossref\": {\n      \"command\": \"C:\\\\Users\\\\YOUR\\\\PATH\\\\miniconda3\\\\envs\\\\mcp_server\\\\python.exe\",\n      \"args\": [\n        \"D:\\\\code\\\\YOUR\\\\PATH\\\\Crossref-MCP-Server\\\\crossref_server.py\"\n      ],\n      \"env\": {},\n      \"disabled\": false,\n      \"autoApprove\": []\n    }\n  }\n}\n```\n\n## 🛠 MCP Tools\n\nThe Crossref MCP Server provides the following tools:\n\n1. `search_works_by_query`: Search for scholarly works using keywords, titles, or authors.\n2. `get_work_metadata`: Get detailed metadata for a specific work using its DOI.\n3. `search_journals`: Search for journals in the Crossref database.\n4. `search_funders`: Search for funding organizations in the Crossref database.\n\n### Searching Works by Query\n\nYou can ask the AI assistant to search for papers using keywords:\n```\nCan you search Crossref for papers about \"machine learning in healthcare\"?\n```\n\n### Getting Work Metadata by DOI\n\nYou can get detailed metadata for a specific paper using its DOI:\n```\nCan you show me the metadata for the paper with DOI 10.1038/nature14539?\n```\n\n### Searching Journals\n\nYou can search for journals in the Crossref database:\n```\nCan you find journals related to \"artificial intelligence\" in Crossref?\n```\n\n### Searching Funders\n\nYou can search for funding organizations:\n```\nCan you find information about the \"National Science Foundation\" in Crossref?\n```\n\n\n## 📁 Project Structure\n\n- `crossref_server.py`: The main MCP server implementation using FastMCP\n- `crossref_search.py`: Contains the logic for searching Crossref and retrieving metadata\n\n## 🔧 Dependencies\n\n- Python 3.10+\n- FastMCP (mcp)\n- requests\n- bs4\n- habanero\n\n## 🤝 Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n\n## 📄 License\n\nThis project is licensed under the MIT License.\n\n## ⚠️ Note\n\nThis tool uses the Crossref API to access publicly available metadata about academic works. For better API access priority, it's recommended to provide your email address when initializing the CrossrefSearch class.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "crossref",
        "journals",
        "metadata",
        "research tools",
        "paper metadata",
        "metadata retrieval"
      ],
      "category": "scientific-research-tools"
    },
    "JackKuo666--Google-Scholar-MCP-Server": {
      "owner": "JackKuo666",
      "name": "Google-Scholar-MCP-Server",
      "url": "https://github.com/JackKuo666/Google-Scholar-MCP-Server",
      "imageUrl": "/freedevtools/mcp/pfp/JackKuo666.webp",
      "description": "Enables AI assistants to perform searches on Google Scholar to access academic papers, retrieve paper metadata, and gather detailed author information. Facilitates efficient academic research through a seamless programmatic interface.",
      "stars": 138,
      "forks": 25,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-10-03T08:00:22Z",
      "readme_content": "# Google Scholar MCP Server\n[![smithery badge](https://smithery.ai/badge/@JackKuo666/google-scholar-mcp-server)](https://smithery.ai/server/@JackKuo666/google-scholar-mcp-server)\n\n🔍 Enable AI assistants to search and access Google Scholar papers through a simple MCP interface.\n\nThe Google Scholar MCP Server provides a bridge between AI assistants and Google Scholar through the Model Context Protocol (MCP). It allows AI models to search for academic papers and access their content in a programmatic way.\n\n## ✨ Core Features\n- 🔎 Paper Search: Query Google Scholar papers with custom search strings or advanced search parameters ✅\n- 🚀 Efficient Retrieval: Fast access to paper metadata ✅\n- 👤 Author Information: Retrieve detailed information about authors ✅\n- 📊 Research Support: Facilitate academic research and analysis ✅\n\n## 🚀 Quick Start\n\n### Installing Manually\n### Installing via Smithery\n\nTo install google-scholar Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@JackKuo666/google-scholar-mcp-server):\n\n#### claude\n\n```sh\nnpx -y @smithery/cli@latest install @JackKuo666/google-scholar-mcp-server --client claude --config \"{}\"\n```\n\n#### Cursor\n\nPaste the following into Settings → Cursor Settings → MCP → Add new server: \n- Mac/Linux  \n```s\nnpx -y @smithery/cli@latest run @JackKuo666/google-scholar-mcp-server --client cursor --config \"{}\" \n```\n#### Windsurf\n```sh\nnpx -y @smithery/cli@latest install @JackKuo666/google-scholar-mcp-server --client windsurf --config \"{}\"\n```\n### CLine\n```sh\nnpx -y @smithery/cli@latest install @JackKuo666/google-scholar-mcp-server --client cline --config \"{}\"\n```\n\n1. Clone the repository:\n   ```\n   git clone https://github.com/JackKuo666/google-scholar-MCP-Server.git\n   cd google-scholar-MCP-Server\n   ```\n\n2. Install the required dependencies:\n   ```\n   pip install -r requirements.txt\n   ```\n\n\nFor development:\n\n```bash\n# Clone and set up development environment\ngit clone https://github.com/JackKuo666/Google-Scholar-MCP-Server.git\ncd Google-Scholar-MCP-Server\n\n# Create and activate virtual environment\npython -m venv venv\nsource venv/bin/activate  # On Windows use `venv\\Scripts\\activate`\n\n# Install dependencies\npip install -r requirements.txt\n```\n\n## 📊 Usage\n\nStart the MCP server:\n\n```bash\npython google_scholar_server.py\n```\n\nOnce the server is running, you can use the provided MCP tools in your AI assistant or application. Here are some examples of how to use the tools:\n\n### Example 1: Search for papers using keywords\n\n```python\nresult = await mcp.use_tool(\"search_google_scholar_key_words\", {\n    \"query\": \"artificial intelligence ethics\",\n    \"num_results\": 5\n})\nprint(result)\n```\n\n### Example 2: Perform an advanced search\n\n```python\nresult = await mcp.use_tool(\"search_google_scholar_advanced\", {\n    \"query\": \"machine learning\",\n    \"author\": \"Hinton\",\n    \"year_range\": [2020, 2023],\n    \"num_results\": 3\n})\nprint(result)\n```\n\n### Example 3: Get author information\n\n```python\nresult = await mcp.use_tool(\"get_author_info\", {\n    \"author_name\": \"Geoffrey Hinton\"\n})\nprint(result)\n```\n\nThese examples demonstrate how to use the three main tools provided by the Google Scholar MCP Server. Adjust the parameters as needed for your specific use case.\n\n## Usage with Claude Desktop\n\nAdd this configuration to your `claude_desktop_config.json`:\n\n(Mac OS)\n\n```json\n{\n  \"mcpServers\": {\n    \"google-scholar\": {\n      \"command\": \"python\",\n      \"args\": [\"-m\", \"google_scholar_mcp_server\"]\n      }\n  }\n}\n```\n\n(Windows version):\n\n```json\n{\n  \"mcpServers\": {\n    \"google-scholar\": {\n      \"command\": \"C:\\\\Users\\\\YOUR\\\\PATH\\\\miniconda3\\\\envs\\\\mcp_server\\\\python.exe\",\n      \"args\": [\n        \"D:\\\\code\\\\YOUR\\\\PATH\\\\Google-Scholar-MCP-Server\\\\google_scholar_server.py\"\n      ],\n      \"env\": {},\n      \"disabled\": false,\n      \"autoApprove\": []\n    }\n  }\n}\n```\nUsing with Cline\n```json\n{\n  \"mcpServers\": {\n    \"google-scholar\": {\n      \"command\": \"bash\",\n      \"args\": [\n        \"-c\",\n        \"source /home/YOUR/PATH/.venv/bin/activate && python /home/YOUR/PATH/google_scholar_mcp_server.py\"\n      ],\n      \"env\": {},\n      \"disabled\": false,\n      \"autoApprove\": []\n    }\n  }\n}\n```\n\n\n## 🛠 MCP Tools\n\nThe Google Scholar MCP Server provides the following tools:\n\n### search_google_scholar_key_words\n\nSearch for articles on Google Scholar using key words.\n\n**Parameters:**\n- `query` (str): Search query string\n- `num_results` (int, optional): Number of results to return (default: 5)\n\n**Returns:** List of dictionaries containing article information\n\n### search_google_scholar_advanced\n\nPerform an advanced search for articles on Google Scholar.\n\n**Parameters:**\n- `query` (str): General search query\n- `author` (str, optional): Author name\n- `year_range` (tuple, optional): Tuple containing (start_year, end_year)\n- `num_results` (int, optional): Number of results to return (default: 5)\n\n**Returns:** List of dictionaries containing article information\n\n### get_author_info\n\nGet detailed information about an author from Google Scholar.\n\n**Parameters:**\n- `author_name` (str): Name of the author to search for\n\n**Returns:** Dictionary containing author information\n\n## 📁 Project Structure\n\n- `google_scholar_server.py`: The main MCP server implementation using FastMCP\n- `google_scholar_web_search.py`: Contains the web scraping logic for searching Google Scholar\n\n## 🔧 Dependencies\n\n- Python 3.10+\n- mcp[cli]>=1.4.1\n- scholarly>=1.7.0\n- asyncio>=3.4.3\n\nYou can install the required dependencies using:\n\n```bash\npip install -r requirements.txt\n```\n\n## 🤝 Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n\n## 📄 License\n\nThis project is licensed under the MIT License.\n\n## ⚠️ Disclaimer\n\nThis tool is for research purposes only. Please respect Google Scholar's terms of service and use this tool responsibly.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "scholar",
        "google",
        "academic",
        "google scholar",
        "scholar access",
        "research tools"
      ],
      "category": "scientific-research-tools"
    },
    "JackKuo666--PubChem-MCP-Server": {
      "owner": "JackKuo666",
      "name": "PubChem-MCP-Server",
      "url": "https://github.com/JackKuo666/PubChem-MCP-Server",
      "imageUrl": "/freedevtools/mcp/pfp/JackKuo666.webp",
      "description": "Access and retrieve information about chemical compounds from PubChem, including detailed property data and molecular structures through a simple interface. Supports queries by name, SMILES, or CID to streamline research and analysis in chemistry.",
      "stars": 3,
      "forks": 1,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-06-13T09:03:06Z",
      "readme_content": "# PubChem MCP Server\n[![smithery badge](https://smithery.ai/badge/@JackKuo666/pubchem-mcp-server)](https://smithery.ai/server/@JackKuo666/pubchem-mcp-server)\n\n🧪 Enable AI assistants to search and access chemical compound information through a simple MCP interface.\n\nThe PubChem MCP Server provides a bridge between AI assistants and PubChem's chemical database through the Model Context Protocol (MCP). It allows AI models to search for chemical compounds and access their detailed information in a programmatic way.\n\n🤝 Contribute • 📝 Report Bug\n\n## ✨ Core Features\n- 🔎 Compound Search: Query PubChem compounds by name, SMILES, or CID ✅\n- 🧪 Chemical Structure: Access molecular structures and identifiers ✅\n- 📊 Property Data: Retrieve detailed chemical and physical properties ✅\n- 🔬 Advanced Search: Combine multiple parameters for precise queries ✅\n- 🧬 Molecular Visualization: Generate and display molecular structures 📝\n- 📈 Property Analysis: Compare properties across multiple compounds 📝\n- 🗃️ Local Storage: Save frequently used compounds for faster access 📝\n- 📝 Chemistry Prompts: Specialized prompts for chemical analysis 📝\n\n## 🚀 Quick Start\n\n### Installing via Smithery\n\nTo install PubChem Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@JackKuo666/pubchem-mcp-server):\n\n#### Claude\n\n```bash\nnpx -y @smithery/cli@latest install @JackKuo666/pubchem-mcp-server --client claude --config \"{}\"\n```\n\n#### Cursor\n\nPaste the following into Settings → Cursor Settings → MCP → Add new server: \n- Mac/Linux  \n```s\nnpx -y @smithery/cli@latest run @JackKuo666/pubchem-mcp-server --client cursor --config \"{}\" \n```\n#### Windsurf\n```sh\nnpx -y @smithery/cli@latest install @JackKuo666/pubchem-mcp-server --client windsurf --config \"{}\"\n```\n### CLine\n```sh\nnpx -y @smithery/cli@latest install @JackKuo666/pubchem-mcp-server --client cline --config \"{}\"\n```\n\n### Installing Manually\nInstall using uv:\n\n```bash\nuv tool install pubchem-mcp-server\n```\n\nFor development:\n\n```bash\n# Clone and set up development environment\ngit clone https://github.com/JackKuo666/PubChem-MCP-Server.git\ncd PubChem-MCP-Server\n\n# Create and activate virtual environment\nuv venv\nsource .venv/bin/activate\nuv pip install -r requirements.txt\n```\n\n## 📊 Usage\n\nStart the MCP server:\n\n```bash\npython pubchem_server.py\n```\n\nOnce the server is running, you can use the provided MCP tools in your AI assistant or application. Here are some examples of how to use the tools:\n\n### Example 1: Search for compounds by name\n\n```python\nresult = await mcp.use_tool(\"search_pubchem_by_name\", {\n    \"name\": \"aspirin\",\n    \"max_results\": 3\n})\nprint(result)\n```\n\n### Example 2: Search for compounds by SMILES notation\n\n```python\nresult = await mcp.use_tool(\"search_pubchem_by_smiles\", {\n    \"smiles\": \"CC(=O)OC1=CC=CC=C1C(=O)O\",  # Aspirin's SMILES\n    \"max_results\": 2\n})\nprint(result)\n```\n\n### Example 3: Get detailed information for a specific compound\n\n```python\nresult = await mcp.use_tool(\"get_pubchem_compound_by_cid\", {\n    \"cid\": 2244  # Aspirin's CID\n})\nprint(result)\n```\n\n### Example 4: Perform an advanced search with multiple parameters\n\n```python\nresult = await mcp.use_tool(\"search_pubchem_advanced\", {\n    \"name\": \"caffeine\",\n    \"formula\": \"C8H10N4O2\",\n    \"max_results\": 2\n})\nprint(result)\n```\n\nThese examples demonstrate how to use the four main tools provided by the PubChem MCP Server. Adjust the parameters as needed for your specific use case.\n\n## 🛠 MCP Tools\n\nThe PubChem MCP Server provides the following tools:\n\n### search_pubchem_by_name\n\nSearch for chemical compounds on PubChem using a compound name.\n\n**Parameters:**\n- `name` (str): Name of the chemical compound\n- `max_results` (int, optional): Maximum number of results to return (default: 5)\n\n**Returns:** List of dictionaries containing compound information\n\n### search_pubchem_by_smiles\n\nSearch for chemical compounds on PubChem using a SMILES string.\n\n**Parameters:**\n- `smiles` (str): SMILES notation of the chemical compound\n- `max_results` (int, optional): Maximum number of results to return (default: 5)\n\n**Returns:** List of dictionaries containing compound information\n\n### get_pubchem_compound_by_cid\n\nFetch detailed information about a chemical compound using its PubChem CID.\n\n**Parameters:**\n- `cid` (int): PubChem Compound ID (CID)\n\n**Returns:** Dictionary containing compound information\n\n### search_pubchem_advanced\n\nPerform an advanced search for compounds on PubChem.\n\n**Parameters:**\n- `name` (str, optional): Name of the chemical compound\n- `smiles` (str, optional): SMILES notation of the chemical compound\n- `formula` (str, optional): Molecular formula\n- `cid` (int, optional): PubChem Compound ID\n- `max_results` (int, optional): Maximum number of results to return (default: 5)\n\n**Returns:** List of dictionaries containing compound information\n\n## Usage with Claude Desktop\n\nAdd this configuration to your `claude_desktop_config.json`:\n\n(Mac OS)\n\n```json\n{\n  \"mcpServers\": {\n    \"pubchem\": {\n      \"command\": \"python\",\n      \"args\": [\"-m\", \"pubchem-mcp-server\"]\n      }\n  }\n}\n```\n\n(Windows version):\n\n```json\n{\n  \"mcpServers\": {\n    \"pubchem\": {\n      \"command\": \"C:\\\\Users\\\\YOUR_USERNAME\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\python.exe\",\n      \"args\": [\n        \"-m\",\n        \"pubchem-mcp-server\"\n      ]\n    }\n  }\n}\n```\nUsing with Cline\n```json\n{\n  \"mcpServers\": {\n    \"pubchem\": {\n      \"command\": \"bash\",\n      \"args\": [\n        \"-c\",\n        \"source /home/YOUR/PATH/mcp-hub/PubChem-MCP-Server/.venv/bin/activate && python /home/YOUR/PATH/mcp-hub/PubChem-MCP-Server/pubchem_server.py\"\n      ],\n      \"env\": {},\n      \"disabled\": false,\n      \"autoApprove\": []\n    }\n  }\n}\n```\n\nAfter restarting Claude Desktop, the following capabilities will be available:\n\n### Searching Compounds\n\nYou can ask Claude to search for chemical compounds using queries like:\n```\nCan you search PubChem for information about aspirin?\n```\n\nThe search will return basic information about matching compounds including:\n\n• Compound name\n\n• CID (PubChem Compound ID)\n\n• Molecular formula\n\n• Molecular weight\n\n### Getting Compound Details\n\nOnce you have a CID, you can ask for more details:\n```\nCan you show me the details for compound with CID 2244?\n```\n\nThis will return:\n\n• IUPAC name\n\n• Molecular formula\n\n• Molecular weight\n\n• SMILES notation\n\n• InChI and InChIKey\n\n• Physical and chemical properties\n\n• Synonyms\n\n## 📝 TODO\n\n### visualize_compound\n\nGenerate and display a 2D or 3D visualization of a chemical compound.\n\n### compare_compounds\n\nCompare properties and structures of multiple compounds.\n\n### save_compound\n\nSave a compound locally for faster access.\n\n### list_saved_compounds\n\nList all saved compounds.\n\n### 📝 Chemistry Prompts\n\nThe server will offer specialized prompts to help analyze chemical compounds:\n\n#### Compound Analysis Prompt\n\nA comprehensive workflow for analyzing chemical compounds that only requires a compound ID:\n\n```python\nresult = await call_prompt(\"deep-compound-analysis\", {\n    \"compound_id\": \"2244\"\n})\n```\n\nThis prompt will include:\n\n- Detailed instructions for using available tools\n- A systematic workflow for compound analysis\n- Comprehensive analysis structure covering:\n  - Chemical structure and properties\n  - Pharmacological properties\n  - Biological activities\n  - Applications and uses\n  - Safety and toxicity information\n  - Related compounds\n\n## 📁 Project Structure\n\n- `pubchem_server.py`: The main MCP server implementation using FastMCP\n- `pubchem_search.py`: Contains example code for searching PubChem\n\n## 🔧 Dependencies\n\n- Python 3.10+\n- FastMCP\n- asyncio\n- logging\n- pubchempy (for PubChem API access)\n- pandas (for data handling)\n\nYou can install the required dependencies using:\n\n```bash\npip install mcp pubchempy pandas\n```\n\n## 🤝 Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n\n## 📄 License\n\nThis project is licensed under the MIT License.\n\n## ⚠️ Disclaimer\n\nThis tool is for research purposes only. Please respect PubChem's terms of service and use this tool responsibly.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "pubchem",
        "molecular",
        "compounds",
        "compounds pubchem",
        "pubchem mcp",
        "pubchem including"
      ],
      "category": "scientific-research-tools"
    },
    "JackKuo666--PubMed-MCP-Server": {
      "owner": "JackKuo666",
      "name": "PubMed-MCP-Server",
      "url": "https://github.com/JackKuo666/PubMed-MCP-Server",
      "imageUrl": "/freedevtools/mcp/pfp/JackKuo666.webp",
      "description": "Connects AI models to PubMed for searching, accessing, and analyzing biomedical articles, allowing retrieval of detailed metadata and deep analysis of research papers. Facilitates access to full-text PDFs and supports specialized prompts for comprehensive paper examination.",
      "stars": 64,
      "forks": 26,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-29T17:15:22Z",
      "readme_content": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/jackkuo666-pubmed-mcp-server-badge.png)](https://mseep.ai/app/jackkuo666-pubmed-mcp-server)\n\n# PubMed MCP Server\n\n[![smithery badge](https://smithery.ai/badge/@JackKuo666/pubmed-mcp-server)](https://smithery.ai/server/@JackKuo666/pubmed-mcp-server)\n\n🔍 Enable AI assistants to search, access, and analyze PubMed articles through a simple MCP interface.\n\nThe PubMed MCP Server provides a bridge between AI assistants and PubMed's vast repository of biomedical literature through the Model Context Protocol (MCP). It allows AI models to search for scientific articles, access their metadata, and perform deep analysis in a programmatic way.\n\n🤝 Contribute • 📝 Report Bug\n\n## ✨ Core Features\n- 🔎 Paper Search: Query PubMed articles with keywords or advanced search ✅\n- 🚀 Efficient Retrieval: Fast access to paper metadata ✅\n- 📊 Metadata Access: Retrieve detailed metadata for specific papers ✅\n- 📊 Research Support: Facilitate biomedical sciences research and analysis ✅\n- 📄 Paper Access: Attempt to download full-text PDF content ✅\n- 🧠 Deep Analysis: Perform comprehensive analysis of papers ✅\n- 📝 Research Prompts: A set of specialized prompts for paper analysis ✅\n\n## 🚀 Quick Start\n\n### Prerequisites\n\n- Python 3.10+\n- FastMCP library\n\n### Installation\n### Installing via Smithery\n\nTo install pubmed-mcp-server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@JackKuo666/pubmed-mcp-server):\n\n#### claude\n\n```bash\nnpx -y @smithery/cli install @JackKuo666/pubmed-mcp-server --client claude\n```\n\n#### Cursor\n\nPaste the following into Settings → Cursor Settings → MCP → Add new server: \n- Mac/Linux  \n```s\nnpx -y @smithery/cli@latest run @JackKuo666/pubmed-mcp-server --client cursor --config \"{}\" \n```\n#### Windsurf\n```sh\nnpx -y @smithery/cli@latest install @JackKuo666/pubmed-mcp-server --client windsurf --config \"{}\"\n```\n### CLine\n```sh\nnpx -y @smithery/cli@latest install @JackKuo666/pubmed-mcp-server --client cline --config \"{}\"\n```\n\n1. Clone the repository:\n   ```\n   git clone https://github.com/JackKuo666/PubMed-MCP-Server.git\n   cd PubMed-MCP-Server\n   ```\n\n2. Install the required dependencies:\n   ```\n   pip install -r requirements.txt\n   ```\n\n## 📊 Usage\n\nStart the MCP server:\n\n```bash\npython pubmed_server.py\n```\n## Usage with Claude Desktop\n\nAdd this configuration to your `claude_desktop_config.json`:\n\n(Mac OS)\n\n```json\n{\n  \"mcpServers\": {\n    \"pubmed\": {\n      \"command\": \"python\",\n      \"args\": [\"-m\", \"pubmed-mcp-server\"]\n      }\n  }\n}\n```\n\n(Windows version):\n\n```json\n{\n  \"mcpServers\": {\n    \"pubmed\": {\n      \"command\": \"C:\\\\Users\\\\YOUR\\\\PATH\\\\miniconda3\\\\envs\\\\mcp_server\\\\python.exe\",\n      \"args\": [\n        \"D:\\\\code\\\\YOUR\\\\PATH\\\\PubMed-MCP-Server\\\\pubmed_server.py\"\n      ],\n      \"env\": {},\n      \"disabled\": false,\n      \"autoApprove\": []\n    }\n  }\n}\n```\nUsing with Cline\n```json\n{\n  \"mcpServers\": {\n    \"pubmed\": {\n      \"command\": \"bash\",\n      \"args\": [\n        \"-c\",\n        \"source /home/YOUR/PATH/mcp-server-pubmed/.venv/bin/activate && python /home/YOUR/PATH/pubmed-mcp-server.py\"\n      ],\n      \"env\": {},\n      \"disabled\": false,\n      \"autoApprove\": []\n    }\n  }\n}\n```\n\n## 🛠 MCP Tools\n\nThe PubMed MCP Server provides the following tools:\n\n1. `search_pubmed_key_words`: Search for articles on PubMed using keywords.\n2. `search_pubmed_advanced`: Perform an advanced search for articles on PubMed with multiple parameters.\n3. `get_pubmed_article_metadata`: Fetch metadata for a PubMed article using its PMID.\n4. `download_pubmed_pdf`: Attempt to download the full-text PDF for a PubMed article.\n5. `deep_paper_analysis`: Perform a comprehensive analysis of a PubMed article.\n\n### Searching Papers\n\nYou can ask the AI assistant to search for papers using queries like:\n```\nCan you search PubMed for recent papers about CRISPR?\n```\n\n### Getting Paper Details\n\nOnce you have a PMID, you can ask for more details:\n```\nCan you show me the metadata for the paper with PMID 12345678?\n```\n\n### Analyzing Papers\n\nYou can request a deep analysis of a paper:\n```\nCan you perform a deep analysis of the paper with PMID 12345678?\n```\n\n## 📁 Project Structure\n\n- `pubmed_server.py`: The main MCP server implementation using FastMCP\n- `pubmed_web_search.py`: Contains the logic for searching PubMed and retrieving article information\n\n## 🔧 Dependencies\n\n- Python 3.10+\n- FastMCP\n- asyncio\n- logging\n- requests\n- beautifulsoup4\n\n## 🤝 Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n\n## 📄 License\n\nThis project is licensed under the MIT License.\n\n## ⚠️ Disclaimer\n\nThis tool is for research purposes only. Please respect PubMed's terms of service and use this tool responsibly.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "pubmed",
        "biomedical",
        "papers",
        "pubmed searching",
        "models pubmed",
        "pubmed mcp"
      ],
      "category": "scientific-research-tools"
    },
    "JackKuo666--PubTator-MCP-Server": {
      "owner": "JackKuo666",
      "name": "PubTator-MCP-Server",
      "url": "https://github.com/JackKuo666/PubTator-MCP-Server",
      "imageUrl": "/freedevtools/mcp/pfp/JackKuo666.webp",
      "description": "Access and analyze biomedical literature with advanced annotation and relationship mining capabilities. Programmatically search and export literature data through a user-friendly interface.",
      "stars": 8,
      "forks": 0,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-24T13:18:53Z",
      "readme_content": "# PubTator MCP Server\n[![smithery badge](https://smithery.ai/badge/@JackKuo666/pubtator-mcp-server)](https://smithery.ai/server/@JackKuo666/pubtator-mcp-server)\n\n🔍 A biomedical literature annotation and relationship mining server based on PubTator3, providing convenient access through the MCP interface.\n\nPubTator MCP Server provides AI assistants with access to the PubTator3 biomedical literature annotation system through the Model Context Protocol (MCP). It allows AI models to programmatically search scientific literature, obtain annotation information, and analyze entity relationships.\n\n🤝 Contribute • 📝 Report Issues\n\n## ✨ Core Features\n- 🔎 Literature Annotation Export: Support exporting PubTator annotation results in multiple formats ✅\n- 🚀 Entity ID Lookup: Query standard identifiers for biological concepts through free text ✅\n- 📊 Relationship Mining: Discover biomedical relationships between entities ✅\n- 📄 Literature Search: Support literature retrieval by keywords and entity IDs ✅\n- 🧠 Batch Processing: Support batch export of annotation information from search results ✅\n\n## 🚀 Quick Start\n\n### Requirements\n\n- Python 3.10+\n- FastMCP library\n\n### Installation\n\n#### Via Smithery\n\nUse [Smithery](https://smithery.ai/server/@JackKuo666/pubtator-mcp-server) to automatically install PubTator Server:\n\n##### Claude\n\n```sh\nnpx -y @smithery/cli@latest install @JackKuo666/pubtator-mcp-server --client claude --config \"{}\"\n```\n\n##### Cursor\n\nPaste in Settings → Cursor Settings → MCP → Add new server:\n- Mac/Linux  \n```s\nnpx -y @smithery/cli@latest run @JackKuo666/pubtator-mcp-server --client cursor --config \"{}\" \n```\n\n##### Windsurf\n```sh\nnpx -y @smithery/cli@latest install @JackKuo666/pubtator-mcp-server --client windsurf --config \"{}\"\n```\n\n##### CLine\n```sh\nnpx -y @smithery/cli@latest install @JackKuo666/pubtator-mcp-server --client cline --config \"{}\"\n```\n\n#### Manual Installation\n\n1. Clone the repository:\n   ```\n   git clone https://github.com/JackKuo666/PubTator-MCP-Server.git\n   cd PubTator-MCP-Server\n   ```\n\n2. Install dependencies:\n   ```\n   pip install -r requirements.txt\n   ```\n\n## 📊 Usage\n\n### Running the Server Directly\n\nStart the MCP server:\n\n```bash\npython pubtator_server.py\n```\n\nThe server now supports both stdio and TCP transports. By default, it uses TCP transport. You can configure the following environment variables:\n\n- `MCP_TRANSPORT`: Set to \"tcp\" for TCP transport or \"stdio\" for stdio transport (default is \"tcp\")\n- `MCP_HOST`: The host to bind to (default is \"0.0.0.0\")\n- `MCP_PORT`: The port to listen on (default is 8080)\n\nExample of starting the server with custom settings:\n\n```bash\nMCP_TRANSPORT=tcp MCP_HOST=127.0.0.1 MCP_PORT=8888 python pubtator_server.py\n```\n\nThe server implements lazy initialization and proper error handling. It will gracefully handle shutdown signals (SIGINT and SIGTERM) and log any errors that occur during startup or operation.\n\n### Using Docker\n\nWe provide a Dockerfile for easy deployment. To use the Docker container:\n\n1. Build the Docker image:\n   ```bash\n   docker build -t pubtator-mcp-server .\n   ```\n\n2. Run the Docker container:\n   ```bash\n   docker run -p 8080:8080 pubtator-mcp-server\n   ```\n\nThis will start the PubTator MCP server inside a Docker container, exposing it on port 8080.\n\n### Troubleshooting\n\nIf you encounter any issues starting the server:\n\n1. Check the console output for error messages.\n2. Ensure all required dependencies are installed (see Requirements section).\n3. Verify that the environment variables are set correctly.\n4. If the server fails to start, try running it with increased verbosity:\n\n```bash\npython -v pubtator_server.py\n```\n\nThis will provide more detailed logging information to help identify the source of any issues.\n\nWhen using Docker, you can check the logs with:\n\n```bash\ndocker logs <container_id>\n```\n\n### Configuration\n\n#### Claude Desktop Configuration\n\nAdd to `claude_desktop_config.json`:\n\n(Mac OS)\n\n```json\n{\n  \"mcpServers\": {\n    \"pubtator\": {\n      \"command\": \"python\",\n      \"args\": [\"-m\", \"pubtator-mcp-server\"]\n      }\n  }\n}\n```\n\n(Windows)\n\n```json\n{\n  \"mcpServers\": {\n    \"pubtator\": {\n      \"command\": \"C:\\\\Users\\\\YOUR\\\\PATH\\\\miniconda3\\\\envs\\\\mcp_server\\\\python.exe\",\n      \"args\": [\n        \"D:\\\\code\\\\YOUR\\\\PATH\\\\PubTator-MCP-Server\\\\pubtator_server.py\"\n      ],\n      \"env\": {},\n      \"disabled\": false,\n      \"autoApprove\": []\n    }\n  }\n}\n```\n\n#### CLine Configuration\n```json\n{\n  \"mcpServers\": {\n    \"pubtator\": {\n      \"command\": \"bash\",\n      \"args\": [\n        \"-c\",\n        \"source /home/YOUR/PATH/mcp-server-pubtator/.venv/bin/activate && python /home/YOUR/PATH/pubtator_server.py\"\n      ],\n      \"env\": {\n        \"MCP_TRANSPORT\": \"stdio\"\n      },\n      \"disabled\": false,\n      \"autoApprove\": []\n    }\n  }\n}\n```\n\nTo use TCP transport, modify the configuration as follows:\n\n```json\n{\n  \"mcpServers\": {\n    \"pubtator\": {\n      \"command\": \"bash\",\n      \"args\": [\n        \"-c\",\n        \"source /home/YOUR/PATH/mcp-server-pubtator/.venv/bin/activate && python /home/YOUR/PATH/pubtator_server.py\"\n      ],\n      \"env\": {\n        \"MCP_TRANSPORT\": \"tcp\",\n        \"MCP_HOST\": \"127.0.0.1\",\n        \"MCP_PORT\": \"8888\"\n      },\n      \"disabled\": false,\n      \"autoApprove\": []\n    }\n  }\n}\n```\n\n## 🛠 API Features\n\nPubTator MCP Server provides the following core features:\n\n### 1. Export Publications (export_publications)\n\nExport PubTator annotation results for specified PMID literature:\n```python\nresult = await export_publications(\n    ids=[\"32133824\", \"34170578\"],\n    id_type=\"pmid\",\n    format=\"biocjson\",  # Supported: pubtator, biocxml, biocjson\n    full_text=False     # Whether to include full text\n)\n```\n\n### 2. Entity ID Lookup (find_entity_id)\n\nQuery standard identifiers for biological concepts through free text:\n```python\nresult = await find_entity_id(\n    query=\"COVID-19\",\n    concept=\"disease\",  # Optional: gene, disease, chemical, species, mutation\n    limit=5             # Optional: limit number of results\n)\n```\n\n### 3. Relationship Query (find_related_entities)\n\nFind other entities related to a specified entity:\n```python\nresult = await find_related_entities(\n    entity_id=\"@DISEASE_COVID_19\",\n    relation_type=\"treat\",    # Optional: treat, cause, interact, etc.\n    target_entity_type=\"chemical\",  # Optional: gene, disease, chemical\n    max_results=5       # Optional: limit number of results\n)\n```\n\n### 4. Literature Search (search_pubtator)\n\nSearch the PubTator database:\n```python\nresults = await search_pubtator(\n    query=\"COVID-19\",\n    max_pages=1     # Optional: maximum number of pages to retrieve\n)\n```\n\n### 5. Batch Export (batch_export_from_search)\n\nSearch and batch export literature annotations:\n```python\nresults = await batch_export_from_search(\n    query=\"COVID-19\",\n    format=\"biocjson\",\n    max_pages=1,\n    full_text=False,\n    batch_size=5\n)\n```\n\nNote: The actual function calls may vary depending on your implementation. These examples are based on our recent tests and may need to be adjusted to match your exact API.\n\n## ⚠️ Usage Limitations\n\n- API request rate limit: maximum 3 requests per second\n- When batch exporting, use a reasonable batch_size to avoid request timeout\n- For relationship queries, entity IDs must start with \"@\", e.g., \"@DISEASE_COVID-19\"\n\n## 📄 License\n\nThis project is licensed under the MIT License.\n\n## ⚠️ Disclaimer\n\nThis tool is for research purposes only. Please comply with PubTator's terms of service and use this tool responsibly.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "biomedical",
        "annotation",
        "literature",
        "biomedical literature",
        "analyze biomedical",
        "literature data"
      ],
      "category": "scientific-research-tools"
    },
    "JackKuo666--Sci-Hub-MCP-Server": {
      "owner": "JackKuo666",
      "name": "Sci-Hub-MCP-Server",
      "url": "https://github.com/JackKuo666/Sci-Hub-MCP-Server",
      "imageUrl": "/freedevtools/mcp/pfp/JackKuo666.webp",
      "description": "Connects to Sci-Hub for searching, accessing, and analyzing academic papers. Supports retrieving metadata and downloading full-text PDFs programmatically.",
      "stars": 35,
      "forks": 13,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-09-13T00:11:18Z",
      "readme_content": "# Sci-Hub MCP Server\n\n[![smithery badge](https://smithery.ai/badge/@JackKuo666/sci-hub-mcp-server)](https://smithery.ai/server/@JackKuo666/sci-hub-mcp-server)\n\n🔍 Enable AI assistants to search, access, and analyze academic papers through Sci-Hub using a simple MCP interface.\n\nThe Sci-Hub MCP Server provides a bridge between AI assistants and Sci-Hub's repository of academic literature through the Model Context Protocol (MCP). It allows AI models to search for scientific articles by DOI, title, or keywords, access their metadata, and download PDFs in a programmatic way.\n\n## ✨ Core Features\n\n- 🔎 Paper Search by DOI: Find papers using their Digital Object Identifier ✅\n- 🔍 Paper Search by Title: Locate papers using their full or partial title ✅\n- 🔑 Paper Search by Keyword: Discover papers related to specific research areas ✅\n- 📊 Metadata Access: Retrieve detailed metadata for specific papers ✅\n- 📄 PDF Download: Download full-text PDF content when available ✅\n\n## 🚀 Quick Start\n\n### Prerequisites\n\n- Python 3.10+\n- FastMCP library\n\n### Installation\n\n1. Clone the repository:\n   ```\n   git clone https://github.com/JackKuo666/Sci-Hub-MCP-Server.git\n   cd Sci-Hub-MCP-Server\n   ```\n\n2. Install the required dependencies:\n   ```\n   pip install -r requirements.txt\n   ```\n\n## 📊 Usage\n\nStart the MCP server:\n\n```bash\npython sci_hub_server.py\n```\n\n## Usage with Claude Desktop\n\nAdd this configuration to your `claude_desktop_config.json`:\n\n(Mac OS)\n\n```json\n{\n  \"mcpServers\": {\n    \"scihub\": {\n      \"command\": \"python\",\n      \"args\": [\"-m\", \"sci_hub_server.py\"]\n      }\n  }\n}\n```\n\n(Windows version):\n\n```json\n{\n  \"mcpServers\": {\n    \"scihub\": {\n      \"command\": \"C:\\\\Users\\\\YOUR\\\\PATH\\\\miniconda3\\\\envs\\\\mcp_server\\\\python.exe\",\n      \"args\": [\n        \"D:\\\\code\\\\YOUR\\\\PATH\\\\Sci-Hub-MCP-Server\\\\sci_hub_server.py\"\n      ],\n      \"env\": {},\n      \"disabled\": false,\n      \"autoApprove\": []\n    }\n  }\n}\n```\n\n## 🛠 MCP Tools\n\nThe Sci-Hub MCP Server provides the following tools:\n\n1. `search_scihub_by_doi`: Search for a paper on Sci-Hub using its DOI (Digital Object Identifier).\n2. `search_scihub_by_title`: Search for a paper on Sci-Hub using its title.\n3. `search_scihub_by_keyword`: Search for papers on Sci-Hub using a keyword.\n4. `download_scihub_pdf`: Download a paper PDF from Sci-Hub.\n5. `get_paper_metadata`: Get metadata information for a paper using its DOI.\n\n### Searching Papers by DOI\n\nYou can ask the AI assistant to search for papers using DOI:\n```\nCan you search Sci-Hub for the paper with DOI 10.1038/nature09492?\n```\n\n### Searching Papers by Title\n\nYou can search for papers using their title:\n```\nCan you find the paper titled \"Choosing Assessment Instruments for Posttraumatic Stress Disorder Screening and Outcome Research\" on Sci-Hub?\n```\n\n### Searching Papers by Keyword\n\nYou can search for papers related to specific keywords:\n```\nCan you search Sci-Hub for recent papers about artificial intelligence in medicine?\n```\n\n### Downloading Papers\n\nOnce you have found a paper, you can download it:\n```\nCan you download the PDF for this paper to my_paper.pdf?\n```\n\n### Getting Paper Metadata\n\nYou can request metadata for a paper using its DOI:\n```\nCan you show me the metadata for the paper with DOI 10.1038/nature09492?\n```\n\n## 📁 Project Structure\n\n- `sci_hub_server.py`: The main MCP server implementation using FastMCP\n- `sci_hub_search.py`: Contains the logic for searching Sci-Hub and retrieving paper information\n\n## 🔧 Dependencies\n\n- Python 3.10+\n- FastMCP\n- requests\n- bs4\n- scihub\n\n## 🤝 Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n\n## 📄 License\n\nThis project is licensed under the MIT License.\n\n## ⚠️ Disclaimer\n\nThis tool is for research purposes only. Please respect copyright laws and use this tool responsibly. The authors do not endorse or encourage any copyright infringement.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "pdfs",
        "sci",
        "metadata",
        "research tools",
        "sci hub",
        "metadata downloading"
      ],
      "category": "scientific-research-tools"
    },
    "JackKuo666--bioRxiv-MCP-Server": {
      "owner": "JackKuo666",
      "name": "bioRxiv-MCP-Server",
      "url": "https://github.com/JackKuo666/bioRxiv-MCP-Server",
      "imageUrl": "/freedevtools/mcp/pfp/JackKuo666.webp",
      "description": "Search for bioRxiv papers, retrieve detailed metadata, and download papers to support biological sciences research and analysis.",
      "stars": 20,
      "forks": 6,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-09-04T18:06:12Z",
      "readme_content": "# bioRxiv MCP Server\n\n🔍 Enable AI assistants to search and access bioRxiv papers through a simple MCP interface.\n\nThe bioRxiv MCP Server provides a bridge between AI assistants and bioRxiv's preprint repository through the Model Context Protocol (MCP). It allows AI models to search for biology preprints and access their metadata in a programmatic way.\n\n🤝 Contribute • 📝 Report Bug\n\n## ✨ Core Features\n- 🔎 Paper Search: Query bioRxiv papers with keywords or advanced search ✅\n- 🚀 Efficient Retrieval: Fast access to paper metadata ✅\n- 📊 Metadata Access: Retrieve detailed metadata for specific papers ✅\n- 📊 Research Support: Facilitate biological sciences research and analysis ✅\n- 📄 Paper Access: Download and read paper content 📝\n- 📋 Paper Listing: View all downloaded papers 📝\n- 🗃️ Local Storage: Papers are saved locally for faster access 📝\n- 📝 Research Prompts: A set of specialized prompts for paper analysis 📝\n\n## 🚀 Quick Start\n\n### Prerequisites\n\n- Python 3.10+\n- FastMCP library\n\n### Installation\n\n1. Clone the repository:\n   ```\n   git clone https://github.com/JackKuo666/bioRxiv-MCP-Server.git\n   cd bioRxiv-MCP-Server\n   ```\n\n2. Install the required dependencies:\n   ```\n   pip install -r requirements.txt\n   ```\n\n### Installing via Smithery\n\nTo install bioRxiv Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@JackKuo666/biorxiv-mcp-server):\n\n#### claude\n\n```bash\nnpx -y @smithery/cli@latest install @JackKuo666/biorxiv-mcp-server --client claude --config \"{}\"\n```\n\n#### Cursor\n\nPaste the following into Settings → Cursor Settings → MCP → Add new server: \n- Mac/Linux  \n```s\nnpx -y @smithery/cli@latest run @JackKuo666/biorxiv-mcp-server --client cursor --config \"{}\" \n```\n#### Windsurf\n```sh\nnpx -y @smithery/cli@latest install @JackKuo666/biorxiv-mcp-server --client windsurf --config \"{}\"\n```\n#### CLine\n```sh\nnpx -y @smithery/cli@latest install @JackKuo666/biorxiv-mcp-server --client cline --config \"{}\"\n```\n\n#### Usage with Claude Desktop\n\nAdd this configuration to your `claude_desktop_config.json`:\n\n(Mac OS)\n\n```json\n{\n  \"mcpServers\": {\n    \"biorxiv\": {\n      \"command\": \"python\",\n      \"args\": [\"-m\", \"biorxiv-mcp-server\"]\n      }\n  }\n}\n```\n\n(Windows version):\n\n```json\n{\n  \"mcpServers\": {\n    \"biorxiv\": {\n      \"command\": \"C:\\\\Users\\\\YOUR_USERNAME\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\python.exe\",\n      \"args\": [\n        \"-m\",\n        \"biorxiv-mcp-server\"\n      ]\n    }\n  }\n}\n```\nUsing with Cline\n```json\n{\n  \"mcpServers\": {\n    \"biorxiv\": {\n      \"command\": \"bash\",\n      \"args\": [\n        \"-c\",\n        \"source /home/YOUR/PATH/mcp-server-bioRxiv/.venv/bin/activate && python /home/YOUR/PATH/mcp-server-bioRxiv/biorxiv_server.py\"\n      ],\n      \"env\": {},\n      \"disabled\": false,\n      \"autoApprove\": []\n    }\n  }\n}\n```\n\n\n## 📊 Usage\n\nStart the MCP server:\n\n```bash\npython biorxiv_server.py\n```\n\n## 🛠 MCP Tools\n\nThe bioRxiv MCP Server provides the following tools:\n\n1. `search_biorxiv_key_words`: Search for articles on bioRxiv using keywords.\n2. `search_biorxiv_advanced`: Perform an advanced search for articles on bioRxiv with multiple parameters.\n3. `get_biorxiv_metadata`: Fetch metadata for a bioRxiv article using its DOI.\n\n### Searching Papers\n\nYou can ask the AI assistant to search for papers using queries like:\n```\nCan you search bioRxiv for recent papers about genomics?\n```\n\n### Getting Paper Details\n\nOnce you have a DOI, you can ask for more details:\n```\nCan you show me the metadata for the paper with DOI 10.1101/123456?\n```\n\n## 📁 Project Structure\n\n- `biorxiv_server.py`: The main MCP server implementation using FastMCP\n- `biorxiv_web_search.py`: Contains the web scraping logic for searching bioRxiv\n\n## 🔧 Dependencies\n\n- Python 3.10+\n- FastMCP\n- asyncio\n- logging\n\n## 🤝 Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n\n## 📄 License\n\nThis project is licensed under the MIT License.\n\n## ⚠️ Disclaimer\n\nThis tool is for research purposes only. Please respect bioRxiv's terms of service and use this tool responsibly.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "biorxiv",
        "biological",
        "scientific",
        "biorxiv papers",
        "search biorxiv",
        "jackkuo666 biorxiv"
      ],
      "category": "scientific-research-tools"
    },
    "JotaDeRodriguez--Bonsai_mcp": {
      "owner": "JotaDeRodriguez",
      "name": "Bonsai_mcp",
      "url": "https://github.com/JotaDeRodriguez/Bonsai_mcp",
      "imageUrl": "/freedevtools/mcp/pfp/JotaDeRodriguez.webp",
      "description": "Connects Blender with Claude to analyze and manipulate IFC models, enabling querying of project information, exploration of spatial structures, and execution of Python code within Blender.",
      "stars": 26,
      "forks": 6,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-10-02T21:36:13Z",
      "readme_content": "\n# Bonsai-mcp - Model Context Protocol Integration for IFC through IfcOpenShell and Blender\n\nBonsai-mcp is a fork of [BlenderMCP](https://github.com/ahujasid/blender-mcp) that extends the original functionality with dedicated support for IFC (Industry Foundation Classes) models through Bonsai. This integration is a platform to let LLMs read and modify IFC files.\n\n## Features\n\n- **IFC-specific functionality**: Query IFC models, analyze spatial structures, examine building elements and extract quantities\n\n- **Eleven IFC tools included**: Inspect project info, list entities, examine properties, explore spatial structure, analyze relationships and more\n\n- **Sequential Thinking**: Includes the sequential thinking tool from [modelcontextprotocol/servers](https://github.com/modelcontextprotocol/servers/tree/main/src/sequentialthinking) for structured problem solving\n\n- **Execute Code tool from the original BlenderMCP implementation**: Create and modify objects, apply materials, and execute Python code in Blender\n\n## Components\n\nThe system consists of two main components:\n\n1. **Blender Addon (`addon.py`)**: A Blender addon that creates a socket server within Blender to receive and execute commands, including IFC-specific operations\n\n2. **MCP Server (`tools.py`)**: A Python server that implements the Model Context Protocol and connects to the Blender addon\n\n## Installation - Through MCP Client Settings\n\n### Prerequisites\n\n- Blender 4.0 or newer\n\n- Python 3.12 or newer\n\n- uv package manager\n\n- Bonsai BIM addon for Blender (for IFC functionality)\n\n**Installing uv:**\n\n**Mac:**\n\n```bash\n\nbrew  install  uv\n\n```\n\n**Windows:**\n\n```bash\n\npowershell  -c  \"irm https://astral.sh/uv/install.ps1 | iex\"\n\nset  Path=C:\\Users\\[username]\\.local\\bin;%Path%\n\n```\n\nFor other platforms, see the [uv installation guide](https://docs.astral.sh/uv/getting-started/installation/).\n\n### Clone the repository\n\n```bash\n\ngit  clone  https://github.com/JotaDeRodriguez/Bonsai_mcp\n\n```\n\n### Claude for Desktop Integration\n\nEdit your `claude_desktop_config.json` file (Claude > Settings > Developer > Edit Config) to include:\n\n```json\n{\n    \"mcpServers\": {\n        \"Bonsai-mcp\": {\n            \"command\": \"uv\",\n            \"args\": [\n              \"--directory\",\n              \"\\\\your\\\\path\\\\to\\\\Bonsai_mcp\",\n              \"run\",\n              \"tools.py\"\n          ]\n        }\n    }\n}\n\n```\n\n## Installation via Docker\n\nThe repository comes with a Dockerfile that makes deployment simple and consistent across different environments.\n\n## Quick Start\n\n```bash\n# Clone the repository\ngit clone https://github.com/JotaDeRodriguez/Bonsai_mcp\ncd Bonsai_mcp\n\n# Build the Docker image\ndocker build -t bonsai_mcp .\n\n# Run the container\ndocker run -p 8000:8000 --name bonsai_mcp bonsai_mcp\n```\n\nOnce running, the container will expose the MCP tools as REST/OpenAPI APIs at `http://localhost:8000`.\n\n- To verify youtr installation, open your browser and navigate to\n- `http://localhost:8000/docs`\n- You'll see the Swagger UI with all available endpoints\n- Test an endpoint by clicking on it, then click \"Try it out\" and \"Execute\"\n\n### Connecting to Open WebUI or Other API Clients\n\nTo connect this API to Open WebUI:\n\n1. In Open WebUI, go to Settings > Manage Tool Servers\n2. Add a new connection with:\n\n- URL: `http://localhost:8000`\n- Path to OpenAPI spec: `/openapi.json`\n- Authentication: None (unless configured otherwise)\n\n### Environment Variables\n\nThe Docker container accepts several environment variables to customize its behavior:\n\n```bash\n# Example with custom settings\ndocker run -p 8000:8000 \\\n  -e BLENDER_HOST=host.docker.internal \\\n  -e BLENDER_PORT=9876 \\\n  -e MCP_HOST=0.0.0.0 \\\n  -e MCP_PORT=8000 \\\n  --name bonsai_mcp bonsai_mcp\n```\n\n## Installing the Blender Addon\n\n1. Download the `addon.py` file from this repo\n\n2. Open Blender\n\n3. Go to Edit > Preferences > Add-ons\n\n4. Click \"Install...\" and select the `addon.py` file\n\n5. Enable the addon by checking the box next to \"Interface: Blender MCP - IFC\"\n\n## Usage\n\n### Starting the Connection\n\n1. In Blender, go to the 3D View sidebar (press N if not visible)\n\n2. Find the \"Blender MCP - IFC\" tab\n\n3. Click \"Connect to Claude\"\n\n4. Make sure the MCP server is running\n\n### Using with Claude\n\nOnce connected, you'll see a hammer icon in Claude's interface with tools for the Blender MCP IFC integration.\n\n## IFC Tools\n\nThis repo includes multiple IFC-specific tools that enable comprehensive querying and manipulation of IFC models:\n\n**get_ifc_project_info**: Retrieves basic information about the IFC project, including name, description, and counts of different entity types. Example: \"What is the basic information about this IFC project?\"\n\n**list_ifc_entities**: Lists IFC entities of a specific type (walls, doors, spaces, etc.) with options to limit results and filter by selection. Example: \"List all the walls in this IFC model\" or \"Show me the windows in this building\"\n\n**get_ifc_properties**: Retrieves all properties of a specific IFC entity by its GlobalId or from currently selected objects. Example: \"What are the properties of this wall with ID 1Dvrgv7Tf5IfTEapMkwDQY?\"\n\n**get_ifc_spatial_structure**: Gets the spatial hierarchy of the IFC model (site, building, storeys, spaces). Example: \"Show me the spatial structure of this building\"\n\n**get_ifc_relationships**: Retrieves all relationships for a specific IFC entity. Example: \"What are the relationships of the entrance door?\"\n\n**get_selected_ifc_entities**: Gets information about IFC entities corresponding to objects currently selected in the Blender UI. Example: \"Tell me about the elements I've selected in Blender\"\n\n**get_user_view**: Captures the current Blender viewport as an image, allowing visualization of the model from the user's perspective. Example: \"Show me what the user is currently seeing in Blender\"\n\n**export_ifc_data**: Exports IFC data to a structured JSON or CSV file, with options to filter by entity type or building level. Example: \"Export all wall data to a CSV file\"\n\n**place_ifc_object**: Creates and positions an IFC element in the model at specified coordinates with optional rotation. Example: \"Place a door at coordinates X:10, Y:5, Z:0 with 90 degrees rotation\"\n\n**get_ifc_quantities**: Calculate and get quantities (m2, m3, etc.) for IFC elements, with options to filter by entity type or selected ones. Example: \"Give me the area of all the walls in the building using the tool get_ifc_quantities\"\n\n**export_drawing_png**: Exports 2D and 3D drawings as high-resolution PNG images with customizable resolution and view parameters. Creates orthographic plan views from above at specified height offsets. Example: \"Generate a floor plan PNG for the ground floor at 1920x1080 resolution\"\n\n**get_ifc_georeferencing_info**: Retrieves comprehensive georeferencing information from IFC files including coordinate reference systems (CRS), map conversions, world coordinate systems, true north direction, and site geographic coordinates. Example: \"What georeferencing information is available in this IFC model?\"\n\n**georeference_ifc_model**: Creates or updates georeferencing information in IFC models, allowing you to set coordinate reference systems using EPSG codes or custom CRS definitions, establish map conversions with eastings/northings coordinates, and configure site geographic positioning. Example: \"Georeference this IFC model using EPSG:4326 with coordinates at latitude 40.7589, longitude -73.9851\"\n\n## MCP Resources\n\nThis integration provides access to structured documentation through MCP resources:\n\n**file://table_of_contents.md**: Contains the complete technical report structure template for generating comprehensive building reports. This resource provides a standardized table of contents that can be used as a reference when creating technical documentation from IFC models.\n\n## MCP Prompts\n\nThe server includes specialized MCP Prompts for automated report generation:\n\n**Technical_building_report**: Generates comprehensive technical building reports based on IFC models loaded in Blender. This prompt provides a structured workflow for creating professional architectural documentation in multiple languages (English, Spanish, French, German, Italian, Portuguese). The prompt guides the analysis through systematic data extraction from the IFC model, including spatial structure, quantities, materials, and building systems, culminating in a complete technical report with drawings and 3D visualizations.\n\n## Execute Blender Code\n\nLegacy feature from the original MCP implementation. Allows Claude to execute arbitrary Python code in Blender. Use with caution.\n\n## Sequential Thinking Tool\n\nThis integration includes the Sequential Thinking tool for structured problem-solving and analysis. It facilitates a step-by-step thinking process that can branch, revise, and adapt as understanding deepens - perfect for complex IFC model analysis or planning tasks.\n\nExample: \"Use sequential thinking to analyze this building's energy efficiency based on the IFC model\"\n\n## Example Commands\n\nHere are some examples of what you can ask Claude to do with IFC models:\n\n- \"Analyze this IFC model and tell me how many walls, doors and windows it has\"\n\n- \"Show me the spatial structure of this building model\"\n\n- \"List all spaces in this IFC model and their properties\"\n\n- \"Identify all structural elements in this building\"\n\n- \"What are the relationships between this wall and other elements?\"\n\n- \"Generate a report of the measurements from the IFC model opened in Blender\"\n\n- \"Use sequential thinking to create a maintenance plan for this building based on the IFC model\"\n\n## Troubleshooting\n\n- **Connection issues**: Make sure the Blender addon server is running, and the MCP server is configured in Claude\n- **IFC model not loading**: Verify that you have the Bonsai BIM addon installed and that an IFC file is loaded\n- **Timeout errors**: Try simplifying your requests or breaking them into smaller steps\n\n**Docker:**\n\n- **\"Connection refused\" errors**: Make sure Blender is running and the addon is enabled with the server started\n- **CORS issues**: The API has CORS enabled by default for all origins. If you encounter issues, check your client's CORS settings\n- **Performance concerns**: For large IFC models, the API responses might be slower. Consider adjusting timeouts in your client\n\n## Technical Details\n\nThe IFC integration uses the Bonsai BIM module to access ifcopenshell functionality within Blender. The communication follows the same JSON-based protocol over TCP sockets as the original BlenderMCP.\n\n## Limitations & Security Considerations\n\n- The `execute_blender_code` tool from the original project is still available, allowing running arbitrary Python code in Blender. Use with caution and always save your work.\n\n- Complex IFC models may require breaking down operations into smaller steps.\n\n- IFC query performance depends on model size and complexity.\n\n- Get User View tool returns a base64 encoded image. Please ensure the client supports it.\n\n## Contributions\n\nThis MIT licensed repo is open to be forked, modified and used in any way. I'm open to ideas and collaborations, so don't hesitate to get in contact with me for contributions.\n\n## Credits\n\n- Original BlenderMCP by [Siddharth Ahuja](https://github.com/ahujasid/blender-mcp)\n\n- Sequential Thinking tool from [modelcontextprotocol/servers](https://github.com/modelcontextprotocol/servers/tree/main/src/sequentialthinking)\n\n- IFC integration built upon the Bonsai BIM addon for Blender\n\n## TO DO\n\nIntegration and testing with more MCP Clients\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "python",
        "blender",
        "tools",
        "ifc models",
        "research tools",
        "code blender"
      ],
      "category": "scientific-research-tools"
    },
    "KangDeokWon-droid--Deep-Learning-": {
      "owner": "KangDeokWon-droid",
      "name": "Deep-Learning-",
      "url": "https://github.com/KangDeokWon-droid/Deep-Learning-",
      "imageUrl": "/freedevtools/mcp/pfp/KangDeokWon-droid.webp",
      "description": "Provides tools and resources for training and experimenting with artificial intelligence models. Facilitates model development and evaluation with integrated datasets and utilities.",
      "stars": 0,
      "forks": 0,
      "license": "No License",
      "language": "Python",
      "updated_at": "2024-09-21T08:46:24Z",
      "readme_content": "# Deep-Learning\n인공지능 학습\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "droid",
        "datasets",
        "deep",
        "droid deep",
        "kangdeokwon droid",
        "deep learning"
      ],
      "category": "scientific-research-tools"
    },
    "MJBeauty--open_model_zoo": {
      "owner": "MJBeauty",
      "name": "open_model_zoo",
      "url": "https://github.com/MJBeauty/open_model_zoo",
      "imageUrl": "/freedevtools/mcp/pfp/MJBeauty.webp",
      "description": "A collection of optimized deep learning models and demos designed to facilitate the development of high-performance deep learning inference applications using pre-trained models. Offers integration and validation tools with automation scripts for rapid deployment.",
      "stars": 0,
      "forks": 0,
      "license": "Apache License 2.0",
      "language": "",
      "updated_at": "2020-07-16T05:29:44Z",
      "readme_content": "# [OpenVINO™ Toolkit](https://01.org/openvinotoolkit) - Open Model Zoo repository\n[![Stable release](https://img.shields.io/badge/version-2020.2-green.svg)](https://github.com/opencv/open_model_zoo/releases/tag/2020.2)\n[![Gitter chat](https://badges.gitter.im/gitterHQ/gitter.png)](https://gitter.im/open_model_zoo/community)\n[![Apache License Version 2.0](https://img.shields.io/badge/license-Apache_2.0-green.svg)](LICENSE)\n\nThis repository includes optimized deep learning models and a set of demos to expedite development of high-performance deep learning inference applications. Use these free pre-trained models instead of training your own models to speed-up the development and production deployment process.\n\n## Repository Components:\n* [Pre-Trained Models](models/intel/index.md)\n* [Public Models Description](models/public/index.md)\n* [Model Downloader](tools/downloader/README.md) and other automation tools\n* [Demos](demos/README.md) that demonstrate models usage with Deep Learning Deployment Toolkit\n* [Accuracy Checker](tools/accuracy_checker/README.md) tool for models accuracy validation\n\n## License\nOpen Model Zoo is licensed under [Apache License Version 2.0](LICENSE).\n\n## Documentation\n* [OpenVINO™ Release Notes](https://software.intel.com/en-us/articles/OpenVINO-RelNotes)\n* [Pre-Trained Models](https://software.intel.com/en-us/openvino-toolkit/documentation/pretrained-models)\n* [Demos and samples](https://software.intel.com/en-us/articles/OpenVINO-IE-Samples)\n\n## Other usage examples\n* [Open Visual Cloud](https://01.org/openvisualcloud)\n  * [Tutorial: Build and Run the AD Insertion Sample on public cloud or local machine](https://01.org/openvisualcloud/documents/tutorial-build-and-run-ad-insertion-sample-public-cloud-or-local-machine)\n  * [GitHub Repo for Ad Insertion Sample](https://github.com/OpenVisualCloud/Ad-Insertion-Sample)\n* [OpenVINO for Smart City](https://github.com/incluit/OpenVino-For-SmartCity)\n* [OpenVINO Driver Behavior](https://github.com/incluit/OpenVino-Driver-Behaviour)\n\n## How to Contribute\nWe welcome community contributions to the Open Model Zoo repository. If you have an idea how to improve the product, please share it with us doing the following steps:\n* Make sure you can build the product and run all the demos with your patch.\n* In case of a larger feature, provide a relevant demo.\n* Submit a pull request at https://github.com/opencv/open_model_zoo/pulls\n\nYou can find additional information about model contribution [here](CONTRIBUTING.md).\n\nWe will review your contribution and, if any additional fixes or modifications are needed, may give you feedback to guide you. When accepted, your pull request will be merged into the GitHub* repositories.\n\nOpen Model Zoo is licensed under Apache License, Version 2.0. By contributing to the project, you agree to the license and copyright terms therein and release your contribution under these terms.\n\n## Support\nPlease report questions, issues and suggestions using:\n* [\\#open_model_zoo](https://stackoverflow.com/search?q=%23open_model_zoo) tag on StackOverflow*\n* [GitHub* Issues](https://github.com/opencv/open_model_zoo/issues)\n* [Forum](https://software.intel.com/en-us/forums/intel-distribution-of-openvino-toolkit)\n* [Gitter](https://gitter.im/open_model_zoo/community)\n\n---\n\\* Other names and brands may be claimed as the property of others.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "open_model_zoo",
        "models",
        "deep",
        "mjbeauty open_model_zoo",
        "deep learning",
        "optimized deep"
      ],
      "category": "scientific-research-tools"
    },
    "MushroomFleet--DeepLucid3D-MCP": {
      "owner": "MushroomFleet",
      "name": "DeepLucid3D-MCP",
      "url": "https://github.com/MushroomFleet/DeepLucid3D-MCP",
      "imageUrl": "/freedevtools/mcp/pfp/MushroomFleet.webp",
      "description": "Provides advanced cognitive analysis and problem-solving capabilities using structured cognitive tools, enabling users to assess cognitive states, generate novel perspectives, and decompose complex problems for effective solutions.",
      "stars": 12,
      "forks": 3,
      "license": "No License",
      "language": "TypeScript",
      "updated_at": "2025-07-28T12:04:44Z",
      "readme_content": "# DeepLucid3D UCPF Server\n\nA Model Context Protocol (MCP) server implementing the Unified Cognitive Processing Framework (UCPF) for advanced cognitive analysis, creative problem-solving, and structured thinking.\n\n## What is it?\n\nThe DeepLucid3D UCPF Server is an implementation of the Unified Cognitive Processing Framework as an MCP server. It combines recursive self-awareness with dimensional knowledge categorization to provide a powerful approach to problem-solving and creative thinking.\n\nThis server extends AI capabilities by providing structured cognitive tools that help:\n- Assess cognitive states\n- Map knowledge dimensions\n- Apply recursive self-questioning\n- Generate creative perspectives\n- Decompose and reintegrate complex problems\n\n## What it does\n\nThe UCPF Server enables advanced cognitive processing through several key features:\n\n### Core Capabilities\n\n1. **Cognitive State Assessment**: Identifies current cognitive states (Dark Inertia, Passion, or Approaching Lucidity) to improve self-awareness during problem-solving.\n\n2. **Knowledge Dimension Mapping**: Maps knowledge across three dimensions:\n   - Awareness (Known vs. Unknown)\n   - Content (Knowns vs. Unknowns)\n   - Accessibility (Knowable vs. Unknowable)\n\n3. **Recursive Self-Questioning**: Challenges initial assumptions and identifies potential cognitive biases.\n\n4. **Creative Perspective Generation**: Produces novel viewpoints and metaphorical thinking to inspire new solutions.\n\n5. **Problem Decomposition**: Breaks complex problems into manageable components and reintegrates them with awareness of the whole system.\n\n6. **Optional State Management**: Maintains context between sessions for ongoing analysis.\n\n## Setup and Installation\n\n### Prerequisites\n\n- Node.js (v14 or higher)\n- npm (v6 or higher)\n- An environment compatible with the Model Context Protocol\n\n### Installation Steps\n\n1. **Clone the repository**\n   ```bash\n   git clone https://github.com/yourusername/DeepLucid3D-UCPF-Server.git\n   cd DeepLucid3D-UCPF-Server\n   ```\n\n2. **Install dependencies**\n   ```bash\n   npm install\n   ```\n\n3. **Build the project**\n   ```bash\n   npm run build\n   ```\n\n4. **Configure MCP settings**\n\n   Add the server to your MCP settings file. For Claude/Cline, this is typically located at:\n   - For Claude Desktop: `~/Library/Application Support/Claude/claude_desktop_config.json` (macOS)\n   - For VSCode Cline: `~/.config/Code/User/globalStorage/saoudrizwan.claude-dev/settings/cline_mcp_settings.json` (Linux)\n\n   Add the following configuration:\n   ```json\n   {\n     \"mcpServers\": {\n      \"ucpf\": {\n      \"command\": \"node\",\n      \"args\": [\"path/to/DeepLucid3D-MCP/build/index.js\"],\n      \"env\": {},\n      \"disabled\": false,\n      \"autoApprove\": []\n       }\n     }\n   }\n   ```\n\n5. **Restart your MCP-compatible application** (Claude, VSCode with Cline, etc.)\n\n## How to Use\n\nThe server exposes three main tools and several resources that can be accessed through MCP:\n\n### Tools\n\n1. **analyze_problem**: Process a problem statement through the full UCPF framework\n   ```\n   <use_mcp_tool>\n   <server_name>ucpf</server_name>\n   <tool_name>analyze_problem</tool_name>\n   <arguments>\n   {\n     \"problem\": \"Your problem statement here\",\n     \"session_id\": \"optional-session-id\",\n     \"enable_state\": false,\n     \"detailed\": false\n   }\n   </arguments>\n   </use_mcp_tool>\n   ```\n\n2. **creative_exploration**: Generate novel perspectives and connections for a topic\n   ```\n   <use_mcp_tool>\n   <server_name>ucpf</server_name>\n   <tool_name>creative_exploration</tool_name>\n   <arguments>\n   {\n     \"topic\": \"Your topic here\",\n     \"constraints\": [\"Optional constraint 1\", \"Optional constraint 2\"],\n     \"perspective_count\": 3,\n     \"include_metaphors\": true\n   }\n   </arguments>\n   </use_mcp_tool>\n   ```\n\n3. **manage_state**: Control state management for UCPF processing\n   ```\n   <use_mcp_tool>\n   <server_name>ucpf</server_name>\n   <tool_name>manage_state</tool_name>\n   <arguments>\n   {\n     \"action\": \"enable\", // or \"disable\", \"reset\", \"status\"\n     \"session_id\": \"optional-specific-session-id\"\n   }\n   </arguments>\n   </use_mcp_tool>\n   ```\n\n### Resources\n\nAccess framework documentation:\n```\n<access_mcp_resource>\n<server_name>ucpf</server_name>\n<uri>ucpf://framework/overview</uri>\n</access_mcp_resource>\n```\n\nAvailable resources:\n- `ucpf://framework/overview`: Overview of the UCPF framework\n- `ucpf://framework/cognitive-states`: Guide to cognitive states\n- `ucpf://framework/knowledge-dimensions`: Reference for knowledge dimensions\n- `ucpf://session/{sessionId}/analysis`: Analysis results for a specific session\n\n## Example Use Case: Using UCPF to Solve a Riddle\n\nLet's solve the classic riddle: \"What walks on four legs in the morning, two legs in the afternoon, and three legs in the evening?\"\n\n### Step 1: Enable state management for session persistence\n\n```\n<use_mcp_tool>\n<server_name>ucpf</server_name>\n<tool_name>manage_state</tool_name>\n<arguments>\n{\n  \"action\": \"enable\"\n}\n</arguments>\n</use_mcp_tool>\n```\n\n### Step 2: Analyze the riddle using the UCPF framework\n\n```\n<use_mcp_tool>\n<server_name>ucpf</server_name>\n<tool_name>analyze_problem</tool_name>\n<arguments>\n{\n  \"problem\": \"What walks on four legs in the morning, two legs in the afternoon, and three legs in the evening?\",\n  \"session_id\": \"sphinx-riddle\",\n  \"enable_state\": true\n}\n</arguments>\n</use_mcp_tool>\n```\n\nThe analysis will provide:\n- Cognitive state assessment (likely identifying potential metaphorical thinking)\n- Knowledge mapping of what we know and don't know\n- Recursive questions to challenge initial assumptions (e.g., \"Are we assuming literal legs?\")\n- Structured perspectives on different interpretations\n\n### Step 3: Explore creative perspectives to find the solution\n\n```\n<use_mcp_tool>\n<server_name>ucpf</server_name>\n<tool_name>creative_exploration</tool_name>\n<arguments>\n{\n  \"topic\": \"Walking with different numbers of legs at different times of day\",\n  \"constraints\": [\"morning\", \"afternoon\", \"evening\", \"four\", \"two\", \"three\"],\n  \"include_metaphors\": true,\n  \"session_id\": \"sphinx-riddle\"\n}\n</arguments>\n</use_mcp_tool>\n```\n\nThis exploration might reveal:\n- The metaphorical interpretation of \"legs\" as support structures\n- The metaphorical interpretation of times of day as stages of life\n- Leading to the classic answer: a human, who crawls on four limbs as a baby, walks on two legs as an adult, and uses a cane (third \"leg\") in old age\n\n### Step 4: Review the session analysis\n\n```\n<access_mcp_resource>\n<server_name>ucpf</server_name>\n<uri>ucpf://session/sphinx-riddle/analysis</uri>\n</access_mcp_resource>\n```\n\nThis provides the complete analysis journey, showing how the framework led to the solution through structured cognitive processing.\n\n## Acknowledgments\n\nThis project stands on the shoulders of giants:\n\n- The **Model Context Protocol (MCP)** team for creating the foundational protocol that enables AI systems to access external tools and resources\n- The **Anthropic Claude** team for their work on advanced AI systems capable of utilizing MCP\n- Contributors to the **Unified Cognitive Processing Framework** concepts that power the cognitive analysis methodology\n- The open-source community whose libraries and tools make projects like this possible\n\n## License\n\nMIT License\n\n## Project Structure\n\n```\nDeepLucid3D-UCPF-Server/\n├── src/\n│   ├── engine/\n│   │   ├── ucpf-core.ts       # Core UCPF processing logic\n│   │   ├── creative-patterns.ts  # Creative thinking utilities\n│   │   └── state-manager.ts   # Session state management\n│   ├── tools/\n│   │   ├── analyze-problem.ts # Problem analysis tool\n│   │   └── creative-exploration.ts  # Creative exploration tool\n│   └── index.ts               # Main server implementation\n├── build/                     # Compiled JavaScript files\n├── package.json               # Project dependencies and scripts\n└── README.md                  # This documentation\n```\n\n---\n\n© 2025 DeepLucid3D UCPF Server\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "cognitive",
        "deeplucid3d",
        "tools",
        "cognitive tools",
        "structured cognitive",
        "mushroomfleet deeplucid3d"
      ],
      "category": "scientific-research-tools"
    },
    "PRIDE-Archive--mcp_pride_archive_search": {
      "owner": "PRIDE-Archive",
      "name": "mcp_pride_archive_search",
      "url": "https://github.com/PRIDE-Archive/mcp_pride_archive_search",
      "imageUrl": "/freedevtools/mcp/pfp/null.webp",
      "description": "Search and explore proteomics datasets from the PRIDE Archive using structured queries. It facilitates the integration of AI models with biomedical research data for efficient data retrieval and analysis.",
      "stars": 0,
      "forks": 0,
      "license": "Unknown",
      "language": "Unknown",
      "updated_at": "",
      "readme_content": "",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "proteomics",
        "mcp_pride_archive_search",
        "datasets",
        "proteomics datasets",
        "explore proteomics",
        "mcp_pride_archive_search search"
      ],
      "category": "scientific-research-tools"
    },
    "ProgramComputer--NASA-MCP-server": {
      "owner": "ProgramComputer",
      "name": "NASA-MCP-server",
      "url": "https://github.com/ProgramComputer/NASA-MCP-server",
      "imageUrl": "/freedevtools/mcp/pfp/ProgramComputer.webp",
      "description": "Connects AI models to over 20 NASA APIs, providing a standardized interface for data retrieval and interaction. Supports automatic parameter validation and error handling for optimized data consumption.",
      "stars": 72,
      "forks": 15,
      "license": "ISC License",
      "language": "TypeScript",
      "updated_at": "2025-09-28T04:02:10Z",
      "readme_content": "[![NPM Version](https://img.shields.io/npm/v/%40programcomputer%2Fnasa-mcp-server?link=https%3A%2F%2Fwww.npmjs.com%2Fpackage%2F%40programcomputer%2Fnasa-mcp-server)](https://www.npmjs.com/package/@programcomputer/nasa-mcp-server)\n\n# NASA MCP Server\n\nA Model Context Protocol (MCP) server for NASA APIs, providing a standardized interface for AI models to interact with NASA's vast array of data sources. This server implements the official Model Context Protocol specification.\n\nBig thanks to the MCP community for their support and guidance!\n\n## Features\n\n* Access to 20+ NASA data sources through a single, consistent interface\n* Standardized data formats optimized for AI consumption\n* Automatic parameter validation and error handling\n* Rate limit management for NASA API keys\n* Comprehensive documentation and examples\n* Support for various NASA imagery formats\n* Data conversion and formatting for LLM compatibility\n* Cross-platform support (Windows, macOS, Linux)\n\n## Disclaimer\n\n**This project is not affiliated with, endorsed by, or related to NASA (National Aeronautics and Space Administration) or any of its subsidiaries or its affiliates.** It is an independent implementation that accesses NASA's publicly available APIs. All NASA data used is publicly available and subject to NASA's data usage policies.\n\n## Installation\n\n### Running with npx\n\n```bash\nenv NASA_API_KEY=YOUR_API_KEY npx -y @programcomputer/nasa-mcp-server@latest\n```\n\nYou can also pass the API key as a command line argument:\n\n```bash\nnpx -y @programcomputer/nasa-mcp-server@latest --nasa-api-key=YOUR_API_KEY\n```\n\n### Using SuperGateway for Server-Sent Events (SSE)\n\nYou can use [SuperGateway](https://github.com/supercorp-ai/supergateway) for Server-Sent Events (SSE).\n\n**The developers of NASA-MCP-server DO NOT ENDORSE the SuperGateway repository. This information is provided for those who wish to implement SSE functionality at their own discretion.**\n\n### Manual Installation\n\n```bash\n# Clone the repository\ngit clone https://github.com/ProgramComputer/NASA-MCP-server.git\n\n# Install dependencies\ncd NASA-MCP-server\nnpm install\n\n# Run with your API key\nNASA_API_KEY=YOUR_API_KEY npm start\n```\n\n### Running on Cursor\n\nConfiguring Cursor 🖥️ Note: Requires Cursor version 0.45.6+\n\nTo configure NASA MCP Server in Cursor:\n\nCreate or edit an `mcp.json` file in your Cursor configuration directory with the following content:\n\n```json\n{\n  \"mcpServers\": {\n    \"nasa-mcp\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@programcomputer/nasa-mcp-server@latest\"],\n      \"env\": {\n        \"NASA_API_KEY\": \"your-api-key\"\n      }\n    }\n  }\n}\n```\n\nReplace `your-api-key` with your NASA API key from https://api.nasa.gov/.\n\nAfter adding the configuration, restart Cursor to see the new NASA tools. The Composer Agent will automatically use NASA MCP when appropriate for space-related queries.\n\n## Environment Variables\n\nThe server can be configured with the following environment variables:\n\n| Variable | Description |\n|----------|-------------|\n| `NASA_API_KEY` | Your NASA API key (get at api.nasa.gov) |\n\n## Included NASA APIs\n\nThis MCP server integrates the following NASA APIs:\n\n1. **NASA Open API** (api.nasa.gov):\n   - APOD (Astronomy Picture of the Day)\n   - EPIC (Earth Polychromatic Imaging Camera)\n   - DONKI (Space Weather Database Of Notifications, Knowledge, Information)\n   - Insight (Mars Weather Service)\n   - Mars Rover Photos\n   - NEO (Near Earth Object Web Service)\n   - EONET (Earth Observatory Natural Event Tracker)\n   - TLE (Two-Line Element)\n   - NASA Image and Video Library\n   - Exoplanet Archive\n   - NASA Sounds API (Beta)\n   - POWER (Prediction Of Worldwide Energy Resources)\n\n2. **JPL Solar System Dynamics API** (ssd-api.jpl.nasa.gov):\n   - SBDB (Small-Body DataBase)\n   - SBDB Close-Approach Data\n   - Fireball Data\n   - Scout API\n\n3. **Earth Data APIs**:\n   - GIBS (Global Imagery Browse Services)\n   - CMR (Common Metadata Repository) - Enhanced with advanced search capabilities\n   - EPIC (Earth Polychromatic Imaging Camera)\n   - FIRMS (Fire Information for Resource Management System)\n\n## API Methods\n\nEach NASA API is exposed through standardized MCP methods:\n\n### APOD (Astronomy Picture of the Day)\n\n```json\n{\n  \"method\": \"nasa/apod\",\n  \"params\": {\n    \"date\": \"2023-01-01\", // Optional: YYYY-MM-DD format\n    \"count\": 5, // Optional: Return a specified number of random images\n    \"thumbs\": true // Optional: Return URL of video thumbnail\n  }\n}\n```\n\n### Mars Rover Photos\n\n```json\n{\n  \"method\": \"nasa/mars-rover\",\n  \"params\": {\n    \"rover\": \"curiosity\", // Required: \"curiosity\", \"opportunity\", or \"spirit\"\n    \"sol\": 1000, // Either sol or earth_date is required\n    \"earth_date\": \"2023-01-01\", // YYYY-MM-DD format\n    \"camera\": \"FHAZ\" // Optional: Filter by camera type\n  }\n}\n```\n\n### Near Earth Objects\n\n```json\n{\n  \"method\": \"nasa/neo\",\n  \"params\": {\n    \"start_date\": \"2023-01-01\", // Required: YYYY-MM-DD format\n    \"end_date\": \"2023-01-07\" // Required: YYYY-MM-DD format (max 7 days from start)\n  }\n}\n```\n\n### GIBS (Global Imagery Browse Services)\n\n```json\n{\n  \"method\": \"nasa/gibs\",\n  \"params\": {\n    \"layer\": \"MODIS_Terra_CorrectedReflectance_TrueColor\", // Required: Layer ID\n    \"date\": \"2023-01-01\", // Required: YYYY-MM-DD format\n    \"format\": \"png\" // Optional: \"png\" or \"jpg\"\n  }\n}\n```\n\n### POWER (Prediction Of Worldwide Energy Resources)\n\n```json\n{\n  \"method\": \"nasa/power\",\n  \"params\": {\n    \"parameters\": \"T2M,PRECTOTCORR,WS10M\", // Required: Comma-separated list\n    \"community\": \"re\", // Required: Community identifier\n    \"latitude\": 40.7128, // Required: Latitude\n    \"longitude\": -74.0060, // Required: Longitude\n    \"start\": \"20220101\", // Required: Start date (YYYYMMDD)\n    \"end\": \"20220107\" // Required: End date (YYYYMMDD)\n  }\n}\n```\n\nFor complete documentation of all available methods and parameters, see the API reference in the `/docs` directory.\n\n## Logging System\n\nThe server includes comprehensive logging:\n\n* Operation status and progress\n* Performance metrics\n* Rate limit tracking\n* Error conditions\n* Request validation\n\nExample log messages:\n\n```\n[INFO] NASA MCP Server initialized successfully\n[INFO] Processing APOD request for date: 2023-01-01\n[INFO] Fetching Mars Rover data for Curiosity, sol 1000\n[WARNING] Rate limit threshold reached (80%)\n[ERROR] Invalid parameter: 'date' must be in YYYY-MM-DD format\n```\n\n## Security Considerations\n\nThis MCP server implements security best practices following the Model Context Protocol specifications:\n\n* Input validation and sanitization using Zod schemas\n* No execution of arbitrary code\n* Protection against command injection\n* Proper error handling to prevent information leakage\n* Rate limiting and timeout controls for API requests\n* No persistent state that could be exploited across sessions\n\n## Development\n\n```bash\n# Clone the repository\ngit clone https://github.com/ProgramComputer/NASA-MCP-server.git\n\n# Install dependencies\nnpm install\n\n# Copy the example environment file and update with your API keys\ncp .env.example .env\n\n# Build the TypeScript code\nnpm run build\n\n# Start the development server\nnpm run dev\n\n# Run tests\nnpm test\n```\n\n## Testing with MCP Inspector\n\nThe NASA MCP Server includes a script to help you test the APIs using the MCP Inspector:\n\n```bash\n# Run the provided test script\n./scripts/test-with-inspector.sh\n```\n\nThis will:\n1. Build the project to ensure the latest changes are included\n2. Start the MCP Inspector with the NASA MCP server running\n3. Allow you to interactively test all the NASA APIs\n\n### Example Test Requests\n\nThe repository includes example test requests for each API that you can copy and paste into the MCP Inspector:\n\n```bash\n# View the example test requests\ncat docs/inspector-test-examples.md\n```\n\nFor detailed examples, see the [Inspector Test Examples](docs/inspector-test-examples.md) document.\n\n## MCP Client Usage\n\nThis server follows the official Model Context Protocol. Here's an example of how to use it with the MCP SDK:\n\n```typescript\nimport { Client } from \"@modelcontextprotocol/sdk/client/index.js\";\nimport { HttpClientTransport } from \"@modelcontextprotocol/sdk/client/http.js\";\n\nconst transport = new HttpClientTransport({\n  url: \"http://localhost:3000\",\n});\n\nconst client = new Client({\n  name: \"mcp-client\",\n  version: \"1.0.0\",\n});\n\nawait client.connect(transport);\n\n// Example: Get today's Astronomy Picture of the Day\nconst apodResult = await client.request({\n  method: \"nasa/apod\", \n  params: {}\n});\n\n// Example: Get Mars Rover photos\nconst marsRoverResult = await client.request({\n  method: \"nasa/mars-rover\",\n  params: { rover: \"curiosity\", sol: 1000 }\n});\n\n// Example: Search for Near Earth Objects\nconst neoResults = await client.request({\n  method: \"nasa/neo\",\n  params: {\n    start_date: '2023-01-01',\n    end_date: '2023-01-07'\n  }\n});\n\n// Example: Get satellite imagery from GIBS\nconst satelliteImage = await client.request({\n  method: \"nasa/gibs\",\n  params: {\n    layer: 'MODIS_Terra_CorrectedReflectance_TrueColor',\n    date: '2023-01-01'\n  }\n});\n\n// Example: Use the new POWER API\nconst powerData = await client.request({\n  method: \"nasa/power\",\n  params: {\n    parameters: \"T2M,PRECTOTCORR,WS10M\",\n    community: \"re\",\n    latitude: 40.7128,\n    longitude: -74.0060,\n    start: \"20220101\",\n    end: \"20220107\"\n  }\n});\n```\n\n## Contributing\n\n1. Fork the repository\n2. Create your feature branch\n3. Run tests: `npm test`\n4. Submit a pull request\n\n## License\n\nISC License - see LICENSE file for details \n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "nasa",
        "apis",
        "ai",
        "nasa apis",
        "programcomputer nasa",
        "nasa mcp"
      ],
      "category": "scientific-research-tools"
    },
    "SeanMcLoughlin--mcp-vcd": {
      "owner": "SeanMcLoughlin",
      "name": "mcp-vcd",
      "url": "https://github.com/SeanMcLoughlin/mcp-vcd",
      "imageUrl": "/freedevtools/mcp/pfp/SeanMcLoughlin.webp",
      "description": "Handles value change dump (VCD) waveforms for AI models, providing specific signal changes within defined timestamp ranges to optimize context management for large waveform files.",
      "stars": 5,
      "forks": 4,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-08-13T03:09:17Z",
      "readme_content": "# mcp-vcd\r\n\r\nA model context protocol (MCP) server for value change dump (VCD) waveforms.\r\n\r\nhttps://github.com/user-attachments/assets/9d1a6a64-de22-4b5a-a597-685c663c9c79\r\n\r\n<a href=\"https://glama.ai/mcp/servers/kdvs90ijbl\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/kdvs90ijbl/badge\" alt=\"mcp-vcd MCP server\" /></a>\r\n\r\n# Tools\r\n\r\n- `get-signal`: Provide all changes of the specified signal name to the model's context. This is useful for large waveform files with many signals where you cannot fit the entire VCD file into the model's context window.\r\n  - Can optionally specify a start and end timestamp range for the model to look at.\r\n\r\n# Installation\r\n\r\n`uv pip install --system mcp-vcd`\r\n\r\nAnd add the following to your `claude_desktop_config.json`:\r\n\r\n```json\r\n\"mcpServers\": {\r\n  \"mcp-vcd\": {\r\n    \"command\": \"uv\",\r\n    \"args\": [\r\n      \"run\",\r\n      \"mcp-vcd\"\r\n    ]\r\n  }\r\n}\r\n```\r\nSee [Anthropic's MCP documentation](https://modelcontextprotocol.io/quickstart/user) for more info.\r\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "vcd",
        "mcp",
        "waveforms",
        "mcp vcd",
        "vcd waveforms",
        "dump vcd"
      ],
      "category": "scientific-research-tools"
    },
    "StarGazer1995--mcp-stargazing": {
      "owner": "StarGazer1995",
      "name": "mcp-stargazing",
      "url": "https://github.com/StarGazer1995/mcp-stargazing",
      "imageUrl": "/freedevtools/mcp/pfp/StarGazer1995.webp",
      "description": "Calculate altitude, rise, and set times of celestial objects based on geographic location, while also analyzing light pollution to improve stargazing conditions. Supports a variety of celestial objects including the Sun, Moon, planets, and deep-space entities.",
      "stars": 0,
      "forks": 0,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-05-18T09:45:10Z",
      "readme_content": "# mcp-stargazing\n\nCalculate the altitude, rise, and set times of celestial objects (Sun, Moon, planets, stars, and deep-space objects) for any location on Earth, with optional light pollution analysis.\n\n## Features\n- **Altitude/Azimuth Calculation**: Get elevation and compass direction for any celestial object.\n- **Rise/Set Times**: Determine when objects appear/disappear above the horizon.\n- **Light Pollution Analysis**: Load and analyze light pollution maps (GeoTIFF format).\n- **Supports**:\n  - Solar system objects (Sun, Moon, planets)\n  - Stars (e.g., \"sirius\")\n  - Deep-space objects (e.g., \"andromeda\", \"orion_nebula\")\n- **Time Zone Aware**: Works with local or UTC times.\n\n## Installation\n```bash\npip install astropy pytz numpy astroquery rasterio geopy\n```\n\n## Usage\n\n### Calculate Altitude/Azimuth\n```python src/main.py\nfrom src.celestial import celestial_pos\nfrom astropy.coordinates import EarthLocation\nimport pytz\nfrom datetime import datetime\n\n# Observer location (New York)\nlocation = EarthLocation(lat=40.7128, lon=-74.0060)\n\n# Time (local timezone-aware)\nlocal_time = pytz.timezone(\"America/New_York\").localize(datetime(2023, 10, 1, 12, 0))\naltitude, azimuth = celestial_pos(\"sun\", location, local_time)\nprint(f\"Sun Position: Altitude={altitude:.1f}°, Azimuth={azimuth:.1f}°\")\n```\n\n### Calculate Rise/Set Times\n```python src/main.py\nfrom src.celestial import celestial_rise_set\n\nrise, set_ = celestial_rise_set(\"andromeda\", location, local_time.date())\nprint(f\"Andromeda: Rise={rise.iso}, Set={set_.iso}\")\n```\n\n### Load Light Pollution Map\n```python src/light_pollution.py\nfrom src.light_pollution import load_map\n\n# Load a GeoTIFF light pollution map\nvriis_data, bounds, crs, transform = load_map(\"path/to/map.tif\")\nprint(f\"Map Bounds: {bounds}\")\n```\n\n## API Reference\n\n### `celestial_pos(celestial_object, observer_location, time)` (`src/celestial.py`)\n- **Inputs**:\n  - `celestial_object`: Name (e.g., `\"sun\"`, `\"andromeda\"`).\n  - `observer_location`: `EarthLocation` object.\n  - `time`: `datetime` (timezone-aware) or Astropy `Time`.\n- **Returns**: `(altitude_degrees, azimuth_degrees)`.\n\n### `celestial_rise_set(celestial_object, observer_location, date, horizon=0.0)` (`src/celestial.py`)\n- **Inputs**: \n  - `date`: Timezone-aware `datetime`.\n  - `horizon`: Horizon elevation (default: 0°).\n- **Returns**: `(rise_time, set_time)` as UTC `Time` objects.\n\n### `load_map(map_path)` (`src/light_pollution.py`)\n- **Inputs**:\n  - `map_path`: Path to GeoTIFF file.\n- **Returns**: Tuple `(vriis_data, bounds, crs, transform)` for light pollution analysis.\n\n## Testing\nRun tests with:\n```bash\npytest tests/\n```\n\n### Key Test Cases (`tests/test_celestial.py`)\n```python tests/test_celestial.py\ndef test_calculate_altitude_deepspace():\n    \"\"\"Test deep-space object resolution.\"\"\"\n    altitude, _ = celestial_pos(\"andromeda\", NYC, Time.now())\n    assert -90 <= altitude <= 90\n\ndef test_calculate_rise_set_sun():\n    \"\"\"Validate Sun rise/set times.\"\"\"\n    rise, set_ = celestial_rise_set(\"sun\", NYC, datetime(2023, 10, 1))\n    assert rise < set_\n```\n\n## Project Structure\n```\n.\n├── src/\n│   ├── celestial.py          # Core celestial calculations\n│   ├── light_pollution.py    # Light pollution map utilities\n│   ├── utils.py              # Time/location helpers\n│   └── main.py               # CLI entry point\n├── tests/\n│   ├── test_celestial.py\n│   └── test_utils.py\n└── README.md\n```\n\n## Future Work\n- Add support for comets/asteroids.\n- Optimize SIMBAD queries for offline use.\n- Integrate light pollution data into visibility predictions.\n\n### Key Updates:\n1. **Light Pollution**: Added `light_pollution.py` to features and API reference.\n2. **Dependencies**: Added `rasterio` and `geopy` to installation instructions.\n3. **Project Structure**: Clarified file roles and test coverage.\n```",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "stargazing",
        "stargazer1995",
        "celestial",
        "mcp stargazing",
        "stargazer1995 mcp",
        "stargazing calculate"
      ],
      "category": "scientific-research-tools"
    },
    "TakumiY235--uniprot-mcp-server": {
      "owner": "TakumiY235",
      "name": "uniprot-mcp-server",
      "url": "https://github.com/TakumiY235/uniprot-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/TakumiY235.webp",
      "description": "Fetch protein information from UniProt, enabling access to detailed data about protein functions and sequences. Supports both individual and batch retrieval of protein information, including name, function description, full sequence, and organism details.",
      "stars": 9,
      "forks": 6,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-08-14T07:44:52Z",
      "readme_content": "# UniProt MCP Server\n\nA Model Context Protocol (MCP) server that provides access to UniProt protein information. This server allows AI assistants to fetch protein function and sequence information directly from UniProt.\n\n<a href=\"https://glama.ai/mcp/servers/ttjbai3lpx\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/ttjbai3lpx/badge\" alt=\"UniProt Server MCP server\" />\n</a>\n\n## Features\n\n- Get protein information by UniProt accession number\n- Batch retrieval of multiple proteins\n- Caching for improved performance (24-hour TTL)\n- Error handling and logging\n- Information includes:\n  - Protein name\n  - Function description\n  - Full sequence\n  - Sequence length\n  - Organism\n\n## Quick Start\n\n1. Ensure you have Python 3.10 or higher installed\n2. Clone this repository:\n   ```bash\n   git clone https://github.com/TakumiY235/uniprot-mcp-server.git\n   cd uniprot-mcp-server\n   ```\n3. Install dependencies:\n   ```bash\n   # Using uv (recommended)\n   uv pip install -r requirements.txt\n   \n   # Or using pip\n   pip install -r requirements.txt\n   ```\n\n## Configuration\n\nAdd to your Claude Desktop config file:\n\n- Windows: `%APPDATA%\\Claude\\claude_desktop_config.json`\n- macOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\n- Linux: `~/.config/Claude/claude_desktop_config.json`\n\n```json\n{\n  \"mcpServers\": {\n    \"uniprot\": {\n      \"command\": \"uv\",\n      \"args\": [\"--directory\", \"path/to/uniprot-mcp-server\", \"run\", \"uniprot-mcp-server\"]\n    }\n  }\n}\n```\n\n## Usage Examples\n\nAfter configuring the server in Claude Desktop, you can ask questions like:\n\n```\nCan you get the protein information for UniProt accession number P98160?\n```\n\nFor batch queries:\n\n```\nCan you get and compare the protein information for both P04637 and P02747?\n```\n\n## API Reference\n\n### Tools\n\n1. `get_protein_info`\n   - Get information for a single protein\n   - Required parameter: `accession` (UniProt accession number)\n   - Example response:\n     ```json\n     {\n       \"accession\": \"P12345\",\n       \"protein_name\": \"Example protein\",\n       \"function\": [\"Description of protein function\"],\n       \"sequence\": \"MLTVX...\",\n       \"length\": 123,\n       \"organism\": \"Homo sapiens\"\n     }\n     ```\n\n2. `get_batch_protein_info`\n   - Get information for multiple proteins\n   - Required parameter: `accessions` (array of UniProt accession numbers)\n   - Returns an array of protein information objects\n\n## Development\n\n### Setting up development environment\n\n1. Clone the repository\n2. Create a virtual environment:\n   ```bash\n   python -m venv .venv\n   source .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\n   ```\n3. Install development dependencies:\n   ```bash\n   pip install -e \".[dev]\"\n   ```\n\n### Running tests\n\n```bash\npytest\n```\n\n### Code style\n\nThis project uses:\n- Black for code formatting\n- isort for import sorting\n- flake8 for linting\n- mypy for type checking\n- bandit for security checks\n- safety for dependency vulnerability checks\n\nRun all checks:\n```bash\nblack .\nisort .\nflake8 .\nmypy .\nbandit -r src/\nsafety check\n```\n\n## Technical Details\n\n- Built using the MCP Python SDK\n- Uses httpx for async HTTP requests\n- Implements caching with 24-hour TTL using an OrderedDict-based cache\n- Handles rate limiting and retries\n- Provides detailed error messages\n\n### Error Handling\n\nThe server handles various error scenarios:\n- Invalid accession numbers (404 responses)\n- API connection issues (network errors)\n- Rate limiting (429 responses)\n- Malformed responses (JSON parsing errors)\n- Cache management (TTL and size limits)\n\n## Contributing\n\nWe welcome contributions! Please feel free to submit a Pull Request. Here's how you can contribute:\n\n1. Fork the repository\n2. Create your feature branch (`git checkout -b feature/amazing-feature`)\n3. Commit your changes (`git commit -m 'Add some amazing feature'`)\n4. Push to the branch (`git push origin feature/amazing-feature`)\n5. Open a Pull Request\n\nPlease make sure to update tests as appropriate and adhere to the existing coding style.\n\n## License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n\n## Acknowledgments\n\n- UniProt for providing the protein data API\n- Anthropic for the Model Context Protocol specification\n- Contributors who help improve this project",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "uniprot",
        "protein",
        "retrieval",
        "protein information",
        "retrieval protein",
        "uniprot mcp"
      ],
      "category": "scientific-research-tools"
    },
    "Tsuchijo--matlab-mcp": {
      "owner": "Tsuchijo",
      "name": "matlab-mcp",
      "url": "https://github.com/Tsuchijo/matlab-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/Tsuchijo.webp",
      "description": "Integrate with MATLAB to create and execute MATLAB scripts and functions via MCP clients. Supports automation of MATLAB programming tasks within various applications.",
      "stars": 32,
      "forks": 4,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-09-26T18:58:11Z",
      "readme_content": "# MATLAB MCP Server\n\nThis Model Context Protocol (MCP) server provides integration with MATLAB, allowing you to create and execute MATLAB scripts and functions through Claude or other MCP clients.\n\n## Setup Requirements\n\n- Python 3.11 (Python 3.13 and 3.12 are not currently supported by MATLAB Engine)\n- MATLAB R2024a (or compatible version)\n- uv package manager\n\n## Installation\n\n1. Create and set up the Python environment:\n```bash\n# Pin Python version\nuv python pin 3.11\n\n# Create virtual environment\nuv venv\n\n# Activate virtual environment\nsource .venv/bin/activate\n\n# Install MCP\nuv add \"mcp[cli]\"\n```\n\n2. Install MATLAB Engine\nThe MATLAB Engine will be installed automatically when the server first runs, using the MATLAB installation specified in the `MATLAB_PATH` environment variable.\n\n## Directory Structure\n\n- `matlab_server.py`: The main MCP server implementation\n- `matlab_scripts/`: Directory where all MATLAB scripts and functions are saved (created automatically)\n- `pyproject.toml`: Python project configuration\n- `.python-version`: Specifies Python version for uv\n\n## Claude Desktop Integration\n\n1. Open your Claude Desktop configuration:\n```bash\n# On macOS\ncode ~/Library/Application\\ Support/Claude/claude_desktop_config.json\n```\n\n2. Add the MATLAB server configuration:\n```json\n{\n    \"mcpServers\": {\n        \"matlab\": {\n            \"command\": \"uv\",\n            \"args\": [\n                \"--directory\",\n                \"/absolute/path/to/matlab-mcp\",\n                \"run\",\n                \"matlab_server.py\"\n            ],\n            \"env\": {\n                \"MATLAB_PATH\": \"/Applications/MATLAB_R2024a.app\"\n            }\n        }\n    }\n}\n```\n\nMake sure to:\n- Replace `/absolute/path/to/matlab-mcp` with the actual path to your project directory\n- Verify the `MATLAB_PATH` points to your MATLAB installation\n- Use absolute paths (not relative)\n\n## Features\n\nThe server provides several tools:\n\n1. `create_matlab_script`: Create a new MATLAB script file\n   - Scripts are saved in the `matlab_scripts` directory\n   - File names must be valid MATLAB identifiers\n\n2. `create_matlab_function`: Create a new MATLAB function file\n   - Functions are saved in the `matlab_scripts` directory\n   - Must include valid function definition\n\n3. `execute_matlab_script`: Run a MATLAB script and get results\n   - Returns output text, generated figures, and workspace variables\n   - Can pass arguments to scripts\n\n4. `call_matlab_function`: Call a MATLAB function with arguments\n   - Returns function output and any generated figures\n\n## Testing\n\nYou can test the server using the MCP Inspector:\n```bash\n# Make sure you're in your virtual environment\nsource .venv/bin/activate\n\n# Run the inspector\nMATLAB_PATH=/Applications/MATLAB_R2024a.app mcp dev matlab_server.py\n```\n\nExample test script:\n```matlab\nt = 0:0.01:2*pi;\ny = sin(t);\nplot(t, y);\ntitle('Test Plot');\nxlabel('Time');\nylabel('Amplitude');\n```\n\n## Script Storage\n\n- All MATLAB scripts and functions are saved in the `matlab_scripts` directory\n- This directory is created automatically when the server starts\n- Files are named `<script_name>.m` or `<function_name>.m`\n- The directory is in the same location as `matlab_server.py`\n\n## Environment Variables\n\n- `MATLAB_PATH`: Path to your MATLAB installation\n  - Default: `/Applications/MATLAB_R2024a.app`\n  - Set in Claude Desktop config or when running directly\n\n## Troubleshooting\n\n1. **MATLAB Engine Installation Fails**\n   - Verify MATLAB_PATH is correct\n   - Try installing engine manually:\n     ```bash\n     cd $MATLAB_PATH/extern/engines/python\n     python setup.py install\n     ```\n\n2. **Python Version Issues**\n   - Make sure you're using Python 3.11\n   - Check with: `python --version`\n   - Use `uv python pin 3.11` if needed\n\n3. **Script Execution Errors**\n   - Check the `matlab_scripts` directory exists\n   - Verify script syntax is valid\n   - Look for error messages in MATLAB output\n\n## Updates and Maintenance\n\n- Keep your MATLAB installation updated\n- Update Python packages as needed: `uv pip install --upgrade mcp[cli]`\n- Check MATLAB engine compatibility when updating Python",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "matlab",
        "mcp",
        "scripts",
        "matlab scripts",
        "matlab mcp",
        "automation matlab"
      ],
      "category": "scientific-research-tools"
    },
    "YuChenSSR--quantum-simulator-mcp": {
      "owner": "YuChenSSR",
      "name": "quantum-simulator-mcp",
      "url": "https://github.com/YuChenSSR/quantum-simulator-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/YuChenSSR.webp",
      "description": "Simulates quantum circuits incorporating noise models and visualizes results. Supports OpenQASM 2.0 and various quantum algorithms with integration into MCP clients for streamlined use.",
      "stars": 10,
      "forks": 2,
      "license": "MIT License",
      "language": "",
      "updated_at": "2025-06-19T23:08:17Z",
      "readme_content": "# Quantum Simulator MCP Server\n\nA Docker image providing a quantum circuit simulator that implements the Model Context Protocol (MCP), allowing integration with MCP clients such as Claude for Desktop.\n\n\n\n## Features\n\n- Quantum computing simulator with noise models\n- Support for OpenQASM 2.0 quantum circuits\n- Quantum circuit simulation using Qiskit\n- Support for various noise models (depolarizing, thermal relaxation, readout error)\n- Multiple result types including counts, statevector, and visualized histograms\n- Pre-configured example circuits\n- Seamless integration with MCP clients\n\n## Quick Start\n\nget the docker image\n\n```bash\ndocker pull ychen94/quantum-simulator-mcp:latest\n```\n\n\nSimply run the container with the following command:\n\n```bash\ndocker run -i --rm -v /tmp:/data/quantum_simulator_results -e HOST_OUTPUT_DIR=\"/tmp\" ychen94/quantum-simulator-mcp:latest\n```\n\nThis command:\n- Mounts the `/tmp` directory on your host to store histogram output files\n- Sets the `HOST_OUTPUT_DIR` environment variable to `/tmp`\n- Keeps the container running with `-i` (interactive mode)\n- Automatically removes the container when it exits with `--rm`\n\n## Using with Claude for Desktop\n\n1. Install Claude for Desktop\n2. Edit the Claude configuration file:\n   - macOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\n   - Windows: `%APPDATA%\\Claude\\claude_desktop_config.json`\n\n3. Add the following configuration to the `mcpServers` section:\n\n```json\n{\n  \"mcpServers\": {\n    \"quantum-simulator\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"-i\",\n        \"--rm\",\n        \"-v\", \"/tmp:/data/quantum_simulator_results\",\n        \"-e\", \"HOST_OUTPUT_DIR=/tmp\",\n        \"ychen94/quantum-simulator-mcp:latest\"\n      ]\n    }\n  }\n}\n```\n\n4. Restart Claude for Desktop\n5. Look for the hammer icon in the Claude UI, indicating available MCP tools\n\n## MCP Tools\n\nThe server provides the following MCP tools:\n\n- **run_circuit**: Run a quantum circuit with specified noise model\n- **list_noise_models**: List all available noise models and their descriptions\n- **list_result_types**: List all available result types and their descriptions\n- **get_circuit_stats**: Analyze a quantum circuit and return statistics\n- **create_test_histogram**: Create a test histogram file to verify output directory configuration\n\n## MCP Resources\n\nThe server provides example quantum circuits:\n\n- `qasm://examples/bell-state.qasm`: Bell state preparation circuit\n- `qasm://examples/grover-2qubit.qasm`: 2-qubit Grover's algorithm implementation\n- `qasm://examples/qft-4qubit.qasm`: 4-qubit Quantum Fourier Transform\n- `quantum://noise-models/examples.json`: Example noise model configurations\n\n## Example Usage in Claude\n\nHere are some prompts you can use in Claude:\n\n1. \"Run a Bell state circuit and show me the results\"\n\n2. \"What noise models are available in the quantum simulator?\"\n\n3. \"Simulate a 2-qubit Grover's algorithm with 0.01 depolarizing noise\"\n\n4. \"Create a test histogram and show me the file path\"\n\n5. \"Please provide a simple QAOA algorithm, only get the result_types: histogram, and view the histogram using iterm\"\n\n![chat](https://raw.githubusercontent.com/YuChenSSR/pics/master/imgs/2025-03-22/FHE8cIDqLRN36pOm.png)\n\n![result_pic](https://raw.githubusercontent.com/YuChenSSR/pics/master/imgs/2025-03-22/OKD2nqE0aHYuWBan.png)\n\n\n## Volume Mapping\n\nThe container generates histogram PNG files in `/data/quantum_simulator_results`. These files need to be accessible from your host system. The volume mapping (`-v /tmp:/data/quantum_simulator_results`) makes these files available in your host's `/tmp` directory.\n\n## Environment Variables\n\n- `QUANTUM_OUTPUT_DIR`: Output directory for histogram files inside the container (default: `/data/quantum_simulator_results`)\n- `HOST_OUTPUT_DIR`: Corresponding path on the host system (default: `/tmp`)\n\n## Multi-Architecture Support\n\nThis image supports the following architectures:\n- linux/amd64\n- linux/arm64 (confirmed working on Mac M-series chips)\n\nNote: The image has not been tested on Windows systems yet, but should work as long as Docker Desktop is properly configured.\n\n## Troubleshooting\n\n**Issue**: Claude cannot access the histogram files.  \n**Solution**: Ensure the volume mapping is correct and the `HOST_OUTPUT_DIR` environment variable matches the host path in your volume mapping.\n\n**Issue**: Docker container exits immediately.  \n**Solution**: Make sure to use the `-i` flag to keep stdin open, which is required for the MCP STDIO transport.\n\n## License\n\nThis project is licensed under the MIT License. For more details, please see the LICENSE file in [this project repository](https://github.com/YuChenSSR/quantum-simulator-mcp).\n\n\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "quantum",
        "openqasm",
        "simulates",
        "quantum simulator",
        "simulates quantum",
        "quantum algorithms"
      ],
      "category": "scientific-research-tools"
    },
    "YuChenSSR--symbolica-mcp": {
      "owner": "YuChenSSR",
      "name": "symbolica-mcp",
      "url": "https://github.com/YuChenSSR/symbolica-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/YuChenSSR.webp",
      "description": "Facilitates symbolic computing, mathematical analysis, and data visualization for scientific and engineering applications using libraries like NumPy, SciPy, and Matplotlib in a containerized setup. Supports complex matrix calculations, statistical analysis, and machine learning tasks.",
      "stars": 10,
      "forks": 2,
      "license": "MIT License",
      "language": "",
      "updated_at": "2025-07-28T02:42:49Z",
      "readme_content": "# symbolica-mcp\n\nA scientific computing Model Context Protocol (MCP) server allows AI, such as Claude, to perform symbolic computing, conduct calculations, analyze data, and generate visualizations. This is particularly useful for scientific and engineering applications, including quantum computing, all within a containerized environment.\n\n## Features\n\n- Run scientific computing operations with NumPy, SciPy, SymPy, Pandas\n- Perform symbolic mathematics and solve differential equations\n- Support for linear algebra operations and matrix manipulations\n- Quantum computing analysis\n- Create data visualizations with Matplotlib and Seaborn\n- Perform machine learning operations with scikit-learn\n- Execute tensor operations and complex matrix calculations\n- Analyze data sets with statistical tools\n- Cross-platform support (automatically detects Windows, macOS, and Linux), especially for users with Mac M series chips\n- Works on both Intel/AMD (x86_64) and ARM processors\n\n## Quick Start\n\n### Using the Docker image\n\n```bash\n# Pull the image from Docker Hub\ndocker pull ychen94/computing-mcp:latest\n\n# Run the container (automatically detects host OS)\ndocker run -i --rm -v /tmp:/app/shared ychen94/computing-mcp:latest\n```\n\nFor Windows users:\n```powershell\ndocker run -i --rm -v $env:TEMP:/app/shared ychen94/computing-mcp:latest\n```\n\n### Integrating with Claude for Desktop\n\n1. Open Claude for Desktop\n2. Open Settings ➝ Developer ➝ Edit Config\n3. Add the following configuration:\n\nFor MacOS/Linux:\n```json\n{\n  \"mcpServers\": {\n    \"computing-mcp\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"-i\",\n        \"--rm\",\n        \"-v\",\n        \"/tmp:/app/shared\",\n        \"ychen94/computing-mcp:latest\"\n      ]\n    }\n  }\n}\n```\n\nFor Windows:\n```json\n{\n  \"mcpServers\": {\n    \"computing-mcp\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"-i\",\n        \"--rm\",\n        \"-v\",\n        \"%TEMP%:/app/shared\",\n        \"ychen94/computing-mcp:latest\"\n      ]\n    }\n  }\n}\n```\n\n\n\n## Examples\n\n### Tensor Products\n\n```\nCan you calculate and visualize the tensor product of two matrices? Please run:\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Define two matrices\nA = np.array([[1, 2], \n              [3, 4]])\nB = np.array([[5, 6],\n              [7, 8]])\n\n# Calculate tensor product using np.kron (Kronecker product)\ntensor_product = np.kron(A, B)\n\n# Display the result\nprint(\"Matrix A:\")\nprint(A)\nprint(\"\\nMatrix B:\")\nprint(B)\nprint(\"\\nTensor Product A ⊗ B:\")\nprint(tensor_product)\n\n# Create a visualization of the tensor product\nplt.figure(figsize=(8, 6))\nplt.imshow(tensor_product, cmap='viridis')\nplt.colorbar(label='Value')\nplt.title('Visualization of Tensor Product A ⊗ B')\n```\n\n### Symbolic Mathematics\n\n```\nCan you solve this differential equation? Please run:\nimport sympy as sp\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Define symbolic variable\nx = sp.Symbol('x')\ny = sp.Function('y')(x)\n\n# Define the differential equation: y''(x) + 2*y'(x) + y(x) = 0\ndiff_eq = sp.Eq(sp.diff(y, x, 2) + 2*sp.diff(y, x) + y, 0)\n\n# Solve the equation\nsolution = sp.dsolve(diff_eq)\nprint(\"Solution:\")\nprint(solution)\n\n# Plot a particular solution (C1=1, C2=0)\nsolution_func = solution.rhs.subs({sp.symbols('C1'): 1, sp.symbols('C2'): 0})\nprint(\"Particular solution:\")\nprint(solution_func)\n\n# Create a numerical function we can evaluate\nsolution_lambda = sp.lambdify(x, solution_func)\n\n# Plot the solution\nx_vals = np.linspace(0, 5, 100)\ny_vals = [float(solution_lambda(x_val)) for x_val in x_vals]\n\nplt.figure(figsize=(10, 6))\nplt.plot(x_vals, y_vals)\nplt.grid(True)\nplt.title(\"Solution to y''(x) + 2*y'(x) + y(x) = 0\")\nplt.xlabel('x')\nplt.ylabel('y(x)')\nplt.show()\n```\n\n### Data Analysis\n\n```\nCan you perform a clustering analysis on this dataset? Please run:\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\n\n# Create a sample dataset\nnp.random.seed(42)\nn_samples = 300\n\n# Create three clusters\ncluster1 = np.random.normal(loc=[2, 2], scale=0.5, size=(n_samples//3, 2))\ncluster2 = np.random.normal(loc=[7, 7], scale=0.5, size=(n_samples//3, 2))\ncluster3 = np.random.normal(loc=[2, 7], scale=0.5, size=(n_samples//3, 2))\n\n# Combine clusters\nX = np.vstack([cluster1, cluster2, cluster3])\n\n# Create DataFrame\ndf = pd.DataFrame(X, columns=['Feature1', 'Feature2'])\nprint(df.head())\n\n# Standardize data\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# Apply KMeans clustering\nkmeans = KMeans(n_clusters=3, random_state=42)\ndf['Cluster'] = kmeans.fit_predict(X_scaled)\n\n# Plot the clusters\nplt.figure(figsize=(10, 6))\nfor cluster_id in range(3):\n    cluster_data = df[df['Cluster'] == cluster_id]\n    plt.scatter(cluster_data['Feature1'], cluster_data['Feature2'], \n                label=f'Cluster {cluster_id}', alpha=0.7)\n\n# Plot cluster centers\ncenters = scaler.inverse_transform(kmeans.cluster_centers_)\nplt.scatter(centers[:, 0], centers[:, 1], s=200, c='red', marker='X', label='Centers')\n\nplt.title('K-Means Clustering Results')\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\nplt.legend()\nplt.grid(True)\n```\n\n### Quantum Computing\n\n![quantum example](https://raw.githubusercontent.com/YuChenSSR/pics/master/imgs/2025-03-23/UaybujIK2o3tLUtR.png)\n\n### Gallery\n\n**laser physics**:\n![laser](https://raw.githubusercontent.com/YuChenSSR/pics/master/imgs/2025-03-23/4t06m3iM17NXpW6O.png)\n\n**elliptic integral**:\n![elliptic integral](https://raw.githubusercontent.com/YuChenSSR/pics/master/imgs/2025-03-23/rvshlS2blGv7jnoi.png)\n![elliptic integral pic](https://raw.githubusercontent.com/YuChenSSR/pics/master/imgs/2025-03-23/oRGJIHgKQV8kMMHd.png)\n\n\n## Troubleshooting\n\n### Common Issues\n\n1. **Permission errors with volume mounts**\n   - Ensure the mount directory exists and has appropriate permissions\n\n2. **Plot pciture files not appearing**\n   - Check the path in your host system: `/tmp` for macOS/Linux or your temp folder for Windows\n   - Verify Docker has permissions to write to the mount location\n   - check the mcp tool's output content\n     ![find the pic](https://raw.githubusercontent.com/YuChenSSR/pics/master/imgs/2025-03-23/Ik3JZeLhdptLqgq3.png)\n     then open it in the terminal or your picture viewer.\n\n      ⭐️ ⭐️\n     I use the [iterm-mcp-server](https://github.com/ferrislucas/iterm-mcp) or other terminals' mcp servers to open the file  without interrupting your workflow. \n      ⭐️ ⭐️\n\n\n\n### Support\n\nIf you encounter issues, please open a GitHub issue with:\n1. Error messages\n2. Your operating system and Docker version\n3. Steps to reproduce the problem\n\n## License\nThis project is licensed under the MIT License.   \nFor more details, please see the LICENSE file in [this project repository](https://github.com/YuChenSSR/symbolica-mcp).\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "scipy",
        "matplotlib",
        "symbolica",
        "symbolic computing",
        "symbolica mcp",
        "visualization scientific"
      ],
      "category": "scientific-research-tools"
    },
    "acashmoney--bio-mcp": {
      "owner": "acashmoney",
      "name": "bio-mcp",
      "url": "https://github.com/acashmoney/bio-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/acashmoney.webp",
      "description": "Enhances biomedical research through advanced protein structure analysis, enabling the examination of protein active sites and the discovery of disease-related proteins. It integrates with established protein databases for reliable insights and data access.",
      "stars": 17,
      "forks": 5,
      "license": "No License",
      "language": "TypeScript",
      "updated_at": "2025-06-27T01:43:39Z",
      "readme_content": "# BioMCP: Enabling agent-based biomedical R&D\n\n[![smithery badge](https://smithery.ai/badge/@acashmoney/bio-mcp)](https://smithery.ai/server/@acashmoney/bio-mcp)\n\n\n## Overview\n\nBioMCP is a Model Context Protocol [(MCP)](https://modelcontextprotocol.io/introduction) server designed to enhance large language models with protein structure analysis capabilities. It provides tools for analyzing protein active sites and searching for disease-related proteins by interfacing with established protein databases. \n\nFuture work will be centered around enabling agents to utilize the BioMCP.\n\n## Features\n\n- **Active Site Analysis**: Examine the binding sites and functional residues of proteins using PDB IDs\n- **Disease-Protein Search**: Find protein structures associated with specific diseases or medical conditions\n- **Integrated Data Access**: Connect seamlessly with RCSB Protein Data Bank [(PDB)](https://www.rcsb.org/)\n\n## Technical Details\n\nBioMCP implements the Model Context Protocol, allowing language models to access specialized protein structure knowledge without requiring this information to be part of their training data. The server handles API connections, data formatting, and error handling to provide reliable protein structure insights.\n\n## API Endpoints\n\nBioMCP exposes two primary tools:\n\n1. `analyze-active-site`: Provides detailed information about protein binding sites using a PDB ID\n2. `search-disease-proteins`: Returns proteins related to specified diseases or medical conditions\n\n## Getting Started\n\n### Installing via Smithery\n\nTo install BioMCP for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@acashmoney/bio-mcp):\n\n```bash\nnpx -y @smithery/cli install @acashmoney/bio-mcp --client claude\n```\n\n### Manual Installation\n```bash\n# Clone the repository\ngit clone https://github.com/acashmoney/bio-mcp.git\n\n# Install dependencies\nnpm install\n\n# Start the server\nnpm start\n```\n\n## Setup Instructions\n\n### Running the MCP Inspector\n\n1. Start the BioMCP server:\n   ```bash\n   npm start\n   ```\n\n2. In a separate terminal, install the MCP Inspector globally (if not already installed):\n   ```bash\n   npm install -g @anthropic-ai/mcp-inspector\n   ```\n\n3. Launch the MCP Inspector and connect to your local BioMCP server:\n   ```bash\n   npx @modelcontextprotocol/inspector node build/index.js\n   ```\n\n4. Use the inspector interface to test tools and view responses.\n\n### Using with Claude Desktop\n\n1. Build the BioMCP server:\n   ```bash\n   npm run build\n   ```\n\n2. Configure Claude Desktop to launch the MCP server:\n\n   a. Locate your Claude Desktop config.json file (typically in your user directory)\n   \n   b. Edit the config.json to include the BioMCP server build path. Example configuration:\n   ```json\n   {\n     \"globalShortcut\": \"\",\n     \"mcpServers\": {\n       \"bio-mcp\": {\n         \"command\": \"node\",\n         \"args\": [\n           \"/path/to/your/build/index.js\"\n         ]\n       }\n     }\n   }\n   ```\n   \n   c. Replace `/path/to/your/build` with your actual path to the project directory.\n\n3. Restart Claude Desktop for the changes to take effect.\n\n4. You can now ask Claude questions that utilize the BioMCP tools:\n   - \"What are the key residues in the active site of PDB structure 6LU7?\"\n   - \"Find proteins related to Alzheimer's disease\"\n\n## Example Usage\n\nWhen integrated with a compatible language model, Bio-MCP enables queries like:\n\n- \"What are the key residues in the active site of PDB structure 6LU7?\"\n- \"Find proteins related to Alzheimer's disease\"\n\n## Requirements\n\n- Node.js 20.0.0 or higher\n- TypeScript 5.0+\n- Compatible MCP client implementation\n\n## Testing\n\nBioMCP includes a comprehensive testing suite with unit, integration, and end-to-end tests.\n\n### Running Tests\n\nRun all tests:\n```bash\nnpm test\n```\n\nRun specific test suites:\n```bash\n# Unit tests only\nnpm run test:unit\n\n# Integration tests only (API interactions)\nnpm run test:integration\n\n# End-to-end tests only\nnpm run test:e2e\n```\n\n### Linting\n\nCheck code quality:\n```bash\nnpm run lint\n```\n\nFix linting issues automatically:\n```bash\nnpm run lint:fix\n```\n\n## Roadmap\n\n- Expand level of detail for active site descriptions\n- Leverage 3-D coordinates\n- Tools for interfacing with literature\n- Tools for interfacing with computational biology models:\n  - RFdiffusion\n  - ProteinMPNN\n  - ColabFold\n  - Additional protein design and structure prediction tools\n- Agent-based research pipelines\n- Introduce client with protein visualization tools",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "proteins",
        "protein",
        "mcp",
        "protein databases",
        "advanced protein",
        "bio mcp"
      ],
      "category": "scientific-research-tools"
    },
    "agan2021--stata-prov-map": {
      "owner": "agan2021",
      "name": "stata-prov-map",
      "url": "https://github.com/agan2021/stata-prov-map",
      "imageUrl": "/freedevtools/mcp/pfp/agan2021.webp",
      "description": "Provides data and visualization tools for creating detailed Chinese provincial maps in Stata, including features for scale bars, north arrows, and various geographic lines. Supports plotting discrete and continuous variables, point plotting, and combined charts like pie charts on maps, along with geographic coordinate transformations and map customization.",
      "stars": 0,
      "forks": 0,
      "license": "GNU General Public License v3.0",
      "language": "",
      "updated_at": "2024-08-03T06:31:54Z",
      "readme_content": "# 使用 Stata 绘制中国省级地图！比例尺、指北针、秦岭-淮河线、胡焕庸线、海岸线、未定国界都有！\n\n> 欢迎前往 [哔哩哔哩](https://www.bilibili.com/video/BV12L411J7Vt/) 查看该项目的视频讲解：https://www.bilibili.com/video/BV12L411J7Vt/ \n\n根据大家之前的一些反馈，我们又重新设计了一份使用 Stata 绘制省级地图的数据，这次的地图数据比起之前的版本有如下变化：\n\n1. 指北针变得竖直了！\n2. 包含了未定国界；\n3. 添加了海岸线；\n4. 添加了秦岭-淮河线；\n5. 添加了胡焕庸线；\n6. 添加了英文版本！\n7. 比例尺和指北针的位置可以移动了！\n\n为了让大家更好的学习这份地图数据的使用，我们设计了下面的这些案例：\n\n## 2019 年中国省级行政区划\n\n我们首先看一下离散变量的填充地图如何绘制，例如 `chinaprov40_db.dta` 数据里面有个类型变量就是离散变量，我们可以使用 encode 命令生成因子变量用于填充绘图：\n\n```stata\n* 离散变量的绘制\nuse chinaprov40_db.dta, clear \nencode 类型, gen(type)\ncodebook type\n\ngrmap type using chinaprov40_coord.dta, ///\n  id(ID) osize(vvthin ...) ocolor(white ...) ///\n  clmethod(custom) clbreaks(0 1 2 3 4 5) ///\n  fcolor(\"254 212 57\" \"253 116 70\" \"138 145 151\" \"213 228 162\" \"210 175 129\") ///\n  leg(order(2 \"不统计\" 3 \"特别行政区\" 4 \"直辖市\" 5 \"省\" 6 \"自治区\" 11 \"秦岭-淮河线\" 14 \"胡焕庸线\")) ///\n  graphr(margin(medium)) ///\n  line(data(chinaprov40_line_coord.dta) by(group) size(vvthin *1 *0.5 *1.2 *0.5 *0.5 *1.2) pattern(solid ...) ///\n    color(white /// 省界颜色\n        black /// 国界线颜色\n        \"0 85 170\" /// 海岸线颜色\n        \"24 188 156\" /// 秦岭淮河线颜色\n        black /// 小地图框格颜色\n        black /// 比例尺和指北针颜色\n        \"227 26 28\" /// 胡焕庸线颜色\n        )) ///\n  polygon(data(polygon) fcolor(black) ///\n    osize(vvthin)) ///\n  label(data(chinaprov40_label) x(X) y(Y) label(cname) length(20) size(*0.8)) ///\n  ti(\"使用 Stata 绘制 2019 年中国省级行政区划\") ///\n  subti(\"绘制：微信公众号 RStata\") ///\n  caption(\"版本：使用 Stata 绘制中国省级地图数据包 4.0\", size(*0.8))\ngr export pic1.png, replace width(1200)\n```\n\n![image_20210614204201](https://mdniceczx.oss-cn-beijing.aliyuncs.com/image_20210614204201.png)\n\nline() 选项里面的 size() 和 color() 子选项分别用了 7 种尺寸和颜色，依照顺序分别对应省界线、国界线、海岸线、秦岭-淮河线、小地图框格、比例尺指北针、胡焕庸线。\n\n> 关于颜色的选择，可以使用这个网站：https://tidyfriday.cn/colors \n\n## 英文地图\n\n`chinaprov40_label.dta` 数据决定了文本标签的位置和内容，如果想绘制英文版的地图，可以使用 ename 变量：\n\n```stata\ngrmap type using chinaprov40_coord.dta, ///\n  id(ID) osize(vvthin ...) ocolor(white ...) ///\n  clmethod(custom) clbreaks(0 1 2 3 4 5) ///\n  fcolor(\"254 212 57\" \"253 116 70\" \"138 145 151\" \"213 228 162\" \"210 175 129\") ///\n  leg(order(2 \"Not within the scope of statistics\" 3 \"Special administrative region\" 4 \"Municipality directly under\" \"the Central Government\" 5 \"Province\" 6 \"Autonomous Region\" 11 \"Qinling Huaihe River Line\" 14 \"Hu Huanyong line\")) ///\n  graphr(margin(medium)) ///\n  line(data(chinaprov40_line_coord.dta) by(group) size(vvthin *1 *0.5 *1.2 *0.5 *0.5 *1.2) pattern(solid ...) ///\n    color(white /// 省界颜色\n        black /// 国界线颜色\n        \"0 85 170\" /// 海岸线颜色\n        \"24 188 156\" /// 秦岭淮河线颜色\n        black /// 小地图框格颜色\n        black /// 比例尺和指北针颜色\n        \"227 26 28\" /// 胡焕庸线颜色\n        )) ///\n  polygon(data(polygon) fcolor(black) ///\n    osize(vvthin)) ///\n  label(data(chinaprov40_label) x(X) y(Y) label(ename) length(20) size(*0.6)) ///\n  ti(\"Using Stata to draw China's provincial map in 2019\") ///\n  subti(\"WeChat Subscription: RStata\") ///\n  caption(\"Version: 4.0\", size(*0.8))\ngr export pic2.png, replace width(1200)\n```\n\n![image_20210614204222](https://mdniceczx.oss-cn-beijing.aliyuncs.com/image_20210614204222.png)\n\n## 2020 年中国各省市地区生产总值\n\n连续变量的绘制方法和离散变量的类似，通过恰当的分组和颜色填充就可以绘制出精美的图表了，下面以2020 年中国各省市地区生产总值的填充地图绘制为例：\n\n```stata\nimport delimited using \"2020年中国各省市地区生产总值.csv\", clear encoding(utf8)\ngen prov = substr(省份, 1, 6)\nsave 2020年中国各省市地区生产总值, replace \n\nuse chinaprov40_db.dta, clear \ngen prov = substr(省, 1, 6)\nmerge 1:1 prov using 2020年中国各省市地区生产总值\nreplace 地区生产总值 = -1 if missing(地区生产总值)\ngrmap 地区生产总值 using chinaprov40_coord.dta, ///\n  id(ID) osize(vvthin ...) ocolor(white ...) ///\n  clmethod(custom) clbreaks(-1 0 20000 40000 60000 80000 120000) ///\n  fcolor(gray \"224 242 241\" \"178 223 219\" \"128 203 196\" \"77 182 172\" \"38 166 154\") ///\n  leg(order(2 \"无数据\" 3 \"< 2 万亿元\" 4 \"2～4 万亿元\" 5 \"4～6 万亿元\" 6 \"6～8 万亿元\" 7 \"> 8 万亿元\")) ///\n  graphr(margin(medium)) ///\n  line(data(chinaprov40_line_coord.dta) ///\n    /// 去除秦岭淮河线(4)、胡焕庸线(7)\n    select(keep if inlist(group, 1, 2, 3, 5, 6)) ///\n    by(group) size(vvthin *1 *0.5 *0.5 *0.5) ///\n    pattern(solid ...) ///\n    color(white /// 省界颜色\n        black /// 国界线颜色\n        \"0 85 170\" /// 海岸线颜色\n        black /// 小地图框格颜色\n        black /// 比例尺和指北针颜色\n        )) ///\n  polygon(data(polygon) fcolor(black) ///\n    osize(vvthin)) ///\n  label(data(chinaprov40_label) x(X) y(Y) label(cname) length(20) size(*0.8)) ///\n  ti(\"2020 年中国各省市地区生产总值\") ///\n  subti(\"绘制：微信公众号 RStata\") ///\n  caption(\"数据来源：各地统计局\", size(*0.8))\ngr export pic3.png, replace width(1200)\n```\n\n![image_20210614204252](https://mdniceczx.oss-cn-beijing.aliyuncs.com/image_20210614204252.png)\n\n## 2013 年中国工企业分布及距离秦岭淮河的距离\n\n除了填充地图外，我们还经常会需要绘制描点地图，也就是在地图上添加坐标点，下面以2013 年中国工企业分布及距离秦岭淮河的距离可视化为例。\n\n由于提供的地图底图数据是经过投影变换的，所以如果大家想把经纬度坐标点描制在底图上，也需要对坐标进行投影变换，为此，我编写了一个网页应用帮助大家进行投影变换：https://czxb.shinyapps.io/crs-trans/\n\n另外如果你有一些 R 语言的使用基础，也可以使用 `坐标转换.R` 进行坐标转换。\n\n```stata\n* 转换坐标系\n* 转换方式一：https://czxb.shinyapps.io/crs-trans/\n* 注意事项，上传的 csv 文件应该包含数值型的 lon 和 lat 变量，观测值上限大概是 10 万个，不可多人同时使用。\nuse gq2013sample, clear \nkeep 经度 纬度\nren 经度 lon\nren 纬度 lat\nexport delimited using \"待转换.csv\", replace \n\n* 转换方式二：使用附件中的 R 脚本转换\n\n* 处理转换后的数据\nimport delimited using \"转换后的数据.csv\", clear \ngen id = _n\nsave 转换后的数据, replace \n\nuse gq2013sample, clear \ngen id = _n\nmerge 1:1 id using 转换后的数据\ndrop _m id *度\nencode 北方或南方, gen(north)\nsave pointdata, replace \n\nuse chinaprov40_db.dta, clear\nspmap using chinaprov40_coord.dta, id(ID) ///\n  ocolor(\"black\" ...) osize(vvthin ...) ///\n    line(data(chinaprov40_line_coord.dta) ///\n    /// 胡焕庸线（7）\n    select(keep if inlist(group, 1, 2, 3, 4, 5, 6)) ///\n    by(group) size(vvthin *1 *0.5 *1.5 *0.5 *0.5) ///\n    pattern(solid ...) ///\n    color(white /// 省界颜色\n        black /// 国界线颜色\n        \"0 85 170\" /// 海岸线颜色\n        \"0 85 170\" /// 秦岭淮河线颜色\n        black /// 小地图框格颜色\n        black /// 比例尺和指北针颜色\n        )) ///\n  polygon(data(polygon) fcolor(black) ///\n    osize(vvthin)) ///\n  label(data(chinaprov40_label) x(X) y(Y) label(cname) length(20) size(*0.8)) ///\n    point(data(pointdata) by(north) ///\n      fcolor(\"227 26 28%30\" \"24 188 156%30\") ///\n        x(x) y(y) ///\n        proportional(与秦岭淮河线的距离) ///\n        size(*0.1) legenda(on)) ///\n    leg(order(7 \"秦岭-淮河线\" 10 \"北方工企业\" 11 \"南方工企业\")) ///\n    ti(\"2013 年中国工业企业与秦岭-淮河线的距离\", color(black)) /// \n    subti(\"绘制：微信公众号 RStata\") ///\n    graphr(margin(medium)) ///\n    caption(\"数据来源：2013 年中国工业企业数据库，使用高德地图地理编码接口解析经纬度\", size(*0.8))\ngr export pic4.png, replace width(1200)\n```\n\n![image_20210614204304](https://mdniceczx.oss-cn-beijing.aliyuncs.com/image_20210614204304.png)\n\n## 2019 年中国各省地区生产总值 & 产业结构\n\n也可以绘制地图+饼图，其中饼图的位置可以用标签位置：\n\n```stata\nuse 各省历年GDP, clear \ndrop if 省份 == \"中国\"\n\nreplace 地区生产总值_亿元 = 地区生产总值_亿元 / 1000\nmerge m:m 省代码 using chinaprov40_db.dta\nreplace 地区生产总值_亿元 = -1 if missing(年份)\ngrmap 地区生产总值_亿元 if 年份 == 2019 | missing(年份) ///\n  using chinaprov40_coord.dta, id(ID) ///\n  clmethod(custom) clbreaks(-1 0 40 60 80 100 120) /// \n  fcolor(\"gray\" \"237 248 233\" \"199 233 192\" \"161 217 155\" \"116 196 118\" \"49 163 84\") ///\n  ocolor(\"gray\" ...) ///\n  ti(\"2019 年中国各省地区生产总值 & 产业结构\", size(*1.1)) ///\n  subtitle(\"数据来源：CSMAR经济金融数据库\") ///\n  graphr(margin(medium)) ///\n  osize(vvthin ...) ///\n  legend(size(*1.1) ///\n    order(2 \"无数据\" 3 \"< 40千亿\" ///\n      4 \"40～60千亿\" 5 \"60～80千亿\" ///\n      6 \"80～100千亿\" 7 \"> 100千亿\" ///\n      14 \"第一产业\" 15 \"第二产业\" 16 \"第三产业\")) ///\n  caption(\"绘制：微信公众号 RStata\", size(*0.8)) ///\n  line(data(chinaprov40_line_coord.dta) ///\n    /// 去除秦岭淮河线(4) 胡焕庸线（7）\n    select(keep if inlist(group, 1, 2, 3, 5, 6)) ///\n    by(group) size(vvthin *1 *0.5 *0.5 *0.5) ///\n    pattern(solid ...) ///\n    color(white /// 省界颜色\n        black /// 国界线颜色\n        \"0 85 170\" /// 海岸线颜色\n        black /// 小地图框格颜色\n        black /// 比例尺和指北针颜色\n        )) ///\n  polygon(data(polygon) fcolor(black) ///\n    osize(vvthin)) ///\n  label(data(chinaprov40_label) x(X) y(Y) label(cname) length(20) size(*0.8)) ///\n  diagram(data(piedata) x(X) y(Y) v(第一产业占GDP比重_百分比 第二产业占GDP比重_百分比 第三产业占GDP比重_百分比) ///\n    type(pie) legenda(on) os(vvthin) ///\n      size(1.5) fc(\"102 194 165\" \"252 141 98\" \"229 196 148\") ///\n      oc(\"102 194 165\" \"252 141 98\" \"229 196 148\"))\n\ngr export \"pic5.png\", replace width(1200)\n```\n\n![image_20210614204357](https://mdniceczx.oss-cn-beijing.aliyuncs.com/image_20210614204357.png)\n\n另外饼图也可以换成框架矩形图：\n\n```stata\ngrmap 地区生产总值_亿元 if 年份 == 2019 | missing(年份) ///\n  using chinaprov40_coord.dta, id(ID) ///\n  clmethod(custom) clbreaks(-1 0 40 60 80 100 120) /// \n  fcolor(\"gray\" \"237 248 233\" \"199 233 192\" \"161 217 155\" \"116 196 118\" \"49 163 84\") ///\n  ocolor(\"gray\" ...) ///\n  ti(\"2019 年中国各省地区生产总值 & 第一产业比重\", size(*1.1)) ///\n  subtitle(\"数据来源：CSMAR经济金融数据库\") ///\n  graphr(margin(medium)) ///\n  osize(vvthin ...) ///\n  legend(size(*1.1) ///\n    order(2 \"无数据\" 3 \"< 40千亿\" ///\n      4 \"40～60千亿\" 5 \"60～80千亿\" ///\n      6 \"80～100千亿\" 7 \"> 100千亿\" ///\n      15 \"第一产业比重\")) ///\n  caption(\"绘制：微信公众号 RStata\", size(*0.8)) ///\n  line(data(chinaprov40_line_coord.dta) ///\n    /// 去除秦岭淮河线(4) 胡焕庸线（7）\n    select(keep if inlist(group, 1, 2, 3, 5, 6)) ///\n    by(group) size(vvthin *1 *0.5 *0.5 *0.5) ///\n    pattern(solid ...) ///\n    color(white /// 省界颜色\n        black /// 国界线颜色\n        \"0 85 170\" /// 海岸线颜色\n        black /// 小地图框格颜色\n        black /// 比例尺和指北针颜色\n        )) ///\n  polygon(data(polygon) fcolor(black) ///\n    osize(vvthin)) ///\n  diagram(data(piedata) x(X) y(Y) v(第一产业占GDP比重_百分比) ///\n    type(frect) legenda(on) os(vvthin) ///\n      size(1.5) fc(\"252 141 98\") ///\n      oc(\"252 141 98\") refsize(none))\ngr export \"pic6.png\", replace width(1200)\n```\n\n![image_20210614204522](https://mdniceczx.oss-cn-beijing.aliyuncs.com/image_20210614204522.png)\n\n## 各省人口密度\n\n最后我们再演示下胡焕庸线的用法：\n\n```stata\nuse 中国人口空间分布省级面板数据集.dta, clear \nren 省份 省\nmerge m:1 省 using chinaprov40_db.dta\nkeep if 年份 == 2015 | missing(年份)\nreplace 均值 = -1 if missing(均值)\ngrmap 均值 using chinaprov40_coord.dta, ///\n  id(ID) osize(vvthin ...) ocolor(white ...) ///\n  clmethod(custom) clbreaks(-1 0 100 1000 2000 3000 4000) ///\n  fcolor(gray \"224 242 241\" \"178 223 219\" \"128 203 196\" \"77 182 172\" \"38 166 154\") ///\n  leg(order(2 \"无数据\" 3 \"< 100 人/平方公里\" 4 \"100～1000 人/平方公里\" 5 \"1000～2000 人/平方公里\" 6 \"2000～3000 人/平方公里\" 7 \"> 3000 人/平方公里\" 14 \"胡焕庸线\")) ///\n  graphr(margin(medium)) ///\n  line(data(chinaprov40_line_coord.dta) ///\n    /// 去除秦岭淮河线(4)\n    select(keep if inlist(group, 1, 2, 3, 5, 6, 7)) ///\n    by(group) size(vvthin *1 *0.5 *0.5 *0.5 *1.2) ///\n    pattern(solid ...) ///\n    color(white /// 省界颜色\n        black /// 国界线颜色\n        \"0 85 170\" /// 海岸线颜色\n        black /// 小地图框格颜色\n        black /// 比例尺和指北针颜色\n        \"227 26 28\" /// 胡焕庸线颜色\n        )) ///\n  polygon(data(polygon) fcolor(black) ///\n    osize(vvthin)) ///\n  label(data(chinaprov40_label) x(X) y(Y) label(cname) length(20) size(*0.8)) ///\n  ti(\"2015 年中国各省平均人口密度\") ///\n  subti(\"绘制：微信公众号 RStata\") ///\n  caption(\"数据来源：中国科学院资源环境科学与数据中心\", size(*0.8))\ngr export pic7.png, replace width(1200)\n```\n\n![image_20210614204537](https://mdniceczx.oss-cn-beijing.aliyuncs.com/image_20210614204537.png)\n\n## 调整指北针和比例尺的位置\n\n指北针和比例尺都包括三个部分，line、polygon 和 label，所以我们调整指北针和比例尺的位置也需要修改这三个数据里面的对应坐标，在调整坐标前我们可以给绘图语句加上 freestyle 选项来帮助确定要移动的数值：\n\n![image_20210614204555](https://mdniceczx.oss-cn-beijing.aliyuncs.com/image_20210614204555.png)\n\n然后就可以通过修改数据来调整指北针和比例尺的位置了：\n\n```stata\n* 移动指北针的位置到右上方\nuse chinaprov40_line_db.dta, clear\n* 指北针对应的 ID 是 40 和 41\nuse chinaprov40_line_coord.dta, clear\nreplace _X = _X + 3000000 if inlist(_ID, 40, 41)\nreplace _Y = _Y + 4000000 if inlist(_ID, 40, 41)\nsave chinaprov40_line_coord2.dta, replace \n\nuse polygon, clear\nreplace _X = _X + 3000000 if _ID == 38\nreplace _Y = _Y + 4000000 if _ID == 38\nsave polygon2, replace\n\nuse chinaprov40_label, clear\nreplace X = X + 3000000 if cname == \"N\"\nreplace Y = Y + 4000000 if cname == \"N\"\nsave chinaprov40_label2, replace \n\nuse chinaprov40_db.dta, clear \nencode 类型, gen(type)\ngrmap type using chinaprov40_coord.dta, ///\n  id(ID) osize(vvthin ...) ocolor(white ...) ///\n  clmethod(custom) clbreaks(0 1 2 3 4 5) ///\n  fcolor(\"254 212 57\" \"253 116 70\" \"138 145 151\" \"213 228 162\" \"210 175 129\") ///\n  leg(order(2 \"不统计\" 3 \"特别行政区\" 4 \"直辖市\" 5 \"省\" 6 \"自治区\" 11 \"秦岭-淮河线\" 14 \"胡焕庸线\")) ///\n  graphr(margin(medium)) ///\n  line(data(chinaprov40_line_coord2.dta) by(group) size(vvthin *1 *0.5 *1.2 *0.5 *0.5 *1.2) pattern(solid ...) ///\n    color(white /// 省界颜色\n        black /// 国界线颜色\n        \"0 85 170\" /// 海岸线颜色\n        \"24 188 156\" /// 秦岭淮河线颜色\n        black /// 小地图框格颜色\n        black /// 比例尺和指北针颜色\n        \"227 26 28\" /// 胡焕庸线颜色\n        )) ///\n  polygon(data(polygon2) fcolor(black) ///\n    osize(vvthin)) ///\n  label(data(chinaprov40_label2) x(X) y(Y) label(cname) length(20) size(*0.8)) ///\n  ti(\"使用 Stata 绘制 2019 年中国省级行政区划\") ///\n  subti(\"绘制：微信公众号 RStata\") ///\n  caption(\"版本：使用 Stata 绘制中国省级地图数据包 4.0\", size(*0.8))\ngr export pic8.png, replace width(1200)\n```\n\n\n![image_20210614204623](https://mdniceczx.oss-cn-beijing.aliyuncs.com/image_20210614204623.png)\n\n\n```stata\n* 调节比例尺的位置（微微上移）\nuse chinaprov40_line_db.dta, clear\n* 比例尺对应的 ID 是 42 和 43\nuse chinaprov40_line_coord2.dta, clear\nreplace _Y = _Y + 200000 if inlist(_ID, 42, 43)\nsave chinaprov40_line_coord3.dta, replace \n\nuse polygon2, clear\nreplace _Y = _Y + 200000 if _ID == 39\nsave polygon3, replace\n\nuse chinaprov40_label2, clear\nreplace Y = Y + 200000 if cname == \"1000km\"\nsave chinaprov40_label3, replace \n\nuse chinaprov40_db.dta, clear \nencode 类型, gen(type)\ngrmap type using chinaprov40_coord.dta, ///\n  id(ID) osize(vvthin ...) ocolor(white ...) ///\n  clmethod(custom) clbreaks(0 1 2 3 4 5) ///\n  fcolor(\"254 212 57\" \"253 116 70\" \"138 145 151\" \"213 228 162\" \"210 175 129\") ///\n  leg(order(2 \"不统计\" 3 \"特别行政区\" 4 \"直辖市\" 5 \"省\" 6 \"自治区\" 11 \"秦岭-淮河线\" 14 \"胡焕庸线\")) ///\n  graphr(margin(medium)) ///\n  line(data(chinaprov40_line_coord3.dta) by(group) size(vvthin *1 *0.5 *1.2 *0.5 *0.5 *1.2) pattern(solid ...) ///\n    color(white /// 省界颜色\n        black /// 国界线颜色\n        \"0 85 170\" /// 海岸线颜色\n        \"24 188 156\" /// 秦岭淮河线颜色\n        black /// 小地图框格颜色\n        black /// 比例尺和指北针颜色\n        \"227 26 28\" /// 胡焕庸线颜色\n        )) ///\n  polygon(data(polygon3) fcolor(black) ///\n    osize(vvthin)) ///\n  label(data(chinaprov40_label3) x(X) y(Y) label(cname) length(20) size(*0.8)) ///\n  ti(\"使用 Stata 绘制 2019 年中国省级行政区划\") ///\n  subti(\"绘制：微信公众号 RStata\") ///\n  caption(\"版本：使用 Stata 绘制中国省级地图数据包 4.0\", size(*0.8))\ngr export pic9.png, replace width(1200)\n```\n\n![image_20210614204639](https://mdniceczx.oss-cn-beijing.aliyuncs.com/image_20210614204639.png)\n\n\n------------\n\n<h4 align=\"center\">\n\nCode of Conduct\n\n</h4>\n\n<h6 align=\"center\">\n\nPlease note that this project is released with a [Contributor Code of\nConduct](CODE_OF_CONDUCT.md).<br>By participating in this project you\nagree to abide by its terms.\n\n</h6>\n\n<h4 align=\"center\">\n\nLicense\n\n</h4>\n\n<h6 align=\"center\">\n\nMIT © 微信公众号 RStata\n\n</h6>\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "charts",
        "stata",
        "maps",
        "maps stata",
        "provincial maps",
        "charts maps"
      ],
      "category": "scientific-research-tools"
    },
    "akalaric--mcp-wolframalpha": {
      "owner": "akalaric",
      "name": "mcp-wolframalpha",
      "url": "https://github.com/akalaric/mcp-wolframalpha",
      "imageUrl": "/freedevtools/mcp/pfp/akalaric.webp",
      "description": "Integrates Wolfram Alpha into chat applications to perform computational queries and retrieve structured knowledge, enhancing conversational capabilities with large language models. Supports a modular architecture for easy extension and multiple client interactions.",
      "stars": 44,
      "forks": 7,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-10-03T21:33:50Z",
      "readme_content": "# MCP Wolfram Alpha (Server + Client)\nSeamlessly integrate Wolfram Alpha into your chat applications.\n\nThis project implements an MCP (Model Context Protocol) server designed to interface with the Wolfram Alpha API. It enables chat-based applications to perform computational queries and retrieve structured knowledge, facilitating advanced conversational capabilities.\n\nIncluded is an MCP-Client example utilizing Gemini via LangChain, demonstrating how to connect large language models to the MCP server for real-time interactions with Wolfram Alpha’s knowledge engine.\n\n[![Ask DeepWiki](https://deepwiki.com/badge.svg)](https://deepwiki.com/akalaric/mcp-wolframalpha)\n---\n\n## Features\n\n-  **Wolfram|Alpha Integration** for math, science, and data queries.\n\n-  **Modular Architecture** Easily extendable to support additional APIs and functionalities.\n\n-  **Multi-Client Support** Seamlessly handle interactions from multiple clients or interfaces.\n\n-  **MCP-Client example** using Gemini (via LangChain).\n-  **UI Support** using Gradio for a user-friendly web interface to interact with Google AI and Wolfram Alpha MCP server.\n\n---\n\n##  Installation\n\n\n### Clone the Repo\n   ```bash\n   git clone https://github.com/ricocf/mcp-wolframalpha.git\n\n   cd mcp-wolframalpha\n   ```\n  \n\n### Set Up Environment Variables\n\nCreate a .env file based on the example:\n\n- WOLFRAM_API_KEY=your_wolframalpha_appid\n\n- GeminiAPI=your_google_gemini_api_key *(Optional if using Client method below.)*\n\n### Install Requirements\n   ```bash\n   pip install -r requirements.txt\n   ```\n\n  Install the required dependencies with uv:\n  Ensure [`uv`](https://github.com/astral-sh/uv) is installed.\n\n   ```bash\n   uv sync\n   ```\n\n### Configuration\n\nTo use with the VSCode MCP Server:\n1.  Create a configuration file at `.vscode/mcp.json` in your project root.\n2.  Use the example provided in `configs/vscode_mcp.json` as a template.\n3.  For more details, refer to the [VSCode MCP Server Guide](https://sebastian-petrus.medium.com/vscode-mcp-server-42286eed3ee7).\n\nTo use with Claude Desktop:\n```json\n{\n  \"mcpServers\": {\n    \"WolframAlphaServer\": {\n      \"command\": \"python3\",\n      \"args\": [\n        \"/path/to/src/core/server.py\"\n      ]\n    }\n  }\n}\n```\n## Client Usage Example\n\nThis project includes an LLM client that communicates with the MCP server.\n\n#### Run with Gradio UI\n- Required: GeminiAPI\n- Provides a local web interface to interact with Google AI and Wolfram Alpha.\n- To run the client directly from the command line:\n```bash\npython main.py --ui\n```\n#### Docker\nTo build and run the client inside a Docker container:\n```bash\ndocker build -t wolframalphaui -f .devops/ui.Dockerfile .\n\ndocker run wolframalphaui\n```\n#### UI\n- Intuitive interface built with Gradio to interact with both Google AI (Gemini) and the Wolfram Alpha MCP server.\n- Allows users to switch between Wolfram Alpha, Google AI (Gemini), and query history.\n  \n\n\n#### Run as CLI Tool\n- Required: GeminiAPI\n- To run the client directly from the command line:\n```bash\npython main.py\n```\n#### Docker\nTo build and run the client inside a Docker container:\n```bash\ndocker build -t wolframalpha -f .devops/llm.Dockerfile .\n\ndocker run -it wolframalpha\n```\n\n## Contact\n\nFeel free to give feedback. The e-mail address is shown if you execute this in a shell:\n\n```sh\nprintf \"\\x61\\x6b\\x61\\x6c\\x61\\x72\\x69\\x63\\x31\\x40\\x6f\\x75\\x74\\x6c\\x6f\\x6f\\x6b\\x2e\\x63\\x6f\\x6d\\x0a\"\n```",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "wolfram",
        "wolframalpha",
        "chat",
        "alpha chat",
        "chat applications",
        "wolfram alpha"
      ],
      "category": "scientific-research-tools"
    },
    "alti3--stk-mcp": {
      "owner": "alti3",
      "name": "stk-mcp",
      "url": "https://github.com/alti3/stk-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/alti3.webp",
      "description": "Facilitates interaction between AI models and Ansys/AGI STK for advanced simulation tasks, enabling control over STK Desktop using natural language. Allows users to create simulation scenarios and manage satellite configurations through an intuitive MCP interface.",
      "stars": 16,
      "forks": 2,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-09-23T06:47:53Z",
      "readme_content": "# STK-MCP\n\n[![Python Version](https://img.shields.io/badge/python-3.12%2B-blue.svg)](https://www.python.org/downloads/) [![MCP Version](https://img.shields.io/pypi/v/mcp.svg)](https://pypi.org/project/mcp/)\n\nSTK-MCP is an MCP (Model Context Protocol) server designed to enable Large Language Models (LLMs) or other MCP clients to interact with [Ansys/AGI STK](https://www.ansys.com/products/missions/ansys-stk) (Systems Tool Kit) - the leading Digital Mission Engineering software.\n\nThis project allows controlling STK Desktop via MCP tools, leveraging the STK Python API. It utilizes `FastMCP` from the official [MCP Python SDK](https://github.com/modelcontextprotocol/python-sdk). The server logic resides within the `src/stk_mcp` package.\n\n## Overview\n\nThe primary goal of this project is to bridge the gap between programmatic or natural language interaction (via LLMs) and the powerful simulation capabilities of STK. By exposing STK functionalities through an MCP server, users can command STK simulations using MCP clients or potentially LLM-driven applications.\n\nThe MCP server, defined in `src/stk_mcp/server.py`, manages STK interaction and exposes functionality, showcasing how to:\n*   Connect to a running STK Desktop instance or launch a new one via the server's lifecycle management (`src/stk_mcp/stk_logic/core.py`).\n*   Expose STK operations as MCP tools (defined in `src/stk_mcp/tools/`).\n*   Create and configure STK scenarios using the `setup_scenario` tool.\n*   Programmatically add and configure Satellite objects using the `create_satellite` tool.\n*   Define satellite orbits using common orbital parameters.\n*   Utilize the TwoBody propagator for orbit simulation within the tools.\n\n## Features (MCP Server)\n\n*   **MCP Server Implementation:** Uses `FastMCP` within `src/stk_mcp/server.py`.\n*   **Modular Structure:** Code organized into server setup (`server.py`), STK interaction logic (`stk_logic/`), and MCP tools (`tools/`).\n*   **STK Desktop Integration:** Automatically connects to or starts STK 12 (requires Windows) when the server starts, managed via MCP lifespan.\n*   **Scenario Management Tool (`setup_scenario`):** Creates new scenarios, sets analysis time periods, and closes any pre-existing scenario.\n*   **Satellite Creation Tool (`create_satellite`):** Adds Satellite objects to the currently active scenario.\n*   **Orbit Definition:** Configures satellite orbits using Apogee/Perigee Altitude, RAAN, and Inclination.\n*   **TwoBody Propagation:** Sets up and runs the TwoBody propagator for created satellites.\n\n## Prerequisites\n\n*   **Operating System:** Windows (due to STK Desktop and `win32com` dependency).\n*   **Python:** Version 3.12 or higher.\n*   **Ansys/AGI STK:** Version 12.x Desktop installed.\n*   **STK Python API:** The `agi.stk12` Python wheel corresponding to your STK installation must be installed. This typically involves:\n    *   Locating the wheel file (e.g., `agi.stk12-py3-none-any.whl`) within your STK installation directory (often under `CodeSamples\\Automation\\Python`).\n    *   Installing it using pip: `uv pip install path/to/agi.stk12-py3-none-any.whl`\n*   **Project Dependencies:** Requires the `mcp` library (installed via `uv sync`).\n\n## Installation\n\n1.  **Clone the repository:**\n    ```bash\n    git clone <repository-url>\n    cd stk-mcp\n    ```\n2.  **Ensure Prerequisites are met:** Install Python, STK Desktop, and the STK Python API wheel as described above.\n3.  **Install project dependencies:**\n    ```bash\n    # Install dependencies defined in pyproject.toml (includes mcp[cli])\n    uv sync\n    ```\n\n## Usage (MCP Server)\n\nThe MCP server logic is now structured within the `src/stk_mcp` directory. You interact with it using an MCP client or development tools.\n\n1.  **Ensure STK Desktop is closed** (the server will launch its own instance via the lifespan manager).\n2.  **Run the MCP server in development mode:**\n    *   **Option A (Using the helper script):**\n        ```bash\n        python scripts/run_dev.py\n        ```\n    *   **Option B (Directly using `mcp` command from the project root):**\n        ```bash\n        mcp dev src/stk_mcp/server.py\n        ```\n    *   The server will start, attempt to launch/connect to STK, and print logs.\n    *   STK Desktop should become visible.\n    *   The server listens for MCP connections.\n3.  **Interact with the Server (Example using MCP Inspector):**\n    *   Open the MCP Inspector URL provided in the console (e.g., `http://127.0.0.1:8765`).\n    *   Find the \"STK Control\" server.\n    *   Use the \"Tools\" section to execute `setup_scenario` and `create_satellite` as before.\n4.  **Stop the Server:** Press `Ctrl+C` in the terminal where the server is running. The lifespan manager will attempt to close STK.\n\n## MCP Tools Available\n\n*   **`setup_scenario`**: (Defined in `src/stk_mcp/tools/scenario.py`)\n    *   Description: Creates/Configures an STK Scenario. Closes any existing scenario first.\n    *   Arguments: `scenario_name` (str, default: \"MCP_STK_Scenario\"), `start_time` (str, default: \"20 Jan 2020 17:00:00.000\"), `duration_hours` (float, default: 48.0).\n*   **`create_satellite`**: (Defined in `src/stk_mcp/tools/satellite.py`)\n    *   Description: Creates or modifies an STK satellite. Assumes a scenario is active.\n    *   Arguments: `name` (str), `apogee_alt_km` (float), `perigee_alt_km` (float), `raan_deg` (float), `inclination_deg` (float).\n\n## Future Development (MCP Server & Expanded Automation)\n\nPlanned enhancements include:\n\n*   **Adding Ground Locations:** MCP tool to add a Facility or Place object.\n*   **Resource Endpoints:** Add MCP resources (e.g., `get_scenario_details`, `list_objects`) to query STK state.\n*   **Enhanced Satellite/Object Definition:** More comprehensive configuration options.\n*   **Access Analysis Tool:** Compute and report access between objects.\n*   **Error Handling:** More robust error reporting back to the MCP client.\n\n## Dependencies\n\n*   `agi.stk12`: For interacting with STK Desktop (Requires manual installation from STK).\n*   `mcp[cli]>=1.6.0`: Model Context Protocol library.\n*   `pywin32`: Automatically installed on Windows; used for COM interactions via `win32com.client`.\n\n## Limitations\n\n*   **Windows Only:** Relies heavily on STK Desktop automation via COM.\n*   **STK Desktop Required:** Does not currently support STK Engine.\n*   **Basic Functionality:** Currently limited to scenario setup and basic satellite creation.\n*   **Single Scenario Focus:** Assumes interaction with a single active scenario managed by the `setup_scenario` tool.\n\n## Contributing\n\nContributions are welcome! Please see the [CONTRIBUTING.md](CONTRIBUTING.md) file for guidelines.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "stk",
        "ai",
        "alti3",
        "alti3 stk",
        "agi stk",
        "stk advanced"
      ],
      "category": "scientific-research-tools"
    },
    "angrysky56--mcp-logic": {
      "owner": "angrysky56",
      "name": "mcp-logic",
      "url": "https://github.com/angrysky56/mcp-logic",
      "imageUrl": "/freedevtools/mcp/pfp/angrysky56.webp",
      "description": "Provides automated reasoning capabilities through Prover9 and Mace4, enabling logical theorem proving and model verification. Facilitates formal validation of knowledge representations and implications via a clean MCP interface.",
      "stars": 33,
      "forks": 7,
      "license": "MIT License",
      "language": "Shell",
      "updated_at": "2025-10-03T23:37:00Z",
      "readme_content": "# MCP-Logic\n\nAn MCP server providing automated reasoning capabilities using Prover9/Mace4 for AI systems. This server enables logical theorem proving and logical model verification through a clean MCP interface.\n\n## Design Philosophy\n\nMCP-Logic bridges the gap between AI systems and formal logic by providing a robust interface to Prover9/Mace4. What makes it special:\n\n- **AI-First Design**: Built specifically for AI systems to perform automated reasoning\n- **Knowledge Validation**: Enables formal verification of knowledge representations and logical implications\n- **Clean Integration**: Seamless integration with the Model Context Protocol (MCP) ecosystem\n- **Deep Reasoning**: Support for complex logical proofs with nested quantifiers and multiple premises\n- **Real-World Applications**: Particularly useful for validating AI knowledge models and reasoning chains\n\n## Features\n\n- Seamless integration with Prover9 for automated theorem proving\n- Support for complex logical formulas and proofs\n- Built-in syntax validation\n- Clean MCP server interface\n- Extensive error handling and logging\n- Support for knowledge representation and reasoning about AI systems\n\n## Quick Example\n\n![image](https://github.com/user-attachments/assets/42756e3d-c2fa-475f-8e8a-25f7e444b2a4)\n\n```python\n# Prove that understanding + context leads to application\nresult = await prove(\n    premises=[\n        \"all x all y (understands(x,y) -> can_explain(x,y))\",\n        \"all x all y (can_explain(x,y) -> knows(x,y))\",\n        \"all x all y (knows(x,y) -> believes(x,y))\",\n        \"all x all y (believes(x,y) -> can_reason_about(x,y))\",\n        \"all x all y (can_reason_about(x,y) & knows_context(x,y) -> can_apply(x,y))\",\n        \"understands(system,domain)\",\n        \"knows_context(system,domain)\"\n    ],\n    conclusion=\"can_apply(system,domain)\"\n)\n# Returns successful proof!\n```\n\n![image](https://github.com/user-attachments/assets/61cecc1f-9ba1-4586-a6a2-83823088f763)\n\n## Installation\n\n### Prerequisites\n\n- Python 3.10+\n- UV package manager\n- Git for cloning the repository\n- CMake and build tools (for building LADR/Prover9)\n\n### Setup\n\nClone this repository\n\n```bash\ngit clone https://github.com/angrysky56/mcp-logic\ncd mcp-logic\n```\n\nRun the setup script:\nWindows run:\n\n```bash\nwindows-setup-mcp-logic.bat\n```\n\nLinux/macOS:\n\n```bash\nchmod +x linux-setup-script.sh\n./linux-setup-script.sh\n```\n\nThe setup script:\n\n- Checks for dependencies (git, cmake, build tools)\n- Downloads LADR (Prover9/Mace4) from the external repository: [laitep/LADR](https://github.com/laitep/ladr.git)\n- Builds the LADR library to create Prover9 binaries in the ladr/bin directory\n- Creates a Python virtual environment\n- Sets up configuration files for running with or without Docker\n\nIMPORTANT: The LADR directory is not included in the repository itself and will be installed through the setup script or manually.\n\n### Using Docker- no idea if this is working right, mainly designed for direct use with Claude Desktop\n\nIf you prefer to run with Docker this script:\n\n- Finds an available port\n- Activates the virtual environment\n- Runs the server with the correct paths to the installed Prover9\n\n```bash\n# Linux/macOS\n./run-mcp-logic.sh\n```\n\n```bash\n# Windows\nrun-mcp-logic.bat\n```\n\nThese scripts will build and run a Docker container with the necessary environment.\n\n### Claude Desktop Integration\n\nTo use MCP-Logic with Claude Desktop, use this configuration:\n\n```json\n{\n  \"mcpServers\": {\n    \"mcp-logic\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"--directory\", \n        \"/path/to/mcp-logic/src/mcp_logic\",\n        \"run\", \n        \"mcp_logic\", \n        \"--prover-path\", \n        \"/path/to/mcp-logic/ladr/bin\"\n      ]\n    }\n  }\n}\n```\n\nReplace \"/path/to/mcp-logic\" with your actual repository path.\n\n## Available Tools\n\n![image](https://github.com/user-attachments/assets/a78ba32a-c0a8-4e44-9429-b2c9af57129d)\n\n### prove\n\nRun logical proofs using Prover9:\n\n```json\n{\n  \"tool\": \"prove\",\n  \"arguments\": {\n    \"premises\": [\n      \"all x (man(x) -> mortal(x))\",\n      \"man(socrates)\"\n    ],\n    \"conclusion\": \"mortal(socrates)\"\n  }\n}\n```\n\n### check-well-formed\n\nValidate logical statement syntax:\n\n```json\n{\n  \"tool\": \"check-well-formed\",\n  \"arguments\": {\n    \"statements\": [\n      \"all x (man(x) -> mortal(x))\",\n      \"man(socrates)\"\n    ]\n  }\n}\n```\n\n## Documentation\n\nSee the [Documents](./Documents) folder for detailed analysis and examples:\n\n- [Knowledge to Application](./Documents/KnowledgeToApplication.md): A formal logical analysis of understanding and practical application in AI systems\n\n## Project Structure\n\n```markdown\nmcp-logic/\n├── src/\n│   └── mcp_logic/\n│       └── server.py   # Main MCP server implementation\n├── tests/\n│   ├── test_proofs.py  # Core functionality tests\n│   └── test_debug.py   # Debug utilities\n├── Documents/          # Analysis and documentation\n├── pyproject.toml      # Python package config\n├── setup-script.sh     # Setup script (installs LADR & dependencies)\n├── run-mcp-logic.sh    # Docker-based run script (Linux/macOS)\n├── run-mcp-logic.bat   # Docker-based run script (Windows)\n├── run-mcp-logic-local.sh # Local run script (no Docker)\n└── README.md           # This file\n```\n\nNote: After running setup-script.sh, a \"ladr\" directory will be created containing the Prover9 binaries, but this directory is not included in the repository itself.\n\n## Development\n\nRun tests:\n\n```bash\nuv pip install pytest\nuv run pytest\n```\n\n## License\n\nMIT\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "prover9",
        "mcp",
        "logic",
        "mcp logic",
        "automated reasoning",
        "logic provides"
      ],
      "category": "scientific-research-tools"
    },
    "angrysky56--mcp-rocq": {
      "owner": "angrysky56",
      "name": "mcp-rocq",
      "url": "https://github.com/angrysky56/mcp-rocq",
      "imageUrl": "/freedevtools/mcp/pfp/angrysky56.webp",
      "description": "Provides advanced logical reasoning capabilities through integration with the Coq proof assistant, enabling automated dependent type checking, inductive type definitions, and property proving with custom tactics and automation.",
      "stars": 6,
      "forks": 3,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-07-23T14:43:03Z",
      "readme_content": "# MCP-RoCQ (Coq Reasoning Server)\n\n# Currently shows tools but Claude can't use it properly for some reason- invalid syntax generally seems the issue but there could be something else.\n\nThere may be a better way to set this up with the coq cli or something.\nAnyone want to try and fix it who knows what they are doing would be great.\n\nMCP-RoCQ is a Model Context Protocol server that provides advanced logical reasoning capabilities through integration with the Coq proof assistant. It enables automated dependent type checking, inductive type definitions, and property proving with both custom tactics and automation.\n\n## Features\n\n- **Automated Dependent Type Checking**: Verify terms against complex dependent types\n- **Inductive Type Definition**: Define and automatically verify custom inductive data types\n- **Property Proving**: Prove logical properties using custom tactics and automation\n- **XML Protocol Integration**: Reliable structured communication with Coq\n- **Rich Error Handling**: Detailed feedback for type errors and failed proofs\n\n## Installation\n\n1. Install the Coq Platform 8.19 (2024.10)\n\nCoq is a formal proof management system. It provides a formal language to write mathematical definitions, executable algorithms and theorems together with an environment for semi-interactive development of machine-checked proofs.\n\n[https://github.com/coq/platform](https://github.com/coq/platform)\n\n2. Clone this repository:\n\n```bash\ngit clone https://github.com/angrysky56/mcp-rocq.git\n```\n\ncd to the repo\n\n```bash\nuv venv\n./venv/Scripts/activate\nuv pip install -e .\n```\n\n# JSON for the Claude App or mcphost config- set your paths according to how you installed coq and the repository.\n\n```json\n    \"mcp-rocq\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"--directory\",\n        \"F:/GithubRepos/mcp-rocq\",\n        \"run\",\n        \"mcp_rocq\",\n        \"--coq-path\",\n        \"F:/Coq-Platform~8.19~2024.10/bin/coqtop.exe\",\n        \"--lib-path\",\n        \"F:/Coq-Platform~8.19~2024.10/lib/coq\"\n      ]\n    },\n```\n\n\n# This might work- I got it going with uv and most of this could be hallucinatory though:\n\n3. Install dependencies:\n\n```bash\npip install -r requirements.txt\n```\n\n## Usage\n\nThe server provides three main capabilities:\n\n### 1. Type Checking\n\n```python\n{\n    \"tool\": \"type_check\",\n    \"args\": {\n        \"term\": \"<term to check>\",\n        \"expected_type\": \"<type>\",\n        \"context\": [\"relevant\", \"modules\"] \n    }\n}\n```\n\n### 2. Inductive Types\n\n```python\n{\n    \"tool\": \"define_inductive\",\n    \"args\": {\n        \"name\": \"Tree\",\n        \"constructors\": [\n            \"Leaf : Tree\",\n            \"Node : Tree -> Tree -> Tree\"\n        ],\n        \"verify\": true\n    }\n}\n```\n\n### 3. Property Proving\n\n```python\n{\n    \"tool\": \"prove_property\",\n    \"args\": {\n        \"property\": \"<statement>\",\n        \"tactics\": [\"<tactic sequence>\"],\n        \"use_automation\": true\n    }\n}\n```\n\n## License\n\nThis project is licensed under the MIT License - see the LICENSE file for details.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "rocq",
        "automated",
        "mcp",
        "rocq provides",
        "mcp rocq",
        "advanced logical"
      ],
      "category": "scientific-research-tools"
    },
    "blake365--macrostrat-mcp": {
      "owner": "blake365",
      "name": "macrostrat-mcp",
      "url": "https://github.com/blake365/macrostrat-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/blake365.webp",
      "description": "Query and analyze comprehensive geologic data using the Macrostrat API, providing access to geologic units, minerals, timescales, and more. Create visualizations based on geological data to enhance understanding.",
      "stars": 3,
      "forks": 5,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-08-26T01:43:52Z",
      "readme_content": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/blake365-macrostrat-mcp-badge.png)](https://mseep.ai/app/blake365-macrostrat-mcp)\n\n# macrostrat-mcp\nMCP server for Macrostrat API\n\n[![smithery badge](https://smithery.ai/badge/@blake365/macrostrat-mcp)](https://smithery.ai/server/@blake365/macrostrat-mcp)\n\n<a href=\"https://glama.ai/mcp/servers/v67anfiq0s\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/v67anfiq0s/badge\" alt=\"Macrostrat Server MCP server\" />\n</a>\n\nAn MCP server implementation for providing access to the [Macrostrat API](https://macrostrat.org/api) within [Claude Desktop](https://claude.ai/download).\n\n## Overview\n\nThis Model Context Protocol (MCP) server enables AI assistants and applications to:\n\n- Query comprehensive geologic data and information provided by the Macrostrat API\n- Access geologic units, columns, minerals, timescales, and more\n- Get geologic map tiles that the LLM can interpret (questionable utility, but might be useful for some applications)\n- Analyze geologic data to improve geological understanding\n- Create graphics and perform data analysis from the data\n\nUsers can run queries using natural language.\n\n## Sample Output\n\n**Q. What is the geologic history of Nashville, TN?**\n\nA. \nLet me check the geological data for Nashville's location.\n\nView Result from find-units from macrostrat (local)\n```json\n{\n  \"lat\": 36.1627,\n  \"lng\": -86.7816,\n  \"responseType\": \"long\"\n}\n```\n\n### Geological History of Nashville, Tennessee\n1. Mesoproterozoic Basement (1.6-1.1 billion years ago)\n- Igneous basement rocks (granite and rhyolite)\n- Part of the ancient North American craton\n\n2. Early-Middle Cambrian (518-504 million years ago)\n- Rome Formation: Mixed carbonate-siliciclastic sediments\n- Shallow marine environment\n- Part of the developing continental shelf\n\n3. Middle-Late Cambrian (504-498 million years ago)\n- Conasauga Formation: Limestone and mixed sediments\n- Continued marine deposition\n- Alternating limestone and shale indicating fluctuating sea levels\n\n4. Late Cambrian-Early Ordovician (498-470 million years ago)\n- Knox Dolomite: Massive dolomite sequence\n- Extensive shallow marine platform\n- Up to 1,200 meters thick\n- Represents long period of stable marine conditions\n\n5. Middle-Late Ordovician (470-443 million years ago)\nSeveral important formations:\n- Stones River Group (including Murfreesboro, Pierce, Lebanon Limestones)\n- Nashville Group (including Hermitage, Bigby-Cannon, Catheys Formations)\n- Primarily limestone deposits\n- Rich in fossils\n- Shallow marine environment\n- Multiple cycles of sea level change\n\n6. Early-Middle Silurian (443-423 million years ago)\n- Brassfield through Decatur Formations\n- Continued marine limestone deposition\n- Various marine environments from shallow to deeper water\n\n7. Middle Devonian-Early Mississippian (390-357 million years ago)\n- Pegram Formation through Chattanooga Shale\n- Shift to more clastic (shale) deposition\n- Chattanooga Shale represents deeper, oxygen-poor conditions\n\n8. Early-Late Mississippian (357-331 million years ago)\nSeveral limestone formations:\n- Fort Payne Formation\n- Warsaw Limestone\n- St. Louis Limestone\n- Monteagle Limestone\n- Hartselle Formation\n- Return to carbonate platform conditions\n- Diverse marine environments\n\n9. Cenozoic to Present\n- Long period of erosion and exposure\n- Development of the current landscape\n- Formation of the Nashville Dome structure\n- River and stream deposits\n\n### Key Features\n- The area was predominantly a marine environment for most of its geological history\n- Dominated by limestone formations indicating warm, shallow seas\n- Multiple cycles of sea level change\n- Rich fossil record, especially in Ordovician rocks\n- Part of the Nashville Dome structural feature\n- Long periods of erosion have removed younger rocks\n- Current topography shaped by differential erosion of various limestone units\n\n### Present Day Bedrock\nThe bedrock of Nashville today consists primarily of Ordovician limestone formations exposed at the surface, particularly the Stones River and Nashville Group limestones. These rocks are well exposed in many areas and have influenced the development of local karst topography, including caves and sinkholes that are common in the region.\n\n## Installation\n\n### Installing via Smithery\n\nTo install Macrostrat API Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@blake365/macrostrat-mcp):\n\n```bash\nnpx -y @smithery/cli install @blake365/macrostrat-mcp --client claude\n```\n\n### Extra Steps\n\nIf you want to make changes to the server you can do so by editing the `src/index.ts` file.\n- Run `npm install` to install dependencies\n- Run `npm run build` to build the server\n- Quit and restart Claude Desktop after making changes\n\n\n## Connecting with Claude Desktop\n\n1. Open your Claude Desktop configuration at:\n   - macOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\n   - Windows: `%APPDATA%\\Claude\\claude_desktop_config.json`\n\n2. Add the server configuration:\n```json\n{\n    \"mcpServers\": {\n        \"macrostrat\": {\n            \"command\": \"node\",\n            \"args\": [\n                \"/Full/Route/to/Folder/macrostrat/build/index.js\"\n            ]\n        }\n    }\n}\n```\n\n3. Close/Quit then restart Claude Desktop\n\nOnce you restart you should see an icon for search and tools. Click this button to see the MCP tools available.\n\n## Troubleshooting\n\nIf you get errors when running the server you may need to provide the full path to the `node` command. For example, on macOS: `/usr/local/bin/node`",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "geologic",
        "geological",
        "macrostrat",
        "geologic data",
        "geological data",
        "access geologic"
      ],
      "category": "scientific-research-tools"
    },
    "blazickjp--arxiv-mcp-server": {
      "owner": "blazickjp",
      "name": "arxiv-mcp-server",
      "url": "https://github.com/blazickjp/arxiv-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/blazickjp.webp",
      "description": "Provides access to arXiv research papers via a straightforward MCP interface, enabling AI models to search and retrieve academic articles.",
      "stars": 1743,
      "forks": 121,
      "license": "Apache License 2.0",
      "language": "Python",
      "updated_at": "2025-10-04T01:41:17Z",
      "readme_content": "[![Twitter Follow](https://img.shields.io/twitter/follow/JoeBlazick?style=social)](https://twitter.com/JoeBlazick)\n[![smithery badge](https://smithery.ai/badge/arxiv-mcp-server)](https://smithery.ai/server/arxiv-mcp-server)\n[![Python Version](https://img.shields.io/badge/python-3.11+-blue.svg)](https://www.python.org/downloads/)\n[![Tests](https://github.com/blazickjp/arxiv-mcp-server/actions/workflows/tests.yml/badge.svg)](https://github.com/blazickjp/arxiv-mcp-server/actions/workflows/tests.yml)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![PyPI Downloads](https://img.shields.io/pypi/dm/arxiv-mcp-server.svg)](https://pypi.org/project/arxiv-mcp-server/)\n[![PyPI Version](https://img.shields.io/pypi/v/arxiv-mcp-server.svg)](https://pypi.org/project/arxiv-mcp-server/)\n\n# ArXiv MCP Server\n\n> 🔍 Enable AI assistants to search and access arXiv papers through a simple MCP interface.\n\nThe ArXiv MCP Server provides a bridge between AI assistants and arXiv's research repository through the Model Context Protocol (MCP). It allows AI models to search for papers and access their content in a programmatic way.\n\n<div align=\"center\">\n  \n🤝 **[Contribute](https://github.com/blazickjp/arxiv-mcp-server/blob/main/CONTRIBUTING.md)** • \n📝 **[Report Bug](https://github.com/blazickjp/arxiv-mcp-server/issues)**\n\n<a href=\"https://www.pulsemcp.com/servers/blazickjp-arxiv-mcp-server\"><img src=\"https://www.pulsemcp.com/badge/top-pick/blazickjp-arxiv-mcp-server\" width=\"400\" alt=\"Pulse MCP Badge\"></a>\n</div>\n\n## ✨ Core Features\n\n- 🔎 **Paper Search**: Query arXiv papers with filters for date ranges and categories\n- 📄 **Paper Access**: Download and read paper content\n- 📋 **Paper Listing**: View all downloaded papers\n- 🗃️ **Local Storage**: Papers are saved locally for faster access\n- 📝 **Prompts**: A Set of Research Prompts\n\n## 🚀 Quick Start\n\n### Installing via Smithery\n\nTo install ArXiv Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/arxiv-mcp-server):\n\n```bash\nnpx -y @smithery/cli install arxiv-mcp-server --client claude\n```\n\n### Installing Manually\nInstall using uv:\n\n```bash\nuv tool install arxiv-mcp-server\n```\n\nFor development:\n\n```bash\n# Clone and set up development environment\ngit clone https://github.com/blazickjp/arxiv-mcp-server.git\ncd arxiv-mcp-server\n\n# Create and activate virtual environment\nuv venv\nsource .venv/bin/activate\n\n# Install with test dependencies\nuv pip install -e \".[test]\"\n```\n\n### 🔌 MCP Integration\n\nAdd this configuration to your MCP client config file:\n\n```json\n{\n    \"mcpServers\": {\n        \"arxiv-mcp-server\": {\n            \"command\": \"uv\",\n            \"args\": [\n                \"tool\",\n                \"run\",\n                \"arxiv-mcp-server\",\n                \"--storage-path\", \"/path/to/paper/storage\"\n            ]\n        }\n    }\n}\n```\n\nFor Development:\n\n```json\n{\n    \"mcpServers\": {\n        \"arxiv-mcp-server\": {\n            \"command\": \"uv\",\n            \"args\": [\n                \"--directory\",\n                \"path/to/cloned/arxiv-mcp-server\",\n                \"run\",\n                \"arxiv-mcp-server\",\n                \"--storage-path\", \"/path/to/paper/storage\"\n            ]\n        }\n    }\n}\n```\n\n## 💡 Available Tools\n\nThe server provides four main tools:\n\n### 1. Paper Search\nSearch for papers with optional filters:\n\n```python\nresult = await call_tool(\"search_papers\", {\n    \"query\": \"transformer architecture\",\n    \"max_results\": 10,\n    \"date_from\": \"2023-01-01\",\n    \"categories\": [\"cs.AI\", \"cs.LG\"]\n})\n```\n\n### 2. Paper Download\nDownload a paper by its arXiv ID:\n\n```python\nresult = await call_tool(\"download_paper\", {\n    \"paper_id\": \"2401.12345\"\n})\n```\n\n### 3. List Papers\nView all downloaded papers:\n\n```python\nresult = await call_tool(\"list_papers\", {})\n```\n\n### 4. Read Paper\nAccess the content of a downloaded paper:\n\n```python\nresult = await call_tool(\"read_paper\", {\n    \"paper_id\": \"2401.12345\"\n})\n```\n\n## 📝 Research Prompts\n\nThe server offers specialized prompts to help analyze academic papers:\n\n### Paper Analysis Prompt\nA comprehensive workflow for analyzing academic papers that only requires a paper ID:\n\n```python\nresult = await call_prompt(\"deep-paper-analysis\", {\n    \"paper_id\": \"2401.12345\"\n})\n```\n\nThis prompt includes:\n- Detailed instructions for using available tools (list_papers, download_paper, read_paper, search_papers)\n- A systematic workflow for paper analysis\n- Comprehensive analysis structure covering:\n  - Executive summary\n  - Research context\n  - Methodology analysis\n  - Results evaluation\n  - Practical and theoretical implications\n  - Future research directions\n  - Broader impacts\n\n## ⚙️ Configuration\n\nConfigure through environment variables:\n\n| Variable | Purpose | Default |\n|----------|---------|---------|\n| `ARXIV_STORAGE_PATH` | Paper storage location | ~/.arxiv-mcp-server/papers |\n\n## 🧪 Testing\n\nRun the test suite:\n\n```bash\npython -m pytest\n```\n\n## 📄 License\n\nReleased under the MIT License. See the LICENSE file for details.\n\n---\n\n<div align=\"center\">\n\nMade with ❤️ by the Pearl Labs Team\n\n<a href=\"https://glama.ai/mcp/servers/04dtxi5i5n\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/04dtxi5i5n/badge\" alt=\"ArXiv Server MCP server\" /></a>\n</div>\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "articles",
        "search",
        "research",
        "arxiv mcp",
        "academic articles",
        "research papers"
      ],
      "category": "scientific-research-tools"
    },
    "bmorphism--penrose-mcp": {
      "owner": "bmorphism",
      "name": "penrose-mcp",
      "url": "https://github.com/bmorphism/penrose-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/bmorphism.webp",
      "description": "Create mathematical diagrams using domain-specific languages that define mathematical types, objects, relationships, and visual representation rules.",
      "stars": 8,
      "forks": 2,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-09-30T21:49:31Z",
      "readme_content": "# Penrose MCP Server\n\nA Model Context Protocol (MCP) server for [Penrose](https://penrose.cs.cmu.edu/) - Create beautiful mathematical diagrams through natural language.\n\n## Overview\n\nThis MCP server provides tools and resources for creating mathematical diagrams using Penrose's domain-specific languages:\n\n- **Domain (DSL)**: Define mathematical types and relationships\n- **Substance**: Describe mathematical objects and their relationships\n- **Style**: Specify visual representation rules\n\n## Project Structure\n\n- `.topos/`: Research materials and documentation (gitignored)\n  - `penrose-research/`: Design documents and specifications\n  - `mcp-examples/`: Reference MCP server implementations\n  - `mcp-spec/`: Official MCP protocol documentation\n\n## Development\n\nUse the justfile to access documentation and reference materials:\n\n```bash\n# List all available commands\njust --list\n\n# View the server architecture\njust architecture\n\n# Access MCP specifications\njust mcp-spec\n```\n\n## License\n\nMIT License - See LICENSE file for details\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "diagrams",
        "mathematical",
        "types",
        "mathematical diagrams",
        "mathematical types",
        "diagrams using"
      ],
      "category": "scientific-research-tools"
    },
    "cnitlrt--headless-ida-mcp-server": {
      "owner": "cnitlrt",
      "name": "headless-ida-mcp-server",
      "url": "https://github.com/cnitlrt/headless-ida-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/cnitlrt.webp",
      "description": "Analyze binary files and manage functions and variables remotely using IDA Pro's headless mode. Provides tools for efficient remote operations through the Multi-Client Protocol.",
      "stars": 18,
      "forks": 4,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-09-11T13:34:27Z",
      "readme_content": "# Acknowledgments\n\nThis project builds upon the work of:\n- Tools code adapted from [ida-pro-mcp](https://github.com/mrexodia/ida-pro-mcp) by mrexodia\n- Utilizes the [headless-ida](https://github.com/DennyDai/headless-ida) library by DennyDai\n\n# Headless IDA MCP Server\n\nIf you want to run the server directly as a cli app, rather than an IDA plugin interactively,you can chose it.\n\n## Project Description\n\nThis project uses IDA Pro's headless mode to analyze binary files and provides a suite of tools via MCP to manage and manipulate functions, variables, and more.\n\n## Prerequisites\n\n- Python 3.12 or higher\n- IDA Pro with headless support (idat) https://github.com/DennyDai/headless-ida\n\n## Installation\n\n1. Clone the project locally:\n\n   ```bash\n   git clone https://github.com/cnitlrt/headless-ida-mcp-server.git \n   cd headless-ida-mcp-server\n   ```\n\n2. Install dependencies:\n\n   ```bash\n   uv python install 3.12\n   uv venv --python 3.12\n   uv pip install -e .\n   ```\n\n## Configuration\n\n1. Copy the example environment file:\n   ```bash\n   cp .env_example .env\n   ```\n\n2. Configure the following environment variables in `.env`:\n\n   - `IDA_PATH`: Path to IDA Pro's headless executable (idat), e.g., `/home/ubuntu/idapro/idat`\n   - `PORT`: Port number for the MCP server, e.g., `8888`\n   - `HOST`: Host address for the MCP server, e.g., `127.0.0.1`\n   - `TRANSPORT`: MCP transport mode (`sse` or `stdio`)\n\n## Usage\n\n1. Start the server:\n   ```bash\n   uv run headless_ida_mcp_server\n   ```\n\n2. Connect to the server using an MCP client:\n\n   Debug it: \n   ```bash\n   npx -y @modelcontextprotocol/inspector\n   ```\n   or\n   ```json\n   {\n   \"mcpServers\": {\n      \"ida\": {\n         \"command\": \"/path/to/uv\",\n         \"args\": [\"--directory\",\"path/to/headless-ida-mcp-server\",\"run\",\"headless_ida_mcp_server\"]\n      }\n   }\n   }\n   ```",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "ida",
        "cnitlrt",
        "headless",
        "headless ida",
        "using ida",
        "ida pro"
      ],
      "category": "scientific-research-tools"
    },
    "couchrishi--sd-for-designers": {
      "owner": "couchrishi",
      "name": "sd-for-designers",
      "url": "https://github.com/couchrishi/sd-for-designers",
      "imageUrl": "/freedevtools/mcp/pfp/couchrishi.webp",
      "description": "Automate the fine-tuning and deployment of custom Stable Diffusion models using a fully automated MLOps pipeline that integrates various Google Cloud components. The workflow facilitates Triggering, managing, and tracking training jobs while simplifying machine learning processes for model enhancement.",
      "stars": 2,
      "forks": 2,
      "license": "No License",
      "language": "Jupyter Notebook",
      "updated_at": "2023-08-21T14:02:15Z",
      "readme_content": "# sd-for-designers\nA fully automated workflow for triggering, running &amp; managing fine tuning, training &amp; deploying custom stable diffusion models using Vertex AI\n\n## Description\n\nSd-aa-S is a full automated MLOps pipeline for triggering, managing & tracking Stable diffusion finetuning jobs on GCP using GCP components such as Google Cloud Storage, Cloud Build, Cloud PubSub, Firestore, Cloud Run, Cloud Functions and Vertex AI. It aims to simplify the ML workflows for tuning Stable diffusion using different techniques, starting with Dreambooth. Support for Lora, ControlNet etc. coming soon. The project is targeted ML/Data Engineers, Data Scientists & anybody else interested in or on the road towards building a platform for finetuning stable diffusion at scale. \n\n## Three Parts\n\n# 1. The App part\n\n    1. Set up your Cloud Environment\n    2. Create a backend service for handling uploads to a GCS bucket\n        - Receive images from clients and store them under a predefined GCS bucket path\n        - Track the status of individual uploads in a Firestore collection\n        - Track the status of the overall upload job in a separate Firestore collection \n        - Once the job is compelted, publish the jobID as the message on a predefined PubSub topic\n    3. Deploy this backend service as a Cloud Run endpoint using Cloud build\n    4. Create a frontend portal to upload images using ReactJs\n    5. Deploy the frontend service on Cloud Run\n\n# 2. The Vertex AI part\n\n    1. Set up your Cloud Environment\n    2. Create a new custom container artifact for running the pipeline components\n    3. Create a new custom container artifact for running the training job itself \n    4. Create a Jupyter notebook outlining the Pipeline flow & components\n    5. Compile a YAML file from a Vertex AI workbench and store the precompiled YAML file under a GCS bucket path\n\n \n# 3. The Plumbing part\n\n    1. Set up your Cloud Environment\n    2. Create a cloud function that gets triggered every time the jobID is published on a predefinied topic (from 1st part)\n    3. Within the cloud function, the python code subscribes to the topic and triggers a Vertex AI pipeline job using the precomiled YAML file (from 2nd part)\n    4. The pipeline jobs finetunes the stable diffusion model using Dreambooth, uploads the new custom model to Model registy & deploys an endpoint\n    5. The job also updates Firestore with the status of the pipeline job from start to end",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "mlops",
        "cloud",
        "pipeline",
        "automated mlops",
        "mlops pipeline",
        "google cloud"
      ],
      "category": "scientific-research-tools"
    },
    "daheepk--arxiv-paper-mcp": {
      "owner": "daheepk",
      "name": "arxiv-paper-mcp",
      "url": "https://github.com/daheepk/arxiv-paper-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/daheepk.webp",
      "description": "Facilitates efficient search and exploration of research papers from arXiv.org, enabling users to access detailed metadata, conduct keyword and author-based searches, and generate prompts for summarization and comparison. Provides updates on trending topics across various research categories.",
      "stars": 8,
      "forks": 3,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-30T18:39:37Z",
      "readme_content": "# 🧠 arXiv Research Assistant MCP Server\n\n[![smithery badge](https://smithery.ai/badge/@daheepk/arxiv-paper-mcp)](https://smithery.ai/server/@daheepk/arxiv-paper-mcp)\n\n\nThis project is an MCP (Model Context Protocol) server built to interact with the vast arXiv.org paper database.\n\nIt allows clients like **Claude AI** to search, explore, and compare arXiv papers efficiently — all through a custom-built, local server. It’s built with **Python** and the **FastMCP** framework, and uses **uv** for lightweight package management.\n\n<table>\n  <tr>\n    <td>\n      <a href=\"https://glama.ai/mcp/servers/@daheepk/arxiv-mcp-server\">\n        <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@daheepk/arxiv-mcp-server/badge\" alt=\"arXiv Research Assistant Server MCP server\" />\n      </a>\n    </td>\n    <td style=\"vertical-align: top; padding-left: 10px;\">\n      <a href=\"https://mseep.ai/app/daheepk-arxiv-paper-mcp\">\n        <img src=\"https://mseep.net/pr/daheepk-arxiv-paper-mcp-badge.png\" alt=\"MseeP.ai Security Assessment Badge\" width=\"140\" />\n      </a>\n    </td>\n  </tr>\n</table>\n\n\n## ✨ Features\n\n- **🔍 Keyword-based Paper Search**  \n  Search arXiv papers by keywords, with options to sort by relevance or most recent.\n\n- **📚 Latest Papers by Category**  \n  Specify an arXiv category code (e.g., `cs.AI`, `math.AP`) to fetch the most recent papers in that field.\n\n- **📄 Paper Details Lookup**  \n  Fetch detailed metadata using a paper's arXiv ID: title, authors, abstract, categories, DOI, PDF link, and more.\n\n- **🧑‍🔬 Author-based Paper Search**  \n  Retrieve a list of papers published by a specific author.\n\n- **📊 Trend Analysis (Experimental)**  \n  Get an overview of trending keywords or topics based on recent papers in a category (currently uses mock data).\n\n- **📝 Summarization Prompt Generator**  \n  Dynamically generate prompts that help LLMs summarize a selected paper more effectively.\n\n- **🆚 Comparison Prompt Generator**  \n  Provide two paper IDs to generate a structured prompt for comparing their content.\n\n---\n\n## 🛠️ Tech Stack\n\n- Python 3.11+\n- [FastMCP](https://github.com/modelcontextprotocol/fastmcp)\n- uv (for dependency & environment management)\n- requests (for API communication)\n- xml.etree.ElementTree (for parsing XML responses)\n\n---\n\n## 🚀 Getting Started\n\n### Installing via Smithery\n\nTo install arXiv Research Assistant MCP Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/arxiv-paper-mcp):\n\n```bash\nnpx -y @smithery/cli install arxiv-paper-mcp --client claude\n```\n\n### Installation from PyPI\n\n```bash\nuv pip install arxiv-paper-mcp\n```\n\n### 🔧 Clone the repository (for development)\n```bash\ngit clone https://github.com/daheepk/arxiv-mcp-server.git\ncd arxiv-mcp-server\n```\n### 🔧 Install Dependencies (for development)\n\nUse `uv` to install all dependencies in editable mode:\n\n```bash\nuv pip install -e .\n```\n\n\n## ⚙️ How to Run\n\n### ▶️ Run the server (locally)\n\n```bash\narxiv-paper-mcp\n```\n\n## 🔌 Use with Claude\n\nTo use this MCP server with Claude, add the following JSON configuration to Claude's MCP settings:\n\n```json\n{\n  \"mcpServers\": {\n    \"arXivPaper\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"tool\",\n        \"run\",\n        \"arxiv-paper-mcp\"\n      ]\n    }\n  }\n}\n```\n\n## Project Structure\n```\narxiv-mcp-server/\n├── arxiv_mcp/              # Main package\n│   ├── __init__.py\n│   ├── app.py              # FastMCP app setup\n│   ├── server.py           # Server entry point\n│   ├── utils.py            # arXiv API communication logic\n│   ├── resources/          # MCP resources (categories, authors, etc.)\n│   ├── tools/              # MCP tools (search, detail lookup, trends)\n│   └── prompts/            # Prompt templates (summarize, compare)\n├── pyproject.toml          # Project config & dependencies\n└── README.md               # This file\n```\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "research",
        "searches",
        "arxiv",
        "research tools",
        "research papers",
        "papers arxiv"
      ],
      "category": "scientific-research-tools"
    },
    "datalayer--earthdata-mcp-server": {
      "owner": "datalayer",
      "name": "earthdata-mcp-server",
      "url": "https://github.com/datalayer/earthdata-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/datalayer.webp",
      "description": "Enables efficient discovery and retrieval of NASA Earth Data datasets and granules, facilitating geospatial analysis and data exploration. Integrates with workflows for AI-powered analysis of Earthdata datasets.",
      "stars": 21,
      "forks": 6,
      "license": "BSD 3-Clause \"New\" or \"Revised\" License",
      "language": "Python",
      "updated_at": "2025-09-19T09:33:45Z",
      "readme_content": "<!--\n  ~ Copyright (c) 2023-2024 Datalayer, Inc.\n  ~\n  ~ BSD 3-Clause License\n-->\n\n[![Datalayer](https://assets.datalayer.tech/datalayer-25.svg)](https://datalayer.io)\n\n[![Become a Sponsor](https://img.shields.io/static/v1?label=Become%20a%20Sponsor&message=%E2%9D%A4&logo=GitHub&style=flat&color=1ABC9C)](https://github.com/sponsors/datalayer)\n\n# 🪐 ✨ Earthdata MCP Server\n\n[![PyPI - Version](https://img.shields.io/pypi/v/earthdata-mcp-server)](https://pypi.org/project/earthdata-mcp-server)\n[![smithery badge](https://smithery.ai/badge/@datalayer/earthdata-mcp-server)](https://smithery.ai/server/@datalayer/earthdata-mcp-server)\n\nEarthdata MCP Server is a [Model Context Protocol](https://modelcontextprotocol.io/introduction) (MCP) server implementation that provides tools to interact with [NASA Earth Data](https://www.earthdata.nasa.gov/). It enables efficient dataset discovery, retrieval and analysis for Geospatial analysis.\n\n🚀 **NEW**: This server now includes all [Jupyter MCP Server](https://github.com/datalayer/jupyter-mcp-server) tools through composition, providing a unified interface for both Earth data discovery and analysis in Jupyter Notebooks.\n\n## 🚀 Key Features\n\n- **Efficient Data Retrieval**: Search and download Earthdata datasets\n- **Unified Interface**: Combines Earthdata research and Jupyter notebook manipulation tools for analysis\n\nThe following demo uses this MCP server to search for datasets and data granules on NASA Earthdata, download the data in Jupyter and run further analysis.\n\n<div>\n  <a href=\"https://www.loom.com/share/c2b5b05f548d4f1492d5c107f0c48dbc\">\n    <p>Analyzing Sea Level Rise with AI-Powered Geospatial Tools and Jupyter - Watch Video</p>\n  </a>\n  <a href=\"https://www.loom.com/share/c2b5b05f548d4f1492d5c107f0c48dbc\">\n    <img alt=\"c2b5b05f548d4f1492d5c107f0c48dbc_598a84f02de7e74e_full_play\" style=\"max-width:100%;\" src=\"https://cdn.loom.com/sessions/thumbnails/c2b5b05f548d4f1492d5c107f0c48dbc-598a84f02de7e74e-full-play.gif\">\n  </a>\n</div>\n\n## 🏁 Getting Started\n\nFor comprehensive setup instructions—including `Streamable HTTP` transport and advanced configuration—check out [the Jupyter MCP Server documentation](https://jupyter-mcp-server.datalayer.tech/). Or, get started quickly with `JupyterLab` and `stdio` transport here below.\n\n### 1. Set Up Your Environment\n\n```bash\npip install jupyterlab==4.4.1 jupyter-collaboration==4.0.2 ipykernel\npip uninstall -y pycrdt datalayer_pycrdt\npip install datalayer_pycrdt==0.12.17\n```\n\n### 2. Start JupyterLab\n\n```bash\n# make jupyterlab\njupyter lab --port 8888 --IdentityProvider.token MY_TOKEN --ip 0.0.0.0\n```\n\n### 3. Configure Your Preferred MCP Client\n\n> [!NOTE]\n>\n> Ensure the `port` of the `DOCUMENT_URL` and `RUNTIME_URL` match those used in the `jupyter lab` command.\n>\n> The `DOCUMENT_ID` which is the path to the notebook you want to connect to, should be relative to the directory where JupyterLab was started.\n>\n> In a basic setup, `DOCUMENT_URL` and `RUNTIME_URL` are the same. `DOCUMENT_TOKEN`, and `RUNTIME_TOKEN` are also the same and is actually the Jupyter Token.\n\n> [!NOTE]\n> \n> The `EARTHDATA_USERNAME` and `EARTHDATA_PASSWORD` environment variables are used for NASA Earthdata authentication to download datasets via the `earthaccess` library. See [NASA Earthdata Authentication](./docs/authentication.md) for more details.\n\n#### MacOS and Windows\n\n```json\n{\n  \"mcpServers\": {\n    \"earthdata\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"-i\",\n        \"--rm\",\n        \"-e\",\n        \"DOCUMENT_URL\",\n        \"-e\",\n        \"DOCUMENT_TOKEN\",\n        \"-e\",\n        \"DOCUMENT_ID\",\n        \"-e\",\n        \"RUNTIME_URL\",\n        \"-e\",\n        \"RUNTIME_TOKEN\",\n        \"datalayer/earthdata-mcp-server:latest\"\n      ],\n      \"env\": {\n        \"DOCUMENT_URL\": \"http://host.docker.internal:8888\",\n        \"DOCUMENT_TOKEN\": \"MY_TOKEN\",\n        \"DOCUMENT_ID\": \"notebook.ipynb\",\n        \"RUNTIME_URL\": \"http://host.docker.internal:8888\",\n        \"RUNTIME_TOKEN\": \"MY_TOKEN\",\n        \"EARTHDATA_USERNAME\": \"your_username\",\n        \"EARTHDATA_PASSWORD\": \"your_password\"\n      }\n    }\n  }\n}\n```\n\n#### Linux\n\n```json\n{\n  \"mcpServers\": {\n    \"earthdata\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"-i\",\n        \"--rm\",\n        \"-e\",\n        \"DOCUMENT_URL\",\n        \"-e\",\n        \"DOCUMENT_TOKEN\",\n        \"-e\",\n        \"DOCUMENT_ID\",\n        \"-e\",\n        \"RUNTIME_URL\",\n        \"-e\",\n        \"RUNTIME_TOKEN\",\n        \"--network=host\",\n        \"datalayer/earthdata-mcp-server:latest\"\n      ],\n      \"env\": {\n        \"DOCUMENT_URL\": \"http://localhost:8888\",\n        \"DOCUMENT_TOKEN\": \"MY_TOKEN\",\n        \"DOCUMENT_ID\": \"notebook.ipynb\",\n        \"RUNTIME_URL\": \"http://localhost:8888\",\n        \"RUNTIME_TOKEN\": \"MY_TOKEN\",\n        \"EARTHDATA_USERNAME\": \"your_username\",\n        \"EARTHDATA_PASSWORD\": \"your_password\"\n      }\n    }\n  }\n}\n```\n\n## Tools\n\nThe server offers **15 tools total**: 3 Earthdata-specific tools plus 12 Jupyter notebook manipulation tools.\n\n### Earthdata Tools\n\n#### `search_earth_datasets`\n\n- Search for datasets on NASA Earthdata.\n- Input:\n  - search_keywords (str): Keywords to search for in the dataset titles.\n  - count (int): Number of datasets to return.\n  - temporal (tuple): (Optional) Temporal range in the format (date_from, date_to).\n  - bounding_box (tuple): (Optional) Bounding box in the format (lower_left_lon, lower_left_lat, upper_right_lon, upper_right_lat).\n- Returns: List of dataset abstracts.\n\n#### `search_earth_datagranules`\n\n- Search for data granules on NASA Earthdata.\n- Input:\n  - short_name (str): Short name of the dataset.\n  - count (int): Number of data granules to return.\n  - temporal (tuple): (Optional) Temporal range in the format (date_from, date_to).\n  - bounding_box (tuple): (Optional) Bounding box in the format (lower_left_lon, lower_left_lat, upper_right_lon, upper_right_lat).\n- Returns: List of data granules.\n\n#### `download_earth_data_granules`\n\n- Download Earth data granules from NASA Earth Data and integrate with Jupyter notebooks.\n- This tool combines earthdata search capabilities with jupyter notebook manipulation to create a seamless download workflow.\n- **Authentication**: Requires NASA Earthdata Login credentials (see [Authentication section](#nasa-earthdata-authentication))\n- Input:\n  - folder_name (str): Local folder name to save the data.\n  - short_name (str): Short name of the Earth dataset to download.\n  - count (int): Number of data granules to download.\n  - temporal (tuple): (Optional) Temporal range in the format (date_from, date_to).\n  - bounding_box (tuple): (Optional) Bounding box in the format (lower_left_lon, lower_left_lat, upper_right_lon, upper_right_lat).\n- Returns: Success message with download code preparation details.\n\n### Jupyter Tools (Composed)\n\nThe following Jupyter notebook manipulation tools are available:\n\n- **`append_markdown_cell`**: Add markdown cells to notebooks\n- **`insert_markdown_cell`**: Insert markdown cells at specific positions\n- **`overwrite_cell_source`**: Modify existing cell content\n- **`append_execute_code_cell`**: Add and execute code cells\n- **`insert_execute_code_cell`**: Insert and execute code cells at specific positions\n- **`execute_cell_with_progress`**: Execute cells with progress monitoring\n- **`execute_cell_simple_timeout`**: Execute cells with timeout\n- **`execute_cell_streaming`**: Execute cells with streaming output\n- **`read_all_cells`**: Read all notebook cells\n- **`read_cell`**: Read specific notebook cells\n- **`get_notebook_info`**: Get notebook metadata\n- **`delete_cell`**: Delete notebook cells\n\nFor detailed documentation of the Jupyter tools, see the [Jupyter MCP Server documentation](https://github.com/datalayer/jupyter-mcp-server).\n\n## Prompts\n\n1. `download_analyze_global_sea_level` 🆕\n   - Generate a comprehensive workflow for downloading and analyzing Global Mean Sea Level Trend dataset.\n   - Uses both earthdata download tools and jupyter analysis capabilities.\n   - Returns: Detailed prompt for complete sea level analysis workflow.\n\n2. `sealevel_rise_dataset`\n   - Search for datasets related to sea level rise worldwide.\n   - Input:\n     - `start_year` (int): Start year to consider.\n      - `end_year` (int): End year to consider.\n   - Returns: Prompt correctly formatted.\n\n3. `ask_datasets_format`\n    - To ask about the format of the datasets.\n    - Returns: Prompt correctly formatted.\n\n## Building\n\n```bash\n# or run `docker build -t datalayer/earthdata-mcp-server .`\nmake build-docker\n```\n\nIf you prefer, you can pull the prebuilt images.\n\n```bash\nmake pull-docker\n```\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "earthdata",
        "nasa",
        "datasets",
        "earthdata mcp",
        "earthdata datasets",
        "analysis earthdata"
      ],
      "category": "scientific-research-tools"
    },
    "doomdagadiggiedahdah--iacr-mcp-server": {
      "owner": "doomdagadiggiedahdah",
      "name": "iacr-mcp-server",
      "url": "https://github.com/doomdagadiggiedahdah/iacr-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/doomdagadiggiedahdah.webp",
      "description": "Provides a programmatic interface to retrieve and search for cryptographic research papers from the IACR Cryptology ePrint Archive, along with access to paper metadata.",
      "stars": 2,
      "forks": 7,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-05-16T06:16:51Z",
      "readme_content": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/doomdagadiggiedahdah-iacr-mcp-server-badge.png)](https://mseep.ai/app/doomdagadiggiedahdah-iacr-mcp-server)\n\n# IACR Cryptology ePrint Archive MCP Server\n\n[![smithery badge](https://smithery.ai/badge/iacr-mcp-server)](https://smithery.ai/server/iacr-mcp-server)\n\n## Overview\n\nThis Model Context Protocol (MCP) server provides a programmatic interface to the IACR Cryptology ePrint Archive, enabling efficient retrieval of cryptographic research papers.\n\n<a href=\"https://glama.ai/mcp/servers/e2oh3a96de\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/e2oh3a96de/badge\" alt=\"IACR Server MCP server\" /></a>\n\n## Features\n\n- 🔍 Search cryptographic papers\n- 📋 Retrieve paper metadata\n- 🔒 Secure access to research publications\n\n## Prerequisites\n\n- Node.js (v16+)\n- npm or yarn\n\n## Installation\n\n### Installing via Smithery\n\nTo install IACR Cryptology ePrint Archive for Claude Desktop automatically via [Smithery](https://smithery.ai/server/iacr-mcp-server):\n\n```bash\nnpx -y @smithery/cli install iacr-mcp-server --client claude\n```\n\n### Manual Installation\n```bash\ngit clone https://github.com/yourusername/iacr-mcp-server.git\ncd iacr-mcp-server\nnpm install\n```\n\n## Configuration\n\nNo additional configuration is required. The server uses the IACR ePrint Archive's RSS feed for data retrieval.\n\n## Usage\n\n### Available Tools\n\n1. `search_papers`: Search for papers\n   - Parameters:\n     - `query`: Search term (required)\n     - `year`: Publication year (optional)\n     - `max_results`: Maximum number of results (default: 20)\n\n2. `get_paper_details`: Retrieve details for a specific paper\n   - Parameters:\n     - `paper_id`: Unique paper identifier (required)\n\n## Contributing\n\n1. Fork the repository\n2. Create a feature branch\n3. Commit your changes\n4. Push to the branch\n5. Create a Pull Request\n\n## Disclaimer\n\nThis is an unofficial tool. Always refer to the original IACR Cryptology ePrint Archive for the most accurate and up-to-date research publications.\n\n## Contact\n\nFor issues, questions, or suggestions, please open a GitHub issue.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "cryptology",
        "archive",
        "iacr",
        "iacr cryptology",
        "cryptology eprint",
        "research papers"
      ],
      "category": "scientific-research-tools"
    },
    "dougwithseismic--withseismic-mcp": {
      "owner": "dougwithseismic",
      "name": "withseismic-mcp",
      "url": "https://github.com/dougwithseismic/withseismic-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/dougwithseismic.webp",
      "description": "Facilitates integration of AI models with various tools and prompts, allowing for rapid development and automated type safety in TypeScript without infrastructure management.",
      "stars": 14,
      "forks": 1,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-09-04T12:23:11Z",
      "readme_content": "# D0UGWITHSEISMIC/WITHSEISMIC-MCP\n\n\n\n```\nSYSTEMS ONLINE • NEURAL INTERFACE ACTIVE • COMBAT DATA ANALYSIS • TACTICAL OVERLAY ENABLED • PERFORMANCE METRICS NOMINAL\n```\n\n[![TypeScript](https://img.shields.io/badge/TypeScript-5.0-blue.svg)](https://www.typescriptlang.org/)\n[![Node.js](https://img.shields.io/badge/Node.js-18.x-green.svg)](https://nodejs.org/)\n[![pnpm](https://img.shields.io/badge/pnpm-8.x-orange.svg)](https://pnpm.io/)\n[![License](https://img.shields.io/badge/License-MIT-blue.svg)](LICENSE)\n[![Discord](https://img.shields.io/badge/Discord-Join%20Us-7289da.svg?logo=discord&logoColor=white)](https://discord.gg/BX25FcS53N)\n[![Deploy on Railway](https://railway.app/button.svg)](https://railway.app/template/RTm6ec?referralCode=dougie)\n\nA production-ready MCP server template for hosting your own AI tools and prompts. Deploy remotely or run locally - built for developers who want to expose their tools to AI models without the infrastructure headaches.\n\n## ://WHO_IS_THIS_FOR?\n\nThis template is perfect for:\n\n- **AI Tool Developers**: Build and host your own tools that AI models can interact with\n- **Teams & Organizations**: Set up a shared MCP server for your AI applications\n- **Individual Developers**: Quick-start your MCP development with a production-ready setup\n- **API Providers**: Expose your services to AI models through a standardized interface\n\nWhether you want to:\n\n- 🌐 Host a remote MCP server that multiple clients can connect to\n- 🔧 Run locally via stdio for development and testing\n- 🚀 Deploy a production-ready MCP implementation\n- 🛠️ Build tools without worrying about infrastructure\n\nThis template provides everything you need to get started.\n\n## ://OVERVIEW\n\nThe MCP server provides two ways to expose your tools to AI models:\n\n1. **Remote Server Mode (SSE)**: Deploy as a remote server that multiple clients can connect to\n\n   - Perfect for production deployments\n     (`pnpm start:sse`)\n   - Supports multiple simultaneous connections\n   - Easy deployment to platforms like Railway\n   - Secure and scalable\n\n2. **Local Mode (stdio)**: Run locally for development and testing\n   (`pnpm start`)\n   - Great for local development\n   - Direct stdio communication\n   - Fast iteration and debugging\n\nKey Features:\n\n- **Tools**: Executable functions that models can call\n- **Prompts**: Template-based message generators\n- **Auto-Discovery**: Automatic registration of your components\n- **Type Safety**: Full TypeScript support throughout\n\n## ://WHY_USE_THIS_MCP_SERVER_TEMPLATE?\n\nWhile the Model Context Protocol (MCP) is in its early stages, one thing is clear: **rapid adoption requires frictionless development**. This implementation is built with a singular focus: **letting developers focus on building great tools, not infrastructure**.\n\n### INSTANT_DEPLOYMENT\n\n```bash\n# Option 1: One-Click Deploy\nClick the \"Deploy on Railway\" button above ☝️\n\n# Option 2: Local Setup\npnpm install && pnpm dev\n```\n\n### ZERO_FRICTION_DEVELOPMENT\n\n- **No Boilerplate**: Define your tool, import it, and you're done. We handle the rest.\n- **Auto-Registration**: Tools and prompts are automatically discovered and registered.\n- **Type Safety**: Full TypeScript support with zero configuration.\n\n### BATTLE_TESTED_ARCHITECTURE\n\n- **Production Ready**: Built for reliability and performance.\n- **Scalable Design**: From simple tools to complex AI interactions.\n- **Error Resilient**: Robust error handling and debugging built-in.\n\n### DEVELOPER_EXPERIENCE_FIRST\n\n```typescript\n// That's it. This is all you need to create a tool.\nexport const myTool = new Tool(\n  {\n    name: \"myTool\",\n    description: \"What my tool does\",\n    inputSchema: z.object({ query: z.string() }),\n    outputSchema: z.object({ result: z.string() }),\n  },\n  async (args) => {\n    // Your logic here\n    return { result: \"Done!\" };\n  },\n);\n```\n\nWe handle:\n\n- ⚡ Connection management\n- 🔄 Tool registration\n- 🛡️ Input validation\n- 📦 State management\n- 🚨 Error handling\n- 📊 Logging and metrics\n\nYou focus on:\n\n- 🎯 Building amazing tools\n- 🧠 Creating powerful prompts\n- 🚀 Shipping features\n\n## ://GETTING_STARTED\n\n### INSTALLATION\n\n```bash\npnpm install\n```\n\n### RUNNING_THE_SERVER\n\nTwo modes are available:\n\n1. Standard mode (stdio):\n\n```bash\npnpm dev      # Development with hot reload\npnpm start    # Production\n```\n\n2. SSE (Server-Sent Events) mode:\n\n```bash\npnpm dev:sse  # Development with hot reload\npnpm start:sse # Production\n```\n\nWhen running in SSE mode, connect to: `http://localhost:3001/sse`\n\n## ://CORE_CONCEPTS\n\n### TOOLS\n\nTools are executable functions that models can invoke. Each tool:\n\n- Has defined input/output schemas using Zod\n- Is automatically registered with the registry\n- Can perform any operation (calculation, file I/O, API calls, etc.)\n\nExample tool:\n\n```typescript\nimport { z } from \"zod\";\nimport { Tool } from \"../core\";\n\nconst MyToolInputSchema = z.object({\n  param1: z.string().describe(\"Parameter description\"),\n});\n\nconst MyToolOutputSchema = z.object({\n  result: z.string().describe(\"Result description\"),\n});\n\nexport const myTool = new Tool(\n  {\n    name: \"myTool\",\n    description: \"What my tool does\",\n    inputSchema: MyToolInputSchema,\n    outputSchema: MyToolOutputSchema,\n  },\n  async (args) => {\n    const input = MyToolInputSchema.parse(args);\n    // Tool logic here\n    return { result: \"processed result\" };\n  },\n);\n```\n\n### PROMPTS\n\nPrompts are message generators that help structure model interactions. Each prompt:\n\n- Defines its argument schema\n- Generates messages in a consistent format\n- Is automatically registered with the registry\n\nExample prompt:\n\n```typescript\nimport { Prompt } from \"../core\";\n\nexport const myPrompt = new Prompt(\n  {\n    name: \"myPrompt\",\n    description: \"What my prompt does\",\n    arguments: [\n      {\n        name: \"arg1\",\n        description: \"Argument description\",\n        required: true,\n      },\n    ],\n  },\n  async (args) => {\n    return [\n      {\n        role: \"system\",\n        content: {\n          type: \"text\",\n          text: `Generated message using ${args.arg1}`,\n        },\n      },\n    ];\n  },\n);\n```\n\n## ://ADDING_NEW_COMPONENTS\n\n### Creating a New Tool\n\n1. Create a new file in `src/modules/tools/`\n2. Define your input/output schemas using Zod\n3. Create and export your tool instance\n4. Add the export to `src/modules/tools/index.ts`\n\nThe registry will automatically:\n\n- Register your tool\n- Make it available to models\n- Handle validation and error handling\n\n### Creating a New Prompt\n\n1. Create a new file in `src/modules/prompts/`\n2. Define your argument schema\n3. Create and export your prompt instance\n4. Add the export to `src/modules/prompts/index.ts`\n\nThe registry will automatically:\n\n- Register your prompt\n- Make it available to models\n- Handle message generation and errors\n\n## Architecture\n\n### Core Components\n\n- **Registry**: Central manager for all tools and prompts\n- **Tool**: Base class for executable functions\n- **Prompt**: Base class for message generators\n- **Server**: MCP protocol implementation\n\n### Auto-Registration\n\nThe system uses a singleton Registry pattern that:\n\n1. Automatically registers tools and prompts on import\n2. Provides type-safe access to components\n3. Handles all MCP protocol interactions\n\n### Error Handling\n\nThe system includes robust error handling:\n\n- Type validation via Zod schemas\n- Execution error wrapping\n- Detailed error messages for debugging\n\n## Development\n\n### Type Safety\n\nAll components use TypeScript for full type safety:\n\n- Input/output schemas are defined using Zod\n- Type inference for tool arguments and results\n- Comprehensive error types\n\n### Testing\n\nRun tests using:\n\n```bash\npnpm test\n```\n\n### Best Practices\n\n1. Always define clear input/output schemas\n2. Use descriptive names and documentation\n3. Handle errors gracefully\n4. Follow the TypeScript guidelines in the codebase\n\n## ://CONTRIBUTING\n\n```\nNEURAL INTERFACE DETECTED • INITIATING COLLABORATION PROTOCOLS • READY FOR UPLINK\n```\n\nWe welcome contributions! Please see our [Contributing Guide](CONTRIBUTING.md) for details on:\n\n- Development workflow\n- Code style guidelines\n- Pull request process\n- Issue reporting\n\nJoin our [Discord community](https://discord.gg/withseismic) to connect with other contributors!\n\n## ://SUPPORT\n\n```\nSUPPORT PROTOCOLS ACTIVE • COMMUNICATION CHANNELS OPEN • READY TO ASSIST\n```\n\n- 📫 Email: <hello@withseismic.com>\n- 💬 Discord: [Join our server](https://discord.gg/BX25FcS53N)\n\n## ://LICENSE\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n\n© 2025 Doug, at WithSeismic dot com.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "withseismic",
        "typescript",
        "ai",
        "tools dougwithseismic",
        "withseismic mcp",
        "dougwithseismic withseismic"
      ],
      "category": "scientific-research-tools"
    },
    "emekaokoye--mcp-rdf-explorer": {
      "owner": "emekaokoye",
      "name": "mcp-rdf-explorer",
      "url": "https://github.com/emekaokoye/mcp-rdf-explorer",
      "imageUrl": "/freedevtools/mcp/pfp/emekaokoye.webp",
      "description": "Explore and analyze RDF data with a conversational interface, enabling execution of SPARQL queries in both local and endpoint modes for insights from knowledge graphs.",
      "stars": 33,
      "forks": 7,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-30T07:49:44Z",
      "readme_content": "# RDF Explorer v1.0.0 \r\n\r\n## Overview\r\nA Model Context Protocol (MCP) server that provides conversational interface for the exploration and analysis of RDF (Turtle) based Knowledge Graph in Local File mode or SPARQL Endpoint mode. This server facilitates communication between AI applications (hosts/clients) and RDF data, making graph exploration and analyzing graph data through SPARQL queries. A perfect tool for knowledge graph research and AI data preparation. \r\n\r\n\r\n## Components\r\n\r\n### Tools\r\nThe server implements SPARQL queries and search functionality:\r\n\r\n- `execute_on_endpoint`\r\n   - Execute a SPARQL query directly on an external endpoint\r\n   - Input:\r\n     - `endpoint` (str): The SPARQL endpoint URL to query.\r\n     - `query` (str): The SPARQL query to execute.\r\n     - `ctx` (Context): The FastMCP context object.\r\n   - Returns: Query results as a newline-separated string, or an error message.\r\n\r\n- `sparql_query`\r\n   - Execute a SPARQL query on the current graph or active external endpoint\r\n   - Input:\r\n     - `query` (str): The SPARQL query to execute.\r\n     - `ctx` (Context): The FastMCP context object.\r\n     - `use_service` (bool): Whether to use a SERVICE clause for federated queries in local mode (default: True).\r\n   - Returns: Query results as a newline-separated string, or an error message.\r\n\r\n- `graph_stats`\r\n   - Calculate and return statistics about the graph in JSON format\r\n   - Input:\r\n     - `ctx` (Context): The FastMCP context object.\r\n   - Returns: JSON string containing graph statistics (e.g., triple count, unique subjects).\r\n\r\n- `count_triples`\r\n   - Count triples in the graph. Disabled in SPARQL Endpoint Mode; use a custom prompt instead.\r\n   - Input:\r\n     - `ctx` (Context): The FastMCP context object.\r\n   - Returns: Number of triples as a string, or an error message.\r\n\r\n\r\n- `full_text_search`\r\n   - Perform a full-text search on the graph or endpoint, avoiding proprietary syntax.\r\n   - Input:\r\n     - `search_term` (str): The term to search for.\r\n     - `ctx` (Context): The FastMCP context object.\r\n   - Returns: Search results as a newline-separated string, or an error message.\r\n\r\n\r\n- `health_check`\r\n   - Check the health of the triplestore connection.\r\n   - Input:\r\n     - `ctx` (Context): The FastMCP context object.\r\n   - Returns: 'Healthy' if the connection is good, 'Unhealthy: <error>' otherwise.\r\n\r\n\r\n- `get_mode`\r\n   - Get the current mode of RDF Explorer. Useful for knowledge graph and semantic tech users to verify data source.\r\n   - Input:\r\n     - `ctx` (Context): The FastMCP context object.\r\n   - Returns: A message indicating the mode and dataset or endpoint.\r\n\r\n\r\n### Resources\r\n\r\nThe server exposes the following resources:\r\n- `schema://all`: Retrieve schema information (classes and properties) from the graph.\r\n  - Returns: A newline-separated string of schema elements (classes and properties).\r\n\r\n- `queries://{template_name}`: Retrieve a predefined SPARQL query template by name.\r\n  - Returns: The SPARQL query string or 'Template not found'.\r\n\r\n- `explore://{query_name}`: Execute an exploratory SPARQL query by name and return results in JSON.\r\n  - `query_name` (str): The name of the exploratory query (e.g., 'classes', 'relationships/URI').\r\n  - Returns: JSON string of query results.\r\n\r\n- `explore://report`: Generate a Markdown report of exploratory queries.\r\n  - Returns: A Markdown-formatted report string.\r\n\r\n\r\n\r\n### Prompts\r\n\r\nThe server exposes the following prompts:\r\n- `analyze_graph_structure`: Initiate an analysis of the graph structure with schema data.\r\n  - Returns: A list of messages to guide graph structure analysis.\r\n\r\n- `find_relationships`: Generate a SPARQL query to find relationships for a given subject.\r\n  - Returns: A SPARQL query string to find relationships.\r\n\r\n- `text_to_sparql`: Convert a text prompt to a SPARQL query and execute it, with token limit checks.\r\n  - `prompt` (str): The text prompt to convert to SPARQL.\r\n  - Returns: Query results with usage stats, or an error message.\r\n \r\n\r\n\r\n\r\n## Setup\r\n\r\n## Configuration\r\n\r\n### Installing on Claude Desktop\r\nBefore starting make sure [Claude Desktop](https://claude.ai/download) is installed.\r\n1. Go to: Settings > Developer > Edit Config\r\n\r\n2. Add the following to your `claude_desktop_config.json`:\r\nOn MacOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\r\nOn Windows: `%APPDATA%/Claude/claude_desktop_config.json`\r\n\r\nTo use with a local RDF Turtle file, use this version with `--triple-file` args\r\n```json\r\n{\r\n  \"mcpServers\": {\r\n    \"rdf_explorer\": {\r\n      \"command\": \"C:\\\\path\\\\to\\\\venv\\\\Scripts\\\\python.exe\",\r\n      \"args\": [\"C:\\\\path\\\\to\\\\server.py\", \"--triple-file\", \"your_file.ttl\"]\r\n    }\r\n  }\r\n}\r\n```\r\n\r\nTo use with a SPARQL Endpoint, use this version with `--sparql-endpoint` args\r\n```json\r\n{\r\n  \"mcpServers\": {\r\n    \"rdf_explorer\": {\r\n      \"command\": \"C:\\\\path\\\\to\\\\venv\\\\Scripts\\\\python.exe\",\r\n      \"args\": [\"C:\\\\path\\\\to\\\\server.py\", \"--sparql-endpoint\", \"https://example.com/sparql\"]\r\n    }\r\n  }\r\n}\r\n```\r\n\r\n3. Restart Claude Desktop and start querying and exploring graph data.\r\n\r\n4. Prompt: \"what mode is RDF Explorer running?\"\r\n\r\n\r\n\r\n\r\n## Usage Examples\r\n\r\nHere are examples of how you can explore RDF data using natural language:\r\n\r\n### Querying Data in Local File Mode\r\n\r\nYou can ask questions like:\r\n- \"Show me all employees in the Sales department\"\r\n- \"Find the top 5 oldest customers\"\r\n- \"Who has purchased more than 3 products in the last month?\"\r\n- \"List all entities\" \r\n- \"Using the DBpedia endpoint, list 10 songs by Michael Jackson\" \r\n- \"Using the Wikidata endpoint, list 5 cities\"\r\n- \"count the triples\"\r\n- \"analyze the graph structure\"\r\n- \"Select ...\"\r\n- \"search '{text}' \"\r\n- \"find relationships of '{URI}'\"\r\n- \"what mode is RDF Explorer running?\"\r\n\r\n### Querying Data in SPARQL Endpoint Mode\r\n\r\nYou can ask questions like:\r\n- \"Using the DBpedia endpoint, list 10 songs by Michael Jackson\" \r\n- \"Using the Wikidata endpoint, list 5 cities\"\r\n- \"Select ...\"\r\n- \"search '{text}' \"\r\n- \"find relationships of '{URI}'\"\r\n- \"what mode is RDF Explorer running?\"\r\n\r\n## Development\r\n```\r\n# clone the repository\r\ngit clone https://github.com/emekaokoye/mcp-rdf-explorer.git\r\ncd mcp-rdf-explorer\r\n\r\n# setup a virtual environment\r\npython -m venv venv\r\nsource venv/bin/activate\r\n# windows: venv\\Scripts\\activate\r\n\r\n# install development dependencies\r\npip install -r requirements.txt\r\n\r\n# run tests\r\npytest test_rdf_explorer.py -v\r\n```\r\n\r\n## License\r\n\r\nThis MCP server is licensed under the MIT License. This means you are free to use, modify, and distribute the software, subject to the terms and conditions of the MIT License. For more details, please see the [license](LICENSE) file in the project repository.\r\n\r\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "sparql",
        "rdf",
        "queries",
        "analyze rdf",
        "rdf data",
        "rdf explorer"
      ],
      "category": "scientific-research-tools"
    },
    "fdrechsler--mcp-server-idapro": {
      "owner": "fdrechsler",
      "name": "mcp-server-idapro",
      "url": "https://github.com/fdrechsler/mcp-server-idapro",
      "imageUrl": "/freedevtools/mcp/pfp/fdrechsler.webp",
      "description": "Automate reverse engineering and binary analysis tasks using IDA Pro through a standardized interface. Execute scripts, retrieve binary information, and perform advanced analysis to enhance productivity in software analysis.",
      "stars": 72,
      "forks": 21,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-10-04T02:08:23Z",
      "readme_content": "# IDA Pro MCP Server\n\nA Model Context Protocol (MCP) server that enables AI assistants to interact with IDA Pro for reverse engineering and binary analysis tasks.\n\n<a href=\"https://glama.ai/mcp/servers/@fdrechsler/mcp-server-idapro\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@fdrechsler/mcp-server-idapro/badge\" alt=\"IDA Pro Server MCP server\" />\n</a>\n\n## Overview\n\nThis project provides a bridge between AI assistants and IDA Pro, a popular disassembler and debugger used for reverse engineering software. It consists of three main components:\n\n1. **IDA Pro Remote Control Plugin** (`ida_remote_server.py`): An IDA Pro plugin that creates an HTTP server to remotely control IDA Pro functions.\n2. **IDA Remote Client** (`idaremoteclient.ts`): A TypeScript client for interacting with the IDA Pro Remote Control Server.\n3. **MCP Server** (`index.ts`): A Model Context Protocol server that exposes IDA Pro functionality to AI assistants.\n\n## Features\n\n- Execute Python scripts in IDA Pro from AI assistants\n- Retrieve information about binaries:\n  - Strings\n  - Imports\n  - Exports\n  - Functions\n- Advanced binary analysis capabilities:\n  - Search for immediate values in instructions\n  - Search for text strings in the binary\n  - Search for specific byte sequences\n  - Get disassembly for address ranges\n- Automate IDA Pro operations through a standardized interface\n- Secure communication between components\n\n## Prerequisites\n\n- IDA Pro 8.3 or later\n- Node.js 18 or later\n- TypeScript\n\n### Example usage ida_remote_server.py\n\n```bash\ncurl -X POST -H \"Content-Type: application/json\" -d '{\"script\":\"print(\\\"Script initialization...\\\")\"}' http://127.0.0.1:9045/api/execute\n{\"success\": true, \"output\": \"Script initialization...\\n\"}\n```\n\n### Example usage MCP Server\n\n\n\n## Installation\n\n### 1. Install the IDA Pro Remote Control Plugin\n\n1. Copy `ida_remote_server.py` to your IDA Pro plugins directory:\n   - Windows: `%PROGRAMFILES%\\IDA Pro\\plugins`\n   - macOS: `/Applications/IDA Pro.app/Contents/MacOS/plugins`\n   - Linux: `/opt/idapro/plugins`\n\n2. Start IDA Pro and open a binary file.\n\n3. The plugin will automatically start an HTTP server on `127.0.0.1:9045`.\n\n### 2. Install the MCP Server\n\n1. Clone this repository:\n   ```bash\n   git clone <repository-url>\n   cd ida-server\n   ```\n\n2. Install dependencies:\n   ```bash\n   npm install\n   ```\n\n3. Build the project:\n   ```bash\n   npm run build\n   ```\n\n4. Configure the MCP server in your AI assistant's MCP settings file:\n   ```json\n   {\n     \"mcpServers\": {\n       \"ida-pro\": {\n         \"command\": \"node\",\n         \"args\": [\"path/to/ida-server/dist/index.js\"],\n         \"env\": {}\n       }\n     }\n   }\n   ```\n\n## Usage\n\nOnce installed and configured, the MCP server provides the following tool to AI assistants:\n\n### run_ida_command\n\nExecutes an IDA Pro Python script.\n\n**Parameters:**\n- `scriptPath` (required): Absolute path to the script file to execute\n- `outputPath` (optional): Absolute path to save the script's output to\n\n**Example:**\n\n```python\n# Example IDA Pro script (save as /path/to/script.py)\nimport idautils\n\n# Count functions\nfunction_count = len(list(idautils.Functions()))\nprint(f\"Binary has {function_count} functions\")\n\n# Get the first 5 function names\nfunctions = list(idautils.Functions())[:5]\nfor func_ea in functions:\n    print(f\"Function: {ida_name.get_ea_name(func_ea)} at {hex(func_ea)}\")\n\n# Return data\nreturn_value = function_count\n```\n\nThe AI assistant can then use this script with:\n\n```\n<use_mcp_tool>\n<server_name>ida-pro</server_name>\n<tool_name>run_ida_command</tool_name>\n<arguments>\n{\n  \"scriptPath\": \"/path/to/script.py\"\n}\n</arguments>\n</use_mcp_tool>\n```\n\n### search_immediate_value\n\nSearches for immediate values in the binary's instructions.\n\n**Parameters:**\n- `value` (required): Value to search for (number or string)\n- `radix` (optional): Radix for number conversion (default: 16)\n- `startAddress` (optional): Start address for search\n- `endAddress` (optional): End address for search\n\n**Example:**\n\n```\n<use_mcp_tool>\n<server_name>ida-pro</server_name>\n<tool_name>search_immediate_value</tool_name>\n<arguments>\n{\n  \"value\": \"42\",\n  \"radix\": 10\n}\n</arguments>\n</use_mcp_tool>\n```\n\n### search_text\n\nSearches for text strings in the binary.\n\n**Parameters:**\n- `text` (required): Text to search for\n- `caseSensitive` (optional): Whether the search is case sensitive (default: false)\n- `startAddress` (optional): Start address for search\n- `endAddress` (optional): End address for search\n\n**Example:**\n\n```\n<use_mcp_tool>\n<server_name>ida-pro</server_name>\n<tool_name>search_text</tool_name>\n<arguments>\n{\n  \"text\": \"password\",\n  \"caseSensitive\": false\n}\n</arguments>\n</use_mcp_tool>\n```\n\n### search_byte_sequence\n\nSearches for a specific byte sequence in the binary.\n\n**Parameters:**\n- `bytes` (required): Byte sequence to search for (e.g., \"90 90 90\" for three NOPs)\n- `startAddress` (optional): Start address for search\n- `endAddress` (optional): End address for search\n\n**Example:**\n\n```\n<use_mcp_tool>\n<server_name>ida-pro</server_name>\n<tool_name>search_byte_sequence</tool_name>\n<arguments>\n{\n  \"bytes\": \"90 90 90\"\n}\n</arguments>\n</use_mcp_tool>\n```\n\n### get_disassembly\n\nGets disassembly for an address range.\n\n**Parameters:**\n- `startAddress` (required): Start address for disassembly\n- `endAddress` (optional): End address for disassembly\n- `count` (optional): Number of instructions to disassemble\n\n**Example:**\n\n```\n<use_mcp_tool>\n<server_name>ida-pro</server_name>\n<tool_name>get_disassembly</tool_name>\n<arguments>\n{\n  \"startAddress\": \"0x401000\",\n  \"count\": 10\n}\n</arguments>\n</use_mcp_tool>\n```\n\n### get_functions\n\nGets the list of functions from the binary.\n\n**Parameters:**\n- None required\n\n**Example:**\n\n```\n<use_mcp_tool>\n<server_name>ida-pro</server_name>\n<tool_name>get_functions</tool_name>\n<arguments>\n{}\n</arguments>\n</use_mcp_tool>\n```\n\n### get_exports\n\nGets the list of exports from the binary.\n\n**Parameters:**\n- None required\n\n**Example:**\n\n```\n<use_mcp_tool>\n<server_name>ida-pro</server_name>\n<tool_name>get_exports</tool_name>\n<arguments>\n{}\n</arguments>\n</use_mcp_tool>\n```\n\n### get_strings\n\nGets the list of strings from the binary.\n\n**Parameters:**\n- None required\n\n**Example:**\n\n```\n<use_mcp_tool>\n<server_name>ida-pro</server_name>\n<tool_name>get_strings</tool_name>\n<arguments>\n{}\n</arguments>\n</use_mcp_tool>\n```\n\n## IDA Pro Remote Control API\n\nThe IDA Pro Remote Control Plugin exposes the following HTTP endpoints:\n\n- `GET /api/info`: Get plugin information\n- `GET /api/strings`: Get strings from the binary\n- `GET /api/exports`: Get exports from the binary\n- `GET /api/imports`: Get imports from the binary\n- `GET /api/functions`: Get function list\n- `GET /api/search/immediate`: Search for immediate values in instructions\n- `GET /api/search/text`: Search for text in the binary\n- `GET /api/search/bytes`: Search for byte sequences in the binary\n- `GET /api/disassembly`: Get disassembly for an address range\n- `POST /api/execute`: Execute Python script (JSON/Form)\n- `POST /api/executebypath`: Execute Python script from file path\n- `POST /api/executebody`: Execute Python script from raw body\n\n## Security Considerations\n\nBy default, the IDA Pro Remote Control Plugin only listens on `127.0.0.1` (localhost) for security reasons. This prevents remote access to your IDA Pro instance.\n\nIf you need to allow remote access, you can modify the `DEFAULT_HOST` variable in `ida_remote_server.py`, but be aware of the security implications.\n\n## Development\n\n### Building from Source\n\n```bash\nnpm run build\n```\n\n### Running Tests\n\n```bash\nnpm test\n```\n\n## License\n\nThis project is licensed under the MIT License. See the [LICENSE](LICENSE) file for details.\n\n## Author\n\nFlorian Drechsler (@fdrechsler) fd@fdrechsler.com",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "idapro",
        "ida",
        "binary",
        "idapro automate",
        "server idapro",
        "ida pro"
      ],
      "category": "scientific-research-tools"
    },
    "finite-sample--rmcp": {
      "owner": "finite-sample",
      "name": "rmcp",
      "url": "https://github.com/finite-sample/rmcp",
      "imageUrl": "/freedevtools/mcp/pfp/finite-sample.webp",
      "description": "Perform advanced econometric analyses using R, including linear regression, panel data modeling, and various diagnostic tests. Facilitate data-driven decision-making through sophisticated statistical analyses.",
      "stars": 183,
      "forks": 10,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-10-04T10:17:45Z",
      "readme_content": "# RMCP: Statistical Analysis through Natural Conversation\n\n[![Python application](https://github.com/finite-sample/rmcp/actions/workflows/ci.yml/badge.svg)](https://github.com/finite-sample/rmcp/actions/workflows/ci.yml)\n[![PyPI version](https://img.shields.io/pypi/v/rmcp.svg)](https://pypi.org/project/rmcp/)\n[![Downloads](https://pepy.tech/badge/rmcp)](https://pepy.tech/project/rmcp)\n[![Documentation](https://img.shields.io/badge/docs-latest-brightgreen.svg)](https://finite-sample.github.io/rmcp/)\n[![License](https://img.shields.io/github/license/finite-sample/rmcp)](https://github.com/finite-sample/rmcp/blob/main/LICENSE)\n\n**Turn conversations into comprehensive statistical analysis** - A Model Context Protocol (MCP) server with 44 statistical analysis tools across 11 categories. RMCP enables AI assistants to perform sophisticated statistical modeling, econometric analysis, machine learning, time series analysis, and data science tasks through natural conversation.\n\n## 🚀 Quick Start (30 seconds)\n\n```bash\npip install rmcp\nrmcp start\n```\n\nThat's it! RMCP is now ready to handle statistical analysis requests via Claude Desktop or any MCP client.\n\n**🎯 [Working examples →](examples/quick_start_guide.md)** | **🔧 [Troubleshooting →](#-quick-troubleshooting)**\n\n## ✨ What Can RMCP Do?\n\n### 📊 **Regression & Economics** \nLinear regression, logistic models, panel data, instrumental variables → *\"Analyze ROI of marketing spend\"*\n\n### ⏰ **Time Series & Forecasting**\nARIMA models, decomposition, stationarity testing → *\"Forecast next quarter's sales\"*\n\n### 🧠 **Machine Learning** \nClustering, decision trees, random forests → *\"Segment customers by behavior\"*\n\n### 📈 **Statistical Testing**\nT-tests, ANOVA, chi-square, normality tests → *\"Is my A/B test significant?\"*\n\n### 📋 **Data Analysis**\nDescriptive stats, outlier detection, correlation analysis → *\"Summarize this dataset\"*\n\n### 🔄 **Data Transformation**\nStandardization, winsorization, lag/lead variables → *\"Prepare data for modeling\"*\n\n### 📊 **Professional Visualizations**\nInline plots in Claude: scatter plots, histograms, heatmaps → *\"Show me a correlation matrix\"*\n\n### 📁 **Smart File Operations**\nCSV, Excel, JSON import with validation → *\"Load and analyze my sales data\"*\n\n### 🤖 **Natural Language Features**\nFormula building, error recovery, example datasets → *\"Help me build a regression formula\"*\n\n**👉 [See working examples →](examples/quick_start_guide.md)**\n\n## 📊 Real Usage with Claude\n\n### Business Analysis\n**You:** *\"I have sales data and marketing spend. Can you analyze the ROI?\"*\n\n**Claude:** *\"I'll run a regression analysis to measure marketing effectiveness...\"*\n\n**Result:** *\"Every $1 spent on marketing generates $4.70 in sales. The relationship is highly significant (p < 0.001) with R² = 0.979\"*\n\n### Economic Research\n**You:** *\"Test if GDP growth and unemployment follow Okun's Law using my country data\"*\n\n**Claude:** *\"I'll analyze the correlation between GDP growth and unemployment...\"*\n\n**Result:** *\"Strong support for Okun's Law: correlation r = -0.944. Higher GDP growth significantly reduces unemployment.\"*\n\n### Customer Analytics  \n**You:** *\"Predict customer churn using tenure and monthly charges\"*\n\n**Claude:** *\"I'll build a logistic regression model for churn prediction...\"*\n\n**Result:** *\"Model achieves 100% accuracy. Each additional month of tenure reduces churn risk by 11.3%. Higher charges increase churn risk by 3% per dollar.\"*\n\n## 📦 Installation\n\n### Prerequisites\n- **Python 3.10+** \n- **R 4.0+** with packages: Install all at once:\n\n```r\ninstall.packages(c(\n  \"jsonlite\", \"plm\", \"lmtest\", \"sandwich\", \"AER\", \"dplyr\",\n  \"forecast\", \"vars\", \"urca\", \"tseries\", \"nortest\", \"car\",\n  \"rpart\", \"randomForest\", \"ggplot2\", \"gridExtra\", \"tidyr\", \n  \"rlang\", \"knitr\", \"broom\"\n))\n```\n\n### Install RMCP\n\n```bash\n# Standard installation\npip install rmcp\n\n# With HTTP transport support\npip install rmcp[http]\n\n# Development installation\ngit clone https://github.com/finite-sample/rmcp.git\ncd rmcp\npip install -e \".[dev]\"\n```\n\n### Claude Desktop Integration\n\nAdd to your Claude Desktop MCP configuration:\n\n```json\n{\n  \"mcpServers\": {\n    \"rmcp\": {\n      \"command\": \"rmcp\",\n      \"args\": [\"start\"]\n    }\n  }\n}\n```\n\n### Command Line Usage\n\n```bash\n# Start MCP server (for Claude Desktop)\nrmcp start\n\n# Start HTTP server (for web apps)\nrmcp serve-http --port 8080\n\n# Check installation\nrmcp --version\n```\n\n## 🔥 Key Features\n\n- **🎯 Natural Conversation**: Ask questions in plain English, get statistical analysis\n- **📊 Professional Output**: Formatted results with markdown tables and inline visualizations  \n- **🔒 Production Ready**: Full MCP protocol compliance with HTTP transport and SSE\n- **⚡ Fast & Reliable**: 100% test success rate across all scenarios\n- **🌐 Multiple Transports**: stdio (Claude Desktop) and HTTP (web applications)\n- **🛡️ Secure**: Controlled R execution with configurable permissions\n\n## 📚 Documentation\n\n| Resource | Description |\n|----------|-------------|\n| **[Quick Start Guide](examples/quick_start_guide.md)** | Copy-paste ready examples with real data |\n| **[Economic Research Examples](examples/economic_research_example.md)** | Panel data, time series, advanced econometrics |\n| **[Time Series Examples](examples/advanced_time_series_example.md)** | ARIMA, forecasting, decomposition |\n| **[Image Display Examples](examples/image_display_example.md)** | Inline visualizations in Claude |\n| **[API Documentation](docs/)** | Auto-generated API reference |\n\n## 🧪 Validation\n\nRMCP has been tested with real-world scenarios achieving **100% success rate**:\n\n- ✅ **Business Analysts**: Sales forecasting with 97.9% R², $4.70 ROI per marketing dollar\n- ✅ **Economists**: Macroeconomic analysis confirming Okun's Law (r=-0.944)  \n- ✅ **Data Scientists**: Customer churn prediction with 100% accuracy\n- ✅ **Researchers**: Treatment effect analysis with significant results (p<0.001)\n\n## 🤝 Contributing\n\nWe welcome contributions! \n\n```bash\ngit clone https://github.com/finite-sample/rmcp.git\ncd rmcp\npip install -e \".[dev]\"\n\n# Run tests\npython tests/unit/test_new_tools.py\npython tests/e2e/test_claude_desktop_scenarios.py\n\n# Format code\nblack rmcp/\n```\n\nSee [CONTRIBUTING.md](CONTRIBUTING.md) for detailed guidelines.\n\n## 📄 License\n\nMIT License - see [LICENSE](LICENSE) file for details.\n\n## 🛠️ Quick Troubleshooting\n\n**R not found?**\n```bash\n# macOS: brew install r\n# Ubuntu: sudo apt install r-base\nR --version\n```\n\n**Missing R packages?**\n```bash\nrmcp check-r-packages  # Check what's missing\n```\n\n**MCP connection issues?**\n```bash\necho '{\"jsonrpc\":\"2.0\",\"id\":1,\"method\":\"tools/list\",\"params\":{}}' | rmcp start\n```\n\n**📖 Need more help?** Check the [examples](examples/) directory for working code.\n\n## 🙋 Support\n\n- 🐛 **Issues**: [GitHub Issues](https://github.com/finite-sample/rmcp/issues)\n- 💬 **Discussions**: [GitHub Discussions](https://github.com/finite-sample/rmcp/discussions)\n- 📖 **Examples**: [Working examples](examples/quick_start_guide.md)\n\n---\n\n**Ready to turn conversations into statistical insights?** Install RMCP and start analyzing data through AI assistants today! 🚀\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "econometric",
        "analyses",
        "rmcp",
        "econometric analyses",
        "advanced econometric",
        "statistical analyses"
      ],
      "category": "scientific-research-tools"
    },
    "gp-zhong--Physics": {
      "owner": "gp-zhong",
      "name": "Physics",
      "url": "https://github.com/gp-zhong/Physics",
      "imageUrl": "/freedevtools/mcp/pfp/gp-zhong.webp",
      "description": "A high-fidelity prototype for learning physics concepts through interactive 3D simulations and AI-assisted learning. It offers a data visualization feature to help students understand physical experiments more intuitively.",
      "stars": 0,
      "forks": 0,
      "license": "No License",
      "language": "HTML",
      "updated_at": "2025-04-05T09:09:24Z",
      "readme_content": "# 物理模拟实验室小程序 - 高保真原型\n\n这是一个基于 HTML、CSS 和 JavaScript 开发的物理模拟实验室小程序的高保真原型。通过模拟真实的物理实验场景，帮助用户通过互动和视觉化的方式学习物理知识。\n\n## 项目概述\n\n物理模拟实验室小程序旨在通过交互式 3D 模拟实验，帮助学生更直观地理解和学习物理概念。原型采用现代简约的设计风格，强调线条和几何形状，营造高端科技感，适合各年龄段的学习者。\n\n## 设计规范\n\n- **整体风格**：现代简约的黑白灰配色，强调线条和几何形状\n- **配色方案**：\n  - 主色调：天空蓝 (#4A90E2) 和白色 (#FFFFFF)\n  - 辅助色：浅绿色 (#7ED321)\n  - 点缀色：黄色 (#F8E71C)\n  - 背景色：浅灰 (#F5F5F5)\n  - 文本色：深灰 (#333333)\n- **字体选择**：思源黑体/Noto Sans SC\n- **视觉元素**：简洁线条图标，高质量特写产品图片，简单几何图形背景\n\n## 如何运行原型\n\n1. 克隆或下载本项目代码\n2. 使用现代浏览器（如 Chrome、Firefox、Safari）打开 `index.html` 文件\n3. 你将看到包含所有原型页面的预览页面，每个页面显示在 iPhone 15 Pro 的模拟框架中\n4. 点击各个页面可以查看详细的交互原型\n\n## 包含的页面\n\n1. **启动页**：展示应用名称、logo和开始按钮\n2. **首页/实验库**：展示分类实验、推荐实验和最近使用\n3. **实验详情页**：展示实验信息、介绍和相关知识点\n4. **3D实验操作界面**：自由落体实验模拟，可调整各种物理参数\n5. **实验数据分析页面**：展示实验数据的图表和表格分析\n6. **AI辅助学习对话界面**：提供智能问答功能解释物理概念\n7. **游戏化成就系统**：展示用户学习进度、勋章和挑战任务\n8. **个人中心（我的）**：展示用户信息、学习统计、实验历史和功能入口\n\n## 核心功能\n\n- **实验库与分类管理**：直观的卡片界面，方便用户查找实验\n- **3D交互式实验模拟**：参数可调整的自由落体实验模拟\n- **实验教学与引导**：详细的实验介绍、相关知识点和步骤引导\n- **游戏化学习系统**：成就系统和勋章展示激发学习动力\n- **数据可视化与报告生成**：图表和数据表格展示实验结果\n- **AI辅助学习**：智能问答解决物理问题\n- **社交与协作功能**：排行榜和成就分享功能\n- **个人中心管理**：查看学习数据、实验历史和系统设置\n\n## 技术栈\n\n- HTML5\n- CSS3（使用 TailwindCSS）\n- JavaScript\n- Font Awesome 图标库\n- Chart.js 数据可视化库\n\n## 后续开发建议\n\n1. 使用 React 或 Vue 等前端框架重构界面，提升代码复用性和维护性\n2. 集成 Three.js 和 Cannon.js 等物理引擎实现真实的 3D 模拟\n3. 使用 ECharts 或 D3.js 增强数据可视化能力\n4. 接入实际的 AI 接口实现智能问答功能\n5. 使用 WebSocket 或 WebRTC 技术实现多人协作功能\n\n## 预览\n\n打开 `index.html` 查看完整的原型预览。\n\n## 作者\n\n本原型是基于物理模拟实验室小程序的低保真原型设计，升级为高保真原型的设计实现。 ",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "physics",
        "simulations",
        "zhong",
        "zhong physics",
        "learning physics",
        "gp zhong"
      ],
      "category": "scientific-research-tools"
    },
    "gradusnikov--pubmed-search-mcp-server": {
      "owner": "gradusnikov",
      "name": "pubmed-search-mcp-server",
      "url": "https://github.com/gradusnikov/pubmed-search-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/gradusnikov.webp",
      "description": "Search and retrieve academic papers from the PubMed database, providing access to titles, authors, journals, abstracts, and DOIs.",
      "stars": 6,
      "forks": 5,
      "license": "BSD 3-Clause \"New\" or \"Revised\" License",
      "language": "Python",
      "updated_at": "2025-07-08T04:10:26Z",
      "readme_content": "# PubMedSearch MCP Server\n[![smithery badge](https://smithery.ai/badge/@gradusnikov/pubmed-search-mcp-server)](https://smithery.ai/server/@gradusnikov/pubmed-search-mcp-server)\n\nA Model Content Protocol server that provides tools to search and retrieve academic papers from PubMed database.\n\n## Features\n\n- Search PubMed by keywords in title/abstract or author names\n- Retrieve detailed information including title, authors, journal, abstract, DOI and more\n\n### Installing via Smithery\n\nTo install pubmed-search-mcp-server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@gradusnikov/pubmed-search-mcp-server):\n\n```bash\nnpx -y @smithery/cli install @gradusnikov/pubmed-search-mcp-server --client claude\n```\n\n### Installing Manually\n1. Clone this repository:\n   ```\n   git clone <repository-url>\n   cd pubmed-search-mcp-server\n   ```\n\n2. Install dependencies:\n   ```\n   pip install fastmcp requests python-dotenv\n   ```\n\n3. Create a `.env` file in the project root (if needed for configuration)\n\n## Usage\n\n1. Start the server in development mode:\n   ```\n   mcp dev pubmed_search_mcp_server.py\n   ```\n\n2. or add the server to Claude Desktop app *CLAUDE_DIRECTORY/claude_desktop_config.json*. For example if you are using Windows Subsystem for Linux (WSL) it may look like this:\n\n   ```\n   \"pubmed-search\": {\n               \"command\": \"wsl.exe\",\n               \"args\": [\n                   \"bash\",\n                   \"-c\",\n                   \"source /home/[user]/anaconda3/etc/profile.d/conda.sh && conda activate mcp && mcp run /home/[user]/pubmed-search-mpc-server/pubmed_search_mcp_server.py\"\n               ]\n           },\n   ```\n\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "pubmed",
        "journals",
        "gradusnikov",
        "pubmed search",
        "gradusnikov pubmed",
        "pubmed database"
      ],
      "category": "scientific-research-tools"
    },
    "graphistry--graphistry-mcp": {
      "owner": "graphistry",
      "name": "graphistry-mcp",
      "url": "https://github.com/graphistry/graphistry-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/graphistry.webp",
      "description": "Provides GPU-accelerated graph visualization and advanced network analytics for complex network data, facilitating pattern discovery and community detection through a standardized API.",
      "stars": 8,
      "forks": 2,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-06-24T09:08:33Z",
      "readme_content": "# Graphistry MCP Integration\n\nGPU-accelerated graph visualization and analytics for Large Language Models using Graphistry and MCP.\n\n## Overview\n\nThis project integrates Graphistry's powerful GPU-accelerated graph visualization platform with the Model Control Protocol (MCP), enabling advanced graph analytics capabilities for AI assistants and LLMs. It allows LLMs to visualize and analyze complex network data through a standardized, LLM-friendly interface.\n\n**Key features:**\n- GPU-accelerated graph visualization via Graphistry\n- Advanced pattern discovery and relationship analysis\n- Network analytics (community detection, centrality, path finding, anomaly detection)\n- Support for various data formats (Pandas, NetworkX, edge lists)\n- LLM-friendly API: single `graph_data` dict for graph tools\n\n## 🚨 Important: Graphistry Registration Required\n\n**This MCP server requires a free Graphistry account to use visualization features.**\n\n1. Sign up for a free account at [hub.graphistry.com](https://hub.graphistry.com)\n2. Set your credentials as environment variables or in a `.env` file before starting the server:\n   ```bash\n   export GRAPHISTRY_USERNAME=your_username\n   export GRAPHISTRY_PASSWORD=your_password\n   # or create a .env file with:\n   # GRAPHISTRY_USERNAME=your_username\n   # GRAPHISTRY_PASSWORD=your_password\n   ```\n   See `.env.example` for a template.\n\n## MCP Configuration (.mcp.json)\n\nTo use this project with Cursor or other MCP-compatible tools, you need a `.mcp.json` file in your project root. A template is provided as `.mcp.json.example`.\n\n**Setup:**\n\n```bash\ncp .mcp.json.example .mcp.json\n```\n\nEdit `.mcp.json` to:\n- Set the correct paths for your environment (e.g., project root, Python executable, server script)\n- Set your Graphistry credentials (or use environment variables/.env)\n- Choose between HTTP and stdio modes:\n  - `graphistry-http`: Connects via HTTP (set the `url` to match your server's port)\n  - `graphistry`: Connects via stdio (set the `command`, `args`, and `env` as needed)\n\n**Note:**\n- `.mcp.json.example` contains both HTTP and stdio configurations. Enable/disable as needed by setting the `disabled` field.\n- See `.env.example` for environment variable setup.\n\n## Installation\n\n### Recommended Installation (Python venv + pip)\n\n```bash\n# Clone the repository\ngit clone https://github.com/graphistry/graphistry-mcp.git\ncd graphistry-mcp\n\n# Set up virtual environment and install dependencies\npython3 -m venv .venv\nsource .venv/bin/activate\npip install -e \".[dev]\"\n\n# Set up your Graphistry credentials (see above)\n```\n\nOr use the setup script:\n\n```bash\n./setup-graphistry-mcp.sh\n```\n\n## Usage\n\n### Starting the Server\n\n```bash\n# Activate your virtual environment if not already active\nsource .venv/bin/activate\n\n# Start the server (stdio mode)\npython run_graphistry_mcp.py\n\n# Or use the start script for HTTP or stdio mode (recommended, sources .env securely)\n./start-graphistry-mcp.sh --http 8080\n```\n\n### Security & Credential Handling\n\n- The server loads credentials from environment variables or `.env` using [python-dotenv](https://pypi.org/project/python-dotenv/), so you can safely use a `.env` file for local development.\n- The `start-graphistry-mcp.sh` script sources `.env` and is the most robust and secure way to launch the server.\n\n### Adding to Cursor (or other LLM tools)\n\n- Add the MCP server to your `.cursor/mcp.json` or equivalent config:\n  ```json\n  {\n    \"graphistry\": {\n      \"command\": \"/path/to/your/.venv/bin/python\",\n      \"args\": [\"/path/to/your/run_graphistry_mcp.py\"],\n      \"env\": {\n        \"GRAPHISTRY_USERNAME\": \"your_username\",\n        \"GRAPHISTRY_PASSWORD\": \"your_password\"\n      },\n      \"type\": \"stdio\"\n    }\n  }\n  ```\n- Make sure the virtual environment is used (either by using the full path to the venv's python, or by activating it before launching).\n- If you see errors about API version or missing credentials, double-check your environment variables and registration.\n\n### Example: Visualizing a Graph (LLM-friendly API)\n\nThe main tool, `visualize_graph`, now accepts a single `graph_data` dictionary. Example:\n\n```python\n{\n  \"graph_data\": {\n    \"graph_type\": \"graph\",\n    \"edges\": [\n      {\"source\": \"A\", \"target\": \"B\"},\n      {\"source\": \"A\", \"target\": \"C\"},\n      {\"source\": \"A\", \"target\": \"D\"},\n      {\"source\": \"A\", \"target\": \"E\"},\n      {\"source\": \"B\", \"target\": \"C\"},\n      {\"source\": \"B\", \"target\": \"D\"},\n      {\"source\": \"B\", \"target\": \"E\"},\n      {\"source\": \"C\", \"target\": \"D\"},\n      {\"source\": \"C\", \"target\": \"E\"},\n      {\"source\": \"D\", \"target\": \"E\"}\n    ],\n    \"nodes\": [\n      {\"id\": \"A\"}, {\"id\": \"B\"}, {\"id\": \"C\"}, {\"id\": \"D\"}, {\"id\": \"E\"}\n    ],\n    \"title\": \"5-node, 10-edge Complete Graph\",\n    \"description\": \"A complete graph of 5 nodes (K5) where every node is connected to every other node.\"\n  }\n}\n```\n\nExample (hypergraph):\n\n```python\n{\n  \"graph_data\": {\n    \"graph_type\": \"hypergraph\",\n    \"edges\": [\n      {\"source\": \"A\", \"target\": \"B\", \"group\": \"G1\", \"weight\": 0.7},\n      {\"source\": \"A\", \"target\": \"C\", \"group\": \"G1\", \"weight\": 0.6},\n      {\"source\": \"B\", \"target\": \"C\", \"group\": \"G2\", \"weight\": 0.8},\n      {\"source\": \"A\", \"target\": \"D\", \"group\": \"G2\", \"weight\": 0.5}\n    ],\n    \"columns\": [\"source\", \"target\", \"group\"],\n    \"title\": \"Test Hypergraph\",\n    \"description\": \"A simple test hypergraph.\"\n  }\n}\n```\n\n## Available MCP Tools\n\nThe following MCP tools are available for graph visualization, analysis, and manipulation:\n\n- **visualize_graph**: Visualize a graph or hypergraph using Graphistry's GPU-accelerated renderer.\n- **get_graph_ids**: List all stored graph IDs in the current session.\n- **get_graph_info**: Get metadata (node/edge counts, title, description) for a stored graph.\n- **apply_layout**: Apply a standard layout (force_directed, radial, circle, grid) to a graph.\n- **detect_patterns**: Run network analysis (centrality, community detection, path finding, anomaly detection).\n- **encode_point_color**: Set node color encoding by column (categorical or continuous).\n- **encode_point_size**: Set node size encoding by column (categorical or continuous).\n- **encode_point_icon**: Set node icon encoding by column (categorical, with icon mapping or binning).\n- **encode_point_badge**: Set node badge encoding by column (categorical, with icon mapping or binning).\n- **apply_ring_categorical_layout**: Arrange nodes in rings by a categorical column (e.g., group/type).\n- **apply_group_in_a_box_layout**: Arrange nodes in group-in-a-box layout (requires igraph).\n- **apply_modularity_weighted_layout**: Arrange nodes by modularity-weighted layout (requires igraph).\n- **apply_ring_continuous_layout**: Arrange nodes in rings by a continuous column (e.g., score).\n- **apply_time_ring_layout**: Arrange nodes in rings by a datetime column (e.g., created_at).\n- **apply_tree_layout**: Arrange nodes in a tree (layered hierarchical) layout.\n- **set_graph_settings**: Set advanced visualization settings (point size, edge influence, etc.).\n\n## Contributing\n\nPRs and issues welcome! This project is evolving rapidly as we learn more about LLM-driven graph analytics and tool integration.\n\n## License\n\nMIT",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "graphistry",
        "graph",
        "visualization",
        "tools graphistry",
        "graphistry mcp",
        "graphistry graphistry"
      ],
      "category": "scientific-research-tools"
    },
    "hbg--mcp-paperswithcode": {
      "owner": "hbg",
      "name": "mcp-paperswithcode",
      "url": "https://github.com/hbg/mcp-paperswithcode",
      "imageUrl": "/freedevtools/mcp/pfp/hbg.webp",
      "description": "Interface with the PapersWithCode API to retrieve information about research papers, authors, conferences, and methods. Utilize helper tools for enhancing the information processing and searching academic papers.",
      "stars": 14,
      "forks": 5,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-08-09T01:30:55Z",
      "readme_content": "# mcp-paperswithcode\n\n[![smithery badge](https://smithery.ai/badge/@hbg/mcp-paperswithcode)](https://smithery.ai/server/@hbg/mcp-paperswithcode)\n\n# 🦾 Features\n\n> Allows AI assistants to find and read papers, as well as view related code repositories for further context.\n\nThis MCP server provides a Model Context Protocol (MCP) client that interfaces with the PapersWithCode API.\n\nIt includes tools for searching, retrieving, and parsing information on research papers, authors, datasets, conferences, and more.\n\n# 🚀 Getting Started\n\n### Installing via Smithery\n\nTo install mcp-paperswithcode for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@hbg/mcp-paperswithcode):\n\n```bash\nnpx -y @smithery/cli install @hbg/mcp-paperswithcode --client claude\n```\n\n# 🛠️ Tools\n\n## 📚 Paper Tools\n\n### `search_papers`\nSearch for papers using optional filters.\n\n- `abstract` (str, optional): Filter by abstract text.\n- `title` (str, optional): Filter by title text.\n- `arxiv_id` (str, optional): Filter by ArXiv ID.\n\n### `get_paper`\nGet a paper's metadata by its ID.\n\n- `paper_id` (str): The paper ID.\n\n### `read_paper_from_url`\nExtract readable text from a paper given its URL.\n\n- `paper_url` (str): The direct PDF or HTML URL to a paper.\n\n### `list_paper_results`\nList benchmark results associated with a paper.\n\n- `paper_id` (str): The paper ID.\n\n### `list_paper_tasks`\nList tasks associated with a paper.\n\n- `paper_id` (str): The paper ID.\n\n### `list_paper_methods`\nList methods discussed in a paper.\n\n- `paper_id` (str): The paper ID.\n\n### `list_paper_repositories`\nList code repositories linked to a paper.\n\n- `paper_id` (str): The paper ID.\n\n### `list_paper_datasets`\nList datasets mentioned or used in a paper.\n\n- `paper_id` (str): The paper ID.\n\n## 🧠 Research Area Tools\n\n### `search_research_areas`\nSearch research areas by name.\n\n- `name` (str): Partial or full name of the research area.\n\n### `get_research_area`\nGet metadata for a specific research area.\n\n- `area_id` (str): The area ID.\n\n### `list_research_area_tasks`\nList tasks associated with a research area.\n\n- `area_id` (str): The area ID.\n\n## 👨‍🔬 Author Tools\n\n### `search_authors`\nSearch authors by full name.\n\n- `full_name` (str): Full name of the author.\n\n### `get_paper_author`\nGet metadata for an author by ID.\n\n- `author_id` (str): The author ID.\n\n### `list_papers_by_author_id`\nList all papers written by an author via ID.\n\n- `author_id` (str): The author ID.\n\n### `list_papers_by_author_name`\nSearch by name and return papers for the first matching author.\n\n- `author_name` (str): Full name of the author.\n\n## 🎓 Conference Tools\n\n### `list_conferences`\nList conferences, optionally filter by name.\n\n- `conference_name` (str, optional): Full or partial name.\n\n### `get_conference`\nGet metadata for a specific conference.\n\n- `conference_id` (str): The conference ID.\n\n### `list_conference_proceedings`\nList all proceedings under a conference.\n\n- `conference_id` (str): The conference ID.\n\n### `get_conference_proceeding`\nGet details for a specific conference proceeding.\n\n- `conference_id` (str): The conference ID.\n- `proceeding_id` (str): The proceeding ID.\n\n### `list_conference_papers`\nList all papers for a specific conference proceeding.\n\n- `conference_id` (str): The conference ID.\n- `proceeding_id` (str): The proceeding ID.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "paperswithcode",
        "papers",
        "searching",
        "mcp paperswithcode",
        "paperswithcode interface",
        "paperswithcode api"
      ],
      "category": "scientific-research-tools"
    },
    "huanongfish--arxiv-mcp": {
      "owner": "huanongfish",
      "name": "arxiv-mcp",
      "url": "https://github.com/huanongfish/arxiv-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/huanongfish.webp",
      "description": "Search and access arXiv research papers through a simple interface, facilitating the retrieval and download of academic content for analysis. Streamline research with local storage and efficient access to extensive academic resources.",
      "stars": 6,
      "forks": 3,
      "license": "Apache License 2.0",
      "language": "Python",
      "updated_at": "2025-05-02T06:37:47Z",
      "readme_content": "[![Twitter Follow](https://img.shields.io/twitter/follow/JoeBlazick?style=social)](https://twitter.com/JoeBlazick)\n[![smithery badge](https://smithery.ai/badge/arxiv-mcp-server)](https://smithery.ai/server/arxiv-mcp-server)\n[![Python Version](https://img.shields.io/badge/python-3.11+-blue.svg)](https://www.python.org/downloads/)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![PyPI Downloads](https://img.shields.io/pypi/dm/arxiv-mcp-server.svg)](https://pypi.org/project/arxiv-mcp-server/)\n[![PyPI Version](https://img.shields.io/pypi/v/arxiv-mcp-server.svg)](https://pypi.org/project/arxiv-mcp-server/)\n\n# ArXiv MCP Server\n\n> 🔍 Enable AI assistants to search and access arXiv papers through a simple MCP interface.\n\nThe ArXiv MCP Server provides a bridge between AI assistants and arXiv's research repository through the Message Control Protocol (MCP). It allows AI models to search for papers and access their content in a programmatic way.\n\n<div align=\"center\">\n  \n🤝 **[Contribute](https://github.com/blazickjp/arxiv-mcp-server/blob/main/CONTRIBUTING.md)** • \n📝 **[Report Bug](https://github.com/blazickjp/arxiv-mcp-server/issues)**\n\n</div>\n\n## ✨ Core Features\n\n- 🔎 **Paper Search**: Query arXiv papers with filters for date ranges and categories\n- 📄 **Paper Access**: Download and read paper content\n- 📋 **Paper Listing**: View all downloaded papers\n- 🗃️ **Local Storage**: Papers are saved locally for faster access\n- 📝 **Prompts**: A Set of Research Prompts\n\n## 🚀 Quick Start\n\n### Installing via Smithery\n\nTo install ArXiv Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/arxiv-mcp-server):\n\n```bash\nnpx -y @smithery/cli install arxiv-mcp-server --client claude\n```\n\n### Installing Manually\nInstall using uv:\n\n```bash\nuv tool install arxiv-mcp-server\n```\n\nFor development:\n\n```bash\n# Clone and set up development environment\ngit clone https://github.com/blazickjp/arxiv-mcp-server.git\ncd arxiv-mcp-server\n\n# Create and activate virtual environment\nuv venv\nsource .venv/bin/activate\n\n# Install with test dependencies\nuv pip install -e \".[test]\"\n```\n\n### 🔌 MCP Integration\n\nAdd this configuration to your MCP client config file:\n\n```json\n{\n    \"mcpServers\": {\n        \"arxiv-mcp-server\": {\n            \"command\": \"uv\",\n            \"args\": [\n                \"tool\",\n                \"run\",\n                \"arxiv-mcp-server\",\n                \"--storage-path\", \"/path/to/paper/storage\"\n            ]\n        }\n    }\n}\n```\n\nFor Development:\n\n```json\n{\n    \"mcpServers\": {\n        \"arxiv-mcp-server\": {\n            \"command\": \"uv\",\n            \"args\": [\n                \"--directory\",\n                \"path/to/cloned/arxiv-mcp-server\",\n                \"run\",\n                \"arxiv-mcp-server\",\n                \"--storage-path\", \"/path/to/paper/storage\"\n            ]\n        }\n    }\n}\n```\n\n## 💡 Available Tools\n\nThe server provides four main tools:\n\n### 1. Paper Search\nSearch for papers with optional filters:\n\n```python\nresult = await call_tool(\"search_papers\", {\n    \"query\": \"transformer architecture\",\n    \"max_results\": 10,\n    \"date_from\": \"2023-01-01\",\n    \"categories\": [\"cs.AI\", \"cs.LG\"]\n})\n```\n\n### 2. Paper Download\nDownload a paper by its arXiv ID:\n\n```python\nresult = await call_tool(\"download_paper\", {\n    \"paper_id\": \"2401.12345\"\n})\n```\n\n### 3. List Papers\nView all downloaded papers:\n\n```python\nresult = await call_tool(\"list_papers\", {})\n```\n\n### 4. Read Paper\nAccess the content of a downloaded paper:\n\n```python\nresult = await call_tool(\"read_paper\", {\n    \"paper_id\": \"2401.12345\"\n})\n```\n\n## ⚙️ Configuration\n\nConfigure through environment variables:\n\n| Variable | Purpose | Default |\n|----------|---------|---------|\n| `ARXIV_STORAGE_PATH` | Paper storage location | ~/.arxiv-mcp-server/papers |\n\n## 🧪 Testing\n\nRun the test suite:\n\n```bash\npython -m pytest\n```\n\n## 📄 License\n\nReleased under the MIT License. See the LICENSE file for details.\n\n---\n\n<div align=\"center\">\n\nMade with ❤️ by the Pear Labs Team\n\n<a href=\"https://glama.ai/mcp/servers/04dtxi5i5n\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/04dtxi5i5n/badge\" alt=\"ArXiv Server MCP server\" /></a>\n</div>\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "research",
        "arxiv",
        "retrieval",
        "arxiv research",
        "research tools",
        "huanongfish arxiv"
      ],
      "category": "scientific-research-tools"
    },
    "hugohow--mcp-music-analysis": {
      "owner": "hugohow",
      "name": "mcp-music-analysis",
      "url": "https://github.com/hugohow/mcp-music-analysis",
      "imageUrl": "/freedevtools/mcp/pfp/hugohow.webp",
      "description": "Analyze audio files to extract features such as beat, duration, and MFCCs, including support for audio from local files, YouTube links, or direct audio links.",
      "stars": 16,
      "forks": 3,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-18T23:39:21Z",
      "readme_content": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/hugohow-mcp-music-analysis-badge.png)](https://mseep.ai/app/hugohow-mcp-music-analysis)\n\n# MCP Music Analysis\n[![smithery badge](https://smithery.ai/badge/@hugohow/mcp-music-analysis)](https://smithery.ai/server/@hugohow/mcp-music-analysis)\n\nThis repository contains a **Model Context Provider (MCP)** that uses MCP and [librosa](https://librosa.org/) for audio analysis on audio in local, youtube link, or audio link.\n\n## Usage with Claude Desktop\n\n<div style=\"display: flex; gap: 1rem;\">\n  \n  \n</div>\n\n## Installation\n\n### Installing via Smithery\n\nTo install Music Analysis for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@hugohow/mcp-music-analysis):\n\n```bash\nnpx -y @smithery/cli install @hugohow/mcp-music-analysis --client claude\n```\n\n### Manual Installation\n\n```bash\n# Clone repository\ngit clone git@github.com:hugohow/mcp-music-analysis.git\ncd mcp-music-analysis\n\n# Create virtual environment and install\nuv venv\nsource .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\nuv pip install -e .\n```\n\n### Usage with Claude Desktop\n\n#### Locate Configuration File\n\nThe configuration file location depends on your operating system:\n\n- **macOS**:\n  ```\n  ~/Library/Application\\ Support/Claude/claude_desktop_config.json\n  ```\n\n- **Windows**:\n  ```\n  %APPDATA%\\Claude\\claude_desktop_config.json\n  ```\n\n- **Linux**:\n  ```\n  ~/.config/Claude/claude_desktop_config.json\n  ```\n\nAdd the following to your `claude_desktop_config.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"music-analysis\": {\n      \"command\": \"uvx\",\n      \"args\": [\"-n\", \"mcp-music-analysis\"]\n    }\n  }\n}\n```\n\n## Example Prompts\n\nHere are some sample prompts you might use in a conversational or chat-based context once the server is running. The MCP will understand these requests and execute the relevant tools:\n\n```\nCan you analyze the beat of /Users/hugohow-choong/Desktop/sample-6s.mp3?\nCould you give me the duration of https://download.samplelib.com/mp3/sample-15s.mp3 ?\nPlease compute the MFCC for this file: /path/to/another_audio.mp3\nWhat are the spectral centroid values for /path/to/music.wav?\nI'd like to know the onset times for https://www.youtube.com/watch?v=8HFiFd9vx1c\n```\n\n## To-Do List\n\n- [x] Add URL to audio file download\n- [x] Add YouTube to audio file transformation\n- [ ] Experiment with multiple Python environments (testing)\n- [ ] Improve installation guide\n- [ ] Integrate Whisper for lyrics\n- [ ] Implement a Docker solution\n\n## Author\n\nHugo How-Choong",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "mcp",
        "mfccs",
        "audio",
        "mcp music",
        "analyze audio",
        "hugohow mcp"
      ],
      "category": "scientific-research-tools"
    },
    "iamleoluo--Quantum-Computing-Approach": {
      "owner": "iamleoluo",
      "name": "Quantum-Computing-Approach",
      "url": "https://github.com/iamleoluo/Quantum-Computing-Approach",
      "imageUrl": "/freedevtools/mcp/pfp/iamleoluo.webp",
      "description": "Simulate classical physics phenomena using quantum computing techniques and tools like Qiskit. Provides interactive visualizations and insights into complex systems such as superconductivity and magnetism.",
      "stars": 0,
      "forks": 0,
      "license": "No License",
      "language": "Jupyter Notebook",
      "updated_at": "2024-11-17T09:32:14Z",
      "readme_content": "# Quantum Computing Approach\n\n**Author:** Sheng Yun Wu  \n\n**Converted by:** Wei Che Lo \n\n**Course Website** [https://sites.google.com/gms.ndhu.edu.tw/qca](https://sites.google.com/gms.ndhu.edu.tw/qca)\n\n## Course Overview\n\nThe **Quantum Computing Approach** course introduces the application of quantum computing to simulate classical physics topics using qubits and quantum gates. It provides precise simulations of phenomena such as mechanics, thermodynamics, magnetism, and phase transitions.\n\n### Course Structure:\n- **Duration:** One to two semesters (12–15 weeks)\n- **Prerequisites:** Basic linear algebra, probability theory, and classical computing concepts\n- **Tools:** IBM Qiskit, Python\n\n## Introduction\n\nQuantum computing offers a revolutionary method for simulating and understanding classical physics topics by utilizing quantum mechanical principles like superposition, entanglement, and quantum interference. Classical physics often describes phenomena such as magnetism, thermodynamics, and phase transitions using deterministic models. Quantum computing, however, allows us to model these systems with unprecedented precision, capturing their quantum nature through qubits and quantum gates.\n\nThis approach allows quantum computers to simulate:\n- Spin systems\n- Wave-particle duality\n- Quantum tunneling\n- Quantum phase transitions\n- Superconductivity\n- Magnetism\n\nUsing frameworks like Qiskit, quantum computing provides tools for interactive simulations, enabling deeper insights into both classical and quantum phenomena.\n\n## Features\n\n- **Quantum Simulations**: Use quantum circuits to simulate classical physics phenomena.\n- **Interactive Visualizations**: Leverage Qiskit to visualize the evolution of quantum systems.\n- **Real-World Physics Applications**: Explore complex systems like superconductivity and magnetism with quantum models.\n\n## Installation\n\nTo set up the project, follow these steps:\n\n1. Clone the repository:\n    ```bash\n    git clone https://github.com/iamleoluo/Quantum-Computing-Approach.git\n    ```\n2. Install the required dependencies:\n    ```bash\n    pip install -r requirements.txt\n    ```\n## Usage\n\nWorking on it, please wait.\n\n## Addition Note for Qiskit 1.0 feature changes\nQiskit 1.0 brings new stability guarantees and performance improvements, but it does break compatibility with all previous versions of Qiskit.\nfor more information please check out https://docs.quantum.ibm.com/migration-guides/qiskit-1.0-features\n\n## Contact\n\nFor more information, contact the **Modern NanoMag Lab**, founded by Professor Sheng Yun Wu:\n\n**Address:**  \nDepartment of Physics, National Dong Hwa University  \n1-12, Sec 2, Da-Hsuen Rd., Shou-Feng, Hualien, 974 Taiwan\n\n**Phone:** +886-3-8903732  \n**Fax:** +886-3-8900166  \n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "quantum",
        "qiskit",
        "computing",
        "quantum computing",
        "iamleoluo quantum",
        "qiskit provides"
      ],
      "category": "scientific-research-tools"
    },
    "jayanthbagare--mcp-scholarly": {
      "owner": "jayanthbagare",
      "name": "mcp-scholarly",
      "url": "https://github.com/jayanthbagare/mcp-scholarly",
      "imageUrl": "/freedevtools/mcp/pfp/jayanthbagare.webp",
      "description": "Search for accurate academic articles by keyword to facilitate research workflows. Integrates with applications to provide seamless retrieval of scholarly papers.",
      "stars": 0,
      "forks": 0,
      "license": "MIT License",
      "language": "",
      "updated_at": "2025-05-06T10:07:43Z",
      "readme_content": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/adityak74-mcp-scholarly-badge.png)](https://mseep.ai/app/adityak74-mcp-scholarly)\n\n# mcp-scholarly MCP server\n[![smithery badge](https://smithery.ai/badge/mcp-scholarly)](https://smithery.ai/server/mcp-scholarly)\n\nA MCP server to search for accurate academic articles. More scholarly vendors will be added soon.\n\n\n\n![image](https://github.com/user-attachments/assets/13202184-bc12-4530-b7c1-2ee698f3e1cc)\n\n<a href=\"https://glama.ai/mcp/servers/aq05b2p0ql\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/aq05b2p0ql/badge\" alt=\"Scholarly Server MCP server\" /></a>\n\n![star-history-202551](https://github.com/user-attachments/assets/f22d5796-017c-4c53-b230-101a09a28118)\n\n\n## Components\n\n### Tools\n\nThe server implements one tool:\n- search-arxiv: Search arxiv for articles related to the given keyword.\n  - Takes \"keyword\" as required string arguments\n\n## Quickstart\n\n### Install\n\n#### Claude Desktop\n\nOn MacOS: `~/Library/Application\\ Support/Claude/claude_desktop_config.json`\nOn Windows: `%APPDATA%/Claude/claude_desktop_config.json`\n\n<details>\n  <summary>Development/Unpublished Servers Configuration</summary>\n  ```\n  \"mcpServers\": {\n    \"mcp-scholarly\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"--directory\",\n        \"/Users/adityakarnam/PycharmProjects/mcp-scholarly/mcp-scholarly\",\n        \"run\",\n        \"mcp-scholarly\"\n      ]\n    }\n  }\n  ```\n</details>\n\n<details>\n  <summary>Published Servers Configuration</summary>\n  ```\n  \"mcpServers\": {\n    \"mcp-scholarly\": {\n      \"command\": \"uvx\",\n      \"args\": [\n        \"mcp-scholarly\"\n      ]\n    }\n  }\n  ```\n</details>\n\nor if you are using Docker\n\n<details>\n  <summary>Published Docker Servers Configuration</summary>\n  ```\n  \"mcpServers\": {\n    \"mcp-scholarly\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\", \"--rm\", \"-i\",\n        \"mcp/scholarly\"\n      ]\n    }\n  }\n  ```\n</details>\n\n### Installing via Smithery\n\nTo install mcp-scholarly for Claude Desktop automatically via [Smithery](https://smithery.ai/server/mcp-scholarly):\n\n```bash\nnpx -y @smithery/cli install mcp-scholarly --client claude\n```\n\n## Development\n\n### Building and Publishing\n\nTo prepare the package for distribution:\n\n1. Sync dependencies and update lockfile:\n```bash\nuv sync\n```\n\n2. Build package distributions:\n```bash\nuv build\n```\n\nThis will create source and wheel distributions in the `dist/` directory.\n\n3. Publish to PyPI:\n```bash\nuv publish\n```\n\nNote: You'll need to set PyPI credentials via environment variables or command flags:\n- Token: `--token` or `UV_PUBLISH_TOKEN`\n- Or username/password: `--username`/`UV_PUBLISH_USERNAME` and `--password`/`UV_PUBLISH_PASSWORD`\n\n### Debugging\n\nSince MCP servers run over stdio, debugging can be challenging. For the best debugging\nexperience, we strongly recommend using the [MCP Inspector](https://github.com/modelcontextprotocol/inspector).\n\n\nYou can launch the MCP Inspector via [`npm`](https://docs.npmjs.com/downloading-and-installing-node-js-and-npm) with this command:\n\n```bash\nnpx @modelcontextprotocol/inspector uv --directory /Users/adityakarnam/PycharmProjects/mcp-scholarly/mcp-scholarly run mcp-scholarly\n```\n\n\nUpon launching, the Inspector will display a URL that you can access in your browser to begin debugging.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "scholarly",
        "research",
        "search",
        "retrieval scholarly",
        "scholarly search",
        "scholarly papers"
      ],
      "category": "scientific-research-tools"
    },
    "jigarbhoye04--MatlabMCP": {
      "owner": "jigarbhoye04",
      "name": "MatlabMCP",
      "url": "https://github.com/jigarbhoye04/MatlabMCP",
      "imageUrl": "/freedevtools/mcp/pfp/jigarbhoye04.webp",
      "description": "Execute MATLAB code seamlessly from Python, enabling efficient computation and data analysis with a shared MATLAB session that optimizes performance and resource utilization.",
      "stars": 20,
      "forks": 2,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-27T09:06:12Z",
      "readme_content": "# MATLAB MCP Integration\n\nThis is an implementation of a Model Context Protocol (MCP) server for MATLAB. It allows MCP clients (like LLM agents or Claude Desktop) to interact with a shared MATLAB session using the MATLAB Engine API for Python.\n\n## Features\n\n*   **Execute MATLAB Code:** Run arbitrary MATLAB code snippets via the `runMatlabCode` tool.\n*   **Retrieve Variables:** Get the value of variables from the MATLAB workspace using the `getVariable` tool.\n*   **Structured Communication:** Tools return results and errors as structured JSON for easier programmatic use by clients.\n*   **Non-Blocking Execution:** MATLAB engine calls are run asynchronously using `asyncio.to_thread` to prevent blocking the server.\n*   **Standard Logging:** Uses Python's standard `logging` module, outputting to `stderr` for visibility in client logs.\n*   **Shared Session:** Connects to an existing shared MATLAB session.\n\n> [!TIP]\n> MatlabMCP wikipedia by DEVIN.\n Checkout [DeepWiki](https://deepwiki.com/jigarbhoye04/MatlabMCP) for more detailed and illustrative information about architecture.\n\n## Requirements\n\n*   Python 3.12 or higher\n*   MATLAB (**R2023a or higher recommended** - check MATLAB Engine API for Python compatibility) with the MATLAB Engine API for Python installed.\n*   `numpy` Python package.\n\n## Installation\n\n1.  Clone this repository:\n    ```bash\n    git clone https://github.com/jigarbhoye04/MatlabMCP.git\n    cd MatlabMCP\n    ```\n\n2.  Set up a Python virtual environment (recommended):\n    ```bash\n    # Install uv if you haven't already: https://github.com/astral-sh/uv\n    uv init\n    uv venv\n    source .venv/bin/activate  # On Windows use: .venv\\Scripts\\activate\n    ```\n\n3.  Install dependencies:\n    ```bash\n    uv pip sync\n    ```\n\n4.  Ensure MATLAB is installed and the MATLAB Engine API for Python is configured for your Python environment. See [MATLAB Documentation](https://www.mathworks.com/help/matlab/matlab_external/install-the-matlab-engine-for-python.html).\n\n5.  **Start MATLAB and share its engine:** Run the following command in the MATLAB Command Window:\n    ```matlab\n    matlab.engine.shareEngine\n    ```\n    You can verify it's shared by running `matlab.engine.isEngineShared` in MATLAB (it should return `true` or `1`). The MCP server needs this shared engine to connect.\n\n## Configuration (for Claude Desktop)\n\nTo use this server with Claude Desktop:\n\n1.  Go to Claude Desktop -> Settings -> Developer -> Edit Config.\n2.  This will open `claude_desktop_config.json`. Add or modify the `mcpServers` section to include the `MatlabMCP` configuration:\n\n    ```json\n    {\n      \"mcpServers\": {\n        \"MatlabMCP\": {\n          \"command\": \"C:\\\\Users\\\\username\\\\.local\\\\bin\\\\uv.exe\", // Path to your uv executable\n          \"args\": [\n            \"--directory\",\n            \"C:\\\\Users\\\\username\\\\Desktop\\\\MatlabMCP\\\\\", // ABSOLUTE path to the cloned repository directory\n            \"run\",\n            \"main.py\"\n          ]\n          // Optional: Add environment variables if needed\n          // \"env\": {\n          //   \"MY_VAR\": \"value\"\n          // }\n        }\n        // Add other MCP servers here if you have them\n      }\n    }\n    ```\n3.  **IMPORTANT:** Replace `C:\\\\Users\\\\username\\\\...` paths with the correct **absolute paths** for your system.\n4.  Save the file and **restart Claude Desktop**.\n5.  **Logging:** Server logs (from Python's `logging` module) will appear in Claude Desktop's MCP log files (accessible via `tail -f ~/Library/Logs/Claude/mcp-server-MatlabMCP.log` on macOS or checking `%APPDATA%\\Claude\\logs\\` on Windows).\n\n\n## Development\n\nProject Structure:\n```\nMatlabMCP/\n├── .venv/                     # Virtual environment created by uv\n├── Docs/\n│   └── Images/\n│   └── Updates.md             # Documentation for updates and changes\n├── main.py                    # The MCP server script\n├── pyproject.toml             # Project metadata and dependencies\n├── README.md                  # This file\n└── uv.lock                    # Lock file for dependencies\n```\n\n## Documentation\nCheck out [Updates](./Docs/Updates.md) for detailed documentation on the server's features, usage, and development notes.\n\n## Contributing\nContributions are welcome! If you have any suggestions or improvements, feel free to open an issue or submit a pull request.\n\nLet's make this even better together!\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "matlabmcp",
        "matlab",
        "python",
        "jigarbhoye04 matlabmcp",
        "execute matlab",
        "shared matlab"
      ],
      "category": "scientific-research-tools"
    },
    "jinsoo--DDA_julia.jl": {
      "owner": "jinsoo",
      "name": "DDA_julia.jl",
      "url": "https://github.com/jinsoo/DDA_julia.jl",
      "imageUrl": "/freedevtools/mcp/pfp/jinsoo.webp",
      "description": "Perform scattering calculations using the discrete-dipole approximation for various geometrical shapes, including spheres. Utilize wave parameters to analyze scattering effects based on defined objects and their properties.",
      "stars": 0,
      "forks": 0,
      "license": "MIT License",
      "language": "Julia",
      "updated_at": "2023-09-18T12:48:13Z",
      "readme_content": "# JuliaDDA\n\n[![Build Status](https://github.com/cyclinghiter/JuliaDDA.jl/actions/workflows/CI.yml/badge.svg?branch=main)](https://github.com/cyclinghiter/JuliaDDA.jl/actions/workflows/CI.yml?query=branch%3Amain)\n\n## Usage\n\nThis package is based on the reference \n\n*Draine, B. T., & Flatau, P. J. (1994). Discrete-dipole approximation for scattering calculations. Josa a, 11(4), 1491-1499.*\n\n# Testing\n\n    using JuliaDDA\n    using StaticArrays\n\n    # Wave parameters\n    k = 1\n    ka = 7\n    a = ka / k\n\n    # Define Container for load objects \n    C = Container(k)\n\n    # Generate Sphere object \n    Object = Sphere(a, 20, 2+1.5im, 1)\n    # You can also Rotate or Pan the Object\n    # Ojbect = Rotate(Object, pi/4, n̂=SA[0, 0, 1])\n\n    # plane wave\n    # propagation axis :z\n    # polarization axis : x\n    Src = PlaneWave(SA[0, 0, 1], SA[1, 0, 0]) \n\n    # Define Recorder to save Electric field at certain position\n    Rec = SphericalRecorder(50, 50, R=100, mode=\"full\")\n\n    # push object to the container\n    push!(C, Object)\n\n    # Calculate the incident field\n    CalEinc(C, Src)\n\n    # Calculate the Polarization of the dipoles\n    CalPolarization(C)\n\n    # Calculate the Farfield at recorder positions\n    CalFarField(C, Rec, \"sca\")\n\n    Volume = 4/3 * pi * a^3\n    Po_in = ϵ0 * Volume \n\n    # Plot scatteres in the Container (backend : PlotlyJS)\n    PlotScatterers(C, :Px, plot_mode=:real)\n\n    # Plot Far field power\n    Plot3DPower(Rec, log=true, Po_in = Po_in)\n\n    # Plot DifferentialCrossSection (not accurate yet)\n    PlotDifferentialCrossSection(C, 50, 50, scale=\"log\", r=100)\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "scattering",
        "spheres",
        "dipole",
        "scattering calculations",
        "scattering effects",
        "analyze scattering"
      ],
      "category": "scientific-research-tools"
    },
    "jjsantos01--qgis_mcp": {
      "owner": "jjsantos01",
      "name": "qgis_mcp",
      "url": "https://github.com/jjsantos01/qgis_mcp",
      "imageUrl": "/freedevtools/mcp/pfp/jjsantos01.webp",
      "description": "Connects Claude AI to QGIS for project management and layer manipulation, enabling the creation, loading, and saving of projects, along with executing processing algorithms and running Python code directly within QGIS.",
      "stars": 660,
      "forks": 88,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-10-02T08:20:10Z",
      "readme_content": "# QGISMCP - QGIS Model Context Protocol Integration\n\nQGISMCP connects [QGIS](https://qgis.org/) to [Claude AI](https://claude.ai/chat) through the [Model Context Protocol (MCP)](https://modelcontextprotocol.io/docs/getting-started/intro), allowing Claude to directly interact with and control QGIS. This integration enables prompt assisted project creation, layer loading, code execution and more.\n\nThis project is strongly based on the [BlenderMCP](https://github.com/ahujasid/blender-mcp/tree/main) project by [Siddharth Ahuja](https://x.com/sidahuj)\n\n## Features\n\n- **Two-way communication**: Connect Claude AI to QGIS through a socket-based server.\n- **Project manipulation**: Create, load and save projects in QGIS.\n- **Layer manipulation**: Add and remove vector or raster layers to a project.\n- **Execute processing**: Execute processing algorithms ([Processing Toolbox](https://docs.qgis.org/3.40/en/docs/user_manual/processing/toolbox.html)).\n- **Code execution**: Run arbitrary Python code in QGIS from Claude. Very powerful, but also be very cautious using this tool.\n\n## Components\n\nThe system consists of two main components:\n\n1. **[QGIS plugin](/qgis_mcp_plugin/)**: A QGIS plugin that creates a socket server within QGIS to receive and execute commands.\n2. **[MCP Server](/src/qgis_mcp/qgis_mcp_server.py)**: A Python server that implements the Model Context Protocol and connects to the QGIS plugin.\n\n## Installation\n\n### Prerequisites\n\n- QGIS 3.X (only tested on 3.22)\n- Claude desktop\n- Python 3.10 or newer\n- uv package manager:\n\nIf you're on Mac, please install uv as\n\n```bash\nbrew install uv\n```\n\nOn Windows Powershell\n\n```bash\npowershell -ExecutionPolicy ByPass -c \"irm https://astral.sh/uv/install.ps1 | iex\"\n```\n\nOtherwise installation instructions are on their website: [Install uv](https://docs.astral.sh/uv/getting-started/installation/)\n\n**⚠️ Do not proceed before installing UV**\n\n### Download code\n\nDownload this repo to your computer. You can clone it with:\n\n```bash\ngit clone git@github.com:jjsantos01/qgis_mcp.git\n```\n\n### QGIS plugin\n\nYou need to copy the folder [qgis_mcp_plugin](/qgis_mcp_plugin/) and its content on your QGIS profile plugins folder.\n\nYou can get your profile folder in QGIS going to menu `Settings` -> `User profiles` -> `Open active profile folder` Then, go to `Python/plugins` and paste the folder `qgis_mcp_plugin`.\n\n> On a Windows machine the plugins folder is usually located at: `C:\\Users\\USER\\AppData\\Roaming\\QGIS\\QGIS3\\profiles\\default\\python\\plugins` \n\nand on MacOS: `~/Library/Application\\ Support/QGIS/QGIS3/profiles/default/python/plugins`\n\n Then close QGIS and open it again. Go to the menu option `Plugins` > `Installing and Managing Plugins`, select the `All` tab and search for \"QGIS MCP\", then mark the QGIS MCP checkbox.\n\n### Claude for Desktop Integration\n\nGo to `Claude` > `Settings` > `Developer` > `Edit Config` > `claude_desktop_config.json` to include the following:\n\n> If you can't find the \"Developers tab\" or the `claude_desktop_config.json` look at this [documentation](https://modelcontextprotocol.io/quickstart/user#2-add-the-filesystem-mcp-server).\n\n```json\n{\n    \"mcpServers\": {\n        \"qgis\": {\n            \"command\": \"uv\",\n            \"args\": [\n                \"--directory\",\n                \"/ABSOLUTE/PATH/TO/PARENT/REPO/FOLDER/qgis_mcp/src/qgis_mcp\",\n                \"run\",\n                \"qgis_mcp_server.py\"\n            ]\n        }\n\n    }\n}\n```\n\n## Usage\n\n### Starting the Connection\n\n1. In QGIS, go to `plugins` > `QGIS MCP` > `QGIS MCP`\n    \n2. Click \"Start Server\"\n    \n\n### Using with Claude\n\nOnce the config file has been set on Claude, and the server is running on QGIS, you will see a hammer icon with tools for the QGIS MCP.\n\n\n\n#### Tools\n\n- `ping` - Simple ping command to check server connectivity\n- `get_qgis_info` - Get QGIS information about the current installation\n- `load_project` - Load a QGIS project from the specified path\n- `create_new_project` - Create a new project and save it\n- `get_project_info` - Get current project information\n- `add_vector_layer` - Add a vector layer to the project\n- `add_raster_layer` - Add a raster layer to the project\n- `get_layers` - Retrieve all layers in the current project\n- `remove_layer` - Remove a layer from the project by its ID\n- `zoom_to_layer` - Zoom to the extent of a specified layer\n- `get_layer_features` - Retrieve features from a vector layer with an optional limit\n- `execute_processing` - Execute a processing algorithm with the given parameters\n- `save_project` - Save the current project to the given path\n- `render_map` - Render the current map view to an image file\n- `execute_code` - Execute arbitrary PyQGIS code provided as a string\n\n### Example Commands\n\nThis is the example I used for the [demo](https://x.com/jjsantoso/status/1900293848271667395):\n\n```plain\nYou have access to the tools to work with QGIS. You will do the following:\n\t1. Ping to check the connection. If it works, continue with the following steps.\n\t2. Create a new project and save it at: \"C:/Users/USER/GitHub/qgis_mcp/data/cdmx.qgz\"\n\t3. Load the vector layer: \"\"C:/Users/USER/GitHub/qgis_mcp/data/cdmx/mgpc_2019.shp\" and name it \"Colonias\".\n\t4. Load the raster layer: \"C:/Users/USER/GitHub/qgis_mcp/data/09014.tif\" and name it \"BJ\"\n\t5. Zoom to the \"BJ\" layer.\n\t6. Execute the centroid algorithm on the \"Colonias\" layer. Skip the geometry check. Save the output to \"colonias_centroids.geojson\".\n\t7. Execute code to create a choropleth map using the \"POB2010\" field in the \"Colonias\" layer. Use the quantile classification method with 5 classes and the Spectral color ramp.\n\t8. Render the map to \"C:/Users/USER/GitHub/qgis_mcp/data/cdmx.png\"\n\t9. Save the project.\n```",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "qgis_mcp",
        "qgis",
        "projects",
        "jjsantos01 qgis_mcp",
        "ai qgis",
        "qgis project"
      ],
      "category": "scientific-research-tools"
    },
    "jkingsman--qanon-mcp-server": {
      "owner": "jkingsman",
      "name": "qanon-mcp-server",
      "url": "https://github.com/jkingsman/qanon-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/jkingsman.webp",
      "description": "Provides access to a dataset of Q-Anon posts for research purposes, enabling users to search, filter, and analyze the content for sociological insights.",
      "stars": 1,
      "forks": 1,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-06-20T16:40:02Z",
      "readme_content": "## QAnon is a dangerous cult. This archive is for research purposes only, and I do _not_ endorse any material in this repo.\r\n\r\n# Q-Anon Posts/Drops MCP Server\r\n\r\n![](https://badge.mcpx.dev?type=server 'qanon-mcp')\r\n[![smithery badge](https://smithery.ai/badge/@jkingsman/qanon-mcp-server)](https://smithery.ai/server/@jkingsman/qanon-mcp-server)\r\n\r\nAn MCP (Model Context Protocol) server that provides access to a dataset of Q-Anon posts for anthropological/sociological research. This server allows AI assistants like Claude to search, filter, and analyze the Q-Anon drops.\r\n\r\nPosts are drawn from https://github.com/jkingsman/JSON-QAnon. You can learn more about how the source data was composed there, as well as find alternate formats, schemas, etc.\r\n\r\n### Warning: This tool was entirely vibe coded. Use at your own risk.\r\n\r\n## Prerequisites\r\n\r\n- Python 3.10 or higher\r\n- `uv` package manager\r\n- Claude Desktop (for Claude integration)\r\n\r\n## Installation\r\n\r\nThis tool is compatible with `uvx` and doesn't need to be cloned/installed.\r\n\r\n### Installing via Smithery\r\n\r\nTo install qanon-mcp-server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@jkingsman/qanon-mcp-server):\r\n\r\n```bash\r\nnpx -y @smithery/cli install @jkingsman/qanon-mcp-server --client claude\r\n```\r\n\r\n### Manual\r\n\r\n1. Clone or download this repository to your local machine\r\n2. Install the required packages using `uv`:\r\n\r\n```bash\r\nuv pip install -e .\r\n```\r\n\r\n## Usage\r\n\r\nYou can run the server directly with `uvx`:\r\n\r\n```bash\r\nuvx qanon_mcp\r\n```\r\n\r\n## Claude Desktop Integration\r\n\r\nTo use this MCP server with Claude Desktop:\r\n\r\n1. Make sure you have [Claude Desktop](https://claude.ai/download) installed\r\n2. Open the Claude menu and select \"Settings...\"\r\n3. Click on \"Developer\" in the left-hand bar and then \"Edit Config\"\r\n4. Add the following configuration to the `claude_desktop_config.json` file:\r\n\r\n```json\r\n{\r\n  \"mcpServers\": {\r\n    \"qanon_mcp\": {\r\n      \"command\": \"uvx\",\r\n      \"args\": [\r\n        \"qanon_mcp\"\r\n      ]\r\n    }\r\n  }\r\n}\r\n```\r\n\r\nor, if you don't have `uvx` installed:\r\n\r\n```json\r\n{\r\n  \"mcpServers\": {\r\n    \"qanon_mcp\": {\r\n      \"command\": \"uv\",\r\n      \"args\": [\r\n        \"tool\",\r\n        \"run\",\r\n        \"qanon_mcp\"\r\n      ]\r\n    }\r\n  }\r\n}\r\n```\r\n\r\n\r\n5. Save the file and restart Claude Desktop\r\n6. Start a new conversation in Claude Desktop\r\n7. You should see a hammer icon in the input box, indicating that tools are available\r\n\r\n## Features\r\n\r\n### Resources\r\n\r\n- `qanon://posts/count` - Get the total number of posts\r\n- `qanon://posts/{post_id}` - Access a specific post by ID\r\n- `qanon://posts/raw/{post_id}` - Get the raw JSON data for a specific post\r\n- `qanon://authors` - List all unique authors\r\n- `qanon://stats` - Get dataset statistics\r\n\r\n### Tools\r\n\r\n- **get_post_by_id_tool** - Retrieve a specific post by its ID\r\n- **search_posts** - Find posts containing specific keywords or phrases\r\n- **get_posts_by_date** - Retrieve posts from a specific date range\r\n- **get_posts_by_author_id** - Find posts by a specific author ID\r\n- **analyze_post** - Get detailed analysis of a specific post including references and context\r\n- **get_timeline_summary** - Generate a chronological timeline, optionally within a date range\r\n- **word_cloud_by_post_ids** - Generate a word frequency analysis for posts within a specified ID range\r\n- **word_cloud_by_date_range** - Generate a word frequency analysis for posts within a specified date range\r\n\r\n## Example Queries for Claude\r\n\r\nOnce the MCP server is connected to Claude Desktop, you can ask questions like:\r\n\r\n- \"How many Q-Anon posts are in the dataset?\"\r\n- \"Search for posts that mention 'storm'\"\r\n- \"Show me posts from October 2020\"\r\n- \"Analyze post #3725\"\r\n- \"Create a timeline of Q-Anon posts from 2018\"\r\n- \"Generate a word cloud for Q-Anon posts between January and March 2019\"\r\n- \"Get the raw data for post #4500\"\r\n- \"What are the most common words used in posts #1000-2000?\"\r\n\r\n## Troubleshooting\r\n\r\n- If Claude Desktop doesn't show the hammer icon, check your configuration and restart Claude Desktop\r\n- Ensure the `posts.json` file is in the same directory as the script\r\n- Check the output in the terminal for any error messages\r\n- Make sure you're using the absolute path to the script in your Claude Desktop configuration\r\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "sociological",
        "research",
        "qanon",
        "posts research",
        "research tools",
        "content sociological"
      ],
      "category": "scientific-research-tools"
    },
    "kbsooo--MCP_Atom_of_Thoughts": {
      "owner": "kbsooo",
      "name": "MCP_Atom_of_Thoughts",
      "url": "https://github.com/kbsooo/MCP_Atom_of_Thoughts",
      "imageUrl": "/freedevtools/mcp/pfp/kbsooo.webp",
      "description": "Enhance applications with advanced language models for idea generation, analysis, and manipulation. Context-aware interactions enable transformation of workflows through intelligent thought processing.",
      "stars": 45,
      "forks": 10,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-10-04T00:52:14Z",
      "readme_content": "# Atom of Thoughts (AoT)\n\n[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/kbsooo-mcp-atom-of-thoughts-badge.png)](https://mseep.ai/app/kbsooo-mcp-atom-of-thoughts)\n\n[![smithery badge](https://smithery.ai/badge/@kbsooo/mcp_atom_of_thoughts)](https://smithery.ai/server/@kbsooo/mcp_atom_of_thoughts)\n\n[![Verified on MseeP](https://mseep.ai/badge.svg)](https://mseep.ai/app/672e0733-4c20-461f-b9cf-59ec2c9dafff)\n\nA Model Context Protocol (MCP) server implementation of Atom of Thoughts, a decomposition-based reasoning framework.\n\n> **Note**: This implementation is based on the research paper [\"Atom of Thoughts for Markov LLM Test-Time Scaling\"](https://arxiv.org/abs/2502.12018) (Teng et al., 2025).\n\n<a href=\"https://glama.ai/mcp/servers/@kbsooo/MCP_Atom_of_Thoughts\">\n  <img alt=\"badge\" width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@kbsooo/MCP_Atom_of_Thoughts/badge\" />\n</a>\n\n[MCP.so](https://mcp.so/server/atom-of-thoughts/kbsooo)\n\n[한국어 설명](https://github.com/kbsooo/MCP_Atom_of_Thoughts?tab=readme-ov-file#%ED%95%9C%EA%B5%AD%EC%96%B4-%EC%84%A4%EB%AA%85)\n\n## English Documentation\n\nThis repository implements Atom of Thoughts (AoT), a decomposition-based reasoning framework, as a Model Context Protocol (MCP) server. This implementation is based on the concepts presented in the paper [\"Atom of Thoughts for Markov LLM Test-Time Scaling\"](https://arxiv.org/abs/2502.12018) (Teng et al., 2025).\n\n### Available Tools\n\nTwo main tools are provided:\n\n1. **AoT (Full Version)**: A complete Atom of Thoughts tool with full capabilities for deep analysis and complex problem solving.\n2. **AoT-light (Lightweight Version)**: A streamlined version optimized for faster processing and quicker results.\n\n### AoT-light: Lightweight Version\n\nAoT-light is designed for faster processing in time-sensitive situations:\n\n- **Key Features**:\n  - Lower maximum depth (3 instead of 5) for faster processing\n  - Simplified verification process\n  - Immediate conclusion suggestion for high-confidence hypotheses\n  - Reduced computational overhead and response payload\n  - Optimized for speed rather than exhaustive analysis\n\n- **Use Cases**:\n  - Quick brainstorming sessions requiring atomic thought organization\n  - Time-sensitive problem solving where speed is prioritized over exhaustive analysis\n  - Simpler reasoning tasks that don't require deep decomposition\n  - Initial exploration before using the full AoT for deeper analysis\n  - Learning or demonstration purposes where response time is important\n\n### Use Cases\n\nAtom of Thoughts is effective in the following scenarios:\n- Solving problems requiring complex reasoning\n- Generating hypotheses that need verification from multiple perspectives\n- Deriving high-confidence conclusions in scenarios where accuracy is crucial\n- Minimizing logical errors in critical tasks\n- Decision-making requiring multiple verification steps\n\n### Atom Types\n\nAoT uses five types of atoms:\n\n1. **premise**: Basic assumptions or given information for problem solving\n2. **reasoning**: Logical reasoning process based on other atoms\n3. **hypothesis**: Proposed solutions or intermediate conclusions\n4. **verification**: Process to evaluate the validity of other atoms (especially hypotheses)\n5. **conclusion**: Verified hypotheses or final problem solutions\n\n### Core Features\n\n#### 1. Decomposition-Contraction Mechanism\n\nA mechanism to decompose atoms into smaller sub-atoms and contract them back after verification.\n\n- **Decomposition**: Breaking complex atoms into smaller sub-atoms.\n  - `startDecomposition(atomId)`: Start atom decomposition\n  - `addToDecomposition(decompositionId, atomId)`: Add sub-atom to decomposition\n  - `completeDecomposition(decompositionId)`: Complete decomposition process\n\n- **Contraction**: Contract back to the original atom once all sub-atoms are verified.\n  - Calculate confidence of the original atom based on sub-atoms' confidence levels\n  - Automatically suggest conclusions for high-confidence verified hypotheses\n\n#### 2. Automatic Termination Mechanism\n\n- Automatically terminates when reaching maximum depth or finding a high-confidence conclusion.\n- `getTerminationStatus()`: Return current termination status and reason\n- `getBestConclusion()`: Return the conclusion with highest confidence\n\n### Parameter Descriptions\n\n- **atomId**: Unique identifier for the atom (e.g., 'A1', 'H2')\n- **content**: Actual content of the atom\n- **atomType**: Type of atom (one of: premise, reasoning, hypothesis, verification, conclusion)\n- **dependencies**: List of IDs of other atoms this atom depends on\n- **confidence**: Confidence level of this atom (value between 0-1)\n- **isVerified**: Whether this atom has been verified\n- **depth**: Depth level of this atom in the decomposition-contraction process\n\n### Usage Method\n\n1. Understand the problem and define necessary premise atoms\n2. Create reasoning atoms based on premises\n3. Create hypothesis atoms based on reasoning\n4. Create verification atoms to verify hypotheses\n5. Derive conclusion atoms based on verified hypotheses\n6. Use atom decomposition to explore deeper when necessary\n7. Present the high-confidence conclusion atom as the final answer\n\n### Comparing Sequential Thinking and Atom of Thoughts (More Testing Needed)\n\nAfter applying both thinking tools to the same topic, the following differences and performance characteristics were observed:\n\n#### Structural Differences\n\n**Sequential Thinking:**\n- Linear thinking process: progresses sequentially from one thought to the next\n- Predicts the total number of thoughts in advance\n- Each thinking stage is built upon previous stages\n\n**Atom of Thoughts:**\n- Non-linear, network structure: multiple thought units (atoms) interconnect with dependencies\n- Forms systematic structure according to atom types (premise, reasoning, hypothesis, verification, conclusion)\n- Explicitly evaluates the confidence level of each atom\n\n#### Comparative Strengths\n\n**Sequential Thinking Strengths:**\n- Intuitive flow: similar to natural human thinking processes\n- Simplicity: simple structure allows quick application to straightforward problems\n- Flexibility: can modify previous stages or change direction during the thinking process\n\n**Atom of Thoughts Strengths:**\n- Confidence evaluation: explicitly measures the confidence of each thought to improve conclusion validity\n- Verification process: evaluates hypotheses through systematic verification stages\n- Dependency tracking: clearly tracks which premises or reasoning influenced specific conclusions\n- Parallel processing: can consider multiple thought atoms simultaneously\n\n#### Efficiency and Accuracy\n\n**Efficiency:**\n- Sequential Thinking: more efficient for simple problems, with faster progression of thought\n- Atom of Thoughts: more efficient for complex problems, but has initial overhead in building systematic structures\n\n**Accuracy:**\n- Sequential Thinking: possibility of error accumulation from previous stages as the thinking process deepens\n- Atom of Thoughts: reduced error possibility through verification stages and confidence assessment, leading to more reliable conclusions\n\n#### Suitability by Purpose\n\n**Cases Suitable for Sequential Thinking:**\n- Simple to moderately complex problems\n- Time-constrained situations\n- When natural storytelling or explanation is needed\n\n**Cases Suitable for Atom of Thoughts:**\n- Highly complex problems\n- Situations where accuracy and reliability are crucial\n- Hypotheses requiring verification from multiple perspectives\n- Reasoning with complex dependency relationships\n\n#### Conclusion\nBoth tools can contribute to improving artificial intelligence's reasoning abilities, but the appropriate tool varies depending on the nature of the problem and requirements. Sequential Thinking is useful when intuitive and quick thinking processes are needed, while Atom of Thoughts is more suitable for complex problems requiring systematic verification and high reliability.\n\n### Command Tool (atomcommands)\n\nA command tool to control the decomposition-contraction mechanism and automatic termination of Atom of Thoughts.\n\n**Available Commands**:\n1. **decompose**: Decompose a specified atom into smaller sub-atoms\n   - Required parameter: `atomId`\n2. **complete_decomposition**: Complete an ongoing decomposition process\n   - Required parameter: `decompositionId`\n3. **termination_status**: Check the termination status of the current AoT process\n4. **best_conclusion**: Get the verified conclusion with the highest confidence\n5. **set_max_depth**: Change the maximum depth limit\n   - Required parameter: `maxDepth`\n\n### Installing via Smithery\n\nTo install Atom of Thoughts for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@kbsooo/mcp_atom_of_thoughts):\n\n```bash\nnpx -y @smithery/cli install @kbsooo/mcp_atom_of_thoughts --client claude\n```\n\n### MCP Server Configuration\n\nTo use the Atom of Thoughts MCP server, you need to register it in your Claude Desktop or Cline MCP settings. Here is an example configuration:\n\n```json\n{ \n  \"mcpServers\": { \n    \"atom-of-thoughts\": { \n      \"command\": \"node\", \n      \"args\": [\"/ABSOLUTE/PATH/TO/PARENT/FOLDER/atom-of-thoughts/build/index.js\"], \n      \"disabled\": false, \n      \"autoApprove\": [] \n    } \n  } \n}\n```\n\nReplace `/ABSOLUTE/PATH/TO/PARENT/FOLDER` with the actual absolute path to the project on your system. After saving the configuration, restart Claude Desktop or Cline to use the Atom of Thoughts MCP server.\n\nFor detailed implementation and code-level documentation, please refer to the source code in this repository.\n\n## 한국어 설명\n\n### Atom of Thoughts란?\n\nAtom of Thoughts(AoT)는 복잡한 문제를 독립적이고 재사용 가능한 원자 단위의 사고로 분해하여 문제를 해결하는 도구입니다. 이 도구는 전통적인 순차적 사고 방식과 달리, 사고의 기본 단위인 '원자'들이 서로 의존성을 갖고 구성되어 더 강력한 문제 해결을 가능하게 합니다. 이 구현은 \"Atom of Thoughts for Markov LLM Test-Time Scaling\"(Teng et al., 2025) 논문의 개념을 기반으로 합니다.\n\n### 제공되는 도구\n\n현재 다음과 같은 두 가지 주요 도구가 제공됩니다:\n\n1. **AoT (전체 버전)**: 심층적인 분석과 복잡한 문제 해결을 위한 완전한 기능을 갖춘 Atom of Thoughts 도구입니다.\n2. **AoT-light (경량 버전)**: 더 빠른 처리와 신속한 결과를 위해 최적화된 경량 버전입니다.\n\n### AoT-light: 경량 버전\n\nAoT-light는 시간이 중요한 상황에서 더 빠른 처리를 위해 설계된 경량 버전입니다:\n\n- **주요 특징**:\n  - 낮은 최대 깊이 (5 대신 3) 설정으로 빠른 처리\n  - 간소화된 검증 프로세스\n  - 높은 신뢰도의 가설에 대한 즉각적인 결론 제안\n  - 축소된 계산 오버헤드 및 응답 데이터\n  - 철저한 분석보다 속도에 최적화\n\n- **사용 시나리오**:\n  - 원자적 사고 구성이 필요한 빠른 브레인스토밍 세션\n  - 철저한 분석보다 속도가 중요한 시간에 민감한 문제 해결\n  - 깊은 분해가 필요하지 않은 단순한 추론 작업\n  - 전체 AoT를 사용한 심층 분석 전 초기 탐색\n  - 응답 시간이 중요한 학습 또는 시연 목적\n\n### 사용 시나리오\n\n다음과 같은 경우에 Atom of Thoughts를 사용하면 효과적입니다:\n- 복잡한 추론이 필요한 문제 해결\n- 여러 관점에서 검증이 필요한 가설 생성\n- 정확도가 중요한 문제에서 신뢰도 높은 결론 도출\n- 논리적 오류를 최소화해야 하는 작업\n- 여러 단계의 검증이 필요한 의사결정\n\n### 원자 유형\n\nAtom of Thoughts에서는 다섯 가지 유형의 원자를 사용합니다:\n\n1. **premise (전제)**: 문제 해결을 위한 기본 가정이나 주어진 정보\n2. **reasoning (추론)**: 다른 원자들을 기반으로 한 논리적 추론 과정\n3. **hypothesis (가설)**: 가능한 해결책이나 중간 결론에 대한 제안\n4. **verification (검증)**: 다른 원자(특히 가설)의 유효성을 평가하는 과정\n5. **conclusion (결론)**: 검증된 가설이나 최종 문제 해결책\n\n### 핵심 기능\n\n#### 1. 분해-수축 메커니즘 (Decomposition-Contraction)\n\n원자를 더 작은 하위 원자로 분해하고 검증 후 다시 수축하는 메커니즘입니다.\n\n- **원자 분해 (Decomposition)**: 복잡한 원자를 더 작은 하위 원자로 분해합니다.\n  - `startDecomposition(atomId)`: 원자 분해 시작\n  - `addToDecomposition(decompositionId, atomId)`: 분해에 하위 원자 추가\n  - `completeDecomposition(decompositionId)`: 분해 과정 완료\n\n- **원자 수축 (Contraction)**: 하위 원자들이 모두 검증되면 원래 원자로 다시 수축합니다.\n  - 하위 원자들의 신뢰도에 기반하여 원래 원자의 신뢰도를 계산\n  - 검증된 가설이 고신뢰도를 가지면 자동으로 결론을 제안\n\n#### 2. 자동 종료 메커니즘 (Automatic Termination)\n\n- 최대 깊이(depth)에 도달하거나 높은 신뢰도의 결론을 찾으면 자동 종료됩니다.\n- `getTerminationStatus()`: 현재 종료 상태와 이유를 반환\n- `getBestConclusion()`: 가장 높은 신뢰도의 결론을 반환\n\n### 매개변수 설명\n\n- **atomId**: 원자의 고유 식별자 (예: 'A1', 'H2' 등)\n- **content**: 원자의 실제 내용\n- **atomType**: 원자의 유형 (premise, reasoning, hypothesis, verification, conclusion 중 하나)\n- **dependencies**: 이 원자가 의존하는 다른 원자들의 ID 목록\n- **confidence**: 이 원자의 신뢰도 (0~1 사이의 값)\n- **isVerified**: 이 원자가 검증되었는지 여부\n- **depth**: 이 원자의 깊이 (분해-수축 프로세스에서의 깊이 수준)\n\n### 사용 방법\n\n1. 문제를 이해하고 필요한 전제(premise) 원자들을 정의\n2. 전제를 바탕으로 추론(reasoning) 원자 생성\n3. 추론을 바탕으로 가설(hypothesis) 원자 생성\n4. 가설을 검증(verification)하는 원자 생성\n5. 검증된 가설을 바탕으로 결론(conclusion) 원자 도출\n6. 필요시 원자 분해(decomposition)를 사용하여 더 깊이 탐색\n7. 높은 신뢰도의 결론 원자를 최종 답변으로 제시\n\n### Sequential Thinking과 Atom of Thoughts 비교 (조금 더 테스트가 필요함)\n\n두 가지 사고 도구를 동일한 주제에 적용한 후 관찰된 차이점과 성능 특성은 다음과 같습니다:\n\n#### 구조적 차이점\n\n**Sequential Thinking:**\n- 선형적 사고 과정: 한 사고에서 다음 사고로 순차적으로 진행\n- 전체 사고 수를 미리 예측\n- 각 사고 단계는 이전 단계를 기반으로 구축됨\n\n**Atom of Thoughts:**\n- 비선형, 네트워크 구조: 여러 사고 단위(원자)가 의존성을 가지고 연결됨\n- 원자 유형(전제, 추론, 가설, 검증, 결론)에 따라 체계적인 구조 형성\n- 각 원자의 신뢰도 수준을 명시적으로 평가\n\n#### 비교 강점\n\n**Sequential Thinking 강점:**\n- 직관적 흐름: 자연스러운 인간의 사고 과정과 유사\n- 단순성: 간단한 구조로 직관적인 문제에 빠르게 적용 가능\n- 유연성: 사고 과정 중에 이전 단계를 수정하거나 방향을 변경할 수 있음\n\n**Atom of Thoughts 강점:**\n- 신뢰도 평가: 각 사고의 신뢰도를 명시적으로 측정하여 결론의 유효성 개선\n- 검증 과정: 체계적인 검증 단계를 통해 가설 평가\n- 의존성 추적: 어떤 전제나 추론이 특정 결론에 영향을 미쳤는지 명확하게 추적\n- 병렬 처리: 여러 사고 원자를 동시에 고려 가능\n\n#### 효율성과 정확성\n\n**효율성:**\n- Sequential Thinking: 단순한 문제에 더 효율적이며, 사고가 빠르게 진행됨\n- Atom of Thoughts: 복잡한 문제에 더 효율적이지만, 체계적인 구조를 만드는 초기 오버헤드가 있음\n\n**정확성:**\n- Sequential Thinking: 사고 과정이 깊어질수록 이전 단계에서의 오류 누적 가능성\n- Atom of Thoughts: 검증 단계와 신뢰도 평가를 통해 오류 가능성 감소, 더 신뢰할 수 있는 결론 도출\n\n#### 목적별 적합성\n\n**Sequential Thinking에 적합한 경우:**\n- 단순하거나 중간 정도 복잡한 문제\n- 시간 제약이 있는 상황\n- 자연스러운 스토리텔링이나 설명이 필요한 경우\n\n**Atom of Thoughts에 적합한 경우:**\n- 매우 복잡한 문제\n- 정확성과 신뢰성이 중요한 상황\n- 여러 관점에서 검증이 필요한 가설\n- 복잡한 의존 관계가 있는 추론\n\n#### 결론\n두 도구 모두 인공 지능의 추론 능력을 향상시키는 데 기여할 수 있지만, 적절한 도구는 문제의 특성과 요구 사항에 따라 달라집니다. Sequential Thinking은 직관적이고 빠른 사고 과정이 필요할 때 유용하며, Atom of Thoughts는 체계적인 검증과 높은 신뢰성이 필요한 복잡한 문제에 더 적합합니다.\n\n### 명령어 도구 (atomcommands)\n\nAtom of Thoughts의 분해-수축 메커니즘과 자동 종료를 제어하는 명령어 도구입니다.\n\n**사용 가능한 명령어**:\n1. **decompose**: 지정된 원자를 더 작은 하위 원자로 분해합니다.\n   - 필요 매개변수: `atomId`\n2. **complete_decomposition**: 진행 중인 분해 프로세스를 완료합니다.\n   - 필요 매개변수: `decompositionId`\n3. **termination_status**: 현재 AoT 프로세스의 종료 상태를 확인합니다.\n4. **best_conclusion**: 가장 높은 신뢰도의 검증된 결론을 가져옵니다.\n5. **set_max_depth**: 최대 깊이 제한을 변경합니다.\n   - 필요 매개변수: `maxDepth`\n\n### Installing via Smithery\n\nTo install Atom of Thoughts for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@kbsooo/mcp_atom_of_thoughts):\n\n```bash\nnpx -y @smithery/cli install @kbsooo/mcp_atom_of_thoughts --client claude\n```\n\n### MCP 서버 설정 방법\n\nAtom of Thoughts MCP 서버를 사용하기 위해서는 Claude Desktop 또는 Cline의 MCP 설정에 서버를 등록해야 합니다. 다음은 서버 구성의 예시입니다:\n\n```json\n{ \n  \"mcpServers\": { \n    \"atom-of-thoughts\": { \n      \"command\": \"node\", \n      \"args\": [\"/ABSOLUTE/PATH/TO/PARENT/FOLDER/atom-of-thoughts/build/index.js\"], \n      \"disabled\": false, \n      \"autoApprove\": [] \n    } \n  } \n}\n```\n\n위 설정에서 `/ABSOLUTE/PATH/TO/PARENT/FOLDER`는 실제 프로젝트가 위치한 절대 경로로 변경해주세요. 설정을 저장한 후 Claude Desktop 또는 Cline을 재시작하면 Atom of Thoughts MCP 서버를 사용할 수 있습니다.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "mcp_atom_of_thoughts",
        "workflows",
        "kbsooo",
        "kbsooo mcp_atom_of_thoughts",
        "mcp_atom_of_thoughts enhance",
        "thought processing"
      ],
      "category": "scientific-research-tools"
    },
    "leescot--pubmed-mcp-smithery": {
      "owner": "leescot",
      "name": "pubmed-mcp-smithery",
      "url": "https://github.com/leescot/pubmed-mcp-smithery",
      "imageUrl": "/freedevtools/mcp/pfp/leescot.webp",
      "description": "Search and retrieve academic papers from the PubMed database using enhanced tools such as MeSH term lookup, publication statistics, and structured PICO-based evidence searches.",
      "stars": 5,
      "forks": 5,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-07-12T02:43:23Z",
      "readme_content": "# PubMed Enhanced Search MCP Server\n\n[![smithery badge](https://smithery.ai/badge/@leescot/pubmed-mcp-smithery)](https://smithery.ai/server/@leescot/pubmed-mcp-smithery)\n\nA Model Content Protocol server that provides enhanced tools to search and retrieve academic papers from PubMed database, with additional features such as MeSH term lookup, publication count statistics, and PICO-based evidence search.\n\n## Features\n\n- Search PubMed by keywords with optional journal filter\n- Support for sorting results by relevance or date (newest/oldest first)\n- Get MeSH (Medical Subject Headings) terms related to a search word\n- Get publication counts for multiple search terms (useful for comparing prevalence)\n- Retrieve detailed paper information including abstract, DOI, authors, and keywords\n- Perform structured PICO-based searches with support for synonyms and combination queries\n\n## Installing\n\n### Prerequisites\n\n- Python 3.6+\n- pip\n\n### Installation\n\n1. Clone this repository:\n\n   ```\n   git clone https://github.com/leescot/pubmed-mcp-smithery\n   cd pubmed-mcp-smithery\n   ```\n\n2. Install dependencies:\n   ```\n   pip install fastmcp requests\n   ```\n\n## Usage\n\n### Running locally\n\nStart the server:\n\n```\npython pubmed_enhanced_mcp_server.py\n```\n\nFor development mode with auto-reloading:\n\n```\nmcp dev pubmed_enhanced_mcp_server.py\n```\n\n### Adding to Claude Desktop\n\nEdit your Claude Desktop configuration file (_CLAUDE_DIRECTORY/claude_desktop_config.json_) to add the server:\n\n```json\n\"pubmed-enhanced\": {\n    \"command\": \"python\",\n    \"args\": [\n        \"/path/pubmed-mcp-smithery/pubmed_enhanced_mcp_server.py\"\n    ]\n}\n```\n\n## MCP Functions\n\nThe server provides these main functions:\n\n1. `search_pubmed` - Search PubMed for articles matching keywords with optional journal filtering\n\n   ```python\n   # Example\n   results = await search_pubmed(\n       keywords=[\"diabetes\", \"insulin resistance\"],\n       journal=\"Nature Medicine\",\n       num_results=5,\n       sort_by=\"date_desc\"\n   )\n   ```\n\n2. `get_mesh_terms` - Look up MeSH terms related to a medical concept\n\n   ```python\n   # Example\n   mesh_terms = await get_mesh_terms(\"diabetes\")\n   ```\n\n3. `get_pubmed_count` - Get the count of publications for multiple search terms\n\n   ```python\n   # Example\n   counts = await get_pubmed_count([\"diabetes\", \"obesity\", \"hypertension\"])\n   ```\n\n4. `format_paper_details` - Get detailed information about specific papers by PMID\n\n   ```python\n   # Example\n   paper_details = await format_paper_details([\"12345678\", \"87654321\"])\n   ```\n\n5. `pico_search` - Perform structured PICO (Population, Intervention, Comparison, Outcome) searches with synonyms\n   ```python\n   # Example\n   pico_results = await pico_search(\n       p_terms=[\"diabetes\", \"type 2 diabetes\", \"T2DM\"],\n       i_terms=[\"metformin\", \"glucophage\"],\n       c_terms=[\"sulfonylurea\", \"glipizide\"],\n       o_terms=[\"HbA1c reduction\", \"glycemic control\"]\n   )\n   ```\n\n## PICO Search Functionality\n\nThe PICO search tool helps researchers conduct evidence-based literature searches by:\n\n1. Allowing multiple synonym terms for each PICO element\n2. Combining terms within each element using OR operators\n3. Performing AND combinations between elements (P AND I, P AND I AND C, etc.)\n4. Returning both search queries and publication counts for each combination\n\nThis approach helps refine research questions and identify the most relevant literature.\n\n## Rate Limiting\n\nThe server implements automatic retry mechanism with backoff delays to handle potential rate limiting by NCBI's E-utilities service.\n\n## License\n\nThis project is licensed under the BSD 3-Clause License - see the LICENSE file for details.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "pubmed",
        "searches",
        "search",
        "pubmed database",
        "papers pubmed",
        "research tools"
      ],
      "category": "scientific-research-tools"
    },
    "liuwanglei-tal--nasa-mcp": {
      "owner": "liuwanglei-tal",
      "name": "nasa-mcp",
      "url": "https://github.com/liuwanglei-tal/nasa-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/liuwanglei-tal.webp",
      "description": "Access a variety of NASA data through natural language queries, including astronomical images, Mars rover photos, Earth satellite images, near-Earth object data, and space weather information.",
      "stars": 2,
      "forks": 0,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-05-10T14:15:58Z",
      "readme_content": "# NASA MCP Service\n\n一个功能丰富的 NASA 数据查询 MCP 工具，支持多种 NASA API 服务和自然语言查询。\n\n## 功能特点\n\n- 天文图片服务 (APOD)\n  - 获取 NASA 每日天文图片\n  - 支持历史日期查询\n  - 返回图片标题、描述、URL等信息\n\n- 火星探测器照片 (Mars Rovers)\n  - 支持多个火星探测器：好奇号(Curiosity)、机遇号(Opportunity)、勇气号(Spirit)、毅力号(Perseverance)\n  - 按日期查询火星照片\n  - 获取不同相机拍摄的图片\n\n- 地球卫星图像 (Earth)\n  - 根据经纬度获取卫星图像\n  - 支持历史图像查询\n  - 提供高分辨率卫星照片\n\n- 近地天体数据 (NEO - Near Earth Objects)\n  - 获取特定日期的近地小行星信息\n  - 包含小行星大小、距离、速度等数据\n  - 危险系数评估\n\n- 太空天气数据 (Space Weather)\n  - 太阳耀斑事件信息\n  - 日冕物质抛射数据\n  - 实时太空天气状况\n\n## Cursor 安装\n\n1. 打开 Cursor IDE\n2. 在命令面板中输入：\n```bash\n/mcp install @cursor/nasa-mcp\n```\n\n## 配置\n\n1. 访问 [NASA API 门户](https://api.nasa.gov/)\n2. 获取你的 API key\n3. 设置环境变量 `NASA_API_KEY` 或在使用时通过参数传入\n\n## 使用方法\n\n### 在 Cursor 中使用\n\n```bash\n# 获取今天的天文图片\n今天的天文图\n\n# 获取火星照片\n火星 好奇号 2024年3月18日\n\n# 获取地球卫星图像\n地球 39.9042,116.4074\n\n# 获取近地天体数据\n近地小行星 2024年3月18日\n\n# 获取太空天气信息\n太空天气 今天\n```\n\n### 在 Node.js 中使用\n\n```javascript\nconst client = require('@smithery/client');\n\nasync function main() {\n  // 获取火星照片\n  const marsPhotos = await client.call('@cursor/nasa-mcp', '火星 好奇号 今天');\n  \n  // 获取近地天体数据\n  const neoData = await client.call('@cursor/nasa-mcp', '近地小行星 今天');\n  \n  console.log(marsPhotos);\n  console.log(neoData);\n}\n\nmain();\n```\n\n### 命令行使用\n\n```bash\n# 使用环境变量中的 API key\necho \"火星 好奇号 今天\" | NASA_API_KEY=your_api_key node nasa-mcp.js\n\n# 使用自定义 API key\necho \"地球 39.9042,116.4074\" | node nasa-mcp.js --api_key=your-api-key\n```\n\n## 返回数据格式\n\n### 天文图片 (APOD)\n```json\n{\n  \"标题\": \"图片标题\",\n  \"日期\": \"2024-03-19\",\n  \"说明\": \"详细解释\",\n  \"图片链接\": \"图片URL\",\n  \"媒体类型\": \"image\",\n  \"版权\": \"版权信息\"\n}\n```\n\n### 火星照片\n```json\n{\n  \"火星车\": \"curiosity\",\n  \"日期\": \"2024-03-19\",\n  \"照片数量\": 5,\n  \"照片列表\": [\n    {\n      \"id\": 1234567,\n      \"拍摄时间\": \"2024-03-19\",\n      \"相机\": \"NAVCAM\",\n      \"图片链接\": \"https://...\"\n    }\n  ]\n}\n```\n\n### 近地天体数据\n```json\n{\n  \"日期\": \"2024-03-19\",\n  \"小行星数量\": 5,\n  \"小行星列表\": [\n    {\n      \"名称\": \"小行星名称\",\n      \"直径\": {\n        \"最小\": 100,\n        \"最大\": 200\n      },\n      \"是否危险\": \"否\",\n      \"最近距离\": \"1000000 公里\",\n      \"相对速度\": \"50000 公里/小时\"\n    }\n  ]\n}\n```\n\n### 太空天气数据\n```json\n{\n  \"日期\": \"2024-03-19\",\n  \"太阳耀斑\": [\n    {\n      \"开始时间\": \"2024-03-19T10:00:00Z\",\n      \"结束时间\": \"2024-03-19T11:00:00Z\",\n      \"等级\": \"M2.5\",\n      \"位置\": \"N30W60\"\n    }\n  ],\n  \"日冕物质抛射\": [\n    {\n      \"时间\": \"2024-03-19T12:00:00Z\",\n      \"速度\": \"800 km/s\",\n      \"类型\": \"CME\"\n    }\n  ]\n}\n```\n\n## 许可证\n\nMIT\n\n## 作者\n\nsimon liu",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "nasa",
        "liuwanglei",
        "mars",
        "nasa mcp",
        "nasa data",
        "tools liuwanglei"
      ],
      "category": "scientific-research-tools"
    },
    "luojunhui1--ImageOnC": {
      "owner": "luojunhui1",
      "name": "ImageOnC",
      "url": "https://github.com/luojunhui1/ImageOnC",
      "imageUrl": "/freedevtools/mcp/pfp/luojunhui1.webp",
      "description": "Implement vehicle license plate recognition using C/C++ on FPGA, utilizing OpenCV for image display and Eigen for optimized matrix operations. The project includes code for training neural networks and processing license plate images.",
      "stars": 1,
      "forks": 0,
      "license": "No License",
      "language": "C++",
      "updated_at": "2024-05-17T18:19:28Z",
      "readme_content": "# ImageOnC\n## 1.介绍\n本仓库为实现在FPGA上的车牌识别而创建，但本仓库只保有C/C++部分的代码，并未保存使用HLS工具后的代码。要特别说明的是，本项目中的代码均用C实现，其中出现的C++主要为便于OpenCV进行图像显示或者Eigen库加速矩阵运算，但均可删除或改成C中的数组而不影响其正常功能。\n## 2.文件组成\n```\n.\n├── build\n├── cmake-build-debug\n├── CMakeLists.txt\n├── database\n├── Fit.cpp\n├── Han\n├── include\n├── Letters\n├── main.cpp\n├── paramLetters.txt\n├── param.txt\n├── README.md\n└── Train.cpp\n```\n\n其中build和cmake-build-debug文件均为编译执行过程产生的文件；CMakeLists.txt用于指导编译方式；database为车牌图片文件夹；Fit.cpp原作测试网络准确性，但其内容在测试后被整合到main.cpp中，故该文件无实际意义；Han文件夹保存了用于训练汉字识别的图像；Letters中则保存了用于训练字母和数字识别的代码；main.cpp为执行的识别车牌的主函数；Train.cpp用于训练神经网络；param.txt及paramLetters.txt则保存了网络参数；include文件中保存了一写自定义的功能函数，其文件树如下：\n```\n.\n├── Config.h\n├── Eigen\n├── FileProcess.h\n├── ModelTrans.h\n├── Net.h\n├── Process.h\n├── SaveLoad.h\n└── unsupported\n```\n**Config.h**: 用于约定网络参数和一些全局变量，便于项目代码组织\n\n**Eigen**: Eigen库代码\n\n**unsupported**: Eigen库代码,原为使用Tensor类表示高维矩阵，但Tensor使用不便，实际未使用\n\n**FileProcess**: 用于系统文件操作，主要是查询文件夹下的所有文件并遍历\n\n**ModelTrans**: 用于从图像的数据矩阵中读取BGR图像并将其分割、保存\n\n**Net.h**: 神经网络的定义、训练及使用部分\n\n**SaveLoad**: 用于从图像路径读取bmp图像并分通道保存图像数据部分\n\n## 3. 实际效果\n数据集比较简单，能做到100%。\n\n\n\n\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "opencv",
        "recognition",
        "imageonc",
        "plate recognition",
        "opencv image",
        "utilizing opencv"
      ],
      "category": "scientific-research-tools"
    },
    "mahdin75--gis-mcp": {
      "owner": "mahdin75",
      "name": "gis-mcp",
      "url": "https://github.com/mahdin75/gis-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/mahdin75.webp",
      "description": "Integrates GIS capabilities for geospatial analysis and operations, supporting geometric operations, coordinate transformations, and spatial calculations. Utilizes GIS libraries such as Shapely and PyProj for enhanced AI spatial functionality.",
      "stars": 58,
      "forks": 16,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-10-04T02:01:53Z",
      "readme_content": "# GIS MCP Server\n\n<div align=\"center\">\n  <h3>✨ Want to perform accurate geospatial analysis in your chatbot? ✨</h3>\n  <p><strong>Install GIS-MCP and transform your AI's spatial capabilities!</strong></p>\n  <br/>\n  \n  <br/>\n  <br/>\n</div>\n\nA Model Context Protocol (MCP) server implementation that connects Large Language Models (LLMs) to GIS operations using GIS libraries, enabling AI assistants to perform geospatial operations and transformations.\n\n🌐 **Website:** [gis-mcp.com](https://gis-mcp.com)\n\n> Current version is 0.9.0 (Beta):\n>\n> Version 0.10.0 (Beta) is under active development. We welcome contributions and developers to join us in building this project.\n\n## 🎥 Demo\n\n<div align=\"center\">\n  \n</div>\n\n### Rasterio Demo\n\n<div align=\"center\">\n  <a href=\"https://www.veed.io/view/95ff85f4-efbb-4154-9a04-d966c6ae1737?panel=share\">\n    <br/>\n    <em>Click to watch the Rasterio demo video or go to docs folder</em>\n  </a>\n</div>\n\n## 📋 Table of Contents\n\n- [Features](#-features)\n- [Prerequisites](#-prerequisites)\n- [Installation](#-installation)\n  - [Installing via Smithery](#-installing-via-smithery)\n  - [pip Installation](#-pip-installation)\n  - [Development Installation](#-development-installation)\n- [Available Functions](#-available-functions)\n  - [Shapely Functions](#-shapely-functions-29-total)\n  - [PyProj Functions](#-pyproj-functions-13-total)\n  - [GeoPandas Functions](#-geopandas-functions-13-total)\n  - [Rasterio Functions](#-rasterio-functions-20-total)\n  - [PySAL Functions](#-pysal-functions-15-total)\n  - [Visualization Functions](#-visualization-functions-2-total)\n    - [Static Map Functions](#-static-map-functions-1-total)\n    - [Interactive Web Map Functions](#-interactive-web-map-functions-1-total)\n  - [Administrative Boundaries Functions](#-administrative-boundaries-functions-1-total)\n  - [Climate Data Functions](#-climate-data-functions-1-total)\n  - [Ecology Data Functions](#-ecology-data-functions-2-total)\n  - [Movement Data Functions](#-movement-data-functions-2-total)\n  - [Land Cover Data Functions](#-land-cover-data-functions-2-total)\n  - [Satellite Imagery Functions](#-satellite-imagery-functions-1-total)\n- [Client Development](#-client-development)\n- [Planned Features](#-planned-features)\n- [Contributing](#-contributing)\n- [License](#-license)\n- [Related Projects](#-related-projects)\n- [Support](#-support)\n- [Badges](#-badges)\n\n## 🚀 Features\n\nGIS MCP Server empowers AI assistants with advanced geospatial intelligence. Key features include:\n\n- 🔹 **Comprehensive Geometry Operations** – Perform intersection, union, buffer, difference, and other geometric transformations with ease.\n- 🔹 **Advanced Coordinate Transformations** – Effortlessly reproject and transform geometries between coordinate reference systems.\n- 🔹 **Accurate Measurements** – Compute distances, areas, lengths, and centroids precisely.\n- 🔹 **Spatial Analysis & Validation** – Validate geometries, run proximity checks, and perform spatial overlays or joins.\n- 🔹 **Raster & Vector Support** – Process raster layers, compute indices like NDVI, clip, resample, and merge with vector data.\n- 🔹 **Spatial Statistics & Modeling** – Leverage PySAL for spatial autocorrelation, clustering, and neighborhood analysis.\n- 🔹 **Easy Integration** – Connect seamlessly with MCP-compatible clients like Claude Desktop or Cursor IDE.\n- 🔹 **Flexible & Extensible** – Supports Python-based GIS libraries and is ready for custom tools or workflow extensions.\n\n> 🌟 **Tip:** With GIS MCP Server, your AI can now “think spatially,” unlocking new capabilities for environmental analysis, mapping, and location intelligence.\n\n---\n\n## 📋 Prerequisites\n\n- Python 3.10 or higher\n- MCP-compatible client (like Claude Desktop or Cursor)\n- Internet connection for package installation\n\n## 🛠 Installation\n\nChoose the installation method that best suits your needs:\n\n### 🛠 Installing via Smithery\n\nTo install GIS MCP Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@mahdin75/gis-mcp):\n\n```bash\nnpx -y @smithery/cli install @mahdin75/gis-mcp --client claude\n```\n\n### 📦 pip Installation\n\nThe pip installation is recommended for most users:\n\n1. Install uv package manager:\n\n```bash\npip install uv\n```\n\n2. Create the Virtual Environment (Python 3.10+):\n\n```bash\nuv venv --python=3.10\n```\n\n3. Install the package:\n\n```bash\nuv pip install gis-mcp\n```\n\n#### Install with Visualization Features\n\nTo install with visualization capabilities (Folium and PyDeck for interactive maps):\n\n```bash\nuv pip install gis-mcp[visualize]\n```\n\nThis will install additional dependencies:\n\n- `folium>=0.15.0` - For creating interactive web maps\n- `pydeck>=0.9.0` - For advanced 3D visualizations\n\n4. Start the server:\n\n```bash\ngis-mcp\n```\n\n#### pip Configuration\n\nTo use the pip installation with Claude or Cursor, add the following configuration:\n\n**Claude Desktop:**\n\n**Windows:**\n\n```json\n{\n  \"mcpServers\": {\n    \"gis-mcp\": {\n      \"command\": \"C:\\\\Users\\\\YourUsername\\\\.venv\\\\Scripts\\\\gis-mcp\",\n      \"args\": []\n    }\n  }\n}\n```\n\n**Linux/Mac:**\n\n```json\n{\n  \"mcpServers\": {\n    \"gis-mcp\": {\n      \"command\": \"/home/YourUsername/.venv/bin/gis-mcp\",\n      \"args\": []\n    }\n  }\n}\n```\n\n**Cursor IDE** (create `.cursor/mcp.json`):\n\n**Windows:**\n\n```json\n{\n  \"mcpServers\": {\n    \"gis-mcp\": {\n      \"command\": \"C:\\\\Users\\\\YourUsername\\\\.venv\\\\Scripts\\\\gis-mcp\",\n      \"args\": []\n    }\n  }\n}\n```\n\n**Linux/Mac:**\n\n```json\n{\n  \"mcpServers\": {\n    \"gis-mcp\": {\n      \"command\": \"/home/YourUsername/.venv/bin/gis-mcp\",\n      \"args\": []\n    }\n  }\n}\n```\n\nAfter configuration:\n\n1. Make sure to replace `YourUsername` with your actual username\n2. For development installation, replace `/path/to/gis-mcp` with the actual path to your project\n3. Restart your IDE to apply the changes\n4. You can now use all GIS operations through Claude or Cursor!\n\n### 🛠 Development Installation\n\nFor contributors and developers:\n\n1. Install uv package manager:\n\n```bash\npip install uv\n```\n\n2. Create the Virtual Environment:\n\n```bash\nuv venv --python=3.10\n```\n\n3. Install the package in development mode:\n\n```bash\nuv pip install -e .\n```\n\n4. Start the server:\n\n```bash\npython -m gis_mcp\n```\n\n#### Development Configuration\n\nTo use the development installation with Claude or Cursor, add the following configuration:\n\n**Claude Desktop:**\n\n**Windows:**\n\n```json\n{\n  \"mcpServers\": {\n    \"gis-mcp\": {\n      \"command\": \"C:\\\\path\\\\to\\\\gis-mcp\\\\.venv\\\\Scripts\\\\python\",\n      \"args\": [\"-m\", \"gis_mcp\"]\n    }\n  }\n}\n```\n\n**Linux/Mac:**\n\n```json\n{\n  \"mcpServers\": {\n    \"gis-mcp\": {\n      \"command\": \"/path/to/gis-mcp/.venv/bin/python\",\n      \"args\": [\"-m\", \"gis_mcp\"]\n    }\n  }\n}\n```\n\n**Cursor IDE** (create `.cursor/mcp.json`):\n\n**Windows:**\n\n```json\n{\n  \"mcpServers\": {\n    \"gis-mcp\": {\n      \"command\": \"C:\\\\path\\\\to\\\\gis-mcp\\\\.venv\\\\Scripts\\\\python\",\n      \"args\": [\"-m\", \"gis_mcp\"]\n    }\n  }\n}\n```\n\n**Linux/Mac:**\n\n```json\n{\n  \"mcpServers\": {\n    \"gis-mcp\": {\n      \"command\": \"/path/to/gis-mcp/.venv/bin/python\",\n      \"args\": [\"-m\", \"gis_mcp\"]\n    }\n  }\n}\n```\n\nAfter configuration:\n\n1. Make sure to replace `YourUsername` with your actual username\n2. For development installation, replace `/path/to/gis-mcp` with the actual path to your project\n3. Restart your IDE to apply the changes\n4. You can now use all GIS operations through Claude or Cursor!\n\n## 📚 Available Functions\n\nThis section provides a comprehensive list of all available functions organized by library.\n\n### 🔷 Shapely Functions (29 total)\n\n**Basic Geometric Operations:**\n\n- `buffer` - Create buffer around geometry\n- `intersection` - Find intersection of two geometries\n- `union` - Combine two geometries\n- `difference` - Find difference between geometries\n- `symmetric_difference` - Find symmetric difference\n\n**Geometric Properties:**\n\n- `convex_hull` - Calculate convex hull\n- `envelope` - Get bounding box\n- `minimum_rotated_rectangle` - Get minimum rotated rectangle\n- `get_centroid` - Get centroid point\n- `get_bounds` - Get geometry bounds\n- `get_coordinates` - Extract coordinate array\n- `get_geometry_type` - Get geometry type name\n\n**Transformations:**\n\n- `rotate_geometry` - Rotate geometry by angle\n- `scale_geometry` - Scale geometry by factors\n- `translate_geometry` - Move geometry by offset\n\n**Advanced Operations:**\n\n- `triangulate_geometry` - Create triangulation\n- `voronoi` - Create Voronoi diagram\n- `unary_union_geometries` - Union multiple geometries\n\n**Measurements:**\n\n- `get_length` - Calculate geometry length\n- `get_area` - Calculate geometry area\n\n**Validation & Utilities:**\n\n- `is_valid` - Check geometry validity\n- `make_valid` - Fix invalid geometry\n- `simplify` - Simplify geometry\n- `snap_geometry` - Snap to reference geometry\n- `nearest_point_on_geometry` - Find nearest point\n- `normalize_geometry` - Normalize orientation\n- `geometry_to_geojson` - Convert to GeoJSON\n- `geojson_to_geometry` - Convert from GeoJSON\n\n### 🔷 PyProj Functions (13 total)\n\n**Coordinate Transformations:**\n\n- `transform_coordinates` - Transform point coordinates\n- `project_geometry` - Project geometry between CRS\n\n**CRS Information:**\n\n- `get_crs_info` - Get detailed CRS information\n- `get_available_crs` - List available CRS systems\n- `get_utm_zone` - Get UTM zone for coordinates\n- `get_utm_crs` - Get UTM CRS for coordinates\n- `get_geocentric_crs` - Get geocentric CRS\n\n**Geodetic Calculations:**\n\n- `get_geod_info` - Get ellipsoid information\n- `calculate_geodetic_distance` - Calculate distance on ellipsoid\n- `calculate_geodetic_point` - Calculate point at distance/azimuth\n- `calculate_geodetic_area` - Calculate area on ellipsoid\n\n### 🔷 GeoPandas Functions (13 total)\n\n**I/O Operations:**\n\n- `read_file_gpd` - Read geospatial file with preview\n- `write_file_gpd` - Export GeoDataFrame to file\n\n**Join & Merge Operations:**\n\n- `append_gpd` - Concatenate GeoDataFrames vertically\n- `merge_gpd` - Database-style attribute joins\n- `overlay_gpd` - Spatial overlay operations\n- `dissolve_gpd` - Dissolve by attribute\n- `explode_gpd` - Split multi-part geometries\n\n**Spatial Operations:**\n\n- `clip_vector` - Clip geometries\n- `sjoin_gpd` - Spatial joins\n- `sjoin_nearest_gpd` - Nearest neighbor spatial joins\n- `point_in_polygon` - Point-in-polygon tests\n\n### 🔷 Rasterio Functions (20 total)\n\n**Basic Raster Operations:**\n\n- `metadata_raster` - Get raster metadata\n- `get_raster_crs` - Get raster CRS\n- `extract_band` - Extract single band\n- `raster_band_statistics` - Calculate band statistics\n- `raster_histogram` - Compute pixel histograms\n\n**Raster Processing:**\n\n- `clip_raster_with_shapefile` - Clip raster with polygons\n- `resample_raster` - Resample by scale factor\n- `reproject_raster` - Reproject to new CRS\n- `tile_raster` - Split into tiles\n\n**Raster Analysis:**\n\n- `compute_ndvi` - Calculate vegetation index\n- `raster_algebra` - Mathematical operations on bands\n- `concat_bands` - Combine single-band rasters\n- `weighted_band_sum` - Weighted band combination\n\n**Advanced Analysis:**\n\n- `zonal_statistics` - Statistics within polygons\n- `reclassify_raster` - Reclassify pixel values\n- `focal_statistics` - Moving window statistics\n- `hillshade` - Generate hillshade from DEM\n- `write_raster` - Write array to raster file\n\n### 🔷 PySAL Functions (15 total)\n\n**Spatial Autocorrelation:**\n\n- `morans_i` - Global Moran's I statistic\n- `gearys_c` - Global Geary's C statistic\n- `gamma_statistic` - Gamma index\n- `getis_ord_g` - Global Getis-Ord G statistic\n\n**Local Statistics:**\n\n- `moran_local` - Local Moran's I\n- `getis_ord_g_local` - Local Getis-Ord G\\*\n- `join_counts_local` - Local join counts\n\n**Global Statistics:**\n\n- `join_counts` - Binary join counts test\n- `adbscan` - Adaptive density-based clustering\n\n**Spatial Weights:**\n\n- `weights_from_shapefile` - Create weights from shapefile\n- `distance_band_weights` - Distance-based weights\n- `knn_weights` - K-nearest neighbors weights\n- `build_transform_and_save_weights` - Build, transform, and save weights\n- `ols_with_spatial_diagnostics_safe` - OLS regression with spatial diagnostics\n- `build_and_transform_weights` - Build and transform weights\n\n### 🔷 Visualization Functions (2 total)\n\n**Static Map Visualization (Matplotlib/GeoPandas):**\n\n- `create_map` – Generate high-quality static maps (PNG, PDF, JPG) from multiple geospatial data sources including shapefiles, rasters, WKT geometries, and coordinate arrays. Supports multiple layers with individual styling options, legends, titles, and grid overlays.\n\n**Interactive Web Map Visualization (Folium):**\n\n- `create_web_map` – Generate interactive HTML maps using Folium with layer controls, legends, scale bars, dynamic titles, tooltips, and minimap. Supports multiple basemap options and responsive design for web browsers.\n\n### 🔷 Administrative Boundaries Functions (1 total)\n\n**Boundary Download:**\n\n- `download_boundaries` - Download GADM administrative boundaries and save as GeoJSON\n\n### 🔷 Climate Data Functions (1 total)\n\n**Climate Data Download:**\n\n- `download_climate_data` - Download climate data (ERA5 or other CDS datasets)\n\n### 🔷 Ecology Data Functions (2 total)\n\n**Ecology Data Download and Info:**\n\n- `get_species_info` – Retrieve taxonomic information for a given species name\n- `download_species_occurrences` – Download occurrence records for a given species and save as JSON\n\n### 🔷 Movement Data Functions (2 total)\n\n**Movement Data Download and Routing (via [OSMnx](https://osmnx.readthedocs.io/en/stable/)):**\n\n- `download_street_network` – Download a street network for a given place and save as GraphML\n- `calculate_shortest_path` – Calculate the shortest path between two points using a saved street network\n\n### 🔷 Land Cover Data Functions (2 total)\n\n**Land Cover from Planetary Computer:**\n\n- `download_worldcover` – Download ESA WorldCover for AOI/year; optional crop and reprojection\n- `compute_s2_ndvi` – Compute NDVI from Sentinel-2 L2A; crop and reprojection supported\n\n### 🔷 Satellite Imagery Functions (1 total)\n\n**STAC-based Satellite Download:**\n\n- `download_satellite_imagery` – Download and stack bands from STAC items (e.g., Sentinel-2, Landsat), with optional crop and reprojection\n\n**Total Functions Available: 89**\n\n## 🛠 Client Development\n\nExample usage of the tools:\n\n### Buffer Operation\n\n```python\nTool: buffer\nParameters: {\n    \"geometry\": \"POINT(0 0)\",\n    \"distance\": 10,\n    \"resolution\": 16,\n    \"join_style\": 1,\n    \"mitre_limit\": 5.0,\n    \"single_sided\": false\n}\n```\n\n### Coordinate Transformation\n\n```python\nTool: transform_coordinates\nParameters: {\n    \"coordinates\": [0, 0],\n    \"source_crs\": \"EPSG:4326\",\n    \"target_crs\": \"EPSG:3857\"\n}\n```\n\n### Geodetic Distance\n\n```python\nTool: calculate_geodetic_distance\nParameters: {\n    \"point1\": [0, 0],\n    \"point2\": [10, 10],\n    \"ellps\": \"WGS84\"\n}\n```\n\n### Static Map Creation\n\n```python\nTool: create_map\nParameters: {\n    \"layers\": [\n        {\n            \"data\": \"buildings.shp\",\n            \"style\": {\"label\": \"Buildings\", \"color\": \"red\", \"alpha\": 0.7}\n        },\n        {\n            \"data\": \"roads.shp\",\n            \"style\": {\"label\": \"Roads\", \"color\": \"black\", \"linewidth\": 1}\n        }\n    ],\n    \"filename\": \"city_analysis\",\n    \"filetype\": \"png\",\n    \"title\": \"City Infrastructure Analysis\",\n    \"show_grid\": true,\n    \"add_legend\": true\n}\n```\n\n### Interactive Web Map Creation\n\n```python\nTool: create_web_map\nParameters: {\n    \"layers\": [\n        {\n            \"data\": \"buildings.shp\",\n            \"style\": {\"label\": \"Buildings\", \"color\": \"red\"}\n        },\n        {\n            \"data\": \"parks.geojson\",\n            \"style\": {\"label\": \"Parks\", \"color\": \"green\"}\n        }\n    ],\n    \"filename\": \"city_interactive.html\",\n    \"title\": \"City Infrastructure Map\",\n    \"basemap\": \"CartoDB positron\",\n    \"show_grid\": true,\n    \"add_legend\": true,\n    \"add_minimap\": true\n}\n```\n\n## 🔮 Planned Features\n\n- Implement advanced spatial indexing\n- Implement network analysis capabilities\n- Add support for 3D geometries\n- Implement performance optimizations\n- Add support for more GIS libraries\n\n## 🤝 Contributing\n\nWe welcome contributions! Here's how you can help:\n\n1. Fork the repository\n2. Create a feature branch (`git checkout -b feature/AmazingFeature`)\n3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)\n4. Push to the branch (`git push origin feature/AmazingFeature`)\n5. Open a Pull Request\n\nPlease ensure your PR description clearly describes the problem and solution. Include the relevant issue number if applicable.\n\n## 📄 License\n\nThis project is licensed under the MIT License - see the LICENSE file for details.\n\n## 🔗 Related Projects\n\n| Project Name                                                                                        | Category                   | Description                                                                                                   |\n| --------------------------------------------------------------------------------------------------- | -------------------------- | ------------------------------------------------------------------------------------------------------------- |\n| [Model Context Protocol](https://github.com/anthropics/model-context-protocol)                      | MCP Related                | The core MCP implementation                                                                                   |\n| [Shapely](https://github.com/shapely/shapely)                                                       | Geospatial Analysis        | Python package for manipulation and analysis of geometric objects                                             |\n| [PyProj](https://github.com/pyproj4/pyproj)                                                         | Geospatial Analysis        | Python interface to PROJ library                                                                              |\n| [GeoPandas](https://github.com/geopandas/geopandas)                                                 | Geospatial Analysis        | Python package for working with geospatial data                                                               |\n| [Rasterio](https://github.com/rasterio/rasterio)                                                    | Geospatial Analysis        | Python package for reading and writing geospatial raster data                                                 |\n| [PySAL](https://github.com/pysal/pysal)                                                             | Geospatial Analysis        | Python spatial analysis library for geospatial data science                                                   |\n| [cdsapi](https://github.com/ecmwf/cdsapi)                                                           | Geospatial Data Collecting | Python API to access the Copernicus Climate Data Store (CDS)                                                  |\n| [pygadm](https://github.com/12rambau/pygadm)                                                        | Geospatial Data Collecting | Easy access to administrative boundary defined by GADM from Python scripts                                    |\n| [pygbif](https://github.com/gbif/pygbif)                                                            | Geospatial Data Collecting | Python client for the GBIF API (ecology and biodiversity data)                                                |\n| [OSMnx](https://osmnx.readthedocs.io/en/stable/)                                                    | Geospatial Data Collecting | Python package for downloading, modeling, and analyzing street networks and urban features from OpenStreetMap |\n| [pystac-client](https://github.com/stac-utils/pystac-client)                                        | Geospatial Data Collecting | Python client for STAC catalogs; search and access spatiotemporal assets                                      |\n| [Planetary Computer SDK for Python](https://github.com/microsoft/planetary-computer-sdk-for-python) | Geospatial Data Collecting | Python SDK for Microsoft Planetary Computer; auth and helpers for STAC/COGs                                   |\n\n## 📞 Support\n\nFor support, please open an issue in the GitHub repository.\n\n## 💬 Community\n\nJoin our Discord community for discussions, updates, and support:\n\n[![Join our Discord](https://img.shields.io/badge/Discord-Join%20our%20community-7289DA?style=for-the-badge&logo=discord&logoColor=white)](https://discord.gg/dzkXZsZK)\n\n## 👥 Contributors\n\n<a href=\"https://github.com/mahdin75/gis-mcp/graphs/contributors\">\n  <img alt=\"gis_mcp\" src=\"https://contrib.rocks/image?repo=mahdin75/gis-mcp\" />\n</a>\n\nMade with [contrib.rocks](https://contrib.rocks).\n\n<br/>\n\n## 🏆 Badges\n\n<div align=\"center\">\n\n[![Trust Score](https://archestra.ai/mcp-catalog/api/badge/quality/mahdin75/gis-mcp)](https://archestra.ai/mcp-catalog/mahdin75__gis-mcp)\n<br/></br>\n\n  <a href=\"https://glama.ai/mcp/servers/@mahdin75/gis-mcp\">\n    <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@mahdin75/gis-mcp/badge\" alt=\"GIS Server MCP server\" />\n  </a>\n  <br/><br/><br/>\n  \n  <a href=\"https://mcp.so/server/gis-mcp-server/mahdin75\">\n    <img src=\"https://mcp.so/logo.png\" alt=\"MCP.so Badge\" width=\"150\"/>\n  </a>\n</div>",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "gis",
        "mahdin75",
        "mcp",
        "mahdin75 gis",
        "gis mcp",
        "utilizes gis"
      ],
      "category": "scientific-research-tools"
    },
    "mrexodia--ida-pro-mcp": {
      "owner": "mrexodia",
      "name": "ida-pro-mcp",
      "url": "https://github.com/mrexodia/ida-pro-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/mrexodia.webp",
      "description": "Enables reverse engineering in IDA Pro through a simple integration that allows interaction with various MCP tools. Provides functionality for checking connection status, retrieving metadata, and accessing functions by name or address.",
      "stars": 3801,
      "forks": 382,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-10-04T11:29:25Z",
      "readme_content": "# IDA Pro MCP\r\n\r\nSimple [MCP Server](https://modelcontextprotocol.io/introduction) to allow vibe reversing in IDA Pro.\r\n\r\nhttps://github.com/user-attachments/assets/6ebeaa92-a9db-43fa-b756-eececce2aca0\r\n\r\nThe binaries and prompt for the video are available in the [mcp-reversing-dataset](https://github.com/mrexodia/mcp-reversing-dataset) repository.\r\n\r\nAvailable functionality:\r\n\r\n- `check_connection()`: Check if the IDA plugin is running.\r\n- `get_metadata()`: Get metadata about the current IDB.\r\n- `get_function_by_name(name)`: Get a function by its name.\r\n- `get_function_by_address(address)`: Get a function by its address.\r\n- `get_current_address()`: Get the address currently selected by the user.\r\n- `get_current_function()`: Get the function currently selected by the user.\r\n- `convert_number(text, size)`: Convert a number (decimal, hexadecimal) to different representations.\r\n- `list_functions(offset, count)`: List all functions in the database (paginated).\r\n- `list_globals_filter(offset, count, filter)`: List matching globals in the database (paginated, filtered).\r\n- `list_globals(offset, count)`: List all globals in the database (paginated).\r\n- `list_strings_filter(offset, count, filter)`: List matching strings in the database (paginated, filtered).\r\n- `list_strings(offset, count)`: List all strings in the database (paginated).\r\n- `list_local_types()`: List all Local types in the database.\r\n- `decompile_function(address)`: Decompile a function at the given address.\r\n- `disassemble_function(start_address)`: Get assembly code (address: instruction; comment) for a function.\r\n- `get_xrefs_to(address)`: Get all cross references to the given address.\r\n- `get_xrefs_to_field(struct_name, field_name)`: Get all cross references to a named struct field (member).\r\n- `get_entry_points()`: Get all entry points in the database.\r\n- `set_comment(address, comment)`: Set a comment for a given address in the function disassembly and pseudocode.\r\n- `rename_local_variable(function_address, old_name, new_name)`: Rename a local variable in a function.\r\n- `rename_global_variable(old_name, new_name)`: Rename a global variable.\r\n- `set_global_variable_type(variable_name, new_type)`: Set a global variable's type.\r\n- `rename_function(function_address, new_name)`: Rename a function.\r\n- `set_function_prototype(function_address, prototype)`: Set a function's prototype.\r\n- `declare_c_type(c_declaration)`: Create or update a local type from a C declaration.\r\n- `set_local_variable_type(function_address, variable_name, new_type)`: Set a local variable's type.\r\n\r\nUnsafe functions (`--unsafe` flag required):\r\n\r\n- `dbg_get_registers()`: Get all registers and their values. This function is only available when debugging.\r\n- `dbg_get_call_stack()`: Get the current call stack.\r\n- `dbg_list_breakpoints()`: List all breakpoints in the program.\r\n- `dbg_start_process()`: Start the debugger.\r\n- `dbg_exit_process()`: Exit the debugger.\r\n- `dbg_continue_process()`: Continue the debugger.\r\n- `dbg_run_to(address)`: Run the debugger to the specified address.\r\n- `dbg_set_breakpoint(address)`: Set a breakpoint at the specified address.\r\n- `dbg_delete_breakpoint(address)`: del a breakpoint at the specified address.\r\n- `dbg_enable_breakpoint(address, enable)`: Enable or disable a breakpoint at the specified address.\r\n\r\n## Prerequisites\r\n\r\n- [Python](https://www.python.org/downloads/) (**3.11 or higher**) \r\n  - Use `idapyswitch` to switch to the newest Python version\r\n- [IDA Pro](https://hex-rays.com/ida-pro) (8.3 or higher, 9 recommended), **IDA Free is not supported**\r\n- Supported MCP Client (pick one you like)\r\n  - [Cline](https://cline.bot)\r\n  - [Roo Code](https://roocode.com)\r\n  - [Claude](https://claude.ai/download)\r\n  - [Cursor](https://cursor.com)\r\n  - [VSCode Agent Mode](https://github.blog/news-insights/product-news/github-copilot-agent-mode-activated/)\r\n  - [Windsurf](https://windsurf.com)\r\n  - [Other MCP Clients](https://modelcontextprotocol.io/clients#example-clients): Run `ida-pro-mcp --config` to get the JSON config for your client.\r\n\r\n## Installation\r\n\r\nInstall the latest version of the IDA Pro MCP package:\r\n\r\n```sh\r\npip uninstall ida-pro-mcp\r\npip install https://github.com/mrexodia/ida-pro-mcp/archive/refs/heads/main.zip\r\n```\r\n\r\nConfigure the MCP servers and install the IDA Plugin:\r\n\r\n```\r\nida-pro-mcp --install\r\n```\r\n\r\n**Important**: Make sure you completely restart IDA/Visual Studio Code/Claude for the installation to take effect. Claude runs in the background and you need to quit it from the tray icon.\r\n\r\nhttps://github.com/user-attachments/assets/65ed3373-a187-4dd5-a807-425dca1d8ee9\r\n\r\n_Note_: You need to load a binary in IDA before the plugin menu will show up.\r\n\r\n## Prompt Engineering\r\n\r\nLLMs are prone to hallucinations and you need to be specific with your prompting. For reverse engineering the conversion between integers and bytes are especially problematic. Below is a minimal example prompt, feel free to start a discussion or open an issue if you have good results with a different prompt:\r\n\r\n> Your task is to analyze a crackme in IDA Pro. You can use the MCP tools to retrieve information. In general use the following strategy:\r\n> - Inspect the decompilation and add comments with your findings\r\n> - Rename variables to more sensible names\r\n> - Change the variable and argument types if necessary (especially pointer and array types)\r\n> - Change function names to be more descriptive\r\n> - If more details are necessary, disassemble the function and add comments with your findings\r\n> - NEVER convert number bases yourself. Use the convert_number MCP tool if needed!\r\n> - Do not attempt brute forcing, derive any solutions purely from the disassembly and simple python scripts\r\n> - Create a report.md with your findings and steps taken at the end\r\n> - When you find a solution, prompt to user for feedback with the password you found\r\n\r\nThis prompt was just the first experiment, please share if you found ways to improve the output!\r\n\r\nLive stream discussing prompting and showing some real-world malware analysis:\r\n\r\n[![0](https://img.youtube.com/vi/iFxNuk3kxhk/0.jpg)](https://www.youtube.com/watch?v=iFxNuk3kxhk)\r\n\r\n## Tips for Enhancing LLM Accuracy\r\n\r\nLarge Language Models (LLMs) are powerful tools, but they can sometimes struggle with complex mathematical calculations or exhibit \"hallucinations\" (making up facts). Make sure to tell the LLM to use the `conver_number` MCP and you might also need [math-mcp](https://github.com/EthanHenrickson/math-mcp) for certain operations.\r\n\r\nAnother thing to keep in mind is that LLMs will not perform well on obfuscated code. Before trying to use an LLM to solve the problem, take a look around the binary and spend some time (automatically) removing the following things:\r\n\r\n- String encryption\r\n- Import hashing\r\n- Control flow flattening\r\n- Code encryption\r\n- Anti-decompilation tricks\r\n\r\nYou should also use a tool like Lumina or FLIRT to try and resolve all the open source library code and the C++ STL, this will further improve the accuracy.\r\n\r\n## SSE Transport & Headless MCP\r\n\r\nYou can run an SSE server to connect to the user interface like this:\r\n\r\n```sh\r\nuv run ida-pro-mcp --transport http://127.0.0.1:8744/sse\r\n```\r\n\r\nAfter installing [`idalib`](https://docs.hex-rays.com/user-guide/idalib) you can also run a headless SSE server:\r\n\r\n```sh\r\nuv run idalib-mcp --host 127.0.0.1 --port 8745 path/to/executable\r\n```\r\n\r\n_Note_: The `idalib` feature was contributed by [Willi Ballenthin](https://github.com/williballenthin).\r\n\r\n## Manual Installation\r\n\r\n_Note_: This section is for LLMs and power users who need detailed installation instructions.\r\n\r\n<details>\r\n\r\n## Manual MCP Server Installation (Cline/Roo Code)\r\n\r\nTo install the MCP server yourself, follow these steps:\r\n\r\n1. Install [uv](https://github.com/astral-sh/uv) globally:\r\n   - Windows: `pip install uv`\r\n   - Linux/Mac: `curl -LsSf https://astral.sh/uv/install.sh | sh`\r\n2. Clone this repository, for this example `C:\\MCP\\ida-pro-mcp`.\r\n3. Navigate to the Cline/Roo Code _MCP Servers_ configuration (see screenshot).\r\n4. Click on the _Installed_ tab.\r\n5. Click on _Configure MCP Servers_, which will open `cline_mcp_settings.json`.\r\n6. Add the `ida-pro-mcp` server:\r\n\r\n```json\r\n{\r\n  \"mcpServers\": {\r\n    \"github.com/mrexodia/ida-pro-mcp\": {\r\n      \"command\": \"uv\",\r\n      \"args\": [\r\n        \"--directory\",\r\n        \"c:\\\\MCP\\\\ida-pro-mcp\",\r\n        \"run\",\r\n        \"server.py\",\r\n        \"--install-plugin\"\r\n      ],\r\n      \"timeout\": 1800,\r\n      \"disabled\": false\r\n    }\r\n  }\r\n}\r\n```\r\n\r\nTo check if the connection works you can perform the following tool call:\r\n\r\n```\r\n<use_mcp_tool>\r\n<server_name>github.com/mrexodia/ida-pro-mcp</server_name>\r\n<tool_name>check_connection</tool_name>\r\n<arguments></arguments>\r\n</use_mcp_tool>\r\n```\r\n\r\n## IDA Plugin installation\r\n\r\nThe IDA Pro plugin will be installed automatically when the MCP server starts. If you disabled the `--install-plugin` option, use the following steps:\r\n\r\n1. Copy (**not move**) `src/ida_pro_mcp/mcp-plugin.py` in your plugins folder (`%appdata%\\Hex-Rays\\IDA Pro\\plugins` on Windows).\r\n2. Open an IDB and click `Edit -> Plugins -> MCP` to start the server.\r\n\r\n</details>\r\n\r\n## Comparison with other MCP servers\r\n\r\nThere are a few IDA Pro MCP servers floating around, but I created my own for a few reasons:\r\n\r\n1. Installation should be fully automated.\r\n2. The architecture of other plugins make it difficult to add new functionality quickly (too much boilerplate of unnecessary dependencies).\r\n3. Learning new technologies is fun!\r\n\r\nIf you want to check them out, here is a list (in the order I discovered them):\r\n\r\n- https://github.com/taida957789/ida-mcp-server-plugin (SSE protocol only, requires installing dependencies in IDAPython).\r\n- https://github.com/fdrechsler/mcp-server-idapro (MCP Server in TypeScript, excessive boilerplate required to add new functionality).\r\n- https://github.com/MxIris-Reverse-Engineering/ida-mcp-server (custom socket protocol, boilerplate).\r\n\r\nFeel free to open a PR to add your IDA Pro MCP server here.\r\n\r\n## Development\r\n\r\nAdding new features is a super easy and streamlined process. All you have to do is add a new `@jsonrpc` function to [`mcp-plugin.py`](https://github.com/mrexodia/ida-pro-mcp/blob/164df8cf4ae251cc9cc0f464591fa6df8e0d9df4/src/ida_pro_mcp/mcp-plugin.py#L406-L419) and your function will be available in the MCP server without any additional boilerplate! Below is a video where I add the `get_metadata` function in less than 2 minutes (including testing):\r\n\r\nhttps://github.com/user-attachments/assets/951de823-88ea-4235-adcb-9257e316ae64\r\n\r\nTo test the MCP server itself:\r\n\r\n```sh\r\nuv run mcp dev src/ida_pro_mcp/server.py\r\n```\r\n\r\nThis will open a web interface at http://localhost:5173 and allow you to interact with the MCP tools for testing.\r\n\r\nFor testing I create a symbolic link to the IDA plugin and then POST a JSON-RPC request directly to `http://localhost:13337/mcp`. After [enabling symbolic links](https://learn.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development) you can run the following command:\r\n\r\n```sh\r\nuv run ida-pro-mcp --install\r\n```\r\n\r\nGenerate the changelog of direct commits to `main`:\r\n\r\n```sh\r\ngit log --first-parent --no-merges 1.2.0..main \"--pretty=- %s\"\r\n```\r\n\r\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "ida",
        "mrexodia",
        "tools",
        "mrexodia ida",
        "mcp tools",
        "ida pro"
      ],
      "category": "scientific-research-tools"
    },
    "nansasuke--GarbageSorting": {
      "owner": "nansasuke",
      "name": "GarbageSorting",
      "url": "https://github.com/nansasuke/GarbageSorting",
      "imageUrl": "/freedevtools/mcp/pfp/nansasuke.webp",
      "description": "Identify and classify waste using image and voice recognition techniques to streamline the recycling process and enhance environmental awareness.",
      "stars": 0,
      "forks": 0,
      "license": "No License",
      "language": "",
      "updated_at": "2025-03-11T13:08:27Z",
      "readme_content": "# GarbageSorting\n图片识别、语音识别、垃圾分类\n\n一个完整的垃圾分类的app\n \n\n![image](https://github.com/hyyz3293/GarbageSorting/blob/master/Images/a.png) ![image](https://github.com/hyyz3293/GarbageSorting/blob/master/Images/b.png)\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "recycling",
        "waste",
        "garbagesorting",
        "classify waste",
        "recycling process",
        "nansasuke garbagesorting"
      ],
      "category": "scientific-research-tools"
    },
    "nguyenvanduocit--all-in-one-model-context-protocol": {
      "owner": "nguyenvanduocit",
      "name": "all-in-one-model-context-protocol",
      "url": "https://github.com/nguyenvanduocit/all-in-one-model-context-protocol",
      "imageUrl": "/freedevtools/mcp/pfp/nguyenvanduocit.webp",
      "description": "Integrate various tools for enhanced development workflows, including managing emails, calendars, and research tasks. It provides functionalities for development tools, data fetching, and leveraging academic resources.",
      "stars": 88,
      "forks": 8,
      "license": "No License",
      "language": "",
      "updated_at": "2025-09-30T10:28:07Z",
      "readme_content": "# THE PROJECT IS MOVED\n\nTHE PROJECT HAS BEEN SPLIT AND MOVED TO INDIVIDUAL REPOSITORIES.\n\n- [Google Kit](https://github.com/nguyenvanduocit/google-kit): Tools for Gmail, Google Calendar, Google Chat\n- [RAG Kit](https://github.com/nguyenvanduocit/rag-kit): Tools for RAG, Memory\n- [Dev Kit](https://github.com/nguyenvanduocit/dev-kit): Tools for developers, jira, confluence, gitlab, github, ...\n- [Fetch Kit](https://github.com/nguyenvanduocit/fetch-kit): Tools for fetch, scrape, ...\n- [Research Kit](https://github.com/nguyenvanduocit/research-kit): Tools for research, academic, reasoning, ...\n\n# MyMCP Server\n\n[![smithery badge](https://smithery.ai/badge/@nguyenvanduocit/all-in-one-model-context-protocol)](https://smithery.ai/server/@nguyenvanduocit/all-in-one-model-context-protocol)\nA powerful Model Context Protocol (MCP) server implementation with integrations for GitLab, Jira, Confluence, YouTube, and more. This server provides AI-powered search capabilities and various utility tools for development workflows.\n\n[Tutorial](https://www.youtube.com/watch?v=XnDFtYKU6xU)\n\n## Community\n\nFor community support, discussions, and updates, please visit our forum at [community.aiocean.io](https://community.aiocean.io/).\n\n\n## Prerequisites\n\n- Go 1.23.2 or higher\n- Various API keys and tokens for the services you want to use\n\n## Installation\n\n### Installing via Smithery\n\nTo install MyMCP Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@nguyenvanduocit/all-in-one-model-context-protocol) (will guide you through interactive CLI setup):\n\n```bash\nnpx -y @smithery/cli install @nguyenvanduocit/all-in-one-model-context-protocol --client claude\n```\n\n*Note: Smithery will interactively prompt you for required configuration values and handle environment setup automatically*\n\n### Installing via Go\n\n1. Install the server:\n```bash\ngo install github.com/nguyenvanduocit/all-in-one-model-context-protocol@latest\n```\n\n2. **Manual setup required** - Create a `.env` file with your configuration:\n```env\nENABLE_TOOLS=\nQDRANT_HOST=\nATLASSIAN_HOST=\nATLASSIAN_EMAIL=\nGITLAB_HOST=\nGITLAB_TOKEN=\nBRAVE_API_KEY=\nATLASSIAN_TOKEN=\nGOOGLE_AI_API_KEY=\nPROXY_URL=\nOPENAI_API_KEY=\nDEEPSEEK_API_KEY=\nQDRANT_PORT=\nGOOGLE_TOKEN_FILE=\nGOOGLE_CREDENTIALS_FILE=\nQDRANT_API_KEY=\n```\n\n3. Config your claude's config:\n\n```json{claude_desktop_config.json}\n{\n  \"mcpServers\": {\n    \"my_mcp_server\": {\n      \"command\": \"all-in-one-model-context-protocol\",\n      \"args\": [\"-env\", \"/path/to/.env\"],\n    }\n  }\n}\n```\n\n## Secret\n\n*GOOGLE_TOKEN_FILE* is using for Google AI like Gemini, Google Search, Google Calendar, etc.\n\n*GOOGLE_CREDENTIALS_FILE* is service account for advanced feature like Google Chat. You need to create project in Google Cloud Console and create service account. Then also need to create Google cloud bot chat for it. Ask any LLM model to create these for you.\n\n\n\n## Enable Tools\n\nThere are a hidden variable `ENABLE_TOOLS` in the environment variable. It is a comma separated list of tools group to enable. If not set, all tools will be enabled. Leave it empty to enable all tools.\n\n\nHere is the list of tools group:\n\n- `gemini`: Gemini-powered search\n- `fetch`: Fetch tools\n- `confluence`: Confluence tools\n- `youtube`: YouTube tools\n- `jira`: Jira tools\n- `gitlab`: GitLab tools\n- `script`: Script tools\n- `rag`: RAG tools\n- `deepseek`: Deepseek AI tools\n\n## Available Tools\n\n### calendar_create_event\n\nCreate a new event in Google Calendar\n\nArguments:\n\n- `summary` (String) (Required): Title of the event\n- `description` (String): Description of the event\n- `start_time` (String) (Required): Start time of the event in RFC3339 format (e.g., 2023-12-25T09:00:00Z)\n- `end_time` (String) (Required): End time of the event in RFC3339 format\n- `attendees` (String): Comma-separated list of attendee email addresses\n\n### calendar_list_events\n\nList upcoming events in Google Calendar\n\nArguments:\n\n- `time_min` (String): Start time for the search in RFC3339 format (default: now)\n- `time_max` (String): End time for the search in RFC3339 format (default: 1 week from now)\n- `max_results` (Number): Maximum number of events to return (default: 10)\n\n### calendar_update_event\n\nUpdate an existing event in Google Calendar\n\nArguments:\n\n- `event_id` (String) (Required): ID of the event to update\n- `summary` (String): New title of the event\n- `description` (String): New description of the event\n- `start_time` (String): New start time of the event in RFC3339 format\n- `end_time` (String): New end time of the event in RFC3339 format\n- `attendees` (String): Comma-separated list of new attendee email addresses\n\n### calendar_respond_to_event\n\nRespond to an event invitation in Google Calendar\n\nArguments:\n\n- `event_id` (String) (Required): ID of the event to respond to\n- `response` (String) (Required): Your response (accepted, declined, or tentative)\n\n### confluence_search\n\nSearch Confluence\n\nArguments:\n\n- `query` (String) (Required): Atlassian Confluence Query Language (CQL)\n\n### confluence_get_page\n\nGet Confluence page content\n\nArguments:\n\n- `page_id` (String) (Required): Confluence page ID\n\n### confluence_create_page\n\nCreate a new Confluence page\n\nArguments:\n\n- `space_key` (String) (Required): The key of the space where the page will be created\n- `title` (String) (Required): Title of the page\n- `content` (String) (Required): Content of the page in storage format (XHTML)\n- `parent_id` (String): ID of the parent page (optional)\n\n### confluence_update_page\n\nUpdate an existing Confluence page\n\nArguments:\n\n- `page_id` (String) (Required): ID of the page to update\n- `title` (String): New title of the page (optional)\n- `content` (String): New content of the page in storage format (XHTML)\n- `version_number` (String): Version number for optimistic locking (optional)\n\n### deepseek_reasoning\n\nadvanced reasoning engine using Deepseek's AI capabilities for multi-step problem solving, critical analysis, and strategic decision support\n\nArguments:\n\n- `question` (String) (Required): The structured query or problem statement requiring deep analysis and reasoning\n- `context` (String) (Required): Defines the operational context and purpose of the query within the MCP ecosystem\n- `knowledge` (String): Provides relevant chat history, knowledge base entries, and structured data context for MCP-aware reasoning\n\n### get_web_content\n\nFetches content from a given HTTP/HTTPS URL. This tool allows you to retrieve text content from web pages, APIs, or any accessible HTTP endpoints. Returns the raw content as text.\n\nArguments:\n\n- `url` (String) (Required): The complete HTTP/HTTPS URL to fetch content from (e.g., https://example.com)\n\n### gchat_list_spaces\n\nList all available Google Chat spaces/rooms\n\n### gchat_send_message\n\nSend a message to a Google Chat space or direct message\n\nArguments:\n\n- `space_name` (String) (Required): Name of the space to send the message to\n- `message` (String) (Required): Text message to send\n\n### ai_web_search\n\nsearch the web by using Google AI Search. Best tool to update realtime information\n\nArguments:\n\n- `question` (String) (Required): The question to ask. Should be a question\n- `context` (String) (Required): Context/purpose of the question, helps Gemini to understand the question better\n\n### gitlab_list_projects\n\nList GitLab projects\n\nArguments:\n\n- `group_id` (String) (Required): gitlab group ID\n- `search` (String): Multiple terms can be provided, separated by an escaped space, either + or %20, and will be ANDed together. Example: one+two will match substrings one and two (in any order).\n\n### gitlab_get_project\n\nGet GitLab project details\n\nArguments:\n\n- `project_path` (String) (Required): Project/repo path\n\n### gitlab_list_mrs\n\nList merge requests\n\nArguments:\n\n- `project_path` (String) (Required): Project/repo path\n- `state` (String) (Default: all): MR state (opened/closed/merged)\n\n### gitlab_get_mr_details\n\nGet merge request details\n\nArguments:\n\n- `project_path` (String) (Required): Project/repo path\n- `mr_iid` (String) (Required): Merge request IID\n\n### gitlab_create_MR_note\n\nCreate a note on a merge request\n\nArguments:\n\n- `project_path` (String) (Required): Project/repo path\n- `mr_iid` (String) (Required): Merge request IID\n- `comment` (String) (Required): Comment text\n\n### gitlab_get_file_content\n\nGet file content from a GitLab repository\n\nArguments:\n\n- `project_path` (String) (Required): Project/repo path\n- `file_path` (String) (Required): Path to the file in the repository\n- `ref` (String) (Required): Branch name, tag, or commit SHA\n\n### gitlab_list_pipelines\n\nList pipelines for a GitLab project\n\nArguments:\n\n- `project_path` (String) (Required): Project/repo path\n- `status` (String) (Default: all): Pipeline status (running/pending/success/failed/canceled/skipped/all)\n\n### gitlab_list_commits\n\nList commits in a GitLab project within a date range\n\nArguments:\n\n- `project_path` (String) (Required): Project/repo path\n- `since` (String) (Required): Start date (YYYY-MM-DD)\n- `until` (String): End date (YYYY-MM-DD). If not provided, defaults to current date\n- `ref` (String) (Required): Branch name, tag, or commit SHA\n\n### gitlab_get_commit_details\n\nGet details of a commit\n\nArguments:\n\n- `project_path` (String) (Required): Project/repo path\n- `commit_sha` (String) (Required): Commit SHA\n\n### gitlab_list_user_events\n\nList GitLab user events within a date range\n\nArguments:\n\n- `username` (String) (Required): GitLab username\n- `since` (String) (Required): Start date (YYYY-MM-DD)\n- `until` (String): End date (YYYY-MM-DD). If not provided, defaults to current date\n\n### gitlab_list_group_users\n\nList all users in a GitLab group\n\nArguments:\n\n- `group_id` (String) (Required): GitLab group ID\n\n### gitlab_create_mr\n\nCreate a new merge request\n\nArguments:\n\n- `project_path` (String) (Required): Project/repo path\n- `source_branch` (String) (Required): Source branch name\n- `target_branch` (String) (Required): Target branch name\n- `title` (String) (Required): Merge request title\n- `description` (String): Merge request description\n\n### gmail_search\n\nSearch emails in Gmail using Gmail's search syntax\n\nArguments:\n\n- `query` (String) (Required): Gmail search query. Follow Gmail's search syntax\n\n### gmail_move_to_spam\n\nMove specific emails to spam folder in Gmail by message IDs\n\nArguments:\n\n- `message_ids` (String) (Required): Comma-separated list of message IDs to move to spam\n\n### gmail_create_filter\n\nCreate a Gmail filter with specified criteria and actions\n\nArguments:\n\n- `from` (String): Filter emails from this sender\n- `to` (String): Filter emails to this recipient\n- `subject` (String): Filter emails with this subject\n- `query` (String): Additional search query criteria\n- `add_label` (Boolean): Add label to matching messages\n- `label_name` (String): Name of the label to add (required if add_label is true)\n- `mark_important` (Boolean): Mark matching messages as important\n- `mark_read` (Boolean): Mark matching messages as read\n- `archive` (Boolean): Archive matching messages\n\n### gmail_list_filters\n\nList all Gmail filters in the account\n\n### gmail_list_labels\n\nList all Gmail labels in the account\n\n### gmail_delete_filter\n\nDelete a Gmail filter by its ID\n\nArguments:\n\n- `filter_id` (String) (Required): The ID of the filter to delete\n\n### gmail_delete_label\n\nDelete a Gmail label by its ID\n\nArguments:\n\n- `label_id` (String) (Required): The ID of the label to delete\n\n### jira_get_issue\n\nRetrieve detailed information about a specific Jira issue including its status, assignee, description, subtasks, and available transitions\n\nArguments:\n\n- `issue_key` (String) (Required): The unique identifier of the Jira issue (e.g., KP-2, PROJ-123)\n\n### jira_search_issue\n\nSearch for Jira issues using JQL (Jira Query Language). Returns key details like summary, status, assignee, and priority for matching issues\n\nArguments:\n\n- `jql` (String) (Required): JQL query string (e.g., 'project = KP AND status = \\\"In Progress\\\"')\n\n### jira_list_sprints\n\nList all active and future sprints for a specific Jira board, including sprint IDs, names, states, and dates\n\nArguments:\n\n- `board_id` (String) (Required): Numeric ID of the Jira board (can be found in board URL)\n\n### jira_create_issue\n\nCreate a new Jira issue with specified details. Returns the created issue's key, ID, and URL\n\nArguments:\n\n- `project_key` (String) (Required): Project identifier where the issue will be created (e.g., KP, PROJ)\n- `summary` (String) (Required): Brief title or headline of the issue\n- `description` (String) (Required): Detailed explanation of the issue\n- `issue_type` (String) (Required): Type of issue to create (common types: Bug, Task, Story, Epic)\n\n### jira_update_issue\n\nModify an existing Jira issue's details. Supports partial updates - only specified fields will be changed\n\nArguments:\n\n- `issue_key` (String) (Required): The unique identifier of the issue to update (e.g., KP-2)\n- `summary` (String): New title for the issue (optional)\n- `description` (String): New description for the issue (optional)\n\n### jira_list_statuses\n\nRetrieve all available issue status IDs and their names for a specific Jira project\n\nArguments:\n\n- `project_key` (String) (Required): Project identifier (e.g., KP, PROJ)\n\n### jira_transition_issue\n\nTransition an issue through its workflow using a valid transition ID. Get available transitions from jira_get_issue\n\nArguments:\n\n- `issue_key` (String) (Required): The issue to transition (e.g., KP-123)\n- `transition_id` (String) (Required): Transition ID from available transitions list\n- `comment` (String): Optional comment to add with transition\n\n### RAG_memory_index_content\n\nIndex a content into memory, can be inserted or updated\n\nArguments:\n\n- `collection` (String) (Required): Memory collection name\n- `filePath` (String) (Required): content file path\n- `payload` (String) (Required): Plain text payload\n\n### RAG_memory_index_file\n\nIndex a local file into memory\n\nArguments:\n\n- `collection` (String) (Required): Memory collection name\n- `filePath` (String) (Required): Path to the local file to be indexed\n\n### RAG_memory_create_collection\n\nCreate a new vector collection in memory\n\nArguments:\n\n- `collection` (String) (Required): Memory collection name\n\n### RAG_memory_delete_collection\n\nDelete a vector collection in memory\n\nArguments:\n\n- `collection` (String) (Required): Memory collection name\n\n### RAG_memory_list_collections\n\nList all vector collections in memory\n\n### RAG_memory_search\n\nSearch for memory in a collection based on a query\n\nArguments:\n\n- `collection` (String) (Required): Memory collection name\n- `query` (String) (Required): search query, should be a keyword\n\n### RAG_memory_delete_index_by_filepath\n\nDelete a vector index by filePath\n\nArguments:\n\n- `collection` (String) (Required): Memory collection name\n- `filePath` (String) (Required): Path to the local file to be deleted\n\n### execute_comand_line_script\n\nSafely execute command line scripts on the user's system with security restrictions. Features sandboxed execution, timeout protection, and output capture. Supports cross-platform scripting with automatic environment detection.\n\nArguments:\n\n- `content` (String) (Required): \n- `interpreter` (String) (Default: /bin/sh): Path to interpreter binary (e.g. /bin/sh, /bin/bash, /usr/bin/python, cmd.exe). Validated against allowed list for security\n- `working_dir` (String): Execution directory path (default: user home). Validated to prevent unauthorized access to system locations\n\n### web_search\n\nSearch the web using Brave Search API\n\nArguments:\n\n- `query` (String) (Required): Query to search for (max 400 chars, 50 words)\n- `count` (Number) (Default: 5): Number of results (1-20, default 5)\n- `country` (String) (Default: ALL): Country code\n\n### sequentialthinking\n\n`A detailed tool for dynamic and reflective problem-solving through thoughts.\nThis tool helps analyze problems through a flexible thinking process that can adapt and evolve.\nEach thought can build on, question, or revise previous insights as understanding deepens.\n\nWhen to use this tool:\n- Breaking down complex problems into steps\n- Planning and design with room for revision\n- Analysis that might need course correction\n- Problems where the full scope might not be clear initially\n- Problems that require a multi-step solution\n- Tasks that need to maintain context over multiple steps\n- Situations where irrelevant information needs to be filtered out\n\nKey features:\n- You can adjust total_thoughts up or down as you progress\n- You can question or revise previous thoughts\n- You can add more thoughts even after reaching what seemed like the end\n- You can express uncertainty and explore alternative approaches\n- Not every thought needs to build linearly - you can branch or backtrack\n- Generates a solution hypothesis\n- Verifies the hypothesis based on the Chain of Thought steps\n- Repeats the process until satisfied\n- Provides a correct answer\n\nParameters explained:\n- thought: Your current thinking step, which can include:\n* Regular analytical steps\n* Revisions of previous thoughts\n* Questions about previous decisions\n* Realizations about needing more analysis\n* Changes in approach\n* Hypothesis generation\n* Hypothesis verification\n- next_thought_needed: True if you need more thinking, even if at what seemed like the end\n- thought_number: Current number in sequence (can go beyond initial total if needed)\n- total_thoughts: Current estimate of thoughts needed (can be adjusted up/down)\n- is_revision: A boolean indicating if this thought revises previous thinking\n- revises_thought: If is_revision is true, which thought number is being reconsidered\n- branch_from_thought: If branching, which thought number is the branching point\n- branch_id: Identifier for the current branch (if any)\n- needs_more_thoughts: If reaching end but realizing more thoughts needed\n\nYou should:\n1. Start with an initial estimate of needed thoughts, but be ready to adjust\n2. Feel free to question or revise previous thoughts\n3. Don't hesitate to add more thoughts if needed, even at the \"end\"\n4. Express uncertainty when present\n5. Mark thoughts that revise previous thinking or branch into new paths\n6. Ignore information that is irrelevant to the current step\n7. Generate a solution hypothesis when appropriate\n8. Verify the hypothesis based on the Chain of Thought steps\n9. Repeat the process until satisfied with the solution\n10. Provide a single, ideally correct answer as the final output\n11. Only set next_thought_needed to false when truly done and a satisfactory answer is reached`\n\nArguments:\n\n- `thought` (String) (Required): Your current thinking step\n- `nextThoughtNeeded` (Boolean) (Required): Whether another thought step is needed\n- `thoughtNumber` (Number) (Required): Current thought number\n- `totalThoughts` (Number) (Required): Estimated total thoughts needed\n- `isRevision` (Boolean): Whether this revises previous thinking\n- `revisesThought` (Number): Which thought is being reconsidered\n- `branchFromThought` (Number): Branching point thought number\n- `branchId` (String): Branch identifier\n- `needsMoreThoughts` (Boolean): If more thoughts are needed\n\n### tool_manager\n\nManage MCP tools - enable or disable tools\n\nArguments:\n\n- `action` (String) (Required): Action to perform: list, enable, disable\n- `tool_name` (String): Tool name to enable/disable\n\n### tool_use_plan\n\nTạo kế hoạch sử dụng các công cụ đang kích hoạt để giải quyết yêu cầu\n\nArguments:\n\n- `request` (String) (Required): Yêu cầu cần lập kế hoạch\n- `context` (String) (Required): Ngữ cảnh liên quan đến yêu cầu\n\n### youtube_transcript\n\nGet YouTube video transcript\n\nArguments:\n\n- `video_id` (String) (Required): YouTube video ID\n\n### youtube_update_video\n\nUpdate a video's title and description on YouTube\n\nArguments:\n\n- `video_id` (String) (Required): ID of the video to update\n- `title` (String) (Required): New title of the video\n- `description` (String) (Required): New description of the video\n- `keywords` (String) (Required): Comma-separated list of keywords for the video\n- `category` (String) (Required): Category ID for the video. See https://developers.google.com/youtube/v3/docs/videoCategories/list for more information.\n\n### youtube_get_video_details\n\nGet details (title, description, ...) for a specific video\n\nArguments:\n\n- `video_id` (String) (Required): ID of the video\n\n### youtube_list_videos\n\nList YouTube videos managed by the user\n\nArguments:\n\n- `channel_id` (String) (Required): ID of the channel to list videos for\n- `max_results` (Number) (Required): Maximum number of videos to return\n\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "workflows",
        "tools",
        "development",
        "research tools",
        "development tools",
        "development workflows"
      ],
      "category": "scientific-research-tools"
    },
    "noahzeidenberg--ncbi-mcp": {
      "owner": "noahzeidenberg",
      "name": "ncbi-mcp",
      "url": "https://github.com/noahzeidenberg/ncbi-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/noahzeidenberg.webp",
      "description": "Query and analyze NCBI Entrez databases, including PubMed, Gene, and Protein, to retrieve detailed gene information and summaries. Facilitate exploration of gene relationships and integrate with bioinformatics workflows.",
      "stars": 3,
      "forks": 0,
      "license": "Apache License 2.0",
      "language": "Python",
      "updated_at": "2025-09-24T16:47:25Z",
      "readme_content": "# NCBI Model Context Protocol (MCP)\n\nA Python implementation of the Model Context Protocol for interacting with NCBI databases.\n\n## Setup\n\n1. Clone this repository\n2. Install dependencies:\n   ```\n   pip install -r requirements.txt\n   ```\n3. Create a `.env` file with your NCBI API key:\n   ```\n   NCBI_API_KEY=your_api_key_here\n   NCBI_EMAIL=your_email@example.com\n   ```\n\n## Running the MCP Server\n\n```\npython ncbi_mcp.py\n```\n\n## Using with Cursor/Claude\n\nOnce the MCP server is running, you can interact with it using natural language in Cursor/Claude.\n\n### Using Natural Language Queries\n\nYou can use natural language to perform searches and retrieve information:\n\n```\ntools/call\n{\n  \"name\": \"nlp-query\",\n  \"arguments\": {\n    \"query\": \"Find research articles about BRCA1\"\n  }\n}\n```\n\nOr more simply, just use the query directly:\n\n```\n@ncbi-mcp Find research articles about BRCA1\n```\n\n### Example Natural Language Queries\n\nHere are some example natural language queries you can try:\n\n1. Gene function information:\n   ```\n   @ncbi-mcp Please summarize the function of TNF-alpha\n   ```\n\n2. Genome size and statistics:\n   ```\n   @ncbi-mcp How big is the genome for Saccharomyces cerevisiae?\n   ```\n\n3. Assembly statistics:\n   ```\n   @ncbi-mcp What is the reported L50 and N50 statistics for the most recent E.coli genome?\n   ```\n\n4. Dataset counts:\n   ```\n   @ncbi-mcp How many datasets are available in the biosample database for b16f10 mouse melanoma cells?\n   ```\n\n5. Search for scientific articles:\n   ```\n   @ncbi-mcp Find the latest research on COVID-19 vaccines\n   ```\n\n6. Get gene information:\n   ```\n   @ncbi-mcp Tell me about the BRCA1 gene\n   ```\n\n7. Fetch genome information:\n   ```\n   @ncbi-mcp Get genome information for Homo sapiens\n   ```\n\n## Testing\n\nTo test the MCP server with various queries, you can use the included test files:\n\n```\n# Test natural language query functionality (default)\n.\\run_test.bat\n\n# Test all tools\n.\\run_test.bat all\n\n# Test specific test file\n.\\run_test.bat test_all_tools.jsonl\n\n# Test high-level tools\n.\\run_test.bat test_high_level_tools.jsonl\n```\n\nThe test script will:\n1. Start the MCP server in background\n2. Send test requests from the specified file\n3. Wait for a few seconds to allow processing\n4. Terminate the server and display the output\n\nThis approach is used because the MCP server is designed to run continuously as a service. For manual testing without automatic termination, you can use:\n\n```\n# Run manually with any test file\ntype test_nlp_query.jsonl | python ncbi_mcp.py\n```\n\nThe test files contain example JSON-RPC requests that simulate how Cursor/Claude would interact with the MCP server.\n\n## Available Tools\n\nThe NCBI MCP provides both high-level tools that understand natural language and low-level tools for direct database interaction.\n\n## Tool Usage Guidelines for LLMs\n\n### Recommended Workflow Patterns\n\n**For most biological queries, start with `nlp-query`** - it's the most intelligent tool that can handle complex questions and automatically route to appropriate specialized tools.\n\n**Common Research Workflows:**\n\n1. **Gene Analysis Workflow:**\n   - Start with `nlp-query` for general gene questions\n   - Use `summarize-gene` for comprehensive gene information\n   - Use `get_gene_info` for detailed structured data\n   - Use `ncbi-search` + `ncbi-fetch` for specific database queries\n\n2. **Genome Analysis Workflow:**\n   - Use `genome-stats` for organism genome statistics\n   - Use `get_genome_info` for detailed genome metadata\n   - Use `count-datasets` to explore available genome assemblies\n\n3. **Literature Research Workflow:**\n   - Use `nlp-query` for natural language literature searches\n   - Use `ncbi-search` with database=\"pubmed\" for precise searches\n   - Use `ncbi-fetch` to get full publication details\n\n4. **Dataset Discovery Workflow:**\n   - Use `count-datasets` to assess data availability\n   - Use `nlp-query` to explore datasets with natural language\n   - Use `ncbi-search` for systematic database exploration\n\n5. **E-utilities Workflow (Advanced):**\n   - Use `ncbi-info` to discover available databases\n   - Use `ncbi-global-query` to see which databases contain your search term\n   - Use `ncbi-search` to find specific UIDs in target databases\n   - Use `ncbi-summary` to get overview information about records\n   - Use `ncbi-fetch` to retrieve complete records\n   - Use `ncbi-link` to find related records across databases\n\n6. **Cross-Database Analysis Workflow:**\n   - Use `ncbi-search` to find genes of interest\n   - Use `ncbi-link` to find related proteins, structures, or literature\n   - Use `ncbi-summary` to get metadata about related records\n   - Use `ncbi-fetch` to retrieve detailed information\n\n### Tool Selection Guide\n\n**High-Level Tools (Recommended for most users):**\n- **`nlp-query`**: Use for general biological questions, complex queries, and when you're unsure which tool to use\n- **`summarize-gene`**: Use for comprehensive gene analysis and understanding gene function\n- **`genome-stats`**: Use for genome size, assembly quality, and organism comparison\n- **`count-datasets`**: Use for research planning and data availability assessment\n- **`get_gene_info`**: Use for detailed, structured gene information\n- **`get_genome_info`**: Use for detailed, structured genome information\n\n**Low-Level E-utilities Tools (For advanced users):**\n- **`ncbi-search` (ESearch)**: Use for precise database searches with specific filters, Boolean operators, and field qualifiers\n- **`ncbi-fetch` (EFetch)**: Use to retrieve complete records after searching, supports multiple formats (GenBank, FASTA, XML)\n- **`ncbi-summary` (ESummary)**: Use to get document summaries without fetching complete records\n- **`ncbi-link` (ELink)**: Use to find related records across databases (e.g., gene to protein, protein to structure)\n- **`ncbi-info` (EInfo)**: Use to discover available databases and their capabilities\n- **`ncbi-global-query` (EGQuery)**: Use to search across all databases simultaneously\n- **`ncbi-spell` (ESpell)**: Use to get spelling suggestions for search terms\n- **`ncbi-citation-match` (ECitMatch)**: Use to find PMIDs from citation information\n\n### Biological Context and Terminology\n\n**Understanding NCBI Databases:**\n- **Gene**: Contains gene records with symbols, names, functions, and genomic locations\n- **Protein**: Contains protein sequences and annotations\n- **Nucleotide**: Contains DNA/RNA sequences (genes, transcripts, genomic regions)\n- **PubMed**: Contains scientific literature and publications\n- **BioSample**: Contains biological sample metadata (tissues, cell lines, etc.)\n- **BioProject**: Contains research project information\n- **SRA**: Contains raw sequencing data\n- **Assembly**: Contains genome assembly information\n\n**Common Biological Terms:**\n- **Gene Symbol**: Short abbreviation (e.g., BRCA1, TP53, TNF)\n- **Gene ID**: Unique NCBI identifier (e.g., 672 for BRCA1)\n- **Accession**: Unique sequence identifier (e.g., NM_001126114.3)\n- **N50/L50**: Assembly quality metrics (larger N50 = better assembly)\n- **Reference Genome**: High-quality representative genome for a species\n- **Organism**: Use scientific names (Homo sapiens) or common names (human)\n\n**Search Strategies:**\n- Use specific gene symbols for precise results\n- Include organism names to avoid ambiguity\n- Use Boolean operators (AND, OR, NOT) for complex searches\n- Use field qualifiers like [Gene], [Organism], [Protein Name] for targeted searches\n\n### High-Level Tools\n\n#### Natural Language Query Processor\n\n```\ntools/call\n{\n  \"name\": \"nlp-query\",\n  \"arguments\": {\n    \"query\": \"Please summarize the function of TNF-alpha\"\n  }\n}\n```\n\n#### Gene Summarizer\n\n```\ntools/call\n{\n  \"name\": \"summarize-gene\",\n  \"arguments\": {\n    \"gene_name\": \"BRCA1\"\n  }\n}\n```\n\n#### Genome Statistics\n\n```\ntools/call\n{\n  \"name\": \"genome-stats\",\n  \"arguments\": {\n    \"organism\": \"Escherichia coli\"\n  }\n}\n```\n\n#### Dataset Counter\n\n```\ntools/call\n{\n  \"name\": \"count-datasets\",\n  \"arguments\": {\n    \"database\": \"biosample\",\n    \"query\": \"mouse melanoma b16f10\"\n  }\n}\n```\n\n### Low-Level Tools\n\n#### Search NCBI Databases\n\n```\ntools/call\n{\n  \"name\": \"ncbi-search\",\n  \"arguments\": {\n    \"database\": \"pubmed\",\n    \"term\": \"BRCA1\",\n    \"filters\": {\n      \"organism\": \"Homo sapiens\",\n      \"date_range\": {\n        \"start\": \"2020\"\n      }\n    }\n  }\n}\n```\n\n#### Fetch NCBI Records\n\n```\ntools/call\n{\n  \"name\": \"ncbi-fetch\",\n  \"arguments\": {\n    \"database\": \"gene\",\n    \"ids\": [\"70\"],\n    \"rettype\": \"gb\"\n  }\n}\n```\n\n#### Get Gene Information\n\n```\ntools/call\n{\n  \"name\": \"get_gene_info\",\n  \"arguments\": {\n    \"gene_id\": \"672\"\n  }\n}\n```\n\n#### Get Genome Information\n\n```\ntools/call\n{\n  \"name\": \"get_genome_info\",\n  \"arguments\": {\n    \"organism\": \"Homo sapiens\",\n    \"reference\": true\n  }\n}\n```\n\n## License\n\nApache-2.0 ",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "bioinformatics",
        "ncbi",
        "gene",
        "noahzeidenberg ncbi",
        "analyze ncbi",
        "integrate bioinformatics"
      ],
      "category": "scientific-research-tools"
    },
    "olaservo--shannon-thinking": {
      "owner": "olaservo",
      "name": "shannon-thinking",
      "url": "https://github.com/olaservo/shannon-thinking",
      "imageUrl": "/freedevtools/mcp/pfp/olaservo.webp",
      "description": "Guides complex problem-solving by breaking down challenges into structured thoughts using a systematic methodology. Supports iterative revisions and empirical validation for engineering design and optimization tasks.",
      "stars": 43,
      "forks": 7,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-10-03T21:33:51Z",
      "readme_content": "# shannon-thinking\n\nAn MCP server demonstrating Claude Shannon's systematic problem-solving methodology. This server provides a tool that helps break down complex problems into structured thoughts following Shannon's approach of problem definition, mathematical modeling, and practical implementation.\n\n<a href=\"https://glama.ai/mcp/servers/iffffhwqqw\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/iffffhwqqw/badge\" alt=\"Shannon Thinking Server MCP server\" />\n</a>\n\n## Overview\n\nClaude Shannon, known as the father of information theory, approached complex problems through a systematic methodology:\n\n1. **Problem Definition**: Strip the problem to its fundamental elements\n2. **Constraints**: Identify system limitations and boundaries\n3. **Model**: Develop mathematical/theoretical frameworks\n4. **Proof/Validation**: Validate through formal proofs or experimental testing\n5. **Implementation/Experiment**: Design and test practical solutions\n\nThis MCP server demonstrates this methodology as a tool that helps guide systematic problem-solving through these stages.\n\n## Installation\n\n### NPX\n\n```json\n{\n  \"mcpServers\": {\n    \"shannon-thinking\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"server-shannon-thinking@latest\"\n      ]\n    }\n  }\n}\n```\n\n## Usage\n\nThe server provides a single tool named `shannonthinking` that structures problem-solving thoughts according to Shannon's methodology.\n\nEach thought must include:\n- The actual thought content\n- Type (problem_definition/constraints/model/proof/implementation)\n- Thought number and total thoughts estimate\n- Confidence level (uncertainty: 0-1)\n- Dependencies on previous thoughts\n- Explicit assumptions\n- Whether another thought step is needed\n\nAdditional capabilities:\n- **Revision**: Thoughts can revise earlier steps as understanding evolves\n- **Recheck**: Mark steps that need re-examination with new information\n- **Experimental Validation**: Support for empirical testing alongside formal proofs\n- **Implementation Notes**: Practical constraints and proposed solutions\n\n### Example Usage\n\n```typescript\nconst thought = {\n  thought: \"The core problem can be defined as an information flow optimization\",\n  thoughtType: \"problem_definition\",\n  thoughtNumber: 1,\n  totalThoughts: 5,\n  uncertainty: 0.2,\n  dependencies: [],\n  assumptions: [\"System has finite capacity\", \"Information flow is continuous\"],\n  nextThoughtNeeded: true,\n  // Optional: Mark as revision of earlier definition\n  isRevision: false,\n  // Optional: Indicate step needs recheck\n  recheckStep: {\n    stepToRecheck: \"constraints\",\n    reason: \"New capacity limitations discovered\",\n    newInformation: \"System shows non-linear scaling\"\n  }\n};\n\n// Use with MCP client\nconst result = await client.callTool(\"shannonthinking\", thought);\n```\n\n## Features\n\n- **Iterative Problem-Solving**: Supports revisions and rechecks as understanding evolves\n- **Flexible Validation**: Combines formal proofs with experimental validation\n- **Dependency Tracking**: Explicitly tracks how thoughts build upon previous ones\n- **Assumption Management**: Requires clear documentation of assumptions\n- **Confidence Levels**: Quantifies uncertainty in each step\n- **Rich Feedback**: Formatted console output with color-coding, symbols, and validation results\n\n## Development\n\n```bash\n# Install dependencies\nnpm install\n\n# Build\nnpm run build\n\n# Run tests\nnpm test\n\n# Watch mode during development\nnpm run watch\n```\n\n## Tool Schema\n\nThe tool accepts thoughts with the following structure:\n\n```typescript\ninterface ShannonThought {\n  thought: string;\n  thoughtType: \"problem_definition\" | \"constraints\" | \"model\" | \"proof\" | \"implementation\";\n  thoughtNumber: number;\n  totalThoughts: number;\n  uncertainty: number; // 0-1\n  dependencies: number[];\n  assumptions: string[];\n  nextThoughtNeeded: boolean;\n  \n  // Optional revision fields\n  isRevision?: boolean;\n  revisesThought?: number;\n  \n  // Optional recheck field\n  recheckStep?: {\n    stepToRecheck: ThoughtType;\n    reason: string;\n    newInformation?: string;\n  };\n  \n  // Optional validation fields\n  proofElements?: {\n    hypothesis: string;\n    validation: string;\n  };\n  experimentalElements?: {\n    testDescription: string;\n    results: string;\n    confidence: number; // 0-1\n    limitations: string[];\n  };\n  \n  // Optional implementation fields\n  implementationNotes?: {\n    practicalConstraints: string[];\n    proposedSolution: string;\n  };\n}\n```\n\n## When to Use\n\nThis thinking pattern is particularly valuable for:\n- Complex system analysis\n- Information processing problems\n- Engineering design challenges\n- Problems requiring theoretical frameworks\n- Optimization problems\n- Systems requiring practical implementation\n- Problems that need iterative refinement\n- Cases where experimental validation complements theory\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "thinking",
        "methodology",
        "challenges",
        "thinking guides",
        "challenges structured",
        "structured thoughts"
      ],
      "category": "scientific-research-tools"
    },
    "openags--paper-search-mcp": {
      "owner": "openags",
      "name": "paper-search-mcp",
      "url": "https://github.com/openags/paper-search-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/openags.webp",
      "description": "Search and download academic papers from various sources such as arXiv, PubMed, bioRxiv, and Sci-Hub. Integrates with large language models to streamline access to scholarly articles and enhance research workflows.",
      "stars": 368,
      "forks": 57,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-10-03T06:37:54Z",
      "readme_content": "# Paper Search MCP\n\nA Model Context Protocol (MCP) server for searching and downloading academic papers from multiple sources, including arXiv, PubMed, bioRxiv, and Sci-Hub (optional). Designed for seamless integration with large language models like Claude Desktop.\n\n![PyPI](https://img.shields.io/pypi/v/paper-search-mcp.svg) ![License](https://img.shields.io/badge/license-MIT-blue.svg) ![Python](https://img.shields.io/badge/python-3.10+-blue.svg)\n[![smithery badge](https://smithery.ai/badge/@openags/paper-search-mcp)](https://smithery.ai/server/@openags/paper-search-mcp)\n\n---\n\n## Table of Contents\n\n- [Overview](#overview)\n- [Features](#features)\n- [Installation](#installation)\n  - [Quick Start](#quick-start)\n    - [Install Package](#install-package)\n    - [Configure Claude Desktop](#configure-claude-desktop)\n  - [For Development](#for-development)\n    - [Setup Environment](#setup-environment)\n    - [Install Dependencies](#install-dependencies)\n- [Contributing](#contributing)\n- [Demo](#demo)\n- [License](#license)\n- [TODO](#todo)\n\n---\n\n## Overview\n\n`paper-search-mcp` is a Python-based MCP server that enables users to search and download academic papers from various platforms. It provides tools for searching papers (e.g., `search_arxiv`) and downloading PDFs (e.g., `download_arxiv`), making it ideal for researchers and AI-driven workflows. Built with the MCP Python SDK, it integrates seamlessly with LLM clients like Claude Desktop.\n\n---\n\n## Features\n\n- **Multi-Source Support**: Search and download papers from arXiv, PubMed, bioRxiv, medRxiv, Google Scholar, IACR ePrint Archive, Semantic Scholar.\n- **Standardized Output**: Papers are returned in a consistent dictionary format via the `Paper` class.\n- **Asynchronous Tools**: Efficiently handles network requests using `httpx`.\n- **MCP Integration**: Compatible with MCP clients for LLM context enhancement.\n- **Extensible Design**: Easily add new academic platforms by extending the `academic_platforms` module.\n\n---\n\n## Installation\n\n`paper-search-mcp` can be installed using `uv` or `pip`. Below are two approaches: a quick start for immediate use and a detailed setup for development.\n\n### Installing via Smithery\n\nTo install paper-search-mcp for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@openags/paper-search-mcp):\n\n```bash\nnpx -y @smithery/cli install @openags/paper-search-mcp --client claude\n```\n\n### Quick Start\n\nFor users who want to quickly run the server:\n\n1. **Install Package**:\n\n   ```bash\n   uv add paper-search-mcp\n   ```\n\n2. **Configure Claude Desktop**:\n   Add this configuration to `~/Library/Application Support/Claude/claude_desktop_config.json` (Mac) or `%APPDATA%\\Claude\\claude_desktop_config.json` (Windows):\n   ```json\n   {\n     \"mcpServers\": {\n       \"paper_search_server\": {\n         \"command\": \"uv\",\n         \"args\": [\n           \"run\",\n           \"--directory\",\n           \"/path/to/your/paper-search-mcp\",\n           \"-m\",\n           \"paper_search_mcp.server\"\n         ],\n         \"env\": {\n           \"SEMANTIC_SCHOLAR_API_KEY\": \"\" // Optional: For enhanced Semantic Scholar features\n         }\n       }\n     }\n   }\n   ```\n   > Note: Replace `/path/to/your/paper-search-mcp` with your actual installation path.\n\n### For Development\n\nFor developers who want to modify the code or contribute:\n\n1. **Setup Environment**:\n\n   ```bash\n   # Install uv if not installed\n   curl -LsSf https://astral.sh/uv/install.sh | sh\n\n   # Clone repository\n   git clone https://github.com/openags/paper-search-mcp.git\n   cd paper-search-mcp\n\n   # Create and activate virtual environment\n   uv venv\n   source .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\n   ```\n\n2. **Install Dependencies**:\n\n   ```bash\n   # Install project in editable mode\n   uv add -e .\n\n   # Add development dependencies (optional)\n   uv add pytest flake8\n   ```\n\n---\n\n## Contributing\n\nWe welcome contributions! Here's how to get started:\n\n1. **Fork the Repository**:\n   Click \"Fork\" on GitHub.\n\n2. **Clone and Set Up**:\n\n   ```bash\n   git clone https://github.com/yourusername/paper-search-mcp.git\n   cd paper-search-mcp\n   pip install -e \".[dev]\"  # Install dev dependencies (if added to pyproject.toml)\n   ```\n\n3. **Make Changes**:\n\n   - Add new platforms in `academic_platforms/`.\n   - Update tests in `tests/`.\n\n4. **Submit a Pull Request**:\n   Push changes and create a PR on GitHub.\n\n---\n\n## Demo\n\n\n\n## TODO\n\n### Planned Academic Platforms\n\n- [√] arXiv\n- [√] PubMed\n- [√] bioRxiv\n- [√] medRxiv\n- [√] Google Scholar\n- [√] IACR ePrint Archive\n- [√] Semantic Scholar\n- [ ] PubMed Central (PMC)\n- [ ] Science Direct\n- [ ] Springer Link\n- [ ] IEEE Xplore\n- [ ] ACM Digital Library\n- [ ] Web of Science\n- [ ] Scopus\n- [ ] JSTOR\n- [ ] ResearchGate\n- [ ] CORE\n- [ ] Microsoft Academic\n\n---\n\n## License\n\nThis project is licensed under the MIT License. See the LICENSE file for details.\n\n---\n\nHappy researching with `paper-search-mcp`! If you encounter issues, open a GitHub issue.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "scholarly",
        "pubmed",
        "research",
        "research tools",
        "scholarly articles",
        "access scholarly"
      ],
      "category": "scientific-research-tools"
    },
    "opensensor--bn_cline_mcp": {
      "owner": "opensensor",
      "name": "bn_cline_mcp",
      "url": "https://github.com/opensensor/bn_cline_mcp",
      "imageUrl": "/freedevtools/mcp/pfp/opensensor.webp",
      "description": "Analyze binaries efficiently using Binary Ninja's powerful analysis capabilities through the MCP framework. Connect to Cline for enhanced functionality and seamless integration.",
      "stars": 4,
      "forks": 1,
      "license": "GNU General Public License v3.0",
      "language": "Python",
      "updated_at": "2025-09-29T15:37:18Z",
      "readme_content": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/opensensor-bn-cline-mcp-badge.png)](https://mseep.ai/app/opensensor-bn-cline-mcp)\n\n# binary_ninja_cline_mcp\nAn MCP server for Cline that works with Binary Ninja (Personal License)\n\nThis repository contains an MCP server that allows Cline to analyze binaries using Binary Ninja.\nNote:  Not all files will be used, there is also prototype of using headless Binary Ninja but my \nlicense is Personal so I can't test it.\n\n## Setup\n\n1. Install the latest of Binary Ninja MCP Plugin https://github.com/fosdickio/binary_ninja_mcp\n2. Open your binary and start the MCP server from within Binary Ninja.\n3. Open a terminal and run python binary_ninja_mcp_http_server.py --port 8088\n4. Open another terminal and run `npm start`\n5. Open Cline and add the following tool:{\nExample:\n```\n{\n  \"mcpServers\": {\n    \"BN MCP\": {\n      \"command\": \"node\",\n      \"args\": [\"/home/matteius/binary_ninja_cline/bn_cline_mcp/binaryninja-mcp-bridge.js\"],\n      \"env\": {\n        \"BN_HTTP_SERVER\": \"http://localhost:8088\"\n      },\n      \"autoApprove\": [],\n      \"disabled\": false,\n      \"timeout\": 30\n    }\n  }\n}\n\n```",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "bn_cline_mcp",
        "binaries",
        "opensensor",
        "bn_cline_mcp analyze",
        "opensensor bn_cline_mcp",
        "analyze binaries"
      ],
      "category": "scientific-research-tools"
    },
    "prashalruchiranga--arxiv-mcp-server": {
      "owner": "prashalruchiranga",
      "name": "arxiv-mcp-server",
      "url": "https://github.com/prashalruchiranga/arxiv-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/prashalruchiranga.webp",
      "description": "Interact with the arXiv API to retrieve scholarly article metadata, download PDFs, and search for articles using natural language queries. Enhance research workflows by directly accessing arXiv content within a large language model environment.",
      "stars": 31,
      "forks": 6,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-10-03T07:45:53Z",
      "readme_content": "# arXiv MCP Server\n\n[![MCP Compatible](https://img.shields.io/badge/MCP-Compatible-purple.svg)](https://modelcontextprotocol.io)\n[![Python](https://img.shields.io/badge/python-3.13+-blue.svg)](https://www.python.org/downloads/)\n[![License](https://img.shields.io/badge/license-MIT-blue.svg)](https://opensource.org/licenses/MIT)\n[![smithery badge](https://smithery.ai/badge/@prashalruchiranga/arxiv-mcp-server)](https://smithery.ai/server/@prashalruchiranga/arxiv-mcp-server)\n\nA Model Context Protocol (MCP) server that enables interacting with the arXiv API using natural language.\n\n## Features\n- Retrieve metadata about scholarly articles hosted on arXiv.org\n- Download articles in PDF format to the local machine\n- Search arXiv database for a particular query\n- Retrieve articles and load them into a large language model (LLM) context\n\n## Tools\n- **get_article_url**\n    - Retrieve the URL of an article hosted on arXiv.org based on its title\n        - `title` (String): Article title\n- **download_article**\n    - Download the article hosted on arXiv.org as a PDF file \n        - `title` (String): Article title\n- **load_article_to_context**\n    - Load the article hosted on arXiv.org into context of a LLM \n        - `title` (String): Article title\n- **get_details**\n    - Retrieve metadata of an article hosted on arXiv.org based on its title\n        - `title` (String): Article title\n- **search_arxiv**\n    - Performs a search query on the arXiv API based on specified parameters and returns matching article metadata\n        - `all_fields` (String): General keyword search across all metadata fields\n        - `title` (String): Keyword(s) to search for within the titles of articles\n        - `author` (String): Author name(s) to filter results by\n        - `abstract` (String): Keyword(s) to search for within article abstracts\n        - `start` (int): Index of the first result to return\n\n## Setup\n\n### MacOS\n\nClone the repository\n```\ngit clone https://github.com/prashalruchiranga/arxiv-mcp-server.git\ncd arxiv-mcp-server\n```\nInstall `uv` package manager. For more details on installing, visit the [official uv documentation](https://docs.astral.sh/uv/getting-started/installation/).\n```\n# Using Homebrew\nbrew install uv\n\n# or\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n```\n\nCreate and activate virtual environment.\n```\nuv venv --python=python3.13\nsource .venv/bin/activate\n```\n\nInstall development dependencies.\n```\nuv sync\n```\n\n### Windows\n\nInstall `uv` package manager. For more details on installing, visit the [official uv documentation](https://docs.astral.sh/uv/getting-started/installation/).\n```\n# Use irm to download the script and execute it with iex\npowershell -ExecutionPolicy ByPass -c \"irm https://astral.sh/uv/install.ps1 | iex\"\n```\nClose and reopen the shell, then clone the repository.\n```\ngit clone https://github.com/prashalruchiranga/arxiv-mcp-server.git\ncd arxiv-mcp-server\n```\n\nCreate and activate virtual environment.\n```\nuv venv --python=python3.13\nsource .venv\\Scripts\\activate\n```\n\nInstall development dependencies.\n```\nuv sync\n```\n\n## Usage with Claude Desktop\nTo enable this integration, add the server configuration to your `claude_desktop_config.json` file. Make sure to create the file if it doesn’t exist.\n\nOn MacOS: `~/Library/Application Support/Claude/claude_desktop_config.json` On Windows: `%APPDATA%/Roaming/Claude/claude_desktop_config.json`\n\n```\n{\n  \"mcpServers\": {\n    \"arxiv-server\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"--directory\",\n        \"/ABSOLUTE/PATH/TO/PARENT/FOLDER/arxiv-mcp-server/src/arxiv_server\",\n        \"run\",\n        \"server.py\"\n      ],\n      \"env\": {\n        \"DOWNLOAD_PATH\": \"/ABSOLUTE/PATH/TO/DOWNLOADS/FOLDER\"\n      }\n    }\n  }\n}\n```\n\nYou may need to put the full path to the uv executable in the command field. You can get this by running `which uv` on MacOS or `where uv` on Windows.\n\n## Example Prompts\n```\nCan you get the details of 'Reasoning to Learn from Latent Thoughts' paper?\n```\n```\nGet the papers authored or co-authored by Yann Lecun on convolutional neural networks\n```\n```\nDownload the attention is all you need paper\n```\n```\nCan you get the papers by Andrew NG which have 'convolutional neural networks' in title?\n```\n```\nCan you display the paper?\n```\n```\nList the titles of papers by Yann LeCun. Paginate through the API until there are 30 titles\n```\n\n## License\n\nLicensed under MIT. See the [LICENSE](https://github.com/prashalruchiranga/arxiv-mcp-server/blob/main/LICENSE).\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "scholarly",
        "metadata",
        "arxiv",
        "retrieve scholarly",
        "research tools",
        "scholarly article"
      ],
      "category": "scientific-research-tools"
    },
    "r-huijts--oorlogsbronnen-mcp": {
      "owner": "r-huijts",
      "name": "oorlogsbronnen-mcp",
      "url": "https://github.com/r-huijts/oorlogsbronnen-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/r-huijts.webp",
      "description": "Enables AI-powered access to the Oorlogsbronnen database for natural language queries related to historical World War II archives from the Netherlands.",
      "stars": 11,
      "forks": 4,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-07-22T13:29:18Z",
      "readme_content": "# Oorlogsbronnen MCP Server\n\nA Model Context Protocol (MCP) server that provides AI-powered access to the Oorlogsbronnen (War Sources) database. This server enables natural language interactions with historical World War II archives from the Netherlands.\n\n## Natural Language Interaction Examples\n\nAsk your AI assistant questions like these to explore Dutch WWII history:\n\n- **\"What happened during the bombing of Rotterdam in May 1940?\"**\n- **\"Tell me about Anne Frank's life in hiding based on historical records.\"**\n- **\"Show me photographs of the Dutch Hunger Winter of 1944-1945.\"**\n- **\"Were any of my ancestors imprisoned in Camp Vught during the war?\"**\n- **\"I'm visiting Arnhem next week. What historical sites related to Operation Market Garden should I see?\"**\n- **\"Find information about resistance activities in Utrecht during the Nazi occupation.\"**\n- **\"What was daily life like for Jewish families in Amsterdam before deportations began?\"**\n- **\"Show me firsthand accounts from people who witnessed the liberation of the Netherlands in 1945.\"**\n- **\"What records exist about children who were hidden by Dutch families during the war?\"**\n- **\"I'm researching the impact of WWII on Dutch infrastructure. Can you find documents about the reconstruction of bridges and railways?\"**\n\n## Features\n\n- 🔍 Natural language search across the Oorlogsbronnen database\n- 🏷️ Filter results by content type (person, photo, article, etc.)\n- 📊 Control the number of results returned\n- 🤖 AI-friendly JSON responses for further processing\n\n## Installation\n\nYou can install this server in two ways:\n\n### 1. Using Claude Desktop with NPX Package\n\nUpdate your Claude configuration file (`~/Library/Application Support/Claude/claude_desktop_config.json`):\n\n```json\n{\n  \"mcpServers\": {\n    \"oorlogsbronnen-server\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"oorlogsbronnen-mcp\"\n      ]\n    }\n  }\n}\n```\n\nAfter updating the configuration, restart Claude Desktop for the changes to take effect.\n\n### 2. From Source\n\n1. Clone this repository:\n```bash\ngit clone https://github.com/r-huijts/oorlogsbronnen-mcp.git\ncd oorlogsbronnen-mcp\n```\n\n2. Install dependencies:\n```bash\nnpm install\n```\n\n3. Build the project:\n```bash\nnpm run build\n```\n\n4. Configure Claude Desktop by updating your configuration file (located at `~/Library/Application Support/Claude/claude_desktop_config.json`):\n\n```json\n{\n  \"mcpServers\": {\n    \"oorlogsbronnen-server\": {\n      \"command\": \"node\",\n      \"args\": [\n        \"/absolute/path/to/oorlogsbronnen-mcp/dist/mcp-server.js\"\n      ]\n    }\n  }\n}\n```\n\nReplace `/absolute/path/to/oorlogsbronnen-mcp` with the actual path to your installation.\n\n## Usage Examples\n\nThe MCP server understands natural language queries and can help you explore World War II archives. Here are some example queries you can use with Claude:\n\n### Basic Searches\n\n- \"Use search_ww2_nl_archives to find documents about the resistance movement in Amsterdam\"\n- \"Search the Dutch WW2 archives for information about Jewish refugees in 1942\"\n- \"Look through the Netherlands war archives for records of Allied bombing raids\"\n\n### Filtering by Type\n\n- \"Use search_ww2_nl_archives to show me photographs of the liberation of Rotterdam\"\n- \"Find personal accounts in the Dutch WW2 archives about life in concentration camps\"\n- \"Search the Netherlands war archives for newspaper articles about food shortages\"\n\n### Specific Queries\n\n- \"Search the Dutch WW2 archives for documents about Anne Frank's time in Amsterdam\"\n- \"Use search_ww2_nl_archives to find records of the February Strike of 1941\"\n- \"Look through the Netherlands war archives for information about Operation Market Garden\"\n\n### Research Examples\n\n1. **Personal History Research**:\n   ```\n   Use search_ww2_nl_archives to find any records or documents about the Rosenberg family in Amsterdam between 1940-1945\n   ```\n\n2. **Local History**:\n   ```\n   Search the Dutch WW2 archives for photographs and documents about daily life in Utrecht during the occupation\n   ```\n\n3. **Military Operations**:\n   ```\n   Use search_ww2_nl_archives to find firsthand accounts and official reports about the Battle of the Scheldt\n   ```\n\n### Advanced Usage\n\nYou can combine different search criteria:\n```\nSearch the Netherlands WW2 archives for photographs and personal accounts of the Dutch famine in 1944-1945, limit to 20 results\n```\n\n## API Reference\n\nThe server exposes the following MCP tool:\n\n### search_ww2_nl_archives\n\nA powerful search tool designed to query the Oorlogsbronnen (War Sources) database for World War II related content in the Netherlands. This tool can be used to find historical documents, photographs, personal accounts, and other archival materials from 1940-1945.\n\n**When to use this tool:**\n- Searching for specific historical events during WWII in the Netherlands\n- Finding information about people, places, or organizations during the war\n- Locating photographs or documents from specific time periods or locations\n- Researching personal or family history related to WWII\n- Finding primary sources about the Dutch resistance, occupation, or liberation\n- Discovering materials about Jewish life and persecution during the war\n- Researching military operations that took place in the Netherlands\n\nParameters:\n- `query` (required): \n  - Type: string\n  - Description: The main search term or phrase to look for in the archives\n  - Can include: names, places, dates, events, or descriptive terms\n  - Examples:\n    - \"Anne Frank\"\n    - \"Rotterdam bombing 1940\"\n    - \"Dutch resistance Amsterdam\"\n    - \"Jewish deportation Westerbork\"\n    - \"Operation Market Garden\"\n\n- `type` (optional):\n  - Type: string\n  - Description: Filter results by specific content type\n  - Available types:\n    - \"person\": Individual biographical records\n    - \"photo\": Historical photographs\n    - \"article\": News articles and written documents\n    - \"video\": Video footage\n    - \"object\": Physical artifacts and objects\n    - \"location\": Places and geographical records\n    - \"book\": Published books, memoirs, and monographs\n  - Use when: You need to focus on specific types of historical materials\n  - Default: All types included\n\n- `count` (optional):\n  - Type: number\n  - Description: Number of results to return in the response\n  - Minimum: 1\n  - Maximum: 100\n  - Default: 10\n  - Use when: You need to control the volume of returned results\n  - Note: Larger numbers will provide more comprehensive results but may take longer to process\n\n**Response Format:**\n```json\n{\n  \"results\": [\n    {\n      \"id\": string,          // Unique identifier for the record\n      \"title\": string,       // Title or name of the item\n      \"type\": string,        // Content type (person, photo, article, etc.)\n      \"description\": string, // Detailed description (if available)\n      \"url\": string         // Direct link to view the item on Oorlogsbronnen\n    }\n  ]\n}\n```\n\n**Example Queries and Their Tool Calls:**\n\n1. Basic Historical Search:\n```typescript\n{\n  query: \"February Strike 1941\",\n  type: \"article\",\n  count: 5\n}\n```\n\n2. Person Research:\n```typescript\n{\n  query: \"Rosenberg family Amsterdam Jewish\",\n  type: \"person\",\n  count: 20\n}\n```\n\n3. Photo Collection Search:\n```typescript\n{\n  query: \"liberation celebrations Amsterdam Dam Square 1945\",\n  type: \"photo\",\n  count: 15\n}\n```\n\n**Error Handling:**\n- The tool will return an error message if:\n  - The query is empty or contains invalid characters\n  - The specified type is not supported\n  - The count is outside the valid range (1-100)\n  - The API is temporarily unavailable\n  - Rate limits are exceeded\n\n**Best Practices:**\n1. Start with broader searches and narrow down with specific terms\n2. Use location names to focus on specific areas\n3. Include dates when searching for specific events\n4. Combine person names with locations for family research\n5. Use type filtering to focus on specific kinds of historical materials\n\n## Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n\n## License\n\nThis project is licensed under the MIT License - see the LICENSE file for details.\n\n## Acknowledgments\n\n- Oorlogsbronnen for providing access to their valuable historical archives\n- The Model Context Protocol (MCP) community for enabling AI-powered interactions ",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "oorlogsbronnen",
        "archives",
        "tools",
        "oorlogsbronnen database",
        "access oorlogsbronnen",
        "huijts oorlogsbronnen"
      ],
      "category": "scientific-research-tools"
    },
    "reading-plus-ai--mcp-server-deep-research": {
      "owner": "reading-plus-ai",
      "name": "mcp-server-deep-research",
      "url": "https://github.com/reading-plus-ai/mcp-server-deep-research",
      "imageUrl": "/freedevtools/mcp/pfp/reading-plus-ai.webp",
      "description": "Conduct comprehensive research on complex topics by generating structured research reports and synthesizing information from diverse sources. This tool enables in-depth exploration of questions and identification of relevant sources.",
      "stars": 181,
      "forks": 23,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-10-03T10:09:42Z",
      "readme_content": "# MCP Server for Deep Research\n\nMCP Server for Deep Research is a tool designed for conducting comprehensive research on complex topics. It helps you explore questions in depth, find relevant sources, and generate structured research reports.\n\nYour personal Research Assistant, turning research questions into comprehensive, well-cited reports.\n\n## 🚀 Try it Out\n\n[![Watch the demo](https://img.youtube.com/vi/_a7sfo5yxoI/maxresdefault.jpg)]([VIDEO_URL](https://youtu.be/_a7sfo5yxoI))\nYoutube: https://youtu.be/_a7sfo5yxoI\n\n1. **Download Claude Desktop**\n   - Get it [here](https://claude.ai/download)\n\n2. **Install and Set Up**\n   - On macOS, run the following command in your terminal:\n   ```bash\n   python setup.py\n   ```\n\n3. **Start Researching**\n   - Select the deep-research prompt template from MCP\n   - Begin your research by providing a research question\n\n## Features\n\nThe Deep Research MCP Server offers a complete research workflow:\n\n1. **Question Elaboration**\n   - Expands and clarifies your research question\n   - Identifies key terms and concepts\n   - Defines scope and parameters\n\n2. **Subquestion Generation**\n   - Creates focused subquestions that address different aspects\n   - Ensures comprehensive coverage of the main topic\n   - Provides structure for systematic research\n\n3. **Web Search Integration**\n   - Uses Claude's built-in web search capabilities\n   - Performs targeted searches for each subquestion\n   - Identifies relevant and authoritative sources\n   - Collects diverse perspectives on the topic\n\n4. **Content Analysis**\n   - Evaluates information quality and relevance\n   - Synthesizes findings from multiple sources\n   - Provides proper citations for all sources\n\n5. **Report Generation**\n   - Creates well-structured, comprehensive reports as artifacts\n   - Properly cites all sources used\n   - Presents a balanced view with evidence-based conclusions\n   - Uses appropriate formatting for clarity and readability\n\n## 📦 Components\n\n### Prompts\n- **deep-research**: Tailored for comprehensive research tasks with a structured approach\n\n## ⚙️ Modifying the Server\n\n### Claude Desktop Configurations\n- macOS: `~/Library/Application\\ Support/Claude/claude_desktop_config.json`\n- Windows: `%APPDATA%/Claude/claude_desktop_config.json`\n\n### Development (Unpublished Servers)\n```json\n\"mcpServers\": {\n  \"mcp-server-deep-research\": {\n    \"command\": \"uv\",\n    \"args\": [\n      \"--directory\",\n      \"/Users/username/repos/mcp-server-application/mcp-server-deep-research\",\n      \"run\",\n      \"mcp-server-deep-research\"\n    ]\n  }\n}\n```\n\n### Published Servers\n```json\n\"mcpServers\": {\n  \"mcp-server-deep-research\": {\n    \"command\": \"uvx\",\n    \"args\": [\n      \"mcp-server-deep-research\"\n    ]\n  }\n}\n```\n\n## 🛠️ Development\n\n### Building and Publishing\n1. **Sync Dependencies**\n   ```bash\n   uv sync\n   ```\n\n2. **Build Distributions**\n   ```bash\n   uv build\n   ```\n   Generates source and wheel distributions in the dist/ directory.\n\n3. **Publish to PyPI**\n   ```bash\n   uv publish\n   ```\n\n## 🤝 Contributing\n\nContributions are welcome! Whether you're fixing bugs, adding features, or improving documentation, your help makes this project better.\n\n## 📜 License\n\nThis project is licensed under the MIT License.\nSee the LICENSE file for details.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "research",
        "comprehensive",
        "topics",
        "deep research",
        "structured research",
        "research complex"
      ],
      "category": "scientific-research-tools"
    },
    "renatokuipers--neural-child": {
      "owner": "renatokuipers",
      "name": "neural-child",
      "url": "https://github.com/renatokuipers/neural-child",
      "imageUrl": "/freedevtools/mcp/pfp/renatokuipers.webp",
      "description": "Integrates psychological growth and emotional intelligence to develop AI systems through defined developmental stages, enhancing cognitive and emotional capabilities.",
      "stars": 6,
      "forks": 3,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-07-17T09:52:16Z",
      "readme_content": "# Neural Child Development System: A Framework for Developmental AI\n\n## Table of Contents\n\n1. [Introduction](#introduction)\n2. [Theoretical Foundations](#theoretical-foundations)\n3. [System Architecture](#system-architecture)\n4. [Developmental Stages](#developmental-stages)\n5. [Psychological Components](#psychological-components)\n6. [Memory and Learning](#memory-and-learning)\n7. [Emotional Processing](#emotional-processing)\n8. [Training Methodology](#training-methodology)\n9. [Model Performance](#model-performance)\n10. [Applications](#applications)\n11. [Technical Implementation](#technical-implementation)\n12. [Future Research Directions](#future-research-directions)\n13. [Ethics and Considerations](#ethics-and-considerations)\n14. [Getting Started](#getting-started)\n15. [Contributing](#contributing)\n\n## Introduction\n\nThe Neural Child Development System represents a groundbreaking approach to artificial intelligence that fundamentally reimagines how neural networks can learn and develop. Instead of following traditional machine learning paradigms, this system implements a sophisticated model of human psychological development, incorporating crucial aspects of cognitive, emotional, and social growth.\n\nThis project emerges from the recognition that current AI systems, while powerful in specific tasks, lack the developmental sophistication that characterizes human intelligence. By implementing a stage-based developmental framework integrated with emotional regulation, attachment theory, and psychological defense mechanisms, this system aims to create AI that develops more naturally and demonstrates genuine emotional intelligence.\n\n### Core Innovation\n\nThe system's primary innovation lies in its integration of developmental psychology with modern neural network architectures. Unlike traditional AI systems that start with full capabilities, this system begins in a \"newborn\" state and progressively develops more sophisticated abilities through interaction and learning, mirroring human developmental stages.\n\n### Key Objectives\n\nThe project addresses several fundamental challenges in AI development:\n- Creating AI systems that develop naturally through defined developmental stages\n- Implementing emotional intelligence as a core feature rather than an add-on\n- Modeling psychological defense mechanisms and trauma processing\n- Developing genuine theory of mind capabilities\n- Creating systems that can form and maintain attachment relationships\n\n## Theoretical Foundations\n\n### Developmental Psychology Integration\n\nThe system's architecture is deeply rooted in established developmental psychology theories, including:\n\n1. Piaget's Stages of Cognitive Development\n   - Sensorimotor stage\n   - Preoperational stage\n   - Concrete operational stage\n   - Formal operational stage\n\n2. Attachment Theory (Bowlby and Ainsworth)\n   - Secure attachment patterns\n   - Anxious attachment patterns\n   - Avoidant attachment patterns\n   - Disorganized attachment patterns\n\n3. Emotional Development Theory\n   - Basic emotion recognition\n   - Emotional regulation development\n   - Complex emotion understanding\n   - Social-emotional learning\n\n### Neuroscience Foundations\n\nThe architecture incorporates key principles from neuroscience:\n\n1. Neural Plasticity\n   - Critical periods of development\n   - Experience-dependent plasticity\n   - Synaptic pruning mechanisms\n\n2. Memory Systems\n   - Working memory processing\n   - Long-term potentiation\n   - Memory consolidation\n   - Emotional memory processing\n\n3. Social Brain Development\n   - Mirror neuron system implementation\n   - Social cognition networks\n   - Empathy development\n\n## System Architecture\n\n### Core Components\n\nThe system architecture consists of several interconnected neural systems:\n\n1. Sensory Processing System\n   - Multi-modal input processing\n   - Attention mechanisms\n   - Sensory integration\n   - Perceptual development\n\n2. Emotional Processing Network\n   - Basic emotion recognition\n   - Emotional state regulation\n   - Complex emotion processing\n   - Social-emotional integration\n\n3. Memory Systems\n   - Short-term memory buffer\n   - Working memory processor\n   - Long-term memory consolidation\n   - Emotional memory integration\n\n4. Psychological Components\n   - Theory of Mind network\n   - Attachment system\n   - Defense mechanism processor\n   - Self-awareness module\n\n### Neural Integration\n\nThe system employs sophisticated neural integration mechanisms:\n\n1. Cross-Component Communication\n   - Bidirectional information flow\n   - State synchronization\n   - Emotional-cognitive integration\n   - Memory-emotion binding\n\n2. Developmental Plasticity\n   - Stage-appropriate learning rates\n   - Critical period modulation\n   - Experience-dependent modification\n   - Structural adaptation\n\n## Developmental Stages\n\n### Stage Progression\n\nThe system progresses through clearly defined developmental stages:\n\n1. Newborn Stage (0-3 months)\n   - Basic sensory processing\n   - Primary emotional responses\n   - Reflexive behaviors\n   - Initial attachment formation\n\n2. Early Infancy (3-6 months)\n   - Enhanced sensory integration\n   - Social smile development\n   - Basic emotional regulation\n   - Pattern recognition\n\n3. Late Infancy (6-12 months)\n   - Object permanence\n   - Stranger anxiety\n   - Basic intentionality\n   - Enhanced memory capabilities\n\n[Stages continue through to Mature Adult]\n\n### Stage-Specific Capabilities\n\nEach developmental stage implements specific capabilities:\n\n1. Cognitive Capabilities\n   - Stage-appropriate processing\n   - Learning rate modulation\n   - Complexity handling\n   - Abstract thinking development\n\n2. Emotional Capabilities\n   - Emotion recognition scope\n   - Regulation sophistication\n   - Social-emotional understanding\n   - Empathy development\n\n3. Social Capabilities\n   - Attachment behaviors\n   - Social cognition\n   - Theory of mind\n   - Relationship formation\n\n## Psychological Components\n\n### Emotional Regulation\n\nThe emotional regulation system implements sophisticated mechanisms:\n\n1. Basic Regulation\n   - Emotion recognition\n   - State modulation\n   - Response inhibition\n   - Arousal control\n\n2. Advanced Regulation\n   - Context integration\n   - Social regulation\n   - Complex emotion processing\n   - Emotional memory integration\n\n### Defense Mechanisms\n\nThe system implements psychological defense mechanisms:\n\n1. Primary Defenses\n   - Repression\n   - Denial\n   - Projection\n   - Regression\n\n2. Mature Defenses\n   - Sublimation\n   - Humor\n   - Anticipation\n   - Altruism\n\n### Theory of Mind\n\nThe Theory of Mind implementation includes:\n\n1. Basic Components\n   - Perspective taking\n   - Intention recognition\n   - Belief modeling\n   - Desire understanding\n\n2. Advanced Components\n   - Complex mental state attribution\n   - Social prediction\n   - Multiple perspective integration\n   - Meta-representation\n\n## Memory and Learning\n\n### Memory Systems\n\nThe memory architecture implements multiple memory types:\n\n1. Short-Term Memory\n   - Rapid encoding\n   - Limited capacity\n   - Quick decay\n   - Attention-dependent processing\n\n2. Working Memory\n   - Active manipulation\n   - Information integration\n   - Temporary storage\n   - Processing capacity\n\n3. Long-Term Memory\n   - Consolidated storage\n   - Pattern recognition\n   - Semantic networks\n   - Episodic memories\n\n### Learning Mechanisms\n\nThe system employs sophisticated learning mechanisms:\n\n1. Supervised Learning\n   - Error-driven adaptation\n   - Feedback integration\n   - Performance optimization\n   - Skill acquisition\n\n2. Unsupervised Learning\n   - Pattern discovery\n   - Feature extraction\n   - Statistical learning\n   - Structure detection\n\n3. Emotional Learning\n   - Attachment-based learning\n   - Social learning\n   - Emotional memory formation\n   - Experience integration\n\n## Model Performance\n\n### Current Capabilities\n\nThe trained model demonstrates several sophisticated capabilities:\n\n1. Emotional Processing\n   - Basic emotion recognition\n   - Simple emotional regulation\n   - Attachment behavior\n   - Social response patterns\n\n2. Cognitive Processing\n   - Pattern recognition\n   - Simple problem solving\n   - Basic memory formation\n   - Early stage learning\n\n3. Social Understanding\n   - Basic theory of mind\n   - Simple intention recognition\n   - Early attachment patterns\n   - Social response generation\n\n### Benchmarks and Evaluation\n\nThe system's performance has been evaluated across multiple dimensions:\n\n1. Developmental Progression\n   - Stage-appropriate behavior\n   - Capability acquisition\n   - Learning rate\n   - Skill development\n\n2. Emotional Intelligence\n   - Emotion recognition accuracy\n   - Regulation effectiveness\n   - Social response appropriateness\n   - Attachment pattern stability\n\n3. Cognitive Development\n   - Problem-solving capability\n   - Memory formation\n   - Learning efficiency\n   - Pattern recognition accuracy\n\n## Applications\n\n### Current Applications\n\nThe system shows promise in several domains:\n\n1. Developmental Psychology Research\n   - Theory testing\n   - Development modeling\n   - Intervention testing\n   - Pattern analysis\n\n2. Educational Technology\n   - Adaptive learning systems\n   - Emotional support\n   - Developmental tracking\n   - Personalized education\n\n3. Therapeutic Applications\n   - Attachment therapy modeling\n   - Trauma response research\n   - Intervention testing\n   - Treatment planning\n\n### Future Applications\n\nPotential future applications include:\n\n1. Clinical Psychology\n   - Disorder modeling\n   - Treatment simulation\n   - Outcome prediction\n   - Intervention development\n\n2. Social Robotics\n   - Emotional intelligence\n   - Social interaction\n   - Development simulation\n   - Attachment formation\n\n3. AI Development\n   - Developmental frameworks\n   - Emotional intelligence\n   - Social capability\n   - Natural learning\n\n## Technical Implementation\n\n### System Requirements\n\nThe system requires specific technical resources:\n\n1. Hardware Requirements\n   - CUDA-capable GPU\n   - Minimum 16GB RAM\n   - SSD storage\n   - Multi-core processor\n\n2. Software Requirements\n   - Python 3.8+\n   - PyTorch 1.8+\n   - CUDA 11.0+\n   - Additional dependencies\n\n### Installation and Setup\n\nDetailed setup instructions are provided for:\n\n1. Environment Setup\n   - Virtual environment creation\n   - Dependency installation\n   - CUDA setup\n   - System configuration\n\n2. Model Installation\n   - Pretrained model download\n   - Configuration setup\n   - Testing procedures\n   - Validation checks\n\n## Future Research Directions\n\n### Planned Developments\n\nSeveral key areas for future development have been identified:\n\n1. Enhanced Capabilities\n   - Multi-modal processing\n   - Advanced theory of mind\n   - Complex emotion handling\n   - Sophisticated learning\n\n2. Technical Improvements\n   - Efficiency optimization\n   - Scale improvement\n   - Architecture refinement\n   - Performance enhancement\n\n3. New Features\n   - Additional developmental stages\n   - Enhanced psychological mechanisms\n   - Advanced social capabilities\n   - Improved learning systems\n\n### Research Opportunities\n\nThe system opens numerous research opportunities:\n\n1. Developmental Psychology\n   - Theory testing\n   - Model validation\n   - Intervention research\n   - Pattern discovery\n\n2. AI Development\n   - Architecture innovation\n   - Learning mechanisms\n   - Emotional intelligence\n   - Social capability\n\n3. Clinical Applications\n   - Therapeutic modeling\n   - Intervention testing\n   - Outcome prediction\n   - Treatment planning\n\n## Ethics and Considerations\n\n### Ethical Framework\n\nThe project adheres to strict ethical guidelines:\n\n1. Development Ethics\n   - Responsible AI development\n   - Bias consideration\n   - Safety protocols\n   - Privacy protection\n\n2. Application Ethics\n   - Appropriate use cases\n   - Limitation recognition\n   - Risk management\n   - User protection\n\n### Safety Considerations\n\nImportant safety aspects are addressed:\n\n1. Technical Safety\n   - System boundaries\n   - Control mechanisms\n   - Error handling\n   - Security measures\n\n2. Psychological Safety\n   - Attachment considerations\n   - Emotional impact\n   - Development effects\n   - User well-being\n\n## Getting Started\n\n### Initial Setup\n\nDetailed setup instructions include:\n\n1. Installation\n   - Environment preparation\n   - Dependency management\n   - System configuration\n   - Testing procedures\n\n2. Configuration\n   - Parameter settings\n   - System optimization\n   - Performance tuning\n   - Customization options\n\n### Basic Usage\n\nGuidelines for basic system usage cover:\n\n1. Model Loading\n   - Initialization procedures\n   - Configuration loading\n   - State management\n   - System validation\n\n2. Interaction\n   - Input formatting\n   - Response handling\n   - State monitoring\n   - Output interpretation\n\n## Contributing\n\n### Development Guidelines\n\nContribution guidelines include:\n\n1. Code Standards\n   - Style guidelines\n   - Documentation requirements\n   - Testing expectations\n   - Review procedures\n\n2. Development Process\n   - Issue tracking\n   - Feature requests\n   - Pull requests\n   - Version control\n\n## License\n\nThis project is licensed under the MIT License. See the LICENSE file for details.\n\n## Citation\n\nIf you use this work in your research, please cite:\n\n```bibtex\n@software{neural_child_development,\n  title = {Neural Child Development System},\n  year = {2025},\n  author = {[Renato Kuipers]},\n  url = {[https://github.com/renatokuipers/neural-child)]},\n  note = {A comprehensive framework for developmental AI implementing psychological growth and emotional intelligence}\n}\n```\n\n## Acknowledgments\n\nThis project builds upon research from multiple fields:\n- Developmental Psychology\n- Neuroscience\n- Machine Learning\n- Cognitive Science\n- Attachment Theory\n- Emotional Intelligence Research\n- Clinical Psychology\n\nThe integration of these diverse fields into a coherent, functional system represents a significant step forward in developmental AI research.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "developmental",
        "ai",
        "renatokuipers",
        "renatokuipers neural",
        "emotional intelligence",
        "develop ai"
      ],
      "category": "scientific-research-tools"
    },
    "sirusb--samtools_mcp": {
      "owner": "sirusb",
      "name": "samtools_mcp",
      "url": "https://github.com/sirusb/samtools_mcp",
      "imageUrl": "/freedevtools/mcp/pfp/sirusb.webp",
      "description": "Provides tools for viewing, converting, sorting, indexing, and analyzing SAM/BAM/CRAM genomic files. Facilitates efficient bioinformatics workflows with region-specific analysis and statistics generation.",
      "stars": 0,
      "forks": 1,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-05-12T12:36:19Z",
      "readme_content": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/sirusb-samtools-mcp-badge.png)](https://mseep.ai/app/sirusb-samtools-mcp)\n\n# SAMtools MCP (Model Control Protocol)\n\nA Model Control Protocol implementation for SAMtools, providing a standardized interface for working with SAM/BAM/CRAM files.\n\n## Features\n\n- View and convert SAM/BAM/CRAM files\n- Sort alignment files\n- Index BAM/CRAM files\n- Generate statistics\n- Merge multiple BAM files\n- Calculate read depth\n- Index FASTA files\n- And more...\n\n### Core Capabilities\n- **File Format Support**: Handle SAM (text), BAM (binary), and CRAM (compressed) alignment files\n- **Format Conversion**: Convert between SAM, BAM, and CRAM formats seamlessly\n- **Region-Specific Analysis**: Extract and analyze specific genomic regions\n- **Flag-Based Filtering**: Filter reads based on SAM flags\n- **Performance Optimization**: Multi-threaded operations for sorting and merging\n- **Statistical Analysis**: Generate comprehensive alignment statistics\n\n### Tools Overview\n\n| Tool | Description | Key Features |\n|------|-------------|--------------|\n| `view` | View and convert alignment files | - Format conversion (SAM/BAM/CRAM)<br>- Region filtering<br>- Flag-based filtering<br>- Header manipulation |\n| `sort` | Sort alignment files | - Coordinate-based sorting<br>- Name-based sorting<br>- Memory per thread control<br>- Multi-threading support |\n| `index` | Index BAM/CRAM files | - BAI index generation<br>- CSI index support<br>- CRAM index creation |\n| `merge` | Merge multiple BAM/CRAM files | - Multi-file merging<br>- Thread-enabled processing<br>- Header reconciliation |\n| `depth` | Calculate read depth | - Per-base depth calculation<br>- Region-specific analysis<br>- Multi-file support |\n| `flagstat` | Generate alignment statistics | - Comprehensive flag statistics<br>- Quality checks<br>- Paired-end metrics |\n| `idxstats` | BAM/CRAM index statistics | - Reference sequence stats<br>- Mapped/unmapped counts<br>- Length information |\n| `faidx` | Index FASTA files | - FASTA indexing<br>- Region extraction<br>- Sequence retrieval |\n\n\n## Installation\n\n### Using Docker (Recommended)\n\nThe easiest way to use SAMtools MCP is through Docker:\n\n```bash\n# Pull the Docker image\ndocker pull nadhir/samtools-mcp:latest\n\n# Run the container\ndocker run -it --rm nadhir/samtools-mcp:latest\n\n# To process BAM files, mount a volume:\ndocker run -it --rm -v /path/to/your/bam/files:/data nadhir/samtools-mcp:latest\n```\n\n### Local Installation\n\n1. Clone the repository:\n```bash\ngit clone https://github.com/your-username/samtools_mcp.git\ncd samtools_mcp\n```\n\n2. Install dependencies:\n```bash\npip install uv\nuv pip install -r requirements.txt\n```\n\n## Configuration\n\n### MCP Server Configuration\n\nTo configure the MCP server to use the Docker image, add the following to your MCP configuration file:\n\n```json\n{\n  \"servers\": {\n    \"samtools\": {\n      \"type\": \"docker\",\n      \"image\": \"nadhir/samtools-mcp:latest\",\n      \"volumes\": [\n        {\n          \"source\": \"/path/to/your/data\",\n          \"target\": \"/data\"\n        }\n      ]\n    }\n  }\n}\n```\n\n### Local MCP Configuration\n\nTo configure the MCP to run using `uv`, add the following to your `~/.cursor/mcp.json`:\n\n```json\n{\n  \"samtools_mcp\": {\n    \"command\": \"uv\",\n    \"args\": [\"run\", \"--with\", \"fastmcp\", \"fastmcp\", \"run\", \"/path/to/samtools_mcp.py\"]\n  }\n}\n```\n\nReplace `/path/to/samtools_mcp.py` with the actual path to your `samtools_mcp.py` file.\n\n## Usage\n\n### Basic Commands\n\n1. View BAM file:\n```python\nfrom samtools_mcp import SamtoolsMCP\n\nmcp = SamtoolsMCP()\nresult = mcp.view(input_file=\"/data/example.bam\")\n```\n\n2. Sort BAM file:\n```python\nresult = mcp.sort(input_file=\"/data/example.bam\", output_file=\"/data/sorted.bam\")\n```\n\n3. Index BAM file:\n```python\nresult = mcp.index(input_file=\"/data/sorted.bam\")\n```\n\n### Advanced Usage\n\n1. View specific region with flags:\n```python\nresult = mcp.view(\n    input_file=\"/data/example.bam\",\n    region=\"chr1:1000-2000\",\n    flags_required=\"0x2\",\n    output_format=\"SAM\"\n)\n```\n\n2. Sort by read name:\n```python\nresult = mcp.sort(\n    input_file=\"/data/example.bam\",\n    output_file=\"/data/namesorted.bam\",\n    sort_by_name=True\n)\n```\n\n3. Calculate depth with multiple input files:\n```python\nresult = mcp.depth(\n    input_files=[\"/data/sample1.bam\", \"/data/sample2.bam\"],\n    region=\"chr1:1-1000000\"\n)\n```\n\n## Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n\n## License\n\nThis project is licensed under the MIT License - see the LICENSE file for details.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "bioinformatics",
        "samtools_mcp",
        "genomic",
        "samtools_mcp provides",
        "cram genomic",
        "bioinformatics workflows"
      ],
      "category": "scientific-research-tools"
    },
    "stat-guy--retrieval-augmented-thinking": {
      "owner": "stat-guy",
      "name": "retrieval-augmented-thinking",
      "url": "https://github.com/stat-guy/retrieval-augmented-thinking",
      "imageUrl": "/freedevtools/mcp/pfp/stat-guy.webp",
      "description": "Enhances AI model reasoning and problem-solving capabilities by employing structured retrieval-augmented thinking processes. Provides features like adaptive thought chains, iterative hypothesis generation, and real-time context coherence for complex reasoning tasks.",
      "stars": 16,
      "forks": 5,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-09-06T07:53:28Z",
      "readme_content": "# RAT MCP Server (Node.js)\n\nRetrieval Augmented Thinking MCP Server - A reasoning tool that processes structured thoughts with metrics, branching, and revision capabilities.\n\n## Installation\n\n### Simple 3-Step Process\n```bash\ngit clone https://github.com/stat-guy/retrieval-augmented-thinking.git\ncd retrieval-augmented-thinking\nnpm install -g .\n```\n\n### Verify Installation\nTest that the installation worked:\n```bash\nnpx mcp-server-rat-node --help\n```\n\n**Success indicator:** If you see `RAT MCP Server (Node.js) running on stdio`, your installation is ready!\n\n## Claude Desktop Configuration\n\nAdd to your Claude Desktop configuration file:\n\n**macOS**: `~/Library/Application Support/Claude/claude_desktop_config.json`  \n**Windows**: `%APPDATA%/Claude/claude_desktop_config.json`\n\n```json\n{\n  \"mcpServers\": {\n    \"retrieval-augmented-thinking\": {\n      \"command\": \"npx\",\n      \"args\": [\"mcp-server-rat-node\"]\n    }\n  }\n}\n```\n\nAfter adding the configuration, restart Claude Desktop to load the RAT server.\n\n## Usage\n\nThe server provides a single `rat` tool for processing structured thoughts:\n\n```javascript\n// Basic usage\n{\n  \"thought\": \"I need to analyze this problem step by step...\",\n  \"nextThoughtNeeded\": true,\n  \"thoughtNumber\": 1,\n  \"totalThoughts\": 3\n}\n\n// With revision\n{\n  \"thought\": \"Let me reconsider my previous analysis...\",\n  \"nextThoughtNeeded\": false,\n  \"thoughtNumber\": 2,\n  \"totalThoughts\": 3,\n  \"isRevision\": true,\n  \"revisesThought\": 1\n}\n\n// With branching\n{\n  \"thought\": \"Alternative approach: what if we consider...\",\n  \"nextThoughtNeeded\": true,\n  \"thoughtNumber\": 2,\n  \"totalThoughts\": 4,\n  \"branchFromThought\": 1,\n  \"branchId\": \"alt-path-1\"\n}\n```\n\n## Tool Parameters\n\n### Required\n- `thought` (string): The thought content to process\n- `nextThoughtNeeded` (boolean): Whether another thought is needed to continue\n- `thoughtNumber` (integer): Current thought number in the sequence\n- `totalThoughts` (integer): Total expected thoughts (adjustable)\n\n### Optional\n- `isRevision` (boolean): Whether this revises a previous thought\n- `revisesThought` (integer): The thought number being revised\n- `branchFromThought` (integer): Thought number to branch from\n- `branchId` (string): Unique identifier for this branch\n- `needsMoreThoughts` (boolean): Extend beyond totalThoughts if needed\n\n## Response Format\n\n```json\n{\n  \"thought_number\": 1,\n  \"total_thoughts\": 3,\n  \"metrics\": {\n    \"complexity\": 0.342,\n    \"depth\": 0.521,\n    \"quality\": 0.643,\n    \"impact\": 0.289,\n    \"confidence\": 0.758\n  },\n  \"analytics\": {\n    \"total_thoughts\": 5,\n    \"average_quality\": 0.612,\n    \"chain_effectiveness\": 0.145\n  },\n  \"next_thought_needed\": true,\n  \"visual_output\": \"┌─ 💭 Thought 1/3 ─────────────────┐\\\\n│ Analysis shows clear patterns... │\\\\n├─ Metrics ──────────────────────┤\\\\n│ Quality: 0.64 | Impact: 0.29... │\\\\n└─────────────────────────────────┘\"\n}\n```\n\n## Troubleshooting\n\n### If Installation Fails\nThe installation process includes automatic permission fixes. If you encounter issues:\n\n1. **Ensure you have Node.js and npm installed**\n2. **Try the alternative installation method:**\n   ```bash\n   npm install -g git+https://github.com/stat-guy/retrieval-augmented-thinking.git\n   ```\n3. **For rare permission issues:**\n   ```bash\n   chmod +x $(npm bin -g)/mcp-server-rat-node\n   ```\n\n### Verification Steps\nBefore configuring Claude Desktop, always verify:\n```bash\nnpx mcp-server-rat-node --help\n```\n\nIf this shows \"RAT MCP Server (Node.js) running on stdio\", you're ready to configure Claude Desktop.\n\n## Testing\n\nRun the test suite:\n```bash\nnpm test\n```\n\nTest tool execution:\n```bash\nnode test-tool.js\n```\n\n## License\n\nThis MCP server is licensed under the MIT License. This means you are free to use, modify, and distribute the software, subject to the terms and conditions of the MIT License. For more details, please see the LICENSE file in the project repository.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "ai",
        "retrieval",
        "augmented",
        "augmented thinking",
        "reasoning tasks",
        "retrieval augmented"
      ],
      "category": "scientific-research-tools"
    },
    "svmlearn---": {
      "owner": "svmlearn",
      "name": "-",
      "url": "https://github.com/svmlearn/-",
      "imageUrl": "/freedevtools/mcp/pfp/svmlearn.webp",
      "description": "Supports training, testing, and deploying support vector machine models for classification tasks. Facilitates advanced algorithm utilization to improve predictive performance within machine learning projects.",
      "stars": 0,
      "forks": 0,
      "license": "No License",
      "language": "",
      "updated_at": "2022-05-09T07:49:49Z",
      "readme_content": "# -",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "svmlearn",
        "classification",
        "vector",
        "tools svmlearn",
        "svmlearn supports",
        "vector machine"
      ],
      "category": "scientific-research-tools"
    },
    "szeider--mcp-solver": {
      "owner": "szeider",
      "name": "mcp-solver",
      "url": "https://github.com/szeider/mcp-solver",
      "imageUrl": "/freedevtools/mcp/pfp/szeider.webp",
      "description": "Integrates SAT, SMT, and constraint solving capabilities with Large Language Models, enabling interactive creation, editing, and solving of constraint models in MiniZinc, SAT models in PySAT, and SMT formulas using Z3 Python.",
      "stars": 133,
      "forks": 17,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-26T09:11:19Z",
      "readme_content": "------\n\n# MCP Solver\n\n[![MCP Compatible](https://img.shields.io/badge/MCP-Compatible-green.svg)](https://modelcontextprotocol.io/) [![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](https://opensource.org/licenses/MIT) [![Python Version](https://img.shields.io/badge/Python-3.11%2B-blue.svg)](https://www.python.org/)\n\nA Model Context Protocol (MCP) server that exposes constraint solving, SAT, SMT, and ASP capabilities to Large Language Models.\n\n------\n\n## Overview\n\nThe *MCP Solver* integrates constraint solving, SAT, SMT, and ASP with LLMs through the Model Context Protocol, enabling AI models to interactively create, edit, and solve:\n\n- Constraint models in [MiniZinc](https://www.minizinc.org/)\n- SAT models in [PySAT](https://pysathq.github.io/)\n- MaxSAT optimization problems in [PySAT](https://pysathq.github.io/)\n- SMT formulas in [Z3 Python](https://ericpony.github.io/z3py-tutorial/guide-examples.htm)\n- Answer Set Programs in [Clingo](https://potassco.org/clingo/)\n\nFor a detailed description of the *MCP Solver's* system architecture and theoretical foundations, see the accompanying research paper: Stefan Szeider, [\"Bridging Language Models and Symbolic Solvers via the Model Context Protocol\"](https://doi.org/10.4230/LIPIcs.SAT.2025.30), SAT 2025.\n\n## Available Tools\n\nIn the following, *item* refers to some part of the (MiniZinc/PySAT/Z3/ASP) code, and *model* to the encoding. \n\n| Tool Name      | Description                                   |\n| -------------- | --------------------------------------------- |\n| `clear_model`  | Remove all items from the model               |\n| `add_item`     | Add new item at a specific index              |\n| `delete_item`  | Delete item at index                          |\n| `replace_item` | Replace item at index                         |\n| `get_model`    | Get current model content with numbered items |\n| `solve_model`  | Solve the model (with timeout parameter)      |\n\n------\n\n## System Requirements\n\n- Python and project manager [uv](https://docs.astral.sh/uv/) \n- Python 3.11+\n- Mode-specific requirements: MiniZinc, PySAT, Python Z3 (required packages are installed via pip)\n- Operating systems: macOS, Windows, Linux (with appropriate adaptations)\n\n------\n\n## Installation\n\nMCP Solver requires Python 3.11+, the `uv` package manager, and solver-specific dependencies (MiniZinc, Z3, or PySAT).\n\nFor detailed installation instructions for Windows, macOS, and Linux, see [INSTALL.md](INSTALL.md).\n\nQuick start:\n```bash\ngit clone https://github.com/szeider/mcp-solver.git\ncd mcp-solver\nuv venv\nsource .venv/bin/activate\nuv pip install -e \".[all]\"  # Install all solvers\n```\n\n------\n\n## Available Modes / Solving Backends\n\nThe MCP Solver provides five distinct operational modes, each integrating with a different constraint solving backend. Each mode requires specific dependencies and offers unique capabilities for addressing different classes of problems.\n\n### MiniZinc Mode\n\nMiniZinc mode provides integration with the MiniZinc constraint modeling language with the following features:\n\n- Rich constraint expression with global constraints\n- Integration with the Chuffed constraint solver\n- Optimization capabilities\n- Access to solution values via `get_solution`\n\n**Dependencies**: Requires the `minizinc` package (`uv pip install -e \".[mzn]\"`)\n\n**Configuration**: To run in MiniZinc mode, use:\n\n```\nmcp-solver-mzn\n```\n\n### PySAT Mode\n\nPySAT mode allows interaction with the Python SAT solving toolkit with the following features:\n\n- Propositional constraint modeling using CNF (Conjunctive Normal Form)\n- Access to various SAT solvers (Glucose3, Glucose4, Lingeling, etc.)\n- Cardinality constraints (at_most_k, at_least_k, exactly_k)\n- Support for boolean constraint solving\n\n**Dependencies**: Requires the `python-sat` package (`uv pip install -e \".[pysat]\"`)\n\n**Configuration**: To run in PySAT mode, use:\n\n```\nmcp-solver-pysat\n```\n\n### MaxSAT Mode\n\nMaxSAT mode provides specialized support for optimization problems with PySAT, featuring:\n\n- Weighted Conjunctive Normal Form (WCNF) support\n- Integration with the RC2 MaxSAT solver\n- Optimization capabilities with objective tracking\n- Support for both hard and soft constraints\n\n**Dependencies**: Requires the `python-sat` package (`uv pip install -e \".[pysat]\"`)\n\n**Configuration**: To run in MaxSAT mode, use:\n\n```\nmcp-solver-maxsat\n```\n\n### Z3 Mode\n\nZ3 mode provides access to Z3 SMT (Satisfiability Modulo Theories) solving capabilities with the following features:\n\n- Rich type system: booleans, integers, reals, bitvectors, arrays\n- Constraint solving with quantifiers\n- Optimization capabilities\n- Template library for common modeling patterns\n\n**Dependencies**: Requires the `z3-solver` package (`uv pip install -e \".[z3]\"`)\n\n**Configuration**: To run in Z3 mode, use:\n\n```\nmcp-solver-z3\n```\n\n### ASP Mode\n\nASP (Answer Set Programming) mode provides integration with ASP solvers (e.g., Clingo) for declarative problem solving with logic programs. Features include:\n\n- Expressive logic programming for combinatorial and knowledge representation problems\n- Support for constraints, choice rules, aggregates, and optimization statements\n- Integration with the Clingo solver for efficient answer set computation\n- Access to answer sets and model inspection\n\n**Dependencies**: Requires the `clingo` package (`uv pip install -e \".[asp]\"` or included in `[all]`)\n\n**Configuration**: To run in ASP mode, use:\n\n```\nmcp-solver-asp\n```\n\n## MCP Test Client\n\nThe MCP Solver includes an MCP client for development, experimentation, and diagnostic purposes, based on the *ReAct* agent framework. This client serves as an intermediary between an LLM and the MCP server, facilitating the translation of natural language problem statements into formal constraint programming solutions. \n\n### Installation\n\n```bash\n# Install client dependencies\nuv pip install -e \".[client]\"\n\n# Verify client installation and configuration\nuv run test-setup-client\n```\n\nThe client requires an **API key** from an LLM provider. For Anthropic (the default LLM is Claude Sonnet 3.7), set the `ANTHROPIC_API_KEY` environment variable. This can be set in your environment or included in a `.env` file in the project root.\n\nThe client also supports other LLM providers including OpenAI, Google (Gemini), OpenRouter and even local models via LM Studio. You can specify which model to use with the `--mc` command line flag. See [INSTALL.md](INSTALL.md) for details on model code formats.\n\n### Usage\n\nThe client provides specialized commands for each solver backend:\n\n```bash\n# MiniZinc mode\nuv run run-test mzn --problem <path/to/problem.md>\n\n# PySAT mode\nuv run run-test pysat --problem <path/to/problem.md>\n\n# MaxSAT mode\nuv run run-test maxsat --problem <path/to/problem.md>\n\n# Z3 mode\nuv run run-test z3 --problem <path/to/problem.md>\n\n# ASP mode\nuv run run-test asp --problem <path/to/problem.md>\n```\n\n------\n\n## Examples\n\n### Example 1: Casting Problem (MiniZinc)\n\nThis is based on an example by George Johnson (NYT, July 13, 1999).\n\n**User**\n\n> Suppose that a theatrical director feels obligated to cast either his ingenue, Actress Alvarez, or his nephew, Actor Cohen, in a production. But Miss Alvarez won't be in a play with Mr. Cohen (her former lover), and she demands that the cast include her new flame, Actor Davenport. The producer, with her own favors to repay, insists that Actor Branislavsky have a part. But Mr. Branislavsky won't be in any play with Miss Alvarez or Mr. Davenport. Can the director cast the play?\n\n**System:** **`clear_model`**\n\n> ```\n> Model cleared\n> ```\n\n**System:** **`add_item`** (multiple)\n\n> ```minizinc\n> var bool: alvarez;\n> var bool: cohen;\n> var bool: branislavsky;\n> var bool: davenport;\n> constraint alvarez \\/ cohen;                          % Must cast either Alvarez or Cohen\n> constraint not (alvarez /\\ cohen);                    % Alvarez won't work with Cohen\n> constraint alvarez -> davenport;                      % If Alvarez is cast, Davenport must be cast\n> constraint branislavsky;                              % Branislavsky must be cast\n> constraint not (branislavsky /\\ alvarez);             % Branislavsky won't work with Alvarez\n> constraint not (branislavsky /\\ davenport);           % Branislavsky won't work with Davenport\n> solve satisfy;\n> ```\n\n**System:** **`solve_model`**\n\n> ```\n> {'status': 'SAT', 'solution': Solution(alvarez=False, cohen=True, branislavsky=True, davenport=False, _checker=''), 'solve_time': 0.0}\n> ```\n\nThe solution (cast Cohen and Branislavsky, but not Alvarez or Davenport) satisfies all constraints.\n\n### Example 2: N-Queens Problem (MiniZinc)\n\n**User**\n\n> Check whether you can place n Queens on an nxn chessboard. Try n=10,20,30,40 and compare the solving times\n\n**System:** **`clear_model`**\n\n> ```\n> Model cleared\n> ```\n\n**System:** **`add_item`** (multiple)\n\n> ```minizinc\n> int: n = 10;\n> array[1..n] of var 1..n: queens;\n> constraint alldifferent(queens);\n> constraint alldifferent([queens[i] + i | i in 1..n]);\n> constraint alldifferent([queens[i] - i | i in 1..n]);\n> solve satisfy;\n> ```\n\n**System:** **`solve_model`** (with n=10, 20, 30, and 40 by changing the value of n)\n\nSolve times comparison: n=10: 0.001s n=20: 0.005s n=30: 0.040s n=40: 0.043s\n\nSolutions exist for all tested board sizes. Time complexity increases non-linearly but remains manageable up to n=40.\n\n### Example 3: Traveling Salesperson Problem (MiniZinc)\n\n**User**\n\n> A saleswoman based in Vienna needs to plan her upcoming tour through Austria, visiting each province capital once. Help find the shortest route. Distances in km: 1=Vienna, 2=St. Pölten, 3=Eisenstadt, 4=Linz, 5=Graz, 6=Klagenfurt, 7=Salzburg, 8=Innsbruck, 9=Bregenz\n>\n> [Distance matrix table provided in user message]\n\n**System:** **`add_item`** (multiple)\n\n> ```minizinc\n> include \"globals.mzn\";\n> int: n = 9;\n> array[1..n,1..n] of int: dist = [|0,65,60,184,195,319,299,478,631|65,0,125,119,130,254,234,413,566|60,125,0,184,157,281,261,440,593|184,119,184,0,208,252,136,315,468|195,130,157,208,0,136,280,459,629|319,254,281,252,136,0,217,391,566|299,234,261,136,280,217,0,188,343|478,413,440,315,459,391,188,0,157|631,566,593,468,629,566,343,157,0|];\n> array[1..n] of var 1..n: succ;\n> var int: total_dist = sum(i in 1..n)(dist[i,succ[i]]);\n> constraint circuit(succ);\n> solve minimize total_dist;\n> ```\n\n**System:** **`solve_model`**\n\n> ```\n> {'status': 'SAT', 'solution': Solution(objective=1564, succ=[2, 4, 1, 7, 3, 5, 9, 6, 8], _checker=''), 'solve_time': 0.002}\n> ```\n\nOptimal route: Vienna → St. Pölten → Linz → Salzburg → Bregenz → Innsbruck → Klagenfurt → Graz → Eisenstadt → Vienna with a total distance of 1,564 km.\n\n## Feedback\n\nYou can provide feedback to the author via this [form](https://form.jotform.com/szeider/mcp-solver-feedback-form).\n\n## Disclaimer\n\nThis MCP Solver is in its prototype stage and should be used with caution. Users are encouraged to experiment, but any use in critical environments is at their own risk.\n\n------\n\n## License\n\nThis project is licensed under the MIT License - see the [LICENSE](https://claude.ai/chat/LICENSE) file for details.\n\n------\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "solver",
        "python",
        "pysat",
        "mcp solver",
        "smt constraint",
        "sat models"
      ],
      "category": "scientific-research-tools"
    },
    "t0mst0ne--pubmed-mcp-easy": {
      "owner": "t0mst0ne",
      "name": "pubmed-mcp-easy",
      "url": "https://github.com/t0mst0ne/pubmed-mcp-easy",
      "imageUrl": "/freedevtools/mcp/pfp/t0mst0ne.webp",
      "description": "Search and retrieve biomedical literature and research papers from PubMed. Supports advanced searches, batch querying, and retrieval of abstracts and full texts of open access articles.",
      "stars": 0,
      "forks": 1,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-06-18T01:29:22Z",
      "readme_content": "# PubMed MCP Server\n\n[![smithery badge](https://smithery.ai/badge/@t0mst0ne/pubmed-mcp-easy)](https://smithery.ai/server/@t0mst0ne/pubmed-mcp-easy)\n\nEnhanced Python-based PubMed MCP Server with API key support for faster and unlimited downloads.\n\n## Certified by MCP Review\n[https://mcpreview.com/mcp-servers/t0mst0ne/pubmed-mcp-easy]\n\n## Features\n\n- Search PubMed for biomedical literature and research papers\n- Find similar articles, cited articles, and citing articles\n- Retrieve abstracts and full text of open access articles\n- Batch search and advanced search capabilities\n- API key support for faster and unlimited downloads\n\n## API Key and Email Setup\n\nThe NCBI E-utilities API recommends using an API key and email address with your requests. This brings several benefits:\n\n- **Higher rate limits**: 10 requests/second instead of 3\n- **More results per query**: Up to 200 results per request instead of 100\n- **Priority service**: Better queue position for your requests\n\n### How to Get an API Key\n\n1. Create an NCBI account at [https://www.ncbi.nlm.nih.gov/account/](https://www.ncbi.nlm.nih.gov/account/)\n2. Go to the API Keys Management page\n3. Generate a new API key\n\n### Setting Up API Key and Email\n\nThe easiest way to set up your API key and email is using the setup script:\n\n```bash\npython setup_api.py\n```\n\nThis interactive script will guide you through the process and offer multiple setup options.\n\nAlternatively, you can set up your API key and email manually using one of these three methods:\n\n#### 1. Environment Variables\n\n```bash\nexport NCBI_API_KEY=your_api_key_here\nexport NCBI_EMAIL=your_email@example.com\n```\n\n#### 2. Command Line Arguments\n\n```bash\npython python-pubmed-mcp-enhanced.py --api-key your_api_key_here --email your_email@example.com\n```\n\n#### 3. Configuration File\n\nCreate a `config.json` file based on the example:\n\n```bash\ncp config.json.example config.json\n```\n\nEdit `config.json` to include your API key and email:\n\n```json\n{\n  \"api_key\": \"your_api_key_here\",\n  \"email\": \"your_email@example.com\"\n}\n```\n\nThen run the server with the config file:\n\n```bash\npython python-pubmed-mcp-enhanced.py --config config.json\n```\n\n## Usage\n\n### Installing via Smithery\n\nTo install pubmed-mcp-easy for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@t0mst0ne/pubmed-mcp-easy):\n\n```bash\nnpx -y @smithery/cli install @t0mst0ne/pubmed-mcp-easy --client claude\n```\n\n### Standard Usage\n\nRun the server:\n\n```bash\npython python-pubmed-mcp-enhanced.py\n```\n\n### Claude Desktop Integration\n\nTo integrate with Claude Desktop, add the following to your `claude_desktop_config.json` file:\n\n```json\n\"pubmed-easy\": {\n    \"command\": \"/opt/anaconda3/bin/python\",\n    \"args\": [\n        \"/GITHUB_cloned_dir/pubmed-mcp-easy/python-pubmed-mcp-enhanced.py\", \"--config\", \"config.json\"\n    ]\n}\n```\n\nMake sure to:\n1. Replace `/opt/anaconda3/bin/python` with the path to your Python executable\n2. Replace `/GITHUB_cloned_dir` with the actual path to your GitHub directory\n3. Create a `config.json` file with your API key and email as described above\n\nAfter adding this configuration, you can access PubMed tools directly from Claude Desktop.\n\n### Available Tools\n\nThe server provides the following MCP tools:\n\n- `pubmed_search`: Search for articles by keyword or query\n- `pubmed_similar`: Find similar articles\n- `pubmed_cites`: Find articles cited by a specific paper\n- `pubmed_cited_by`: Find articles that cite a specific paper\n- `pubmed_abstract`: Retrieve the abstract of an article\n- `pubmed_open_access`: Check if an article is open access\n- `pubmed_full_text`: Retrieve the full text of an open access article\n- `pubmed_batch_search`: Perform multiple searches in parallel\n- `pubmed_author_search`: Search for papers by a specific author\n- `pubmed_advanced_search`: Perform advanced field-based searches\n- `pubmed_journal_search`: Search for papers in a specific journal\n\n## Important Notes\n\n1. Including an email address is recommended by NCBI as it allows them to contact you if there are issues with your requests.\n2. If you make heavy use of the E-utilities, NCBI recommends that you limit large jobs to either weekends or between 9 pm and 5 am Eastern Time weekdays.\n3. Always be considerate in your usage and follow NCBI's usage guidelines.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "pubmed",
        "biomedical",
        "searches",
        "pubmed mcp",
        "papers pubmed",
        "t0mst0ne pubmed"
      ],
      "category": "scientific-research-tools"
    },
    "tutumomo--MCP-civil-tools": {
      "owner": "tutumomo",
      "name": "MCP-civil-tools",
      "url": "https://github.com/tutumomo/MCP-civil-tools",
      "imageUrl": "/freedevtools/mcp/pfp/tutumomo.webp",
      "description": "Provides civil engineering calculation tools including coordinate conversions between latitude/longitude and UTM/TWD97, Manning's coefficient lookup, soil pressure coefficient calculations, and drainage flow computations. Integrates seamlessly with LLM applications and AI tools for streamlined civil engineering workflows.",
      "stars": 2,
      "forks": 2,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-08-19T06:16:30Z",
      "readme_content": "# MCP Civil Tools 伺服器\n\n本專案是一個基於 MCP 協議的 Python 伺服器，主要是將水土保持技術規範(2023.05.04)透過 MCP Server 的方式提供給支援 MCP 的 client 調用，實現以自然語言與大模型對話的方式來進行查詢及計算，提供如曼寧係數查詢、土壓力係數、排水溝流速、邊坡穩定、土壤侵蝕、逕流量、擋土牆檢核、植生建議、材料參數、坡面保護、滲水設施、IDF曲線等查詢及計算功能。另外還提供台灣地區經緯度與 UTM/TWD97 座標的轉換。適用於 LLM 工具、Claude Desktop 等 AI 應用整合。\n\n注意! 程式尚未完整，且每個大模型智力不等，輸出結果使用者務必再次核實，若有錯誤，歡迎反饋~\n---\n\n## 目錄結構\n\n```\nMCP-civil-tools/\n├── src/\n│   ├── mcp_server.py         # MCP 伺服器主程式\n│   ├── util.py              # 座標轉換與土木工程工具邏輯\n│   └── utm_types/\n│       └── __init__.py       # 型別定義\n├── requirements.txt          # 依賴套件\n├── README.md                 # 專案說明\n└── .gitignore                # Git 忽略規則\n```\n\n---\n\n## 安裝\n\n1. 建議先建立虛擬環境：\n   ```bash\n   python -m venv .venv\n   .venv\\Scripts\\activate  # Windows\n   # 或 source .venv/bin/activate  # Linux/macOS\n   ```\n2. 安裝依賴：\n   ```bash\n   pip install -r requirements.txt\n   ```\n\n---\n\n## 啟動方式\n\n### 1. CLI (stdio) 模式\n\n```bash\npython src/mcp_server.py\n```\n\n### 2. HTTP 服務模式（建議用於 API 測試或 HTTP 整合）\n\n```bash\nuvicorn src.mcp_server:app --port 8000\n```\n\n---\n\n## mcp.json 設定範例\n\n若要讓 Claude Desktop 或其他 MCP 客戶端自動偵測並啟動本伺服器，MCP setting JSON 內容如下（請依實際路徑調整）：\n\n```json\n{\n  \"mcpServers\": {\n    \"MCP-civil-tools\": {\n      \"command\": \"path/.venv/Scripts/python.exe\",\n      \"args\": [\n        \"path/src/mcp_server.py\"\n      ]\n    }\n  }\n}\n或是\n    \"MCP-civil-tools\": {\n      \"command\": \"C:/TOMO/MCP-civil-tools/.venv/Scripts/python.exe\",\n      \"args\": [\n        \"C:/TOMO/MCP-civil-tools/src/mcp_server.py\"\n      ],\n      \"disabled\": true,\n      \"autoApprove\": []\n    },\n\n```\n- `command` 請填入你虛擬環境的 python.exe 絕對路徑。\n- `args` 請填入 mcp_server.py 的絕對路徑。\n\n以 Windowss 系統，Command args，輸入格式如下：\nC:\\TOMO\\MCP-civil-tools\\.venv\\Scripts\\python.exe C:\\TOMO\\MCP-civil-tools\\src\\mcp_server.py\n\n以 MAC 系統，Command args，輸入格式如下：\n/Users/tuchengshin/Documents/MCP/MCP-civil-tools/.venv/bin/python3\n/Users/tuchengshin/Documents/MCP/MCP-civil-tools/src/mcp_server.py\n\n---\n\n## 參數預設行為說明\n\n- 使用者只輸入緯度、經度時，沒有輸入其他資訊時，預設的 UTM/TM2 Zone 就是 TM2-121，預設的半球就是北半球。\n- 當使用者只輸入平面座標 X, Y 時，預設的 UTM Zone 是 TWD97，半球是北半球。\n\n---\n\n## 主要功能更新\n\n- **所有查表型工具皆支援「支援清單查詢」API**：\n  - 例如：`list_supported_materials`（常用材料）、`list_supported_manning_materials`（曼寧係數材料）、`list_supported_max_velocity_materials`（最大流速材料）、`list_supported_regions`（地區/IDF/年雨量）、`list_supported_soil_types`（土壤類型）、`list_supported_land_uses`（土地利用）、`list_supported_practices`（水保措施）、`list_supported_runoff_land_uses`（逕流係數土地利用）、`list_supported_slope_protection_methods`（坡面保護工法）、`list_supported_soil_k_types`（滲透係數土壤）、`list_supported_idf_locations`（IDF曲線地點）等。\n  - 查詢時若輸入錯誤或查無資料，會自動提示所有可查詢的支援項目，提升使用體驗。\n\n- **2023/04/23 更新**：\n  - 修正 `list_supported_materials` 功能，現在可正確返回所有支援的常用材料設計參數材料名稱清單，包括：一般黏土、砂土、礫石、混凝土、花崗岩、石灰岩、頁岩、砂岩等。\n  - 新增四個 USLE 因子查詢工具：\n    - `query_r_factor_tool`：查詢降雨沖蝕指數 R 值（依據水土保持技術規範第35條）\n    - `query_k_factor_tool`：查詢土壤沖蝕指數 K 值（依據水土保持技術規範第35條）\n    - `query_c_factor_tool`：查詢覆蓋與管理因子 C 值（依據水土保持技術規範第35條）\n    - `query_p_factor_tool`：查詢水土保持處理因子 P 值（依據水土保持技術規範第35條）\n  - 修正 `calc_catchment_runoff` 功能，增強參數處理能力，現在可以正確處理自然語言輸入，例如「計算一個面積2公頃、降雨強度100mm/hr、逕流係數0.6的集水區最大逕流量」。\n  - 擴充 `check_retaining_wall` 功能，新增地震情況下的安全係數檢核：\n    - 依據水土保持技術規範第120條，增加地震時滑動安全係數（≧1.2）和傾倒安全係數（≧1.5）的檢核\n    - 增加合力作用點位置檢核，支援岩盤基礎和土層基礎兩種情況\n    - 修正承載力安全係數要求為3.0，符合規範要求\n    - 輸出報表中明確區分常時情況和地震情況的安全係數\n\n---\n\n## 個別工具使用範例\n\n### 經緯度轉 UTM\n- 輸入：自然語言描述經緯度，如「將緯度24.125、經度120.641轉換為UTM座標」\n- 回傳：`\"X,Y\"` 字串，數值四捨五入到小數點下4位\n\n#### 範例\n```\n輸入：請將緯度24.125193616011536、經度120.64098341751337轉換為UTM座標\n回傳：203650.6040,2670482.4250\n```\n\n### UTM 轉經緯度\n- 輸入：X, Y 座標（可選 zone, datum, south，預設 TWD97 北半球）\n- 回傳：`\"緯度,經度\"` 字串，數值四捨五入到小數點下15位\n\n#### 範例\n```\n輸入：請將 UTM 座標 X=203650.604、Y=2670482.425 轉換為經緯度\n回傳：24.125193616011536,120.64098341751337\n```\n\n### 曼寧係數查詢\n- 輸入：材料名稱（如「混凝土」、「純細砂」、「全面密草生」等）\n- 回傳：該材料的曼寧係數 n 及最大容許流速範圍\n\n#### 範例\n```\n輸入：全面密草生的曼寧係數是多少？\n回傳：全面密草生 的曼寧係數 n = 0.040，最大容許流速範圍：1.5~2.5 m/s\n依據：水土保持技術規範第18條及附件曼寧係數表\n\n輸入：請查詢混凝土的曼寧係數\n回傳：混凝土 的曼寧係數 n = 0.013，最大容許流速範圍：3.0~6.0 m/s\n依據：水土保持技術規範第18條及附件曼寧係數表\n```\n\n### 主動土壓力係數計算\n- 輸入：內摩擦角等資訊\n- 回傳：主動土壓力係數 Ka、計算公式、依據等\n\n#### 範例\n```\n輸入：內摩擦角30度的主動土壓力係數Ka是多少？\n回傳：主動土壓力係數 Ka = 0.3333\n依據：水土保持技術規範第117條及附件土壓力計算公式\n說明：主動土壓力係數用於計算擋土結構所承受的水平推力，是擋土牆設計的重要參數。\n```\n\n### 被動土壓力係數計算\n- 輸入：內摩擦角等資訊\n- 回傳：被動土壓力係數 Kp、計算公式、依據等\n\n#### 範例\n```\n輸入：內摩擦角30度的被動土壓力係數Kp是多少？\n回傳：被動土壓力係數 Kp = 3.0\n依據：水土保持技術規範第117條及附件土壓力計算公式\n說明：被動土壓力係數用於計算擋土結構的抵抗力，是擋土牆穩定分析的重要參數。\n```\n\n### 排水斷面流速/流深/流量計算（多斷面支援）\n- 輸入：斷面型式、流量、坡度、曼寧係數、渠道材質、尺寸等資訊\n- 回傳：流速、流深、斷面積、水力半徑等計算結果，以及完整報告書\n\n#### 計算邏輯與警告說明（2024/06修正版）\n- 若輸入流量超過該斷面滿流時的最大流量，系統會直接回傳錯誤訊息，提示「所需流量已超過滿流最大流量，請加大尺寸或坡度」。\n- 只要流量未超過滿流，計算會自動求得一個合理的流深（小於溝高），不會誤報滿流。\n- 滿流警告僅在流深極接近高度且流量極接近滿流時才會出現。\n- 計算精度與收斂條件已優化，避免工程常用情境下出現誤判。\n\n#### 最大/最小容許流速規則\n- 最大容許流速：\n  - 僅檢核最大值（如混凝土6.1m/s、鋼筋混凝土12.0m/s、粗砂及細砂0.61m/s等），超過時系統將警告並建議設置消能設施。\n  - 表格中無\"鋼筋混凝土\"之最大容許流速，系統已新增最大值12.0m/s。\n- 最小容許流速：\n  - 僅混凝土與鋼筋混凝土排水設施明確規定最小容許流速為0.8 m/s，低於時系統將警告可能導致泥砂淤積。\n  - 其餘材質未見明確規定，系統不檢核最小值。\n- 檢核建議：設計排水之流速超過其最大容許流速者，應於適當位置設置消能設施。\n\n#### 出水高（流深）規範檢核（第86條）\n- 矩形、梯形排水溝之出水高（流深）應符合：\n  - 不得小於設計水深的25%\n  - 最小值為20公分\n  - L型、拋物線型排水溝不在此限\n- 若計算流深低於上述標準，系統將於回傳結果與報告書中給予明確警告，請調整設計參數。\n\n#### 範例\n```\n輸入：計算寬50公分，高70cm的鋼筋混凝土矩形溝，流量1cms，坡度5%，流速和流深? 輸出完整報告，含計算過程及計算式。\n回傳：\n流速: 7.20 m/s，流深: 0.250 m。警告：計算流速 v = 7.2000 m/s 已超過『鋼筋混凝土』最大容許流速 12.0 m/s，應於適當位置設置消能設施。\n\n輸入：計算寬50公分，高70cm的混凝土矩形溝，流量0.2cms，坡度0.5%，流速和流深?\n回傳：\n流速: 0.65 m/s，流深: 0.120 m。警告：計算流速 v = 0.6500 m/s 低於『混凝土』最小容許流速 0.8 m/s，可能導致泥砂淤積。\n\n【排水斷面流速/流深/流量計算報告】\n斷面型式：矩形\n矩形底寬 b=50.0cm, 高度 h=70.0cm\n流量 Q = 1.000 cms\n坡度 S = 5.000%\n曼寧係數 n = 0.013\n渠道材質：鋼筋混凝土\n\n【計算公式】\nQ = A × V, V = (1/n) × R^(2/3) × S^(1/2)\n【計算步驟】\nA = b×y, P = b+2y, R = A/P, V = (1/n)R^(2/3)S^(1/2), Q = A×V\n\n【計算結果】\n流速 V = 2.340 m/s\n流深 y = 0.210 m\n斷面積 A = 0.1050 m²\n水力半徑 R = 0.0700 m\n周長 P = 1.0000 m\n\n計算結果符合安全流速規範。\n【檢核警告】計算流深 y = 0.210 m 不符第86條規範，應≧max(0.2m, 設計水深25%)=0.250 m。請調整設計。\n```\n\n### 邊坡穩定安全係數計算\n- 輸入：坡度、單位重、摩擦角、凝聚力、地下水位、方法等資訊\n- 回傳：安全係數、方法、是否合格、說明\n\n#### 範例\n```\n輸入：請計算坡度30度、土壤單位重18kN/m³、摩擦角30度、凝聚力10kPa的邊坡穩定安全係數\n回傳：安全係數 = 1.50，方法：簡化法，合格：True\n依據：水土保持技術規範第31條及附件安全係數表\n說明：邊坡穩定安全係數應大於1.5，本案例符合規範要求。\n```\n\n### 土壤侵蝕模數/流失量計算\n- 輸入：坡長、坡度、降雨、土壤類型、土地利用、水保措施等資訊\n- 回傳：侵蝕模數、流失量、方法、說明\n\n#### 範例\n```\n輸入：計算坡長100公尺、坡度10%、年降雨量1200mm、砂土、草地、等高耕作的土壤流失量\n回傳：土壤侵蝕模數/流失量 = 42.75 公噸/公頃/年\n依據：水土保持技術規範第35條及附件USLE公式、參數表\n說明：USLE公式為國際通用土壤流失量推估方法，台灣水保規範明定可用於坡地土壤侵蝕評估。\n```\n\n### 集水區最大逕流量計算\n- 輸入：集水區面積、降雨強度、逕流係數等資訊\n- 回傳：最大逕流量、方法、說明\n\n#### 範例\n```\n輸入：計算一個面積2公頃、降雨強度100mm/hr、逕流係數0.6的集水區最大逕流量\n回傳：最大逕流量 Q = 0.33 cms，方法：Rational\n依據：水土保持技術規範第16、17、18條及附件降雨強度、逕流係數表\n說明：集水區面積小於1000公頃時，無實測資料可採Rational公式計算洪峰流量。\n```\n\n### 護岸/擋土牆穩定檢核\n- 輸入：牆高、厚度（矩形斷面）或頂寬/底寬（梯形斷面）、單位重、摩擦角、凝聚力、背填坡度、地下水位、地震係數、基礎類型等資訊\n- 回傳：常時與地震情況下的滑動、傾倒安全係數、合力作用點位置、是否合格、說明\n\n#### 範例（矩形斷面）\n```\n輸入：檢核一座高2公尺、厚1公尺的擋土牆，土壤單位重18kN/m³、摩擦角30度、凝聚力10kPa、背填坡度10度，地震係數0.15，土層基礎\n回傳：\n【常時情況】\n滑動SF=1.65（≧1.5 合格），傾倒SF=2.30（≧2.0 合格），承載SF=3.10（≧3.0 合格），合力作用點位置合格\n【地震情況】\n滑動SF=1.25（≧1.2 合格），傾倒SF=1.60（≧1.5 合格），合力作用點位置合格\n依據：水土保持技術規範第117、118、120、164條及附件最小安全係數表\n說明：擋土牆設計須同時檢核常時與地震情況下的滑動、傾倒安全係數及合力作用點位置，並符合規範建議標準。\n```\n\n#### 範例（梯形斷面）\n```\n輸入：檢核一座高2公尺、頂寬0.5公尺、底寬1公尺的梯形擋土牆，土壤單位重18kN/m³、摩擦角30度、凝聚力10kPa，地震係數0.15，土層基礎\n回傳：\n【牆體幾何與重量】\n梯形斷面：高度 = 2.00 m，頂寬 = 0.50 m，底寬 = 1.00 m，牆體單位重 = 24.00 kN/m³\n牆體重量 W = 24.00 × (1.00 + 0.50) × 2.00 / 2 = 36.00 kN/m\n重心位置 x_g = 0.56 m\n\n【常時情況】\n滑動SF=1.78（≧1.5 合格），傾倒SF=2.45（≧2.0 合格），承載SF=3.25（≧3.0 合格），合力作用點位置合格\n【地震情況】\n滑動SF=1.32（≧1.2 合格），傾倒SF=1.75（≧1.5 合格），合力作用點位置合格\n依據：水土保持技術規範第117、118、120、164條及附件最小安全係數表\n說明：擋土牆設計須同時檢核常時與地震情況下的滑動、傾倒安全係數及合力作用點位置，並符合規範建議標準。\n```\n\n### 植生護坡設計建議\n- 輸入：坡度、土壤類型、氣候等資訊\n- 回傳：建議工法、草種、覆蓋率、說明\n\n#### 範例\n```\n輸入：請針對坡度30度、壤土、亞熱帶氣候的邊坡提供植生護坡設計建議\n回傳：建議工法：噴播草皮+格框或土工網，分區分期施工\n草種：百慕達草、狗牙根、地毯草等耐旱耐沖蝕草種\n覆蓋率：90.0%\n依據：水土保持技術規範第8、167、172條及附件坡度分級、樣區面積、覆蓋率等規定\n```\n\n### 常用材料設計參數查詢\n- 輸入：材料名稱\n- 回傳：單位重、凝聚力、摩擦角、強度、說明\n\n#### 範例\n```\n輸入：請提供一般黏土的設計參數\n回傳：材料：一般黏土，單位重：18.0kN/m³，凝聚力：20.0kPa，摩擦角：25.0°，強度：200.0kPa\n依據：常用土壤工程手冊\n```\n\n### 坡面保護工法建議\n- 輸入：坡度、土壤類型、降雨量等資訊\n- 回傳：建議工法、說明\n\n#### 範例\n```\n輸入：坡度30%、砂土、年降雨量1200mm的坡面，建議採用什麼保護工法？\n回傳：建議工法：噴播草皮+格框或土工網\n說明：坡度較大時應加強排水設施與坡面穩定措施\n依據：水土保持技術規範第8、167、172條及附件坡度分級表\n```\n\n### 滲水設施設計\n- 輸入：設施型式、土壤滲透係數、集水面積、降雨量等資訊\n- 回傳：設計流量、建議尺寸、說明\n\n#### 範例\n```\n輸入：請設計一個滲水井，土壤滲透係數k=0.001cm/s，集水面積100平方公尺，年降雨量1200mm\n回傳：設施型式：滲水井，設計流量：0.033cms，建議尺寸：直徑1.2m，深度1.8m\n依據：水土保持技術規範第94條及附件滲透設施設計準則\n說明：滲水井設計需考慮土壤滲透性、集水面積及當地降雨特性，並定期維護以確保功能。\n```\n\n### IDF曲線查詢\n- 輸入：地點、重現期、歷時等資訊\n- 回傳：降雨強度、說明\n\n#### 範例\n```\n輸入：請查詢台中市10年重現期、60分鐘歷時的降雨強度\n回傳：地點：台中市，重現期：10年，歷時：60分鐘，強度：78.5 mm/hr\n依據：水土保持技術規範第15條及附件IDF曲線公式\n說明：IDF曲線反映不同重現期與歷時下的降雨強度，用於水文分析與水利設施設計。\n```\n\n### 查詢支援清單\n- 你可以直接查詢有哪些可用的材料、地區、工法等：\n\n```\n查詢：有哪些常用材料？\n回傳：['一般黏土', '砂土', '礫石', '混凝土', ...]\n\n查詢：可以查詢哪些水溝鋪面的曼寧係數？\n回傳：['純細砂', '混凝土', '全面密草生', ...]\n\n查詢：有哪些坡面保護工法？\n回傳：['草皮或直接播種', '噴播草皮+格框/土工網', ...]\n\n查詢：有哪些IDF地點？\n回傳：['台北市', '新北市', '台中市']\n```\n\n### 查詢失敗時自動提示\n- 若查詢時輸入錯誤，會自動回傳所有支援查詢的項目：\n\n```\n輸入：查詢不存在的材料名稱\n回傳：查無此材料，支援查詢的材料有：一般黏土, 砂土, 礫石, 混凝土, ...\n```\n## 土石籠擋土牆穩定分析\n\n此功能用於分析土石籠擋土牆的穩定性，包括主動和被動土壓力計算。\n\n### 使用方法\n\n```python\nresult = check_gabion_stability(\n    height=3.0,          # 土石籠高度 (m)\n    width=2.0,           # 土石籠寬度 (m)\n    wall_weight=100,     # 擋土牆總重 (kN/m)\n    phi=30,              # 土壤內摩擦角 (°)\n    delta=20,            # 牆背摩擦角 (°)，預設 0\n    theta=0,             # 牆背傾斜角 (°)，預設 0\n    i=0,                 # 地表傾斜角 (°)，預設 0\n    gamma=18,            # 土壤飽和單位重 (kN/m³)，預設 18\n    friction_coef=0.5,   # 摩擦係數，預設 0.5\n    pressure_mode=\"active\"  # 土壓力模式 (\"active\" 或 \"passive\")，預設 \"active\"\n)\n```\n\n### 回傳結果\n\n函數回傳一個字典，包含以下內容：\n\n- `success`: 布林值，表示計算是否成功\n- `data`: 計算結果數據，包含：\n  - `earth_pressure_coef`: 土壓力係數\n  - `total_pressure`: 總土壓力 (kN/m)\n  - `vertical_force`: 垂直力分量 (kN/m)\n  - `horizontal_force`: 水平力分量 (kN/m)\n  - `restoring_moment`: 抗傾覆力矩 (kN·m/m)\n  - `overturning_moment`: 傾覆力矩 (kN·m/m)\n  - `overturning_safety_factor`: 抗傾覆安全係數\n  - `sliding_safety_factor`: 抗滑動安全係數\n- `message`: 計算結果摘要\n- `report`: 完整的計算報告書（Markdown 格式）\n\n### 計算報告書內容\n\n報告書包含以下章節：\n\n1. 輸入參數\n2. 計算公式\n3. 計算結果\n4. 穩定評估\n\n### U型溝鋼筋量計算\n- 輸入：\n  - height: 溝高 (m)\n  - wall_slope: 溝壁傾角 (m)\n  - soil_slope: 土方傾角 (°)\n  - soil_angle: 安息角 (°)\n  - effective_depth: 有效厚度 (m)\n  - soil_weight: 土重 (kN/m³)，預設 18.0\n- 回傳：dict，含計算結果與報告書\n\n#### 範例\n```\n輸入：混凝土溝，height=1.5, wall_slope=0.5, soil_slope=15, soil_angle=30, effective_depth=0.2，計算水溝所需的鋼筋量，建議的配筋?\n回傳：\n{\n  \"success\": true,\n  \"data\": {\n    \"earth_pressure_coef\": 0.3333,\n    \"earth_pressure\": 6.750,\n    \"moment\": 3.375,\n    \"rebar_area\": 13.780\n  },\n  \"message\": \"土壓力係數 Ka = 0.3333, 土壓力 P = 6.750 kN/m, 彎矩 M = 3.375 kN·m/m, 鋼筋量 As = 13.780 cm²/m\",\n  \"report\": \"【U型溝鋼筋量計算報告】\\n\\n輸入參數：\\n- 溝高 H = 1.500 m\\n- 溝壁傾角 m = 0.500\\n- 土方傾角 i = 15.00°\\n- 安息角 ψ = 30.00°\\n- 有效厚度 d = 0.200 m\\n- 土重 γ = 18.0 kN/m³\\n\\n計算公式：\\n1. 土壓力係數 Ka = cos²(ψ+m) / [cos²m·(1+√Q)²]\\n   其中 Q = [sinψ·sin(ψ-i)] / [cos(m+i)·cosm]\\n2. 土壓力 P = γ·H²·Ka / (2·cosm)\\n3. 彎矩 M = γ·H³·Ka / (6·cosm)\\n4. 鋼筋量 As = M / (fs·d) × 10⁶ / 1000\\n\\n計算結果：\\n- 土壓力係數 Ka = 0.3333\\n- 土壓力 P = 6.750 kN/m\\n- 彎矩 M = 3.375 kN·m/m\\n- 鋼筋量 As = 13.780 cm²/m\"\n}\n```\n## 鋼筋查詢功能\n本工具提供以下鋼筋資料查詢功能：\n\n1. 列出所有可用的鋼筋編號\n   ```python\n   list_rebar_numbers()\n   ```\n\n2. 查詢特定鋼筋編號的規格資料\n   ```python\n   get_rebar_specs(rebar_number=\"#3\")\n   ```\n\n3. 計算鋼筋重量\n   ```python\n   calculate_rebar_weight(rebar_number=\"#3\", length=10.0)\n   ```\n\n可用的鋼筋編號包括：#3、#4、#5、#6、#7、#8、#9、#10、#11，每個編號對應的規格資料包括：\n- 直徑（mm）\n- 截面積（cm²）\n- 單位重量（kg/m）\n- 周長（mm）\n\n### 鋼筋規格查詢\n輸入「鋼筋規格 #3」或「鋼筋資料 #3」可查詢特定鋼筋的詳細資料，包括：\n- 鋼筋編號\n- 直徑 (mm)\n- 截面積 (cm²)\n- 單位重量 (kg/m)\n- 周長 (mm)\n\n### 鋼筋重量計算\n輸入「鋼筋重量 長度 6 #3」可計算指定長度的鋼筋重量，例如：\n- 長度：6m\n- 鋼筋：#3\n- 重量：2.04 kg\n\n### 鋼筋截面積查詢\n輸入「鋼筋截面積 #3」可查詢特定鋼筋的截面積，例如：\n- #3 鋼筋截面積：0.71 cm²\n\n### 所有鋼筋列表\n輸入「所有鋼筋」可列出所有可用的鋼筋編號，包括：\n- #3 至 #11 鋼筋\n- 各鋼筋的基本規格\n\n### U 型溝配筋建議\n輸入「U型溝配筋 面積 10cm2」可查詢建議的配筋方式，系統會：\n1. 根據輸入的鋼筋斷面積 (cm²/m)\n2. 自動計算並建議主筋與副筋的配筋方式\n3. 間距會取整到最接近的 5cm\n4. 提供完整的配筋建議報告\n\n### 逕流係數查詢\n逕流係數查詢工具可依據水土保持技術規範第18條，查詢不同土地利用類型或集水區狀況的逕流係數C值。\n\n#### 範例\n```\n輸入：請查詢農業區的逕流係數\n回傳：逕流係數 C = 0.53\n來源：農業區逕流係數範圍：0.45~0.60，採用平均值0.53\n依據：水土保持技術規範第18條及附件逕流係數表\n說明：逕流係數C值反映集水區地表特性對降雨逕流之影響，開發中狀態C值以1.0計算。\n\n輸入：開發中的平坦耕地逕流係數是多少？\n回傳：逕流係數 C = 0.85\n來源：開發中狀態，平坦耕地\n依據：水土保持技術規範第18條及附件逕流係數表\n說明：逕流係數C值反映集水區地表特性對降雨逕流之影響，開發中狀態C值較高。\n```\n\n查詢結果包含：\n- 逕流係數值\n- 來源說明\n- 規範依據\n- 土地利用類型\n- 開發狀態\n- 數值範圍（如適用）\n\n### USLE 因子查詢工具\nUSLE 因子查詢工具可依據水土保持技術規範第35條，查詢土壤流失量計算所需的各項因子值。\n\n#### 降雨沖蝕指數 R 值查詢\n```\n輸入：請查詢台北市的降雨沖蝕指數R值\n回傳：地區 台北市 的降雨沖蝕指數 R = 350，單位：百萬焦耳·公釐/公頃·小時·年\n\n輸入：年平均降雨量2000mm的降雨沖蝕指數R值是多少？\n回傳：年平均降雨量 2000 mm 推估的降雨沖蝕指數 R = 600，單位：百萬焦耳·公釐/公頃·小時·年\n```\n\n#### 土壤沖蝕指數 K 值查詢\n```\n輸入：砂土的沖蝕指數K值是多少？\n回傳：土壤類型 砂土 的沖蝕指數 K = 0.30，單位：公噸·公頃·年/公頃·百萬焦耳·公釐\n\n輸入：請查詢黏土的K值\n回傳：土壤類型 黏土 的沖蝕指數 K = 0.20，單位：公噸·公頃·年/公頃·百萬焦耳·公釐\n```\n\n#### 覆蓋與管理因子 C 值查詢\n```\n輸入：草地的覆蓋與管理因子C值是多少？\n回傳：土地利用類型 草地 的覆蓋與管理因子 C = 0.05，單位：無因次\n\n輸入：請查詢農地的C值\n回傳：土地利用類型 農地 的覆蓋與管理因子 C = 0.30，單位：無因次\n```\n\n#### 水土保持處理因子 P 值查詢\n```\n輸入：等高耕作的水土保持處理因子P值是多少？\n回傳：水土保持措施 等高耕作 的處理因子 P = 0.50，單位：無因次\n\n輸入：請查詢梯田的P值\n回傳：水土保持措施 梯田 的處理因子 P = 0.20，單位：無因次\n```\n\n查詢結果包含：\n- 因子值\n- 單位\n- 來源說明\n- 規範依據\n\n\n## 使用範例(功能導向，使用自然語言對話，讓大模型自動調用相關工具來求解)\n\n- 有一集水區面積約5ha，平坦耕地，新北市，重現期50年，降雨延時60min，調用工具取得該地區的降雨強度值，計算該集水區最大逕流量?\n- 有一集水區面積約5ha，平坦耕地，新北市，降雨延時60min，調用工具取得該地區的降雨強度值，請分別計算重現期25年、50年該集水區最大逕流量? 並輸出完整計算式的報告書。\n- 依據25年的逕流量，設計一條寬50cm，深70cm的混凝土溝，設計坡度容許範圍?\n- 依據這個逕流量，設計一條寬50cm，溝深60cm的混凝土溝，坡度2.5%，檢核該設計是否OK，並出具一份完整的檢核報告書。\n- 水溝編號,B(cm),H(cm),坡度(%),流量(CMS),水溝鋪面型式\n  1-1,40,60,1.5,0.25,鋼筋混凝土矩形溝\n  1-2,40,70,1.5,0.30,鋼筋混凝土矩形溝\n  檢核上列水溝設計，以markdown表格型式輸出結果。 \n- 有一個破碎岩盤的坡面，角度約60度，位於熱帶多雨地區，請提出坡面保護建議。\n- 無基礎的混凝土重力式擋土牆，全高2.3M，牆頂寬50cm，牆底寬100cm，土壤單位重18kN/m3，摩擦角30度，凝聚力10kPa，地下水位在牆頂以下2M，地震係數0.15，土層基礎，請調用MCP工具，檢核其常時與地震情況下的穩定性，並輸出完整計算報告書。\n- 混凝土溝，height=1.5, wall_slope=0.5, soil_slope=15, soil_angle=30, effective_depth=0.2，調用工具，計算水溝所需的鋼筋量及建議配筋?\n- 台中的 \"年平均土壤流失量\"如何估算? 需提供哪些參數? \n- 請計算一條直徑60公分的混凝土圓形管渠，流量0.5cms，坡度1.2%，曼寧係數0.013的流速和流深\n- 計算寬30公分，高50cm的鋼筋混凝土矩形溝，流量0.5cms，坡度1.2%，流速和流深?\n- 計算寬50公分，高70cm的鋼筋混凝土矩形溝，流量1cms，坡度5%，流速和流深? 輸出完整報告，含計算過程及計算式。\n- 計算底寬50公分，頂寬100cm，高70cm的\"平常砂土梯形溝\"，流量0.2cms，坡度0.7%，流速和流深? 輸出完整報告，含計算過程及計算式。\n- 有一集水區面積約5ha，平坦耕地，新北市，重現期50年，降雨延時60min，調用工具取得該地區的降雨強度值，計算該集水區最大逕流量? \n- 依據這個逕流量，設計一條寬50cm，溝深60cm的混凝土溝，坡度2.5%，檢核該設計是否OK，並出具一份完整的檢核報告書(含集水區最大逕流量)。\n- 使用 Gemini 2.5 Pro 的輸出結果如下:調用工具：query_idf_curve(降雨強度-歷時-頻率（IDF）曲線查詢)、query_runoff_coeff(逕流係數C值查詢工具)、calc_catchment_runoff(集水區最大逕流量計算)、get_manning_n(常見材料的曼寧係數查詢)、calc_channel_section_flow(排水斷面流速/流深/流量計算)\n\n# 混凝土溝設計檢核報告書\n\n## 1. 設計目標\n\n本報告旨在檢核所設計之混凝土溝斷面，是否能安全容納上游集水區於特定重現期下所產生之最大逕流量。\n\n## 2. 集水區最大逕流量計算摘要\n\n*   **集水區條件:**\n    *   面積 (A): 5.0 公頃 (ha)\n    *   土地利用: 平坦耕地\n    *   地點: 新北市\n*   **設計條件:**\n    *   重現期 (T): 50 年\n    *   降雨延時 (D): 60 分鐘\n*   **計算參數:**\n    *   降雨強度 (I): 140.0 mm/hr (依據新北市50年60分鐘IDF曲線)\n    *   逕流係數 (C): 0.53 (平坦耕地，取規範平均值)\n*   **計算公式 (合理化公式):** Q = (C × I × A) / 360\n*   **計算結果 (設計流量 Q_design):**\n    *   Q_design = (0.53 × 140.0 × 5.0) / 360 ≈ **1.03 cms**\n\n## 3. 混凝土溝設計參數\n\n*   斷面型式: 矩形\n*   渠道材質: 混凝土\n*   曼寧糙度係數 (n): 0.012\n*   溝渠寬度 (b): 50 cm (0.5 m)\n*   溝渠深度 (H): 60 cm (0.6 m)\n*   渠道坡度 (S): 2.5 % (0.025 m/m)\n\n## 4. 水理計算與檢核\n\n使用曼寧公式，針對設計流量 (Q = 1.03 cms) 及上述溝渠參數進行水理計算：\n\n*   **計算公式:**\n    *   Q = A × V\n    *   V = (1/n) × R^(2/3) × S^(1/2)\n    *   A = b × y\n    *   P = b + 2y\n    *   R = A / P\n*   **計算結果:**\n    *   實際流速 (V): **4.014 m/s**\n    *   實際水深 (y): **0.513 m (51.3 cm)**\n\n*   **檢核項目:**\n    *   **通水能力 (Capacity Check):**\n        *   計算水深 (y) = 51.3 cm\n        *   設計溝深 (H) = 60.0 cm\n        *   **出水高 (Freeboard) = H - y = 60.0 - 51.3 = 8.7 cm**\n        *   *評估:* 溝渠斷面可容納設計流量，尚有 8.7 cm 的出水高。一般工程設計建議保留約10-20%溝深或至少10-15cm的出水高作為安全餘裕，此處出水高略顯不足，建議可考慮加大溝深以增加安全係數。\n    *   **流速檢核 (Velocity Check):**\n        *   計算流速 (V) = 4.014 m/s\n        *   混凝土容許流速範圍 (V_allowable): 4.57 ~ 6.1 m/s (依據水保技術規範常見值)\n        *   *評估:* 計算流速 4.014 m/s 低於混凝土的容許流速下限 (4.57 m/s)，表示此流速對混凝土渠道本身是安全的，不會造成渠道沖刷。\n\n## 5. 結論與建議\n\n1.  **通水能力:** 設計之混凝土溝斷面 (寬50cm, 深60cm) **可以容納** 1.03 cms 的設計流量。\n2.  **流速:** 計算流速 4.014 m/s 在混凝土的安全容許範圍內。\n3.  **出水高:** 計算所得的出水高為 8.7 cm，相對偏低。\n4.  **建議:** 雖然此斷面在計算上可通過設計流量，但考量到實際施工誤差、泥砂淤積、或未來流量增加的可能性，**建議將溝渠深度適度加大 (例如增加至 70 cm 或以上)**，以提供更充足的出水高，增加整體設計的安全性與韌性。\n\n---\n\n## 授權\n\n本專案採用 MIT License 授權，歡迎自由使用與貢獻。",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "tools",
        "mcp",
        "tutumomo",
        "civil tools",
        "civil engineering",
        "calculation tools"
      ],
      "category": "scientific-research-tools"
    },
    "vishwa684--unet": {
      "owner": "vishwa684",
      "name": "unet",
      "url": "https://github.com/vishwa684/unet",
      "imageUrl": "/freedevtools/mcp/pfp/vishwa684.webp",
      "description": "Train and deploy U-Net models for biomedical image segmentation using the Medical Decathlon dataset, with support for both 2D and 3D U-Net scripts. Visualize predictions and assess model performance through comprehensive demos and visual outputs.",
      "stars": 0,
      "forks": 0,
      "license": "Apache License 2.0",
      "language": "",
      "updated_at": "2024-05-11T00:13:12Z",
      "readme_content": "# Deep Learning Medical Decathlon Demos for Python*\n### U-Net Biomedical Image Segmentation with Medical Decathlon Dataset.\n\nThis repository contains [2D](https://github.com/IntelAI/unet/tree/master/2D) and [3D](https://github.com/IntelAI/unet/tree/master/3D) U-Net scripts for training models using the [Medical Decathlon](http://medicaldecathlon.com/) dataset (http://medicaldecathlon.com/).\n\n![pred152_3D](https://github.com/IntelAI/unet/blob/master/3D/images/BRATS_152_img3D.gif\n\"BRATS image #152:  Purple voxels indicate a perfect prediction by the model. Red are false positives. Blue are false negatives\").  ![pred195](https://github.com/IntelAI/unet/blob/master/3D/images/BRATS_195_img.gif \"BRATS image #195:  Purple voxels indicate a perfect prediction by the model. Red are false positives. Blue are false negatives\")\n\n\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "biomedical",
        "segmentation",
        "dataset",
        "biomedical image",
        "models biomedical",
        "decathlon dataset"
      ],
      "category": "scientific-research-tools"
    },
    "vitamin3615--uniprot-mcp-server": {
      "owner": "vitamin3615",
      "name": "uniprot-mcp-server",
      "url": "https://github.com/vitamin3615/uniprot-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/vitamin3615.webp",
      "description": "Fetch protein information seamlessly from the UniProt database, including detailed data, sequences, functions, and structures. Supports integration with MCP-compatible AI applications for enhanced protein analysis.",
      "stars": 0,
      "forks": 0,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-04-01T01:49:15Z",
      "readme_content": "# UniProt MCP Server\n\nAn MCP (Model Context Protocol) server that allows language models to fetch protein information from the UniProt database. This server can be integrated with Claude Desktop and other MCP-compatible AI applications.\n\n## Features\n\n- **Search UniProt**: Search for proteins by name, gene, or other criteria\n- **Get Protein Details**: Retrieve detailed information about a protein\n- **Get Protein Sequences**: Fetch amino acid sequences\n- **Get Protein Functions**: Retrieve functional annotations including GO terms\n- **Get Protein Structures**: Find related PDB structures\n\n## Installation\n\n1. Clone this repository:\n   ```bash\n   git clone https://github.com/yourusername/uniprot-mcp-server.git\n   cd uniprot-mcp-server\n   ```\n\n2. Create a virtual environment (optional but recommended):\n   ```bash\n   python -m venv venv\n   source venv/bin/activate  # On Windows: venv\\Scripts\\activate\n   ```\n\n3. Install dependencies:\n   ```bash\n   pip install -r requirements.txt\n   ```\n\n## Local Setup with Claude Desktop\n\n1. Make the server script executable:\n   ```bash\n   chmod +x server.py\n   ```\n\n2. Configure Claude Desktop to use this MCP server:\n\n   ### For macOS/Linux:\n   ```bash\n   code ~/Library/Application\\ Support/Claude/claude_desktop_config.json\n   ```\n\n   ### For Windows:\n   ```powershell\n   code $env:AppData\\Claude\\claude_desktop_config.json\n   ```\n\n3. Add the following configuration (update the path to match your actual file location):\n\n   ```json\n   {\n     \"mcpServers\": {\n       \"uniprot\": {\n         \"command\": \"python\",\n         \"args\": [\n           \"/ABSOLUTE/PATH/TO/uniprot-mcp-server/server.py\"\n         ]\n       }\n     }\n   }\n   ```\n\n4. Restart Claude Desktop\n\n## Usage Examples\n\nOnce set up with Claude Desktop, you can ask questions like:\n\n- \"Find proteins related to Alzheimer's disease\"\n- \"Get the sequence for protein P05067 (Amyloid-beta precursor protein)\"\n- \"What are the functions of the TP53 protein?\"\n- \"Are there any known 3D structures for BRCA1 protein?\"\n\n## Development\n\nTo run the server directly for testing:\n\n```bash\npython server.py\n```\n\n## Testing Utilities\n\nThis project includes two testing utilities to verify functionality:\n\n### 1. Server Health Check\n\nVerifies that the MCP server starts correctly without errors:\n\n```bash\npython server_health_check.py\n```\n\n### 2. UniProt API Test\n\nTests connectivity with the UniProt API and verifies that all API calls work as expected:\n\n```bash\npython test_server.py\n```\n\nRun these tests before using the server to ensure everything is properly configured.\n\n## License\n\nMIT\n\n## Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n\n1. Fork the repository\n2. Create your feature branch (`git checkout -b feature/amazing-feature`)\n3. Commit your changes (`git commit -m 'Add some amazing feature'`)\n4. Push to the branch (`git push origin feature/amazing-feature`)\n5. Open a Pull Request ",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "protein",
        "vitamin3615",
        "uniprot",
        "vitamin3615 uniprot",
        "protein information",
        "uniprot database"
      ],
      "category": "scientific-research-tools"
    },
    "vrtejus--pymol-mcp": {
      "owner": "vrtejus",
      "name": "pymol-mcp",
      "url": "https://github.com/vrtejus/pymol-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/vrtejus.webp",
      "description": "Connects PyMOL to Claude AI for natural language interaction with molecular visualization and analysis, enabling intelligent command parsing and structural analysis.",
      "stars": 22,
      "forks": 4,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-25T21:34:23Z",
      "readme_content": "# PyMOL-MCP: Integrating PyMOL with Claude AI\n\nPyMOL-MCP connects PyMOL to Claude AI through the Model Context Protocol (MCP), enabling Claude to directly interact with and control PyMOL. This powerful integration allows for conversational structural biology, molecular visualization, and analysis through natural language.\n\n\n\nhttps://github.com/user-attachments/assets/687f43dc-d45e-477e-ac2b-7438e175cb36\n\n\n\n## Features\n\n- **Two-way communication**: Connect Claude AI to PyMOL through a socket-based server\n- **Intelligent command parsing**: Natural language processing for PyMOL commands\n- **Molecular visualization control**: Manipulate representations, colors, and views\n- **Structural analysis**: Perform measurements, alignments, and other analyses\n- **Code execution**: Run arbitrary Python code in PyMOL from Claude\n\n## Installation Guide\n\n### Prerequisites\n\n- PyMOL installed on your system\n- Claude for Desktop\n- Python 3.10 or newer\n- Git\n\n### Step 1: Install the UV Package Manager\n\n**On macOS:**\n\n```bash\nbrew install uv\n```\n\n**On Windows:**\n\n```bash\npowershell -c \"irm https://astral.sh/uv/install.ps1 | iex\"\nset Path=C:\\Users\\[YourUsername]\\.local\\bin;%Path%\n```\n\nFor other platforms, visit the [UV installation guide](https://docs.astral.sh/uv/getting-started/installation/).\n\n### Step 2: Clone the Repository\n\n```bash\ngit clone https://github.com/vrtejus/pymol-mcp\ncd pymol-mcp\n```\n\n### Step 3: Set Up the Environment\n\nCreate and activate a Python virtual environment:\n\n```bash\npython -m venv venv\n```\n\n**On macOS/Linux:**\n\n```bash\nsource venv/bin/activate\n```\n\n**On Windows:**\n\n```bash\nvenv\\Scripts\\activate\n```\n\n### Step 4: Install Dependencies\n\nWith the virtual environment activated:\n\n```bash\npip install mcp\n```\n\n### Step 5: Configure Claude Desktop\n\n1. Open Claude Desktop\n2. Go to Claude > Settings > Developer > Edit Config\n3. This will open the `claude_desktop_config.json` file\n4. Add the MCP server configuration:\n\n```json\n{\n  \"mcpServers\": {\n    \"pymol\": {\n      \"command\": \"[Full path to your venv python]\",\n      \"args\": [\"[Full path to pymol_mcp_server.py]\"]\n    }\n  }\n}\n```\n\nFor example:\n\n```json\n{\n  \"mcpServers\": {\n    \"pymol\": {\n      \"command\": \"/Users/username/pymol-mcp/venv/bin/python\",\n      \"args\": [\"/Users/username/pymol-mcp/pymol_mcp_server.py\"]\n    }\n  }\n}\n```\n\n> **Note:** Use the actual full paths on your system. On Windows, use forward slashes (/) instead of backslashes.\n\n### Step 6: Install the PyMOL Plugin\n\n1. Open PyMOL\n2. Go to Plugin → Plugin Manager\n3. Click on \"Install New Plugin\" tab\n4. Select \"Choose file...\" and navigate to the cloned repository\n5. Select the `pymol-mcp-socket-plugin/__init__.py` file\n6. Click \"Open\" and follow the prompts to install the plugin\n\n## Usage\n\n### Starting the Connection\n\n1. In PyMOL:\n\n   - Go to Plugin → PyMOL MCP Socket Plugin\n   - Click \"Start Listening\"\n   - The status should change to \"Listening on port 9876\"\n\n2. In Claude Desktop:\n   - You should see a hammer icon in the tools section when chatting\n   - Click it to access the PyMOL tools\n\n### Example Commands\n\nHere are some examples of what you can ask Claude to do:\n\n- \"Load PDB 1UBQ and display it as cartoon\"\n- \"Color the protein by secondary structure\"\n- \"Highlight the active site residues with sticks representation\"\n- \"Align two structures and show their differences\"\n- \"Calculate the distance between these two residues\"\n- \"Save this view as a high-resolution image\"\n\n## Troubleshooting\n\n- **Connection issues**: Make sure the PyMOL plugin is listening before attempting to connect from Claude\n- **Command errors**: Check the PyMOL output window for any error messages\n- **Plugin not appearing**: Restart PyMOL and check that the plugin was correctly installed\n- **Claude not connecting**: Verify the paths in your Claude configuration file are correct\n\n## Limitations & Notes\n\n- The socket connection requires both PyMOL and Claude to be running on the same machine\n- Some complex operations may need to be broken down into simpler steps\n- Always save your work before using experimental features\n- Join our Bio-MCP Community to troubleshoot, provide feedback & improve Bio-MCPS! https://join.slack.com/t/bio-mcpslack/shared_invite/zt-31z4pho39-K5tb6sZ1hUvrFyoPmKihAA\n\n## Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n\n## License\n\nThis project is licensed under the MIT License - see the LICENSE file for details.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "pymol",
        "molecular",
        "parsing",
        "molecular visualization",
        "vrtejus pymol",
        "connects pymol"
      ],
      "category": "scientific-research-tools"
    },
    "waldzellai--model-enhancement-servers": {
      "owner": "waldzellai",
      "name": "model-enhancement-servers",
      "url": "https://github.com/waldzellai/model-enhancement-servers",
      "imageUrl": "/freedevtools/mcp/pfp/waldzellai.webp",
      "description": "Enables language models to perform visual and spatial reasoning by creating and manipulating diagrammatic representations such as graphs and flowcharts. Supports multi-modal outputs and iterative refinement through a flexible visual element manipulation system.",
      "stars": 63,
      "forks": 4,
      "license": "MIT License",
      "language": "JavaScript",
      "updated_at": "2025-09-30T04:52:01Z",
      "readme_content": "# Cognitive Enhancement MCP Servers\n\nA collection of Model Context Protocol servers that provide cognitive enhancement tools for large language models.\n\n## Servers\n\nThis monorepo contains the following MCP servers:\n\n1. **Structured Argumentation** - A server for formal dialectical reasoning\n2. **Visual Reasoning** - A server for diagrammatic thinking and spatial representation\n3. **Scientific Method** - A server for hypothesis testing and evidence evaluation\n4. **Analogical Reasoning** - A server for structured metaphorical thinking\n5. **Metacognitive Monitoring** - A server for knowledge assessment and confidence tracking\n6. **Decision Framework** - A server for structured decision analysis\n7. **Collaborative Reasoning** - A server for multi-perspective problem solving\n8. **Ethical Reasoning** - A server for evaluating actions with moral frameworks\n9. **Bias Detection** - A server for flagging potentially biased wording\n10. **Constraint Solver** - A server for validating logical and numeric constraints\n11. **Narrative Planner** - A server for generating simple story outlines\n12. **Goal Tracker** - A server for maintaining and completing objectives\n13. **Multimodal Synthesizer** - A server for combining text and image descriptions\n\n## Potential Future Servers\n\nThe following server ideas are under consideration to further extend model reasoning capabilities:\n\n1. **Emotion-Aware Interaction** - Provides sentiment tracking and mood-aware responses to improve empathic communication.\n2. **Long-Term Memory** - Maintains persistent context across sessions for continuity and recall of past interactions.\n\n\n## Installation\n\nEach server can be installed individually:\n\n```bash\n# Using npm\nnpm install @waldzellai/structured-argumentation\n\n# Using yarn\nyarn add @waldzellai/structured-argumentation\n```\n\n## Usage with Claude Desktop\n\nAdd this to your `claude_desktop_config.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"structured-argumentation\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"@waldzellai/structured-argumentation\"\n      ]\n    }\n  }\n}\n```\n\n## Docker\n\nAll servers are available as Docker images:\n\n```bash\ndocker run --rm -i waldzellai/structured-argumentation\n```\n\n## Development\n\nClone the repository and install dependencies:\n\n```bash\ngit clone https://github.com/waldzellai/model-enhancement-servers.git\ncd model-enhancement-servers\nnpm install\n```\n\nBuild all packages:\n\n```bash\nnpm run build\n```\n\n## License\n\nThis project is licensed under the MIT License - see the LICENSE file for details.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "diagrammatic",
        "visual",
        "models",
        "diagrammatic representations",
        "flexible visual",
        "research tools"
      ],
      "category": "scientific-research-tools"
    },
    "wheattoast11--openrouter-deep-research-mcp": {
      "owner": "wheattoast11",
      "name": "openrouter-deep-research-mcp",
      "url": "https://github.com/wheattoast11/openrouter-deep-research-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/wheattoast11.webp",
      "description": "Connects to specialized AI agents for conducting complex research tasks through an orchestration system that enables multi-stage planning and parallel execution, resulting in high-quality, tailored research outcomes.",
      "stars": 29,
      "forks": 7,
      "license": "No License",
      "language": "JavaScript",
      "updated_at": "2025-09-29T22:15:51Z",
      "readme_content": "[![Star on GitHub](https://img.shields.io/github/stars/wheattoast11/openrouter-deep-research?style=social)](https://github.com/wheattoast11/openrouter-deep-research)\n# OpenRouter Agents MCP Server\n\n[![npm version](https://img.shields.io/npm/v/%40terminals-tech%2Fopenrouter-agents?color=2ea043)](https://www.npmjs.com/package/@terminals-tech/openrouter-agents) [![GitHub Packages](https://img.shields.io/badge/GitHub%20Packages-available-24292e?logo=github)](../../packages)\n\n[UPDATE – 2025-08-26] Two modes (set MODE env):\n- AGENT: one simple tool (`agent`) that routes research / follow_up / retrieve / query\n- MANUAL: individual tools for each action\n- ALL (default): both AGENT and MANUAL, plus always-on ops tools\n\nDiagram (simple)\n```\n[Always-On Ops]  ping • get_server_status • job_status • cancel_job\n\nAGENT MODE\nclient → agent → (research | follow_up | retrieve | query)\n\nMANUAL MODE\nclient → (submit_research | conduct_research | retrieve | query | research_follow_up | get_report_content | list_research_history)\n```\n\n- Killer features\n  - Plan → parallelize → synthesize workflow with bounded parallelism\n  - Dynamic model catalog; supports Anthropic Sonnet‑4 and OpenAI GPT‑5 family\n  - Built‑in semantic KB (PGlite + pgvector) with backup, export/import, health, and reindex tools\n  - Lightweight web helpers: quick search and page fetch for context\n  - Robust streaming (SSE), per‑connection auth, clean logs\n\n## Install / Run\n- Install (project dependency)\n```bash\nnpm install @terminals-tech/openrouter-agents\n```\n\n- Global install (optional)\n```bash\nnpm install -g @terminals-tech/openrouter-agents\n```\n\n- Run with npx (no install)\n```bash\nnpx @terminals-tech/openrouter-agents --stdio\n# or daemon\nSERVER_API_KEY=devkey npx @terminals-tech/openrouter-agents\n```\n\n## What’s new (v1.5.0)\n- Version parity across npm, GitHub Releases, and GitHub Packages\n- Dual publish workflow enabled\n\n[Changelog →](docs/CHANGELOG.md)\n\n## Quick start\n1) Prereqs\n- Node 18+ (20 LTS recommended), npm, Git, OpenRouter API key\n\n2) Install\n```bash\nnpm install\n```\n\n3) Configure (.env)\n```dotenv\nOPENROUTER_API_KEY=your_openrouter_key\nSERVER_API_KEY=your_http_transport_key\nSERVER_PORT=3002\n\n# Modes (pick one; default ALL)\n# AGENT  = agent-only + always-on ops (ping/status/jobs)\n# MANUAL = individual tools + always-on ops\n# ALL    = agent + individual tools + always-on ops\nMODE=ALL\n\n# Orchestration\nENSEMBLE_SIZE=2\nPARALLELISM=4\n\n# Models (override as needed) - Updated with state-of-the-art cost-effective models\nPLANNING_MODEL=openai/gpt-5-chat\nPLANNING_CANDIDATES=openai/gpt-5-chat,google/gemini-2.5-pro,anthropic/claude-sonnet-4\nHIGH_COST_MODELS=x-ai/grok-4,openai/gpt-5-chat,google/gemini-2.5-pro,anthropic/claude-sonnet-4,morph/morph-v3-large\nLOW_COST_MODELS=deepseek/deepseek-chat-v3.1,z-ai/glm-4.5v,qwen/qwen3-coder,openai/gpt-5-mini,google/gemini-2.5-flash\nVERY_LOW_COST_MODELS=openai/gpt-5-nano,deepseek/deepseek-chat-v3.1\n\n# Storage\nPGLITE_DATA_DIR=./researchAgentDB\nPGLITE_RELAXED_DURABILITY=true\nREPORT_OUTPUT_PATH=./research_outputs/\n\n# Indexer\nINDEXER_ENABLED=true\nINDEXER_AUTO_INDEX_REPORTS=true\nINDEXER_AUTO_INDEX_FETCHED=true\n\n# MCP features\nMCP_ENABLE_PROMPTS=true\nMCP_ENABLE_RESOURCES=true\n\n# Prompt strategy\nPROMPTS_COMPACT=true\nPROMPTS_REQUIRE_URLS=true\nPROMPTS_CONFIDENCE=true\n\n```\n\n4) Run\n- STDIO (for Cursor/VS Code MCP):\n```bash\nnode src/server/mcpServer.js --stdio\n```\n- HTTP/SSE (local daemon):\n```bash\nSERVER_API_KEY=$SERVER_API_KEY node src/server/mcpServer.js\n```\n\n### Windows PowerShell examples\n- STDIO\n```powershell\n$env:OPENROUTER_API_KEY='your_key'\n$env:INDEXER_ENABLED='true'\nnode src/server/mcpServer.js --stdio\n```\n- HTTP/SSE\n```powershell\n$env:OPENROUTER_API_KEY='your_key'\n$env:SERVER_API_KEY='devkey'\n$env:SERVER_PORT='3002'\nnode src/server/mcpServer.js\n```\n\n### One-liner demo scripts\nDev (HTTP/SSE):\n```bash\nSERVER_API_KEY=devkey INDEXER_ENABLED=true node src/server/mcpServer.js\n```\n\nSTDIO (Cursor/VS Code):\n```bash\nOPENROUTER_API_KEY=your_key INDEXER_ENABLED=true node src/server/mcpServer.js --stdio\n```\n\n### MCP client JSON configuration (no manual start required)\nYou can register this server directly in MCP clients that support JSON server manifests.\n\nMinimal examples:\n\n1) STDIO transport (recommended for IDEs)\n```json\n{\n  \"servers\": {\n    \"openrouter-agents\": {\n      \"command\": \"npx\",\n      \"args\": [\"@terminals-tech/openrouter-agents\", \"--stdio\"],\n      \"env\": {\n        \"OPENROUTER_API_KEY\": \"${OPENROUTER_API_KEY}\",\n        \"SERVER_API_KEY\": \"${SERVER_API_KEY}\",\n        \"PGLITE_DATA_DIR\": \"./researchAgentDB\",\n        \"INDEXER_ENABLED\": \"true\"\n      }\n    }\n  }\n}\n```\n\n2) HTTP/SSE transport (daemon mode)\n```json\n{\n  \"servers\": {\n    \"openrouter-agents\": {\n      \"url\": \"http://127.0.0.1:3002\",\n      \"sse\": \"/sse\",\n      \"messages\": \"/messages\",\n      \"headers\": {\n        \"Authorization\": \"Bearer ${SERVER_API_KEY}\"\n      }\n    }\n  }\n}\n```\n\nWith the package installed globally (or via npx), MCP clients can spawn the server automatically. See your client’s docs for where to place this JSON (e.g., `~/.config/client/mcp.json`).\n\n## Tools (high‑value)\n- Always‑on (all modes): `ping`, `get_server_status`, `job_status`, `get_job_status`, `cancel_job`\n- AGENT: `agent` (single entrypoint for research / follow_up / retrieve / query)\n- MANUAL/ALL toolset: `submit_research` (async), `conduct_research` (sync/stream), `research_follow_up`, `search` (hybrid), `retrieve` (index/sql), `query` (SELECT), `get_report_content`, `list_research_history`\n- Jobs: `get_job_status`, `cancel_job`\n- Retrieval: `search` (hybrid BM25+vector with optional LLM rerank), `retrieve` (index/sql wrapper)\n- SQL: `query` (SELECT‑only, optional `explain`)\n- Knowledge base: `get_past_research`, `list_research_history`, `get_report_content`\n- DB ops: `backup_db` (tar.gz), `export_reports`, `import_reports`, `db_health`, `reindex_vectors`\n- Models: `list_models`\n- Web: `search_web`, `fetch_url`\n- Indexer: `index_texts`, `index_url`, `search_index`, `index_status`\n\n### Tool usage patterns (for LLMs)\nUse `tool_patterns` resource to view JSON recipes describing effective chaining, e.g.:\n- Search → Fetch → Research\n- Async research: submit, stream via SSE `/jobs/:id/events`, then get report content\n\nNotes\n- Data lives locally under `PGLITE_DATA_DIR` (default `./researchAgentDB`). Backups are tarballs in `./backups`.\n- Use `list_models` to discover current provider capabilities and ids.\n\n## Architecture at a glance\nSee `docs/diagram-architecture.mmd` (Mermaid). Render to SVG with Mermaid CLI if installed:\n```bash\nnpx @mermaid-js/mermaid-cli -i docs/diagram-architecture.mmd -o docs/diagram-architecture.svg\n```\nOr use the script:\n```bash\nnpm run gen:diagram\n```\n\n\n\nIf the image doesn’t render in your viewer, open `docs/diagram-architecture-branded.svg` directly.\n\n### Answer crystallization view\n\n\nHow it differs from typical “agent chains”:\n- Not just hardcoded handoffs; the plan is computed, then parallel agents search, then a synthesis step reasons over consensus, contradictions, and gaps.\n- The system indexes what it reads during research, so subsequent queries get faster/smarter.\n- Guardrails shape attention: explicit URL citations, [Unverified] labelling, and confidence scoring.\n\n## Minimal‑token prompt strategy\n- Compact mode strips preambles to essential constraints; everything else is inferred.\n- Enforced rules: explicit URL citations, no guessing IDs/URLs, confidence labels.\n- Short tool specs: use concise param names and rely on server defaults.\n\n## Common user journeys\n- “Give me an executive briefing on MCP status as of July 2025.”\n  - Server plans sub‑queries, fetches authoritative sources, synthesizes with citations.\n  - Indexed outputs make related follow‑ups faster.\n\n- “Find vision‑capable models and route images gracefully.”\n  - `/models` discovered and filtered, router template generated, fallback to text models.\n\n- “Compare orchestration patterns for bounded parallelism.”\n  - Pulls OTel/Airflow/Temporal docs, produces a MECE synthesis and code pointers.\n\n## Cursor IDE usage\n- Add this server in Cursor MCP settings pointing to `node src/server/mcpServer.js --stdio`.\n- Use the new prompts (`planning_prompt`, `synthesis_prompt`) directly in Cursor to scaffold tasks.\n\n## FAQ (quick glance)\n- How does it avoid hallucinations?\n  - Strict citation rules, [Unverified] labels, retrieval of past work, on‑the‑fly indexing.\n- Can I disable features?\n  - Yes, via env flags listed above.\n- Does it support streaming?\n  - Yes, SSE for HTTP; stdio for MCP.\n\n## Command Map (quick reference)\n- Start (stdio): `npm run stdio`\n- Start (HTTP/SSE): `npm start`\n- Run via npx (scoped): `npx @terminals-tech/openrouter-agents --stdio`\n- Generate examples: `npm run gen:examples`\n- List models: MCP `list_models { refresh:false }`\n- Submit research (async): `submit_research { q:\"<query>\", cost:\"low\", aud:\"intermediate\", fmt:\"report\", src:true }`\n- Track job: `get_job_status { job_id:\"...\" }`, cancel: `cancel_job { job_id:\"...\" }`\n- Unified search: `search { q:\"<query>\", k:10, scope:\"both\" }`\n- SQL (read‑only): `query { sql:\"SELECT ... WHERE id = $1\", params:[1], explain:true }`\n- Get past research: `get_past_research { query:\"<query>\", limit:5 }`\n- Index URL (if enabled): `index_url { url:\"https://...\" }`\n- Micro UI (ghost): visit `http://localhost:3002/ui` to stream job events (SSE).\n\n## Package publishing\n- Name: `@terminals-tech/openrouter-agents`\n- Version: 1.3.2\n- Bin: `openrouter-agents`\n- Author: Tej Desai <admin@terminals.tech>\n- Homepage: https://terminals.tech\n\nInstall and run without cloning:\n```bash\nnpx @terminals-tech/openrouter-agents --stdio\n# or daemon\nSERVER_API_KEY=your_key npx @terminals-tech/openrouter-agents\n```\n\n### Publish (scoped)\n```bash\nnpm login\nnpm version 1.3.2 -m \"chore(release): %s\"\ngit push --follow-tags\nnpm publish --access public --provenance\n```\n\n## Validation – MSeeP (Multi‑Source Evidence & Evaluation Protocol)\n- **Citations enforced**: explicit URLs, confidence tags; unknowns marked `[Unverified]`.\n- **Cross‑model triangulation**: plan fans out to multiple models; synthesis scores consensus vs contradictions.\n- **KB grounding**: local hybrid index (BM25+vector) retrieves past work for cross‑checking.\n- **Human feedback**: `rate_research_report { rating, comment }` stored to DB; drives follow‑ups.\n- **Reproducibility**: `export_reports` + `backup_db` capture artifacts for audit.\n\n## Quality feedback loop\n- Run examples: `npm run gen:examples`\n- Review: `list_research_history`, `get_report_content {reportId}`\n- Rate: `rate_research_report { reportId, rating:1..5, comment }`\n- Improve retrieval: `reindex_vectors`, `index_status`, `search_index { query }`\n\n## Architecture diagram (branded)\n- See `docs/diagram-architecture-branded.svg` (logo links to `https://terminals.tech`).\n\n## Stargazers\n[![Star on GitHub](https://img.shields.io/github/stars/wheattoast11/openrouter-deep-research?style=social)](https://github.com/wheattoast11/openrouter-deep-research)\n\n[![Star History Chart](https://api.star-history.com/svg?repos=wheattoast11/openrouter-deep-research&type=Date)](https://star-history.com/#wheattoast11/openrouter-deep-research)",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "ai",
        "research",
        "planning",
        "deep research",
        "research tasks",
        "research tools"
      ],
      "category": "scientific-research-tools"
    },
    "wiSCADA--gpu.cpp": {
      "owner": "wiSCADA",
      "name": "gpu.cpp",
      "url": "https://github.com/wiSCADA/gpu.cpp",
      "imageUrl": "/freedevtools/mcp/pfp/wiSCADA.webp",
      "description": "A lightweight C++ library for portable GPU computation that uses the WebGPU specification, enabling seamless integration of GPU capabilities across a variety of hardware platforms. It supports multiple GPU backends, including Vulkan, Metal, and DirectX, facilitating efficient development and fast iterations.",
      "stars": 0,
      "forks": 0,
      "license": "Apache License 2.0",
      "language": "",
      "updated_at": "2024-07-30T09:11:07Z",
      "readme_content": "# gpu.cpp\n\ngpu.cpp is a lightweight library that makes portable GPU compute with C++ simple.\n\nIt focuses on general purpose native GPU computation, leveraging the WebGPU\nspecification as a portable low-level GPU interface. This means we can drop in\nGPU code in C++ projects and have it run on Nvidia, Intel, AMD, and other GPUs.\nThe same C++ code can work on a wide variety of laptops, workstations, mobile\ndevices or virtually any hardware with Vulkan, Metal, or DirectX support.\n\n## Technical Objectives: Lightweight, Fast Iteration, and Low Boilerplate\n\nWith gpu.cpp we want to enable a high-leverage library for individual developers and researchers to incorporate GPU computation into programs relying on nothing more than a standard C++ compiler as tooling. Our goals are:\n\n- High power-to-weight ratio API: Provide the smallest API surface area that can cover the full range of GPU compute needs.\n- Fast compile/run cycles: Ensure projects can build nearly instantaneously, compile/run cycles should be <5 seconds on a modern laptop.\n- Minimal dependencies and tooling overhead: A standard clang C++ compiler should be enough, no external library dependencies beyond the WebGPU native implementation.\n\nThe implementation aims for a small API surface area with minimum boilerplate. There are a small number of library operations to carry out an broad range of low-level GPU operations. We avoid abstractions that add layers of indirection, making the mapping between the gpu.cpp library to raw WebGPU API clear when it's needed.\n\nIn this spirit of fast experimentation, we also want near-instantaneous C++ builds taking no more than a second or two even on modestly capable personal computing devices. With this in mind, we not only keep the API surface area small, but also keep the implementation small and we also provide a prebuilt binary of the Dawn native WebGPU implementation.\n\nThe core library implementation in the header-only `gpu.h` source code is around 1000 lines of code. In addition to enabling instantaneous, semi-interactive compilation cycles, the small implementation surface area keeps maintenance burden low and the velocity of improvements high.\nWe also pre-build Google's Dawn WebGPU implementation as a shared library binary. This allows builds to link the shared library with each build and incorporate Google's powerful native WebGPU implementation without paying the cost of re-compiling Dawn during development cycles.\n\nFor more advanced users and release deployments, we include `cmake` examples for building both Dawn with gpu.cpp end-to-end, but this is not required nor recommended for most users to get started.\n\n## Quick Start: Building and Running\n\nTo build a gpu.cpp project, you will need to have installed on your system:\n\n- `clang++` compiler installed with support for C++17.\n- `python3` and above, to run the script which downloads the Dawn shared library.\n  make to build the project.\n- `make` to build the project.\n- Only on Linux systems - Vulkan drivers. If Vulkan is not installed, you can run `sudo apt install libvulkan1 mesa-vulkan-drivers vulkan-tools` to install them.\n\nThe only library dependency of gpu.cpp is a WebGPU implementation. Currently we support the Dawn native backend, but we plan to support other targets and WebGPU implementations (web browsers or other native implementations such as wgpu). Currently we support MacOS, Linux, and Windows (via WSL).\n\nOptionally, Dawn can be built from scratch with gpu.cpp using the cmake build scripts provided - see the -cmake targets in the Makefile. However, this is recommended for advanced users only. Building Dawn dependencies with cmake takes much longer than using the precompiled Dawn shared library.\n\nAfter cloning the repo, from the top-level gpu.cpp, you should be able to build and run the hello world GELU example by typing:\n\n```\nmake\n```\n\nThe first time you build and run the project this way, it will download a prebuilt shared library for the Dawn native WebGPU implementation automatically (using the setup.py script). This places the Dawn shared library in the third_party/lib directory. Afterwards you should see `libdawn.dylib` on MacOS or `libdawn.so` on Linux. This download only occurs once.\n\nThe build process itself should take a few seconds. If the build and executions is successful, you should see the output of the GELU computation:\n\n```\nHello gpu.cpp!\n--------------\n\n  gelu(0.00) = 0.00\n  gelu(0.10) = 0.05\n  gelu(0.20) = 0.12\n  gelu(0.30) = 0.19\n  gelu(0.40) = 0.26\n  gelu(0.50) = 0.35\n  gelu(0.60) = 0.44\n  gelu(0.70) = 0.53\n  gelu(0.80) = 0.63\n  gelu(0.90) = 0.73\n  gelu(1.00) = 0.84\n  gelu(1.10) = 0.95\n  ...\n\nComputed 10000 values of GELU(x)\n```\n\nIf you need to clean up the build artifacts, you can run:\n\n```\nmake clean\n```\n\n## Hello World Tutorial: A GELU Kernel\n\nAs a real-world example for how to use gpu.cpp, let's start with a practical-but-simple example of a GPU kernel from neural networks.\n\nGELU is a non-linear embarassingly parallel operation often used in modern large language model transformer-based architectures.\n\nIt takes as input a vector of floats and applies the GELU function to each element of the vector. The function is nonlinear, attenuating values below zero to near zero, approximating the y = x identity function for large positive values. For values close to zero, GELU smoothly interpolates between the identity function and the zero function.\n\nThe GELU code below will illustrate the three main aspects of setting up a GPU computation with gpu.cpp:\n\n1. The code that runs on the GPU (in WebGPU Shading Language, or WGSL), implementing the compute operation.\n\n2. The code that runs on the CPU (in C++) that sets up the GPU computation by allocating and preparing resources. For high performance, this code should be run ahead-of-time from the hot paths of the application.\n\n3. The code that runs on the CPU (in C++) that dispatches the GPU computation and retrieves the results. The key concern of hot-path dispatch code is to eliminate or minimize any unnecessary resource allocation or data movement (offloading such concerns to step 2). A secondary consideration is that GPU dispatches are asynchronous. We work with standard C++ asynchronous primitives to manage the asynchronous aspect of kernel dispatch.\n\nHere's a GELU kernel implemented (based on the CUDA implementation in [llm.c](https://github.com/karpathy/llm.c)) as on-device WebGPU WGSL code and invoked from the host using gpu.cpp library functions and types. It can be compiled using a standard C++ compiler (we recommend Clang):\n\n```\n#include <array>\n#include <cstdio>\n#include <future>\n\n#include \"gpu.h\"\n\nusing namespace gpu; // createContext, createTensor, createKernel,\n                     // dispatchKernel, wait, toCPU Bindings,\n                     // Tensor, Kernel, Context, Shape, kf32\n\nstatic const char *kGelu = R\"(\nconst GELU_SCALING_FACTOR: f32 = 0.7978845608028654; // sqrt(2.0 / PI)\n@group(0) @binding(0) var<storage, read_write> inp: array<{{precision}}>;\n@group(0) @binding(1) var<storage, read_write> out: array<{{precision}}>;\n@compute @workgroup_size({{workgroupSize}})\nfn main(\n    @builtin(global_invocation_id) GlobalInvocationID: vec3<u32>) {\n    let i: u32 = GlobalInvocationID.x;\n    if (i < arrayLength(&inp)) {\n        let x: f32 = inp[i];\n        out[i] = select(0.5 * x * (1.0 + tanh(GELU_SCALING_FACTOR\n                 * (x + .044715 * x * x * x))), x, x > 10.0);\n    }\n}\n)\";\n\nint main(int argc, char **argv) {\n  Context ctx = createContext();\n  static constexpr size_t N = 10000;\n  std::array<float, N> inputArr, outputArr;\n  for (int i = 0; i < N; ++i) {\n    inputArr[i] = static_cast<float>(i) / 10.0; // dummy input data\n  }\n  Tensor input = createTensor(ctx, Shape{N}, kf32, inputArr.data());\n  Tensor output = createTensor(ctx, Shape{N}, kf32);\n  std::promise<void> promise;\n  std::future<void> future = promise.get_future();\n  Kernel op = createKernel(ctx, {kGelu, /* 1-D workgroup size */ 256, kf32},\n                           Bindings{input, output},\n                           /* number of workgroups */ {cdiv(N, 256), 1, 1});\n  dispatchKernel(ctx, op, promise);\n  wait(ctx, future);\n  toCPU(ctx, output, outputArr.data(), sizeof(outputArr));\n  for (int i = 0; i < 16; ++i) {\n    printf(\"  gelu(%.2f) = %.2f\\n\", inputArr[i], outputArr[i]);\n  }\n  return 0;\n}\n```\n\nHere we see the GPU code is quoted in a domain specific language called WGSL (WebGPU Shading Language). In a larger project, you might store this code in a separate file to be loaded at runtime (see [examples/shadertui](https://github.com/AnswerDotAI/gpu.cpp/tree/main/examples/shadertui) for a demonstration of live WGSL code re-loading).\n\nThe CPU code in main() sets up the host coordination for the GPU computation.\nWe can think of the use of gpu.cpp library as a collection of GPU nouns and\nverbs.\n\nThe \"nouns\" are GPU resources modeled by the type definitions of the library\nand the \"verbs\" actions on GPU resources, modeled by the functions of the\nlibrary. The ahead-of-time resource acquisition functions are prefaced with\n`create*`, such as:\n\n- `createContext()` - constructs a reference to the GPU device context (`Context`).\n- `createTensor()` - acquires a contiguous buffer on the GPU (`Tensor`).\n- `createKernel()` - constructs a handle to resources for the GPU computation (`Kernel`), taking the shader code as input and the tensor resources to bind.\n\nThese resource acquisition functions are tied to resource types for interacting with the GPU:\n\n- `Context` - a handle to the state of resources for interacting with the GPU device.\n- `Tensor` - a buffer of data on the GPU.\n- `KernelCode` - the code for a WGSL program that can be dispatched to the\n  GPU. This is a thin wrapper around a WGSL string and also includes the\n  workgroup size the code is designed to run with.\n- `Kernel` - a GPU program that can be dispatched to the GPU. This accepts a\n  `KernelCode` and a list of `Tensor` resources to bind for the dispatch\n  computation. This takes an argument `Bindings` that is a list of `Tensor` instances and should map the bindings declared at the top of the WGSL code. In this example there's two bindings corresponding to the `input` buffer on the GPU and the `ouptut` buffer on the GPU.\n\nIn this example, the GELU computation is performed only once and the program immediately exits so preparing resources and dispatch are side-by-side. Other examples in the [examples/](https://github.com/AnswerDotAI/gpu.cpp/blob/main/examples/) directory illustrate how resource acquisition is prepared ahead of time and dispatch occurs in the hot path like a render, model inference, or simulation loop.\n\nBesides the `create*` resource acquisition functions, there are a few more \"verbs\" in the gpu.cpp library for handling dispatching execution to the GPU and data movement:\n\n- `dispatchKernel()` - dispatches a `Kernel` to the GPU for computation. This is an asynchronous operation that returns immediately.\n- `wait()` - blocks until the GPU computation is complete. This is a standard C++ future/promise pattern.\n- `toCPU()` - moves data from the GPU to the CPU. This is a synchronous operation that blocks until the data is copied.\n- `toGPU()` - moves data from the CPU to the GPU. This is a synchronous operation that blocks until the data is copied. In this particular example, `toGPU()` is not used because there's only one data movement from CPU to GPU in the program and that happens when the `createTensor()` function is called.\n\nThis example is available in [examples/hello_world/run.cpp](https://github.com/AnswerDotAI/gpu.cpp/blob/main/examples/hello_world/run.cpp).\n\n## Other Examples: Matrix Multiplication, Physics Sim, and SDF Rendering\n\nYou can explore the example projects in\n[examples/](https://github.com/AnswerDotAI/gpu.cpp/blob/main/examples/) which\nillustrate how to use gpu.cpp as a library.\n\nAfter you have run `make` in the top-level directory which retrieves the prebuilt Dawn shared library, you can run each example by navigating to its directory and running `make` from the example's directory.\n\nAn example of tiled matrix multiplication is in [examples/matmul](https://github.com/AnswerDotAI/gpu.cpp/blob/main/examples/matmul/). This implements a WebGPU version of the first few kernels of Simon Boehm's [How to Optimize a CUDA Matmul Kernel for cuBLAS-like Performance: a Worklog](https://siboehm.com/articles/22/CUDA-MMM) post. It is only weakly optimized (up to 1D blocktiling, kernel number 4) at ~ 1.2+ TFLOPs on a Macbook Pro M1 laptop, which has a theoretical peak of 10.4 TFLOPs. Contributions to optimize this further are welcome - kernels 5-9 of Simon's post would be a natural starting point.\n\nA parallel physics simulation of an ensemble of double pendulums simulated in parallel with different initial conditions on the GPU is shown in [examples/physics](https://github.com/AnswerDotAI/gpu.cpp/tree/main/examples/physics).\n\n<div align=\"center\">\n\n\n</div>\n\nWe also show some examples of signed distance function computations, rendered in the terminal as ascii. A 3D SDF of spheres is shown in [examples/render](https://github.com/AnswerDotAI/gpu.cpp/tree/main/examples/render]) and a shadertoy-like live-reloading example is in [examples/shadertui](https://github.com/AnswerDotAI/gpu.cpp/tree/main/examples/shadertui).\n\nInterestingly, given a starting example, LLMs such as Claude 3.5 Sonnet can be quite capable at writing low-level WGSL code for you - the other shaders in the shadertui example are written by the LLM.\n\n<div align=\"center\">\n  \n</div>\n\n## Who is gpu.cpp for?\n\ngpu.cpp is aimed at enabling projects requiring portable on-device GPU computation with minimal implementation complexity and friction. Some example use cases are:\n\n- Development of GPU algorithms to be run on personal computing devices\n- Direct standalone implementations of neural network models\n- Physics simulations and simulation environments\n- Multimodal applications - audio and video processing\n- Offline graphics rendering\n- ML inference engines and runtimes\n- Parallel compute intensive data processing applications\n\nAlthough gpu.cpp is meant for any general purpose GPU computation and not strictly AI, one area we're interested in is pushing the limits exploring the intersection of new algorithms for post-training and on-device compute.\n\nTo date, AI research has primarily been built with CUDA as the privileged first-class target. CUDA has been dominant at large scale training and inference but at the other end of the the spectrum in the world of GPU compute on personal devices, there exists far more heterogeneity in the hardware and software stack.\n\nGPU compute in this personal device ecosystem has been largely limited to a small group of experts such as game engine developers and engineers working directly on ML compilers or inference runtimes. Along with that, implementing against the Vulkan or even WebGPU API directly tends to be targeted mostly towards infrastructure scale efforts - game engines, production ML inference engines, large software packages.\n\nWe want to make it easier for a broader range of projects to harness the power of GPUs on personal devices. With a small amount of code, we can access the GPU at a low-level, focusing on directly implementing algorithms rather than the scaffolding and tech stack around the GPU. For example, in our AI research there's much to explore with the various forms of dynamic/conditional post-training computation - dynamic use of adapters, sparsity, model compression, realtime multimodal integrations etc.\n\ngpu.cpp lets us implement and drop-in any algorithm with fine-grained control of data movement and GPU code, and explore outside boundaries of what is supported by existing production-oriented inference runtimes. At the same time we can write code that is portable and immediately usable on a wide variety of and GPU vendors and compute form factors - workstations, laptops, mobile, or even emerging hardware platforms such as AR/VR and robotics.\n\n## What gpu.cpp is not\n\ngpu.cpp is meant for developers with some familiarity with C++ and GPU programming. It is not a high-level numerical computing or machine learning framework or inference engine, though it can be used in support of such implementations.\n\nSecond, in spite of the name, WebGPU has native implementations decoupled from the web and the browser. gpu.cpp leverages WebGPU as a portable _native_ GPU API first and foremost, with the possibility of running in the browser being a convenient additional benefit in the future.\n\nIf you find it counterintuitive, as many do, that WebGPU is a native technology and not just for the web, watch Elie Michel's excellent talk [\"WebGPU is Not Just About the Web\"](https://www.youtube.com/watch?v=qHrx41aOTUQ).\n\nFinally, the focus of gpu.cpp is general-purpose GPU computation rather than rendering/graphics on the GPU, although it can be useful for offline rendering or video processing use cases. We may explore directions with graphics in the future, but for now our focus is GPU compute.\n\n## Limitations and Upcoming Features\n\n_API Improvements_ - gpu.cpp is a work-in-progress and there are many features and improvements to come. At this early stage, we expect the API design to evolve as we identify improvements / needs from use cases. In particular, the handling of structured parameters and asynchronous dispatch will undergo refinement and maturation in the short-term.\n\n_Browser Targets_ - In spite of using WebGPU we haven't tested builds targeting the browser yet though this is a short-term priority.\n\n_Reusable Kernel Library_ - Currently the core library is strictly the operations and types for interfacing with the WebGPU API, with some specific use case example WGSL implementations in `examples/`. Over time, as kernel implementations mature we may migrate some of the reusable operations from specific examples into a small reusable kernel library.\n\n_More Use Case Examples and Tests_ - Expect an iteration loop of use cases to design tweaks and improvements, which in turn make the use cases cleaner and easier to write. One short term use cases to flesh out the kernels from [llm.c](https://github.com/karpathy/llm.c) in WebGPU form. As these mature into a reusable kernel library, we hope to help realize the potential for WebGPU compute in AI.\n\n## Troubleshooting\n\nIf you run into issues building the project, please open an issue.\n\n## Acknowledgements\n\ngpu.cpp makes use of:\n\n- [Dawn](https://dawn.googlesource.com/dawn) as the WebGPU implementation\n- [webgpu-dawn-binaries](https://github.com/jspanchu/webgpu-dawn-binaries) by\n  @jspanchu to build a binary artifact of Dawn.\n- [webgpu-distribution](https://github.com/eliemichel/WebGPU-distribution) by\n  @eliemichel for cmake builds.\n\nThanks also to fellow colleagues at Answer.AI team for their support, testing help, and feedback.\n\n## Discord Community and Contributing\n\nJoin our community in the `#gpu-cpp` channel on the [AnswerDotAI Discord with this invite link](https://discord.gg/zmJVhXsC7f). Feel free to get in touch via X [@austinvhuang](https://twitter.com/austinvhuang) as well.\n\nFeedback, issues and pull requests are welcome.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "gpu",
        "webgpu",
        "vulkan",
        "wiscada gpu",
        "gpu cpp",
        "gpu computation"
      ],
      "category": "scientific-research-tools"
    },
    "wj2333wj--arxiv-mcp-server": {
      "owner": "wj2333wj",
      "name": "arxiv-mcp-server",
      "url": "https://github.com/wj2333wj/arxiv-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/wj2333wj.webp",
      "description": "Enables searching, downloading, and analyzing arXiv research papers through an MCP interface, managing local storage for efficient retrieval and utilizing specialized prompts for comprehensive analysis.",
      "stars": 0,
      "forks": 0,
      "license": "Apache License 2.0",
      "language": "",
      "updated_at": "2025-04-27T06:38:09Z",
      "readme_content": "[![Twitter Follow](https://img.shields.io/twitter/follow/JoeBlazick?style=social)](https://twitter.com/JoeBlazick)\n[![smithery badge](https://smithery.ai/badge/arxiv-mcp-server)](https://smithery.ai/server/arxiv-mcp-server)\n[![Python Version](https://img.shields.io/badge/python-3.11+-blue.svg)](https://www.python.org/downloads/)\n[![Tests](https://github.com/blazickjp/arxiv-mcp-server/actions/workflows/tests.yml/badge.svg)](https://github.com/blazickjp/arxiv-mcp-server/actions/workflows/tests.yml)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![PyPI Downloads](https://img.shields.io/pypi/dm/arxiv-mcp-server.svg)](https://pypi.org/project/arxiv-mcp-server/)\n[![PyPI Version](https://img.shields.io/pypi/v/arxiv-mcp-server.svg)](https://pypi.org/project/arxiv-mcp-server/)\n\n# ArXiv MCP Server\n\n> 🔍 Enable AI assistants to search and access arXiv papers through a simple MCP interface.\n\nThe ArXiv MCP Server provides a bridge between AI assistants and arXiv's research repository through the Model Context Protocol (MCP). It allows AI models to search for papers and access their content in a programmatic way.\n\n<div align=\"center\">\n  \n🤝 **[Contribute](https://github.com/blazickjp/arxiv-mcp-server/blob/main/CONTRIBUTING.md)** • \n📝 **[Report Bug](https://github.com/blazickjp/arxiv-mcp-server/issues)**\n\n<a href=\"https://www.pulsemcp.com/servers/blazickjp-arxiv-mcp-server\"><img src=\"https://www.pulsemcp.com/badge/top-pick/blazickjp-arxiv-mcp-server\" width=\"400\" alt=\"Pulse MCP Badge\"></a>\n</div>\n\n## ✨ Core Features\n\n- 🔎 **Paper Search**: Query arXiv papers with filters for date ranges and categories\n- 📄 **Paper Access**: Download and read paper content\n- 📋 **Paper Listing**: View all downloaded papers\n- 🗃️ **Local Storage**: Papers are saved locally for faster access\n- 📝 **Prompts**: A Set of Research Prompts\n\n## 🚀 Quick Start\n\n### Installing via Smithery\n\nTo install ArXiv Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/arxiv-mcp-server):\n\n```bash\nnpx -y @smithery/cli install arxiv-mcp-server --client claude\n```\n\n### Installing Manually\nInstall using uv:\n\n```bash\nuv tool install arxiv-mcp-server\n```\n\nFor development:\n\n```bash\n# Clone and set up development environment\ngit clone https://github.com/blazickjp/arxiv-mcp-server.git\ncd arxiv-mcp-server\n\n# Create and activate virtual environment\nuv venv\nsource .venv/bin/activate\n\n# Install with test dependencies\nuv pip install -e \".[test]\"\n```\n\n### 🔌 MCP Integration\n\nAdd this configuration to your MCP client config file:\n\n```json\n{\n    \"mcpServers\": {\n        \"arxiv-mcp-server\": {\n            \"command\": \"uv\",\n            \"args\": [\n                \"tool\",\n                \"run\",\n                \"arxiv-mcp-server\",\n                \"--storage-path\", \"/path/to/paper/storage\"\n            ]\n        }\n    }\n}\n```\n\nFor Development:\n\n```json\n{\n    \"mcpServers\": {\n        \"arxiv-mcp-server\": {\n            \"command\": \"uv\",\n            \"args\": [\n                \"--directory\",\n                \"path/to/cloned/arxiv-mcp-server\",\n                \"run\",\n                \"arxiv-mcp-server\",\n                \"--storage-path\", \"/path/to/paper/storage\"\n            ]\n        }\n    }\n}\n```\n\n## 💡 Available Tools\n\nThe server provides four main tools:\n\n### 1. Paper Search\nSearch for papers with optional filters:\n\n```python\nresult = await call_tool(\"search_papers\", {\n    \"query\": \"transformer architecture\",\n    \"max_results\": 10,\n    \"date_from\": \"2023-01-01\",\n    \"categories\": [\"cs.AI\", \"cs.LG\"]\n})\n```\n\n### 2. Paper Download\nDownload a paper by its arXiv ID:\n\n```python\nresult = await call_tool(\"download_paper\", {\n    \"paper_id\": \"2401.12345\"\n})\n```\n\n### 3. List Papers\nView all downloaded papers:\n\n```python\nresult = await call_tool(\"list_papers\", {})\n```\n\n### 4. Read Paper\nAccess the content of a downloaded paper:\n\n```python\nresult = await call_tool(\"read_paper\", {\n    \"paper_id\": \"2401.12345\"\n})\n```\n\n## 📝 Research Prompts\n\nThe server offers specialized prompts to help analyze academic papers:\n\n### Paper Analysis Prompt\nA comprehensive workflow for analyzing academic papers that only requires a paper ID:\n\n```python\nresult = await call_prompt(\"deep-paper-analysis\", {\n    \"paper_id\": \"2401.12345\"\n})\n```\n\nThis prompt includes:\n- Detailed instructions for using available tools (list_papers, download_paper, read_paper, search_papers)\n- A systematic workflow for paper analysis\n- Comprehensive analysis structure covering:\n  - Executive summary\n  - Research context\n  - Methodology analysis\n  - Results evaluation\n  - Practical and theoretical implications\n  - Future research directions\n  - Broader impacts\n\n## ⚙️ Configuration\n\nConfigure through environment variables:\n\n| Variable | Purpose | Default |\n|----------|---------|---------|\n| `ARXIV_STORAGE_PATH` | Paper storage location | ~/.arxiv-mcp-server/papers |\n\n## 🧪 Testing\n\nRun the test suite:\n\n```bash\npython -m pytest\n```\n\n## 📄 License\n\nReleased under the MIT License. See the LICENSE file for details.\n\n---\n\n<div align=\"center\">\n\nMade with ❤️ by the Pearl Labs Team\n\n<a href=\"https://glama.ai/mcp/servers/04dtxi5i5n\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/04dtxi5i5n/badge\" alt=\"ArXiv Server MCP server\" /></a>\n</div>\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "arxiv",
        "retrieval",
        "mcp",
        "arxiv mcp",
        "arxiv research",
        "analyzing arxiv"
      ],
      "category": "scientific-research-tools"
    },
    "wjlim--nf-core_mcp": {
      "owner": "wjlim",
      "name": "nf-core_mcp",
      "url": "https://github.com/wjlim/nf-core_mcp",
      "imageUrl": "/freedevtools/mcp/pfp/wjlim.webp",
      "description": "Manage and navigate nf-core pipeline repositories, providing access to configurations, workflows, and modules while enabling the search through pipeline files to enhance bioinformatics project efficiency.",
      "stars": 0,
      "forks": 3,
      "license": "No License",
      "language": "TypeScript",
      "updated_at": "2025-04-08T14:41:02Z",
      "readme_content": "# nf-core MCP Server\n\nAn MCP server for managing and navigating nf-core pipeline repositories.\n\n## Features\n\n- List local nf-core repositories (rnaseq, sarek, modules, tools)\n- Access pipeline configurations and workflows\n- Search through pipeline files\n- Explore pipeline modules\n\n## Installation\n\n### NPM Version\n\n```bash\n\ncd nf-core_mcp\nnpm install\n\n# Build TypeScript\nnpm run build\n\n# Start the server\nnpm start\n```\n\n### Docker Version\n\n```bash\n# Build the Docker image\ncd nf-core_mcp\ndocker build -t nf-core-mcp .\n# Run the container\ndocker run -i --rm \\\n  -v \"/path/to/your/workspace:/app/workspace\" \\\n  nf-core-mcp\n```\n\n## Adding nf-core Repositories\n\nTo add new nf-core pipeline repositories to the workspace:\n\n1. **Clone the repositories**:\n   ```bash\n   # Navigate to your workspace directory (example for Windows)\n   cd /path/to/your/workspace\n\n   # Clone desired nf-core repositories\n   git clone https://github.com/nf-core/rnaseq.git\n   git clone https://github.com/nf-core/sarek.git\n   git clone https://github.com/nf-core/modules.git\n   # Add any other nf-core pipeline you want to manage\n   ```\n\n2. **Directory Structure**:\n   Your workspace should look like this:\n   ```\n   workspace/\n   ├── rnaseq/\n   ├── sarek/\n   ├── modules/\n   └── your-new-pipeline/\n   ```\n\n3. **Verify Installation**:\n   After starting the MCP server, use the `list-pipelines` command to verify that your new pipelines are detected:\n   ```\n   list-pipelines\n   ```\n\nNote: The MCP server will automatically detect and manage any nf-core pipeline repositories in your workspace directory.\n\n## Available Tools\n\n1. `list-pipelines`\n   - Lists all nf-core pipelines in the workspace\n   - Shows configuration file status\n   - No parameters required\n\n2. `get-pipeline-modules`\n   - Gets module information from a pipeline\n   - Parameters:\n     - `pipeline`: Pipeline name (rnaseq, sarek, or modules)\n\n3. `search-pipelines`\n   - Searches through pipeline files\n   - Parameters:\n     - `query`: Search query\n     - `pipeline` (optional): Specific pipeline to search\n\n## Available Resources\n\n1. `pipeline-config`\n   - Gets pipeline configuration\n   - URI format: `pipeline://{name}/config`\n   - Parameters:\n     - `name`: Pipeline name (rnaseq, sarek, or modules)\n\n2. `pipeline-workflow`\n   - Gets pipeline workflow\n   - URI format: `pipeline://{name}/workflow`\n   - Parameters:\n     - `name`: Pipeline name (rnaseq, sarek, or modules)\n\n## Usage with Cursor IDE\n\n### Using NPX (Recommended)\n\nAdd the following to your `mcp.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"nf-core\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"nf-core-mcp\"]\n    }\n  }\n}\n```\n\n### Using Docker\n\nAdd the following to your `mcp.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"nf-core\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"-i\",\n        \"--rm\",\n        \"-v\",\n        \"/path/to/your/workspace:/app/workspace\",\n        \"nf-core-mcp\"\n      ]\n    }\n  }\n}\n```\n\n## Example Usage\n\nUsing the MCP server in Cursor:\n\n```\n# List available pipelines\nlist-pipelines\n\n# Get modules from rnaseq pipeline\nget-pipeline-modules pipeline=rnaseq\n\n# Search in all pipelines\nsearch-pipelines query=\"fastqc\"\n\n# Search in specific pipeline\nsearch-pipelines query=\"fastqc\" pipeline=rnaseq\n\n# Access pipeline configuration\npipeline://rnaseq/config\n\n# Access workflow\npipeline://rnaseq/workflow\n```\n\n## Running the Server\n\n### Using NPM\n\n```bash\n# If installed globally\nnf-core-mcp\n\n# If installed locally\nnpx nf-core-mcp\n\n# Using npx without installation\nnpx -y nf-core-mcp\n```\n\n### Using Docker\n\n```bash\ndocker run -it --rm \\\n  -v /path/to/your/workspace:/app/workspace \\\n  nf-core-mcp\n```\n\n## Development\n\n```bash\n# Install dependencies\nnpm install\n\n# Build TypeScript\nnpm run build\n\n# Run in development mode\nnpm run dev\n\n# Run tests\nnpm test\n\n# Run linter\nnpm run lint\n```\n\n## License\n\nMIT\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "bioinformatics",
        "core_mcp",
        "pipeline",
        "nf core_mcp",
        "nf core",
        "core pipeline"
      ],
      "category": "scientific-research-tools"
    },
    "woodamsim--models": {
      "owner": "woodamsim",
      "name": "models",
      "url": "https://github.com/woodamsim/models",
      "imageUrl": "/freedevtools/mcp/pfp/null.webp",
      "description": "Provides a collection of TensorFlow models, including examples and research implementations, facilitating access to various machine learning solutions and learning resources. Users can explore TensorFlow capabilities through well-maintained models and associated tutorials.",
      "stars": 0,
      "forks": 0,
      "license": "Unknown",
      "language": "Unknown",
      "updated_at": "",
      "readme_content": "",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "tensorflow",
        "models",
        "woodamsim",
        "tensorflow models",
        "explore tensorflow",
        "woodamsim models"
      ],
      "category": "scientific-research-tools"
    },
    "ygboy1986--AIClaude": {
      "owner": "ygboy1986",
      "name": "AIClaude",
      "url": "https://github.com/ygboy1986/AIClaude",
      "imageUrl": "/freedevtools/mcp/pfp/ygboy1986.webp",
      "description": "Integrates DeepSeek R1's advanced reasoning engine with Claude for complex multi-step reasoning tasks. Enhances language processing capabilities to deliver precise and efficient responses.",
      "stars": 0,
      "forks": 2,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-03-20T03:09:57Z",
      "readme_content": "# DeepSeek-Claude MCP Server\n[![smithery badge](https://smithery.ai/badge/@HarshJ23/deepseek-claude-MCP-server)](https://smithery.ai/server/@HarshJ23/deepseek-claude-MCP-server)\n\n**Enhance Claude's reasoning capabilities** with the integration of DeepSeek R1's advanced reasoning engine. This server enables Claude to tackle complex reasoning tasks by leveraging the reasoning capabilites of deepseek r1 model.\n\n<a href=\"https://glama.ai/mcp/servers/w3q7v1srsw\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/w3q7v1srsw/badge\" alt=\"DeepSeek-Claude Server MCP server\" /></a>\n\n---\n\n## 🚀 Features\n\n### **Advanced Reasoning Capabilities**\n- Seamlessly integrates DeepSeek R1's reasoning with Claude.\n- Supports intricate multi-step reasoning tasks.\n- Designed for precision and efficiency in generating thoughtful responses.\n\n\n\n\n\n\n---\n\n## Complete Setup guide\n\n### Installing via Smithery\n\nTo install DeepSeek-Claude for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@HarshJ23/deepseek-claude-MCP-server):\n\n```bash\nnpx -y @smithery/cli install @HarshJ23/deepseek-claude-MCP-server --client claude\n```\n\n### Prerequisites\n- Python 3.12 or higher\n- `uv` package manager\n- DeepSeek API key (Sign up at [DeepSeek Platform](https://platform.deepseek.com))\n\n\n\n1. **Clone the Repository**\n   ```bash\n   git clone https://github.com/harshj23/deepseek-claude-MCP-server.git\n   cd deepseek-claude-MCP-server\n   ```\n\n2. **Ensure UV is Set Up**\n   - **Windows**: Run the following in PowerShell:\n     ```powershell\n     powershell -ExecutionPolicy ByPass -c \"irm https://astral.sh/uv/install.ps1 | iex\"\n     ```\n   - **Mac**: Run the following:\n     ```bash\n     curl -LsSf https://astral.sh/uv/install.sh | sh\n     ```\n\n3. **Create Virtual Environment**\n   ```bash\n   uv venv\n   source .venv/bin/activate\n   ```\n\n4. **Install Dependencies**\n   ```bash\n   uv add \"mcp[cli]\" httpx\n   ```\n\n5. **Set Up API Key**\n   ```bash\n   Obtain your api key from here : https://platform.deepseek.com/api_keys\n   ```\n\n6. **Configure MCP Server**\n   Edit the `claude_desktop_config.json` file to include the following configuration:\n   \n\n   ```json\n   {\n       \"mcpServers\": {\n           \"deepseek-claude\": {\n               \"command\": \"uv\",\n               \"args\": [\n                   \"--directory\",\n                   \"C:\\\\ABSOLUTE\\\\PATH\\\\TO\\\\PARENT\\\\FOLDER\\\\deepseek-claude\",\n                   \"run\",\n                   \"server.py\"\n               ]\n           }\n       }\n   }\n   ```\n\n7. **Run the Server**\n   ```bash\n   uv run server.py\n   ```\n\n8. **Test Setup**\n   - ##### Restart Claude Desktop.\n   - Verify the tools icon is visible in the interface.\n   \n\n\n   - If the server isn’t visible, consult the [troubleshooting guide](https://modelcontextprotocol.io/quickstart/server#troubleshooting).\n\n---\n\n## 🛠 Usage\n\n### Starting the Server\nThe server automatically starts when used with Claude Desktop. Ensure Claude Desktop is configured to detect the MCP server.\n\n### Example Workflow\n1. Claude receives a query requiring advanced reasoning.\n2. The query is forwarded to DeepSeek R1 for processing.\n3. DeepSeek R1 returns structured reasoning wrapped in `<ant_thinking>` tags.\n4. Claude integrates the reasoning into its final response.\n\n---\n\n\n## 📄 License\n\nThis project is licensed under the MIT License. See the [LICENSE](LICENSE) file for details.\n\n---",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "deepseek",
        "reasoning",
        "aiclaude",
        "reasoning engine",
        "reasoning tasks",
        "advanced reasoning"
      ],
      "category": "scientific-research-tools"
    },
    "yonaka15--mcp-pyodide": {
      "owner": "yonaka15",
      "name": "mcp-pyodide",
      "url": "https://github.com/yonaka15/mcp-pyodide",
      "imageUrl": "/freedevtools/mcp/pfp/yonaka15.webp",
      "description": "Execute Python code through the Model Context Protocol interface, enabling Large Language Models to leverage Python capabilities within their responses. The server supports both stdio and SSE transport modes and is implemented in TypeScript.",
      "stars": 14,
      "forks": 5,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-09-08T05:40:45Z",
      "readme_content": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/yonaka15-mcp-pyodide-badge.png)](https://mseep.ai/app/yonaka15-mcp-pyodide)\n\n# mcp-pyodide\n\nA Pyodide server implementation for the Model Context Protocol (MCP). This server enables Large Language Models (LLMs) to execute Python code through the MCP interface.\n\n<a href=\"https://glama.ai/mcp/servers/pxls43joly\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/pxls43joly/badge\" alt=\"mcp-pyodide MCP server\" />\n</a>\n\n## Features\n\n- Python code execution capability for LLMs using Pyodide\n- MCP compliant server implementation\n- Support for both stdio and SSE transport modes\n- Robust implementation written in TypeScript\n- Available as a command-line tool\n\n## Installation\n\n```bash\nnpm install mcp-pyodide\n```\n\n## Usage\n\n### As a Server\n\n```typescript\nimport { runServer } from \"mcp-pyodide\";\n\n// Start the server\nrunServer().catch((error: unknown) => {\n  console.error(\"Error starting server:\", error);\n  process.exit(1);\n});\n```\n\n### As a Command-line Tool\n\nStart in stdio mode (default):\n\n```bash\nmcp-pyodide\n```\n\nStart in SSE mode:\n\n```bash\nmcp-pyodide --sse\n```\n\n### SSE Mode\n\nWhen running in SSE mode, the server provides the following endpoints:\n\n- SSE Connection: `http://localhost:3020/sse`\n- Message Handler: `http://localhost:3020/messages`\n\nExample client connection:\n\n```typescript\nconst eventSource = new EventSource(\"http://localhost:3020/sse\");\neventSource.onmessage = (event) => {\n  console.log(\"Received:\", JSON.parse(event.data));\n};\n```\n\n## Project Structure\n\n```\nmcp-pyodide/\n├── src/\n│   ├── formatters/    # Data formatting handlers\n│   ├── handlers/      # Request handlers\n│   ├── lib/          # Library code\n│   ├── tools/        # Utility tools\n│   ├── utils/        # Utility functions\n│   └── index.ts      # Main entry point\n├── build/            # Build artifacts\n├── pyodide-packages/ # Pyodide-related packages\n└── package.json\n```\n\n## Dependencies\n\n- `@modelcontextprotocol/sdk`: MCP SDK (^1.4.0)\n- `pyodide`: Python runtime environment (^0.27.1)\n- `arktype`: Type validation library (^2.0.1)\n- `express`: Web framework for SSE support\n- `cors`: CORS middleware for SSE support\n\n## Development\n\n### Requirements\n\n- Node.js 18 or higher\n- npm 9 or higher\n\n### Setup\n\n```bash\n# Clone the repository\ngit clone <repository-url>\n\n# Install dependencies\nnpm install\n\n# Build\nnpm run build\n```\n\n### Scripts\n\n- `npm run build`: Compile TypeScript and set execution permissions\n- `npm start`: Run server in stdio mode\n- `npm run start:sse`: Run server in SSE mode\n\n## Environment Variables\n\n- `PYODIDE_CACHE_DIR`: Directory for Pyodide cache (default: \"./cache\")\n- `PYODIDE_DATA_DIR`: Directory for mounted data (default: \"./data\")\n- `PORT`: Port for SSE server (default: 3020)\n\n## License\n\nMIT\n\n## Contributing\n\n1. Fork the repository\n2. Create a feature branch (`git checkout -b feature/amazing-feature`)\n3. Commit your changes (`git commit -am 'Add some amazing feature'`)\n4. Push to the branch (`git push origin feature/amazing-feature`)\n5. Create a Pull Request\n\n## Important Notes\n\n- This project is under development, and the API may change\n- Thoroughly test before using in production\n- Exercise caution when executing untrusted code for security reasons\n- When using SSE mode, ensure proper CORS configuration if needed\n\n## Support\n\nPlease use the Issue tracker for problems and questions.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "python",
        "pyodide",
        "mcp",
        "python capabilities",
        "mcp pyodide",
        "pyodide execute"
      ],
      "category": "scientific-research-tools"
    },
    "ywwAHU--mcp-scholarly": {
      "owner": "ywwAHU",
      "name": "mcp-scholarly",
      "url": "https://github.com/ywwAHU/mcp-scholarly",
      "imageUrl": "/freedevtools/mcp/pfp/ywwAHU.webp",
      "description": "Search for academic articles using the MCP server by querying a dedicated tool for relevant papers based on keywords. Access trusted academic sources for streamlined research.",
      "stars": 0,
      "forks": 1,
      "license": "MIT License",
      "language": "",
      "updated_at": "2025-04-15T07:27:21Z",
      "readme_content": "# mcp-scholarly MCP server\n[![smithery badge](https://smithery.ai/badge/mcp-scholarly)](https://smithery.ai/server/mcp-scholarly)\n\nA MCP server to search for accurate academic articles. More scholarly vendors will be added soon.\n\n\n\n![image](https://github.com/user-attachments/assets/13202184-bc12-4530-b7c1-2ee698f3e1cc)\n\n<a href=\"https://glama.ai/mcp/servers/aq05b2p0ql\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/aq05b2p0ql/badge\" alt=\"Scholarly Server MCP server\" /></a>\n\n![star-history-2025323](https://github.com/user-attachments/assets/b73c916d-194c-429e-a7f9-0ff7d0db0b02)\n\n\n## Components\n\n### Tools\n\nThe server implements one tool:\n- search-arxiv: Search arxiv for articles related to the given keyword.\n  - Takes \"keyword\" as required string arguments\n\n## Quickstart\n\n### Install\n\n#### Claude Desktop\n\nOn MacOS: `~/Library/Application\\ Support/Claude/claude_desktop_config.json`\nOn Windows: `%APPDATA%/Claude/claude_desktop_config.json`\n\n<details>\n  <summary>Development/Unpublished Servers Configuration</summary>\n  ```\n  \"mcpServers\": {\n    \"mcp-scholarly\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"--directory\",\n        \"/Users/adityakarnam/PycharmProjects/mcp-scholarly/mcp-scholarly\",\n        \"run\",\n        \"mcp-scholarly\"\n      ]\n    }\n  }\n  ```\n</details>\n\n<details>\n  <summary>Published Servers Configuration</summary>\n  ```\n  \"mcpServers\": {\n    \"mcp-scholarly\": {\n      \"command\": \"uvx\",\n      \"args\": [\n        \"mcp-scholarly\"\n      ]\n    }\n  }\n  ```\n</details>\n\nor if you are using Docker\n\n<details>\n  <summary>Published Docker Servers Configuration</summary>\n  ```\n  \"mcpServers\": {\n    \"mcp-scholarly\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\", \"--rm\", \"-i\",\n        \"mcp/scholarly\"\n      ]\n    }\n  }\n  ```\n</details>\n\n### Installing via Smithery\n\nTo install mcp-scholarly for Claude Desktop automatically via [Smithery](https://smithery.ai/server/mcp-scholarly):\n\n```bash\nnpx -y @smithery/cli install mcp-scholarly --client claude\n```\n\n## Development\n\n### Building and Publishing\n\nTo prepare the package for distribution:\n\n1. Sync dependencies and update lockfile:\n```bash\nuv sync\n```\n\n2. Build package distributions:\n```bash\nuv build\n```\n\nThis will create source and wheel distributions in the `dist/` directory.\n\n3. Publish to PyPI:\n```bash\nuv publish\n```\n\nNote: You'll need to set PyPI credentials via environment variables or command flags:\n- Token: `--token` or `UV_PUBLISH_TOKEN`\n- Or username/password: `--username`/`UV_PUBLISH_USERNAME` and `--password`/`UV_PUBLISH_PASSWORD`\n\n### Debugging\n\nSince MCP servers run over stdio, debugging can be challenging. For the best debugging\nexperience, we strongly recommend using the [MCP Inspector](https://github.com/modelcontextprotocol/inspector).\n\n\nYou can launch the MCP Inspector via [`npm`](https://docs.npmjs.com/downloading-and-installing-node-js-and-npm) with this command:\n\n```bash\nnpx @modelcontextprotocol/inspector uv --directory /Users/adityakarnam/PycharmProjects/mcp-scholarly/mcp-scholarly run mcp-scholarly\n```\n\n\nUpon launching, the Inspector will display a URL that you can access in your browser to begin debugging.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "scholarly",
        "research",
        "mcp",
        "mcp scholarly",
        "scholarly search",
        "search academic"
      ],
      "category": "scientific-research-tools"
    },
    "zchaffee1--mcp-server": {
      "owner": "zchaffee1",
      "name": "mcp-server",
      "url": "https://github.com/zchaffee1/mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/zchaffee1.webp",
      "description": "Provides access to scientific computing resources, enabling HDF5 file operations, Slurm job management, and hardware information retrieval. Facilitates data compression and decompression using gzip or zlib.",
      "stars": 0,
      "forks": 0,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-04-21T04:43:22Z",
      "readme_content": "# mcp-server\n\nBy: Zack Chaffee A20478873\n\nA server implementing Model Coupling Protocol (MCP) capabilities for HDF5 file operations and Slurm job management.\n\n## Features\n\n### HDF5 file operations:\n  - Read datasets\n  - List file contents\n\n### Slurm job management:\n  - Submit jobs\n  - Check job status\n\n### Node Hardware Operations\n- Get CPU information\n- Get memory information\n- Get disk information\n- Get comprehensive system information\n\n### Compression Operations\n- Compress string data with gzip or zlib\n- Compress files with gzip or zlib\n- Decompress data\n\n# Initialization\n\nOnce you clone this reponsitory cd into it \n\nAfter this hwe will create a virtual enviornment and install all dependincies:\n```\nuv venv .venv\nsource .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\nuv pip install -e .\nuv pip install -e \".[test]\"\n```\n\n# Running\n\nTo startup the server run:\n```\npython -m src.server\n```\n\nThis will autoclocate the server at http://localhost:8000.\n\n## Endpoints\n- `POST /mcp`: Main endpoint for MCP requests\n- `GET /health`: Health check endpoint\n\n## Examples:\n```python\nimport httpx\n\nasync with httpx.AsyncClient() as client:\n    # Read a dataset\n    response = await client.post(\"http://localhost:8000/mcp\", json={\n        \"capability\": \"hdf5\",\n        \"action\": \"read_dataset\",\n        \"parameters\": {\n            \"file_path\": \"/path/to/data.h5\",\n            \"dataset_path\": \"/path/to/dataset\"\n        }\n    })\n    \n    # List contents\n    response = await client.post(\"http://localhost:8000/mcp\", json={\n        \"capability\": \"hdf5\",\n        \"action\": \"list_contents\",\n        \"parameters\": {\n            \"file_path\": \"/path/to/data.h5\",\n            \"group_path\": \"/\"\n        }\n    })\n```\n\n```bash\ncurl -X POST http://localhost:8000/mcp \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"jsonrpc\": \"2.0\",\n    \"method\": \"mcp/listTools\", \n    \"params\": {}, \n    \"id\": \"1\"\n  }'\n```\n\n# Testing\nFor testing rung: \n```\npytest\n```\n\nFor tests with coverage:\n```\npytest --cov=src\n```\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "gzip",
        "zlib",
        "hdf5",
        "gzip zlib",
        "zchaffee1 mcp",
        "tools zchaffee1"
      ],
      "category": "scientific-research-tools"
    },
    "zinja-coder--apktool-mcp-server": {
      "owner": "zinja-coder",
      "name": "apktool-mcp-server",
      "url": "https://github.com/zinja-coder/apktool-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/zinja-coder.webp",
      "description": "Integrate Apktool with LLMs to enable real-time reverse engineering of Android APKs. Perform decompilation, code review, and AI-driven analysis while managing resources and vulnerabilities.",
      "stars": 224,
      "forks": 30,
      "license": "Apache License 2.0",
      "language": "Python",
      "updated_at": "2025-10-02T12:48:07Z",
      "readme_content": "<div align=\"center\">\n\n# apktool-mcp-server (Part of Zin's Reverse Engineering MCP Suite)\n\n⚡ Fully automated MCP server built on top of apktool to analyze Android APKs using LLMs like Claude — uncover vulnerabilities, parse manifests, and reverse engineer effortlessly.\n\n![GitHub contributors apktool-mcp-server](https://img.shields.io/github/contributors/zinja-coder/apktool-mcp-server)\n![GitHub all releases](https://img.shields.io/github/downloads/zinja-coder/apktool-mcp-server/total)\n![GitHub release (latest by SemVer)](https://img.shields.io/github/downloads/zinja-coder/apktool-mcp-server/latest/total)\n![Latest release](https://img.shields.io/github/release/zinja-coder/apktool-mcp-server.svg)\n![Python 3.10+](https://img.shields.io/badge/python-3%2E10%2B-blue)\n[![License](http://img.shields.io/:license-apache-blue.svg)](http://www.apache.org/licenses/LICENSE-2.0.html)\n\n</div>\n\n<!-- It is a still in early stage of development, so expects bugs, crashes and logical erros.-->\n\n<div align=\"center\">\n    <img alt=\"banner\" height=\"480px\" widht=\"620px\" src=\"https://github.com/user-attachments/assets/eb9037f2-d1c7-45e0-8871-ca8aaade0dd0\">\n</div>\n\n<!--![apktool-mcp-server-banner.png](docs/assets/apktool-mcp-server-banner.png)-->\n\nImage generated using AI tools.\n\n---\n\n## 🤖 What is apktool-mcp-server?\n\n**apktool-mcp-server** is a MCP server for the [Apk Tool](https://github.com/iBotPeaches/apktool) that integrates directly with [Model Context Protocol (MCP)](https://github.com/anthropic/mcp) to provide **live reverse engineering support with LLMs like Claude**.\n\nThink: \"Decompile → Context-Aware Code Review → AI Recommendations\" — all in real time.\n\nWatch the demo!\n\nhttps://github.com/user-attachments/assets/d50251b8-6b1c-4341-b18e-ae54eb24a847\n\n- **Solving the CTFs**\n\n\n\nhttps://github.com/user-attachments/assets/c783a604-a636-4e70-9fa8-37e3d219b20b\n\n\n## Other projects in Zin MCP Suite\n - **[JADX-AI-MCP](https://github.com/zinja-coder/jadx-ai-mcp)**\n - **[JADX-MCP-Server](https://github.com/zinja-coder/jadx-mcp-server)**\n - **[ZIN-MCP-Client](https://github.com/zinja-coder/zin-mcp-client)**\n\n## Current MCP Tools\n\nThe following MCP tools are available:\n\n- `build_apk()` — Build an APK from a decoded APKTool Project.\n- `get_manifest()` — Get the AndroidManifest.xml content from a decoded APK project. \n- `get_apktool_yml()` — Get apktool.yml information from a decoded APK project. \n- `list_smali_directories()` — List all smali directories in a project. \n- `list_smali_files()` — List smali files in a specific smali directory, optinally filtered by package prefix.\n- `get_smali_file()` — Get content of a specific smali file by class name. \n- `modify_smali_file()` — Modify the content of a specific smali file. \n- `list_resources()` — List resources in a project, optionally filtered by resource type. \n- `get_resource_file()` — Get Content of a specific resource file. \n- `modify_resource_file()` — Modify the content of a specific resource file. \n- `search_in_file()` — Search for a pattern in files with specified extensions. \n- `clean_project()` — Clean a project directory to prepare for rebuilding.\n- `decode_apk()` — Decode an APK file using APKTool, extracting resources and smali code. \n\n---\n\n## 🗒️ Sample Prompts\n\n\n### 🔍 Basic Code Understanding\n\n- “List all smali directories for the dvac project.”\n\n- “Show me all the smali files under the package prefix com.vulnerable.component in the dvac project.”\n\n- “Get the smali code for the class com.vulnerable.component.MainActivity.”\n\n- “Compare MainActivity.smali with its previous version and show differences.”\n\n- “Search for usage of startActivity in smali files of dvac project.”\n\n### 🛡️ Vulnerability Detection\n\n- “Analyze declared permissions in the dvac AndroidManifest.xml and flag dangerous ones.”\n\n- “Search for hardcoded URLs or IPs in all .xml and .smali files in the project.”\n\n- “Find all uses of PendingIntent.getActivity in smali files.”\n\n- “Check for exported activities or receivers in dvac’s AndroidManifest.xml.”\n\n- “List all smali files that access android.permission.SEND_SMS or READ_CONTACTS.”\n\n### 🛠️ Reverse Engineering Helpers\n\n- “Decode this APK: dvac.apk and create a project called dvac.”\n\n- “Create a new APKTool project called test-harness.”\n\n- “Clean the dvac project before rebuild.”\n\n- “Extract DEX files from dvac project for external analysis.”\n\n- “Modify MainActivity.smali to insert a log line at the beginning of onCreate().”\n\n### 📦 Static Analysis\n\n- “Get the complete AndroidManifest.xml from dvac project.”\n\n- “Show the contents of apktool.yml for the dvac project.”\n\n- “List all resource files of type layout.”\n\n- “Search for the word password in all resource and smali files.”\n\n- “Check which permissions are used and compare them against typical over-permissioning risks.”\n\n### 🤖 AI Code Modification\n\n- “Modify the onCreate() method in MainActivity.smali to add a toast message.”\n\n- “Replace all http:// links with https:// in strings.xml.”\n\n- “Add the android:exported=false attribute to all activities in the AndroidManifest.xml.”\n\n- “Patch the method validateLogin in LoginManager.smali to always return true.”\n\n- “Add logging statements to every method in MainActivity.smali.”\n\n### 📄 Documentation & Metadata\n\n- “List all decoded APKTool projects in the workspace.”\n\n- “Show me the apktool.yml config to review the version, original APK metadata, and compression settings.”\n\n- “Get all available Android devices connected via ADB. (To be migrated to ADB MCP Server.)”\n\n- “Get metadata about the project dvac from its apktool.yml.”\n\n- “Check which APKTool version is currently installed on the server.”\n---\n\n## 🛠️ Getting Started \n### 1. Downlaod from Releases: https://github.com/zinja-coder/apktool-mcp-server/releases\n\n```bash\n# 0. Download and install apktool\nhttps://apktool.org/docs/install\n\n# 1. Test whether apktool has been correctly configured in the environment variables\n$ apktool -version\n\n# 2. Download the apktool-mcp-server-<version>.zip\nhttps://github.com/zinja-coder/apktool-mcp-server/releases\n\n# 3. \nunzip apktool-mcp-server-<version>.zip\n\n├apktool-mcp-server/\n  ├── apktool_mcp_server.py\n  ├── requirements.txt\n  ├── README.md\n  ├── LICENSE\n\n# 4. Navigate to apktool-mcp-server directory\ncd apktool-mcp-server\n\n# 5. This project uses uv - https://github.com/astral-sh/uv instead of pip for dependency management.\n    ## a. Install uv (if you dont have it yet)\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n    ## b. OPTIONAL, if for any reasons, you get dependecy errors in apktool-mcp-server, Set up the environment\nuv venv\nsource .venv/bin/activate  # or .venv\\Scripts\\activate on Windows\n    ## c. OPTIONAL Install dependencies\nuv pip install httpx fastmcp\n\n# The setup for apktool-mcp-server is done.\n```\n\n## 2. Running on Local LLM Using Ollama and Zin MCP Client - Recommended\n\n<div align=\"center\">\n    <a href=\"https://github.com/zinja-coder/zin-mcp-client\">\n    <img alt=\"zin-mcp-client\" height=\"360px\" widht=\"480px\" src=\"https://github.com/user-attachments/assets/0e8e0ecd-0520-422e-a007-03dc62c4118e\">\n    </a>\n</div>\n\n⚡ Lightweight, Fast, Simple, CLI-Based MCP Client for STDIO MCP Servers, to fill the gap and provide bridge between your local LLMs running Ollama and MCP Servers.\n\nCheck Now: https://github.com/zinja-coder/zin-mcp-client\n\nDemo: Coming soon...\n\n## 🤖 3. Claude Desktop Setup\n\nMake sure Claude Desktop is running with MCP enabled.\n\nFor instance, I have used following for Kali Linux: https://github.com/aaddrick/claude-desktop-debian\n\nConfigure and add MCP server to LLM file:\n```bash\nnano ~/.config/Claude/claude_desktop_config.json\n```\n\n   - Windows: `%APPDATA%\\Claude\\claude_desktop_config.json`\n   - macOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\n\nAnd following content in it:\n```json\n{\n    \"mcpServers\": {\n        \"apktool-mcp-server\": {\n            \"command\": \"/<path>/<to>/uv\", \n            \"args\": [\n                \"--directory\",\n                \"</PATH/TO/>apktool-mcp-server/\",\n                \"run\",\n                \"apktool_mcp_server.py\"\n            ]\n        }\n    }\n}\n```\n\nReplace:\n\n- `path/to/uv` with the actual path to your `uv` executable\n- `path/to/apktool-mcp-server` with the absolute path to where you cloned this\nrepository\n\nThen, navigate code and interact via real-time code review prompts using the built-in integration.\n\n## 4. Cherry Studio Setup\n\nIf you want to configure the MCP tool in Cherry Studio, you can refer to the following configuration.\n- Type: stdio\n- command: uv\n- argument:\n```bash\n--directory\npath/to/apktool-mcp-server\nrun\napktool_mcp_server.py\n```\n- `path/to/apktool-mcp-server` with the absolute path to where you cloned this\nrepository\n\n## To report bugs, issues, feature suggestion, Performance issue, general question, Documentation issue.\n - Kindly open an issue with respective template.\n\n - Tested on Claude Desktop Client, support for other AI will be tested soon!\n\n## 🙏 Credits\n\nThis project is a MCP Server for [Apktool](https://github.com/iBotPeaches/apktool), an amazing open-source Android reverse engineering tool created and maintained by [@iBotPeaches](https://github.com/iBotPeaches). All core APK decoding and resource processing logic belongs to them. I have only extended it to support my MCP server with AI capabilities.\n\n[📎 Original README (Apktool)](https://github.com/iBotPeaches/apktool)\n\nThe original README.md from Apktool is included here in this repository for reference and credit.\n\nAlso huge thanks to [@aaddrick](https://github.com/aaddrick) for developing Claude desktop for Debian based Linux.\n\nAnd in last, thanks to [@anthropics](https://github.com/anthropics) for developing the Model Context Protocol and [@FastMCP](https://github.com/jlowin/fastmcp) team.\n\nAnd all open source project maintainers and contributos which provies libraries and dependencies to make project like this possible.\n\n## 📄 License\n\napktool-mcp-server and all related projects inherits the Apache 2.0 \n\n## ⚖️ Legal Warning\n\n**Disclaimer**\n\nThe tools `apktool-mcp-server` and all related tools under this project are intended strictly for educational, research, and ethical security assessment purposes. They are provided \"as-is\" without any warranties, expressed or implied. Users are solely responsible for ensuring that their use of these tools complies with all applicable laws, regulations, and ethical guidelines.\n\nBy using `apktool-mcp-server`, you agree to use them only in environments you are authorized to test, such as applications you own or have explicit permission to analyze. Any misuse of these tools for unauthorized reverse engineering, infringement of intellectual property rights, or malicious activity is strictly prohibited.\n\nThe developers of `apktool-mcp-server` shall not be held liable for any damage, data loss, legal consequences, or other consequences resulting from the use or misuse of these tools. Users assume full responsibility for their actions and any impact caused by their usage.\n\nUse responsibly. Respect intellectual property. Follow ethical hacking practices.\n\n---\n\n## 🙌 Contribute or Support\n\n## Contributing\n\n[![Contributions Welcome](https://img.shields.io/badge/contributions-welcome-brightgreen.svg?style=flat-square)](CONTRIBUTE.md)\n\n- Found it useful? Give it a ⭐️\n- Got ideas? Open an [issue](https://github.com/zinja-coder/apktool-mcp-server/issues) or submit a PR\n- Built something on top? DM me or mention me — I’ll add it to the README!\n\n---\n## Audited and Received Assessment Badge\n\n[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/zinja-coder-apktool-mcp-server-badge.png)](https://mseep.ai/app/zinja-coder-apktool-mcp-server)\n\nThank you Mseep.net for auditing and providing Assessment Badge.\n---\n\nBuilt with ❤️ for the reverse engineering and AI communities.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "android",
        "apktool",
        "apks",
        "engineering android",
        "android apks",
        "coder apktool"
      ],
      "category": "scientific-research-tools"
    },
    "zongmin-yu--semantic-scholar-fastmcp-mcp-server": {
      "owner": "zongmin-yu",
      "name": "semantic-scholar-fastmcp-mcp-server",
      "url": "https://github.com/zongmin-yu/semantic-scholar-fastmcp-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/zongmin-yu.webp",
      "description": "Provides access to academic paper data, author information, and citation networks through the Semantic Scholar API. Facilitates interaction with various academic resources for research purposes.",
      "stars": 63,
      "forks": 11,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-10-03T23:36:41Z",
      "readme_content": "# Semantic Scholar MCP Server\n\n[![smithery badge](https://smithery.ai/badge/semantic-scholar-fastmcp-mcp-server)](https://smithery.ai/server/semantic-scholar-fastmcp-mcp-server)\n\nA FastMCP server implementation for the Semantic Scholar API, providing comprehensive access to academic paper data, author information, and citation networks.\n\n## Project Structure\n\nThe project has been refactored into a modular structure for better maintainability:\n\n```\nsemantic-scholar-server/\n├── semantic_scholar/            # Main package\n│   ├── __init__.py             # Package initialization\n│   ├── server.py               # Server setup and main functionality\n│   ├── mcp.py                  # Centralized FastMCP instance definition\n│   ├── config.py               # Configuration classes\n│   ├── utils/                  # Utility modules\n│   │   ├── __init__.py\n│   │   ├── errors.py           # Error handling\n│   │   └── http.py             # HTTP client and rate limiting\n│   ├── api/                    # API endpoints\n│       ├── __init__.py\n│       ├── papers.py           # Paper-related endpoints\n│       ├── authors.py          # Author-related endpoints\n│       └── recommendations.py  # Recommendation endpoints\n├── run.py                      # Entry point script\n```\n\nThis structure:\n\n- Separates concerns into logical modules\n- Makes the codebase easier to understand and maintain\n- Allows for better testing and future extensions\n- Keeps related functionality grouped together\n- Centralizes the FastMCP instance to avoid circular imports\n\n## Features\n\n- **Paper Search & Discovery**\n\n  - Full-text search with advanced filtering\n  - Title-based paper matching\n  - Paper recommendations (single and multi-paper)\n  - Batch paper details retrieval\n  - Advanced search with ranking strategies\n\n- **Citation Analysis**\n\n  - Citation network exploration\n  - Reference tracking\n  - Citation context and influence analysis\n\n- **Author Information**\n\n  - Author search and profile details\n  - Publication history\n  - Batch author details retrieval\n\n- **Advanced Features**\n  - Complex search with multiple ranking strategies\n  - Customizable field selection\n  - Efficient batch operations\n  - Rate limiting compliance\n  - Support for both authenticated and unauthenticated access\n  - Graceful shutdown and error handling\n  - Connection pooling and resource management\n\n## System Requirements\n\n- Python 3.8+\n- FastMCP framework\n- Environment variable for API key (optional)\n\n## Installation\n\n### Installing via Smithery\n\nTo install Semantic Scholar MCP Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/semantic-scholar-fastmcp-mcp-server):\n\n```bash\nnpx -y @smithery/cli install semantic-scholar-fastmcp-mcp-server --client claude\n```\n\n### Manual Installation\n\n1. Clone the repository:\n\n```bash\ngit clone https://github.com/YUZongmin/semantic-scholar-fastmcp-mcp-server.git\ncd semantic-scholar-server\n```\n\n2. Install FastMCP and other dependencies following: https://github.com/jlowin/fastmcp\n\n3. Configure FastMCP:\n\nFor Claude Desktop users, you'll need to configure the server in your FastMCP configuration file. Add the following to your configuration (typically in `~/.config/claude-desktop/config.json`):\n\n```json\n{\n  \"mcps\": {\n    \"Semantic Scholar Server\": {\n      \"command\": \"/path/to/your/venv/bin/fastmcp\",\n      \"args\": [\n        \"run\",\n        \"/path/to/your/semantic-scholar-server/run.py\"\n      ],\n      \"env\": {\n        \"SEMANTIC_SCHOLAR_API_KEY\": \"your-api-key-here\"  # Optional\n      }\n    }\n  }\n}\n```\n\nMake sure to:\n\n- Replace `/path/to/your/venv/bin/fastmcp` with the actual path to your FastMCP installation\n- Replace `/path/to/your/semantic-scholar-server/run.py` with the actual path to run.py on your machine\n- If you have a Semantic Scholar API key, add it to the `env` section. If not, you can remove the `env` section entirely\n\n4. Start using the server:\n\nThe server will now be available to your Claude Desktop instance. No need to manually run any commands - Claude will automatically start and manage the server process when needed.\n\n### API Key (Optional)\n\nTo get higher rate limits and better performance:\n\n1. Get an API key from [Semantic Scholar API](https://www.semanticscholar.org/product/api)\n2. Add it to your FastMCP configuration as shown above in the `env` section\n\nIf no API key is provided, the server will use unauthenticated access with lower rate limits.\n\n## Configuration\n\n### Environment Variables\n\n- `SEMANTIC_SCHOLAR_API_KEY`: Your Semantic Scholar API key (optional)\n  - Get your key from [Semantic Scholar API](https://www.semanticscholar.org/product/api)\n  - If not provided, the server will use unauthenticated access\n\n### Rate Limits\n\nThe server automatically adjusts to the appropriate rate limits:\n\n**With API Key**:\n\n- Search, batch and recommendation endpoints: 1 request per second\n- Other endpoints: 10 requests per second\n\n**Without API Key**:\n\n- All endpoints: 100 requests per 5 minutes\n- Longer timeouts for requests\n\n## Available MCP Tools\n\n> Note: All tools are aligned with the official [Semantic Scholar API documentation](https://api.semanticscholar.org/api-docs/). Please refer to the official documentation for detailed field specifications and the latest updates.\n\n### Paper Search Tools\n\n- `paper_relevance_search`: Search for papers using relevance ranking\n\n  - Supports comprehensive query parameters including year range and citation count filters\n  - Returns paginated results with customizable fields\n\n- `paper_bulk_search`: Bulk paper search with sorting options\n\n  - Similar to relevance search but optimized for larger result sets\n  - Supports sorting by citation count, publication date, etc.\n\n- `paper_title_search`: Find papers by exact title match\n\n  - Useful for finding specific papers when you know the title\n  - Returns detailed paper information with customizable fields\n\n- `paper_details`: Get comprehensive details about a specific paper\n\n  - Accepts various paper ID formats (S2 ID, DOI, ArXiv, etc.)\n  - Returns detailed paper metadata with nested field support\n\n- `paper_batch_details`: Efficiently retrieve details for multiple papers\n  - Accepts up to 1000 paper IDs per request\n  - Supports the same ID formats and fields as single paper details\n\n### Citation Tools\n\n- `paper_citations`: Get papers that cite a specific paper\n\n  - Returns paginated list of citing papers\n  - Includes citation context when available\n  - Supports field customization and sorting\n\n- `paper_references`: Get papers referenced by a specific paper\n  - Returns paginated list of referenced papers\n  - Includes reference context when available\n  - Supports field customization and sorting\n\n### Author Tools\n\n- `author_search`: Search for authors by name\n\n  - Returns paginated results with customizable fields\n  - Includes affiliations and publication counts\n\n- `author_details`: Get detailed information about an author\n\n  - Returns comprehensive author metadata\n  - Includes metrics like h-index and citation counts\n\n- `author_papers`: Get papers written by an author\n\n  - Returns paginated list of author's publications\n  - Supports field customization and sorting\n\n- `author_batch_details`: Get details for multiple authors\n  - Efficiently retrieve information for up to 1000 authors\n  - Returns the same fields as single author details\n\n### Recommendation Tools\n\n- `paper_recommendations_single`: Get recommendations based on a single paper\n\n  - Returns similar papers based on content and citation patterns\n  - Supports field customization for recommended papers\n\n- `paper_recommendations_multi`: Get recommendations based on multiple papers\n  - Accepts positive and negative example papers\n  - Returns papers similar to positive examples and dissimilar to negative ones\n\n## Usage Examples\n\n### Basic Paper Search\n\n```python\nresults = await paper_relevance_search(\n    context,\n    query=\"machine learning\",\n    year=\"2020-2024\",\n    min_citation_count=50,\n    fields=[\"title\", \"abstract\", \"authors\"]\n)\n```\n\n### Paper Recommendations\n\n```python\n# Single paper recommendation\nrecommendations = await paper_recommendations_single(\n    context,\n    paper_id=\"649def34f8be52c8b66281af98ae884c09aef38b\",\n    fields=\"title,authors,year\"\n)\n\n# Multi-paper recommendation\nrecommendations = await paper_recommendations_multi(\n    context,\n    positive_paper_ids=[\"649def34f8be52c8b66281af98ae884c09aef38b\", \"ARXIV:2106.15928\"],\n    negative_paper_ids=[\"ArXiv:1805.02262\"],\n    fields=\"title,abstract,authors\"\n)\n```\n\n### Batch Operations\n\n```python\n# Get details for multiple papers\npapers = await paper_batch_details(\n    context,\n    paper_ids=[\"649def34f8be52c8b66281af98ae884c09aef38b\", \"ARXIV:2106.15928\"],\n    fields=\"title,authors,year,citations\"\n)\n\n# Get details for multiple authors\nauthors = await author_batch_details(\n    context,\n    author_ids=[\"1741101\", \"1780531\"],\n    fields=\"name,hIndex,citationCount,paperCount\"\n)\n```\n\n## Error Handling\n\nThe server provides standardized error responses:\n\n```python\n{\n    \"error\": {\n        \"type\": \"error_type\",  # rate_limit, api_error, validation, timeout\n        \"message\": \"Error description\",\n        \"details\": {\n            # Additional context\n            \"authenticated\": true/false  # Indicates if request was authenticated\n        }\n    }\n}\n```\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "scholar",
        "academic",
        "research",
        "scholar api",
        "semantic scholar",
        "scholar fastmcp"
      ],
      "category": "scientific-research-tools"
    },
    "zygi--r-playground-mcp": {
      "owner": "zygi",
      "name": "r-playground-mcp",
      "url": "https://github.com/zygi/r-playground-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/zygi.webp",
      "description": "Executes R code, visualizes plots, and interacts with scientific data within stateful sessions. Supports multimodal outputs to enhance conversations around scientific topics.",
      "stars": 2,
      "forks": 1,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-06-06T20:19:18Z",
      "readme_content": "# MCP R Playground\n[![PyPI version](https://img.shields.io/pypi/v/rplayground-mcp.svg)](https://pypi.org/project/rplayground-mcp/)\n[![smithery badge](https://smithery.ai/badge/@zygi/r-playground-mcp)](https://smithery.ai/server/@zygi/r-playground-mcp)\n\nAn MCP server that allows AI models to execute R code, see its results, and draw and observe plots. \nIt can be used for sophisticated agentic deployments, but also as a way to augment AI clients like Claude Desktop when talking to them about scientific papers. \n\n## Features:\n- Stateful sessions: each conversation thread gets a new session, but the session can persist across calls and user/assistant interactions. \n- Graphics output: multimodal models can draw plots using standard R libraries like ggplot, see those plots, and react to them.\n- Works in all common operating systems/architectures - Windows x64 / arm64, MacOS, Linux.\n- When running locally, ⚠️__NO HOST ISOLATION__⚠️: while each session runs as a separate R environment, they have access to global dependencies and all files on the computer. While unlikely, a rogue model could write R code that deletes your important files.\n    - If you need host isolation, you should run this MCP in Docker. Instructions for that are provided below.\n\n## Configuration\nCurrently there's just one configuration parameter that can be set as an environment variable:\n- `RPLAYGROUND_MCP_SUPPORT_IMAGE_OUTPUT`, default True. If set to False, image output will be disabled, and tool descriptions will be made to reflect that.\n\n## Usage\nBy default, the AIs will have access to all globally installed R packages, and can install whatever package they want. These installations will persist. You can pre-install important packages to make them available in advance.\n\n\n## Installation\nBasic instructions for technical users:\n1) Have R installed, and the R_HOME environment variable set\n2) Have a recent version of the `uv` installed\n3) run `uvx --python=3.13 rplayground-mcp`, and it should just work.\n\n## Detailed Installation\nThis section is for less technical users who want to set up this MCP to use with Claude Desktop or similar AI user interfaces that support MCP extensions.\n\n### Windows\n- Make sure you've set up Claude Desktop to create the MCP configuration file by following instructions here: https://modelcontextprotocol.io/quickstart/user\n- Make sure you have R installed. The recommended source is here https://cran.rstudio.com/ .\n- Make sure you have `uv` installed. `uv` is the project management tool for Python, the programming language this tool is written in. More detailed instructions can be found here https://docs.astral.sh/uv/getting-started/installation/#pypi, we provide the instructions for the most straightforward method:\n    1) Open the Terminal app\n    2) In the terminal, paste in the following installation command: `powershell -ExecutionPolicy ByPass -c \"irm https://astral.sh/uv/install.ps1 | iex\"`\n    3) Close the Terminal app and reopen it\n    4) type in `uv` and confirm you don't see any red errors.\n- We have provided a helper script that you can use to set up the MCP server to work with Claude Desktop. You can run it with `uv run --python=3.13 https://raw.githubusercontent.com/zygi/r-playground-mcp/refs/heads/master/scripts/setup_helper.py`. With your permission, it will:\n    - Set the R_HOME environment variable to your R installation\n    - Install the MCP inside your Claude Desktop configuration.\n- That's it! Starting Claude Desktop should now display the tools as available. Or just ask it to \"test out the `execute_r_command` tool\".\n\n\n### MacOS\n- Make sure you've set up Claude Desktop to create the MCP configuration file by following instructions here: https://modelcontextprotocol.io/quickstart/user\n- Make sure you have R installed. The recommended source is here https://cran.rstudio.com/ .\n- Make sure you have `uv` installed. `uv` is the project management tool for Python, the programming language this tool is written in. More detailed instructions can be found here https://docs.astral.sh/uv/getting-started/installation/#pypi, we provide the instructions for the most straightforward method:\n    1) Open the Terminal app\n    2) In the terminal, paste in the following installation command: `curl -LsSf https://astral.sh/uv/install.sh | sh`\n    3) Close the Terminal app and reopen it\n    4) type in `uv` and confirm you don't see any red errors.\n- We have provided a helper script that you can use to set up the MCP server to work with Claude Desktop. You can run it with `uv run --python=3.13 https://raw.githubusercontent.com/zygi/r-playground-mcp/refs/heads/master/scripts/setup_helper.py`. With your permission, it will:\n    - Set the R_HOME environment variable to your R installation\n    - Install the MCP inside your Claude Desktop configuration.\n- That's it! Starting Claude Desktop should now display the tools as available. Or just ask it to \"test out the `execute_r_command` tool\".\n\n## Installation (Docker)\nWe also provide Dockerfiles to run this MCP in an isolated context. This only supports the platform linux/amd64. \nThe image uses the [`r2u`](https://github.com/eddelbuettel/r2u) project to make precompiled CRAN packages available. The Dockerfile comes with two versions:\n- Slim: `docker build --platform=linux/amd64 .`. This sets up the `r2u` repositories but installing a new package from the R session will still take ~10s.\n- Fat: `docker build --build-arg PREINSTALL_PACKAGES=true --platform=linux/amd64 .`. This preinstalls a big selection of packages to the Docker image, and loading them in an MCP session becomes instant.\n\nYou are also welcome to edit the Dockerfile and preinstall just the packages relevant to your workflow.\n\n## Issues and Contributions\nFeel free to create an Issue if you have questions or requests. Small PRs are welcome anytime, larger PRs should be discussed by creating an Issue before a PR is started. \n\n\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "multimodal",
        "zygi",
        "tools",
        "tools zygi",
        "research tools",
        "interacts scientific"
      ],
      "category": "scientific-research-tools"
    },
    "zzaebok--mcp-wikidata": {
      "owner": "zzaebok",
      "name": "mcp-wikidata",
      "url": "https://github.com/zzaebok/mcp-wikidata",
      "imageUrl": "/freedevtools/mcp/pfp/zzaebok.webp",
      "description": "Interact with Wikidata to search for entities, extract metadata, and execute SPARQL queries for enhanced data insights.",
      "stars": 29,
      "forks": 6,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-10-02T01:29:15Z",
      "readme_content": "## Wikidata MCP Server\n\n[![smithery badge](https://smithery.ai/badge/@zzaebok/mcp-wikidata)](https://smithery.ai/server/@zzaebok/mcp-wikidata)\n\nA server implementation for Wikidata API using the Model Context Protocol (MCP).\nThis project provides tools to interact with Wikidata, such as **searching identifiers** (entity and property), **extracting metadata** (label and description) and **executing sparql query**.\n\n---\n\n### Installation\n\n#### Installing via Smithery\n\nTo install Wikidata MCP Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@zzaebok/mcp-wikidata):\n\n```bash\nnpx -y @smithery/cli install @zzaebok/mcp-wikidata --client claude\n```\n\n#### Installing Manually\nInstall `uv` if it is not installed yet.\n\n```bash\n$ curl -LsSf https://astral.sh/uv/install.sh | sh\n```\n\nThen, install dependencies.\n\n```bash\n$ git clone https://github.com/zzaebok/mcp-wikidata.git\n$ cd mcp-wikidata\n$ uv sync\n# if you want to run client example together\n$ uv sync --extra example\n```\n\n---\n\n### Run\n\nRun the server with:\n\n```bash\n$ uv run src/server.py\n```\n\nIf you want to test it with a simple client code (with `langchain-mcp-adapters`), run the client with:\n\n```bash\n# in another shell\n$ uv run src/client.py\n```\n\nThe LLM extracts valid entity and property identifiers, executes a sparql query, and finally recommend a movie directed by Bong Joon-ho.\n\n<details>\n  <summary><i>See the execution output</i></summary>\n\n```python\n{\n  \"messages\": [\n      HumanMessage(\n          content=\"Can you recommend me a movie directed by Bong Joonho?\",\n      ),\n      AIMessage(\n          tool_calls=[\n              {\n                  \"name\": \"search_entity\",\n                  \"args\": {\"query\": \"Bong Joon-ho\"},\n              }\n          ],\n      ),\n      ToolMessage(\n          content=\"Q495980\",\n          name=\"search_entity\",\n      ),\n      AIMessage(\n          tool_calls=[\n              {\n                  \"name\": \"get_properties\",\n                  \"args\": {\"entity_id\": \"Q495980\"},\n              }\n          ],\n      ),\n      ToolMessage(\n          content='[\"P345\", \"P244\", \"P214\", \"P227\", ...]',\n          name=\"get_properties\",\n      ),\n      AIMessage(\n          tool_calls=[\n              {\n                  \"name\": \"search_property\",\n                  \"args\": {\"query\": \"director\"},\n              }\n          ],\n      ),\n      ToolMessage(\n          content=\"P57\",\n          name=\"search_property\",\n      ),\n      AIMessage(\n          tool_calls=[\n              {\n                  \"name\": \"execute_sparql\",\n                  \"args\": {\n                      \"sparql_query\": 'SELECT ?film ?filmLabel WHERE {\\n  ?film wdt:P57 wd:Q495980.\\n  SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\". }\\n} LIMIT 1'\n                  },\n              }\n          ],\n      ),\n      ToolMessage(\n          content='[{\"film\": {\"type\": \"uri\", \"value\": \"http://www.wikidata.org/entity/Q483761\"}, \"filmLabel\": {\"xml:lang\": \"en\", \"type\": \"literal\", \"value\": \"Mother\"}}]',\n          name=\"execute_sparql\",\n      ),\n      AIMessage(\n          content='I recommend the movie \"Mother,\" which was directed by Bong Joon-ho.',\n      ),\n  ]\n}\n```\n\n</details>\n\n---\n\n### Wikidata MCP Tools\n\nThe following tools are implemented in the server:\n\n| Tool                                                 | Description                                                                |\n| ---------------------------------------------------- | -------------------------------------------------------------------------- |\n| `search_entity(query: str)`                          | Search for a Wikidata entity ID by its query.                              |\n| `search_property(query: str)`                        | Search for a Wikidata property ID by its query.                            |\n| `get_properties(entity_id: str)`                     | Get the properties associated with a given Wikidata entity ID.             |\n| `execute_sparql(sparql_query: str)`                  | Execute a SPARQL query on Wikidata.                                        |\n| `get_metadata(entity_id: str, language: str = \"en\")` | Retrieve the English label and description for a given Wikidata entity ID. |\n\n---\n\n#### License\n\nMIT License\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "wikidata",
        "metadata",
        "sparql",
        "wikidata search",
        "wikidata interact",
        "mcp wikidata"
      ],
      "category": "scientific-research-tools"
    }
  }
}