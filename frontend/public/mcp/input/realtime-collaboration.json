{
  "category": "realtime-collaboration",
  "categoryDisplay": "Real-Time Collaboration",
  "description": "",
  "totalRepositories": 55,
  "repositories": {
    "1313057--TEN-Agent": {
      "owner": "1313057",
      "name": "TEN-Agent",
      "url": "https://github.com/1313057/TEN-Agent",
      "imageUrl": "/freedevtools/mcp/pfp/1313057.webp",
      "description": "TEN Agent is an open-source server designed to enhance AI models by integrating various features that allow them to process multiple types of data and respond in real-time.",
      "stars": 0,
      "forks": 0,
      "license": "Apache License 2.0",
      "language": "",
      "updated_at": "2024-12-18T16:43:07Z",
      "readme_content": "<div align=\"center\">\n\n[![Follow on X](https://img.shields.io/twitter/follow/TenFramework?logo=X&color=%20%23f5f5f5)](https://twitter.com/intent/follow?screen_name=TenFramework)\n[![Discussion posts](https://img.shields.io/github/discussions/TEN-framework/ten-agent?labelColor=%20%23FDB062&color=%20%23f79009)](https://github.com/TEN-framework/ten-agent/discussions/)\n[![Commits](https://img.shields.io/github/commit-activity/m/TEN-framework/ten-agent?labelColor=%20%237d89b0&color=%20%235d6b98)](https://github.com/TEN-framework/ten-agent/graphs/commit-activity)\n[![Issues closed](https://img.shields.io/github/issues-search?query=repo%3ATEN-framework%2Ften-agent%20is%3Aclosed&label=issues%20closed&labelColor=green&color=green)](https://github.com/TEN-framework/ten-agent/issues)\n[![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square)](https://github.com/TEN-framework/ten-agent/pulls)\n[![GitHub license](https://img.shields.io/badge/License-Apache_2.0-blue.svg?labelColor=%20%23155EEF&color=%20%23528bff)](https://github.com/TEN-framework/ten-agent/blob/main/LICENSE)\n\n[](https://discord.gg/VnPftUzAMJ)\n\n<a href=\"https://trendshift.io/repositories/11978\" target=\"_blank\"><img src=\"https://trendshift.io/api/badge/repositories/11978\" alt=\"TEN-framework%2FTEN-Agent | Trendshift\" style=\"width: 250px; height: 55px;\" width=\"250\" height=\"55\"/></a>\n\n[![GitHub watchers](https://img.shields.io/github/watchers/TEN-framework/ten-agent?style=social&label=Watch)](https://GitHub.com/TEN-framework/ten-agent/watchers/?WT.mc_id=academic-105485-koreyst)\n[![GitHub forks](https://img.shields.io/github/forks/TEN-framework/ten-agent?style=social&label=Fork)](https://GitHub.com/TEN-framework/ten-agent/network/?WT.mc_id=academic-105485-koreyst)\n[![GitHub stars](https://img.shields.io/github/stars/TEN-framework/ten-agent?style=social&label=Star)](https://GitHub.com/TEN-framework/ten-agent/stargazers/?WT.mc_id=academic-105485-koreyst)\n\n<a href=\"https://github.com/TEN-framework/ten-agent/blob/main/README.md\"><img alt=\"README in English\" src=\"https://img.shields.io/badge/English-lightgrey\"></a>\n<a href=\"https://github.com/ten-framework/ten-agent/blob/main/docs/readmes/README-CN.md\"><img alt=\"简体中文操作指南\" src=\"https://img.shields.io/badge/简体中文-lightgrey\"></a>\n<a href=\"https://github.com/ten-framework/ten-agent/blob/main/docs/readmes/README-JP.md\"><img alt=\"日本語のREADME\" src=\"https://img.shields.io/badge/日本語-lightgrey\"></a>\n<a href=\"https://github.com/ten-framework/ten-agent/blob/main/docs/readmes/README-KR.md\"><img alt=\"README in 한국어\" src=\"https://img.shields.io/badge/한국어-lightgrey\"></a>\n<a href=\"https://github.com/ten-framework/ten-agent/blob/main/docs/readmes/README-ES.md\"><img alt=\"README en Español\" src=\"https://img.shields.io/badge/Español-lightgrey\"></a>\n<a href=\"https://github.com/ten-framework/ten-agent/blob/main/docs/readmes/README-FR.md\"><img alt=\"README en Français\" src=\"https://img.shields.io/badge/Français-lightgrey\"></a>\n<a href=\"https://github.com/ten-framework/ten-agent/blob/main/docs/readmes/README-IT.md\"><img alt=\"README in Italiano\" src=\"https://img.shields.io/badge/Italiano-lightgrey\"></a>\n\n[Getting Started](https://doc.theten.ai/ten-agent/getting_started)\n<span>&nbsp;&nbsp;•&nbsp;&nbsp;</span>\n[Create Extensions](https://doc.theten.ai/ten-agent/create_a_hello_world_extension)\n<span>&nbsp;&nbsp;•&nbsp;&nbsp;</span>\n[TEN Framework Repository](https://github.com/TEN-framework/ten_framework)\n\n</div>\n\n<br>\n<h2>🌟 Gemini Multimodal Live API Extension with RTC</h2>\n<!-- \n![Usecases](https://github.com/TEN-framework/docs/blob/main/assets/jpg/gemini-with-ten.jpg?raw=true) -->\n\n\n\n[agent.theten.ai](https://agent.theten.ai)\n\nTry **Google Gemini Multimodal Live API** with **realtime vision** and **realtime screenshare detection** capabilities, it is a ready-to-use extension, along with powerful tools like **Weather Check** and **Web Search** integrated perfectly into TEN Agent.\n\n\n<br>\n<h2>TEN Agent Usecases</h2>\n\n\n\n<br>\n<h2>Ready-to-use Extensions</h2>\n\n\n\n\n<br>\n<h2>TEN Agent Playground in Local Environment</h2>\n\n### Prerequisites\n\n| Category | Requirements |\n|----------|-------------|\n| **Keys** | • Agora [ App ID ](https://docs.agora.io/en/video-calling/get-started/manage-agora-account?platform=web#create-an-agora-project) and [ App Certificate ](https://docs.agora.io/en/video-calling/get-started/manage-agora-account?platform=web#create-an-agora-project)(free minutes every month) <br>• [OpenAI](https://openai.com/index/openai-api/) API key<br>• [ Deepgram ](https://deepgram.com/) ASR (free credits available with signup)<br>• [ FishAudio ](https://fish.audio/) TTS (free credits available with signup)|\n| **Installation** | • [Docker](https://www.docker.com/) / [Docker Compose](https://docs.docker.com/compose/)<br>• [Node.js(LTS) v18](https://nodejs.org/en) |\n| **Minimum System Requirements** | • CPU >= 2 Core<br>• RAM >= 4 GB |\n\n<br>\n\n### macOS: Docker setting on Apple Silicon\n\nFor Apple Silicon Macs, uncheck \"Use Rosetta for x86/amd64 emulation\" in Docker settings. Note: This may result in slower build times on ARM, but performance will be normal when deployed to x64 servers.\n\n\n\n<br>\n\n### Next step\n\n#### 1. Create `.env` file\n\n```bash\ncp ./.env.example ./.env\n```\n\n#### 2. Setup Agora App ID and App Certificate in `.env`\n\n```bash\nAGORA_APP_ID=\nAGORA_APP_CERTIFICATE=\n```\n\n#### 3. Start agent development containers\n```bash\ndocker compose up -d\n```\n\n#### 4. Enter container\n```bash\ndocker exec -it ten_agent_dev bash\n```\n\n#### 5. Build agent \n```bash\ntask use\n```\n\n#### 6. Start the web server\n```bash\ntask run\n```\n\n#### 7. Edit playground settings\nOpen the playground at [localhost:3000](http://localhost:3000) to configure your agent.\n 1. Select a graph type (e.g. Voice Agent, Realtime Agent)\n 2. Choose a corresponding module\n 3. Select an extension and configure its API key settings\n\n\n\n#### Running Gemini Realtime Extension\nOpen the playground at [localhost:3000](http://localhost:3000).\n\n 1. Select voice_assistant_realtime graph\n 2. Choose Gemini Realtime module\n 3. Select v2v extension and enter Gemini API key\n\n\n\n<br>\n<h2>TEN Agent Components</h2>\n\n\n\n<br>\n<h2>Stay Tuned</h2>\n\nBefore we get started, be sure to star our repository and get instant notifications for all new releases!\n\n\n\n<br>\n<h2>Join Community</h2>\n\n- [Discord](https://discord.gg/VnPftUzAMJ): Ideal for sharing your applications and engaging with the community.\n- [GitHub Discussion](https://github.com/TEN-framework/ten-agent/discussions): Perfect for providing feedback and asking questions.\n- [GitHub Issues](https://github.com/TEN-framework/ten-agent/issues): Best for reporting bugs and proposing new features. Refer to our [contribution guidelines](./docs/code-of-conduct/contributing.md) for more details.\n- [X](https://img.shields.io/twitter/follow/TenFramework?logo=X&color=%20%23f5f5f5): Great for sharing your agents and interacting with the community.\n\n<br>\n<h2>Star History</h2>\n\n[![Star History Chart](https://api.star-history.com/svg?repos=ten-framework/ten-agent&type=Date)](https://star-history.com/#ten-framework/ten-agent&Date)\n\n <br>\n <h2>Code Contributors</h2>\n\n[![TEN](https://contrib.rocks/image?repo=TEN-framework/ten-agent)](https://github.com/TEN-framework/ten-agent/graphs/contributors)\n\n<br>\n<h2>Contribution Guidelines</h2>\n\nContributions are welcome! Please read the [contribution guidelines](./docs/code-of-conduct/contributing.md) first.\n\n<br>\n<h2>License</h2>\n\nThis project is licensed under the Apache 2.0 License - see the [LICENSE](LICENSE) file for details.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "agent",
        "ai",
        "realtime",
        "realtime collaboration",
        "agent agent",
        "agent open"
      ],
      "category": "realtime-collaboration"
    },
    "3rzy--make-mcp-integration-issue": {
      "owner": "3rzy",
      "name": "make-mcp-integration-issue",
      "url": "https://github.com/3rzy/make-mcp-integration-issue",
      "imageUrl": "/freedevtools/mcp/pfp/3rzy.webp",
      "description": "The Make MCP Server enables users to automate workflows by connecting the Make automation platform with Claude Desktop, using the Model Context Protocol for seamless data exchange and communication.",
      "stars": 0,
      "forks": 2,
      "license": "No License",
      "language": "JavaScript",
      "updated_at": "2025-03-04T20:13:10Z",
      "readme_content": "# Problem integracji Make z Claude poprzez MCP\n\n## Opis problemu\n\nPróbowałem stworzyć integrację między platformą automatyzacji Make (dawniej Integromat) a Claude Desktop za pomocą protokołu MCP (Model Context Protocol). Mimo wielu podejść i modyfikacji, nie udało się ustanowić poprawnego połączenia.\n\n## Co zostało zrobione\n\n1. Stworzenie od podstaw serwera MCP dla Make w Node.js, implementującego:\n   - Komunikację z API Make\n   - Protokół WebSocket dla komunikacji z Claude Desktop\n   - Obsługę JSON-RPC 2.0 zgodną z formatem używanym przez MCP\n   - Różne narzędzia do zarządzania scenariuszami Make\n\n2. Wypróbowanie różnych konfiguracji:\n   - Różne porty (3000, 3001, 3333, 5555)\n   - Różne podejścia do komunikacji (REST API, WebSocket)\n   - Różne opcje konfiguracji w `claude_desktop_config.json`\n   - Zarówno automatyczne jak i ręczne uruchamianie serwera\n\n## Napotkane problemy\n\n1. **Problem z portem** - Claude Desktop próbował uruchomić własną instancję serwera, co powodowało konflikty na tym samym porcie:\n   ```\n   Error: listen EADDRINUSE: address already in use :::3001\n   ```\n\n2. **Problem z protokołem** - Mimo implementacji JSON-RPC 2.0 po stronie serwera, Claude pokazywał komunikat:\n   ```\n   Server disconnected. For troubleshooting guidance, please visit our debugging documentation\n   ```\n   i\n   ```\n   Could not attach to MCP server make\n   ```\n\n3. **Niekompatybilność protokołu** - Logi pokazywały, że Claude oczekuje określonego formatu komunikacji, który trudno odtworzyć bez dokładnej specyfikacji:\n   ```\n   Message from client: {\"method\":\"initialize\",\"params\":{\"protocolVersion\":\"2024-11-05\",\"capabilities\":{},\"clientInfo\":{\"name\":\"claude-ai\",\"version\":\"0.1.0\"}},\"jsonrpc\":\"2.0\",\"id\":0}\n   ```\n\n## Zrzuty ekranu\n\n### Błąd połączenia w Claude Desktop\n\n\n\n### Błędy w logach serwera\n\n \n\n### Edycja konfiguracji env\n\n\n\n## Kod serwera\n\nGłówna implementacja serwera MCP dla Make:\n\n```javascript\nconst express = require('express');\nconst WebSocket = require('ws');\nconst http = require('http');\nconst axios = require('axios');\nconst dotenv = require('dotenv');\n\n// Załaduj zmienne środowiskowe\ndotenv.config();\n\nconst PORT = process.env.PORT || 5555;\nconst MAKE_API_TOKEN = process.env.MAKE_API_TOKEN;\n\n// Utworzenie serwera Express\nconst app = express();\nconst server = http.createServer(app);\nconst wss = new WebSocket.Server({ server });\n\n// Obsługa połączeń WebSocket\nwss.on('connection', (ws) => {\n  console.log('Nowe połączenie WebSocket nawiązane');\n  \n  ws.on('message', (message) => {\n    try {\n      const data = JSON.parse(message);\n      console.log('Otrzymano wiadomość:', data);\n      \n      // Obsługa żądania inicjalizacji\n      if (data.method === 'initialize') {\n        console.log('Obsługa żądania initialize');\n        ws.send(JSON.stringify({\n          jsonrpc: \"2.0\",\n          id: data.id,\n          result: {\n            serverInfo: {\n              name: \"simple-make-mcp-server\",\n              version: \"1.0.0\"\n            },\n            capabilities: {}\n          }\n        }));\n      }\n      // Obsługa listy narzędzi\n      else if (data.method === 'tools/list') {\n        console.log('Obsługa żądania tools/list');\n        ws.send(JSON.stringify({\n          jsonrpc: \"2.0\",\n          id: data.id,\n          result: {\n            tools: [\n              {\n                name: \"list_scenarios\",\n                description: \"Pobiera listę wszystkich scenariuszy w Make\"\n              },\n              {\n                name: \"run_scenario\",\n                description: \"Uruchamia scenariusz w Make\"\n              }\n            ]\n          }\n        }));\n      }\n      // Obsługa błędu nieobsługiwanej metody\n      else {\n        console.log(`Nieobsługiwana metoda: ${data.method}`);\n        ws.send(JSON.stringify({\n          jsonrpc: \"2.0\",\n          id: data.id,\n          error: {\n            code: -32601,\n            message: `Metoda '${data.method}' nie jest obsługiwana`\n          }\n        }));\n      }\n    } catch (error) {\n      console.error('Błąd przetwarzania wiadomości:', error);\n    }\n  });\n  \n  ws.on('close', () => {\n    console.log('Połączenie WebSocket zamknięte');\n  });\n});\n\n// Podstawowy endpoint HTTP\napp.get('/', (req, res) => {\n  res.send('Simple Make MCP Server is running');\n});\n\n// Uruchomienie serwera\nserver.listen(PORT, () => {\n  console.log(`Serwer Make MCP nasłuchuje na porcie ${PORT}`);\n});\n```\n\n## Konfiguracja Claude Desktop\n\nPróbowałem różnych konfiguracji w pliku `claude_desktop_config.json`, w tym:\n\n```json\n\"make\": {\n  \"command\": \"node\",\n  \"args\": [\"/Users/krzyk/google-workspace-mcp/make-mcp-server/make-mcp-server.js\"],\n  \"env\": {\n    \"MAKE_API_TOKEN\": \"[token]\",\n    \"PORT\": \"3001\"\n  },\n  \"url\": \"ws://localhost:3001\",\n  \"disabled\": true\n}\n```\n\nI prostszą wersję:\n\n```json\n\"make\": {\n  \"disabled\": true,\n  \"url\": \"ws://localhost:5555\"\n}\n```\n\n## Pytania i prośby o pomoc\n\n1. Czy istnieje dokładna dokumentacja protokołu MCP, która mogłaby pomóc w zrozumieniu oczekiwanego formatu komunikacji?\n\n2. Czy ktokolwiek ma doświadczenie w tworzeniu niestandardowych serwerów MCP, które działają z Claude Desktop?\n\n3. Czy są jakieś szczególne wymagania dotyczące protokołu WebSocket lub JSON-RPC 2.0, które nie są oczywiste?\n\n4. Czy są jakieś oficjalne narzędzia lub biblioteki, które mogłyby ułatwić tworzenie zgodnych serwerów MCP?\n\nWszelkie wskazówki będą bardzo pomocne. Przełączyłem się tymczasowo na próbę użycia n8n, które ma już gotowe implementacje serwera MCP, ale nadal jestem zainteresowany rozwiązaniem tego problemu dla Make.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "automate",
        "automation",
        "realtime",
        "make automation",
        "realtime collaboration",
        "connecting make"
      ],
      "category": "realtime-collaboration"
    },
    "Chandrakant0110--slack-mcp": {
      "owner": "Chandrakant0110",
      "name": "slack-mcp",
      "url": "https://github.com/Chandrakant0110/slack-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/Chandrakant0110.webp",
      "description": "Interact with Slack by managing channels, posting messages, and retrieving user profiles through a standardized API interface. Enhance team collaboration and workflow automation within a Slack workspace.",
      "stars": 0,
      "forks": 0,
      "license": "No License",
      "language": "JavaScript",
      "updated_at": "2025-03-10T12:46:55Z",
      "readme_content": "# Slack MCP (Model Context Protocol) Server\n\nThis is a Slack MCP server implementation that provides various Slack API functionalities through the Model Context Protocol. It allows AI models to interact with Slack through a standardized interface.\n\n## Features\n\n- List public channels\n- Post messages\n- Reply to threads\n- Add reactions\n- Get channel history\n- Get thread replies\n- List users\n- Get user profiles\n\n## Prerequisites\n\n- Node.js (v16 or higher)\n- npm or yarn\n- A Slack workspace with admin access\n- A Slack Bot Token\n- Your Slack Team ID\n\n## Installation\n\n1. Clone this repository\n2. Install dependencies:\n\n```bash\nnpm install\n# or\nyarn install\n```\n\n3. Build the TypeScript code:\n\n```bash\nnpm run build\n# or\nyarn build\n```\n\n## Configuration\n\n1. Open `index.ts` and replace the placeholder values:\n\n```typescript\npublic static readonly BOT_TOKEN = \"enter-your-bot-token-here\";\npublic static readonly TEAM_ID = \"enter-your-team-id-here\";\n```\n\nReplace these with your actual Slack Bot Token and Team ID.\n\n## Usage\n\n### Running the Server\n\nAfter building the project, you can run the server:\n\n```bash\nnode dist/index.js\n```\n\n### Setting up in Cursor\n\nTo use this MCP server in Cursor:\n\n1. Open Cursor settings\n2. Navigate to the \"Model Context Protocol\" section\n3. Add a new tool with the following configuration:\n   - Name: `slack`\n   - Command: `node /path/to/your/dist/index.js`\n   - Working Directory: `/path/to/your/project`\n\nReplace `/path/to/your` with the actual path to your project directory.\n\n## Available Tools\n\n1. `slack_list_channels`\n   - Lists public channels in the workspace\n   - Optional parameters: limit, cursor\n\n2. `slack_post_message`\n   - Posts a message to a channel\n   - Required parameters: channel_id, text\n\n3. `slack_reply_to_thread`\n   - Replies to a message thread\n   - Required parameters: channel_id, thread_ts, text\n\n4. `slack_add_reaction`\n   - Adds an emoji reaction to a message\n   - Required parameters: channel_id, timestamp, reaction\n\n5. `slack_get_channel_history`\n   - Gets recent messages from a channel\n   - Required parameters: channel_id\n   - Optional parameters: limit\n\n6. `slack_get_thread_replies`\n   - Gets all replies in a thread\n   - Required parameters: channel_id, thread_ts\n\n7. `slack_get_users`\n   - Lists all users in the workspace\n   - Optional parameters: limit, cursor\n\n8. `slack_get_user_profile`\n   - Gets detailed profile information for a user\n   - Required parameters: user_id\n\n## Development\n\nTo modify the server:\n\n1. Make changes to `index.ts`\n2. Rebuild the project:\n```bash\nnpm run build\n# or\nyarn build\n```\n\n## Security Notes\n\n- Never commit your actual Slack Bot Token or Team ID to version control\n- Consider using environment variables for production deployments\n- Ensure your Slack Bot has the necessary OAuth scopes for the actions you want to perform\n\n## Contributing\n\nFeel free to submit issues and pull requests for improvements.\n\n## License\n\nMIT ",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "slack",
        "collaboration",
        "realtime",
        "interact slack",
        "slack managing",
        "automation slack"
      ],
      "category": "realtime-collaboration"
    },
    "ChatterBoxIO--chatterboxio-mcp-server": {
      "owner": "ChatterBoxIO",
      "name": "chatterboxio-mcp-server",
      "url": "https://github.com/ChatterBoxIO/chatterboxio-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/ChatterBoxIO.webp",
      "description": "Integrates with online meeting platforms like Zoom and Google Meet to facilitate AI agents joining meetings, capturing transcripts, and generating concise summaries of discussions.",
      "stars": 7,
      "forks": 3,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-08-22T03:17:05Z",
      "readme_content": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/chatterboxio-chatterboxio-mcp-server-badge.png)](https://mseep.ai/app/chatterboxio-chatterboxio-mcp-server)\n\n# ChatterBox MCP Server\n\n[![smithery badge](https://smithery.ai/badge/@OverQuotaAI/chatterboxio-mcp-server)](https://smithery.ai/server/@OverQuotaAI/chatterboxio-mcp-server)\n\nA Model Context Protocol server implementation for ChatterBox, enabling AI agents to interact with online meetings and generate meeting summaries.\n\n<a href=\"https://glama.ai/mcp/servers/@ChatterBoxIO/chatterboxio-mcp-server\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@ChatterBoxIO/chatterboxio-mcp-server/badge\" alt=\"ChatterBox MCP Server\" />\n</a>\n\n## Overview\n\nThe ChatterBox MCP Server provides tools for AI agents to:\n\n- Join online meetings (Zoom, Google Meet, or Microsoft Teams)\n- Capture transcripts and recordings\n- Generate meeting summaries\n\n## Installation\n\n### Installing via Smithery\n\nTo install chatterboxio-mcp-server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@OverQuotaAI/chatterboxio-mcp-server):\n\n```bash\nnpx -y @smithery/cli install @OverQuotaAI/chatterboxio-mcp-server --client claude\n```\n\n### Manual Installation\n\nYou can install the dependencies using either npm or pnpm:\n\n```bash\n# Using npm\nnpm install\n\n# Using pnpm\npnpm install\n```\n\n## Configuration\n\n### Getting API Keys\n\nYou can get your API keys for free by registering on our website at [ChatterBox](https://chatter-box.io). After registration, you'll receive your API endpoint and key.\n\n### Environment Setup\n\nCreate a `.env` file in the root directory with the following variables:\n\n```env\nCHATTERBOX_API_ENDPOINT=https://api.chatter-box.io\nCHATTERBOX_API_KEY=your_api_key_here\n```\n\n## Usage\n\n### Starting the Server\n\n```bash\npnpm start\n```\n\n### Available Tools\n\n#### joinMeeting\n\nJoin a Zoom or Google Meet meeting and capture transcript and audio recording.\n\n**Parameters:**\n\n- `platform` (string): The online conference platform (\"zoom\", \"googlemeet\", or \"teams\")\n- `meetingId` (string): The ID of the meeting\n- `meetingPassword` (string, optional): The password or the passcode for the meeting\n- `botName` (string): The name of the bot\n- `webhookUrl` (string, optional): URL to receive webhook events for meeting status\n\n#### getMeetingInfo\n\nGet information about a meeting, including transcript and recording.\n\n**Parameters:**\n\n- `sessionId` (string): The session ID to get information for\n\n#### summarizeMeeting\n\nGenerate a concise summary of a meeting's contents from its transcript.\n\n**Parameters:**\n\n- `transcript` (string): The meeting transcript to summarize\n\n## Development\n\n### Prerequisites\n\n- Node.js 16+\n- npm or yarn\n\n### Building\n\n```bash\npnpm run build\n```\n\n### Debugging\n\nTo debug the MCP server using the MCP Inspector:\n\n```bash\nnpx @modelcontextprotocol/inspector\n```\n\n## License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n\n## Support\n\nFor support, please visit [ChatterBox Documentation](https://chatter-box.io/documentation) or contact support@chatter-box.io.\n\n## Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "chatterboxio",
        "meetings",
        "meeting",
        "collaboration chatterboxio",
        "chatterboxio mcp",
        "realtime collaboration"
      ],
      "category": "realtime-collaboration"
    },
    "DynamicEndpoints--Autogen_MCP": {
      "owner": "DynamicEndpoints",
      "name": "Autogen_MCP",
      "url": "https://github.com/DynamicEndpoints/Autogen_MCP",
      "imageUrl": "/freedevtools/mcp/pfp/DynamicEndpoints.webp",
      "description": "Facilitates the creation and management of AI agents that engage in natural language interactions and collaborate to solve problems. Supports orchestration of both individual and group conversations with customizable configurations and built-in error handling.",
      "stars": 15,
      "forks": 3,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-17T01:15:10Z",
      "readme_content": "# Enhanced AutoGen MCP Server\n\n[![smithery badge](https://smithery.ai/badge/@DynamicEndpoints/autogen_mcp)](https://smithery.ai/server/@DynamicEndpoints/autogen_mcp)\n\nA comprehensive MCP server that provides deep integration with Microsoft's AutoGen framework v0.9+, featuring the latest capabilities including prompts, resources, advanced workflows, and enhanced agent types. This server enables sophisticated multi-agent conversations through a standardized Model Context Protocol interface.\n\n## 🚀 Latest Features (v0.2.0)\n\n### ✨ **Enhanced MCP Support**\n- **Prompts**: Pre-built templates for common workflows (code review, research, creative writing)\n- **Resources**: Real-time access to agent status, chat history, and configurations\n- **Dynamic Content**: Template-based prompts with arguments and embedded resources\n- **Latest MCP SDK**: Version 1.12.3 with full feature support\n\n### 🤖 **Advanced Agent Types**\n- **Assistant Agents**: Enhanced with latest LLM capabilities\n- **Conversable Agents**: Flexible conversation patterns\n- **Teachable Agents**: Learning and memory persistence\n- **Retrievable Agents**: Knowledge base integration\n- **Multimodal Agents**: Image and document processing (when available)\n\n### 🔄 **Sophisticated Workflows**\n- **Code Generation**: Architect → Developer → Reviewer → Executor pipeline\n- **Research Analysis**: Researcher → Analyst → Critic → Synthesizer workflow\n- **Creative Writing**: Multi-stage creative collaboration\n- **Problem Solving**: Structured approach to complex problems\n- **Code Review**: Security → Performance → Style review teams\n- **Custom Workflows**: Build your own agent collaboration patterns\n\n### 🎯 **Enhanced Chat Capabilities**\n- **Smart Speaker Selection**: Auto, manual, random, round-robin modes\n- **Nested Conversations**: Hierarchical agent interactions\n- **Swarm Intelligence**: Coordinated multi-agent problem solving\n- **Memory Management**: Persistent agent knowledge and preferences\n- **Quality Checks**: Built-in validation and improvement loops\n\n## 🛠️ Available Tools\n\n### Core Agent Management\n- `create_agent` - Create agents with advanced configurations\n- `create_workflow` - Build complete multi-agent workflows\n- `get_agent_status` - Detailed agent metrics and health monitoring\n\n### Conversation Execution\n- `execute_chat` - Enhanced two-agent conversations\n- `execute_group_chat` - Multi-agent group discussions\n- `execute_nested_chat` - Hierarchical conversation structures\n- `execute_swarm` - Swarm-based collaborative problem solving\n\n### Workflow Orchestration\n- `execute_workflow` - Run predefined workflow templates\n- `manage_agent_memory` - Handle agent learning and persistence\n- `configure_teachability` - Enable/configure agent learning capabilities\n\n## 📝 Available Prompts\n\n### `autogen-workflow`\nCreate sophisticated multi-agent workflows with customizable parameters:\n- **Arguments**: `task_description`, `agent_count`, `workflow_type`\n- **Use case**: Rapid workflow prototyping and deployment\n\n### `code-review`\nSet up collaborative code review with specialized agents:\n- **Arguments**: `code`, `language`, `focus_areas`\n- **Use case**: Comprehensive code quality assessment\n\n### `research-analysis`\nDeploy research teams for in-depth topic analysis:\n- **Arguments**: `topic`, `depth`\n- **Use case**: Academic research, market analysis, technical investigation\n\n## 📊 Available Resources\n\n### `autogen://agents/list`\nLive list of active agents with status and capabilities\n\n### `autogen://workflows/templates`\nAvailable workflow templates and configurations\n\n### `autogen://chat/history`\nRecent conversation history and interaction logs\n\n### `autogen://config/current`\nCurrent server configuration and settings\n\n## Installation\n\n### Installing via Smithery\n\nTo install AutoGen Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@DynamicEndpoints/autogen_mcp):\n\n```bash\nnpx -y @smithery/cli install @DynamicEndpoints/autogen_mcp --client claude\n```\n\n### Manual Installation\n\n1. **Clone the repository:**\n```bash\ngit clone https://github.com/yourusername/autogen-mcp.git\ncd autogen-mcp\n```\n\n2. **Install Node.js dependencies:**\n```bash\nnpm install\n```\n\n3. **Install Python dependencies:**\n```bash\npip install -r requirements.txt --user\n```\n\n4. **Build the TypeScript project:**\n```bash\nnpm run build\n```\n\n5. **Set up configuration:**\n```bash\ncp .env.example .env\ncp config.json.example config.json\n# Edit .env and config.json with your settings\n```\n\n## Configuration\n\n### Environment Variables\n\nCreate a `.env` file from the template:\n\n```bash\n# Required\nOPENAI_API_KEY=your-openai-api-key-here\n\n# Optional - Path to configuration file\nAUTOGEN_MCP_CONFIG=config.json\n\n# Enhanced Features\nENABLE_PROMPTS=true\nENABLE_RESOURCES=true\nENABLE_WORKFLOWS=true\nENABLE_TEACHABILITY=true\n\n# Performance Settings\nMAX_CHAT_TURNS=10\nDEFAULT_OUTPUT_FORMAT=json\n```\n\n### Configuration File\n\nUpdate `config.json` with your preferences:\n\n```json\n{\n  \"llm_config\": {\n    \"config_list\": [\n      {\n        \"model\": \"gpt-4o\",\n        \"api_key\": \"your-openai-api-key\"\n      }\n    ],\n    \"temperature\": 0.7\n  },\n  \"enhanced_features\": {\n    \"prompts\": { \"enabled\": true },\n    \"resources\": { \"enabled\": true },\n    \"workflows\": { \"enabled\": true }\n  }\n}\n```\n\n## Usage Examples\n\n### Using with Claude Desktop\n\nAdd to your `claude_desktop_config.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"autogen\": {\n      \"command\": \"node\",\n      \"args\": [\"path/to/autogen-mcp/build/index.js\"],\n      \"env\": {\n        \"OPENAI_API_KEY\": \"your-key-here\"\n      }\n    }\n  }\n}\n```\n\n### Command Line Testing\n\nTest the server functionality:\n\n```bash\n# Run comprehensive tests\npython test_server.py\n\n# Test CLI interface\npython cli_example.py create_agent \"researcher\" \"assistant\" \"You are a research specialist\"\npython cli_example.py execute_workflow \"code_generation\" '{\"task\":\"Hello world\",\"language\":\"python\"}'\n```\n\n### Using Prompts\n\nThe server provides several built-in prompts:\n\n1. **autogen-workflow** - Create multi-agent workflows\n2. **code-review** - Set up collaborative code review\n3. **research-analysis** - Deploy research teams\n\n### Accessing Resources\n\nAvailable resources provide real-time data:\n\n- `autogen://agents/list` - Current active agents\n- `autogen://workflows/templates` - Available workflow templates  \n- `autogen://chat/history` - Recent conversation history\n- `autogen://config/current` - Server configuration\n\n## Workflow Examples\n\n### Code Generation Workflow\n\n```json\n{\n  \"workflow_name\": \"code_generation\",\n  \"input_data\": {\n    \"task\": \"Create a REST API endpoint\",\n    \"language\": \"python\",\n    \"requirements\": [\"FastAPI\", \"Pydantic\", \"Error handling\"]\n  },\n  \"quality_checks\": true\n}\n```\n\n### Research Workflow\n\n```json\n{\n  \"workflow_name\": \"research\", \n  \"input_data\": {\n    \"topic\": \"AI Ethics in 2025\",\n    \"depth\": \"comprehensive\"\n  },\n  \"output_format\": \"markdown\"\n}\n```\n\n## Advanced Features\n\n### Agent Types\n\n- **Assistant Agents**: LLM-powered conversational agents\n- **User Proxy Agents**: Code execution and human interaction\n- **Conversable Agents**: Flexible conversation patterns\n- **Teachable Agents**: Learning and memory persistence (when available)\n- **Retrievable Agents**: Knowledge base integration (when available)\n\n### Chat Modes\n\n- **Two-Agent Chat**: Direct conversation between agents\n- **Group Chat**: Multi-agent discussions with smart speaker selection\n- **Nested Chat**: Hierarchical conversation structures  \n- **Swarm Intelligence**: Coordinated problem solving (experimental)\n\n### Memory Management\n\n- Persistent agent memory across sessions\n- Conversation history tracking\n- Learning from interactions (teachable agents)\n- Memory cleanup and optimization\n\n## Troubleshooting\n\n### Common Issues\n\n1. **API Key Errors**: Ensure your OpenAI API key is valid and has sufficient credits\n2. **Import Errors**: Install all dependencies with `pip install -r requirements.txt --user`\n3. **Build Failures**: Check Node.js version (>= 18) and run `npm install`\n4. **Chat Failures**: Verify agent creation succeeded before attempting conversations\n\n### Debug Mode\n\nEnable detailed logging:\n\n```bash\nexport LOG_LEVEL=DEBUG\npython test_server.py\n```\n\n### Performance Tips\n\n- Use `gpt-4o-mini` for faster, cost-effective operations\n- Enable caching for repeated operations\n- Set appropriate timeout values for long-running workflows\n- Use quality checks only when needed (increases execution time)\n\n## Development\n\n### Running Tests\n\n```bash\n# Full test suite\npython test_server.py\n\n# Individual workflow tests  \npython -c \"\nimport asyncio\nfrom src.autogen_mcp.workflows import WorkflowManager\nwm = WorkflowManager()\nprint(asyncio.run(wm.execute_workflow('code_generation', {'task': 'test'})))\n\"\n```\n\n### Building\n\n```bash\nnpm run build\nnpm run lint\n```\n\n### Contributing\n\n1. Fork the repository\n2. Create a feature branch\n3. Make your changes\n4. Add tests for new functionality\n5. Submit a pull request\n\n## Version History\n\n### v0.2.0 (Latest)\n- ✨ Enhanced MCP support with prompts and resources\n- 🤖 Advanced agent types (teachable, retrievable)\n- 🔄 Sophisticated workflows with quality checks\n- 🎯 Smart speaker selection and nested conversations\n- 📊 Real-time resource monitoring\n- 🧠 Memory management and persistence\n\n### v0.1.0\n- Basic AutoGen integration\n- Simple agent creation and chat execution\n- MCP tool interface\n\n## Support\n\nFor issues and questions:\n- Check the troubleshooting section above\n- Review the test examples in `test_server.py`\n- Open an issue on GitHub with detailed reproduction steps\n\n## License\n\nMIT License - see LICENSE file for details.\n\n# OpenAI API Key (optional, can also be set in config.json)\nOPENAI_API_KEY=your-openai-api-key\n```\n\n### Server Configuration\n\n1. Copy `config.json.example` to `config.json`:\n```bash\ncp config.json.example config.json\n```\n\n2. Configure the server settings:\n```json\n{\n  \"llm_config\": {\n    \"config_list\": [\n      {\n        \"model\": \"gpt-4\",\n        \"api_key\": \"your-openai-api-key\"\n      }\n    ],\n    \"temperature\": 0\n  },\n  \"code_execution_config\": {\n    \"work_dir\": \"workspace\",\n    \"use_docker\": false\n  }\n}\n```\n\n## Available Operations\n\nThe server supports three main operations:\n\n### 1. Creating Agents\n\n```json\n{\n  \"name\": \"create_agent\",\n  \"arguments\": {\n    \"name\": \"tech_lead\",\n    \"type\": \"assistant\",\n    \"system_message\": \"You are a technical lead with expertise in software architecture and design patterns.\"\n  }\n}\n```\n\n### 2. One-on-One Chat\n\n```json\n{\n  \"name\": \"execute_chat\",\n  \"arguments\": {\n    \"initiator\": \"agent1\",\n    \"responder\": \"agent2\",\n    \"message\": \"Let's discuss the system architecture.\"\n  }\n}\n```\n\n### 3. Group Chat\n\n```json\n{\n  \"name\": \"execute_group_chat\",\n  \"arguments\": {\n    \"agents\": [\"agent1\", \"agent2\", \"agent3\"],\n    \"message\": \"Let's review the proposed solution.\"\n  }\n}\n```\n\n## Error Handling\n\nCommon error scenarios include:\n\n1. Agent Creation Errors\n```json\n{\n  \"error\": \"Agent already exists\"\n}\n```\n\n2. Execution Errors\n```json\n{\n  \"error\": \"Agent not found\"\n}\n```\n\n3. Configuration Errors\n```json\n{\n  \"error\": \"AUTOGEN_MCP_CONFIG environment variable not set\"\n}\n```\n\n## Architecture\n\nThe server follows a modular architecture:\n\n```\nsrc/\n├── autogen_mcp/\n│   ├── __init__.py\n│   ├── agents.py      # Agent management and configuration\n│   ├── config.py      # Configuration handling and validation\n│   ├── server.py      # MCP server implementation\n│   └── workflows.py   # Conversation workflow management\n```\n\n## License\n\nMIT License - See LICENSE file for details\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "autogen_mcp",
        "ai",
        "collaboration",
        "realtime collaboration",
        "collaboration dynamicendpoints",
        "dynamicendpoints autogen_mcp"
      ],
      "category": "realtime-collaboration"
    },
    "DynamicEndpoints--flexable-agents": {
      "owner": "DynamicEndpoints",
      "name": "flexable-agents",
      "url": "https://github.com/DynamicEndpoints/flexable-agents",
      "imageUrl": "/freedevtools/mcp/pfp/DynamicEndpoints.webp",
      "description": "Provides intelligent customer service and content creation solutions through AI-powered agents, supporting multilingual inquiries and automated workflows. Features include sentiment analysis, knowledge base integration, and creative content generation across various formats.",
      "stars": 0,
      "forks": 2,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-06-15T18:41:46Z",
      "readme_content": "\\\n<!-- filepath: c:\\\\Users\\\\Owner\\\\flexable-agents\\\\README.md -->\n# Flexible Agents MCP Server\n\nA robust, AI-powered Model Context Protocol (MCP) server built with Python, FastAPI, and Pydantic. This system provides a standardized interface for exposing various tools (formerly agents) for AI models, including capabilities for Microsoft 365 integration, content generation, data processing, and more.\n\n## 🌟 Features\n\n*   **MCP Compliance**: Fully compliant with the Model Context Protocol for standardized AI tool interaction.\n*   **Extensible Toolset**: Easily add new tools and capabilities.\n    *   **Microsoft 365 Core Tools**: Interact with M365 services (Outlook, SharePoint, Teams, etc.).\n    *   **Specialized Tools**: Azure management, document processing (PDF, DOCX, OCR), data analysis.\n    *   **Workflow Tools**: Orchestrate complex multi-step operations.\n*   **Structured Logging**: Comprehensive logging with Rich console output and JSON/detailed file logs.\n*   **Performance Monitoring**: Real-time monitoring of server health, performance, and error metrics.\n*   **Configuration Management**: Flexible configuration via `config.json` and environment variables.\n*   **Command Line Interface (CLI)**: Utilities for server management, configuration validation, tool listing, and testing.\n*   **Async Architecture**: Built with FastAPI and `asyncio` for high performance.\n*   **Microsoft Graph API Integration**: Seamlessly connect and interact with Microsoft Graph.\n*   **Anthropic Claude Integration**: Leverages Claude models for advanced AI tasks (configurable).\n\n## 🚀 Getting Started\n\n### Prerequisites\n\n*   Python 3.9+\n*   Pip (Python package installer)\n*   Access to necessary cloud services if using corresponding tools (e.g., Azure subscription, M365 tenant).\n\n### Environment Setup\n\n1.  **Clone the Repository**:\n    ```bash\n    git clone <repository-url>\n    cd flexible-agents-mcp\n    ```\n\n2.  **Create a Virtual Environment** (recommended):\n    ```bash\n    python -m venv .venv\n    ```\n    Activate the virtual environment:\n    *   Windows (pwsh): `.\\\\.venv\\\\Scripts\\\\Activate.ps1`\n    *   Windows (cmd): `.\\\\.venv\\\\Scripts\\\\activate.bat`\n    *   Linux/macOS: `source ./.venv/bin/activate`\n\n3.  **Install Dependencies**:\n    The project uses `pyproject.toml` for managing dependencies.\n    ```bash\n    pip install . \n    ```\n    For development, including tools for testing and linting:\n    ```bash\n    pip install -r requirements-dev.txt\n    ```\n\n4.  **Configuration Files**:\n    *   **Main Configuration (`config.json`)**:\n        *   Copy `config.json.example` to `config.json` in the root directory.\n            ```powershell\n            Copy-Item config.json.example config.json\n            ```\n        *   Edit `config.json` to provide your specific settings:\n            *   `m365`: `tenant_id`, `client_id`, `client_secret` for Microsoft Graph API access.\n            *   `anthropic`: `api_key` for Anthropic Claude models.\n            *   `azure`: `subscription_id`, `resource_group` if using Azure tools.\n            *   `server`: Adjust server settings like `log_level`, `debug` mode if needed.\n    *   **Environment Variables (`.env`)**:\n        *   Sensitive information like API keys or secrets can also be placed in a `.env` file in the root directory. Copy `.env.example` to `.env` and fill in the values.\n            ```powershell\n            Copy-Item .env.example .env\n            ```\n        *   Values in `.env` will override those in `config.json` if both are present for the same setting (e.g., `ANTHROPIC_API_KEY`).\n            Example `.env` content:\n            ```env\n            AZURE_TENANT_ID=\"your-tenant-id\"\n            AZURE_CLIENT_ID=\"your-client-id\"\n            AZURE_CLIENT_SECRET=\"your-client-secret\"\n            ANTHROPIC_API_KEY=\"your-anthropic-api-key\"\n            AZURE_SUBSCRIPTION_ID=\"your-azure-subscription-id\"\n            ```\n\n### Running the MCP Server\n\nYou can run the server in several ways:\n\n1.  **Directly with `server.py`**:\n    ```bash\n    python server.py --config config.json --port 8000\n    ```\n    Use `--debug` for more verbose output during development.\n\n2.  **Using the CLI**:\n    ```bash\n    python -m src.cli server --config config.json --port 8000\n    ```\n\n3.  **Using Provided Scripts**:\n    *   Windows PowerShell: `.\\\\run-server.ps1`\n    *   Windows Batch: `.\\\\run-server.bat`\n    These scripts typically run the server with default settings. You might need to edit them if your `config.json` is not in the default location or if you want to specify a different port.\n\nOnce the server is running, it will typically be accessible at `http://localhost:8000` (or the port you specified). The MCP manifest will be available at `http://localhost:8000/mcp.json`.\n\n## ⚙️ MCP Server Usage\n\nThe Flexible Agents MCP Server exposes its tools according to the Model Context Protocol. AI models or other MCP clients can interact with the server by making HTTP requests to its defined endpoints.\n\n*   **Manifest**: The server's capabilities are described in the manifest file, typically available at the `/mcp.json` endpoint (e.g., `http://localhost:8000/mcp.json`).\n*   **Tool Calls**: To execute a tool, an MCP client sends a `POST` request to the `/tool/{tool_name}` endpoint.\n\n**Example MCP Request (conceptual):**\n\nA client wanting to use the `M365_Calendar_Create_Event` tool would send a POST request to `http://localhost:8000/tool/M365_Calendar_Create_Event` with a JSON body like:\n\n```json\n{\n  \"contextId\": \"unique-context-id\",\n  \"invocationId\": \"unique-invocation-id\",\n  \"toolName\": \"M365_Calendar_Create_Event\",\n  \"arguments\": {\n    \"subject\": \"Team Meeting\",\n    \"start_time\": \"2025-06-16T10:00:00\",\n    \"end_time\": \"2025-06-16T11:00:00\",\n    \"attendees\": [\"user1@example.com\", \"user2@example.com\"],\n    \"body\": \"Discuss project updates.\"\n  }\n}\n```\n\n**Example MCP Response (conceptual):**\n\nThe server would respond with:\n\n```json\n{\n  \"contextId\": \"unique-context-id\",\n  \"invocationId\": \"unique-invocation-id\",\n  \"toolName\": \"M365_Calendar_Create_Event\",\n  \"isError\": false,\n  \"content\": [\n    {\n      \"type\": \"application/json\",\n      \"content\": {\n        \"event_id\": \"AAMkAGYz...=\",\n        \"subject\": \"Team Meeting\",\n        \"start\": \"2025-06-16T10:00:00Z\",\n        \"end\": \"2025-06-16T11:00:00Z\",\n        \"attendees\": [\"user1@example.com\", \"user2@example.com\"],\n        \"message\": \"Event created successfully\"\n      }\n    }\n  ]\n}\n```\nIf an error occurs, `isError` would be `true`, and the `content` would typically contain error details.\n\n## 🛠️ Available Tools\n\nThe server comes with a variety of pre-built tools. You can list all available tools using the CLI: `python -m src.cli list-tools`.\n\nHere are some of the categories and example tools:\n\n*   **Microsoft 365 Tools (`src/tools/m365_tools.py`)**:\n    *   `M365_Calendar_Create_Event`: Creates a new calendar event.\n    *   `M365_Calendar_List_Events`: Lists calendar events.\n    *   `M365_Email_Send`: Sends an email.\n    *   `M365_Email_List_Messages`: Lists emails from the inbox.\n    *   `M365_SharePoint_List_Sites`: Lists SharePoint sites.\n    *   `M365_SharePoint_Search_Files`: Searches for files in SharePoint.\n    *   `M365_Teams_Send_Message`: Sends a message to a Teams channel.\n    *   *(Many more...)*\n*   **Azure Tools (`src/tools/azure_tools.py`)**:\n    *   `Azure_VM_List`: Lists virtual machines.\n    *   `Azure_Resource_Group_List`: Lists resource groups.\n*   **Document Tools (`src/tools/document_tools.py`)**:\n    *   `Document_Extract_Text_PDF`: Extracts text from a PDF file.\n    *   `Document_Extract_Text_DOCX`: Extracts text from a DOCX file.\n    *   `Document_OCR_Image`: Performs OCR on an image to extract text.\n*   **Data Tools (`src/tools/data_tools.py`)**:\n    *   `Data_Analyze_CSV`: Performs basic analysis on a CSV file.\n*   **Specialized Tools (`src/tools/specialized_tools.py`)**:\n    *   `Claude_Generate_Text`: Generates text using an Anthropic Claude model.\n*   **Workflow Tools (`src/tools/workflow_tools.py`)**:\n    *   Tools for orchestrating sequences of other tool calls.\n\nEach tool has defined input parameters and output formats, as specified in their implementation and discoverable via the MCP manifest or by inspecting the tool registration in the respective Python modules.\n\n## ⌨️ Command Line Interface (CLI)\n\nThe CLI (`src/cli.py`) provides several utilities for managing and interacting with the MCP server. Access it using `python -m src.cli`.\n\n**Common Commands:**\n\n*   `python -m src.cli --help`: Shows all available commands.\n*   `python -m src.cli server [OPTIONS]`: Starts the MCP server.\n    *   `--config TEXT`: Path to the configuration file (default: `config.json`).\n    *   `--host TEXT`: Host to bind the server to (default: `0.0.0.0`).\n    *   `--port INTEGER`: Port to run the server on (default: `8000`).\n    *   `--debug / --no-debug`: Enable or disable debug mode.\n    *   `--health-check`: Performs a server health check and exits.\n*   `python -m src.cli list-tools [OPTIONS]`: Lists all registered tools.\n    *   `--config TEXT`: Path to the configuration file.\n*   `python -m src.cli test-tool <TOOL_NAME> [PARAMETERS] [OPTIONS]`: Tests a specific tool.\n    *   `<TOOL_NAME>`: The name of the tool to test (e.g., `M365_Email_List_Messages`).\n    *   `[PARAMETERS]`: Optional JSON string of parameters for the tool (e.g., `'{\"folder_name\": \"Inbox\", \"count\": 5}'`).\n    *   `--config TEXT`: Path to the configuration file.\n*   `python -m src.cli validate-config [OPTIONS]`: Validates the configuration file.\n    *   `--config TEXT`: Path to the configuration file.\n*   `python -m src.cli create-config`: Creates sample configuration files (`config.json.example`, `.env.example`).\n*   `python -m src.cli show-config [OPTIONS]`: Shows the current configuration (masks sensitive values).\n    *   `--config TEXT`: Path to the configuration file.\n\n**Example CLI Usage:**\n\n```bash\n# Start the server with a specific config and port\npython -m src.cli server --config my_config.json --port 8080\n\n# List all available tools\npython -m src.cli list-tools\n\n# Test the M365_Email_List_Messages tool\npython -m src.cli test-tool M365_Email_List_Messages '{\"count\": 3}'\n\n# Validate your config.json\npython -m src.cli validate-config --config config.json \n```\n\n## ⚙️ Configuration\n\nThe server's behavior is primarily controlled by `config.json`.\n\n*   **`server`**: General server settings (name, version, debug mode, log level, timeout).\n*   **`m365`**: Microsoft 365 connection details (tenant ID, client ID, client secret, scopes).\n*   **`anthropic`**: Anthropic API settings (API key, model, max tokens, temperature).\n*   **`azure`**: Azure connection details (subscription ID, resource group, default location).\n\nRefer to `config.json.example` for the structure and available options.\n\n## 🧑‍💻 Development\n\n### Project Structure\n\n*   `server.py`: Main entry point for the FastAPI MCP server.\n*   `src/cli.py`: Command Line Interface.\n*   `src/mcp/`: Core MCP handling logic (server, handlers, registry, types, logging).\n*   `src/tools/`: Directory containing all MCP tool implementations (e.g., `m365_tools.py`, `azure_tools.py`).\n*   `src/core/`: Base classes and utilities.\n*   `config.json`: Server configuration file.\n*   `mcp.json`: MCP manifest file (generated based on registered tools).\n\n### Adding a New Tool\n\n1.  **Create or Choose a Module**: Place your tool logic in an appropriate file within `src/tools/` (e.g., `my_new_tools.py`).\n2.  **Implement the Tool Function**:\n    *   The function should be `async def`.\n    *   Use type hints for parameters.\n    *   The function should return a dictionary or a Pydantic model that can be serialized to JSON. This will form the `content` part of the MCP response.\n    *   Incorporate error handling. The `with_error_handling` decorator from `src/mcp/handlers.py` can be used for standardized error responses.\n3.  **Register the Tool**:\n    *   In your tool module, use the `@tool` decorator from `src/mcp/registry.py` or manually register tools with an instance of `ToolRegistry`.\n    *   Ensure your tool module is imported and its tools are registered in `src/tools/__init__.py` within the `register_all_tools` function.\n    *   Provide a clear `name`, `description`, and define `parameters` (if not auto-detected from type hints) and `returns` for the tool metadata.\n\n**Example Tool Snippet (in `src/tools/my_new_tools.py`):**\n```python\nfrom src.mcp.registry import tool\nfrom src.mcp.handlers import with_error_handling, log_request_metrics\nimport time\n\n@tool(\n    name=\"MyTool_Echo\",\n    description=\"A simple tool that echoes back the input message.\",\n    parameters=[{\"name\": \"message\", \"type\": \"string\", \"description\": \"The message to echo.\", \"required\": True}],\n    returns=\"A JSON object containing the echoed message.\"\n)\n@with_error_handling(\"MyTool_Echo\") # For standardized error handling\nasync def my_echo_tool(message: str) -> dict:\n    start_time = time.time()\n    success = True\n    error_message = None\n    try:\n        # Your tool logic here\n        if not message:\n            raise ValueError(\"Message cannot be empty.\")\n        result = {\"echo\": message, \"received_at\": time.time()}\n        return result\n    except Exception as e:\n        success = False\n        error_message = str(e)\n        # The with_error_handling decorator will catch this and format it\n        raise \n    finally:\n        duration = time.time() - start_time\n        log_request_metrics(method=\"MyTool_Echo\", duration=duration, success=success, error=error_message)\n\n```\n4.  **Update `src/tools/__init__.py`**:\n    ```python\n    # In src/tools/__init__.py\n    # ... other imports ...\n    from . import my_new_tools # Import your new module\n\n    def register_all_tools(server, config_manager):\n        # ... existing tool registrations ...\n        my_new_tools.register_tools(server.tool_registry, config_manager) # Assuming you have a register_tools func in your module\n        # Or, if using the @tool decorator and auto-registration from module:\n        # from src.mcp.registry import register_tools_from_module\n        # register_tools_from_module(server.tool_registry, my_new_tools)\n    ```\n\n### Logging and Error Handling\n\n*   The system uses the `logging_system.py` for structured logging. Use the standard `logging` module in your tools; it will be processed by `structlog`.\n*   The `log_request_metrics` function from `src.mcp.logging_system` should be called to record tool execution success/failure and duration.\n*   The `with_error_handling` decorator in `src.mcp.handlers` provides a standardized way to catch exceptions in tools and format them as MCP error responses.\n\n## 📝 License\n\nThis project is licensed under the MIT License - see the `LICENSE` file for details.\n\n## 🙏 Acknowledgments\n\n*   FastAPI, Pydantic, Structlog, Rich\n*   Anthropic for Claude models\n*   Microsoft for Graph API and Azure SDKs\n*   The open-source community\n\n## 🔮 Future Enhancements (Planned / Ideas)\n\n*   **Enhanced Workflow Orchestration**: More sophisticated built-in tools for managing complex, multi-step workflows.\n*   **Dynamic Tool Registration**: Allow tools to be registered/unregistered at runtime.\n*   **Improved Security**: OAuth2/OpenID Connect for server endpoints.\n*   **User Management/Authentication for Tools**: Per-user permissions or context for tool execution.\n*   **Web UI for Management**: A simple web interface for server status, tool management, and log viewing.\n*   **Containerization**: Dockerfile and docker-compose for easy deployment.\n*   **Comprehensive Test Suite**: Expand test coverage for all tools and core components.\n\n---\n\n*This README has been updated to reflect the transition to an MCP server architecture.*\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "dynamicendpoints",
        "agents",
        "ai",
        "collaboration dynamicendpoints",
        "flexable agents",
        "dynamicendpoints flexable"
      ],
      "category": "realtime-collaboration"
    },
    "MNylif--auto-conduwuit": {
      "owner": "MNylif",
      "name": "auto-conduwuit",
      "url": "https://github.com/MNylif/auto-conduwuit",
      "imageUrl": "/freedevtools/mcp/pfp/MNylif.webp",
      "description": "Automates the installation and secure configuration of a Conduwuit server using Docker, including SSL certificate setup and TURN server configuration for voice/video communication. Streamlines the deployment process with minimal manual intervention.",
      "stars": 0,
      "forks": 0,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-02-22T22:30:24Z",
      "readme_content": "# Auto-Conduwuit Install\n\nThis is an automated installation script for setting up a Conduwuit server using Docker. The script handles all necessary dependencies, SSL certificate generation, and server configuration.\n\n## Quick Start\n\nTo install Conduwuit, run this command:\n\n```bash\ncurl -fsSL https://raw.githubusercontent.com/MNylif/auto-conduwuit/main/install.py -o install.py && sudo python3 install.py\n```\n\n## What the Script Does\n\n1. Checks system requirements and port availability\n2. Installs system dependencies (including certbot)\n3. Installs and configures Docker\n4. Installs Docker Compose\n5. Obtains SSL certificates via Let's Encrypt\n6. Sets up Conduwuit with Docker\n7. Creates admin account\n8. Installs and configures Coturn TURN server for voice/video calls\n\n## Requirements\n\n- A VPS or server running Ubuntu/Debian\n- Python 3.6+\n- A domain name pointing to your server\n- Root/sudo access\n- Port 80 and 443 available for HTTPS\n- Ports 3478 (TURN) and 49152-49252 (RTP) available for voice/video calls\n\n## Features\n\n- Automated dependency installation\n- Secure configuration out of the box\n- Automatic SSL certificate generation\n- TURN server for voice/video calls\n- Docker-based deployment\n- Detailed progress information\n- Comprehensive error handling\n\n## Interactive Prompts\n\nThe script will ask for:\n\n1. Domain name for your Conduwuit instance\n2. Email address for SSL certificate\n3. Admin username\n4. Admin password\n\n## Post-Installation\n\nAfter installation, your Conduwuit instance will be:\n\n- Running at `https://your-domain.com`\n- Configured with SSL\n- TURN server running at `turn:your-domain.com:3478`\n- Ready to use with your admin account\n\n## Management Commands\n\n- View logs: `docker-compose logs -f`\n- Stop server: `docker-compose down`\n- Start server: `docker-compose up -d`\n- Restart server: `docker-compose restart`\n\n## Security Features\n\n- SSL certificates automatically obtained and configured\n- Admin registration disabled by default\n- Secure random signing key generated\n- TURN server configured with authentication\n- All credentials collected securely\n- Proper file permissions\n- Safe secret handling\n\n## Error Handling\n\nThe script includes robust error handling:\n- Pre-flight system checks\n- Port availability verification\n- Package installation verification\n- SSL certificate generation retries\n- Service health checks\n- Detailed error messages\n- Automatic retry mechanisms\n\n## Troubleshooting\n\nIf you encounter issues:\n\n1. Check the logs using `docker-compose logs`\n2. Ensure all required ports are available\n3. Verify your domain points to the server\n4. Make sure you have Python 3.6+ installed\n5. Run the script with sudo/root privileges\n\n## Support\n\nFor issues or questions, please visit:\n- [Conduwuit Documentation](https://conduwuit.puppyirl.gay/)\n- [GitHub Issues](https://github.com/MNylif/auto-conduwuit/issues) ",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "conduwuit",
        "mnylif",
        "automates",
        "conduwuit automates",
        "conduwuit server",
        "auto conduwuit"
      ],
      "category": "realtime-collaboration"
    },
    "Mimo-Inverse--realtime-chat-supabase-react": {
      "owner": "Mimo-Inverse",
      "name": "realtime-chat-supabase-react",
      "url": "https://github.com/Mimo-Inverse/realtime-chat-supabase-react",
      "imageUrl": "/freedevtools/mcp/pfp/Mimo-Inverse.webp",
      "description": "Facilitates real-time chat experiences using React and Supabase, allowing users to send and receive messages instantly. The application supports seamless communication through a user-friendly interface, underpinned by a PostgreSQL database.",
      "stars": 0,
      "forks": 0,
      "license": "Apache License 2.0",
      "language": "",
      "updated_at": "2025-03-20T04:19:41Z",
      "readme_content": "# Full-stack real-time chat\n\n[![Netlify Status](https://api.netlify.com/api/v1/badges/38b6f457-50d2-42ac-b9a8-9ca962febebd/deploy-status)](https://app.netlify.com/sites/random-chat/deploys)\n\n- **Data:** PostgeSQL managed by [Supabase](https://supabase.io/) [@supabase_io](https://twitter.com/supabase_io) (awsome real-time API).\n- **Front-end**: React + Vite\n- **UI library**: [chakra-ui](https://chakra-ui.com/) [@chakra_ui](https://twitter.com/chakra_ui)\n- **Hosting**: [Netlify](https://www.netlify.com/)\n- Country flags from [Flagpedia](https://flagpedia.net)\n\n## Install\n\n`npm install` to setup dependencies\n\n## Supabase variables\n\nCreate a `.env` file with `VITE_SUPABASE_URL` and `VITE_SUPABASE_KEY` (see env.example)\n\n## Setup your Supabase project\n\nThe following database table is required:\n\n| Field            | Type      |\n| ---------------- | --------- |\n| id               | BIGINT    |\n| username         | VARCHAR   |\n| text             | TEXT      |\n| country          | VARCHAR   |\n| is_authenticated | BOOLEAN   |\n| timestamp        | timestamp |\n\nSQL query if not using the Supabase interface:\n\n```sql\nCREATE TABLE messages (\n  id bigint GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,\n  username VARCHAR NOT NULL,\n  text TEXT NOT NULL,\n  country VARCHAR,\n  is_authenticated BOOLEAN DEFAULT FALSE,\n  timestamp timestamp default now() NOT NULL\n);\n```\n\nNote: If you're using Supabase interface, don't forget to tick `Enable Realtime` setting after you created the table.\n\n## Setup GitHub authentication (optional)\n\nFollow instrunction [here](https://supabase.io/docs/guides/auth/auth-github)\n\n## Dev\n\n`npm run dev` to run server on port 3000\n\n## Build\n\n`npm run build` to build the react client\n\n# Demo\n\n[https://random-chat.netlify.app](https://random-chat.netlify.app/)\n\n!['demo'](https://random-chat.netlify.app/demo.png \"demo\")\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "chat",
        "supabase",
        "realtime",
        "chat supabase",
        "realtime chat",
        "supabase react"
      ],
      "category": "realtime-collaboration"
    },
    "Monadical-SAS--zulip-mcp": {
      "owner": "Monadical-SAS",
      "name": "zulip-mcp",
      "url": "https://github.com/Monadical-SAS/zulip-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/Monadical-SAS.webp",
      "description": "Enable interaction with Zulip workspaces by managing channels, posting messages, sending direct messages, adding reactions, and retrieving conversation history for effective communication and automation.",
      "stars": 4,
      "forks": 3,
      "license": "Apache License 2.0",
      "language": "JavaScript",
      "updated_at": "2025-09-27T14:27:30Z",
      "readme_content": "# Zulip MCP Server\n\nMCP Server for the Zulip API, enabling AI assistants like Claude to interact with Zulip workspaces.\n\n## Tools\n\n1. `zulip_list_channels`\n   - List available channels (streams) in the Zulip organization\n   - Optional inputs:\n     - `include_private` (boolean, default: false): Whether to include private streams \n     - `include_web_public` (boolean, default: true): Whether to include web-public streams\n     - `include_subscribed` (boolean, default: true): Whether to include streams the bot is subscribed to\n   - Returns: List of streams with their IDs and information\n\n2. `zulip_post_message`\n   - Post a new message to a Zulip channel (stream)\n   - Required inputs:\n     - `channel_name` (string): The name of the stream to post to\n     - `topic` (string): The topic within the stream\n     - `content` (string): The message content to post\n   - Returns: Message posting confirmation and ID\n\n3. `zulip_send_direct_message`\n   - Send a direct message to one or more users\n   - Required inputs:\n     - `recipients` (string[]): Email addresses or user IDs of recipients\n     - `content` (string): The message content to send\n   - Returns: Message sending confirmation and ID\n\n4. `zulip_add_reaction`\n   - Add an emoji reaction to a message\n   - Required inputs:\n     - `message_id` (number): The ID of the message to react to\n     - `emoji_name` (string): Emoji name without colons\n   - Returns: Reaction confirmation\n\n5. `zulip_get_channel_history`\n   - Get recent messages from a channel (stream) and topic\n   - Required inputs:\n     - `channel_name` (string): The name of the stream\n     - `topic` (string): The topic name\n   - Optional inputs:\n     - `limit` (number, default: 20): Number of messages to retrieve\n     - `anchor` (string, default: \"newest\"): Message ID to start from\n   - Returns: List of messages with their content and metadata\n\n6. `zulip_get_topics`\n   - Get topics in a channel (stream)\n   - Required inputs:\n     - `channel_id` (number): The ID of the stream\n   - Returns: List of topics in the stream\n\n7. `zulip_subscribe_to_channel`\n   - Subscribe the bot to a channel (stream)\n   - Required inputs:\n     - `channel_name` (string): The name of the stream to subscribe to\n   - Returns: Subscription confirmation\n\n8. `zulip_get_users`\n   - Get list of users in the Zulip organization\n   - Returns: List of users with their basic information\n\n## Setup\n\n1. Create a Zulip Bot:\n   - Log in to your Zulip instance\n   - Navigate to Settings > Personal > Bots\n   - Click \"Add a new bot\"\n   - Select \"Generic bot\" type\n   - Fill in the required information\n   - Click \"Create bot\"\n\n2. Permissions:\n   - By default, Zulip bots have limited permissions\n   - Make sure to subscribe the bot to any streams it needs to access\n   - If you need the bot to have more permissions, consider using a full user account instead\n\n3. Get the API credentials:\n   - Bot's email address\n   - Bot's API key (displayed when you create the bot)\n   - Zulip instance URL (e.g., https://example.zulipchat.com)\n\n### Usage with Claude Desktop\n\nAdd the following to your `claude_desktop_config.json`:\n\n#### npx\n\n```json\n{\n  \"mcpServers\": {\n    \"zulip\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"@modelcontextprotocol/server-zulip\"\n      ],\n      \"env\": {\n        \"ZULIP_EMAIL\": \"your-bot@example.zulipchat.com\",\n        \"ZULIP_API_KEY\": \"your-bot-api-key\",\n        \"ZULIP_URL\": \"https://example.zulipchat.com\"\n      }\n    }\n  }\n}\n```\n\n#### docker\n\n```json\n{\n  \"mcpServers\": {\n    \"zulip\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"-i\",\n        \"--rm\",\n        \"-e\",\n        \"ZULIP_EMAIL\",\n        \"-e\",\n        \"ZULIP_API_KEY\",\n        \"-e\",\n        \"ZULIP_URL\",\n        \"mcp/zulip\"\n      ],\n      \"env\": {\n        \"ZULIP_EMAIL\": \"your-bot@example.zulipchat.com\",\n        \"ZULIP_API_KEY\": \"your-bot-api-key\",\n        \"ZULIP_URL\": \"https://example.zulipchat.com\"\n      }\n    }\n  }\n}\n```\n\n### Troubleshooting\n\nIf you encounter permission errors, verify that:\n1. The bot API key is correct\n2. The bot has been subscribed to the channels it needs to access\n3. The Zulip URL is correct and accessible\n\n## Build\n\nDocker build:\n\n```bash\ndocker build -t mcp/zulip .\n```\n\n## License\n\nThis MCP server is licensed under the MIT License.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "zulip",
        "collaboration",
        "realtime",
        "realtime collaboration",
        "interaction zulip",
        "zulip workspaces"
      ],
      "category": "realtime-collaboration"
    },
    "SaseQ--discord-mcp": {
      "owner": "SaseQ",
      "name": "discord-mcp",
      "url": "https://github.com/SaseQ/discord-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/SaseQ.webp",
      "description": "Integrate a Discord bot with MCP-compatible applications for enhanced functionalities and interactions. Leverage the Model Context Protocol to facilitate advanced communication and automation within Discord channels.",
      "stars": 83,
      "forks": 18,
      "license": "MIT License",
      "language": "Java",
      "updated_at": "2025-10-01T06:34:29Z",
      "readme_content": "<div align=\"center\">\n  \n</div>\n<hr>\n<div align=\"center\" style=\"line-height: 1;\">\n    <a href=\"https://github.com/modelcontextprotocol/servers\" target=\"_blank\" style=\"margin: 2px;\">\n        <img alt=\"MCP Server\" src=\"https://badge.mcpx.dev?type=server\" style=\"display: inline-block; vertical-align: middle;\"/>\n    </a>\n    <a href=\"https://smithery.ai/server/@SaseQ/discord-mcp\" target=\"_blank\" style=\"margin: 2px;\">\n        <img alt=\"Smithery Badge\" src=\"https://camo.githubusercontent.com/ee5c6c6dc502821f4d57313b2885f7878af52be14142dd98526ea12aedf9b260/68747470733a2f2f736d6974686572792e61692f62616467652f40646d6f6e74676f6d65727934302f646565707365656b2d6d63702d736572766572\" data-canonical-src=\"https://smithery.ai/server/@SaseQ/discord-mcp\" style=\"display: inline-block; vertical-align: middle;\"/>\n    </a>\n    <a href=\"https://discord.gg/5Uvxe5jteM\" target=\"_blank\" style=\"margin: 2px;\">\n        <img alt=\"Discord\" src=\"https://img.shields.io/discord/936242526120194108?color=7389D8&label&logo=discord&logoColor=ffffff\" style=\"display: inline-block; vertical-align: middle;\"/>\n    </a>\n    <a href=\"https://github.com/SaseQ/discord-mcp/blob/main/LICENSE\" target=\"_blank\" style=\"margin: 2px;\">\n        <img alt=\"MIT License\" src=\"https://img.shields.io/github/license/SaseQ/discord-mcp\" style=\"display: inline-block; vertical-align: middle;\"/>\n    </a>\n</div>\n\n\n## 📖 Description\n\nA [Model Context Protocol (MCP)](https://modelcontextprotocol.io/introduction) server for the Discord API [(JDA)](https://jda.wiki/), \nallowing seamless integration of Discord Bot with MCP-compatible applications like Claude Desktop.\n\nEnable your AI assistants to seamlessly interact with Discord. Manage channels, send messages, and retrieve server information effortlessly. Enhance your Discord experience with powerful automation capabilities.\n\n\n## 🔬 Installation\n\n### ► 🐳 Docker Installation (Recommended)\n> NOTE: Docker installation is required. Full instructions can be found on [docker.com](https://www.docker.com/products/docker-desktop/).\n```json\n{\n  \"mcpServers\": {\n    \"mcp-server\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\", \"--rm\", \"-i\",\n        \"-e\", \"DISCORD_TOKEN=<YOUR_DISCORD_BOT_TOKEN>\",\n        \"-e\", \"DISCORD_GUILD_ID=<OPTIONAL_DEFAULT_SERVER_ID>\",\n        \"saseq/discord-mcp:latest\"\n      ]\n    }\n  }\n}\n```\n\n<details>\n    <summary style=\"font-size: 1.35em; font-weight: bold;\">\n        🔧 Manual Installation\n    </summary>\n\n#### Clone the repository\n```bash\ngit clone https://github.com/SaseQ/discord-mcp\n```\n\n#### Build the project\n> NOTE: Maven installation is required to use the mvn command. Full instructions can be found [here](https://www.baeldung.com/install-maven-on-windows-linux-mac).\n```bash\ncd discord-mcp\nmvn clean package # The jar file will be available in the /target directory\n```\n\n#### Configure AI client\nMany code editors and other AI clients use a configuration file to manage MCP servers.\n\nThe Discord MPC server can be configured by adding the following to your configuration file.\n\n> NOTE: You will need to create a Discord Bot token to use this server. Instructions on how to create a Discord Bot token can be found [here](https://discordjs.guide/preparations/setting-up-a-bot-application.html#creating-your-bot).\n```json\n{\n  \"mcpServers\": {\n    \"discord-mcp\": {\n      \"command\": \"java\",\n      \"args\": [\n        \"-jar\",\n        \"/absolute/path/to/discord-mcp-0.0.1-SNAPSHOT.jar\"\n      ],\n      \"env\": {\n        \"DISCORD_TOKEN\": \"YOUR_DISCORD_BOT_TOKEN\",\n        \"DISCORD_GUILD_ID\": \"OPTIONAL_DEFAULT_SERVER_ID\"\n      }\n    }\n  }\n}\n```\nThe `DISCORD_GUILD_ID` environment variable is optional. When provided, it sets a default Discord server ID so any tool that accepts a `guildId` parameter can omit it.\n\n</details>\n\n<details>\n    <summary style=\"font-size: 1.35em; font-weight: bold;\">\n        ⚓ Smithery Installation\n    </summary>\n\nInstall Discord MCP Server automatically via [Smithery](https://smithery.ai/):\n```bash\nnpx -y @smithery/cli@latest install @SaseQ/discord-mcp --client <CLIENT_NAME> --key <YOUR_SMITHERY_KEY>\n```\n\n</details>\n\n<details>\n    <summary style=\"font-size: 1.35em; font-weight: bold;\">\n        🖲 Cursor Installation\n    </summary>\n\nGo to: `Settings` -> `Cursor Settings` -> `MCP` -> `Add new global MCP server`\n\nPasting the following configuration into your Cursor `~/.cursor/mcp.json` file is the recommended approach. You may also install in a specific project by creating `.cursor/mcp.json` in your project folder. See [Cursor MCP docs](https://docs.cursor.com/context/model-context-protocol) for more info.\n```json\n{\n  \"mcpServers\": {\n    \"mcp-server\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\", \"--rm\", \"-i\",\n        \"-e\", \"DISCORD_TOKEN=<YOUR_DISCORD_BOT_TOKEN>\",\n        \"-e\", \"DISCORD_GUILD_ID=<OPTIONAL_DEFAULT_SERVER_ID>\",\n        \"saseq/discord-mcp:latest\"\n      ]\n    }\n  }\n}\n```\n\n</details>\n\n<details>\n    <summary style=\"font-size: 1.35em; font-weight: bold;\">\n        ⌨️ Claude Code Installation\n    </summary>\n\nRun this command. See [Claude Code MCP docs](https://docs.anthropic.com/en/docs/agents-and-tools/claude-code/tutorials#set-up-model-context-protocol-mcp) for more info.\n```bash\nclaude mcp add mcp-server -- docker run --rm -i -e DISCORD_TOKEN=<YOUR_DISCORD_BOT_TOKEN> -e DISCORD_GUILD_ID=<OPTIONAL_DEFAULT_SERVER_ID> saseq/discord-mcp:latest\n```\n\n</details>\n\n## 🛠️ Available Tools\n\n#### Server Information\n - [`get_server_info`](): Get detailed discord server information\n\n#### User Management\n- [`get_user_id_by_name`](): Get a Discord user's ID by username in a guild for ping usage `<@id>`\n- [`send_private_message`](): Send a private message to a specific user\n- [`edit_private_message`](): Edit a private message from a specific user\n- [`delete_private_message`](): Delete a private message from a specific user\n- [`read_private_messages`](): Read recent message history from a specific user\n\n#### Message Management\n - [`send_message`](): Send a message to a specific channel\n - [`edit_message`](): Edit a message from a specific channel\n - [`delete_message`](): Delete a message from a specific channel\n - [`read_messages`](): Read recent message history from a specific channel\n - [`add_reaction`](): Add a reaction (emoji) to a specific message\n - [`remove_reaction`](): Remove a specified reaction (emoji) from a message\n\n#### Channel Management\n - [`create_text_channel`](): Create text a channel\n - [`delete_channel`](): Delete a channel\n - [`find_channel`](): Find a channel type and ID using name and server ID\n - [`list_channels`](): List of all channels\n\n#### Category Management\n - [`create_category`](): Create a new category for channels\n - [`delete_category`](): Delete a category\n - [`find_category`](): Find a category ID using name and server ID\n - [`list_channels_in_category`](): List of channels in a specific category\n\n#### Webhook Management\n - [`create_webhook`](): Create a new webhook on a specific channel\n - [`delete_webhook`](): Delete a webhook\n - [`list_webhooks`](): List of webhooks on a specific channel\n - [`send_webhook_message`](): Send a message via webhook\n\n>If `DISCORD_GUILD_ID` is set, the `guildId` parameter becomes optional for all tools above.\n\n<hr>\n\nA more detailed examples can be found in the [Wiki](https://github.com/SaseQ/discord-mcp/wiki).",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "discord",
        "automation",
        "mcp",
        "automation discord",
        "discord mcp",
        "discord bot"
      ],
      "category": "realtime-collaboration"
    },
    "Xiaoxinkeji--XBotV2": {
      "owner": "Xiaoxinkeji",
      "name": "XBotV2",
      "url": "https://github.com/Xiaoxinkeji/XBotV2",
      "imageUrl": "/freedevtools/mcp/pfp/Xiaoxinkeji.webp",
      "description": "A comprehensive WeChat bot framework that supports various interactive features and game functionalities, equipped with a plugin system for customization and a web management interface for user oversight. It facilitates efficient message forwarding and processing with easy deployment using Docker.",
      "stars": 1,
      "forks": 0,
      "license": "GNU General Public License v3.0",
      "language": "Python",
      "updated_at": "2025-03-30T04:55:02Z",
      "readme_content": "# 🤖 XYBot V2\n\nXYBot V2 是一个功能丰富的微信机器人框架,支持多种互动功能和游戏玩法。\n\n# 免责声明\n\n- 这个项目免费开源，不存在收费。\n- 本工具仅供学习和技术研究使用，不得用于任何商业或非法行为。\n- 本工具的作者不对本工具的安全性、完整性、可靠性、有效性、正确性或适用性做任何明示或暗示的保证，也不对本工具的使用或滥用造成的任何直接或间接的损失、责任、索赔、要求或诉讼承担任何责任。\n- 本工具的作者保留随时修改、更新、删除或终止本工具的权利，无需事先通知或承担任何义务。\n- 本工具的使用者应遵守相关法律法规，尊重微信的版权和隐私，不得侵犯微信或其他第三方的合法权益，不得从事任何违法或不道德的行为。\n- 本工具的使用者在下载、安装、运行或使用本工具时，即表示已阅读并同意本免责声明。如有异议，请立即停止使用本工具，并删除所有相关文件。\n\n# 公告\n\n## 项目还在开发中，有些commit有bug，更新叠代会非常迅速。如果你部署好的能用，在正式发布前，可以不用更新了。\n\n## 统一回复ISSUE内的问题：我敢承诺项目内不会有任何形式的后门程序、病毒程序、木马程序，最多只有一个防滥用倒卖的框架检测。\n\n# 📄 文档\n\n## https://henryxiaoyang.github.io/XYBotV2\n\n# 💬 微信交流群\n\n<div style=\"text-align: center\" align=\"center\">\n    <img alt=\"微信交流群二维码\" src=\"https://qrcode.yangres.com/get_image\" style=\"width: 300px; height: auto;\">\n    <p>微信扫码加入交流群</p>\n    <a href=\"https://qrcode.yangres.com/get_image\">🔗图片会被缓存，点我查看最新二维码</a>\n</div>\n\n# 🙏 赞助\n\n<div style=\"text-align: center\" align=\"center\">\n    <h2>开源不易，请作者喝杯奶茶吧🙏</h2>\n    \n    \n</div>\n\n# ✨ 主要功能\n\n## 🛠️ 基础功能\n\n- 🤖 AI聊天 - 支持文字、图片、语音等多模态交互\n- 📰 每日新闻 - 自动推送每日新闻\n- 🎵 点歌系统 - 支持在线点歌\n- 🌤️ 天气查询 - 查询全国各地天气\n- 🎮 游戏功能 - 五子棋、战争雷霆玩家查询等\n\n## 💎 积分系统\n\n- 📝 每日签到 - 支持连续签到奖励\n- 🎲 抽奖系统 - 多种抽奖玩法\n- 🧧 红包系统 - 群内发积分红包\n- 💰 积分交易 - 用户间积分转账\n- 📊 积分排行 - 查看积分排名\n\n## 👮 管理功能\n\n- ⚙️ 插件管理 - 动态加载/卸载插件\n- 👥 白名单管理 - 控制机器人使用权限\n- 📊 积分管理 - 管理员可调整用户积分\n- 🔄 签到重置 - 重置所有用户签到状态\n\n# 🔌 插件系统\n\nXYBot V2 采用插件化设计,所有功能都以插件形式实现。主要插件包括:\n\n- 👨‍💼 AdminPoint - 积分管理\n- 🔄 AdminSignInReset - 签到重置\n- 🛡️ AdminWhitelist - 白名单管理\n- 🤖 Ai - AI聊天\n- 📊 BotStatus - 机器人状态\n- 📱 GetContact - 获取通讯录\n- 🌤️ GetWeather - 天气查询\n- 🎮 Gomoku - 五子棋游戏\n- 🌅 GoodMorning - 早安问候\n- 📈 Leaderboard - 积分排行\n- 🎲 LuckyDraw - 幸运抽奖\n- 📋 Menu - 菜单系统\n- 🎵 Music - 点歌系统\n- 📰 News - 新闻推送\n- 💱 PointTrade - 积分交易\n- 💰 QueryPoint - 积分查询\n- 🎯 RandomMember - 随机群成员\n- 🖼️ RandomPicture - 随机图片\n- 🧧 RedPacket - 红包系统\n- ✍️ SignIn - 每日签到\n- ✈️ Warthunder - 战争雷霆查询\n\n# 🚀 部署说明\n\n## 💻 Python部署\n\n### 🪟 Windows部署\n\n#### 1. 环境准备\n\n- 安装 Python 3.11: https://www.python.org/downloads/release/python-3119/\n- 安装 ffmpeg: 从[ffmpeg官网](https://www.ffmpeg.org/download.html)下载并添加到环境变量\n- 安装 Redis: 从[Redis](https://github.com/tporadowski/redis/releases/tag/v5.0.14.1)下载并启动服务\n\n#### 2. 安装项目\n\n```bash\ngit clone https://github.com/HenryXiaoYang/XYBotV2.git\ncd XYBotV2\npython -m venv venv\n.\\venv\\Scripts\\activate\npip install -r requirements.txt\n```\n\n#### 3. 启动机器人\n\n```bash\nstart redis-server\npython app.py\n```\n\n### 🐧 Linux部署\n\n#### 1. 环境准备\n\n```bash\nsudo apt update\nsudo apt install python3.11 python3.11-venv redis-server ffmpeg\nsudo systemctl start redis\nsudo systemctl enable redis\n```\n\n#### 2. 安装项目\n\n```bash\ngit clone https://github.com/HenryXiaoYang/XYBotV2.git\ncd XYBotV2\npython3.11 -m venv venv\nsource venv/bin/activate\npip install -r requirements.txt\n```\n\n#### 3. 启动机器人\n\n```bash\npython app.py\n```\n\n### 🌌 无WebUI简单启动\n\n如果你不需要WebUI界面，可以直接使用bot.py：\n\n```bash\npython bot.py\n```\n\n## ⚙️ 配置说明\n\n- 主配置: main_config.toml\n- 插件配置: plugins/all_in_one_config.toml\n\n这几个插件需要配置API密钥:\n- 🤖 Ai\n- 🌤️ GetWeather\n\n## ❓ 常见问题\n\n1. 与网络相关的报错\n   - 检查网络连接\n   - 关闭代理软件\n   - 重启XYBot和Redis\n\n2. `正在运行`相关的报错\n   - 将占用9000端口的进程结束\n\n3. 无法访问Web界面\n   - 确保9999端口已开放\n   - 配置防火墙允许9999端口\n\n# 💻 代码提交\n\n提交代码时请使用 `feat: something` 作为说明，支持的标识如下:\n\n- `feat` 新功能(feature)\n- `fix` 修复bug\n- `docs` 文档(documentation)\n- `style` 格式(不影响代码运行的变动)\n- `ref` 重构(即不是新增功能，也不是修改bug的代码变动)\n- `perf` 性能优化(performance)\n- `test` 增加测试\n- `chore` 构建过程或辅助工具的变动\n- `revert` 撤销",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "xbotv2",
        "wechat",
        "bot",
        "bot framework",
        "wechat bot",
        "comprehensive wechat"
      ],
      "category": "realtime-collaboration"
    },
    "a1351995160--Auto-GPT": {
      "owner": "a1351995160",
      "name": "Auto-GPT",
      "url": "https://github.com/a1351995160/Auto-GPT",
      "imageUrl": "/freedevtools/mcp/pfp/a1351995160.webp",
      "description": "Autonomously achieve goals by chaining thoughts of the GPT-4 model while accessing the internet, managing memory, and storing files. The server supports plugin extensibility to enhance functionality.",
      "stars": 0,
      "forks": 0,
      "license": "MIT License",
      "language": "",
      "updated_at": "2023-05-22T08:25:34Z",
      "readme_content": "# Auto-GPT: An Autonomous GPT-4 Experiment\n[![Official Website](https://img.shields.io/badge/Official%20Website-agpt.co-blue?style=flat&logo=world&logoColor=white)](https://agpt.co)\n[![Unit Tests](https://img.shields.io/github/actions/workflow/status/Significant-Gravitas/Auto-GPT/ci.yml?label=unit%20tests)](https://github.com/Significant-Gravitas/Auto-GPT/actions/workflows/ci.yml)\n[](https://discord.gg/autogpt)\n[![GitHub Repo stars](https://img.shields.io/github/stars/Significant-Gravitas/auto-gpt?style=social)](https://github.com/Significant-Gravitas/Auto-GPT/stargazers)\n[![Twitter Follow](https://img.shields.io/twitter/follow/siggravitas?style=social)](https://twitter.com/SigGravitas)\n\n## 💡 Get help - [Q&A](https://github.com/Significant-Gravitas/Auto-GPT/discussions/categories/q-a) or [Discord 💬](https://discord.gg/autogpt)\n\n<hr/>\n\n### 🔴 USE `stable` not `master` 🔴\n\n**Download the latest `stable` release from here: https://github.com/Significant-Gravitas/Auto-GPT/releases/latest.**\nThe `master` branch is under heavy development and may often be in a **broken** state.\n\n<hr/>\n\n\nAuto-GPT is an experimental open-source application showcasing the capabilities of the GPT-4 language model. This program, driven by GPT-4, chains together LLM \"thoughts\", to autonomously achieve whatever goal you set. As one of the first examples of GPT-4 running fully autonomously, Auto-GPT pushes the boundaries of what is possible with AI.\n\n<h2 align=\"center\"> Demo April 16th 2023 </h2>\n\nhttps://user-images.githubusercontent.com/70048414/232352935-55c6bf7c-3958-406e-8610-0913475a0b05.mp4\n\nDemo made by <a href=https://twitter.com/BlakeWerlinger>Blake Werlinger</a>\n\n<h2 align=\"center\"> 💖 Help Fund Auto-GPT's Development 💖</h2>\n<p align=\"center\">\nIf you can spare a coffee, you can help to cover the costs of developing Auto-GPT and help to push the boundaries of fully autonomous AI!\nYour support is greatly appreciated. Development of this free, open-source project is made possible by all the <a href=\"https://github.com/Significant-Gravitas/Auto-GPT/graphs/contributors\">contributors</a> and <a href=\"https://github.com/sponsors/Torantulino\">sponsors</a>. If you'd like to sponsor this project and have your avatar or company logo appear below <a href=\"https://github.com/sponsors/Torantulino\">click here</a>.\n</p>\n\n\n<p align=\"center\">\n<div align=\"center\" class=\"logo-container\">\n<a href=\"https://www.zilliz.com/\">\n<picture height=\"40px\">\n  <source media=\"(prefers-color-scheme: light)\" srcset=\"https://user-images.githubusercontent.com/22963551/234158272-7917382e-ff80-469e-8d8c-94f4477b8b5a.png\">\n  <img src=\"https://user-images.githubusercontent.com/22963551/234158222-30e2d7a7-f0a9-433d-a305-e3aa0b194444.png\" height=\"40px\" alt=\"Zilliz\" />\n</picture>\n</a>\n\n<a href=\"https://roost.ai\">\n<img src=\"https://user-images.githubusercontent.com/22963551/234180283-b58cb03c-c95a-4196-93c1-28b52a388e9d.png\" height=\"40px\" alt=\"Roost.AI\" />\n</a>\n  \n<a href=\"https://nuclei.ai/\">\n<picture height=\"40px\">\n  <source media=\"(prefers-color-scheme: light)\" srcset=\"https://user-images.githubusercontent.com/22963551/234153428-24a6f31d-c0c6-4c9b-b3f4-9110148f67b4.png\">\n  <img src=\"https://user-images.githubusercontent.com/22963551/234181283-691c5d71-ca94-4646-a1cf-6e818bd86faa.png\" height=\"40px\" alt=\"NucleiAI\" />\n</picture>\n</a>\n\n<a href=\"https://www.algohash.org/\">\n<picture>\n  <source media=\"(prefers-color-scheme: light)\" srcset=\"https://user-images.githubusercontent.com/22963551/234180375-1365891c-0ba6-4d49-94c3-847c85fe03b0.png\" >\n  <img src=\"https://user-images.githubusercontent.com/22963551/234180359-143e4a7a-4a71-4830-99c8-9b165cde995f.png\" height=\"40px\" alt=\"Algohash\" />\n</picture>\n</a>\n\n<a href=\"https://www.typingmind.com/?utm_source=autogpt\">\n<picture height=\"40px\">\n  <source media=\"(prefers-color-scheme: light)\" srcset=\"https://user-images.githubusercontent.com/22963551/233202971-61e77209-58a0-47d9-9f7e-dd081111437b.png\">\n  <img src=\"https://user-images.githubusercontent.com/22963551/234157731-f908b5db-8fe7-4036-89b6-7b2a21f87e3a.png\" height=\"40px\" alt=\"TypingMind\" />\n</picture>\n</a>\n\n<a href=\"https://github.com/weaviate/weaviate\">\n<picture height=\"40px\">\n  <source media=\"(prefers-color-scheme: light)\" srcset=\"https://user-images.githubusercontent.com/22963551/234181699-3d7f6ea8-5a7f-4e98-b812-37be1081be4b.png\">\n  <img src=\"https://user-images.githubusercontent.com/22963551/234181695-fc895159-b921-4895-9a13-65e6eff5b0e7.png\" height=\"40px\" alt=\"TypingMind\" />\n</picture>\n</a>\n\n<a href=\"https://chatgpv.com/?ref=spni76459e4fa3f30a\">\n<img src=\"https://github-production-user-asset-6210df.s3.amazonaws.com/22963551/239132565-623a2dd6-eaeb-4941-b40f-c5a29ca6bebc.png\" height=\"40px\" alt=\"ChatGPV\" />\n</a>\n  \n</div>\n</br>\n\n\n\n<p align=\"center\"><a href=\"https://github.com/robinicus\"><img src=\"https://avatars.githubusercontent.com/robinicus?v=4\" width=\"50px\" alt=\"robinicus\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/0xmatchmaker\"><img src=\"https://avatars.githubusercontent.com/0xmatchmaker?v=4\" width=\"50px\" alt=\"0xmatchmaker\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/jazgarewal\"><img src=\"https://avatars.githubusercontent.com/jazgarewal?v=4\" width=\"50px\" alt=\"jazgarewal\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/MayurVirkar\"><img src=\"https://avatars.githubusercontent.com/MayurVirkar?v=4\" width=\"50px\" alt=\"MayurVirkar\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/avy-ai\"><img src=\"https://avatars.githubusercontent.com/avy-ai?v=4\" width=\"50px\" alt=\"avy-ai\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/TheStoneMX\"><img src=\"https://avatars.githubusercontent.com/TheStoneMX?v=4\" width=\"50px\" alt=\"TheStoneMX\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/goldenrecursion\"><img src=\"https://avatars.githubusercontent.com/goldenrecursion?v=4\" width=\"50px\" alt=\"goldenrecursion\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/MatthewAgs\"><img src=\"https://avatars.githubusercontent.com/MatthewAgs?v=4\" width=\"50px\" alt=\"MatthewAgs\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/eelbaz\"><img src=\"https://avatars.githubusercontent.com/eelbaz?v=4\" width=\"50px\" alt=\"eelbaz\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/rapidstartup\"><img src=\"https://avatars.githubusercontent.com/rapidstartup?v=4\" width=\"50px\" alt=\"rapidstartup\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/gklab\"><img src=\"https://avatars.githubusercontent.com/gklab?v=4\" width=\"50px\" alt=\"gklab\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/VoiceBeer\"><img src=\"https://avatars.githubusercontent.com/VoiceBeer?v=4\" width=\"50px\" alt=\"VoiceBeer\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/DailyBotHQ\"><img src=\"https://avatars.githubusercontent.com/DailyBotHQ?v=4\" width=\"50px\" alt=\"DailyBotHQ\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/lucas-chu\"><img src=\"https://avatars.githubusercontent.com/lucas-chu?v=4\" width=\"50px\" alt=\"lucas-chu\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/knifour\"><img src=\"https://avatars.githubusercontent.com/knifour?v=4\" width=\"50px\" alt=\"knifour\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/refinery1\"><img src=\"https://avatars.githubusercontent.com/refinery1?v=4\" width=\"50px\" alt=\"refinery1\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/st617\"><img src=\"https://avatars.githubusercontent.com/st617?v=4\" width=\"50px\" alt=\"st617\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/neodenit\"><img src=\"https://avatars.githubusercontent.com/neodenit?v=4\" width=\"50px\" alt=\"neodenit\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/CrazySwami\"><img src=\"https://avatars.githubusercontent.com/CrazySwami?v=4\" width=\"50px\" alt=\"CrazySwami\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/Heitechsoft\"><img src=\"https://avatars.githubusercontent.com/Heitechsoft?v=4\" width=\"50px\" alt=\"Heitechsoft\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/RealChrisSean\"><img src=\"https://avatars.githubusercontent.com/RealChrisSean?v=4\" width=\"50px\" alt=\"RealChrisSean\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/abhinav-pandey29\"><img src=\"https://avatars.githubusercontent.com/abhinav-pandey29?v=4\" width=\"50px\" alt=\"abhinav-pandey29\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/Explorergt92\"><img src=\"https://avatars.githubusercontent.com/Explorergt92?v=4\" width=\"50px\" alt=\"Explorergt92\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/SparkplanAI\"><img src=\"https://avatars.githubusercontent.com/SparkplanAI?v=4\" width=\"50px\" alt=\"SparkplanAI\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/crizzler\"><img src=\"https://avatars.githubusercontent.com/crizzler?v=4\" width=\"50px\" alt=\"crizzler\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/kreativai\"><img src=\"https://avatars.githubusercontent.com/kreativai?v=4\" width=\"50px\" alt=\"kreativai\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/omphos\"><img src=\"https://avatars.githubusercontent.com/omphos?v=4\" width=\"50px\" alt=\"omphos\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/Jahmazon\"><img src=\"https://avatars.githubusercontent.com/Jahmazon?v=4\" width=\"50px\" alt=\"Jahmazon\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/tjarmain\"><img src=\"https://avatars.githubusercontent.com/tjarmain?v=4\" width=\"50px\" alt=\"tjarmain\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/ddtarazona\"><img src=\"https://avatars.githubusercontent.com/ddtarazona?v=4\" width=\"50px\" alt=\"ddtarazona\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/saten-private\"><img src=\"https://avatars.githubusercontent.com/saten-private?v=4\" width=\"50px\" alt=\"saten-private\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/anvarazizov\"><img src=\"https://avatars.githubusercontent.com/anvarazizov?v=4\" width=\"50px\" alt=\"anvarazizov\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/lazzacapital\"><img src=\"https://avatars.githubusercontent.com/lazzacapital?v=4\" width=\"50px\" alt=\"lazzacapital\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/m\"><img src=\"https://avatars.githubusercontent.com/m?v=4\" width=\"50px\" alt=\"m\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/Pythagora-io\"><img src=\"https://avatars.githubusercontent.com/Pythagora-io?v=4\" width=\"50px\" alt=\"Pythagora-io\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/Web3Capital\"><img src=\"https://avatars.githubusercontent.com/Web3Capital?v=4\" width=\"50px\" alt=\"Web3Capital\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/toverly1\"><img src=\"https://avatars.githubusercontent.com/toverly1?v=4\" width=\"50px\" alt=\"toverly1\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/digisomni\"><img src=\"https://avatars.githubusercontent.com/digisomni?v=4\" width=\"50px\" alt=\"digisomni\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/concreit\"><img src=\"https://avatars.githubusercontent.com/concreit?v=4\" width=\"50px\" alt=\"concreit\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/LeeRobidas\"><img src=\"https://avatars.githubusercontent.com/LeeRobidas?v=4\" width=\"50px\" alt=\"LeeRobidas\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/Josecodesalot\"><img src=\"https://avatars.githubusercontent.com/Josecodesalot?v=4\" width=\"50px\" alt=\"Josecodesalot\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/dexterityx\"><img src=\"https://avatars.githubusercontent.com/dexterityx?v=4\" width=\"50px\" alt=\"dexterityx\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/rickscode\"><img src=\"https://avatars.githubusercontent.com/rickscode?v=4\" width=\"50px\" alt=\"rickscode\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/Brodie0\"><img src=\"https://avatars.githubusercontent.com/Brodie0?v=4\" width=\"50px\" alt=\"Brodie0\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/FSTatSBS\"><img src=\"https://avatars.githubusercontent.com/FSTatSBS?v=4\" width=\"50px\" alt=\"FSTatSBS\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/nocodeclarity\"><img src=\"https://avatars.githubusercontent.com/nocodeclarity?v=4\" width=\"50px\" alt=\"nocodeclarity\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/jsolejr\"><img src=\"https://avatars.githubusercontent.com/jsolejr?v=4\" width=\"50px\" alt=\"jsolejr\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/amr-elsehemy\"><img src=\"https://avatars.githubusercontent.com/amr-elsehemy?v=4\" width=\"50px\" alt=\"amr-elsehemy\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/RawBanana\"><img src=\"https://avatars.githubusercontent.com/RawBanana?v=4\" width=\"50px\" alt=\"RawBanana\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/horazius\"><img src=\"https://avatars.githubusercontent.com/horazius?v=4\" width=\"50px\" alt=\"horazius\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/SwftCoins\"><img src=\"https://avatars.githubusercontent.com/SwftCoins?v=4\" width=\"50px\" alt=\"SwftCoins\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/tob-le-rone\"><img src=\"https://avatars.githubusercontent.com/tob-le-rone?v=4\" width=\"50px\" alt=\"tob-le-rone\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/RThaweewat\"><img src=\"https://avatars.githubusercontent.com/RThaweewat?v=4\" width=\"50px\" alt=\"RThaweewat\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/jun784\"><img src=\"https://avatars.githubusercontent.com/jun784?v=4\" width=\"50px\" alt=\"jun784\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/joaomdmoura\"><img src=\"https://avatars.githubusercontent.com/joaomdmoura?v=4\" width=\"50px\" alt=\"joaomdmoura\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/rejunity\"><img src=\"https://avatars.githubusercontent.com/rejunity?v=4\" width=\"50px\" alt=\"rejunity\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/mathewhawkins\"><img src=\"https://avatars.githubusercontent.com/mathewhawkins?v=4\" width=\"50px\" alt=\"mathewhawkins\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/caitlynmeeks\"><img src=\"https://avatars.githubusercontent.com/caitlynmeeks?v=4\" width=\"50px\" alt=\"caitlynmeeks\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/jd3655\"><img src=\"https://avatars.githubusercontent.com/jd3655?v=4\" width=\"50px\" alt=\"jd3655\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/Odin519Tomas\"><img src=\"https://avatars.githubusercontent.com/Odin519Tomas?v=4\" width=\"50px\" alt=\"Odin519Tomas\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/DataMetis\"><img src=\"https://avatars.githubusercontent.com/DataMetis?v=4\" width=\"50px\" alt=\"DataMetis\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/webbcolton\"><img src=\"https://avatars.githubusercontent.com/webbcolton?v=4\" width=\"50px\" alt=\"webbcolton\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/rocks6\"><img src=\"https://avatars.githubusercontent.com/rocks6?v=4\" width=\"50px\" alt=\"rocks6\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/cxs\"><img src=\"https://avatars.githubusercontent.com/cxs?v=4\" width=\"50px\" alt=\"cxs\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/fruition\"><img src=\"https://avatars.githubusercontent.com/fruition?v=4\" width=\"50px\" alt=\"fruition\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/nnkostov\"><img src=\"https://avatars.githubusercontent.com/nnkostov?v=4\" width=\"50px\" alt=\"nnkostov\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/morcos\"><img src=\"https://avatars.githubusercontent.com/morcos?v=4\" width=\"50px\" alt=\"morcos\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/pingbotan\"><img src=\"https://avatars.githubusercontent.com/pingbotan?v=4\" width=\"50px\" alt=\"pingbotan\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/maxxflyer\"><img src=\"https://avatars.githubusercontent.com/maxxflyer?v=4\" width=\"50px\" alt=\"maxxflyer\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/tommi-joentakanen\"><img src=\"https://avatars.githubusercontent.com/tommi-joentakanen?v=4\" width=\"50px\" alt=\"tommi-joentakanen\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/hunteraraujo\"><img src=\"https://avatars.githubusercontent.com/hunteraraujo?v=4\" width=\"50px\" alt=\"hunteraraujo\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/projectonegames\"><img src=\"https://avatars.githubusercontent.com/projectonegames?v=4\" width=\"50px\" alt=\"projectonegames\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/tullytim\"><img src=\"https://avatars.githubusercontent.com/tullytim?v=4\" width=\"50px\" alt=\"tullytim\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/comet-ml\"><img src=\"https://avatars.githubusercontent.com/comet-ml?v=4\" width=\"50px\" alt=\"comet-ml\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/thepok\"><img src=\"https://avatars.githubusercontent.com/thepok?v=4\" width=\"50px\" alt=\"thepok\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/prompthero\"><img src=\"https://avatars.githubusercontent.com/prompthero?v=4\" width=\"50px\" alt=\"prompthero\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/sunchongren\"><img src=\"https://avatars.githubusercontent.com/sunchongren?v=4\" width=\"50px\" alt=\"sunchongren\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/neverinstall\"><img src=\"https://avatars.githubusercontent.com/neverinstall?v=4\" width=\"50px\" alt=\"neverinstall\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/josephcmiller2\"><img src=\"https://avatars.githubusercontent.com/josephcmiller2?v=4\" width=\"50px\" alt=\"josephcmiller2\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/yx3110\"><img src=\"https://avatars.githubusercontent.com/yx3110?v=4\" width=\"50px\" alt=\"yx3110\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/MBassi91\"><img src=\"https://avatars.githubusercontent.com/MBassi91?v=4\" width=\"50px\" alt=\"MBassi91\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/SpacingLily\"><img src=\"https://avatars.githubusercontent.com/SpacingLily?v=4\" width=\"50px\" alt=\"SpacingLily\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/arthur-x88\"><img src=\"https://avatars.githubusercontent.com/arthur-x88?v=4\" width=\"50px\" alt=\"arthur-x88\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/ciscodebs\"><img src=\"https://avatars.githubusercontent.com/ciscodebs?v=4\" width=\"50px\" alt=\"ciscodebs\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/christian-gheorghe\"><img src=\"https://avatars.githubusercontent.com/christian-gheorghe?v=4\" width=\"50px\" alt=\"christian-gheorghe\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/EngageStrategies\"><img src=\"https://avatars.githubusercontent.com/EngageStrategies?v=4\" width=\"50px\" alt=\"EngageStrategies\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/jondwillis\"><img src=\"https://avatars.githubusercontent.com/jondwillis?v=4\" width=\"50px\" alt=\"jondwillis\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/Cameron-Fulton\"><img src=\"https://avatars.githubusercontent.com/Cameron-Fulton?v=4\" width=\"50px\" alt=\"Cameron-Fulton\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/AryaXAI\"><img src=\"https://avatars.githubusercontent.com/AryaXAI?v=4\" width=\"50px\" alt=\"AryaXAI\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/AuroraHolding\"><img src=\"https://avatars.githubusercontent.com/AuroraHolding?v=4\" width=\"50px\" alt=\"AuroraHolding\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/Mr-Bishop42\"><img src=\"https://avatars.githubusercontent.com/Mr-Bishop42?v=4\" width=\"50px\" alt=\"Mr-Bishop42\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/doverhq\"><img src=\"https://avatars.githubusercontent.com/doverhq?v=4\" width=\"50px\" alt=\"doverhq\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/johnculkin\"><img src=\"https://avatars.githubusercontent.com/johnculkin?v=4\" width=\"50px\" alt=\"johnculkin\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/marv-technology\"><img src=\"https://avatars.githubusercontent.com/marv-technology?v=4\" width=\"50px\" alt=\"marv-technology\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/ikarosai\"><img src=\"https://avatars.githubusercontent.com/ikarosai?v=4\" width=\"50px\" alt=\"ikarosai\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/ColinConwell\"><img src=\"https://avatars.githubusercontent.com/ColinConwell?v=4\" width=\"50px\" alt=\"ColinConwell\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/humungasaurus\"><img src=\"https://avatars.githubusercontent.com/humungasaurus?v=4\" width=\"50px\" alt=\"humungasaurus\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/terpsfreak\"><img src=\"https://avatars.githubusercontent.com/terpsfreak?v=4\" width=\"50px\" alt=\"terpsfreak\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/iddelacruz\"><img src=\"https://avatars.githubusercontent.com/iddelacruz?v=4\" width=\"50px\" alt=\"iddelacruz\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/thisisjeffchen\"><img src=\"https://avatars.githubusercontent.com/thisisjeffchen?v=4\" width=\"50px\" alt=\"thisisjeffchen\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/nicoguyon\"><img src=\"https://avatars.githubusercontent.com/nicoguyon?v=4\" width=\"50px\" alt=\"nicoguyon\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/arjunb023\"><img src=\"https://avatars.githubusercontent.com/arjunb023?v=4\" width=\"50px\" alt=\"arjunb023\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/Nalhos\"><img src=\"https://avatars.githubusercontent.com/Nalhos?v=4\" width=\"50px\" alt=\"Nalhos\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/belharethsami\"><img src=\"https://avatars.githubusercontent.com/belharethsami?v=4\" width=\"50px\" alt=\"belharethsami\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/Mobivs\"><img src=\"https://avatars.githubusercontent.com/Mobivs?v=4\" width=\"50px\" alt=\"Mobivs\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/txtr99\"><img src=\"https://avatars.githubusercontent.com/txtr99?v=4\" width=\"50px\" alt=\"txtr99\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/ntwrite\"><img src=\"https://avatars.githubusercontent.com/ntwrite?v=4\" width=\"50px\" alt=\"ntwrite\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/founderblocks-sils\"><img src=\"https://avatars.githubusercontent.com/founderblocks-sils?v=4\" width=\"50px\" alt=\"founderblocks-sils\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/kMag410\"><img src=\"https://avatars.githubusercontent.com/kMag410?v=4\" width=\"50px\" alt=\"kMag410\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/angiaou\"><img src=\"https://avatars.githubusercontent.com/angiaou?v=4\" width=\"50px\" alt=\"angiaou\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/garythebat\"><img src=\"https://avatars.githubusercontent.com/garythebat?v=4\" width=\"50px\" alt=\"garythebat\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/lmaugustin\"><img src=\"https://avatars.githubusercontent.com/lmaugustin?v=4\" width=\"50px\" alt=\"lmaugustin\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/shawnharmsen\"><img src=\"https://avatars.githubusercontent.com/shawnharmsen?v=4\" width=\"50px\" alt=\"shawnharmsen\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/clortegah\"><img src=\"https://avatars.githubusercontent.com/clortegah?v=4\" width=\"50px\" alt=\"clortegah\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/MetaPath01\"><img src=\"https://avatars.githubusercontent.com/MetaPath01?v=4\" width=\"50px\" alt=\"MetaPath01\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/sekomike910\"><img src=\"https://avatars.githubusercontent.com/sekomike910?v=4\" width=\"50px\" alt=\"sekomike910\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/MediConCenHK\"><img src=\"https://avatars.githubusercontent.com/MediConCenHK?v=4\" width=\"50px\" alt=\"MediConCenHK\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/svpermari0\"><img src=\"https://avatars.githubusercontent.com/svpermari0?v=4\" width=\"50px\" alt=\"svpermari0\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/jacobyoby\"><img src=\"https://avatars.githubusercontent.com/jacobyoby?v=4\" width=\"50px\" alt=\"jacobyoby\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/turintech\"><img src=\"https://avatars.githubusercontent.com/turintech?v=4\" width=\"50px\" alt=\"turintech\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/allenstecat\"><img src=\"https://avatars.githubusercontent.com/allenstecat?v=4\" width=\"50px\" alt=\"allenstecat\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/CatsMeow492\"><img src=\"https://avatars.githubusercontent.com/CatsMeow492?v=4\" width=\"50px\" alt=\"CatsMeow492\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/tommygeee\"><img src=\"https://avatars.githubusercontent.com/tommygeee?v=4\" width=\"50px\" alt=\"tommygeee\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/judegomila\"><img src=\"https://avatars.githubusercontent.com/judegomila?v=4\" width=\"50px\" alt=\"judegomila\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/cfarquhar\"><img src=\"https://avatars.githubusercontent.com/cfarquhar?v=4\" width=\"50px\" alt=\"cfarquhar\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/ZoneSixGames\"><img src=\"https://avatars.githubusercontent.com/ZoneSixGames?v=4\" width=\"50px\" alt=\"ZoneSixGames\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/kenndanielso\"><img src=\"https://avatars.githubusercontent.com/kenndanielso?v=4\" width=\"50px\" alt=\"kenndanielso\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/CrypteorCapital\"><img src=\"https://avatars.githubusercontent.com/CrypteorCapital?v=4\" width=\"50px\" alt=\"CrypteorCapital\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/sultanmeghji\"><img src=\"https://avatars.githubusercontent.com/sultanmeghji?v=4\" width=\"50px\" alt=\"sultanmeghji\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/jenius-eagle\"><img src=\"https://avatars.githubusercontent.com/jenius-eagle?v=4\" width=\"50px\" alt=\"jenius-eagle\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/josephjacks\"><img src=\"https://avatars.githubusercontent.com/josephjacks?v=4\" width=\"50px\" alt=\"josephjacks\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/pingshian0131\"><img src=\"https://avatars.githubusercontent.com/pingshian0131?v=4\" width=\"50px\" alt=\"pingshian0131\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/AIdevelopersAI\"><img src=\"https://avatars.githubusercontent.com/AIdevelopersAI?v=4\" width=\"50px\" alt=\"AIdevelopersAI\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/ternary5\"><img src=\"https://avatars.githubusercontent.com/ternary5?v=4\" width=\"50px\" alt=\"ternary5\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/ChrisDMT\"><img src=\"https://avatars.githubusercontent.com/ChrisDMT?v=4\" width=\"50px\" alt=\"ChrisDMT\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/AcountoOU\"><img src=\"https://avatars.githubusercontent.com/AcountoOU?v=4\" width=\"50px\" alt=\"AcountoOU\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/chatgpt-prompts\"><img src=\"https://avatars.githubusercontent.com/chatgpt-prompts?v=4\" width=\"50px\" alt=\"chatgpt-prompts\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/Partender\"><img src=\"https://avatars.githubusercontent.com/Partender?v=4\" width=\"50px\" alt=\"Partender\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/Daniel1357\"><img src=\"https://avatars.githubusercontent.com/Daniel1357?v=4\" width=\"50px\" alt=\"Daniel1357\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/KiaArmani\"><img src=\"https://avatars.githubusercontent.com/KiaArmani?v=4\" width=\"50px\" alt=\"KiaArmani\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/zkonduit\"><img src=\"https://avatars.githubusercontent.com/zkonduit?v=4\" width=\"50px\" alt=\"zkonduit\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/fabrietech\"><img src=\"https://avatars.githubusercontent.com/fabrietech?v=4\" width=\"50px\" alt=\"fabrietech\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/scryptedinc\"><img src=\"https://avatars.githubusercontent.com/scryptedinc?v=4\" width=\"50px\" alt=\"scryptedinc\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/coreyspagnoli\"><img src=\"https://avatars.githubusercontent.com/coreyspagnoli?v=4\" width=\"50px\" alt=\"coreyspagnoli\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/AntonioCiolino\"><img src=\"https://avatars.githubusercontent.com/AntonioCiolino?v=4\" width=\"50px\" alt=\"AntonioCiolino\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/Dradstone\"><img src=\"https://avatars.githubusercontent.com/Dradstone?v=4\" width=\"50px\" alt=\"Dradstone\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/CarmenCocoa\"><img src=\"https://avatars.githubusercontent.com/CarmenCocoa?v=4\" width=\"50px\" alt=\"CarmenCocoa\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/bentoml\"><img src=\"https://avatars.githubusercontent.com/bentoml?v=4\" width=\"50px\" alt=\"bentoml\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/merwanehamadi\"><img src=\"https://avatars.githubusercontent.com/merwanehamadi?v=4\" width=\"50px\" alt=\"merwanehamadi\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/vkozacek\"><img src=\"https://avatars.githubusercontent.com/vkozacek?v=4\" width=\"50px\" alt=\"vkozacek\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/ASmithOWL\"><img src=\"https://avatars.githubusercontent.com/ASmithOWL?v=4\" width=\"50px\" alt=\"ASmithOWL\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/tekelsey\"><img src=\"https://avatars.githubusercontent.com/tekelsey?v=4\" width=\"50px\" alt=\"tekelsey\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/GalaxyVideoAgency\"><img src=\"https://avatars.githubusercontent.com/GalaxyVideoAgency?v=4\" width=\"50px\" alt=\"GalaxyVideoAgency\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/wenfengwang\"><img src=\"https://avatars.githubusercontent.com/wenfengwang?v=4\" width=\"50px\" alt=\"wenfengwang\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/rviramontes\"><img src=\"https://avatars.githubusercontent.com/rviramontes?v=4\" width=\"50px\" alt=\"rviramontes\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/indoor47\"><img src=\"https://avatars.githubusercontent.com/indoor47?v=4\" width=\"50px\" alt=\"indoor47\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/ZERO-A-ONE\"><img src=\"https://avatars.githubusercontent.com/ZERO-A-ONE?v=4\" width=\"50px\" alt=\"ZERO-A-ONE\" /></a>&nbsp;&nbsp;</p>\n\n\n\n## 🚀 Features\n\n- 🌐 Internet access for searches and information gathering\n- 💾 Long-term and short-term memory management\n- 🧠 GPT-4 instances for text generation\n- 🔗 Access to popular websites and platforms\n- 🗃️ File storage and summarization with GPT-3.5\n- 🔌 Extensibility with Plugins\n\n## Quickstart\n\n0. Check out the [wiki](https://github.com/Significant-Gravitas/Nexus/wiki)\n1. Get an OpenAI [API Key](https://platform.openai.com/account/api-keys)\n2. Download the [latest release](https://github.com/Significant-Gravitas/Auto-GPT/releases/latest)\n3. Follow the [installation instructions][docs/setup]\n4. Configure any additional features you want, or install some [plugins][docs/plugins]\n5. [Run][docs/usage] the app\n\nPlease see the [documentation][docs] for full setup instructions and configuration options.\n\n[docs]: https://docs.agpt.co/\n\n## 📖 Documentation\n* [⚙️ Setup][docs/setup]\n* [💻 Usage][docs/usage]\n* [🔌 Plugins][docs/plugins]\n* Configuration\n  * [🔍 Web Search](https://docs.agpt.co/configuration/search/)\n  * [🧠 Memory](https://docs.agpt.co/configuration/memory/)\n  * [🗣️ Voice (TTS)](https://docs.agpt.co/configuration/voice/)\n  * [🖼️ Image Generation](https://docs.agpt.co/configuration/imagegen/)\n\n[docs/setup]: https://docs.agpt.co/setup/\n[docs/usage]: https://docs.agpt.co/usage/\n[docs/plugins]: https://docs.agpt.co/plugins/\n\n## ⚠️ Limitations\n\nThis experiment aims to showcase the potential of GPT-4 but comes with some limitations:\n\n1. Not a polished application or product, just an experiment\n2. May not perform well in complex, real-world business scenarios. In fact, if it actually does, please share your results!\n3. Quite expensive to run, so set and monitor your API key limits with OpenAI!\n\n## 🛡 Disclaimer\n\nThis project, Auto-GPT, is an experimental application and is provided \"as-is\" without any warranty, express or implied. By using this software, you agree to assume all risks associated with its use, including but not limited to data loss, system failure, or any other issues that may arise.\n\nThe developers and contributors of this project do not accept any responsibility or liability for any losses, damages, or other consequences that may occur as a result of using this software. You are solely responsible for any decisions and actions taken based on the information provided by Auto-GPT.\n\n**Please note that the use of the GPT-4 language model can be expensive due to its token usage.** By utilizing this project, you acknowledge that you are responsible for monitoring and managing your own token usage and the associated costs. It is highly recommended to check your OpenAI API usage regularly and set up any necessary limits or alerts to prevent unexpected charges.\n\nAs an autonomous experiment, Auto-GPT may generate content or take actions that are not in line with real-world business practices or legal requirements. It is your responsibility to ensure that any actions or decisions made based on the output of this software comply with all applicable laws, regulations, and ethical standards. The developers and contributors of this project shall not be held responsible for any consequences arising from the use of this software.\n\nBy using Auto-GPT, you agree to indemnify, defend, and hold harmless the developers, contributors, and any affiliated parties from and against any and all claims, damages, losses, liabilities, costs, and expenses (including reasonable attorneys' fees) arising from your use of this software or your violation of these terms.\n\n## 🐦 Connect with Us on Twitter\n\nStay up-to-date with the latest news, updates, and insights about Auto-GPT by following our Twitter accounts. Engage with the developer and the AI's own account for interesting discussions, project updates, and more.\n\n- **Developer**: Follow [@siggravitas](https://twitter.com/siggravitas) for insights into the development process, project updates, and related topics from the creator of Entrepreneur-GPT.\n- **Entrepreneur-GPT**: Join the conversation with the AI itself by following [@En_GPT](https://twitter.com/En_GPT). Share your experiences, discuss the AI's outputs, and engage with the growing community of users.\n\nWe look forward to connecting with you and hearing your thoughts, ideas, and experiences with Auto-GPT. Join us on Twitter and let's explore the future of AI together!\n\n<p align=\"center\">\n  <a href=\"https://star-history.com/#Torantulino/auto-gpt&Date\">\n    <img src=\"https://api.star-history.com/svg?repos=Torantulino/auto-gpt&type=Date\" alt=\"Star History Chart\">\n  </a>\n</p>",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "gpt",
        "realtime",
        "auto",
        "auto gpt",
        "gpt autonomously",
        "realtime collaboration"
      ],
      "category": "realtime-collaboration"
    },
    "a1j9o94--swiss": {
      "owner": "a1j9o94",
      "name": "swiss",
      "url": "https://github.com/a1j9o94/swiss",
      "imageUrl": "/freedevtools/mcp/pfp/a1j9o94.webp",
      "description": "Swiss MCP is designed to orchestrate complex AI tasks by coordinating multiple AI tools, enabling streamlined interactions across various resources.",
      "stars": 13,
      "forks": 1,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-07T15:06:13Z",
      "readme_content": "# 🇨🇭 Swiss MCP: A Swiss Army Knife for Multi-Step AI Task Orchestration 🚀\n\nSwiss MCP is your AI-powered command center for orchestrating complex tasks with ease. Think of it as your personal AI assistant that can coordinate multiple AI tools to accomplish amazing things! 🎯\n\n---\n\n## ⚡ Quick Start: Install and Get Running\n\n### Installation\n\n1. **Install `uv` for package management** 📦:\n   ```bash\n   pip install uv\n   ```\n\n2. **Clone this repository** 📥:\n   ```bash\n   git clone https://github.com/your-repo/swiss-mcp.git\n   cd swiss-mcp\n   ```\n\n3. **Install Swiss MCP** 🔧:\n   ```bash\n   pip install fastmcp\n   fastmcp install swiss\n   ```\n\n4. **Start your AI journey** 🎉:\n   ```bash\n   fastmcp swiss\n   ```\n\nYou're all set! Time to unleash the power of Swiss MCP! 💪\n\n---\n\n## 🎮 Mind-Blowing Examples: What Swiss MCP Can Do\n\n### Example 1: AI-Powered Content Creation Studio 🎨\n\n**Assistant**: \"How can I help you create today?\"\n\n**User**: \"Create a viral social media campaign for my new product.\"\n\n**Swiss MCP**:\n1. 🧠 Strategic Planning:\n   - Analyzes market trends using `market-analyzer`\n   - Identifies target audience demographics\n   - Generates content ideas using multiple AI models\n\n2. 🎨 Content Creation:\n   - Creates eye-catching visuals with `stable-diffusion`\n   - Writes engaging copy with `content-writer`\n   - Generates video shorts with `video-generator`\n\n3. 📊 Campaign Optimization:\n   - A/B tests different versions\n   - Schedules posts for optimal timing\n   - Tracks engagement metrics\n\n**Result**: A complete, ready-to-launch campaign with images, videos, and copy optimized for multiple platforms! 🚀\n\n---\n\n### Example 2: Full-Stack App Development Assistant 👨‍💻\n\n**Assistant**: \"Let's build something amazing!\"\n\n**User**: \"Create a modern web app with AI features.\"\n\n**Swiss MCP**:\n1. 🏗️ Architecture Design:\n   - Generates system architecture diagram\n   - Sets up project structure\n   - Creates CI/CD pipeline\n\n2. 🔧 Development:\n   - Scaffolds frontend with `react-builder`\n   - Implements backend with `api-generator`\n   - Integrates AI features using `ai-integrator`\n\n3. 🚀 Deployment:\n   - Containerizes application\n   - Sets up cloud infrastructure\n   - Deploys with monitoring\n\n**Result**: A production-ready web app with AI capabilities, complete with documentation and monitoring! 🎉\n\n---\n\n## 🛠️ Tool Library\n\nThe MCP ecosystem is constantly growing! Just ask Swiss MCP to install any tool you need, and it'll handle the rest. \n\n🔍 Browse the [MCP Tool Library](https://github.com/modelcontextprotocol/servers?tab=readme-ov-file) for more amazing tools!\n\nReady to start building amazing things? Let Swiss MCP be your AI-powered companion! 🚀✨",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "ai",
        "realtime",
        "collaboration",
        "ai tools",
        "multiple ai",
        "realtime collaboration"
      ],
      "category": "realtime-collaboration"
    },
    "abhinavkale-dev--webRTC-basics": {
      "owner": "abhinavkale-dev",
      "name": "webRTC-basics",
      "url": "https://github.com/abhinavkale-dev/webRTC-basics",
      "imageUrl": "/freedevtools/mcp/pfp/abhinavkale-dev.webp",
      "description": "A basic project for learning and experimenting with WebRTC concepts, enabling peer-to-peer communication. Provides foundational tools and implementations for real-time communication technologies.",
      "stars": 0,
      "forks": 0,
      "license": "No License",
      "language": "TypeScript",
      "updated_at": "2025-04-03T17:07:03Z",
      "readme_content": "Basic WebRTC project \n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "webrtc",
        "realtime",
        "collaboration",
        "webrtc concepts",
        "experimenting webrtc",
        "dev webrtc"
      ],
      "category": "realtime-collaboration"
    },
    "abhishekjairath--sonic-pi-mcp": {
      "owner": "abhishekjairath",
      "name": "sonic-pi-mcp",
      "url": "https://github.com/abhishekjairath/sonic-pi-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/abhishekjairath.webp",
      "description": "Interact with Sonic Pi through OSC messages to create and control music programmatically. Execute Sonic Pi code and play notes with customizable synth parameters seamlessly.",
      "stars": 8,
      "forks": 4,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-08-15T23:52:09Z",
      "readme_content": "# Sonic Pi MCP\n[![smithery badge](https://smithery.ai/badge/@abhishekjairath/sonic-pi-mcp)](https://smithery.ai/server/@abhishekjairath/sonic-pi-mcp)\n\nA Model Context Protocol (MCP) server that allows AI assistants to interact with Sonic Pi through OSC messages. This enables AI tools like Claude and Cursor to create music and control Sonic Pi programmatically.\n\n## Features\n\n- Play individual notes with customizable synth parameters\n- Execute arbitrary Sonic Pi code\n- Works with any MCP-compatible client (Claude Desktop, Cursor, etc.)\n\n## Prerequisites\n\n- [Bun](https://bun.sh)\n- [Sonic Pi](https://sonic-pi.net/) (v4.0 or higher)\n- An MCP-compatible client (Cursor, Claude Desktop, etc.)\n\n## Sonic Pi Configuration\n\nBefore using the MCP server, you need to add the following code to your Sonic Pi buffer. This code handles the OSC messages sent by the server:\n\n```ruby\n# Required Sonic Pi configuration\n# Add this to a buffer in Sonic Pi and run it\n\nlive_loop :code_runner do\n  use_real_time\n  code = sync \"/osc*/run-code\"\n  \n  # Since we receive the code as a string, we can use eval to execute it\n  # The code comes as the first element of the message\n  begin\n    eval(code[0].to_s)\n  rescue Exception => e\n    puts \"Error executing code: #{e.message}\"\n  end\nend\n\n```\n\nMake sure this code is running in Sonic Pi before using the MCP server.\n\nYou can also try running [this script](https://github.com/abhishekjairath/sonic-pi-mcp/blob/main/sonic-pi-queue.rb) in SonicPi to have AI assitants play songs in a queue.\n\n\n## Integration with Clients\n\n#### Cursor\n\nAdd to `~/.cursor/mcpServers.json`:\n```json\n{\n  \"mcpServers\": {\n    \"sonic_pi_mcp\": {\n      \"name\": \"Sonic Pi MCP\",\n      \"command\": \"bunx\",\n      \"args\": [\"sonic-pi-mcp\"],\n      \"transport\": {\n        \"type\": \"stdio\"\n      }\n    }\n  }\n}\n```\n\n#### Claude Desktop\n\nAdd to `~/Library/Application Support/Claude/claude_desktop_config.json`:\n```json\n{\n  \"mcpServers\": {\n    \"sonic_pi_mcp\": {\n      \"command\": \"bunx\",\n      \"args\": [\"sonic-pi-mcp\"],\n    }\n  }\n}\n```\n\n## Available Tools\n\n### play_note\n\nPlays a single note with customizable parameters.\n\nParameters:\n- `note` (required): MIDI note number (0-127)\n- `synth` (optional): Synth to use (e.g., \":saw\", \":beep\", \":prophet\")\n- `sustain` (optional): Note duration in seconds (default: 1)\n- `cutoff` (optional): Filter cutoff frequency (default: 100)\n\nExample:\n```typescript\n// Play middle C with saw wave synth\n{\n  \"name\": \"play_note\",\n  \"parameters\": {\n    \"note\": 60,\n    \"synth\": \":saw\",\n    \"sustain\": 0.5,\n    \"cutoff\": 80\n  }\n}\n```\n\n### run_code\n\nExecutes arbitrary Sonic Pi code.\n\nParameters:\n- `code` (required): Sonic Pi code to execute\n\nExample:\n```typescript\n{\n  \"name\": \"run_code\",\n  \"parameters\": {\n    \"code\": \"use_synth :prophet\\nplay_pattern_timed [60, 64, 67], [0.5]\"\n  }\n}\n```\n\n## Example Usage\n\nHere are some example interactions using the MCP tools:\n\n### Simple Melody\n```typescript\n// Play a C major arpeggio\n{\n  \"code\": `\n    use_synth :piano\n    play_pattern_timed [60, 64, 67, 72], [0.25], release: 0.1\n  `\n}\n```\n\n### Complex Pattern\n```typescript\n// Create a rhythmic pattern\n{\n  \"code\": `\n    live_loop :rhythm do\n      use_synth :tb303\n      play choose(chord(:C3, :minor)), release: 0.2, cutoff: rrand(60, 120)\n      sleep 0.25\n    end\n  `\n}\n```\n\n## Troubleshooting\n\n1. **No Sound**\n   - Ensure Sonic Pi is running\n   - Check that the OSC handler code is running in Sonic Pi\n   - Verify Sonic Pi is listening on port 4560 (default)\n\n2. **Connection Errors**\n   - Check if another instance of the server is running\n   - Restart Sonic Pi\n   - Ensure no other applications are using port 4560\n\n3. **Code Execution Errors**\n   - Check the Sonic Pi log window for error messages\n   - Verify the syntax of your Sonic Pi code\n   - Ensure all required synths and samples are available\n\n## Development\n\n```bash\n# Clone the repository\ngit clone https://github.com/abhishekjairath/sonic-pi-mcp.git\ncd sonic-pi-mcp\n\n# Install Bun if you haven't already\ncurl -fsSL https://bun.sh/install | bash\n\n# Install dependencies\nbun install\n\n# Start Sonic Pi and run the OSC handler code (see Sonic Pi Configuration section)\n\n# Start the server in development mode\nbun run dev\n```\n\n### Testing with MCP Inspector\n\n1. Install and start the MCP Inspector:\n```bash\nnpm install -g @modelcontextprotocol/inspector\nmcp-inspector\n```\n\n2. Open your browser and navigate to http://localhost:3000\n\n3. In the MCP Inspector UI, configure the connection:\n   - Command: `bun`\n   - Arguments: `run src/server.ts`\n   - Working Directory: `/path/to/your/sonic-pi-mcp` (use your actual project path)\n   - Transport Type: stdio\n\n4. Test the `play_note` tool:\n```json\n{\n  \"name\": \"play_note\",\n  \"parameters\": {\n    \"note\": 60,\n    \"synth\": \":beep\",\n    \"sustain\": 0.5\n  }\n}\n```\n\n5. Test the `run_code` tool:\n```json\n{\n  \"name\": \"run_code\",\n  \"parameters\": {\n    \"code\": \"use_synth :prophet\\nplay_pattern_timed scale(:c4, :major), [0.25]\"\n  }\n}\n```\n\n### Troubleshooting Development Issues\n\n1. **Bun Installation Issues**\n   - Make sure Bun is in your PATH\n   - Try running `bun --version` to verify the installation\n   - If using Claude Desktop, use the full path to Bun in the config\n\n2. **MCP Inspector Connection Issues**\n   - Verify the server is running (`bun run dev`)\n   - Check that the working directory path is correct\n   - Ensure no other instances of the server are running\n\n3. **OSC Communication Issues**\n   - Confirm Sonic Pi is running and the OSC handler code is active\n   - Check the server logs for connection errors\n   - Verify port 4560 is available and not blocked\n\n## Contributing\n\n1. Fork the repository\n2. Create your feature branch (`git checkout -b feature/amazing-feature`)\n3. Commit your changes (`git commit -m 'Add some amazing feature'`)\n4. Push to the branch (`git push origin feature/amazing-feature`)\n5. Open a Pull Request\n\n## License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details. \n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "sonic",
        "realtime",
        "synth",
        "interact sonic",
        "abhishekjairath sonic",
        "sonic pi"
      ],
      "category": "realtime-collaboration"
    },
    "adamwattis--mcp-proxy-server": {
      "owner": "adamwattis",
      "name": "mcp-proxy-server",
      "url": "https://github.com/adamwattis/mcp-proxy-server",
      "imageUrl": "/freedevtools/mcp/pfp/adamwattis.webp",
      "description": "Aggregates and manages multiple MCP resource servers through a unified interface. Simplifies access to various tools and data sources while handling request routing and response aggregation.",
      "stars": 164,
      "forks": 46,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-10-04T02:28:23Z",
      "readme_content": "# MCP Proxy Server\n\nAn MCP proxy server that aggregates and serves multiple MCP resource servers through a single interface. This server acts as a central hub that can:\n\n- Connect to and manage multiple MCP resource servers\n- Expose their combined capabilities through a unified interface\n- Handle routing of requests to appropriate backend servers\n- Aggregate responses from multiple sources\n\n## Features\n\n### Resource Management\n- Discover and connect to multiple MCP resource servers\n- Aggregate resources from all connected servers\n- Maintain consistent URI schemes across servers\n- Handle resource routing and resolution\n\n### Tool Aggregation\n- Expose tools from all connected servers\n- Route tool calls to appropriate backend servers\n- Maintain tool state and handle responses\n\n### Prompt Handling\n- Aggregate prompts from all connected servers\n- Route prompt requests to appropriate backends\n- Handle multi-server prompt responses\n\n## Configuration\n\nThe server requires a JSON configuration file that specifies the MCP servers to connect to. Copy the example config and modify it for your needs:\n\n```bash\ncp config.example.json config.json\n```\n\nExample config structure:\n```json\n{\n  \"servers\": [\n    {\n      \"name\": \"Server 1\",\n      \"transport\": {\n        \"command\": \"/path/to/server1/build/index.js\"\n      }\n    },\n    {\n      \"name\": \"Server 2\",\n      \"transport\": {\n        \"command\": \"server2-command\",\n        \"args\": [\"--option1\", \"value1\"],\n        \"env\": [\"SECRET_API_KEY\"]\n      }\n    },\n    {\n      \"name\": \"Example Server 3\",\n      \"transport\": {\n        \"type\": \"sse\",\n        \"url\": \"http://localhost:8080/sse\"\n      }\n    }\n  ]\n}\n```\n\nThe config file must be provided when running the server:\n```bash\nMCP_CONFIG_PATH=./config.json mcp-proxy-server\n```\n\n## Development\n\nInstall dependencies:\n```bash\nnpm install\n```\n\nBuild the server:\n```bash\nnpm run build\n```\n\nFor development with auto-rebuild:\n```bash\nnpm run watch\n```\n\nFor development with continuous run:\n```bash\n# Stdio\nnpm run dev\n# SSE\nnpm run dev:sse\n```\n\n## Installation\n\nTo use with Claude Desktop, add the server config:\n\nOn MacOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\nOn Windows: `%APPDATA%/Claude/claude_desktop_config.json`\n\n```json\n{\n  \"mcpServers\": {\n    \"mcp-proxy\": {\n      \"command\": \"/path/to/mcp-proxy-server/build/index.js\",\n      \"env\": {\n        \"MCP_CONFIG_PATH\": \"/absolute/path/to/your/config.json\",\n        \"KEEP_SERVER_OPEN\": \"1\"\n      }\n    }\n  }\n}\n```\n\n- `KEEP_SERVER_OPEN` will keep the SSE running even if a client disconnects. Useful when multiple clients connects to the MCP proxy.\n\n### Debugging\n\nSince MCP servers communicate over stdio, debugging can be challenging. We recommend using the [MCP Inspector](https://github.com/modelcontextprotocol/inspector), which is available as a package script:\n\n```bash\nnpm run inspector\n```\n\nThe Inspector will provide a URL to access debugging tools in your browser.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "mcp",
        "proxy",
        "servers",
        "mcp proxy",
        "mcp resource",
        "multiple mcp"
      ],
      "category": "realtime-collaboration"
    },
    "aech-ai--mcp-teams-test": {
      "owner": "aech-ai",
      "name": "mcp-teams-test",
      "url": "https://github.com/aech-ai/mcp-teams-test",
      "imageUrl": "/freedevtools/mcp/pfp/aech-ai.webp",
      "description": "Integrates Microsoft Teams chat and messaging with MCP-compatible clients, offering advanced search, persistent storage, and live event streaming. Utilizes PostgreSQL and DuckDB for efficient message retrieval and management, while providing a CLI client for local testing and token management.",
      "stars": 0,
      "forks": 1,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-04-25T18:13:52Z",
      "readme_content": "# Teams Messenger MCP App\n\nThis project implements a pure Model Context Protocol (MCP) server that bridges Microsoft Teams and MCP-compatible clients (LLMs, agentic frameworks, and a rich CLI MCP client). All features are exposed via MCP tools, resources, and events—no REST API endpoints.\n\n## Features\n- Microsoft Teams chat/message integration via MCP\n- PostgreSQL-based Information Retrieval (IR) server for advanced search capabilities\n- Persistent storage in DuckDB for chat/message history\n- Hybrid semantic and lexical search (BM25 + vector, FlockMTL-style)\n- CLI for login/token management and a rich MCP client for local testing\n- Polling-based event emission for new messages\n- Live event streaming and search for LLMs and CLI\n- Single-agent (bot) account, not multi-user\n\n## Architecture\n```\n+-------------------+      +-------------------+      +-------------------+\n|   CLI MCP Client  |<---->|    MCP Server     |<---->|  Microsoft Teams  |\n| (rich terminal UI)|      | (Python, FastMCP) |      |  (Graph API)      |\n+-------------------+      +-------------------+      +-------------------+\n         |                        |                          \n         |                        v                          \n         |                +-------------------+      +-------------------+\n         |                |     DuckDB DB     |      |    IR Server      |\n         |                +-------------------+      | (PostgreSQL, API) |\n                                                     +-------------------+\n                                                              |\n                                                              v\n                                                     +-------------------+\n                                                     |  PostgreSQL DB    |\n                                                     |  (with pgvector)  |\n                                                     +-------------------+\n```\n- All chat/message/search logic is via MCP tools/resources/events\n- Teams MCP server uses DuckDB for message storage\n- IR server provides advanced search capabilities with PostgreSQL and pgvector\n- IR server exposes an HTTP API for MCP server communication\n\n## Installation\n\n### Requirements\n- Python 3.9+\n- [pip](https://pip.pypa.io/en/stable/)\n- [Docker](https://www.docker.com/) and [Docker Compose](https://docs.docker.com/compose/) (for containerized deployment)\n\n### Option 1: Local Installation\n\n#### 1. Clone the repository\n```bash\ngit clone <your-repo-url>\ncd mcp-teams\n```\n\n#### 2. Install dependencies\n```bash\npip install -r requirements.txt\n```\n\n#### 3. Configure environment variables\nCopy the template and fill in your Azure AD/Teams credentials:\n```bash\ncp .env.template .env\n# Edit .env and fill in your Azure AD and other settings\n```\nSee the table below for variable descriptions.\n\n### Option 2: Docker Deployment (Recommended)\n\n#### 1. Clone the repository\n```bash\ngit clone <your-repo-url>\ncd mcp-teams\n```\n\n#### 2. Configure environment variables\nCopy the template and fill in your credentials:\n```bash\ncp .env.template .env\n# Edit .env and fill in your settings\n```\n\n#### 3. Build and start services\n```bash\ndocker-compose up -d\n```\n\n## Environment Variables (.env)\n\n| Variable            | Description                                                      | Example / Default           |\n|---------------------|------------------------------------------------------------------|-----------------------------|\n| AZURE_CLIENT_ID      | Azure AD Application (client) ID                                 | `xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx` |\n| AZURE_CLIENT_SECRET  | Azure AD Application secret                                      | `your-secret`               |\n| AZURE_TENANT_ID      | Azure AD Tenant ID                                               | `xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx` |\n| AZURE_APP_OBJECT_ID  | Azure AD Application object ID                                  | `xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx` |\n| DUCKDB_PATH         | Path to DuckDB database file                                     | `db/teams_mcp.duckdb`       |\n| TOKEN_PATH          | Path to store persistent token cache                             | `db/token_cache.json`        |\n| POLL_INTERVAL       | Polling interval (seconds) for new messages                      | `10`                        |\n| DEMO_MODE           | Set to `true` for mock/demo mode (no real Teams API calls)       | `false`                     |\n| OPENAI_API_KEY      | OpenAI API key for embedding generation                         | `sk-...`                    |\n| POSTGRES_USER       | PostgreSQL username                                              | `postgres`                  |\n| POSTGRES_PASSWORD   | PostgreSQL password                                              | `postgres`                  |\n| POSTGRES_DB         | PostgreSQL database name                                         | `mcp_ir`                    |\n| IR_SERVER_HOST      | IR server hostname                                               | `ir_server`                 |\n| IR_SERVER_PORT      | IR server port                                                   | `8090`                      |\n\n## Running the MCP Server\n\n### Local Mode (without Docker)\n```bash\npython mcp_server/server.py\n```\n\n### Docker Mode (All Services)\n```bash\ndocker-compose up -d\n```\n\nTo check logs:\n```bash\ndocker-compose logs -f teams_mcp  # Teams MCP server logs\ndocker-compose logs -f ir_server  # IR server logs\n```\n\n### Demo Mode (no real Teams API calls)\nSet `DEMO_MODE=true` in your `.env` and run as above.\n\n## CLI Usage\n\n### 1. Login and Token Management\n```bash\npython cli/login.py login\npython cli/login.py status\npython cli/login.py logout\n```\n\n### 2. Rich CLI MCP Client\nAll commands below use the MCP stdio protocol to talk to the server.\n\n#### List chats\n```bash\npython cli/mcp_client.py list_chats\n```\n\n#### Get messages from a chat\n```bash\npython cli/mcp_client.py get_messages <chat_id>\n```\n\n#### Send a message\n```bash\npython cli/mcp_client.py send_message <chat_id> \"Hello from CLI!\"\n```\n\n#### Create a new 1:1 chat\n```bash\npython cli/mcp_client.py create_chat <user_id_or_email>\n```\n\n#### Search messages (hybrid, BM25, or vector)\n```bash\npython cli/mcp_client.py search_messages \"project update\" --mode hybrid --top_k 5\n```\n\n#### Stream new incoming messages (live event subscription)\n```bash\npython cli/mcp_client.py stream\n```\n\n## IR Server Usage\n\nThe IR server provides advanced search capabilities with PostgreSQL and pgvector. It exposes an HTTP API for MCP server communication.\n\n### IR Server API Endpoints\n\n#### 1. Health Check\n```\nGET http://localhost:8090/\n```\n\n#### 2. List Available Tools\n```\nGET http://localhost:8090/api/tools\n```\n\n#### 3. Search Content\n```\nPOST http://localhost:8090/api/tools/search\n```\nBody:\n```json\n{\n  \"query\": \"your search query\",\n  \"search_type\": \"hybrid\",\n  \"limit\": 10\n}\n```\n\n#### 4. Index Content\n```\nPOST http://localhost:8090/api/tools/index_content\n```\nBody:\n```json\n{\n  \"content\": \"Text content to index\",\n  \"source_type\": \"teams\",\n  \"metadata\": {\n    \"author\": \"User Name\",\n    \"created\": \"2025-04-01T12:00:00Z\"\n  }\n}\n```\n\nFor more detailed IR server documentation, see [ir/README.md](ir/README.md).\n\n## Search and Event Streaming\n- **Hybrid search**: Combines BM25 and vector search with LLM reranking\n- **Live streaming**: Subscribe to `messages/incoming` for real-time updates\n\n## Development & Extension\n- Add new MCP tools/resources in `mcp_server/server.py`\n- Extend Teams integration in `teams/graph.py`\n- Modify IR capabilities in the IR server\n- Add analytics, summarization, or RAG features using DuckDB, PostgreSQL, and LLMs\n- Use the CLI as a test harness for all MCP features\n\n## Troubleshooting & FAQ\n- **Login fails**: Check your Azure AD credentials and `.env` values\n- **No messages appear**: Ensure polling is running and your bot account is in the Teams chat\n- **DuckDB errors**: Check file permissions and paths in `.env`\n- **IR server not responding**: Check Docker logs and ensure the container is running\n- **Demo mode**: Set `DEMO_MODE=true` for local testing without real Teams\n\n## References\n- [Beyond Quacking: Deep Integration of Language Models and RAG into DuckDB (FlockMTL)](https://arxiv.org/html/2504.01157v1)\n- [Model Context Protocol documentation](https://modelcontextprotocol.io)\n- [Microsoft Graph API docs](https://learn.microsoft.com/en-us/graph/overview)\n- [PostgreSQL with pgvector extension](https://github.com/pgvector/pgvector)\n\n---\nFor full product details, see [`specs/app-spec.md`](specs/app-spec.md).",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "mcp",
        "chat",
        "realtime",
        "microsoft teams",
        "teams chat",
        "realtime collaboration"
      ],
      "category": "realtime-collaboration"
    },
    "agree-able--room-mcp": {
      "owner": "agree-able",
      "name": "room-mcp",
      "url": "https://github.com/agree-able/room-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/agree-able.webp",
      "description": "Connects and interacts with virtual rooms using the Room protocol, enabling agents to collaborate in a peer-to-peer environment for various tasks.",
      "stars": 16,
      "forks": 7,
      "license": "Apache License 2.0",
      "language": "JavaScript",
      "updated_at": "2025-09-03T16:37:58Z",
      "readme_content": "# Room MCP\n\n[![smithery badge](https://smithery.ai/badge/@agree-able/room-mcp)](https://smithery.ai/server/@agree-able/room-mcp)\n\nA command-line tool for using MCP (Model Context Protocol) with the Room protocol.\n\nThis allows claude to create virutal rooms in a p2p space with other agents to accomplish a goal.\n\n<a href=\"https://glama.ai/mcp/servers/p6xyqb1e9e\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/p6xyqb1e9e/badge\" alt=\"Room MCP server\" />\n</a>\n\nHere is claude hosting a room, and giving out the invite code for the other party to join.\n\n<p align=\"center\">\n  \n</p>\n\nHere is an example of connecting to a room for [20 Questions](https://github.com/agree-able/20-questions-bot)\n\n<p align=\"center\">\n  \n</p>\n\nWe've also adding in directives to help the agent balance goals and risk in performing its task.\n\n<p align=\"center\">\n  \n</p>\n\nYou should check out the other [exciting examples](docs/examples.md)\n\n\n## Installation\n\n### Installing via Smithery\n\nTo install Room MCP for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@agree-able/room-mcp):\n\n```bash\nnpx -y @smithery/cli install @agree-able/room-mcp --client claude\n```\n\n### Manual Installation\nYou can use this tool directly with npm:\n\n```bash\nnpm -y @agree-able/room-mcp\n```\n## Adding to Claude Desktop\n\nSee https://modelcontextprotocol.io/quickstart/user for more details.\n\nAdd the following to your claude_desktop_config.json:\n\n```\n{\n  \"mcpServers\": {\n    \"room\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"@agree-able/room-mcp\"\n      ],\n      \"env\": {\n        \"ROOM_TRANSCRIPTS_FOLDER\": \"/path/to/transcripts\" // Optional: Set to save room transcripts\n      }\n    }\n  }\n}\n```\n\n### Environment Variables\n\n- `ROOM_TRANSCRIPTS_FOLDER`: When set, conversation transcripts will be saved as JSON files in this folder when a room is exited. If the folder doesn't exist, it will be created automatically.\n\n## Available Tools\n\nThe Room MCP package provides the following capabilities:\n\n- **Room Protocol Integration**: Connect to and interact with rooms using the Room protocol\n- **MCP Support**: Utilize Model Context Protocol for enhanced model interactions\n- **Invitation Management**: Create and manage invitations using the @agree-able/invite package\n- **Transcript Storage**: Save conversation transcripts to disk when `ROOM_TRANSCRIPTS_FOLDER` environment variable is set\n\n## Related Packages\n\nThis tool depends on:\n\n- [@agree-able/invite](https://github.com/agree-able/invite): For invitation management\n- [@agree-able/room](https://github.com/agree-able/room): For Room protocol implementation\n- [@modelcontextprotocol/sdk](https://github.com/modelcontextprotocol/sdk): For MCP functionality\n\n## License\n\nApache License\nVersion 2.0",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "rooms",
        "collaboration",
        "realtime",
        "room protocol",
        "virtual rooms",
        "realtime collaboration"
      ],
      "category": "realtime-collaboration"
    },
    "ai-mcp-garage--agent_construct": {
      "owner": "ai-mcp-garage",
      "name": "agent_construct",
      "url": "https://github.com/ai-mcp-garage/agent_construct",
      "imageUrl": "/freedevtools/mcp/pfp/ai-mcp-garage.webp",
      "description": "Standardizes access to tools and data for AI applications, facilitating dynamic tool discovery and execution through a unified interface. It serves as a central hub for managing context and tool integration in AI models.",
      "stars": 13,
      "forks": 4,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-13T05:20:39Z",
      "readme_content": "# Agent Construct\n\n<p align=\"center\">\n  \n</p>\n\n> \"We can load anything, from clothing to equipment, weapons, training simulations, anything we need.\" - The Matrix (1999)\n\nAgent Construct is a Model Context Protocol (MCP) server implementation that standardizes how AI applications access tools and context. Just as the Construct in The Matrix provided operators with instant access to any equipment they needed, Agent Construct provides a standardized interface for AI models to access tools and data through the MCP specification.\n\nBuilt on the [Model Context Protocol](https://modelcontextprotocol.io/introduction) specification, it acts as a central hub that manages tool discovery, execution, and context management for AI applications. It provides a robust and scalable way to expose capabilities to AI models through a standardized protocol. It also provides a simplified configuration and tool structure to make adding new capabilities a breeze! An example tool for searching the web with Gemini is included.\n\n## Core Features\n\n### MCP Protocol Implementation\n- **Full MCP Compliance**: Complete implementation of the Model Context Protocol specification\n- **Tool Discovery**: Dynamic tool registration and discovery mechanism\n- **Standardized Communication**: Implements MCP's communication patterns for tool interaction\n\n### Server Architecture\n- **FastAPI Backend**: High-performance asynchronous server implementation\n- **Event Streaming**: Real-time updates via Server-Sent Events (SSE)\n- **Modular Design**: Clean separation between core protocol handling and tool implementations\n- **Handler System**: Extensible request handler architecture for different MCP operations\n- **Tool-Based Rate Limiting**: Let the server handle your configurable per-tool rate limiting.\n\n### Development Features\n- **Tool Decorator System**: Simple way to expose new tools via MCP\n- **Logging & Monitoring**: Comprehensive logging system for debugging and monitoring\n- **Configuration Management**: Environment-based configuration with secure defaults\n- **Testing Framework**: Extensive test suite for protocol compliance\n- **Agent Framework Friendly**: Included implementation examples for custom clients or frameworks like smolagents.\n\n## Getting Started\n\n### Prerequisites\n\n- Python 3.8 or higher\n- pip package manager\n\n### Installation\n\n1. Clone the repository:\n   ```bash\n   git clone https://github.com/yourusername/agent-construct.git\n   cd agent-construct\n   ```\n\n2. Install dependencies:\n   ```bash\n   pip install -r requirements.txt\n   ```\n\n3. Set up environment variables:\n   Create a `.env` file in the root directory with the following variables:\n   ```\n   # Server Configuration\n   SERVER_HOST=localhost\n   SERVER_PORT=8000\n   \n   # MCP Protocol Settings\n   MCP_VERSION=1.0\n   TOOL_DISCOVERY_ENABLED=true\n   \n   # Security Settings\n   ENABLE_AUTH=false  # Enable for production\n   ```\n\n4. Run the server:\n   ```bash\n   python -m mcp_server\n   ```\n\n## Core Architecture\n\n```\nmcp_server/\n├── core/               # Core MCP protocol implementation\n│   ├── server.py      # Main server implementation\n│   ├── protocol.py    # MCP protocol handlers\n│   └── context.py     # Context management\n├── handlers/          # MCP operation handlers\n│   ├── discovery.py   # Tool discovery\n│   ├── execution.py   # Tool execution\n│   └── context.py     # Context operations\n├── utils/            # Utility functions\n│   ├── logging.py    # Logging configuration\n│   ├── security.py   # Security utilities\n│   └── config.py     # Configuration management\n└── __main__.py       # Server entry point\n```\n\n## MCP Protocol Features\n\n### Tool Discovery\n- Dynamic tool registration system\n- Tool capability advertisement\n- Version management\n- Tool metadata and documentation\n\n### Context Management\n- Efficient context storage and retrieval\n- Context scoping and isolation\n- Real-time context updates\n- Context persistence options\n\n### Communication Patterns\n- Synchronous request/response\n- Server-sent events for updates\n- Streaming responses\n- Error handling and recovery\n\n## Future Enhancements\n\n### Protocol Extensions\n- [ ] Advanced context management features\n- [ ] Custom protocol extensions\n- [ ] Plugin system for protocol handlers\n\n### Security\n- [ ] Authentication and authorization\n- [ ] Tool access control\n- [-] Rate limiting and quota management\n- [ ] Audit logging\n- [ ] End-to-end encryption\n\n### Performance\n- [ ] Tool execution optimization\n- [ ] Context caching\n- [ ] Load balancing\n- [ ] Request queuing\n- [ ] Resource management\n\n### Development\n- [ ] Interactive protocol explorer\n- [ ] Tool development SDK\n- [ ] Protocol compliance testing tools\n- [ ] Performance monitoring dashboard\n\n## Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request. For major changes, please open an issue first to discuss what you would like to change.\n\n## License\n\nThis project is licensed under the MIT License - see the LICENSE file for details.\n\n## Acknowledgements\n\n- [Model Context Protocol](https://modelcontextprotocol.io/) for the protocol specification\n- FastAPI for the excellent web framework\n- The open-source community for various tools and libraries used in this project",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "ai",
        "tool",
        "tools",
        "garage agent_construct",
        "collaboration ai",
        "tool discovery"
      ],
      "category": "realtime-collaboration"
    },
    "alexandertsai--mcp-telegram": {
      "owner": "alexandertsai",
      "name": "mcp-telegram",
      "url": "https://github.com/alexandertsai/mcp-telegram",
      "imageUrl": "/freedevtools/mcp/pfp/alexandertsai.webp",
      "description": "Access Telegram accounts to read and send messages, manage chats, and respond to conversations. Retrieve chat lists and mark messages as read for efficient communication management.",
      "stars": 6,
      "forks": 1,
      "license": "Apache License 2.0",
      "language": "Python",
      "updated_at": "2025-07-20T19:18:38Z",
      "readme_content": "# Telegram MCP Server\n\nConnect Claude to your Telegram account to read and send messages.\n\n## Features\n\n### Available Tools\n\n1. **get_chats** - List your Telegram chats\n   - Returns paginated list with chat names, IDs, and unread counts\n   - For page 1, just provide page number\n   - For subsequent pages, use the pagination parameters from the previous response\n\n2. **get_messages** - Read messages from a specific chat\n   - Fetches paginated message history\n   - Automatically marks messages as read\n\n3. **mark_messages_read** - Mark all unread messages in a chat as read\n\n4. **send_message** - Send messages to any chat\n   - Supports replying to specific messages\n\n5. **get_conversation_context** - Analyze chat style for natural responses\n   - Reads your conversation style guide from `convostyle.txt`\n   - Helps Claude match your texting patterns\n\n## Setup Guide\n\n### Step 1: Get Telegram API Credentials\n\n1. Go to [https://my.telegram.org/apps](https://my.telegram.org/apps)\n2. Log in and create an application\n3. Save your **API ID** and **API Hash**\n\n### Step 2: Install\n\n```bash\n# Clone the repository\ngit clone https://github.com/alexandertsai/mcp-telegram\ncd mcp-telegram\n\n# Set up Python environment\npip install uv\nuv venv\nsource .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\nuv sync\n```\n\n### Step 3: Configure\n\n```bash\n# Copy the example file\ncp .env.example .env\n\n# Edit .env and add your API credentials:\n# TELEGRAM_API_ID=your_api_id_here\n# TELEGRAM_API_HASH=your_api_hash_here\n```\n\n### Step 4: Authenticate\n\n```bash\ncd src/mcp_telegram\npython telethon_auth.py\n```\n\nFollow the prompts:\n- Enter your phone number (with country code, e.g., +1234567890)\n- Enter the code sent to your Telegram\n- Enter your 2FA password if you have one\n\n### Step 5: Add to Claude Desktop\n\nFind your Claude Desktop config file:\n- **macOS**: `~/Library/Application Support/Claude/claude_desktop_config.json`  \n- **Windows**: `%APPDATA%\\Claude\\claude_desktop_config.json`\n\nAdd this configuration:\n\n```json\n{\n  \"mcpServers\": {\n    \"telegram\": {\n      \"command\": \"/path/to/python\",\n      \"args\": [\"/path/to/mcp-telegram/src/mcp_telegram/main.py\"]\n    }\n  }\n}\n```\n\n**To find paths:**\n- Python: Run `which python` (Mac) or `where.exe python` (Windows)\n- main.py: Right-click the file and select \"Copy Path\"\n\nRestart Claude Desktop.\n\n## Usage\n\nAfter setup, you can ask Claude to:\n- \"Check my Telegram messages\"\n- \"Send a message to [contact name]\"\n- \"What are my unread chats?\"\n- \"Reply to the last message from [contact name]\"\n\n## Style Guide (Optional)\n\nCreate `src/mcp_telegram/convostyle.txt` to help Claude match your texting style:\n\n```\nI text casually with friends, formally with work contacts.\nI use emojis sparingly and prefer short messages.\n```\n\n## Troubleshooting\n\n### Authentication Issues\n\nIf authentication fails:\n1. Check your API credentials in `.env`\n2. Remove the TELEGRAM_SESSION_STRING line from `.env`\n3. Run `python telethon_auth.py` again\n\n### Common Errors\n\n- **\"Please set TELEGRAM_API_ID and TELEGRAM_API_HASH\"**: Missing `.env` file or credentials\n- **\"Session string is invalid or expired\"**: Re-run authentication\n- **2FA password not showing**: This is normal - keep typing\n\n## Requirements\n\n- Python 3.10+\n- Claude Desktop\n- Telegram account\n\n## License\n\nApache 2.0",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "telegram",
        "chat",
        "chats",
        "mcp telegram",
        "telegram access",
        "access telegram"
      ],
      "category": "realtime-collaboration"
    },
    "alifakih1--discord-mcp": {
      "owner": "alifakih1",
      "name": "discord-mcp",
      "url": "https://github.com/alifakih1/discord-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/alifakih1.webp",
      "description": "Integrate Discord bot functionalities with MCP-compatible applications to manage servers, channels, messages, reactions, categories, and webhooks. Utilize the Discord API capabilities in a standardized way to enhance application interactions.",
      "stars": 0,
      "forks": 0,
      "license": "MIT License",
      "language": "",
      "updated_at": "2025-04-24T07:57:11Z",
      "readme_content": "<div align=\"center\">\n  \n</div>\n<hr>\n<div align=\"center\" style=\"line-height: 1;\">\n    <a href=\"https://smithery.ai/server/@SaseQ/discord-mcp\" target=\"_blank\" style=\"margin: 2px;\">\n        <img alt=\"Smithery Badge\" src=\"https://camo.githubusercontent.com/ee5c6c6dc502821f4d57313b2885f7878af52be14142dd98526ea12aedf9b260/68747470733a2f2f736d6974686572792e61692f62616467652f40646d6f6e74676f6d65727934302f646565707365656b2d6d63702d736572766572\" data-canonical-src=\"https://smithery.ai/server/@SaseQ/discord-mcp\" style=\"display: inline-block; vertical-align: middle;\"/>\n    </a>\n    <a href=\"https://badge.mcpx.dev?type=server\" target=\"_blank\" style=\"margin: 2px;\">\n        <img alt=\"MCP Server\" src=\"https://badge.mcpx.dev?type=server\" style=\"display: inline-block; vertical-align: middle;\"/>\n    </a>\n    <a href=\"https://discord.gg/5Uvxe5jteM\" target=\"_blank\" style=\"margin: 2px;\">\n        <img alt=\"Discord\" src=\"https://img.shields.io/badge/Discord-SaseQcode-7289da?logo=discord&logoColor=white&color=7289da\" style=\"display: inline-block; vertical-align: middle;\"/>\n    </a>\n</div>\n\n\n## 📖 Description\n\nA [Model Context Protocol (MCP)](https://modelcontextprotocol.io/introduction) server for the Discord API [(JDA)](https://jda.wiki/), \nallowing seamless integration of Discord Bot with MCP-compatible applications like Claude Desktop.\n\n\n## 🔬 Installation\n\n#### Clone the repository\n```\ngit clone https://github.com/SaseQ/discord-mcp\n```\n\n#### Build the project\n```\ncd discord-mcp\nmvn clean package\n```\n\n#### Configure Claude Desktop\n```\n{\n  \"mcpServers\": {\n    \"discord-mcp\": {\n      \"command\": \"java\",\n      \"args\": [\n        \"-jar\",\n        \"/absolute/path/to/discord-mcp-0.0.1-SNAPSHOT.jar\"\n      ],\n      \"env\": {\n        \"DISCORD_TOKEN\": \"YOUR_DISCORD_BOT_TOKEN\"\n      }\n    }\n  }\n}\n```\n\n*To get a discord bot token, visit the [Discord Developer Portal](https://discord.com/developers)\n\n\n## ⚓ Smithery\n\nInstall Discord MCP Server automatically via Smithery:\n```\nnpx -y @smithery/cli@latest install @SaseQ/discord-mcp --client claude\n```\n\n\n## 🛠️ Available Tools\n\n#### Server Information\n - [`get_server_info`](): Get detailed discord server information\n\n#### Message Management\n - [`send_message`](): Send a message to a specific channel\n - [`edit_message`](): Edit a message from a specific channel\n - [`delete_message`](): Delete a message from a specific channel\n - [`read_messages`](): Read recent message history from a specific channel\n - [`send_private_message`](): Send a private message to a specific user\n - [`edit_private_message`](): Edit a private message from a specific user\n - [`delete_private_message`](): Delete a private message from a specific user\n - [`read_private_messages`](): Read recent message history from a specific user\n - [`add_reaction`](): Add a reaction (emoji) to a specific message\n - [`remove_reaction`](): Remove a specified reaction (emoji) from a message\n\n#### Channel Management\n - [`delete_channel`](): Delete a channel\n - [`find_channel`](): Find a channel type and ID using name and server ID\n - [`list_channels`](): List of all channels\n\n#### Category Management\n - [`create_category`](): Create a new category for channels\n - [`delete_category`](): Delete a category\n - [`find_category`](): Find a category ID using name and server ID\n - [`list_channels_in_category`](): List of channels in a specific category\n\n#### Webhook Management\n - [`create_webhook`](): Create a new webhook on a specific channel\n - [`delete_webhook`](): Delete a webhook\n - [`list_webhooks`](): List of webhooks on a specific channel\n - [`send_webhook_message`](): Send a message via webhook\n\n\n<hr>\n\nA more detailed examples can be found in the [Wiki](https://github.com/SaseQ/discord-mcp/wiki).",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "discord",
        "webhooks",
        "mcp",
        "discord mcp",
        "discord api",
        "discord bot"
      ],
      "category": "realtime-collaboration"
    },
    "arjunkmrm--mcp-minecraft": {
      "owner": "arjunkmrm",
      "name": "mcp-minecraft",
      "url": "https://github.com/arjunkmrm/mcp-minecraft",
      "imageUrl": "/freedevtools/mcp/pfp/arjunkmrm.webp",
      "description": "Integration with Minecraft enabling AI assistants to observe and interact with the Minecraft world through a bot. Supports interaction through the Model Context Protocol for enhanced functionality within the game.",
      "stars": 88,
      "forks": 8,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-10-03T01:20:49Z",
      "readme_content": "# Minecraft MCP Integration\n\nA Model Context Protocol (MCP) integration for Minecraft that enables AI assistants to interact with a Minecraft server. This integration allows AI models to observe and interact with the Minecraft world through a bot.\n\n\n\n## Prerequisites\n\n1. Minecraft Launcher\n2. Node.js 18 or higher\n3. Claude Desktop App\n4. Java 21.0.5 (recommended)\n\n> ⚠️ Note: Currently only tested on macOS/Linux. Windows compatibility is not guaranteed.\n\n## Important Note\n\n1. **Use the F3+P Shortcut**:\nPress F3 + P together. This toggles the \"Pause on Lost Focus\" feature. Once turned off, you can switch to claude desktop and Minecraft will continue running without pausing.\n\n\n\n2. **Connection Issues on Claude Restart**:\nIf you restart Claude while the Minecraft server is running, you may experience MCP connection issues on the next claude launch due to lingering java process. See [Troubleshooting: MCP Connection Failed](#common-issues) for resolution steps.\n\n## Installation Steps\n\n1. **Download and Setup Minecraft Server**\n   - Download Minecraft server v1.21 from [mcversions.net/1.21](https://mcversions.net/download/1.21)\n   - Install Java 21.0.5 if not already installed (other versions are untested)\n   - Create a dedicated directory (e.g., `~/minecraft-server/`)\n   - Place the downloaded `server.jar` file in this directory\n   - Note down the absolute path to your `server.jar` file\n\n2. **Install and Configure MCP Integration**\n   \n   Quick Install (Recommended):\n   ```bash\n   npx -y @smithery/cli install mcp-minecraft --client claude\n   ```\n   Follow the CLI prompts to complete the setup.\n\n   Or Manual Setup:\n   - Navigate to `~/Library/Application Support/Claude/claude_desktop_config.json`\n   - Add the MCP server configuration:   \n   ```json\n   {\n     \"mcpServers\": {\n       \"mcp-minecraft\": {\n         \"command\": \"npx\",\n         \"args\": [\n           \"-y\",\n           \"mcp-minecraft@latest\",\n           \"--server-jar\",\n           \"/absolute/path/to/minecraft-server/server.jar\"\n         ]\n       }\n     }\n   }   \n   ```\n   > ⚠️ Replace `/absolute/path/to/minecraft-server/server.jar` with your actual server.jar path\n\n4. **Launch Claude Desktop**\n   - Start Claude Desktop after completing the configuration\n\n5. **Connect to Server**\n   - Open Minecraft Launcher\n   - Install and launch Minecraft Java Edition **v1.21**\n   - Click \"Play\" and Select \"Multiplayer\"\n   - Click \"Add Server\"\n   - Enter server details:\n     - Server Name: `Minecraft Server`\n     - Server Address: `localhost:25565`\n   - Click \"Done\"\n\n## Features\n\n### Resources\nThe integration exposes these MCP resources:\n\n- `minecraft://bot/location` - Current bot position in the world\n- `minecraft://bot/status` - Bot connection status\n\n### Tools\nAvailable MCP tools:\n\n- `chat` - Send chat messages to the server\n- `jump` - Make the bot jump\n- `moveForward` - Make the bot move forward\n- `moveBack` - Make the bot move backward\n- `turnLeft` - Make the bot turn left\n- `turnRight` - Make the bot turn right\n- `placeBlock` - Place a block at specified coordinates\n- `digBlock` - Break a block at specified coordinates\n- `getBlockInfo` - Get information about a block at specified coordinates\n- `selectSlot` - Select a hotbar slot (0-8)\n- `getInventory` - Get contents of bot's inventory\n- `equipItem` - Equip an item by name to specified destination\n- `getStatus` - Get bot's current status (health, food, position, etc.)\n- `getNearbyEntities` - Get list of nearby entities within range\n- `attack` - Attack a nearby entity by name\n- `useItem` - Use/activate the currently held item\n- `stopUsingItem` - Stop using/deactivate the current item\n- `lookAt` - Make the bot look at specific coordinates\n- `followPlayer` - Follow a specific player\n- `stopFollowing` - Stop following current target\n- `goToPosition` - Navigate to specific coordinates\n\n## Technical Details\n\n- Server runs in offline mode for local development\n- Default memory allocation: 2GB\n- Default port: 25565\n- Bot username: MCPBot\n\n## Troubleshooting\n\n### Common Issues\n\n1. **MCP Connection Failed**\n   - Look for lingering Java processes\n   - Terminate them manually:\n      - Windows: Use Task Manager (untested)\n      - Mac/Linux: \n         - Go to 'Activity Monitor' and 'Force Quit' java\n   - Restart computer if process termination fails\n   - Note: Latest version should auto-resolve these issues\n\n2. **Server Won't Start**\n   - Verify Java is installed\n   - Check server.jar path is correct\n   - Ensure port 25565 is available\n\n3. **Can't Connect to Server**\n   - Verify server is running (check logs)\n   - Confirm you're using \"localhost\" as server address\n   - Check firewall settings\n\n### Logs Location\n- Minecraft Server logs: Check the minecraft-server directory\n- Claude Desktop logs: `~/Library/Logs/Claude/mcp*.log`\n\n## Contributing\n\nContributions, big or small, are welcome!\n\n## License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "minecraft",
        "realtime",
        "interact",
        "interact minecraft",
        "mcp minecraft",
        "minecraft integration"
      ],
      "category": "realtime-collaboration"
    },
    "atom2ueki--mcp-server-ios-simulator": {
      "owner": "atom2ueki",
      "name": "mcp-server-ios-simulator",
      "url": "https://github.com/atom2ueki/mcp-server-ios-simulator",
      "imageUrl": "/freedevtools/mcp/pfp/atom2ueki.webp",
      "description": "Programmatically control and manage iOS simulators, enabling functions such as starting, stopping, booting, and interacting with simulator instances. It supports app installation, launching, and screenshot capture through a standardized communication protocol.",
      "stars": 32,
      "forks": 5,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-09-13T08:51:58Z",
      "readme_content": "# 📱 MCP Server for iOS Simulator\n[![smithery badge](https://smithery.ai/badge/@atom2ueki/mcp-server-ios-simulator)](https://smithery.ai/server/@atom2ueki/mcp-server-ios-simulator)\n\nA server that implements the Model Context Protocol (MCP) for iOS simulators, built on top of [appium-ios-simulator](https://github.com/appium/appium-ios-simulator) and utilizing the [MCP TypeScript SDK](https://github.com/modelcontextprotocol/typescript-sdk).\n\n<a href=\"https://glama.ai/mcp/servers/@atom2ueki/mcp-server-ios-simulator\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@atom2ueki/mcp-server-ios-simulator/badge\" alt=\"Server for iOS Simulator MCP server\" />\n</a>\n\n## 📋 Overview\n\nThis project provides a bridge between iOS simulators and the Model Context Protocol, allowing for standardized communication with iOS simulator instances. It enables programmatic control of iOS simulators while leveraging the MCP protocol for consistent interfaces across different environments. The server utilizes stdio as its transport mechanism, making it ideal for integration with Claude Desktop and other MCP-compatible clients.\n\n## 🎬 Demo\n\n\n\n*Demo showing how to boot an iOS simulator using Claude AI Desktop*\n\n## 🏗️ Architecture\n\nThe server consists of three main components:\n\n1. **🔄 Simulator Management Layer** - Handles iOS simulator lifecycle and interactions\n2. **🔌 MCP Protocol Implementation** - Implements the Model Context Protocol using the TypeScript SDK with stdio transport\n3. **📊 Logger Component** - Provides file-based logging without interfering with the stdio transport\n\n```\n┌─────────────────┐     ┌─────────────────┐     ┌─────────────────┐\n│  MCP Protocol   │     │     Stdio       │     │    Simulator    │\n│  Implementation │◄────┤    Transport    │◄────┤   Management    │\n│                 │     │                 │     │      Layer      │\n└─────────────────┘     └─────────────────┘     └─────────────────┘\n        ▲                                                ▲\n        │                                                │\n        ▼                                                ▼\n┌─────────────────┐                             ┌─────────────────┐\n│   MCP Client    │                             │  iOS Simulator  │\n│  (e.g. Claude)  │                             │                 │\n└─────────────────┘                             └─────────────────┘\n```\n\n## ✨ Features\n\n- 🚀 Start, stop, and manage iOS simulator instances\n- 🔌 Boot and shutdown simulators\n- 📲 Install and launch applications on simulators\n- 📸 Take screenshots of simulator screens\n- 👆 Perform taps on coordinates\n- 🔄 Support for multiple concurrent simulator sessions\n- 📝 Comprehensive file-based logging without console output\n- 🛡️ Error-resilient operation\n\n## 📋 Prerequisites\n\n- 🟢 Node.js (v16 or later)\n- 🍎 macOS (required for iOS simulators)\n- 🛠️ Xcode with iOS simulators installed\n- 📜 TypeScript 4.5+\n\n## 🔧 Installation\n\n### Installing via Smithery\n\nTo install iOS Simulator Control Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@atom2ueki/mcp-server-ios-simulator):\n\n```bash\nnpx -y @smithery/cli install @atom2ueki/mcp-server-ios-simulator --client claude\n```\n\n### Manual Installation\n```bash\n# Clone the repository\ngit clone https://github.com/atom2ueki/mcp-server-ios-simulator.git\ncd mcp-server-ios-simulator\n\n# Install dependencies\nnpm install\n```\n\n## ⚙️ Configuration\n\nConfiguration is handled through the `src/config.ts` file:\n\n```typescript\nconst config = {\n  simulator: {\n    defaultDevice: process.env.SIMULATOR_DEFAULT_DEVICE || 'iPhone 16',\n    defaultOS: process.env.SIMULATOR_DEFAULT_OS || '18.2',\n    timeout: parseInt(process.env.SIMULATOR_TIMEOUT || '30000', 10),\n  }\n};\n```\n\nYou can customize these settings by setting environment variables:\n\n```\nSIMULATOR_DEFAULT_DEVICE=iPhone 16\nSIMULATOR_DEFAULT_OS=18.2\nSIMULATOR_TIMEOUT=30000\n```\n\n## 🚀 Usage\n\n### 🔨 Building and Starting the Server\n\n```bash\n# Build the project\nnpm run build\n\n# Start the server\nnpm start\n```\n\n### 🧰 MCP Tools\n\nThe server provides two distinct approaches for controlling iOS simulators:\n\n#### 📱 Direct Simulator Management (Recommended)\nThese tools work directly with simulator UDIDs and don't require maintaining sessions:\n\n- 📋 `list-available-simulators` - List all available simulators with their UDIDs\n- ▶️ `boot-simulator-by-udid` - Boot a simulator directly using its UDID\n- ⏹️ `shutdown-simulator-by-udid` - Shutdown a simulator directly using its UDID\n- 📊 `list-booted-simulators` - List all currently booted simulators\n\n**Use this approach when:** You just want to boot, use, and shut down simulators directly.\n\n#### 📱 Session-Based Management (Advanced)\nThese tools use a session layer that tracks simulators with custom session IDs:\n\n- 📋 `list-simulator-sessions` - List all active simulator sessions\n- ➕ `create-simulator-session` - Create a new simulator session\n- ❌ `terminate-simulator-session` - Terminate a session (shuts down simulator and cleans up)\n- 🔄 `create-and-boot-simulator` - Create a new simulator session and boot it\n- ▶️ `boot-simulator` - Boot a simulator for an existing session\n- ⏹️ `shutdown-simulator` - Shutdown a simulator for an existing session\n\n**Use this approach when:** You need to track simulator metadata, reference simulators by custom IDs, or use the more advanced management features.\n\n#### 📲 Application Management\n- 📥 `install-app` - Install an application on a simulator\n- 🚀 `launch-app` - Launch an application on a simulator\n- 🛑 `terminate-app` - Terminate a running application on a simulator\n\n#### 🖱️ Interaction Tools\n- 📷 `take-screenshot` - Take a screenshot of the simulator screen\n- 👆 `tap-coordinate` - Perform a tap at the specified coordinates\n\n### 🤖 Example Usage with Claude Desktop\n\n1. Configure Claude Desktop to use this server as an MCP tool:\n   - Open Claude Desktop\n   - Go to Settings > Advanced\n   - Add the following configuration to the \"MCP Servers\" section:\n\n   ```json\n   {\n     \"mcpServers\": {\n       \"simulator\": {\n         \"command\": \"node\",\n         \"args\": [\n           \"/path/to/your/mcp-server-ios-simulator/dist/index.js\"\n         ]\n       }\n     }\n   }\n   ```\n\n   - Replace `/path/to/your` with the actual path to where you've installed this repository\n   - Save the settings and restart Claude Desktop\n\n2. Use the provided tools to control iOS simulators directly from Claude Desktop:\n   \n   **Direct UDID Approach (Recommended):**\n   1. First, ask Claude to list available simulators:\n      ```\n      \"Show me all available iOS simulators\"\n      ```\n   \n   2. Then use the UDID to boot a specific simulator:\n      ```\n      \"Boot the iOS simulator with UDID 5272EA61-5796-4372-86FE-3B33831D5CC1\"\n      ```\n   \n   3. When finished, shut it down using the same UDID:\n      ```\n      \"Shut down the simulator with UDID 5272EA61-5796-4372-86FE-3B33831D5CC1\"\n      ```\n   \n   The direct UDID approach is simpler and more reliable for most use cases.\n   \n   **Session-Based Approach (Advanced):**\n   Only use this approach if you need the advanced features of session tracking:\n   ```\n   \"Create a new simulator session for iPhone 16 Pro with iOS 18.2\"\n   \"Boot the simulator for session abc-123\"\n   \"Take a screenshot of the simulator for session abc-123\"\n   \"Terminate the simulator session abc-123\"\n   ```\n\n## 👨‍💻 Development\n\n### 📁 Project Structure\n\n```\nsrc/\n├── simulator/       # Simulator management layer\n├── mcp/             # MCP protocol implementation\n├── bridge/          # Bridge component\n├── utils/           # Utility functions including logger\n├── config.ts        # Configuration handling\n└── index.ts         # Entry point\n```\n\n### 🔨 Building the Project\n\n```bash\n# Install development dependencies\nnpm install\n\n# Run TypeScript compiler\nnpm run build\n```\n\n## 📜 License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n\n## 🙏 Acknowledgments\n\n- 📱 [appium-ios-simulator](https://github.com/appium/appium-ios-simulator) for providing the iOS simulator interaction capabilities\n- 🔌 [Model Context Protocol](https://github.com/modelcontextprotocol/typescript-sdk) for the protocol specification and TypeScript SDK",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "simulator",
        "simulators",
        "ios",
        "ios simulators",
        "ios simulator",
        "simulator instances"
      ],
      "category": "realtime-collaboration"
    },
    "barryyip0625--mcp-discord": {
      "owner": "barryyip0625",
      "name": "mcp-discord",
      "url": "https://github.com/barryyip0625/mcp-discord",
      "imageUrl": "/freedevtools/mcp/pfp/barryyip0625.webp",
      "description": "Interact with Discord platform features, manage channels, send messages, and retrieve server information. Facilitate automation for Discord activities like reading and deleting channel messages.",
      "stars": 48,
      "forks": 22,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-10-02T06:08:15Z",
      "readme_content": "# MCP-Discord\n[![smithery badge](https://smithery.ai/badge/@barryyip0625/mcp-discord)](https://smithery.ai/server/@barryyip0625/mcp-discord) ![](https://badge.mcpx.dev?type=server 'MCP Server') [![Docker Hub](https://img.shields.io/docker/v/barryy625/mcp-discord?logo=docker&label=Docker%20Hub)](https://hub.docker.com/r/barryy625/mcp-discord)\n\nA Discord MCP (Model Context Protocol) server that enables AI assistants to interact with the Discord platform.\n\n<a href=\"https://glama.ai/mcp/servers/@barryyip0625/mcp-discord\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@barryyip0625/mcp-discord/badge\" alt=\"MCP-Discord MCP server\" />\n</a>\n\n## Overview\n\nMCP-Discord provides the following Discord-related functionalities:\n\n- Login to Discord bot\n- Get server information\n- Read/delete channel messages\n- Send messages to specified channels (using either channel IDs or channel names)\n- Retrieve forum channel lists\n- Create/delete/reply to forum posts\n- Create/delete text channels\n- Add/remove message reactions\n- Create/edit/delete/use webhooks\n\n## Table of Contents\n\n- [Prerequisites](#prerequisites)\n- [Installation](#installation)\n- [Configuration](#configuration)\n- [Tools Documentation](#tools-documentation)\n  - [Basic Functions](#basic-functions)\n  - [Channel Management](#channel-management)\n  - [Forum Functions](#forum-functions)\n  - [Messages and Reactions](#messages-and-reactions)\n  - [Webhook Management](#webhook-management)\n- [Development](#development)\n- [License](#license)\n\n## Prerequisites\n\n- Node.js (v16.0.0 or higher)\n- npm (v7.0.0 or higher)\n- A Discord bot with appropriate permissions\n  - Bot token (obtainable from the [Discord Developer Portal](https://discord.com/developers/applications))\n  - Message Content Intent enabled\n  - Server Members Intent enabled\n  - Presence Intent enabled\n- Permissions required in your Discord server:\n\n  #### Easiest Setup\n  - Administrator (Recommended for quick setup and full functionality)\n\n  #### Or, select only the required permissions:\n  - Send Messages\n  - Create Public Threads\n  - Send Messages in Threads\n  - Manage Messages\n  - Manage Threads\n  - Manage Channels\n  - Manage Webhooks\n  - Add Reactions\n  - View Channel\n\n- Add your Discord bot to your server\n  - To add your Discord bot to your server, use one of the following invite links (replace `INSERT_CLIENT_ID_HERE` with your bot's client ID):\n    - **Administrator (full access):**\n        https://discord.com/oauth2/authorize?client_id=INSERT_CLIENT_ID_HERE&scope=bot&permissions=8\n    - **Custom permissions (minimum required):**\n        https://discord.com/oauth2/authorize?client_id=INSERT_CLIENT_ID_HERE&scope=bot&permissions=52076489808\n\n> **Note:**  \n> According to Discord's security model, a bot can only access information from servers it has been explicitly added to.  \n> If you want to use this MCP server to access a specific Discord server, you must add the bot to that server first.  \n> Use the invite link below to add the bot to your target server.\n\n## Installation\n\n### Installing via NPM\n\nYou can use it with the following command:\n```bash\nnpx mcp-discord --config ${DISCORD_TOKEN}\n```\n\nFor more details, you can check out the [NPM Package](https://www.npmjs.com/package/mcp-discord).\n\n### Installing via Smithery\n\nTo install mcp-discord automatically via [Smithery](https://smithery.ai/server/@barryyip0625/mcp-discord)\n\n### Installing via Docker\n\nYou can run mcp-discord using Docker. The Docker images are automatically built and published to Docker Hub.\n\n**Docker Hub Repository**: [barryy625/mcp-discord](https://hub.docker.com/r/barryy625/mcp-discord)\n\n```bash\n# Pull the latest image\ndocker pull barryy625/mcp-discord:latest\n\n# Run with environment variable\ndocker run -e DISCORD_TOKEN=your_discord_bot_token -p 8080:8080 barryy625/mcp-discord:latest\n\n# Or run with command line config\ndocker run -p 8080:8080 barryy625/mcp-discord:latest --config \"your_discord_bot_token\"\n```\n\n**Available Tags:**\n- `latest` - Latest stable version from main branch\n- `v1.3.3`, etc. - Specific version releases\n\n### Manual Installation\n```bash\n# Clone the repository\ngit clone https://github.com/barryyip0625/mcp-discord.git\ncd mcp-discord\n\n# Install dependencies\nnpm install\n\n# Compile TypeScript\nnpm run build\n```\n\n## Configuration\n\nA Discord bot token is required for proper operation. The server supports two transport methods: stdio and streamable HTTP.\n\n### Transport Methods\n\n1. **stdio** (Default)\n   - Traditional stdio transport for basic usage\n   - Suitable for simple integrations\n\n2. **streamable HTTP**\n   - HTTP-based transport for more advanced scenarios\n   - Supports stateless operation\n   - Configurable port number\n\n### Configuration Options\n\nYou can provide configuration in two ways:\n\n1. Environment variables:\n```bash\nDISCORD_TOKEN=your_discord_bot_token\n```\n\n2. Using command line arguments:\n```bash\n# For stdio transport (default)\nnode build/index.js --config \"your_discord_bot_token\"\n\n# For streamable HTTP transport\nnode build/index.js --transport http --port 3000 --config \"your_discord_bot_token\"\n```\n\n## Usage with Claude/Cursor\n\n### Docker\n\nYou can use Docker containers with both Claude and Cursor:\n\n```json\n{\n    \"mcpServers\": {\n        \"discord\": {\n            \"command\": \"docker\",\n            \"args\": [\n                \"run\",\n                \"--rm\",\n                \"-e\",\n                \"DISCORD_TOKEN=your_discord_bot_token\",\n                \"-p\",\n                \"8080:8080\",\n                \"barryy625/mcp-discord:latest\",\n                \"--transport\",\n                \"http\",\n                \"--port\",\n                \"8080\"\n            ]\n        }\n    }\n}\n```\n\n### Claude\n\n1. Using stdio transport:\n```json\n{\n    \"mcpServers\": {\n        \"discord\": {\n            \"command\": \"node\",\n            \"args\": [\n                \"path/to/mcp-discord/build/index.js\",\n                \"--config\",\n                \"your_discord_bot_token\"\n            ]\n        }\n    }\n}\n```\n\n2. Using streamable HTTP transport:\n```json\n{\n    \"mcpServers\": {\n        \"discord\": {\n            \"command\": \"node\",\n            \"args\": [\n                \"path/to/mcp-discord/build/index.js\",\n                \"--transport\",\n                \"http\",\n                \"--port\",\n                \"3000\",\n                \"--config\",\n                \"your_discord_bot_token\"\n            ]\n        }\n    }\n}\n```\n\n### Cursor\n\n1. Using stdio transport:\n```json\n{\n    \"mcpServers\": {\n        \"discord\": {\n            \"command\": \"cmd\",\n            \"args\": [\n                \"/c\",\n                \"node\",\n                \"path/to/mcp-discord/build/index.js\",\n                \"--config\",\n                \"your_discord_bot_token\"\n            ]\n        }\n    }\n}\n```\n\n2. Using streamable HTTP transport:\n```json\n{\n    \"mcpServers\": {\n        \"discord\": {\n            \"command\": \"cmd\",\n            \"args\": [\n                \"/c\",\n                \"node\",\n                \"path/to/mcp-discord/build/index.js\",\n                \"--transport\",\n                \"http\",\n                \"--port\",\n                \"3000\",\n                \"--config\",\n                \"your_discord_bot_token\"\n            ]\n        }\n    }\n}\n```\n\n## Tools Documentation\n\n### Basic Functions\n\n- `discord_send`: Send a message to a specified channel (supports both channel ID and channel name)\n- `discord_get_server_info`: Get Discord server information\n\n### Channel Management\n\n- `discord_create_text_channel`: Create a text channel\n- `discord_delete_channel`: Delete a channel\n\n### Forum Functions\n\n- `discord_get_forum_channels`: Get a list of forum channels\n- `discord_create_forum_post`: Create a forum post\n- `discord_get_forum_post`: Get a forum post\n- `discord_reply_to_forum`: Reply to a forum post\n- `discord_delete_forum_post`: Delete a forum post\n\n### Messages and Reactions\n\n- `discord_read_messages`: Read channel messages\n- `discord_add_reaction`: Add a reaction to a message\n- `discord_add_multiple_reactions`: Add multiple reactions to a message\n- `discord_remove_reaction`: Remove a reaction from a message\n- `discord_delete_message`: Delete a specific message from a channel\n\n### Webhook Management\n\n- `discord_create_webhook`: Creates a new webhook for a Discord channel\n- `discord_send_webhook_message`: Sends a message to a Discord channel using a webhook\n- `discord_edit_webhook`: Edits an existing webhook for a Discord channel\n- `discord_delete_webhook`: Deletes an existing webhook for a Discord channel\n\n## Development\n\n```bash\n# Development mode\nnpm run dev\n```\n\n## License\n\n[MIT License](https://github.com/barryyip0625/mcp-discord?tab=MIT-1-ov-file)\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "discord",
        "mcp",
        "barryyip0625",
        "mcp discord",
        "automation discord",
        "interact discord"
      ],
      "category": "realtime-collaboration"
    },
    "block--vscode-mcp": {
      "owner": "block",
      "name": "vscode-mcp",
      "url": "https://github.com/block/vscode-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/block.webp",
      "description": "Facilitates interaction between AI agents and Visual Studio Code, enabling file modifications, project management, and extension status checks directly within the development environment.",
      "stars": 74,
      "forks": 13,
      "license": "Apache License 2.0",
      "language": "TypeScript",
      "updated_at": "2025-09-14T07:44:46Z",
      "readme_content": "# VSCode MCP\n\nThis monorepo contains the VSCode MCP Server and its companion VSCode Extension, which together enable AI agents and assistants, like Goose or Claude, to interact with VSCode through the Model Context Protocol.\n\n## Project Structure\n\n```\nvscode-mcp/\n├── server/    # MCP server implementation\n└── extension/ # VS Code extension\n```\n\n## Quick Start\n\n1. Install the MCP Server\n\n```bash\nnpx vscode-mcp-server install\n```\n\n2. Install the MCP Extension\n\n> [MCP Extension](https://marketplace.visualstudio.com/items?itemName=block.vscode-mcp-extension)\n\n## Configuration\n\n### Goose Desktop Setup\n\n\n\n- ID: `code-mcp`\n- Name: `VS Code`\n- Description: `Allows interaction with VS Code through the Model Context Protocol`\n- Command: `npx vscode-mcp-server`\n\n### Claude Desktop Setup\n\nAdd this to your Claude Desktop config file (`~/Library/Application Support/Claude/claude_desktop_config.json`):\n\n```json\n{\n  \"mcpServers\": {\n    \"vscode-mcp-server\": {\n      \"command\": \"npx\",\n      \"args\": [\"vscode-mcp-server\"],\n      \"env\": {}\n    }\n  }\n}\n```\n\n## Available Tools\n\nThe Code MCP server provides the following tools for AI agents to interact with VS Code:\n\n### `create_diff`\n\nCreates and shows a diff for modifying existing files:\n\n- Shows changes preview before applying\n- Requires user approval\n- Only works with existing files\n\n### `open_file`\n\nOpens files in the VS Code editor:\n\n- Used for viewing new or modified files\n\n### `open_project`\n\nOpens a project folder in VS Code:\n\n- Sets up working directory for AI agent\n\n### `check_extension_status`\n\nChecks if extension is installed and responding\n\n### `get_extension_port`\n\nGets the port number for VS Code MCP Extension\n\n### `list_available_projects`\n\nShows projects from port registry file\n\n## License\n\nThis project is licensed under the Apache License 2.0 - see the [LICENSE](LICENSE) file for details.\n\nCopyright 2025 Block, Inc.\n\nThis product includes software developed at [Block, Inc.](https://block.xyz/)",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "vscode",
        "studio",
        "realtime",
        "vscode mcp",
        "visual studio",
        "block vscode"
      ],
      "category": "realtime-collaboration"
    },
    "bsmi021--mcp-conversation-server": {
      "owner": "bsmi021",
      "name": "mcp-conversation-server",
      "url": "https://github.com/bsmi021/mcp-conversation-server",
      "imageUrl": "/freedevtools/mcp/pfp/bsmi021.webp",
      "description": "Manage and interact with multiple conversations using OpenRouter's language models through a standardized interface, facilitating message creation, real-time streaming, and persistent conversation states. Features include error handling and recovery, along with automatic token counting and model context management.",
      "stars": 2,
      "forks": 1,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-09-03T22:18:09Z",
      "readme_content": "# MCP Conversation Server\n\nA Model Context Protocol (MCP) server implementation for managing conversations with OpenRouter's language models. This server provides a standardized interface for applications to interact with various language models through a unified conversation management system.\n\n## Features\n\n- **MCP Protocol Support**\n  - Full MCP protocol compliance\n  - Resource management and discovery\n  - Tool-based interaction model\n  - Streaming response support\n  - Error handling and recovery\n\n- **OpenRouter Integration**\n  - Support for all OpenRouter models\n  - Real-time streaming responses\n  - Automatic token counting\n  - Model context window management\n  - Available models include:\n    - Claude 3 Opus\n    - Claude 3 Sonnet\n    - Llama 2 70B\n    - And many more from OpenRouter's catalog\n\n- **Conversation Management**\n  - Create and manage multiple conversations\n  - Support for system messages\n  - Message history tracking\n  - Token usage monitoring\n  - Conversation filtering and search\n\n- **Streaming Support**\n  - Real-time message streaming\n  - Chunked response handling\n  - Token counting\n\n- **File System Persistence**\n  - Conversation state persistence\n  - Configurable storage location\n  - Automatic state management\n\n## Installation\n\n```bash\nnpm install mcp-conversation-server\n```\n\n## Configuration\n\n### Configuration\n\nAll configuration for the MCP Conversation Server is now provided via YAML. Please update the `config/models.yaml` file with your settings. For example:\n\n```yaml\n# MCP Server Configuration\nopenRouter:\n  apiKey: \"YOUR_OPENROUTER_API_KEY\"  # Replace with your actual OpenRouter API key.\n\npersistence:\n  path: \"./conversations\"  # Directory for storing conversation data.\n\nmodels:\n  # Define your models here\n  'provider/model-name':\n    id: 'provider/model-name'\n    contextWindow: 123456\n    streaming: true\n    temperature: 0.7\n    description: 'Model description'\n\n# Default model to use if none specified\ndefaultModel: 'provider/model-name'\n```\n\n### Server Configuration\n\nThe MCP Conversation Server now loads all its configuration from the YAML file. In your application, you can load the configuration as follows:\n\n```typescript\nconst config = await loadModelsConfig(); // Loads openRouter, persistence, models, and defaultModel settings from 'config/models.yaml'\n```\n\n*Note: Environment variables are no longer required as all configuration is provided via the YAML file.*\n\n## Usage\n\n### Basic Server Setup\n\n```typescript\nimport { ConversationServer } from 'mcp-conversation-server';\n\nconst server = new ConversationServer(config);\nserver.run().catch(console.error);\n```\n\n### Available Tools\n\nThe server exposes several MCP tools:\n\n1. **create-conversation**\n\n   ```typescript\n   {\n       provider: 'openrouter',    // Provider is always 'openrouter'\n       model: string,             // OpenRouter model ID (e.g., 'anthropic/claude-3-opus-20240229')\n       title?: string;            // Optional conversation title\n   }\n   ```\n\n2. **send-message**\n\n   ```typescript\n   {\n       conversationId: string;  // Conversation ID\n       content: string;         // Message content\n       stream?: boolean;        // Enable streaming responses\n   }\n   ```\n\n3. **list-conversations**\n\n   ```typescript\n   {\n       filter?: {\n           model?: string;      // Filter by model\n           startDate?: string;  // Filter by start date\n           endDate?: string;    // Filter by end date\n       }\n   }\n   ```\n\n### Resources\n\nThe server provides access to several resources:\n\n1. **conversation://{id}**\n   - Access specific conversation details\n   - View message history\n   - Check conversation metadata\n\n2. **conversation://list**\n   - List all active conversations\n   - Filter conversations by criteria\n   - Sort by recent activity\n\n## Development\n\n### Building\n\n```bash\nnpm run build\n```\n\n### Running Tests\n\n```bash\nnpm test\n```\n\n### Debugging\n\nThe server provides several debugging features:\n\n1. **Error Logging**\n   - All errors are logged with stack traces\n   - Token usage tracking\n   - Rate limit monitoring\n\n2. **MCP Inspector**\n\n   ```bash\n   npm run inspector\n   ```\n\n   Use the MCP Inspector to:\n   - Test tool execution\n   - View resource contents\n   - Monitor message flow\n   - Validate protocol compliance\n\n3. **Provider Validation**\n\n   ```typescript\n   await server.providerManager.validateProviders();\n   ```\n\n   Validates:\n   - API key validity\n   - Model availability\n   - Rate limit status\n\n### Troubleshooting\n\nCommon issues and solutions:\n\n1. **OpenRouter Connection Issues**\n   - Verify your API key is valid\n   - Check rate limits on [OpenRouter's dashboard](https://openrouter.ai/dashboard)\n   - Ensure the model ID is correct\n   - Monitor credit usage\n\n2. **Message Streaming Errors**\n   - Verify model streaming support\n   - Check connection stability\n   - Monitor token limits\n   - Handle timeout settings\n\n3. **File System Errors**\n   - Check directory permissions\n   - Verify path configuration\n   - Monitor disk space\n   - Handle concurrent access\n\n## Contributing\n\n1. Fork the repository\n2. Create a feature branch\n3. Commit your changes\n4. Push to the branch\n5. Create a Pull Request\n\n## License\n\nISC License\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "bsmi021",
        "conversations",
        "conversation",
        "conversation server",
        "collaboration bsmi021",
        "conversations using"
      ],
      "category": "realtime-collaboration"
    },
    "bsmi021--mcp-gemini-server": {
      "owner": "bsmi021",
      "name": "mcp-gemini-server",
      "url": "https://github.com/bsmi021/mcp-gemini-server",
      "imageUrl": "/freedevtools/mcp/pfp/bsmi021.webp",
      "description": "Leverage Google's Gemini model capabilities for text generation and state management in chat applications through a standardized tool-based interface. Provides functionalities like content generation and function calling via the MCP standard.",
      "stars": 32,
      "forks": 12,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-09-10T00:29:01Z",
      "readme_content": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/bsmi021-mcp-gemini-server-badge.png)](https://mseep.ai/app/bsmi021-mcp-gemini-server)\n\n# MCP Gemini Server\n\n## Table of Contents\n- [Overview](#overview)\n- [File Uploads vs URL-Based Analysis](#file-uploads-vs-url-based-analysis)\n- [Features](#features)\n- [Prerequisites](#prerequisites)\n- [Installation & Setup](#installation--setup)\n- [Configuration](#configuration)\n- [Available Tools](#available-tools)\n- [Usage Examples](#usage-examples)\n- [Supported Multimedia Analysis Use Cases](#supported-multimedia-analysis-use-cases)\n- [MCP Gemini Server and Gemini SDK's MCP Function Calling](#mcp-gemini-server-and-gemini-sdks-mcp-function-calling)\n- [Environment Variables](#environment-variables)\n- [Security Considerations](#security-considerations)\n- [Error Handling](#error-handling)\n- [Development and Testing](#development-and-testing)\n- [Contributing](#contributing)\n- [Code Review Tools](#code-review-tools)\n- [Server Features](#server-features)\n- [Known Issues](#known-issues)\n\n## Overview\n\nThis project provides a dedicated MCP (Model Context Protocol) server that wraps the `@google/genai` SDK (v0.10.0). It exposes Google's Gemini model capabilities as standard MCP tools, allowing other LLMs (like Claude) or MCP-compatible systems to leverage Gemini's features as a backend workhorse.\n\nThis server aims to simplify integration with Gemini models by providing a consistent, tool-based interface managed via the MCP standard. It supports the latest Gemini models including `gemini-1.5-pro-latest`, `gemini-1.5-flash`, and `gemini-2.5-pro` models.\n\n**Important Note:** This server does not support direct file uploads. Instead, it focuses on URL-based multimedia analysis for images and videos. For text-based content processing, use the standard content generation tools.\n\n## File Uploads vs URL-Based Analysis\n\n### ❌ Not Supported: Direct File Uploads\n\nThis MCP Gemini Server **does not support** the following file upload operations:\n\n- **Local file uploads**: Cannot upload files from your local filesystem to Gemini\n- **Base64 encoded files**: Cannot process base64-encoded image or video data\n- **Binary file data**: Cannot handle raw file bytes or binary data\n- **File references**: Cannot process file IDs or references from uploaded content\n- **Audio file uploads**: Cannot upload and transcribe audio files directly\n\n**Why File Uploads Are Not Supported:**\n- Simplified architecture focused on URL-based processing\n- Enhanced security by avoiding file handling complexities\n- Reduced storage and bandwidth requirements\n- Streamlined codebase maintenance\n\n### ✅ Fully Supported: URL-Based Multimedia Analysis\n\nThis server **fully supports** analyzing multimedia content from publicly accessible URLs:\n\n**Image Analysis from URLs:**\n- **Public image URLs**: Analyze images hosted on any publicly accessible web server\n- **Supported formats**: PNG, JPEG, WebP, HEIC, HEIF via direct URL access\n- **Multiple images**: Process multiple image URLs in a single request\n- **Security validation**: Automatic URL validation and security screening\n\n**YouTube Video Analysis:**\n- **Public YouTube videos**: Full analysis of any public YouTube video content\n- **Video understanding**: Extract insights, summaries, and detailed analysis\n- **Educational content**: Perfect for analyzing tutorials, lectures, and educational videos\n- **Multiple videos**: Process multiple YouTube URLs (up to 10 per request with Gemini 2.5+)\n\n**Web Content Processing:**\n- **HTML content**: Analyze and extract information from web pages\n- **Mixed media**: Combine text content with embedded images and videos\n- **Contextual analysis**: Process URLs alongside text prompts for comprehensive analysis\n\n### Alternatives for Local Content\n\n**If you have local files to analyze:**\n\n1. **Host on a web server**: Upload your files to a public web server and use the URL\n2. **Use cloud storage**: Upload to services like Google Drive, Dropbox, or AWS S3 with public access\n3. **Use GitHub**: Host images in a GitHub repository and use the raw file URLs\n4. **Use image hosting services**: Upload to services like Imgur, ImageBB, or similar platforms\n\n**For audio content:**\n- Use external transcription services (Whisper API, Google Speech-to-Text, etc.)\n- Upload audio to YouTube and analyze the resulting video URL\n- Use other MCP servers that specialize in audio processing\n\n## Features\n\n* **Core Generation:** Standard (`gemini_generateContent`) and streaming (`gemini_generateContentStream`) text generation with support for system instructions and cached content.\n* **Function Calling:** Enables Gemini models to request the execution of client-defined functions (`gemini_functionCall`).\n* **Stateful Chat:** Manages conversational context across multiple turns (`gemini_startChat`, `gemini_sendMessage`, `gemini_sendFunctionResult`) with support for system instructions, tools, and cached content.\n* **URL-Based Multimedia Analysis:** Analyze images from public URLs and YouTube videos without file uploads. Direct file uploads are not supported.\n* **Caching:** Create, list, retrieve, update, and delete cached content to optimize prompts with support for tools and tool configurations.\n* **Image Generation:** Generate images from text prompts using Gemini 2.0 Flash Experimental (`gemini_generateImage`) with control over resolution, number of images, and negative prompts. Also supports the latest Imagen 3.1 model for high-quality dedicated image generation with advanced style controls. Note that Gemini 2.5 models (Flash and Pro) do not currently support image generation.\n* **URL Context Processing:** Fetch and analyze web content directly from URLs with advanced security, caching, and content processing capabilities.\n  * `gemini_generateContent`: Enhanced with URL context support for including web content in prompts\n  * `gemini_generateContentStream`: Streaming generation with URL context integration\n  * `gemini_url_analysis`: Specialized tool for advanced URL content analysis with multiple analysis types\n* **MCP Client:** Connect to and interact with external MCP servers.\n  * `mcpConnectToServer`: Establishes a connection to an external MCP server.\n  * `mcpListServerTools`: Lists available tools on a connected MCP server.\n  * `mcpCallServerTool`: Calls a function on a connected MCP server, with an option for file output.\n  * `mcpDisconnectFromServer`: Disconnects from an external MCP server.\n  * `writeToFile`: Writes content directly to files within allowed directories.\n\n\n## Prerequisites\n\n* Node.js (v18 or later)\n* An API Key from **Google AI Studio** (<https://aistudio.google.com/app/apikey>).\n  * **Important:** The Caching API is **only compatible with Google AI Studio API keys** and is **not supported** when using Vertex AI credentials. This server does not currently support Vertex AI authentication.\n\n## Installation & Setup\n\n### Installing Manually\n\n1. **Clone/Place Project:** Ensure the `mcp-gemini-server` project directory is accessible on your system.\n2. **Install Dependencies:** Navigate to the project directory in your terminal and run:\n\n    ```bash\n    npm install\n    ```\n\n3. **Build Project:** Compile the TypeScript source code:\n\n    ```bash\n    npm run build\n    ```\n\n    This command uses the TypeScript compiler (`tsc`) and outputs the JavaScript files to the `./dist` directory (as specified by `outDir` in `tsconfig.json`). The main server entry point will be `dist/server.js`.\n4. **Generate Connection Token:** Create a strong, unique connection token for secure communication between your MCP client and the server. This is a shared secret that you generate and configure on both the server and client sides.\n\n    **Generate a secure token using one of these methods:**\n\n    **Option A: Using Node.js crypto (Recommended)**\n    ```bash\n    node -e \"console.log(require('crypto').randomBytes(32).toString('hex'))\"\n    ```\n\n    **Option B: Using OpenSSL**\n    ```bash\n    openssl rand -hex 32\n    ```\n\n    **Option C: Using PowerShell (Windows)**\n    ```powershell\n    [System.Convert]::ToBase64String([System.Security.Cryptography.RandomNumberGenerator]::GetBytes(32))\n    ```\n\n    **Option D: Online Generator (Use with caution)**\n    Use a reputable password generator like [1Password](https://1password.com/password-generator/) or [Bitwarden](https://bitwarden.com/password-generator/) to generate a 64-character random string.\n\n    **Important Security Notes:**\n    - The token should be at least 32 characters long and contain random characters\n    - Never share this token or commit it to version control\n    - Use a different token for each server instance\n    - Store the token securely (environment variables, secrets manager, etc.)\n    - Save this token - you'll need to use the exact same value in both server and client configurations\n\n5. **Configure MCP Client:** Add the server configuration to your MCP client's settings file (e.g., `cline_mcp_settings.json` for Cline/VSCode, or `claude_desktop_config.json` for Claude Desktop App). Replace `/path/to/mcp-gemini-server` with the actual **absolute path** on your system, `YOUR_API_KEY` with your Google AI Studio key, and `YOUR_GENERATED_CONNECTION_TOKEN` with the token you generated in step 4.\n\n    ```json\n    {\n      \"mcpServers\": {\n        \"gemini-server\": { // Or your preferred name\n          \"command\": \"node\",\n          \"args\": [\"/path/to/mcp-gemini-server/dist/server.js\"], // Absolute path to the compiled server entry point\n          \"env\": {\n            \"GOOGLE_GEMINI_API_KEY\": \"YOUR_API_KEY\",\n            \"MCP_SERVER_HOST\": \"localhost\",       // Required: Server host\n            \"MCP_SERVER_PORT\": \"8080\",            // Required: Server port  \n            \"MCP_CONNECTION_TOKEN\": \"YOUR_GENERATED_CONNECTION_TOKEN\", // Required: Use the token from step 4\n            \"GOOGLE_GEMINI_MODEL\": \"gemini-1.5-flash\", // Optional: Set a default model\n            // Optional security configurations removed - file operations no longer supported\n            \"ALLOWED_OUTPUT_PATHS\": \"/var/opt/mcp-gemini-server/outputs,/tmp/mcp-gemini-outputs\" // Optional: Comma-separated list of allowed output directories for mcpCallServerTool and writeToFileTool\n          },\n          \"disabled\": false,\n          \"autoApprove\": []\n        }\n        // ... other servers\n      }\n    }\n    ```\n\n    **Important Notes:**\n    - The path in `args` must be the **absolute path** to the compiled `dist/server.js` file\n    - `MCP_SERVER_HOST`, `MCP_SERVER_PORT`, and `MCP_CONNECTION_TOKEN` are required unless `NODE_ENV` is set to `test`\n    - `MCP_CONNECTION_TOKEN` must be the exact same value you generated in step 4\n    - Ensure the path exists and the server has been built using `npm run build`\n6. **Restart MCP Client:** Restart your MCP client application (e.g., VS Code with Cline extension, Claude Desktop App) to load the new server configuration. The MCP client will manage starting and stopping the server process.\n\n## Configuration\n\nThe server uses environment variables for configuration, passed via the `env` object in the MCP settings:\n\n* `GOOGLE_GEMINI_API_KEY` (**Required**): Your API key obtained from Google AI Studio.\n* `GOOGLE_GEMINI_MODEL` (*Optional*): Specifies a default Gemini model name (e.g., `gemini-1.5-flash`, `gemini-1.0-pro`). If set, tools that require a model name (like `gemini_generateContent`, `gemini_startChat`, etc.) will use this default when the `modelName` parameter is omitted in the tool call. This simplifies client calls when primarily using one model. If this environment variable is *not* set, the `modelName` parameter becomes required for those tools. See the [Google AI documentation](https://ai.google.dev/models/gemini) for available model names.\n* `ALLOWED_OUTPUT_PATHS` (*Optional*): A comma-separated list of absolute paths to directories where the `mcpCallServerTool` (with `outputToFile` parameter) and `writeToFileTool` are allowed to write files. If not set, file output will be disabled for these tools. This is a security measure to prevent arbitrary file writes.\n\n## Available Tools\n\nThis server provides the following MCP tools. Parameter schemas are defined using Zod for validation and description.\n\n**Validation and Error Handling:** All parameters are validated using Zod schemas at both the MCP tool level and service layer, providing consistent validation, detailed error messages, and type safety. The server implements comprehensive error mapping to provide clear, actionable error messages.\n\n**Retry Logic:** API requests automatically use exponential backoff retry for transient errors (network issues, rate limits, timeouts), improving reliability for unstable connections. The retry mechanism includes configurable parameters for maximum attempts, delay times, and jitter to prevent thundering herd effects.\n\n**Note on Optional Parameters:** Many tools accept complex optional parameters (e.g., `generationConfig`, `safetySettings`, `toolConfig`, `history`, `functionDeclarations`, `contents`). These parameters are typically objects or arrays whose structure mirrors the types defined in the underlying `@google/genai` SDK (v0.10.0). For the exact structure and available fields within these complex parameters, please refer to:\n    1. The corresponding `src/tools/*Params.ts` file in this project.\n    2. The official [Google AI JS SDK Documentation](https://github.com/google/generative-ai-js).\n\n### Core Generation\n\n* **`gemini_generateContent`**\n  * *Description:* Generates non-streaming text content from a prompt with optional URL context support.\n  * *Required Params:* `prompt` (string)\n  * *Optional Params:* \n    * `modelName` (string) - Name of the model to use\n    * `generationConfig` (object) - Controls generation parameters like temperature, topP, etc.\n      * `thinkingConfig` (object) - Controls model reasoning process\n        * `thinkingBudget` (number) - Maximum tokens for reasoning (0-24576)\n        * `reasoningEffort` (string) - Simplified control: \"none\" (0 tokens), \"low\" (1K), \"medium\" (8K), \"high\" (24K)\n    * `safetySettings` (array) - Controls content filtering by harm category\n    * `systemInstruction` (string or object) - System instruction to guide model behavior\n    * `cachedContentName` (string) - Identifier for cached content to use with this request\n    * `urlContext` (object) - Fetch and include web content from URLs\n      * `urls` (array) - URLs to fetch and include as context (max 20)\n      * `fetchOptions` (object) - Configuration for URL fetching\n        * `maxContentKb` (number) - Maximum content size per URL in KB (default: 100)\n        * `timeoutMs` (number) - Fetch timeout per URL in milliseconds (default: 10000)\n        * `includeMetadata` (boolean) - Include URL metadata in context (default: true)\n        * `convertToMarkdown` (boolean) - Convert HTML to markdown (default: true)\n        * `allowedDomains` (array) - Specific domains to allow for this request\n        * `userAgent` (string) - Custom User-Agent header for URL requests\n    * `modelPreferences` (object) - Model selection preferences\n  * *Note:* Can handle multimodal inputs, cached content, and URL context for comprehensive content generation\n  * *Thinking Budget:* Controls the token budget for model reasoning. Lower values provide faster responses, higher values improve complex reasoning.\n* **`gemini_generateContentStream`**\n  * *Description:* Generates text content via streaming using Server-Sent Events (SSE) for real-time content delivery with URL context support.\n  * *Required Params:* `prompt` (string)\n  * *Optional Params:* \n    * `modelName` (string) - Name of the model to use\n    * `generationConfig` (object) - Controls generation parameters like temperature, topP, etc.\n      * `thinkingConfig` (object) - Controls model reasoning process\n        * `thinkingBudget` (number) - Maximum tokens for reasoning (0-24576)\n        * `reasoningEffort` (string) - Simplified control: \"none\" (0 tokens), \"low\" (1K), \"medium\" (8K), \"high\" (24K)\n    * `safetySettings` (array) - Controls content filtering by harm category\n    * `systemInstruction` (string or object) - System instruction to guide model behavior\n    * `cachedContentName` (string) - Identifier for cached content to use with this request\n    * `urlContext` (object) - Same URL context options as `gemini_generateContent`\n    * `modelPreferences` (object) - Model selection preferences\n\n### Function Calling\n\n* **`gemini_functionCall`**\n  * *Description:* Sends a prompt and function declarations to the model, returning either a text response or a requested function call object (as a JSON string).\n  * *Required Params:* `prompt` (string), `functionDeclarations` (array)\n  * *Optional Params:* \n    * `modelName` (string) - Name of the model to use\n    * `generationConfig` (object) - Controls generation parameters\n    * `safetySettings` (array) - Controls content filtering\n    * `toolConfig` (object) - Configures tool behavior like temperature and confidence thresholds\n\n### Stateful Chat\n\n* **`gemini_startChat`**\n  * *Description:* Initiates a new stateful chat session and returns a unique `sessionId`.\n  * *Optional Params:* \n    * `modelName` (string) - Name of the model to use\n    * `history` (array) - Initial conversation history\n    * `tools` (array) - Tool definitions including function declarations\n    * `generationConfig` (object) - Controls generation parameters\n      * `thinkingConfig` (object) - Controls model reasoning process\n        * `thinkingBudget` (number) - Maximum tokens for reasoning (0-24576)\n        * `reasoningEffort` (string) - Simplified control: \"none\" (0 tokens), \"low\" (1K), \"medium\" (8K), \"high\" (24K)\n    * `safetySettings` (array) - Controls content filtering\n    * `systemInstruction` (string or object) - System instruction to guide model behavior\n    * `cachedContentName` (string) - Identifier for cached content to use with this session\n* **`gemini_sendMessage`**\n  * *Description:* Sends a message within an existing chat session.\n  * *Required Params:* `sessionId` (string), `message` (string)\n  * *Optional Params:* \n    * `generationConfig` (object) - Controls generation parameters\n      * `thinkingConfig` (object) - Controls model reasoning process\n        * `thinkingBudget` (number) - Maximum tokens for reasoning (0-24576)\n        * `reasoningEffort` (string) - Simplified control: \"none\" (0 tokens), \"low\" (1K), \"medium\" (8K), \"high\" (24K)\n    * `safetySettings` (array) - Controls content filtering\n    * `tools` (array) - Tool definitions including function declarations\n    * `toolConfig` (object) - Configures tool behavior\n    * `cachedContentName` (string) - Identifier for cached content to use with this message\n* **`gemini_sendFunctionResult`**\n  * *Description:* Sends the result of a function execution back to a chat session.\n  * *Required Params:* `sessionId` (string), `functionResponse` (string) - The result of the function execution\n  * *Optional Params:* `functionCall` (object) - Reference to the original function call\n* **`gemini_routeMessage`**\n  * *Description:* Routes a message to the most appropriate model from a provided list based on message content. Returns both the model's response and which model was selected.\n  * *Required Params:* \n    * `message` (string) - The text message to be routed to the most appropriate model\n    * `models` (array) - Array of model names to consider for routing (e.g., ['gemini-1.5-flash', 'gemini-1.5-pro']). The first model in the list will be used for routing decisions.\n  * *Optional Params:* \n    * `routingPrompt` (string) - Custom prompt to use for routing decisions. If not provided, a default routing prompt will be used.\n    * `defaultModel` (string) - Model to fall back to if routing fails. If not provided and routing fails, an error will be thrown.\n    * `generationConfig` (object) - Generation configuration settings to apply to the selected model's response.\n      * `thinkingConfig` (object) - Controls model reasoning process\n        * `thinkingBudget` (number) - Maximum tokens for reasoning (0-24576)\n        * `reasoningEffort` (string) - Simplified control: \"none\" (0 tokens), \"low\" (1K), \"medium\" (8K), \"high\" (24K)\n    * `safetySettings` (array) - Safety settings to apply to both routing and final response.\n    * `systemInstruction` (string or object) - A system instruction to guide the model's behavior after routing.\n\n### Remote File Operations (Removed)\n\n**Note:** Direct file upload operations are no longer supported by this server. The server now focuses exclusively on URL-based multimedia analysis for images and videos, and text-based content generation.\n\n**Alternative Approaches:**\n- **For Image Analysis:** Use publicly accessible image URLs with `gemini_generateContent` or `gemini_url_analysis` tools\n- **For Video Analysis:** Use publicly accessible YouTube video URLs for content analysis\n- **For Audio Content:** Audio transcription via file uploads is not supported - consider using URL-based services that provide audio transcripts\n- **For Document Analysis:** Use URL-based document analysis or convert documents to publicly accessible formats\n\n### Caching (Google AI Studio Key Required)\n\n* **`gemini_createCache`**\n  * *Description:* Creates cached content for compatible models (e.g., `gemini-1.5-flash`).\n  * *Required Params:* `contents` (array), `model` (string)\n  * *Optional Params:* \n    * `displayName` (string) - Human-readable name for the cached content\n    * `systemInstruction` (string or object) - System instruction to apply to the cached content\n    * `ttl` (string - e.g., '3600s') - Time-to-live for the cached content\n    * `tools` (array) - Tool definitions for use with the cached content\n    * `toolConfig` (object) - Configuration for the tools\n* **`gemini_listCaches`**\n  * *Description:* Lists existing cached content.\n  * *Required Params:* None\n  * *Optional Params:* `pageSize` (number), `pageToken` (string - Note: `pageToken` may not be reliably returned currently).\n* **`gemini_getCache`**\n  * *Description:* Retrieves metadata for specific cached content.\n  * *Required Params:* `cacheName` (string - e.g., `cachedContents/abc123xyz`)\n* **`gemini_updateCache`**\n  * *Description:* Updates metadata and contents for cached content.\n  * *Required Params:* `cacheName` (string), `contents` (array)\n  * *Optional Params:* \n    * `displayName` (string) - Updated display name\n    * `systemInstruction` (string or object) - Updated system instruction\n    * `ttl` (string) - Updated time-to-live\n    * `tools` (array) - Updated tool definitions\n    * `toolConfig` (object) - Updated tool configuration\n* **`gemini_deleteCache`**\n  * *Description:* Deletes cached content.\n  * *Required Params:* `cacheName` (string - e.g., `cachedContents/abc123xyz`)\n\n### Image Generation\n\n* **`gemini_generateImage`**\n  * *Description:* Generates images from text prompts using available image generation models.\n  * *Required Params:* `prompt` (string - descriptive text prompt for image generation)\n  * *Optional Params:* \n    * `modelName` (string - defaults to \"imagen-3.1-generate-003\" for high-quality dedicated image generation or use \"gemini-2.0-flash-exp-image-generation\" for Gemini models)\n    * `resolution` (string enum: \"512x512\", \"1024x1024\", \"1536x1536\")\n    * `numberOfImages` (number - 1-8, default: 1)\n    * `safetySettings` (array) - Controls content filtering for generated images\n    * `negativePrompt` (string - features to avoid in the generated image)\n    * `stylePreset` (string enum: \"photographic\", \"digital-art\", \"cinematic\", \"anime\", \"3d-render\", \"oil-painting\", \"watercolor\", \"pixel-art\", \"sketch\", \"comic-book\", \"neon\", \"fantasy\")\n    * `seed` (number - integer value for reproducible generation)\n    * `styleStrength` (number - strength of style preset, 0.0-1.0)\n  * *Response:* Returns an array of base64-encoded images with metadata including dimensions and MIME type.\n  * *Notes:* Image generation uses significant resources, especially at higher resolutions. Consider using smaller resolutions for faster responses and less resource usage.\n\n\n### Audio Transcription (Removed)\n\n**Note:** Audio transcription via direct file uploads is no longer supported by this server. The server focuses on URL-based multimedia analysis for images and videos.\n\n**Alternative Approaches for Audio Content:**\n- **YouTube Videos:** Use the YouTube video analysis capabilities to analyze video content that includes audio\n- **External Services:** Use dedicated audio transcription services and analyze their output as text content\n- **URL-Based Audio:** If audio content is available via public URLs in supported formats, consider using external transcription services first, then analyze the resulting text\n\n### URL Content Analysis\n\n* **`gemini_url_analysis`**\n  * *Description:* Advanced URL analysis tool that fetches content from web pages and performs specialized analysis tasks with comprehensive security and performance optimizations.\n  * *Required Params:* \n    * `urls` (array) - URLs to analyze (1-20 URLs supported)\n    * `analysisType` (string enum) - Type of analysis to perform:\n      * `summary` - Comprehensive content summarization\n      * `comparison` - Multi-URL content comparison\n      * `extraction` - Structured information extraction\n      * `qa` - Question-based content analysis\n      * `sentiment` - Emotional tone analysis\n      * `fact-check` - Credibility assessment\n      * `content-classification` - Topic and type categorization\n      * `readability` - Accessibility and complexity analysis\n      * `seo-analysis` - Search optimization evaluation\n  * *Optional Params:*\n    * `query` (string) - Specific query or instruction for the analysis\n    * `extractionSchema` (object) - JSON schema for structured data extraction\n    * `questions` (array) - List of specific questions to answer (for Q&A analysis)\n    * `compareBy` (array) - Specific aspects to compare when using comparison analysis\n    * `outputFormat` (string enum: \"text\", \"json\", \"markdown\", \"structured\") - Desired output format\n    * `includeMetadata` (boolean) - Include URL metadata in the analysis (default: true)\n    * `fetchOptions` (object) - Advanced URL fetching options (same as urlContext fetchOptions)\n    * `modelName` (string) - Specific Gemini model to use (auto-selected if not specified)\n  * *Security Features:* Multi-layer URL validation, domain restrictions, private network protection, and rate limiting\n  * *Performance Features:* Intelligent caching, concurrent processing, and optimal model selection based on content complexity\n\n### MCP Client Tools\n\n* **`mcpConnectToServer`**\n  * *Description:* Establishes a connection to an external MCP server and returns a connection ID.\n  * *Required Params:*\n    * `serverId` (string): A unique identifier for this server connection.\n    * `connectionType` (string enum: \"sse\" | \"stdio\"): The transport protocol to use.\n    * `sseUrl` (string, optional if `connectionType` is \"stdio\"): The URL for SSE connection.\n    * `stdioCommand` (string, optional if `connectionType` is \"sse\"): The command to run for stdio connection.\n    * `stdioArgs` (array of strings, optional): Arguments for the stdio command.\n    * `stdioEnv` (object, optional): Environment variables for the stdio command.\n  * *Important:* This tool returns a `connectionId` that must be used in subsequent calls to `mcpListServerTools`, `mcpCallServerTool`, and `mcpDisconnectFromServer`. This `connectionId` is generated internally and is different from the `serverId` parameter.\n* **`mcpListServerTools`**\n  * *Description:* Lists available tools on a connected MCP server.\n  * *Required Params:*\n    * `connectionId` (string): The connection identifier returned by `mcpConnectToServer`.\n* **`mcpCallServerTool`**\n  * *Description:* Calls a function on a connected MCP server.\n  * *Required Params:*\n    * `connectionId` (string): The connection identifier returned by `mcpConnectToServer`.\n    * `toolName` (string): The name of the tool to call on the remote server.\n    * `toolArgs` (object): The arguments to pass to the remote tool.\n  * *Optional Params:*\n    * `outputToFile` (string): If provided, the tool's output will be written to this file path. The path must be within one of the directories specified in the `ALLOWED_OUTPUT_PATHS` environment variable.\n* **`mcpDisconnectFromServer`**\n  * *Description:* Disconnects from an external MCP server.\n  * *Required Params:*\n    * `connectionId` (string): The connection identifier returned by `mcpConnectToServer`.\n* **`writeToFile`**\n  * *Description:* Writes content directly to a file.\n  * *Required Params:*\n    * `filePath` (string): The absolute path of the file to write to. Must be within one of the directories specified in the `ALLOWED_OUTPUT_PATHS` environment variable.\n    * `content` (string): The content to write to the file.\n  * *Optional Params:*\n    * `overwrite` (boolean, default: false): If true, overwrite the file if it already exists. Otherwise, an error will be thrown if the file exists.\n\n## Usage Examples\n\nHere are examples of how an MCP client (like Claude) might call these tools using the `use_mcp_tool` format:\n\n\n**Example 1: Simple Content Generation (Using Default Model)**\n\n```xml\n<use_mcp_tool>\n  <server_name>gemini-server</server_name>\n  <tool_name>gemini_generateContent</tool_name>\n  <arguments>\n    {\n      \"prompt\": \"Write a short poem about a rubber duck.\"\n    }\n  </arguments>\n</use_mcp_tool>\n```\n\n**Example 2: Content Generation (Specifying Model & Config)**\n\n```xml\n<use_mcp_tool>\n  <server_name>gemini-server</server_name>\n  <tool_name>gemini_generateContent</tool_name>\n  <arguments>\n    {\n      \"modelName\": \"gemini-1.0-pro\",\n      \"prompt\": \"Explain the concept of recursion in programming.\",\n      \"generationConfig\": {\n        \"temperature\": 0.7,\n        \"maxOutputTokens\": 500\n      }\n    }\n  </arguments>\n</use_mcp_tool>\n```\n\n**Example 2b: Content Generation with Thinking Budget Control**\n\n```xml\n<use_mcp_tool>\n  <server_name>gemini-server</server_name>\n  <tool_name>gemini_generateContent</tool_name>\n  <arguments>\n    {\n      \"modelName\": \"gemini-1.5-pro\",\n      \"prompt\": \"Solve this complex math problem: Find all values of x where 2sin(x) = x^2-x+1 in the range [0, 2π].\",\n      \"generationConfig\": {\n        \"temperature\": 0.2,\n        \"maxOutputTokens\": 1000,\n        \"thinkingConfig\": {\n          \"thinkingBudget\": 8192\n        }\n      }\n    }\n  </arguments>\n</use_mcp_tool>\n```\n\n**Example 2c: Content Generation with Simplified Reasoning Effort**\n\n```xml\n<use_mcp_tool>\n  <server_name>gemini-server</server_name>\n  <tool_name>gemini_generateContent</tool_name>\n  <arguments>\n    {\n      \"modelName\": \"gemini-1.5-pro\",\n      \"prompt\": \"Solve this complex math problem: Find all values of x where 2sin(x) = x^2-x+1 in the range [0, 2π].\",\n      \"generationConfig\": {\n        \"temperature\": 0.2,\n        \"maxOutputTokens\": 1000,\n        \"thinkingConfig\": {\n          \"reasoningEffort\": \"high\"\n        }\n      }\n    }\n  </arguments>\n</use_mcp_tool>\n```\n\n**Example 3: Starting and Continuing a Chat**\n\n*Start Chat:*\n\n```xml\n<use_mcp_tool>\n  <server_name>gemini-server</server_name>\n  <tool_name>gemini_startChat</tool_name>\n  <arguments>\n    {}\n  </arguments>\n</use_mcp_tool>\n```\n\n*(Assume response contains `sessionId: \"some-uuid-123\"`)*\n\n*Send Message:*\n\n```xml\n<use_mcp_tool>\n  <server_name>gemini-server</server_name>\n  <tool_name>gemini_sendMessage</tool_name>\n  <arguments>\n    {\n      \"sessionId\": \"some-uuid-123\",\n      \"message\": \"Hello! Can you tell me about the Gemini API?\"\n    }\n  </arguments>\n</use_mcp_tool>\n```\n\n**Example 4: Content Generation with System Instructions (Simplified Format)**\n\n```xml\n<use_mcp_tool>\n  <server_name>gemini-server</server_name>\n  <tool_name>gemini_generateContent</tool_name>\n  <arguments>\n    {\n      \"modelName\": \"gemini-2.5-pro-exp\",\n      \"prompt\": \"What should I do with my day off?\",\n      \"systemInstruction\": \"You are a helpful assistant that provides friendly and detailed advice. You should focus on outdoor activities and wellness.\"\n    }\n  </arguments>\n</use_mcp_tool>\n```\n\n**Example 5: Content Generation with System Instructions (Object Format)**\n\n```xml\n<use_mcp_tool>\n  <server_name>gemini-server</server_name>\n  <tool_name>gemini_generateContent</tool_name>\n  <arguments>\n    {\n      \"modelName\": \"gemini-1.5-pro-latest\",\n      \"prompt\": \"What should I do with my day off?\",\n      \"systemInstruction\": {\n        \"parts\": [\n          {\n            \"text\": \"You are a helpful assistant that provides friendly and detailed advice. You should focus on outdoor activities and wellness.\"\n          }\n        ]\n      }\n    }\n  </arguments>\n</use_mcp_tool>\n```\n\n**Example 6: Using Cached Content with System Instruction**\n\n```xml\n<use_mcp_tool>\n  <server_name>gemini-server</server_name>\n  <tool_name>gemini_generateContent</tool_name>\n  <arguments>\n    {\n      \"modelName\": \"gemini-2.5-pro-exp\",\n      \"prompt\": \"Explain how these concepts relate to my product?\",\n      \"cachedContentName\": \"cachedContents/abc123xyz\",\n      \"systemInstruction\": \"You are a product expert who explains technical concepts in simple terms.\"\n    }\n  </arguments>\n</use_mcp_tool>\n```\n\n**Example 6: Generating an Image**\n\n```xml\n<use_mcp_tool>\n  <server_name>gemini-server</server_name>\n  <tool_name>gemini_generateImage</tool_name>\n  <arguments>\n    {\n      \"prompt\": \"A futuristic cityscape with flying cars and neon lights\",\n      \"modelName\": \"gemini-2.0-flash-exp-image-generation\",\n      \"resolution\": \"1024x1024\",\n      \"numberOfImages\": 1,\n      \"negativePrompt\": \"dystopian, ruins, dark, gloomy\"\n    }\n  </arguments>\n</use_mcp_tool>\n```\n\n**Example 6b: Generating a High-Quality Image with Imagen 3.1**\n\n```xml\n<use_mcp_tool>\n  <server_name>gemini-server</server_name>\n  <tool_name>gemini_generateImage</tool_name>\n  <arguments>\n    {\n      \"prompt\": \"A futuristic cityscape with flying cars and neon lights\",\n      \"modelName\": \"imagen-3.1-generate-003\",\n      \"resolution\": \"1024x1024\",\n      \"numberOfImages\": 4,\n      \"negativePrompt\": \"dystopian, ruins, dark, gloomy\"\n    }\n  </arguments>\n</use_mcp_tool>\n```\n\n**Example 6c: Using Advanced Style Options**\n\n```xml\n<use_mcp_tool>\n  <server_name>gemini-server</server_name>\n  <tool_name>gemini_generateImage</tool_name>\n  <arguments>\n    {\n      \"prompt\": \"A futuristic cityscape with flying cars and neon lights\",\n      \"modelName\": \"imagen-3.1-generate-003\",\n      \"resolution\": \"1024x1024\",\n      \"numberOfImages\": 2,\n      \"stylePreset\": \"anime\",\n      \"styleStrength\": 0.8,\n      \"seed\": 12345\n    }\n  </arguments>\n</use_mcp_tool>\n```\n\n\n**Example 7: Message Routing Between Models**\n\n```xml\n<use_mcp_tool>\n  <server_name>gemini-server</server_name>\n  <tool_name>gemini_routeMessage</tool_name>\n  <arguments>\n    {\n      \"message\": \"Can you create a detailed business plan for a sustainable fashion startup?\",\n      \"models\": [\"gemini-1.5-pro\", \"gemini-1.5-flash\", \"gemini-2.5-pro\"],\n      \"routingPrompt\": \"Analyze this message and determine which model would be best suited to handle it. Consider: gemini-1.5-flash for simpler tasks, gemini-1.5-pro for balanced capabilities, and gemini-2.5-pro for complex creative tasks.\",\n      \"defaultModel\": \"gemini-1.5-pro\",\n      \"generationConfig\": {\n        \"temperature\": 0.7,\n        \"maxOutputTokens\": 1024\n      }\n    }\n  </arguments>\n</use_mcp_tool>\n```\n\nThe response will be a JSON string containing both the text response and which model was chosen:\n\n```json\n{\n  \"text\": \"# Business Plan for Sustainable Fashion Startup\\n\\n## Executive Summary\\n...\",\n  \"chosenModel\": \"gemini-2.5-pro\"\n}\n```\n\n**Example 8: Using URL Context with Content Generation**\n\n```xml\n<use_mcp_tool>\n  <server_name>gemini-server</server_name>\n  <tool_name>gemini_generateContent</tool_name>\n  <arguments>\n    {\n      \"prompt\": \"Summarize the main points from these articles and compare their approaches to sustainable technology\",\n      \"urlContext\": {\n        \"urls\": [\n          \"https://example.com/sustainable-tech-2024\",\n          \"https://techblog.com/green-innovation\"\n        ],\n        \"fetchOptions\": {\n          \"maxContentKb\": 150,\n          \"includeMetadata\": true,\n          \"convertToMarkdown\": true\n        }\n      },\n      \"modelPreferences\": {\n        \"preferQuality\": true,\n        \"taskType\": \"reasoning\"\n      }\n    }\n  </arguments>\n</use_mcp_tool>\n```\n\n**Example 9: Advanced URL Analysis**\n\n```xml\n<use_mcp_tool>\n  <server_name>gemini-server</server_name>\n  <tool_name>gemini_url_analysis</tool_name>\n  <arguments>\n    {\n      \"urls\": [\"https://company.com/about\", \"https://company.com/products\"],\n      \"analysisType\": \"extraction\",\n      \"extractionSchema\": {\n        \"companyName\": \"string\",\n        \"foundedYear\": \"number\",\n        \"numberOfEmployees\": \"string\",\n        \"mainProducts\": \"array\",\n        \"headquarters\": \"string\",\n        \"financialInfo\": \"object\"\n      },\n      \"outputFormat\": \"json\",\n      \"query\": \"Extract comprehensive company information including business details and product offerings\"\n    }\n  </arguments>\n</use_mcp_tool>\n```\n\n**Example 10: Multi-URL Content Comparison**\n\n```xml\n<use_mcp_tool>\n  <server_name>gemini-server</server_name>\n  <tool_name>gemini_url_analysis</tool_name>\n  <arguments>\n    {\n      \"urls\": [\n        \"https://site1.com/pricing\",\n        \"https://site2.com/pricing\", \n        \"https://site3.com/pricing\"\n      ],\n      \"analysisType\": \"comparison\",\n      \"compareBy\": [\"pricing models\", \"features\", \"target audience\", \"value proposition\"],\n      \"outputFormat\": \"markdown\",\n      \"includeMetadata\": true\n    }\n  </arguments>\n</use_mcp_tool>\n```\n\n**Example 11: URL Content with Security Restrictions**\n\n```xml\n<use_mcp_tool>\n  <server_name>gemini-server</server_name>\n  <tool_name>gemini_generateContent</tool_name>\n  <arguments>\n    {\n      \"prompt\": \"Analyze the content from these trusted news sources\",\n      \"urlContext\": {\n        \"urls\": [\n          \"https://reuters.com/article/tech-news\",\n          \"https://bbc.com/news/technology\"\n        ],\n        \"fetchOptions\": {\n          \"allowedDomains\": [\"reuters.com\", \"bbc.com\"],\n          \"maxContentKb\": 200,\n          \"timeoutMs\": 15000,\n          \"userAgent\": \"Research-Bot/1.0\"\n        }\n      }\n    }\n  </arguments>\n</use_mcp_tool>\n```\n\n### URL-Based Image Analysis Examples\n\nThese examples demonstrate how to analyze images from public URLs using Gemini's native image understanding capabilities. The server processes images by fetching them from URLs and converting them to the format required by the Gemini API. Note that this server does not support direct file uploads - all image analysis must be performed using publicly accessible image URLs.\n\n**Example 17: Basic Image Description and Analysis**\n\n```xml\n<use_mcp_tool>\n  <server_name>gemini-server</server_name>\n  <tool_name>gemini_generateContent</tool_name>\n  <arguments>\n    {\n      \"prompt\": \"Please describe this image in detail, including objects, people, colors, setting, and any text you can see.\",\n      \"urlContext\": {\n        \"urls\": [\"https://example.com/images/photo.jpg\"],\n        \"fetchOptions\": {\n          \"includeMetadata\": true,\n          \"timeoutMs\": 15000\n        }\n      }\n    }\n  </arguments>\n</use_mcp_tool>\n```\n\n**Example 18: Object Detection and Identification**\n\n```xml\n<use_mcp_tool>\n  <server_name>gemini-server</server_name>\n  <tool_name>gemini_generateContent</tool_name>\n  <arguments>\n    {\n      \"prompt\": \"Identify and list all objects visible in this image. For each object, describe its location, size relative to other objects, and any notable characteristics.\",\n      \"urlContext\": {\n        \"urls\": [\"https://example.com/images/scene.png\"],\n        \"fetchOptions\": {\n          \"includeMetadata\": false,\n          \"timeoutMs\": 20000\n        }\n      }\n    }\n  </arguments>\n</use_mcp_tool>\n```\n\n**Example 19: Chart and Data Visualization Analysis**\n\n```xml\n<use_mcp_tool>\n  <server_name>gemini-server</server_name>\n  <tool_name>gemini_generateContent</tool_name>\n  <arguments>\n    {\n      \"prompt\": \"Analyze this chart or graph. What type of visualization is it? What are the main data points, trends, and insights? Extract any numerical values, labels, and time periods shown.\",\n      \"urlContext\": {\n        \"urls\": [\"https://example.com/charts/sales-data.png\"]\n      },\n      \"modelPreferences\": {\n        \"preferQuality\": true,\n        \"taskType\": \"reasoning\"\n      }\n    }\n  </arguments>\n</use_mcp_tool>\n```\n\n**Example 20: Comparative Image Analysis**\n\n```xml\n<use_mcp_tool>\n  <server_name>gemini-server</server_name>\n  <tool_name>gemini_generateContent</tool_name>\n  <arguments>\n    {\n      \"prompt\": \"Compare these two images side by side. Describe the differences and similarities in terms of objects, composition, colors, style, and any other notable aspects.\",\n      \"urlContext\": {\n        \"urls\": [\n          \"https://example.com/before-renovation.jpg\",\n          \"https://example.com/after-renovation.jpg\"\n        ],\n        \"fetchOptions\": {\n          \"maxContentKb\": 200,\n          \"includeMetadata\": true\n        }\n      }\n    }\n  </arguments>\n</use_mcp_tool>\n```\n\n**Example 21: Text Extraction from Images (OCR)**\n\n```xml\n<use_mcp_tool>\n  <server_name>gemini-server</server_name>\n  <tool_name>gemini_generateContent</tool_name>\n  <arguments>\n    {\n      \"prompt\": \"Extract all text visible in this image. Include any signs, labels, captions, or written content. Maintain the original formatting and structure as much as possible.\",\n      \"urlContext\": {\n        \"urls\": [\"https://example.com/documents/screenshot.png\"]\n      }\n    }\n  </arguments>\n</use_mcp_tool>\n```\n\n**Example 22: Technical Diagram or Flowchart Analysis**\n\n```xml\n<use_mcp_tool>\n  <server_name>gemini-server</server_name>\n  <tool_name>gemini_generateContent</tool_name>\n  <arguments>\n    {\n      \"prompt\": \"Analyze this technical diagram, flowchart, or schematic. Explain the system architecture, identify components, describe the relationships and data flow, and interpret any symbols or notations used.\",\n      \"urlContext\": {\n        \"urls\": [\"https://docs.example.com/architecture-diagram.png\"],\n        \"fetchOptions\": {\n          \"maxContentKb\": 100,\n          \"includeMetadata\": true\n        }\n      },\n      \"modelPreferences\": {\n        \"preferQuality\": true,\n        \"taskType\": \"reasoning\"\n      }\n    }\n  </arguments>\n</use_mcp_tool>\n```\n\n**Example 23: Image Analysis with Specific Questions**\n\n```xml\n<use_mcp_tool>\n  <server_name>gemini-server</server_name>\n  <tool_name>gemini_generateContent</tool_name>\n  <arguments>\n    {\n      \"prompt\": \"Looking at this image, please answer these specific questions: 1) What is the main subject? 2) What colors dominate the scene? 3) Are there any people visible? 4) What appears to be the setting or location? 5) What mood or atmosphere does the image convey?\",\n      \"urlContext\": {\n        \"urls\": [\"https://example.com/images/landscape.jpg\"]\n      }\n    }\n  </arguments>\n</use_mcp_tool>\n```\n\n**Example 24: Image Analysis with Security Restrictions**\n\n```xml\n<use_mcp_tool>\n  <server_name>gemini-server</server_name>\n  <tool_name>gemini_generateContent</tool_name>\n  <arguments>\n    {\n      \"prompt\": \"Analyze the composition and design elements in this image, focusing on visual hierarchy, layout principles, and aesthetic choices.\",\n      \"urlContext\": {\n        \"urls\": [\"https://trusted-cdn.example.com/design-mockup.jpg\"],\n        \"fetchOptions\": {\n          \"allowedDomains\": [\"trusted-cdn.example.com\", \"assets.example.com\"],\n          \"maxContentKb\": 150,\n          \"timeoutMs\": 25000,\n          \"includeMetadata\": false\n        }\n      }\n    }\n  </arguments>\n</use_mcp_tool>\n```\n\n**Important Notes for URL-Based Image Analysis:**\n- **Supported formats**: PNG, JPEG, WebP, HEIC, HEIF (as per Gemini API specifications)\n- **Image access**: Images must be accessible via public URLs without authentication\n- **Size considerations**: Large images are automatically processed in sections by Gemini\n- **Processing**: The server fetches images from URLs and converts them to the format required by Gemini API\n- **Security**: The server applies restrictions to prevent access to private networks or malicious domains\n- **Performance**: Image analysis may take longer for high-resolution images due to processing complexity\n- **Token usage**: Image dimensions affect token consumption - larger images use more tokens\n\n### YouTube Video Analysis Examples\n\nThese examples demonstrate how to analyze YouTube videos using Gemini's video understanding capabilities. The server can process publicly accessible YouTube videos by providing their URLs. Note that only public YouTube videos are supported - private, unlisted, or region-restricted videos cannot be analyzed.\n\n**Example 25: Basic YouTube Video Analysis**\n\n```xml\n<use_mcp_tool>\n  <server_name>gemini-server</server_name>\n  <tool_name>gemini_generateContent</tool_name>\n  <arguments>\n    {\n      \"prompt\": \"Please analyze this YouTube video and provide a comprehensive summary including the main topics discussed, key points, and overall theme.\",\n      \"urlContext\": {\n        \"urls\": [\"https://www.youtube.com/watch?v=dQw4w9WgXcQ\"],\n        \"fetchOptions\": {\n          \"includeMetadata\": true,\n          \"timeoutMs\": 30000\n        }\n      }\n    }\n  </arguments>\n</use_mcp_tool>\n```\n\n**Example 26: YouTube Video Content Extraction with Timestamps**\n\n```xml\n<use_mcp_tool>\n  <server_name>gemini-server</server_name>\n  <tool_name>gemini_generateContent</tool_name>\n  <arguments>\n    {\n      \"prompt\": \"Analyze this educational YouTube video and create a detailed outline with key topics and approximate timestamps. Identify the main learning objectives and key concepts covered.\",\n      \"urlContext\": {\n        \"urls\": [\"https://www.youtube.com/watch?v=EXAMPLE_VIDEO_ID\"],\n        \"fetchOptions\": {\n          \"maxContentKb\": 300,\n          \"includeMetadata\": true,\n          \"timeoutMs\": 45000\n        }\n      },\n      \"modelPreferences\": {\n        \"preferQuality\": true,\n        \"taskType\": \"reasoning\"\n      }\n    }\n  </arguments>\n</use_mcp_tool>\n```\n\n**Example 27: YouTube Video Analysis with Specific Questions**\n\n```xml\n<use_mcp_tool>\n  <server_name>gemini-server</server_name>\n  <tool_name>gemini_generateContent</tool_name>\n  <arguments>\n    {\n      \"prompt\": \"Watch this YouTube video and answer these specific questions: 1) What is the main message or thesis? 2) Who is the target audience? 3) What evidence or examples are provided? 4) What are the key takeaways? 5) How is the content structured?\",\n      \"urlContext\": {\n        \"urls\": [\"https://www.youtube.com/watch?v=EXAMPLE_VIDEO_ID\"]\n      }\n    }\n  </arguments>\n</use_mcp_tool>\n```\n\n**Example 28: Comparative Analysis of Multiple YouTube Videos**\n\n```xml\n<use_mcp_tool>\n  <server_name>gemini-server</server_name>\n  <tool_name>gemini_generateContent</tool_name>\n  <arguments>\n    {\n      \"prompt\": \"Compare and contrast these YouTube videos. Analyze their different approaches to the topic, presentation styles, key arguments, and conclusions. Identify similarities and differences in their perspectives.\",\n      \"urlContext\": {\n        \"urls\": [\n          \"https://www.youtube.com/watch?v=VIDEO_ID_1\",\n          \"https://www.youtube.com/watch?v=VIDEO_ID_2\"\n        ],\n        \"fetchOptions\": {\n          \"maxContentKb\": 400,\n          \"includeMetadata\": true,\n          \"timeoutMs\": 60000\n        }\n      },\n      \"modelPreferences\": {\n        \"preferQuality\": true,\n        \"taskType\": \"reasoning\"\n      }\n    }\n  </arguments>\n</use_mcp_tool>\n```\n\n**Example 29: YouTube Video Technical Analysis**\n\n```xml\n<use_mcp_tool>\n  <server_name>gemini-server</server_name>\n  <tool_name>gemini_generateContent</tool_name>\n  <arguments>\n    {\n      \"prompt\": \"Analyze this technical YouTube tutorial and extract step-by-step instructions, identify required tools or materials, note any code examples or commands shown, and highlight important warnings or best practices mentioned.\",\n      \"urlContext\": {\n        \"urls\": [\"https://www.youtube.com/watch?v=TECH_TUTORIAL_ID\"],\n        \"fetchOptions\": {\n          \"includeMetadata\": true,\n          \"timeoutMs\": 40000\n        }\n      }\n    }\n  </arguments>\n</use_mcp_tool>\n```\n\n**Example 30: YouTube Video Sentiment and Style Analysis**\n\n```xml\n<use_mcp_tool>\n  <server_name>gemini-server</server_name>\n  <tool_name>gemini_url_analysis</tool_name>\n  <arguments>\n    {\n      \"urls\": [\"https://www.youtube.com/watch?v=EXAMPLE_VIDEO_ID\"],\n      \"analysisType\": \"sentiment\",\n      \"outputFormat\": \"structured\",\n      \"query\": \"Analyze the tone, mood, and presentation style of this YouTube video. Assess the speaker's credibility, engagement level, and overall effectiveness of communication.\"\n    }\n  </arguments>\n</use_mcp_tool>\n```\n\n**Example 31: YouTube Video Educational Content Assessment**\n\n```xml\n<use_mcp_tool>\n  <server_name>gemini-server</server_name>\n  <tool_name>gemini_generateContent</tool_name>\n  <arguments>\n    {\n      \"prompt\": \"Evaluate this educational YouTube video for accuracy, clarity, and pedagogical effectiveness. Identify the teaching methods used, assess how well complex concepts are explained, and suggest improvements if any.\",\n      \"urlContext\": {\n        \"urls\": [\"https://www.youtube.com/watch?v=EDUCATIONAL_VIDEO_ID\"],\n        \"fetchOptions\": {\n          \"maxContentKb\": 250,\n          \"includeMetadata\": true\n        }\n      },\n      \"modelPreferences\": {\n        \"preferQuality\": true,\n        \"taskType\": \"reasoning\"\n      }\n    }\n  </arguments>\n</use_mcp_tool>\n```\n\n**Example 32: YouTube Video with Domain Security Restrictions**\n\n```xml\n<use_mcp_tool>\n  <server_name>gemini-server</server_name>\n  <tool_name>gemini_generateContent</tool_name>\n  <arguments>\n    {\n      \"prompt\": \"Analyze the content and key messages of this YouTube video, focusing on factual accuracy and source credibility.\",\n      \"urlContext\": {\n        \"urls\": [\"https://www.youtube.com/watch?v=TRUSTED_VIDEO_ID\"],\n        \"fetchOptions\": {\n          \"allowedDomains\": [\"youtube.com\", \"www.youtube.com\"],\n          \"maxContentKb\": 200,\n          \"timeoutMs\": 35000,\n          \"includeMetadata\": true\n        }\n      }\n    }\n  </arguments>\n</use_mcp_tool>\n```\n\n**Important Notes for YouTube Video Analysis:**\n- **Public videos only**: Only publicly accessible YouTube videos can be analyzed\n- **URL format**: Use standard YouTube URLs (youtube.com/watch?v=VIDEO_ID or youtu.be/VIDEO_ID)\n- **Processing time**: Video analysis typically takes longer than text or image analysis\n- **Content limitations**: Very long videos may have content truncated or processed in segments\n- **Metadata**: Video metadata (title, description, duration) is included when `includeMetadata: true`\n- **Language support**: Gemini can analyze videos in multiple languages\n- **Content restrictions**: The server applies the same security restrictions as other URL content\n- **Token usage**: Video analysis can consume significant tokens depending on video length and complexity\n\n## Supported Multimedia Analysis Use Cases\n\nThe MCP Gemini Server supports comprehensive multimedia analysis through URL-based processing, leveraging Google Gemini's advanced vision and video understanding capabilities. Below are the key use cases organized by content type:\n\n### Image Analysis Use Cases\n\n**Content Understanding:**\n- **Product Analysis**: Analyze product images for features, design elements, and quality assessment\n- **Document OCR**: Extract and transcribe text from images of documents, receipts, and forms\n- **Chart & Graph Analysis**: Interpret data visualizations, extract key insights, and explain trends\n- **Technical Diagrams**: Understand architectural diagrams, flowcharts, and technical schematics\n- **Medical Images**: Analyze medical charts, X-rays, and diagnostic images (for educational purposes)\n- **Art & Design**: Analyze artistic compositions, color schemes, and design principles\n\n**Comparative Analysis:**\n- **Before/After Comparisons**: Compare multiple images to identify changes and differences\n- **Product Comparisons**: Analyze multiple product images for feature comparison\n- **A/B Testing**: Evaluate design variations and visual differences\n\n**Security & Quality:**\n- **Content Moderation**: Identify inappropriate or harmful visual content\n- **Quality Assessment**: Evaluate image quality, resolution, and technical aspects\n- **Brand Compliance**: Check images for brand guideline adherence\n\n### Video Analysis Use Cases\n\n**Educational Content:**\n- **Lecture Analysis**: Extract key concepts, create summaries, and identify important timestamps\n- **Tutorial Understanding**: Break down step-by-step instructions and highlight key procedures\n- **Training Materials**: Analyze corporate training videos and extract learning objectives\n- **Academic Research**: Process research presentations and extract methodologies\n\n**Content Creation:**\n- **Video Summarization**: Generate concise summaries of long-form video content\n- **Transcript Generation**: Create detailed transcripts with speaker identification\n- **Content Categorization**: Classify videos by topic, genre, or content type\n- **Sentiment Analysis**: Assess emotional tone and audience engagement indicators\n\n**Technical Analysis:**\n- **Software Demonstrations**: Extract software features and usage instructions\n- **Product Reviews**: Analyze product demonstration videos and extract key insights\n- **Troubleshooting Guides**: Parse technical support videos for problem-solving steps\n- **Code Reviews**: Analyze programming tutorial videos and extract code examples\n\n**Business Intelligence:**\n- **Market Research**: Analyze promotional videos and marketing content\n- **Competitive Analysis**: Study competitor video content and strategies\n- **Customer Feedback**: Process video testimonials and feedback sessions\n- **Event Coverage**: Analyze conference presentations and keynote speeches\n\n### Integration Capabilities\n\n**Multi-Modal Analysis:**\n- Combine text prompts with image/video URLs for contextual analysis\n- Process multiple media types in single requests for comprehensive insights\n- Cross-reference visual content with textual instructions\n\n**Workflow Integration:**\n- Chain multiple analysis operations for complex workflows\n- Export results to files for further processing\n- Integrate with external MCP servers for extended functionality\n\n**Security & Performance:**\n- URL validation and security screening for safe content processing\n- Caching support for frequently analyzed content\n- Batch processing capabilities for multiple media items\n\n**Example 12: Connecting to an External MCP Server (SSE)**\n\n```xml\n<use_mcp_tool>\n  <server_name>gemini-server</server_name>\n  <tool_name>mcpConnectToServer</tool_name>\n  <arguments>\n    {\n      \"serverId\": \"my-external-server\",\n      \"connectionType\": \"sse\",\n      \"sseUrl\": \"http://localhost:8080/mcp\"\n    }\n  </arguments>\n</use_mcp_tool>\n```\n\n*(Assume response contains a unique connection ID like: `connectionId: \"12345-abcde-67890\"`)*\n\n**Example 13: Calling a Tool on an External MCP Server and Writing Output to File**\n\n```xml\n<use_mcp_tool>\n  <server_name>gemini-server</server_name>\n  <tool_name>mcpCallServerTool</tool_name>\n  <arguments>\n    {\n      \"connectionId\": \"12345-abcde-67890\", // Use the connectionId returned by mcpConnectToServer\n      \"toolName\": \"remote_tool_name\",\n      \"toolArgs\": { \"param1\": \"value1\" },\n      \"outputToFile\": \"/var/opt/mcp-gemini-server/outputs/result.json\"\n    }\n  </arguments>\n</use_mcp_tool>\n```\n\n**Important: The `connectionId` used in MCP client tools must be the connection identifier returned by `mcpConnectToServer`, not the original `serverId` parameter.**\n\n**Note:** The `outputToFile` path must be within one of the directories specified in the `ALLOWED_OUTPUT_PATHS` environment variable. For example, if `ALLOWED_OUTPUT_PATHS=\"/path/to/allowed/output,/another/allowed/path\"`, then the file path must be a subdirectory of one of these paths.\n\n**Example 14: Writing Content Directly to a File**\n\n```xml\n<use_mcp_tool>\n  <server_name>gemini-server</server_name>\n  <tool_name>writeToFile</tool_name>\n  <arguments>\n    {\n      \"filePath\": \"/path/to/allowed/output/my_notes.txt\",\n      \"content\": \"This is some important content.\",\n      \"overwrite\": true\n    }\n  </arguments>\n</use_mcp_tool>\n```\n\n**Note:** Like with `mcpCallServerTool`, the `filePath` must be within one of the directories specified in the `ALLOWED_OUTPUT_PATHS` environment variable. This is a critical security feature to prevent unauthorized file writes.\n\n## `mcp-gemini-server` and Gemini SDK's MCP Function Calling\n\nThe official Google Gemini API documentation includes examples (such as for [function calling with MCP structure](https://ai.google.dev/gemini-api/docs/function-calling?example=weather#model_context_protocol_mcp)) that demonstrate how you can use the client-side Gemini SDK (e.g., in Python or Node.js) to interact with the Gemini API. In such scenarios, particularly for function calling, the client SDK itself can be used to structure requests and handle responses in a manner that aligns with MCP principles.\n\nThe `mcp-gemini-server` project offers a complementary approach by providing a **fully implemented, standalone MCP server**. Instead of your client application directly using the Gemini SDK to format MCP-style messages for the Gemini API, your client application (which could be another LLM like Claude, a custom script, or any MCP-compatible system) would:\n\n1.  Connect to an instance of this `mcp-gemini-server`.\n2.  Call the pre-defined MCP tools exposed by this server, such as `gemini_functionCall`, `gemini_generateContent`, etc.\n\nThis `mcp-gemini-server` then internally handles all the necessary interactions with the Google Gemini API, including structuring the requests, managing API keys, and processing responses, abstracting these details away from your MCP client.\n\n### Benefits of using `mcp-gemini-server`:\n\n*   **Abstraction & Simplicity:** Client applications don't need to integrate the Gemini SDK directly or manage the specifics of its API for MCP-style interactions. They simply make standard MCP tool calls.\n*   **Centralized Configuration:** API keys, default model choices, safety settings, and other configurations are managed centrally within the `mcp-gemini-server`.\n*   **Rich Toolset:** Provides a broad set of pre-defined MCP tools for various Gemini features (text generation, chat, file handling, image generation, etc.), not just function calling.\n*   **Interoperability:** Enables any MCP-compatible client to leverage Gemini's capabilities without needing native Gemini SDK support.\n\n### When to Choose Which Approach:\n\n*   **Direct SDK Usage (as in Google's MCP examples):**\n    *   Suitable if you are building a client application (e.g., in Python or Node.js) and want fine-grained control over the Gemini API interaction directly within that client.\n    *   Useful if you prefer to manage the Gemini SDK dependencies and logic within your client application and are primarily focused on function calling structured in an MCP-like way.\n*   **Using `mcp-gemini-server`:**\n    *   Ideal if you want to expose Gemini capabilities to an existing MCP-compatible ecosystem (e.g., another LLM, a workflow automation system).\n    *   Beneficial if you want to rapidly prototype or deploy Gemini features as tools without extensive client-side SDK integration.\n    *   Preferable if you need a wider range of Gemini features exposed as consistent MCP tools and want to centralize the Gemini API interaction point.\n\n### A Note on This Server's Own MCP Client Tools:\n\nThe `mcp-gemini-server` also includes tools like `mcpConnectToServer`, `mcpListServerTools`, and `mcpCallServerTool`. These tools allow *this server* to act as an MCP *client* to *other external* MCP servers. This is a distinct capability from how an MCP client would connect *to* `mcp-gemini-server` to utilize Gemini features.\n\n## Environment Variables\n\n### Required:\n- `GOOGLE_GEMINI_API_KEY`: Your Google Gemini API key (required)\n\n### Required for Production (unless NODE_ENV=test):\n- `MCP_SERVER_HOST`: Server host address (e.g., \"localhost\")\n- `MCP_SERVER_PORT`: Port for network transports (e.g., \"8080\")\n- `MCP_CONNECTION_TOKEN`: A strong, unique shared secret token that clients must provide when connecting to this server. This is NOT provided by Google or any external service - you must generate it yourself using a cryptographically secure method. See the installation instructions (step 4) for generation methods. This token must be identical on both the server and all connecting clients.\n\n### Optional - Gemini API Configuration:\n- `GOOGLE_GEMINI_MODEL`: Default model to use (e.g., `gemini-1.5-pro-latest`, `gemini-1.5-flash`)\n- `GOOGLE_GEMINI_DEFAULT_THINKING_BUDGET`: Default thinking budget in tokens (0-24576) for controlling model reasoning\n\n### Optional - URL Context Configuration:\n- `GOOGLE_GEMINI_ENABLE_URL_CONTEXT`: Enable URL context features (options: `true`, `false`; default: `false`)\n- `GOOGLE_GEMINI_URL_MAX_COUNT`: Maximum URLs per request (default: `20`)\n- `GOOGLE_GEMINI_URL_MAX_CONTENT_KB`: Maximum content size per URL in KB (default: `100`)\n- `GOOGLE_GEMINI_URL_FETCH_TIMEOUT_MS`: Fetch timeout per URL in milliseconds (default: `10000`)\n- `GOOGLE_GEMINI_URL_ALLOWED_DOMAINS`: Comma-separated list or JSON array of allowed domains (default: `*` for all domains)\n- `GOOGLE_GEMINI_URL_BLOCKLIST`: Comma-separated list or JSON array of blocked domains (default: empty)\n- `GOOGLE_GEMINI_URL_CONVERT_TO_MARKDOWN`: Convert HTML content to markdown (options: `true`, `false`; default: `true`)\n- `GOOGLE_GEMINI_URL_INCLUDE_METADATA`: Include URL metadata in context (options: `true`, `false`; default: `true`)\n- `GOOGLE_GEMINI_URL_ENABLE_CACHING`: Enable URL content caching (options: `true`, `false`; default: `true`)\n- `GOOGLE_GEMINI_URL_USER_AGENT`: Custom User-Agent header for URL requests (default: `MCP-Gemini-Server/1.0`)\n\n### Optional - Security Configuration:\n- `ALLOWED_OUTPUT_PATHS`: A comma-separated list of absolute paths to directories where tools like `mcpCallServerTool` (with outputToFile parameter) and `writeToFileTool` are allowed to write files. Critical security feature to prevent unauthorized file writes. If not set, file output will be disabled for these tools.\n\n### Optional - Server Configuration:\n- `MCP_CLIENT_ID`: Default client ID used when this server acts as a client to other MCP servers (defaults to \"gemini-sdk-client\")\n- `MCP_TRANSPORT`: Transport to use for MCP server (options: `stdio`, `sse`, `streamable`, `http`; default: `stdio`)\n  - IMPORTANT: SSE (Server-Sent Events) is NOT deprecated and remains a critical component of the MCP protocol\n  - SSE is particularly valuable for bidirectional communication, enabling features like dynamic tool updates and sampling\n  - Each transport type has specific valid use cases within the MCP ecosystem\n- `MCP_LOG_LEVEL`: Log level for MCP operations (options: `debug`, `info`, `warn`, `error`; default: `info`)\n- `MCP_ENABLE_STREAMING`: Enable SSE streaming for HTTP transport (options: `true`, `false`; default: `false`)\n- `MCP_SESSION_TIMEOUT`: Session timeout in seconds for HTTP transport (default: `3600` = 1 hour)\n- `SESSION_STORE_TYPE`: Session storage backend (`memory` or `sqlite`; default: `memory`)\n- `SQLITE_DB_PATH`: Path to SQLite database file when using sqlite store (default: `./data/sessions.db`)\n\n### Optional - GitHub Integration:\n- `GITHUB_API_TOKEN`: Personal Access Token for GitHub API access (required for GitHub code review features). For public repos, token needs 'public_repo' and 'read:user' scopes. For private repos, token needs 'repo' scope.\n\n### Optional - Legacy Server Configuration (Deprecated):\n- `MCP_TRANSPORT_TYPE`: Deprecated - Use `MCP_TRANSPORT` instead\n- `MCP_WS_PORT`: Deprecated - Use `MCP_SERVER_PORT` instead\n- `ENABLE_HEALTH_CHECK`: Enable health check server (options: `true`, `false`; default: `true`)\n- `HEALTH_CHECK_PORT`: Port for health check HTTP server (default: `3000`)\n\nYou can create a `.env` file in the root directory with these variables:\n\n```env\n# Required API Configuration\nGOOGLE_GEMINI_API_KEY=your_api_key_here\n\n# Required for Production (unless NODE_ENV=test)\nMCP_SERVER_HOST=localhost\nMCP_SERVER_PORT=8080\nMCP_CONNECTION_TOKEN=your_secure_token_here\n\n# Optional API Configuration\nGOOGLE_GEMINI_MODEL=gemini-1.5-pro-latest\nGOOGLE_GEMINI_DEFAULT_THINKING_BUDGET=4096\n\n# Security Configuration\nALLOWED_OUTPUT_PATHS=/var/opt/mcp-gemini-server/outputs,/tmp/mcp-gemini-outputs   # For mcpCallServerTool and writeToFileTool\n\n# URL Context Configuration\nGOOGLE_GEMINI_ENABLE_URL_CONTEXT=true  # Enable URL context features\nGOOGLE_GEMINI_URL_MAX_COUNT=20          # Maximum URLs per request\nGOOGLE_GEMINI_URL_MAX_CONTENT_KB=100    # Maximum content size per URL in KB\nGOOGLE_GEMINI_URL_FETCH_TIMEOUT_MS=10000 # Fetch timeout per URL in milliseconds\nGOOGLE_GEMINI_URL_ALLOWED_DOMAINS=*     # Allowed domains (* for all, or comma-separated list)\nGOOGLE_GEMINI_URL_BLOCKLIST=malicious.com,spam.net # Blocked domains (comma-separated)\nGOOGLE_GEMINI_URL_CONVERT_TO_MARKDOWN=true # Convert HTML to markdown\nGOOGLE_GEMINI_URL_INCLUDE_METADATA=true # Include URL metadata in context\nGOOGLE_GEMINI_URL_ENABLE_CACHING=true   # Enable URL content caching\nGOOGLE_GEMINI_URL_USER_AGENT=MCP-Gemini-Server/1.0 # Custom User-Agent\n\n# Server Configuration\nMCP_CLIENT_ID=gemini-sdk-client  # Optional: Default client ID for MCP connections (defaults to \"gemini-sdk-client\")\nMCP_TRANSPORT=stdio  # Options: stdio, sse, streamable, http (replaced deprecated MCP_TRANSPORT_TYPE)\nMCP_LOG_LEVEL=info   # Optional: Log level for MCP operations (debug, info, warn, error)\nMCP_ENABLE_STREAMING=true # Enable SSE streaming for HTTP transport\nMCP_SESSION_TIMEOUT=3600  # Session timeout in seconds for HTTP transport\nSESSION_STORE_TYPE=memory  # Options: memory, sqlite\nSQLITE_DB_PATH=./data/sessions.db  # Path to SQLite database file when using sqlite store\nENABLE_HEALTH_CHECK=true\nHEALTH_CHECK_PORT=3000\n\n# GitHub Integration\nGITHUB_API_TOKEN=your_github_token_here\n```\n\n## Security Considerations\n\nThis server implements several security measures to protect against common vulnerabilities. Understanding these security features is critical when deploying in production environments.\n\n### File System Security\n\n1. **Path Validation and Isolation**\n   - **ALLOWED_OUTPUT_PATHS**: Critical security feature that restricts where file writing tools can write files\n   - **Security Principle**: Files can only be created, read, or modified within explicitly allowed directories\n   - **Production Requirement**: Always use absolute paths to prevent potential directory traversal attacks\n\n2. **Path Traversal Protection**\n   - The `FileSecurityService` implements robust path traversal protection by:\n     - Fully resolving paths to their absolute form\n     - Normalizing paths to handle \"..\" and \".\" segments properly\n     - Validating that normalized paths stay within allowed directories\n     - Checking both string-based prefixes and relative path calculations for redundant security\n\n3. **Symlink Security**\n   - Symbolic links are fully resolved and checked against allowed directories\n   - Both the symlink itself and its target are validated\n   - Parent directory symlinks are iteratively checked to prevent circumvention \n   - Multi-level symlink chains are fully resolved before validation\n\n### Authentication & Authorization\n\n1. **Connection Tokens**\n   - `MCP_CONNECTION_TOKEN` provides basic authentication for clients connecting to this server\n   - Should be treated as a secret and use a strong, unique value in production\n\n2. **API Key Security**\n   - `GOOGLE_GEMINI_API_KEY` grants access to Google Gemini API services\n   - Must be kept secure and never exposed in client-side code or logs\n   - Use environment variables or secure secret management systems to inject this value\n\n### URL Context Security\n\n1. **Multi-Layer URL Validation**\n   - **Protocol Validation**: Only HTTP/HTTPS protocols are allowed\n   - **Private Network Protection**: Blocks access to localhost, private IP ranges, and internal domains\n   - **Domain Control**: Configurable allowlist/blocklist with wildcard support\n   - **Suspicious Pattern Detection**: Identifies potential path traversal, dangerous characters, and malicious patterns\n   - **IDN Homograph Attack Prevention**: Detects potentially confusing Unicode domain names\n\n2. **Rate Limiting and Resource Protection**\n   - **Per-domain rate limiting**: Default 10 requests per minute per domain\n   - **Content size limits**: Configurable maximum content size per URL (default 100KB)\n   - **Request timeout controls**: Prevents hanging requests (default 10 seconds)\n   - **Concurrent request limits**: Controlled batch processing to prevent overload\n\n3. **Content Security**\n   - **Content type validation**: Only processes text-based content types\n   - **HTML sanitization**: Removes script tags, style blocks, and dangerous content\n   - **Metadata extraction**: Safely parses HTML metadata without executing code\n   - **Memory protection**: Content truncation prevents memory exhaustion attacks\n\n### Network Security\n\n1. **Transport Options**\n   - stdio: Provides process isolation when used as a spawned child process\n   - SSE/HTTP: Ensure proper network-level protection when exposing over networks\n\n2. **Port Configuration**\n   - Configure firewall rules appropriately when exposing server ports\n   - Consider reverse proxies with TLS termination for production deployments\n\n### Production Deployment Recommendations\n\n1. **File Paths**\n   - Always use absolute paths for `ALLOWED_OUTPUT_PATHS`\n   - Use paths outside the application directory to prevent source code modification\n   - Restrict to specific, limited-purpose directories with appropriate permissions\n   - NEVER include sensitive system directories like \"/\", \"/etc\", \"/usr\", \"/bin\", or \"/home\"\n\n2. **Process Isolation**\n   - Run the server with restricted user permissions\n   - Consider containerization (Docker) for additional isolation\n\n3. **Secrets Management**\n   - Use a secure secrets management solution instead of .env files in production\n   - Rotate API keys and connection tokens regularly\n\n4. **URL Context Security**\n   - Enable URL context only when needed: Set `GOOGLE_GEMINI_ENABLE_URL_CONTEXT=false` if not required\n   - Use restrictive domain allowlists: Avoid `GOOGLE_GEMINI_URL_ALLOWED_DOMAINS=*` in production\n   - Configure comprehensive blocklists: Add known malicious domains to `GOOGLE_GEMINI_URL_BLOCKLIST`\n   - Set conservative resource limits: Use appropriate values for `GOOGLE_GEMINI_URL_MAX_CONTENT_KB` and `GOOGLE_GEMINI_URL_MAX_COUNT`\n   - Monitor URL access patterns: Review logs for suspicious URL access attempts\n   - Consider network-level protection: Use firewalls or proxies to add additional URL filtering\n\n## Error Handling\n\nThe server provides enhanced error handling using the MCP standard `McpError` type when tool execution fails. This object contains:\n\n* `code`: An `ErrorCode` enum value indicating the type of error:\n  * `InvalidParams`: Parameter validation errors (wrong type, missing required field, etc.)\n  * `InvalidRequest`: General request errors, including safety blocks and not found resources \n  * `PermissionDenied`: Authentication or authorization failures\n  * `ResourceExhausted`: Rate limits, quotas, or resource capacity issues\n  * `FailedPrecondition`: Operations that require conditions that aren't met\n  * `InternalError`: Unexpected server or API errors\n* `message`: A human-readable description of the error with specific details.\n* `details`: (Optional) An object with more specific information from the Gemini SDK error.\n\n### Implementation Details\n\nThe server uses a multi-layered approach to error handling:\n\n1. **Validation Layer**: Zod schemas validate all parameters at both the tool level (MCP request) and service layer (before API calls).\n2. **Error Classification**: A detailed error mapping system categorizes errors from the Google GenAI SDK into specific error types:\n   * `GeminiValidationError`: Parameter validation failures \n   * `GeminiAuthError`: Authentication issues\n   * `GeminiQuotaError`: Rate limiting and quota exhaustion\n   * `GeminiContentFilterError`: Content safety filtering\n   * `GeminiNetworkError`: Connection and timeout issues\n   * `GeminiModelError`: Model-specific problems\n3. **Retry Mechanism**: Automatic retry with exponential backoff for transient errors:\n   * Network issues, timeouts, and rate limit errors are automatically retried\n   * Configurable retry parameters (attempts, delay, backoff factor)\n   * Jitter randomization to prevent synchronized retry attempts\n   * Detailed logging of retry attempts for debugging\n\n**Common Error Scenarios:**\n\n* **Authentication Failures:** `PermissionDenied` - Invalid API key, expired credentials, or unauthorized access.\n* **Parameter Validation:** `InvalidParams` - Missing required fields, wrong data types, invalid values.\n* **Safety Blocks:** `InvalidRequest` - Content blocked by safety filters with details indicating `SAFETY` as the block reason.\n* **File/Cache Not Found:** `InvalidRequest` - Resource not found, with details about the missing resource.\n* **Rate Limits:** `ResourceExhausted` - API quota exceeded or rate limits hit, with details about limits.\n* **File API Unavailable:** `FailedPrecondition` - When attempting File API operations without a valid Google AI Studio key.\n* **Path Traversal Security:** `InvalidParams` - Attempts to access audio files outside the allowed directory with details about the security validation failure.\n* **Image/Audio Processing Errors:** \n  * `InvalidParams` - For format issues, size limitations, or invalid inputs\n  * `InternalError` - For processing failures during analysis\n  * `ResourceExhausted` - For resource-intensive operations exceeding limits\n\nThe server includes additional context in error messages to help with troubleshooting, including session IDs for chat-related errors and specific validation details for parameter errors.\n\nCheck the `message` and `details` fields of the returned `McpError` for specific troubleshooting information.\n\n## Development and Testing\n\nThis server includes a comprehensive test suite to ensure functionality and compatibility with the Gemini API. The tests are organized into unit tests (for individual components) and integration tests (for end-to-end functionality).\n\n### Test Structure\n\n- **Unit Tests**: Located in `tests/unit/` - Test individual components in isolation with mocked dependencies\n- **Integration Tests**: Located in `tests/integration/` - Test end-to-end functionality with real server interaction\n- **Test Utilities**: Located in `tests/utils/` - Helper functions and fixtures for testing\n\n### Running Tests\n\n```bash\n# Install dependencies first\nnpm install\n\n# Run all tests\nnpm run test\n\n# Run only unit tests\nnpm run test:unit\n\n# Run only integration tests\nnpm run test:integration\n\n# Run a specific test file\nnode --test --loader ts-node/esm tests/path/to/test-file.test.ts\n```\n\n### Testing Approach\n\n1. **Service Mocking**: The tests use a combination of direct method replacement and mock interfaces to simulate the Gemini API response. This is particularly important for the `@google/genai` SDK (v0.10.0) which has a complex object structure.\n\n2. **Environmental Variables**: Tests automatically check for required environment variables and will skip tests that require API keys if they're not available. This allows core functionality to be tested without credentials.\n\n3. **Test Server**: Integration tests use a test server fixture that creates an isolated HTTP server instance with the MCP handler configured for testing.\n\n4. **RetryService**: The retry mechanism is extensively tested to ensure proper handling of transient errors with exponential backoff, jitter, and configurable retry parameters.\n\n5. **Image Generation**: Tests specifically address the complex interactions with the Gemini API for image generation, supporting both Gemini models and the dedicated Imagen 3.1 model.\n\n### Test Environment Setup\n\nFor running tests that require API access, create a `.env.test` file in the project root with the following variables:\n\n```env\n# Required for basic API tests\nGOOGLE_GEMINI_API_KEY=your_api_key_here\n\n# Required for router tests\nGOOGLE_GEMINI_MODEL=gemini-1.5-flash\n```\n\nThe test suite will automatically detect available environment variables and skip tests that require missing configuration.\n\n## Contributing\n\nWe welcome contributions to improve the MCP Gemini Server! This section provides guidelines for contributing to the project.\n\n### Development Environment Setup\n\n1. **Fork and Clone the Repository**\n   ```bash\n   git clone https://github.com/yourusername/mcp-gemini-server.git\n   cd mcp-gemini-server\n   ```\n\n2. **Install Dependencies**\n   ```bash\n   npm install\n   ```\n\n3. **Set Up Environment Variables**\n   Create a `.env` file in the project root with the necessary variables as described in the Environment Variables section.\n\n4. **Build and Run**\n   ```bash\n   npm run build\n   npm run dev\n   ```\n\n### Development Process\n\n1. **Create a Feature Branch**\n   ```bash\n   git checkout -b feature/your-feature-name\n   ```\n\n2. **Make Your Changes**\n   Implement your feature or fix, following the code style guidelines.\n\n3. **Write Tests**\n   Add tests for your changes to ensure functionality and prevent regressions.\n\n4. **Run Tests and Linting**\n   ```bash\n   npm run test\n   npm run lint\n   npm run format\n   ```\n\n5. **Commit Your Changes**\n   Use clear, descriptive commit messages that explain the purpose of your changes.\n\n### Testing Guidelines\n\n- Write unit tests for all new functionality\n- Update existing tests when modifying functionality\n- Ensure all tests pass before submitting a pull request\n- Include both positive and negative test cases\n- Mock external dependencies to ensure tests can run without external services\n\n### Pull Request Process\n\n1. **Update Documentation**\n   Update the README.md and other documentation to reflect your changes.\n\n2. **Submit a Pull Request**\n   - Provide a clear description of the changes\n   - Link to any related issues\n   - Explain how to test the changes\n   - Ensure all CI checks pass\n\n3. **Code Review**\n   - Address any feedback from reviewers\n   - Make requested changes and update the PR\n\n### Coding Standards\n\n- Follow the existing code style (PascalCase for classes/interfaces/types, camelCase for functions/variables)\n- Use strong typing with TypeScript interfaces\n- Document public APIs with JSDoc comments\n- Handle errors properly by extending base error classes\n- Follow the service-based architecture with dependency injection\n- Use Zod for schema validation\n- Format code according to the project's ESLint and Prettier configuration\n\n## Code Review Tools\n\nThe MCP Gemini Server provides powerful code review capabilities leveraging Gemini's models to analyze git diffs and GitHub repositories. These tools help identify potential issues, suggest improvements, and provide comprehensive feedback on code changes.\n\n### Local Git Diff Review\n\nReview local git changes directly from your command line:\n\n```bash\n# Using the included CLI script\n./scripts/gemini-review.sh\n\n# Options\n./scripts/gemini-review.sh --focus=security --reasoning=high\n```\n\nThe CLI script supports various options:\n- `--focus=FOCUS`: Focus of the review (security, performance, architecture, bugs, general)\n- `--model=MODEL`: Model to use (defaults to gemini-flash-2.0 for cost efficiency)\n- `--reasoning=LEVEL`: Reasoning effort (none, low, medium, high)\n- `--exclude=PATTERN`: Files to exclude using glob patterns\n\n### GitHub Repository Review\n\nReview GitHub repositories, branches, and pull requests using the following tools:\n\n- **GitHub PR Review Tool**: Analyzes pull requests for issues and improvements\n- **GitHub Repository Review Tool**: Analyzes entire repositories or branches\n\n### Cost Optimization\n\nBy default, code review tools use the more cost-efficient `gemini-flash-2.0` model, which offers a good balance between cost and capability for most code review tasks. For particularly complex code bases or when higher reasoning depth is needed, you can specify more powerful models:\n\n```bash\n# Using a more powerful model for complex code\n./scripts/gemini-review.sh --model=gemini-1.5-pro --reasoning=high\n```\n\n### Running Tests\n\nTests for the GitHub code review functionality can also use the cheaper model:\n\n```bash\n# Run tests with the default gemini-flash-2.0 model\nnpm run test:unit\n```\n\n## Server Features\n\n### Health Check Endpoint\n\nThe server provides a built-in health check HTTP endpoint that can be used for monitoring and status checks. This is separate from the MCP server transport and runs as a lightweight HTTP server.\n\nWhen enabled, you can access the health check at:\n```\nhttp://localhost:3000/health\n```\n\nThe health check endpoint returns a JSON response with the following information:\n```json\n{\n  \"status\": \"running\",\n  \"uptime\": 1234,  // Seconds since the server started\n  \"transport\": \"StdioServerTransport\",  // Current transport type\n  \"version\": \"0.1.0\"  // Server version\n}\n```\n\nYou can check the health endpoint using curl:\n```bash\ncurl http://localhost:3000/health\n```\n\nYou can configure the health check using these environment variables:\n- `ENABLE_HEALTH_CHECK`: Set to \"false\" to disable the health check server (default: \"true\")\n- `HEALTH_CHECK_PORT`: Port number for the health check server (default: 3000)\n\n### Session Persistence\n\nThe server supports persistent session storage for HTTP/SSE transports, allowing sessions to survive server restarts and enabling horizontal scaling.\n\n#### Storage Backends\n\n1. **In-Memory Store (Default)**\n   - Sessions stored in server memory\n   - Fast performance for development\n   - Sessions lost on server restart\n   - No external dependencies\n\n2. **SQLite Store**\n   - Sessions persisted to local SQLite database\n   - Survives server restarts\n   - Automatic cleanup of expired sessions\n   - Good for single-instance production deployments\n\n#### Configuration\n\nEnable SQLite session persistence:\n```bash\nexport SESSION_STORE_TYPE=sqlite\nexport SQLITE_DB_PATH=./data/sessions.db  # Optional, this is the default\n```\n\nThe SQLite database file and directory will be created automatically on first use. The database includes:\n- Automatic indexing for performance\n- Built-in cleanup of expired sessions\n- ACID compliance for data integrity\n\n#### Session Lifecycle\n\n- Sessions are created when clients connect via HTTP/SSE transport\n- Each session has a configurable timeout (default: 1 hour)\n- Session expiration is extended on each activity\n- Expired sessions are automatically cleaned up every minute\n\n### Graceful Shutdown\n\nThe server implements graceful shutdown handling for SIGTERM and SIGINT signals. When the server receives a shutdown signal:\n\n1. It attempts to properly disconnect the MCP server transport\n2. It closes the health check server if running\n3. It logs the shutdown status\n4. It exits with the appropriate exit code (0 for successful shutdown, 1 if errors occurred)\n\nThis ensures clean termination when the server is run in containerized environments or when stopped manually.\n\n## Known Issues\n\n* **Pagination Issues:** `gemini_listCaches` may not reliably return `nextPageToken` due to limitations in iterating the SDK's Pager object. A workaround is implemented but has limited reliability.\n* **Path Requirements:** Audio transcription operations require absolute paths when run from the server environment. Relative paths are not supported.\n* **File Size Limitations:** Audio files for transcription are limited to 20MB (original file size, before base64 encoding). The server reads the file and converts it to base64 internally. Larger files will be rejected with an error message.\n* **API Compatibility:** Caching API is **not supported with Vertex AI credentials**, only Google AI Studio API keys.\n* **Model Support:** This server is primarily tested and optimized for the latest Gemini 1.5 and 2.5 models. While other models should work, these models are the primary focus for testing and feature compatibility.\n* **TypeScript Build Issues:** The TypeScript build may show errors primarily in test files. These are type compatibility issues that don't affect the runtime functionality. The server itself will function properly despite these build warnings.\n* **Resource Usage:** \n  * Image processing requires significant resource usage, especially for large resolution images. Consider using smaller resolutions (512x512) for faster responses.\n  * Generating multiple images simultaneously increases resource usage proportionally.\n  * Audio transcription is limited to files under 20MB (original file size). The server reads files from disk and handles base64 conversion internally. Processing may take significant time and resources depending on file size and audio complexity.\n* **Content Handling:** \n  * Base64-encoded images are streamed in chunks to handle large file sizes efficiently.\n  * Visual content understanding may perform differently across various types of visual content (charts vs. diagrams vs. documents).\n  * Audio transcription accuracy depends on audio quality, number of speakers, and background noise.\n* **URL Context Features:**\n  * URL context is disabled by default and must be explicitly enabled via `GOOGLE_GEMINI_ENABLE_URL_CONTEXT=true`\n  * JavaScript-rendered content is not supported - only static HTML content is processed\n  * Some websites may block automated access or require authentication that is not currently supported\n  * Content extraction quality may vary depending on website structure and formatting\n  * Rate limiting per domain (10 requests/minute by default) may affect bulk processing scenarios\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "chat",
        "bsmi021",
        "gemini",
        "google gemini",
        "gemini server",
        "mcp gemini"
      ],
      "category": "realtime-collaboration"
    },
    "carterlasalle--mac_messages_mcp": {
      "owner": "carterlasalle",
      "name": "mac_messages_mcp",
      "url": "https://github.com/carterlasalle/mac_messages_mcp",
      "imageUrl": "/freedevtools/mcp/pfp/carterlasalle.webp",
      "description": "Interact with the macOS Messages app to read recent messages, filter them by contact, and send messages via iMessage through a simple API interface.",
      "stars": 167,
      "forks": 20,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-09-29T09:05:28Z",
      "readme_content": "# Mac Messages MCP\n\nA Python bridge for interacting with the macOS Messages app using MCP (Multiple Context Protocol). \n\n[![PyPI Downloads](https://static.pepy.tech/badge/mac-messages-mcp)](https://pepy.tech/projects/mac-messages-mcp)\n\n[![Trust Score](https://archestra.ai/mcp-catalog/api/badge/quality/carterlasalle/mac_messages_mcp)](https://archestra.ai/mcp-catalog/carterlasalle__mac_messages_mcp)\n\n![a-diagram-of-a-mac-computer-with-the-tex_FvvnmbaBTFeKy6F2GMlLqA_IfCBMgJARcia1WTH7FaqwA](https://github.com/user-attachments/assets/dbbdaa14-fadd-434d-a265-9e0c0071c11d)\n\n[![Verified on MseeP](https://mseep.ai/badge.svg)](https://mseep.ai/app/fdc62324-6ac9-44e2-8926-722d1157759a)\n\n\n<a href=\"https://glama.ai/mcp/servers/gxvaoc9znc\">\n  <img alt=\"badge\" width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/gxvaoc9znc/badge\" />\n</a>\n\n## Quick Install\n\n### For Cursor Users\n\n[![Install MCP Server](https://cursor.com/deeplink/mcp-install-light.svg)](https://cursor.com/install-mcp?name=mac-messages-mcp&config=eyJjb21tYW5kIjoidXZ4IG1hYy1tZXNzYWdlcy1tY3AifQ%3D%3D)\n\n*Click the button above to automatically add Mac Messages MCP to Cursor*\n\n### For Claude Desktop Users\n\nSee the [Integration section](#integration) below for setup instructions.\n\n## Features\n\n- **Universal Message Sending**: Automatically sends via iMessage or SMS/RCS based on recipient availability\n- **Smart Fallback**: Seamless fallback to SMS when iMessage is unavailable (perfect for Android users)\n- **Message Reading**: Read recent messages from the macOS Messages app\n- **Contact Filtering**: Filter messages by specific contacts or phone numbers\n- **Fuzzy Search**: Search through message content with intelligent matching\n- **iMessage Detection**: Check if recipients have iMessage before sending\n- **Cross-Platform**: Works with both iPhone/Mac users (iMessage) and Android users (SMS/RCS)\n\n## Prerequisites\n\n- macOS (tested on macOS 11+)\n- Python 3.10+\n- **uv package manager**\n\n### Installing uv\n\nIf you're on Mac, install uv using Homebrew:\n\n```bash\nbrew install uv\n```\n\nOtherwise, follow the installation instructions on the [uv website](https://github.com/astral-sh/uv).\n\n⚠️ **Do not proceed before installing uv**\n\n## Installation\n\n### Full Disk Access Permission\n\n⚠️ This application requires **Full Disk Access** permission for your terminal or application to access the Messages database. \n\nTo grant Full Disk Access:\n1. Open **System Preferences/Settings** > **Security & Privacy/Privacy** > **Full Disk Access**\n2. Click the lock icon to make changes\n3. Add your terminal app (Terminal, iTerm2, etc.) or Claude Desktop/Cursor to the list\n4. Restart your terminal or application after granting permission\n\n## Integration\n\n### Claude Desktop Integration\n\n1. Go to **Claude** > **Settings** > **Developer** > **Edit Config** > **claude_desktop_config.json**\n2. Add the following configuration:\n\n```json\n{\n    \"mcpServers\": {\n        \"messages\": {\n            \"command\": \"uvx\",\n            \"args\": [\n                \"mac-messages-mcp\"\n            ]\n        }\n    }\n}\n```\n\n### Cursor Integration\n\n#### Option 1: One-Click Install (Recommended)\n\n[![Install MCP Server](https://cursor.com/deeplink/mcp-install-light.svg)](https://cursor.com/install-mcp?name=mac-messages-mcp&config=eyJjb21tYW5kIjoidXZ4IG1hYy1tZXNzYWdlcy1tY3AifQ%3D%3D)\n\n#### Option 2: Manual Setup\n\nGo to **Cursor Settings** > **MCP** and paste this as a command:\n\n```\nuvx mac-messages-mcp\n```\n\n⚠️ Only run one instance of the MCP server (either on Cursor or Claude Desktop), not both\n\n### Docker Container Integration\n\nIf you need to connect to `mac-messages-mcp` from a Docker container, you'll need to use the `mcp-proxy` package to bridge the stdio-based server to HTTP.\n\n#### Setup Instructions\n\n1. **Install mcp-proxy on your macOS host:**\n```bash\nnpm install -g mcp-proxy\n```\n\n2. **Start the proxy server:**\n```bash\n# Using the published version\nnpx mcp-proxy uvx mac-messages-mcp --port 8000 --host 0.0.0.0\n\n# Or using local development (if you encounter issues)\nnpx mcp-proxy uv run python -m mac_messages_mcp.server --port 8000 --host 0.0.0.0\n```\n\n3. **Connect from Docker:**\nYour Docker container can now connect to:\n- URL: `http://host.docker.internal:8000/mcp` (on macOS/Windows)\n- URL: `http://<host-ip>:8000/mcp` (on Linux)\n\n4. **Docker Compose example:**\n```yaml\nversion: '3.8'\nservices:\n  your-app:\n    image: your-image\n    environment:\n      MCP_MESSAGES_URL: \"http://host.docker.internal:8000/mcp\"\n    extra_hosts:\n      - \"host.docker.internal:host-gateway\"  # For Linux hosts\n```\n\n5. **Running multiple MCP servers:**\n```bash\n# Terminal 1 - Messages MCP on port 8001\nnpx mcp-proxy uvx mac-messages-mcp --port 8001 --host 0.0.0.0\n\n# Terminal 2 - Another MCP server on port 8002\nnpx mcp-proxy uvx another-mcp-server --port 8002 --host 0.0.0.0\n```\n\n**Note:** Binding to `0.0.0.0` exposes the service to all network interfaces. In production, consider using more restrictive host bindings and adding authentication.\n\n\n### Option 1: Install from PyPI\n\n```bash\nuv pip install mac-messages-mcp\n```\n\n### Option 2: Install from source\n\n```bash\n# Clone the repository\ngit clone https://github.com/carterlasalle/mac_messages_mcp.git\ncd mac_messages_mcp\n\n# Install dependencies\nuv install -e .\n```\n\n\n## Usage\n\n### Smart Message Delivery\n\nMac Messages MCP automatically handles message delivery across different platforms:\n\n- **iMessage Users** (iPhone, iPad, Mac): Messages sent via iMessage\n- **Android Users**: Messages automatically fall back to SMS/RCS\n- **Mixed Groups**: Optimal delivery method chosen per recipient\n\n```python\n# Send to iPhone user - uses iMessage\nsend_message(\"+1234567890\", \"Hey! This goes via iMessage\")\n\n# Send to Android user - automatically uses SMS\nsend_message(\"+1987654321\", \"Hey! This goes via SMS\") \n\n# Check delivery method before sending\ncheck_imessage_availability(\"+1234567890\")  # Returns availability status\n```\n\n### As a Module\n\n```python\nfrom mac_messages_mcp import get_recent_messages, send_message\n\n# Get recent messages\nmessages = get_recent_messages(hours=48)\nprint(messages)\n\n# Send a message (automatically chooses iMessage or SMS)\nresult = send_message(recipient=\"+1234567890\", message=\"Hello from Mac Messages MCP!\")\nprint(result)  # Shows whether sent via iMessage or SMS\n```\n\n### As a Command-Line Tool\n\n```bash\n# Run the MCP server directly\nmac-messages-mcp\n```\n\n## Development\n\n### Versioning\n\nThis project uses semantic versioning. See [VERSIONING.md](VERSIONING.md) for details on how the versioning system works and how to release new versions.\n\nTo bump the version:\n\n```bash\npython scripts/bump_version.py [patch|minor|major]\n```\n\n## Security Notes\n\nThis application accesses the Messages database directly, which contains personal communications. Please use it responsibly and ensure you have appropriate permissions.\n\n[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/carterlasalle-mac-messages-mcp-badge.png)](https://mseep.ai/app/carterlasalle-mac-messages-mcp)\n\n## License\n\nMIT\n\n## Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request. \n## Star History\n\n[![Star History Chart](https://api.star-history.com/svg?repos=carterlasalle/mac_messages_mcp&type=Date)](https://www.star-history.com/#carterlasalle/mac_messages_mcp&Date)\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "mac_messages_mcp",
        "imessage",
        "messages",
        "carterlasalle mac_messages_mcp",
        "mac_messages_mcp interact",
        "macos messages"
      ],
      "category": "realtime-collaboration"
    },
    "chigwell--telegram-mcp": {
      "owner": "chigwell",
      "name": "telegram-mcp",
      "url": "https://github.com/chigwell/telegram-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/chigwell.webp",
      "description": "Interact with Telegram chats by retrieving chat lists, fetching messages, and sending messages through MCP-compatible hosts. Compatible with AI models, allowing seamless integration with Telegram functionalities.",
      "stars": 359,
      "forks": 102,
      "license": "Apache License 2.0",
      "language": "Python",
      "updated_at": "2025-10-04T12:32:19Z",
      "readme_content": "# Telegram MCP Server\n\n![MCP Badge](https://badge.mcpx.dev)\n[![License: Apache 2.0](https://img.shields.io/badge/license-Apache%202.0-green?style=flat-square)](https://opensource.org/licenses/Apache-2.0)\n[![Python Lint & Format Check](https://github.com/chigwell/telegram-mcp/actions/workflows/python-lint-format.yml/badge.svg)](https://github.com/chigwell/telegram-mcp/actions/workflows/python-lint-format.yml)\n[![Docker Build & Compose Validation](https://github.com/chigwell/telegram-mcp/actions/workflows/docker-build.yml/badge.svg)](https://github.com/chigwell/telegram-mcp/actions/workflows/docker-build.yml)\n\n---\n\n## 🤖 MCP in Action\n\nHere's a demonstration of the Telegram MCP capabilities in [Claude](https://docs.anthropic.com/en/docs/agents-and-tools/mcp):\n\n **Basic usage example:**\n\n\n\n1. **Example: Asking Claude to analyze chat history and send a response:**\n\n\n\n2. **Successfully sent message to the group:**\n\n\n\nAs you can see, the AI can seamlessly interact with your Telegram account, retrieving and displaying your chats, messages, and other data in a natural way.\n\n---\n\nA full-featured Telegram integration for Claude, Cursor, and any MCP-compatible client, powered by [Telethon](https://docs.telethon.dev/) and the [Model Context Protocol (MCP)](https://modelcontextprotocol.io/). This project lets you interact with your Telegram account programmatically, automating everything from messaging to group management.\n\n\n---\n\n## 🚀 Features & Tools\n\nThis MCP server exposes a huge suite of Telegram tools. **Every major Telegram/Telethon feature is available as a tool!**\n\n### Chat & Group Management\n- **get_chats(page, page_size)**: Paginated list of chats\n- **list_chats(chat_type, limit)**: List chats with metadata and filtering\n- **get_chat(chat_id)**: Detailed info about a chat\n- **create_group(title, user_ids)**: Create a new group\n- **create_channel(title, about, megagroup)**: Create a channel or supergroup\n- **edit_chat_title(chat_id, title)**: Change chat/group/channel title\n- **delete_chat_photo(chat_id)**: Remove chat/group/channel photo\n- **leave_chat(chat_id)**: Leave a group or channel\n- **get_participants(chat_id)**: List all participants\n- **get_admins(chat_id)**: List all admins\n- **get_banned_users(chat_id)**: List all banned users\n- **promote_admin(chat_id, user_id)**: Promote user to admin\n- **demote_admin(chat_id, user_id)**: Demote admin to user\n- **ban_user(chat_id, user_id)**: Ban user\n- **unban_user(chat_id, user_id)**: Unban user\n- **get_invite_link(chat_id)**: Get invite link\n- **export_chat_invite(chat_id)**: Export invite link\n- **import_chat_invite(hash)**: Join chat by invite hash\n- **join_chat_by_link(link)**: Join chat by invite link\n\n### Messaging\n- **get_messages(chat_id, page, page_size)**: Paginated messages\n- **list_messages(chat_id, limit, search_query, from_date, to_date)**: Filtered messages\n- **send_message(chat_id, message)**: Send a message\n- **reply_to_message(chat_id, message_id, text)**: Reply to a message\n- **edit_message(chat_id, message_id, new_text)**: Edit your message\n- **delete_message(chat_id, message_id)**: Delete a message\n- **forward_message(from_chat_id, message_id, to_chat_id)**: Forward a message\n- **pin_message(chat_id, message_id)**: Pin a message\n- **unpin_message(chat_id, message_id)**: Unpin a message\n- **mark_as_read(chat_id)**: Mark all as read\n- **get_message_context(chat_id, message_id, context_size)**: Context around a message\n- **get_history(chat_id, limit)**: Full chat history\n- **get_pinned_messages(chat_id)**: List pinned messages\n- **get_last_interaction(contact_id)**: Most recent message with a contact\n\n### Contact Management\n- **list_contacts()**: List all contacts\n- **search_contacts(query)**: Search contacts\n- **add_contact(phone, first_name, last_name)**: Add a contact\n- **delete_contact(user_id)**: Delete a contact\n- **block_user(user_id)**: Block a user\n- **unblock_user(user_id)**: Unblock a user\n- **import_contacts(contacts)**: Bulk import contacts\n- **export_contacts()**: Export all contacts as JSON\n- **get_blocked_users()**: List blocked users\n- **get_contact_ids()**: List all contact IDs\n- **get_direct_chat_by_contact(contact_query)**: Find direct chat with a contact\n- **get_contact_chats(contact_id)**: List all chats with a contact\n\n### User & Profile\n- **get_me()**: Get your user info\n- **update_profile(first_name, last_name, about)**: Update your profile\n- **delete_profile_photo()**: Remove your profile photo\n- **get_user_photos(user_id, limit)**: Get a user's profile photos\n- **get_user_status(user_id)**: Get a user's online status\n\n### Media\n- **get_media_info(chat_id, message_id)**: Get info about media in a message\n\n### Search & Discovery\n- **search_public_chats(query)**: Search public chats/channels/bots\n- **search_messages(chat_id, query, limit)**: Search messages in a chat\n- **resolve_username(username)**: Resolve a username to ID\n\n### Stickers, GIFs, Bots\n- **get_sticker_sets()**: List sticker sets\n- **get_bot_info(bot_username)**: Get info about a bot\n- **set_bot_commands(bot_username, commands)**: Set bot commands (bot accounts only)\n\n### Privacy, Settings, and Misc\n- **get_privacy_settings()**: Get privacy settings\n- **set_privacy_settings(key, allow_users, disallow_users)**: Set privacy settings\n- **mute_chat(chat_id)**: Mute notifications\n- **unmute_chat(chat_id)**: Unmute notifications\n- **archive_chat(chat_id)**: Archive a chat\n- **unarchive_chat(chat_id)**: Unarchive a chat\n- **get_recent_actions(chat_id)**: Get recent admin actions\n\n## Removed Functionality\n\nPlease note that tools requiring direct file path access on the server (`send_file`, `download_media`, `set_profile_photo`, `edit_chat_photo`, `send_voice`, `send_sticker`, `upload_file`) have been removed from `main.py`. This is due to limitations in the current MCP environment regarding handling file attachments and local file system paths.\n\nAdditionally, GIF-related tools (`get_gif_search`, `get_saved_gifs`, `send_gif`) have been removed due to ongoing issues with reliability in the Telethon library or Telegram API interactions.\n\n---\n\n## 📋 Requirements\n- Python 3.10+\n- [Telethon](https://docs.telethon.dev/)\n- [MCP Python SDK](https://modelcontextprotocol.io/docs/)\n- [Claude Desktop](https://claude.ai/desktop) or [Cursor](https://cursor.so/) (or any MCP client)\n\n---\n\n## 🔧 Installation & Setup\n\n### 1. Fork & Clone\n\n```bash\ngit clone https://github.com/chigwell/telegram-mcp.git\ncd telegram-mcp\n```\n\n### 2. Install Dependencies with uv\n\n```bash\nuv sync\n```\n\n### 3. Generate a Session String\n\n```bash\nuv run session_string_generator.py\n```\nFollow the prompts to authenticate and update your `.env` file.\n\n### 4. Configure .env\n\nCopy `.env.example` to `.env` and fill in your values:\n\n```\nTELEGRAM_API_ID=your_api_id_here\nTELEGRAM_API_HASH=your_api_hash_here\nTELEGRAM_SESSION_NAME=anon\nTELEGRAM_SESSION_STRING=your_session_string_here\n```\nGet your API credentials at [my.telegram.org/apps](https://my.telegram.org/apps).\n\n---\n\n## 🐳 Running with Docker\n\nIf you have Docker and Docker Compose installed, you can build and run the server in a container, simplifying dependency management.\n\n### 1. Build the Image\n\nFrom the project root directory, build the Docker image:\n\n```bash\ndocker build -t telegram-mcp:latest .\n```\n\n### 2. Running the Container\n\nYou have two options:\n\n**Option A: Using Docker Compose (Recommended for Local Use)**\n\nThis method uses the `docker-compose.yml` file and automatically reads your credentials from a `.env` file.\n\n1.  **Create `.env` File:** Ensure you have a `.env` file in the project root containing your `TELEGRAM_API_ID`, `TELEGRAM_API_HASH`, and `TELEGRAM_SESSION_STRING` (or `TELEGRAM_SESSION_NAME`). Use `.env.example` as a template.\n2.  **Run Compose:**\n    ```bash\n    docker compose up --build\n    ```\n    *   Use `docker compose up -d` to run in detached mode (background).\n    *   Press `Ctrl+C` to stop the server.\n\n**Option B: Using `docker run`**\n\nYou can run the container directly, passing credentials as environment variables.\n\n```bash\ndocker run -it --rm \\\n  -e TELEGRAM_API_ID=\"YOUR_API_ID\" \\\n  -e TELEGRAM_API_HASH=\"YOUR_API_HASH\" \\\n  -e TELEGRAM_SESSION_STRING=\"YOUR_SESSION_STRING\" \\\n  telegram-mcp:latest\n```\n*   Replace placeholders with your actual credentials.\n*   Use `-e TELEGRAM_SESSION_NAME=your_session_file_name` instead of `TELEGRAM_SESSION_STRING` if you prefer file-based sessions (requires volume mounting, see `docker-compose.yml` for an example).\n*   The `-it` flags are crucial for interacting with the server.\n\n---\n\n## ⚙️ Configuration for Claude & Cursor\n\n### MCP Configuration\nEdit your Claude desktop config (e.g. `~/Library/Application Support/Claude/claude_desktop_config.json`) or Cursor config (`~/.cursor/mcp.json`):\n\n```json\n{\n  \"mcpServers\": {\n    \"telegram-mcp\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"--directory\",\n        \"/full/path/to/telegram-mcp\",\n        \"run\",\n        \"main.py\"\n      ]\n    }\n  }\n}\n```\n\n## 📝 Tool Examples with Code & Output\n\nBelow are examples of the most commonly used tools with their implementation and sample output.\n\n### Getting Your Chats\n\n```python\n@mcp.tool()\nasync def get_chats(page: int = 1, page_size: int = 20) -> str:\n    \"\"\"\n    Get a paginated list of chats.\n    Args:\n        page: Page number (1-indexed).\n        page_size: Number of chats per page.\n    \"\"\"\n    try:\n        dialogs = await client.get_dialogs()\n        start = (page - 1) * page_size\n        end = start + page_size\n        if start >= len(dialogs):\n            return \"Page out of range.\"\n        chats = dialogs[start:end]\n        lines = []\n        for dialog in chats:\n            entity = dialog.entity\n            chat_id = entity.id\n            title = getattr(entity, \"title\", None) or getattr(entity, \"first_name\", \"Unknown\")\n            lines.append(f\"Chat ID: {chat_id}, Title: {title}\")\n        return \"\\n\".join(lines)\n    except Exception as e:\n        logger.exception(f\"get_chats failed (page={page}, page_size={page_size})\")\n        return \"An error occurred (code: GETCHATS-ERR-001). Check mcp_errors.log for details.\"\n```\n\nExample output:\n```\nChat ID: 123456789, Title: John Doe\nChat ID: -100987654321, Title: My Project Group\nChat ID: 111223344, Title: Jane Smith\nChat ID: -200123456789, Title: News Channel\n```\n\n### Sending Messages\n\n```python\n@mcp.tool()\nasync def send_message(chat_id: int, message: str) -> str:\n    \"\"\"\n    Send a message to a specific chat.\n    Args:\n        chat_id: The ID of the chat.\n        message: The message content to send.\n    \"\"\"\n    try:\n        entity = await client.get_entity(chat_id)\n        await client.send_message(entity, message)\n        return \"Message sent successfully.\"\n    except Exception as e:\n        logger.exception(f\"send_message failed (chat_id={chat_id})\")\n        return \"An error occurred (code: SENDMSG-ERR-001). Check mcp_errors.log for details.\"\n```\n\nExample output:\n```\nMessage sent successfully.\n```\n\n### Getting Chat Invite Links\n\nThe `get_invite_link` function is particularly robust with multiple fallback methods:\n\n```python\n@mcp.tool()\nasync def get_invite_link(chat_id: int) -> str:\n    \"\"\"\n    Get the invite link for a group or channel.\n    \"\"\"\n    try:\n        entity = await client.get_entity(chat_id)\n        \n        # Try using ExportChatInviteRequest first\n        try:\n            from telethon.tl import functions\n            result = await client(functions.messages.ExportChatInviteRequest(\n                peer=entity\n            ))\n            return result.link\n        except AttributeError:\n            # If the function doesn't exist in the current Telethon version\n            logger.warning(\"ExportChatInviteRequest not available, using alternative method\")\n        except Exception as e1:\n            # If that fails, log and try alternative approach\n            logger.warning(f\"ExportChatInviteRequest failed: {e1}\")\n            \n        # Alternative approach using client.export_chat_invite_link\n        try:\n            invite_link = await client.export_chat_invite_link(entity)\n            return invite_link\n        except Exception as e2:\n            logger.warning(f\"export_chat_invite_link failed: {e2}\")\n            \n        # Last resort: Try directly fetching chat info\n        try:\n            if isinstance(entity, (Chat, Channel)):\n                full_chat = await client(functions.messages.GetFullChatRequest(\n                    chat_id=entity.id\n                ))\n                if hasattr(full_chat, 'full_chat') and hasattr(full_chat.full_chat, 'invite_link'):\n                    return full_chat.full_chat.invite_link or \"No invite link available.\"\n        except Exception as e3:\n            logger.warning(f\"GetFullChatRequest failed: {e3}\")\n            \n        return \"Could not retrieve invite link for this chat.\"\n    except Exception as e:\n        logger.exception(f\"get_invite_link failed (chat_id={chat_id})\")\n        return f\"Error getting invite link: {e}\"\n```\n\nExample output:\n```\nhttps://t.me/+AbCdEfGhIjKlMnOp\n```\n\n### Joining Chats via Invite Links\n\n```python\n@mcp.tool()\nasync def join_chat_by_link(link: str) -> str:\n    \"\"\"\n    Join a chat by invite link.\n    \"\"\"\n    try:\n        # Extract the hash from the invite link\n        if '/' in link:\n            hash_part = link.split('/')[-1]\n            if hash_part.startswith('+'):\n                hash_part = hash_part[1:]  # Remove the '+' if present\n        else:\n            hash_part = link\n            \n        # Try checking the invite before joining\n        try:\n            # Try to check invite info first (will often fail if not a member)\n            invite_info = await client(functions.messages.CheckChatInviteRequest(hash=hash_part))\n            if hasattr(invite_info, 'chat') and invite_info.chat:\n                # If we got chat info, we're already a member\n                chat_title = getattr(invite_info.chat, 'title', 'Unknown Chat')\n                return f\"You are already a member of this chat: {chat_title}\"\n        except Exception:\n            # This often fails if not a member - just continue\n            pass\n            \n        # Join the chat using the hash\n        result = await client(functions.messages.ImportChatInviteRequest(hash=hash_part))\n        if result and hasattr(result, 'chats') and result.chats:\n            chat_title = getattr(result.chats[0], 'title', 'Unknown Chat')\n            return f\"Successfully joined chat: {chat_title}\"\n        return f\"Joined chat via invite hash.\"\n    except Exception as e:\n        err_str = str(e).lower()\n        if \"expired\" in err_str:\n            return \"The invite hash has expired and is no longer valid.\"\n        elif \"invalid\" in err_str:\n            return \"The invite hash is invalid or malformed.\"\n        elif \"already\" in err_str and \"participant\" in err_str:\n            return \"You are already a member of this chat.\"\n        logger.exception(f\"join_chat_by_link failed (link={link})\")\n        return f\"Error joining chat: {e}\"\n```\n\nExample output:\n```\nSuccessfully joined chat: Developer Community\n```\n\n### Searching Public Chats\n\n```python\n@mcp.tool()\nasync def search_public_chats(query: str) -> str:\n    \"\"\"\n    Search for public chats, channels, or bots by username or title.\n    \"\"\"\n    try:\n        result = await client(functions.contacts.SearchRequest(q=query, limit=20))\n        return json.dumps([format_entity(u) for u in result.users], indent=2)\n    except Exception as e:\n        return f\"Error searching public chats: {e}\"\n```\n\nExample output:\n```json\n[\n  {\n    \"id\": 123456789,\n    \"name\": \"TelegramBot\",\n    \"type\": \"user\",\n    \"username\": \"telegram_bot\"\n  },\n  {\n    \"id\": 987654321,\n    \"name\": \"Telegram News\",\n    \"type\": \"user\",\n    \"username\": \"telegram_news\"\n  }\n]\n```\n\n### Getting Direct Chats with Contacts\n\n```python\n@mcp.tool()\nasync def get_direct_chat_by_contact(contact_query: str) -> str:\n    \"\"\"\n    Find a direct chat with a specific contact by name, username, or phone.\n    \n    Args:\n        contact_query: Name, username, or phone number to search for.\n    \"\"\"\n    try:\n        # Fetch all contacts using the correct Telethon method\n        result = await client(functions.contacts.GetContactsRequest(hash=0))\n        contacts = result.users\n        found_contacts = []\n        for contact in contacts:\n            if not contact:\n                continue\n            name = f\"{getattr(contact, 'first_name', '')} {getattr(contact, 'last_name', '')}\".strip()\n            username = getattr(contact, 'username', '')\n            phone = getattr(contact, 'phone', '')\n            if (contact_query.lower() in name.lower() or \n                (username and contact_query.lower() in username.lower()) or \n                (phone and contact_query in phone)):\n                found_contacts.append(contact)\n        if not found_contacts:\n            return f\"No contacts found matching '{contact_query}'.\"\n        # If we found contacts, look for direct chats with them\n        results = []\n        dialogs = await client.get_dialogs()\n        for contact in found_contacts:\n            contact_name = f\"{getattr(contact, 'first_name', '')} {getattr(contact, 'last_name', '')}\".strip()\n            for dialog in dialogs:\n                if isinstance(dialog.entity, User) and dialog.entity.id == contact.id:\n                    chat_info = f\"Chat ID: {dialog.entity.id}, Contact: {contact_name}\"\n                    if getattr(contact, 'username', ''):\n                        chat_info += f\", Username: @{contact.username}\"\n                    if dialog.unread_count:\n                        chat_info += f\", Unread: {dialog.unread_count}\"\n                    results.append(chat_info)\n                    break\n        \n        if not results:\n            return f\"Found contacts matching '{contact_query}', but no direct chats with them.\"\n        \n        return \"\\n\".join(results)\n    except Exception as e:\n        return f\"Error searching for direct chat: {e}\"\n```\n\nExample output:\n```\nChat ID: 123456789, Contact: John Smith, Username: @johnsmith, Unread: 3\n```\n\n---\n\n## 🎮 Usage Examples\n\n- \"Show my recent chats\"\n- \"Send 'Hello world' to chat 123456789\"\n- \"Add contact with phone +1234567890, name John Doe\"\n- \"Create a group 'Project Team' with users 111, 222, 333\"\n- \"Download the media from message 42 in chat 123456789\"\n- \"Mute notifications for chat 123456789\"\n- \"Promote user 111 to admin in group 123456789\"\n- \"Search for public channels about 'news'\"\n- \"Join the Telegram group with invite link https://t.me/+AbCdEfGhIjK\"\n- \"Send a sticker to my Saved Messages\"\n- \"Get all my sticker sets\"\n\nYou can use these tools via natural language in Claude, Cursor, or any MCP-compatible client.\n\n---\n\n## 🧠 Error Handling & Robustness\n\nThis implementation includes comprehensive error handling:\n\n- **Session management**: Works with both file-based and string-based sessions\n- **Error reporting**: Detailed errors logged to `mcp_errors.log`\n- **Graceful degradation**: Multiple fallback approaches for critical functions\n- **User-friendly messages**: Clear, actionable error messages instead of technical errors\n- **Account type detection**: Functions that require bot accounts detect and notify when used with user accounts\n- **Invite link processing**: Handles various link formats and already-member cases\n\nThe code is designed to be robust against common Telegram API issues and limitations.\n\n---\n\n## 🛠️ Contribution Guide\n\n1. **Fork this repo:** [chigwell/telegram-mcp](https://github.com/chigwell/telegram-mcp)\n2. **Clone your fork:**\n   ```bash\n   git clone https://github.com/<your-github-username>/telegram-mcp.git\n   ```\n3. **Create a new branch:**\n   ```bash\n   git checkout -b my-feature\n   ```\n4. **Make your changes, add tests/docs if needed.**\n5. **Push and open a Pull Request** to [chigwell/telegram-mcp](https://github.com/chigwell/telegram-mcp) with a clear description.\n6. **Tag @chigwell or @l1v0n1** in your PR for review.\n\n---\n\n## 🔒 Security Considerations\n- **Never commit your `.env` or session string.**\n- The session string gives full access to your Telegram account—keep it safe!\n- All processing is local; no data is sent anywhere except Telegram's API.\n- Use `.env.example` as a template and keep your actual `.env` file private.\n- Test files are automatically excluded in `.gitignore`.\n\n---\n\n## 🛠️ Troubleshooting\n- **Check logs** in your MCP client (Claude/Cursor) and the terminal for errors.\n- **Detailed error logs** can be found in `mcp_errors.log`.\n- **Interpreter errors?** Make sure your `.venv` is created and selected.\n- **Database lock?** Use session string authentication, not file-based sessions.\n- **iCloud/Dropbox issues?** Move your project to a local path without spaces if you see odd errors.\n- **Regenerate session string** if you change your Telegram password or see auth errors.\n- **Bot-only functions** will show clear messages when used with regular user accounts.\n- **Test script failures?** Check test configuration in `.env` for valid test accounts/groups.\n\n---\n\n## 📄 License\n\nThis project is licensed under the [Apache 2.0 License](LICENSE).\n\n---\n\n## 🙏 Acknowledgements\n- [Telethon](https://github.com/LonamiWebs/Telethon)\n- [Model Context Protocol](https://modelcontextprotocol.io/)\n- [Claude](https://www.anthropic.com/) and [Cursor](https://cursor.so/)\n- [chigwell/telegram-mcp](https://github.com/chigwell/telegram-mcp) (upstream)\n\n---\n\n**Maintained by [@chigwell](https://github.com/chigwell) and [@l1v0n1](https://github.com/l1v0n1). PRs welcome!**\n\n## Star History\n\n[![Star History Chart](https://api.star-history.com/svg?repos=chigwell/telegram-mcp&type=Date)](https://www.star-history.com/#chigwell/telegram-mcp&Date)",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "telegram",
        "chat",
        "chats",
        "telegram mcp",
        "interact telegram",
        "telegram chats"
      ],
      "category": "realtime-collaboration"
    },
    "chuanmingliu--servers": {
      "owner": "chuanmingliu",
      "name": "servers",
      "url": "https://github.com/chuanmingliu/servers",
      "imageUrl": "/freedevtools/mcp/pfp/chuanmingliu.webp",
      "description": "A collection of implementations for the Model Context Protocol (MCP) that enables secure, controlled access for large language models (LLMs) to various tools and data sources. These servers showcase versatility and extensibility through reference implementations using Typescript and Python MCP SDKs.",
      "stars": 1,
      "forks": 0,
      "license": "MIT License",
      "language": "",
      "updated_at": "2025-03-01T17:10:31Z",
      "readme_content": "# Model Context Protocol servers\n\nThis repository is a collection of *reference implementations* for the [Model Context Protocol](https://modelcontextprotocol.io/) (MCP), as well as references\nto community built servers and additional resources.\n\nThe servers in this repository showcase the versatility and extensibility of MCP, demonstrating how it can be used to give Large Language Models (LLMs) secure, controlled access to tools and data sources.\nEach MCP server is implemented with either the [Typescript MCP SDK](https://github.com/modelcontextprotocol/typescript-sdk) or [Python MCP SDK](https://github.com/modelcontextprotocol/python-sdk).\n\n> Note: Lists in this README are maintained in alphabetical order to minimize merge conflicts when adding new items.\n\n## 🌟 Reference Servers\n\nThese servers aim to demonstrate MCP features and the TypeScript and Python SDKs.\n\n- **[AWS KB Retrieval](src/aws-kb-retrieval-server)** - Retrieval from AWS Knowledge Base using Bedrock Agent Runtime\n- **[Brave Search](src/brave-search)** - Web and local search using Brave's Search API\n- **[EverArt](src/everart)** - AI image generation using various models\n- **[Everything](src/everything)** - Reference / test server with prompts, resources, and tools\n- **[Fetch](src/fetch)** - Web content fetching and conversion for efficient LLM usage\n- **[Filesystem](src/filesystem)** - Secure file operations with configurable access controls\n- **[Git](src/git)** - Tools to read, search, and manipulate Git repositories\n- **[GitHub](src/github)** - Repository management, file operations, and GitHub API integration\n- **[GitLab](src/gitlab)** - GitLab API, enabling project management\n- **[Google Drive](src/gdrive)** - File access and search capabilities for Google Drive\n- **[Google Maps](src/google-maps)** - Location services, directions, and place details\n- **[Memory](src/memory)** - Knowledge graph-based persistent memory system\n- **[PostgreSQL](src/postgres)** - Read-only database access with schema inspection\n- **[Puppeteer](src/puppeteer)** - Browser automation and web scraping\n- **[Sentry](src/sentry)** - Retrieving and analyzing issues from Sentry.io\n- **[Sequential Thinking](src/sequentialthinking)** - Dynamic and reflective problem-solving through thought sequences\n- **[Slack](src/slack)** - Channel management and messaging capabilities\n- **[Sqlite](src/sqlite)** - Database interaction and business intelligence capabilities\n- **[Time](src/time)** - Time and timezone conversion capabilities\n\n## 🤝 Third-Party Servers\n\n### 🎖️ Official Integrations\n\nOfficial integrations are maintained by companies building production ready MCP servers for their platforms.\n\n- <img height=\"12\" width=\"12\" src=\"https://www.21st.dev/favicon.ico\" alt=\"21st.dev Logo\" /> **[21st.dev Magic](https://github.com/21st-dev/magic-mcp)** - Create crafted UI components inspired by the best 21st.dev design engineers.\n- <img height=\"12\" width=\"12\" src=\"https://apify.com/favicon.ico\" alt=\"Apify Logo\" /> **[Apify](https://github.com/apify/actors-mcp-server)** - [Actors MCP Server](https://apify.com/apify/actors-mcp-server): Use 3,000+ pre-built cloud tools to extract data from websites, e-commerce, social media, search engines, maps, and more\n- <img height=\"12\" width=\"12\" src=\"https://axiom.co/favicon.ico\" alt=\"Axiom Logo\" /> **[Axiom](https://github.com/axiomhq/mcp-server-axiom)** - Query and analyze your Axiom logs, traces, and all other event data in natural language\n- <img height=\"12\" width=\"12\" src=\"https://browserbase.com/favicon.ico\" alt=\"Browserbase Logo\" /> **[Browserbase](https://github.com/browserbase/mcp-server-browserbase)** - Automate browser interactions in the cloud (e.g. web navigation, data extraction, form filling, and more)\n- <img alt=\"cloudflare\" height=\"12\" width=\"12\" src=\"https://cdn.simpleicons.org/cloudflare\" /> **[Cloudflare](https://github.com/cloudflare/mcp-server-cloudflare)** - Deploy, configure & interrogate your resources on the Cloudflare developer platform (e.g. Workers/KV/R2/D1)\n- <img height=\"12\" width=\"12\" src=\"https://e2b.dev/favicon.ico\" alt=\"E2B Logo\" /> **[E2B](https://github.com/e2b-dev/mcp-server)** - Run code in secure sandboxes hosted by [E2B](https://e2b.dev)\n- <img height=\"12\" width=\"12\" src=\"https://esignatures.com/favicon.ico\" alt=\"eSignatures Logo\" /> **[eSignatures](https://github.com/esignaturescom/mcp-server-esignatures)** - Contract and template management for drafting, reviewing, and sending binding contracts.\n- <img height=\"12\" width=\"12\" src=\"https://exa.ai/images/favicon-32x32.png\" alt=\"Exa Logo\" /> **[Exa](https://github.com/exa-labs/exa-mcp-server)** - Search Engine made for AIs by [Exa](https://exa.ai)\n- <img height=\"12\" width=\"12\" src=\"https://firecrawl.dev/favicon.ico\" alt=\"Firecrawl Logo\" /> **[Firecrawl](https://github.com/mendableai/firecrawl-mcp-server)** - Extract web data with [Firecrawl](https://firecrawl.dev)\n- <img height=\"12\" width=\"12\" src=\"https://fireproof.storage/favicon.ico\" alt=\"Fireproof Logo\" /> **[Fireproof](https://github.com/fireproof-storage/mcp-database-server)** - Immutable ledger database with live synchronization\n- <img height=\"12\" width=\"12\" src=\"https://grafana.com/favicon.ico\" alt=\"Grafana Logo\" /> **[Grafana](https://github.com/grafana/mcp-grafana)** - Search dashboards, investigate incidents and query datasources in your Grafana instance\n- **[IBM wxflows](https://github.com/IBM/wxflows/tree/main/examples/mcp/javascript)** - Tool platform by IBM to build, test and deploy tools for any data source\n- <img height=\"12\" width=\"12\" src=\"https://integration.app/favicon.ico\" alt=\"Integration App Icon\" /> **[Integration App](https://github.com/integration-app/mcp-server)** - Interact with any other SaaS applications on behalf of your customers. \n- <img alt=\"jetbrains\" height=\"12\" width=\"12\" src=\"https://cdn.simpleicons.org/jetbrains\" /> **[JetBrains](https://github.com/JetBrains/mcp-jetbrains)** – Work on your code with JetBrains IDEs\n- <img height=\"12\" width=\"12\" src=\"https://kagi.com/favicon.ico\" alt=\"Kagi Logo\" /> **[Kagi Search](https://github.com/kagisearch/kagimcp)** - Search the web using Kagi's search API\n- <img height=\"12\" width=\"12\" src=\"https://lingo.dev/favicon.ico\" alt=\"Lingo.dev Logo\" /> **[Lingo.dev](https://github.com/lingodotdev/lingo.dev/blob/main/mcp.md)** - Make your AI agent speak every language on the planet, using [Lingo.dev](https://lingo.dev) Localization Engine.\n- <img height=\"12\" width=\"12\" src=\"https://www.meilisearch.com/favicon.ico\" alt=\"Meilisearch Logo\" /> **[Meilisearch](https://github.com/meilisearch/meilisearch-mcp)** - Interact & query with Meilisearch (Full-text & semantic search API)\n-  **[Metoro](https://github.com/metoro-io/metoro-mcp-server)** - Query and interact with kubernetes environments monitored by Metoro\n- <img height=\"12\" width=\"12\" src=\"https://www.motherduck.com/favicon.ico\" alt=\"MotherDuck Logo\" /> **[MotherDuck](https://github.com/motherduckdb/mcp-server-motherduck)** - Query and analyze data with MotherDuck and local DuckDB\n- <img height=\"12\" width=\"12\" src=\"https://needle-ai.com/images/needle-logo-orange-2-rounded.png\" alt=\"Needle AI Logo\" /> **[Needle](https://github.com/needle-ai/needle-mcp)** - Production-ready RAG out of the box to search and retrieve data from your own documents.\n- <img height=\"12\" width=\"12\" src=\"https://neo4j.com/favicon.ico\" alt=\"Neo4j Logo\" /> **[Neo4j](https://github.com/neo4j-contrib/mcp-neo4j/)** - Neo4j graph database server (schema + read/write-cypher) and separate graph database backed memory\n- **[Neon](https://github.com/neondatabase/mcp-server-neon)** - Interact with the Neon serverless Postgres platform\n- <img height=\"12\" width=\"12\" src=\"https://oxylabs.io/favicon.ico\" alt=\"Oxylabs Logo\" /> **[Oxylabs](https://github.com/oxylabs/oxylabs-mcp)** - Scrape websites with Oxylabs Web API, supporting dynamic rendering and parsing for structured data extraction.\n- <img alt=\"logomark\" height=\"12\" width=\"12\" src=\"https://qdrant.tech/img/brand-resources-logos/logomark.svg\" /> **[Qdrant](https://github.com/qdrant/mcp-server-qdrant/)** - Implement semantic memory layer on top of the Qdrant vector search engine\n- **[Raygun](https://github.com/MindscapeHQ/mcp-server-raygun)** - Interact with your crash reporting and real using monitoring data on your Raygun account\n- <img height=\"12\" width=\"12\" src=\"https://riza.io/favicon.ico\" alt=\"Riza logo\" /> **[Riza](https://github.com/riza-io/riza-mcp)** - Arbitrary code execution and tool-use platform for LLMs by [Riza](https://riza.io)\n- <img alt=\"56912e614b35093426c515860f9f2234\" height=\"12\" width=\"12\" src=\"https://pics.fatwang2.com/56912e614b35093426c515860f9f2234.svg\" /> [Search1API](https://github.com/fatwang2/search1api-mcp) - One API for Search, Crawling, and Sitemaps\n- <img height=\"12\" width=\"12\" src=\"https://stripe.com/favicon.ico\" alt=\"Stripe Logo\" /> **[Stripe](https://github.com/stripe/agent-toolkit)** - Interact with Stripe API\n- <img height=\"12\" width=\"12\" src=\"https://tavily.com/favicon.ico\" alt=\"Tavily Logo\" /> **[Tavily](https://github.com/tavily-ai/tavily-mcp)** - Search engine for AI agents (search + extract) powered by [Tavily](https://tavily.com/)\n- <img height=\"12\" width=\"12\" src=\"https://www.tinybird.co/favicon.ico\" alt=\"Tinybird Logo\" /> **[Tinybird](https://github.com/tinybirdco/mcp-tinybird)** - Interact with Tinybird serverless ClickHouse platform\n- <img height=\"12\" width=\"12\" src=\"https://verodat.io/assets/favicon-16x16.png\" alt=\"Verodat Logo\" /> **[Verodat](https://github.com/Verodat/verodat-mcp-server)** - Interact with Verodat AI Ready Data platform\n\n### 🌎 Community Servers\n\nA growing set of community-developed and maintained servers demonstrates various applications of MCP across different domains.\n\n> **Note:** Community servers are **untested** and should be used at **your own risk**. They are not affiliated with or endorsed by Anthropic.\n\n- **[AWS S3](https://github.com/aws-samples/sample-mcp-server-s3)** - A sample MCP server for AWS S3 that flexibly fetches objects from S3 such as PDF documents\n- **[AWS](https://github.com/rishikavikondala/mcp-server-aws)** - Perform operations on your AWS resources using an LLM\n- **[Airtable](https://github.com/domdomegg/airtable-mcp-server)** - Read and write access to [Airtable](https://airtable.com/) databases, with schema inspection.\n- **[Airtable](https://github.com/felores/airtable-mcp)** - Airtable Model Context Protocol Server.\n- **[AlphaVantage](https://github.com/calvernaz/alphavantage)** - MCP server for stock market data API [AlphaVantage](https://www.alphavantage.co)\n- **[Anki](https://github.com/scorzeth/anki-mcp-server)** - An MCP server for interacting with your [Anki](https://apps.ankiweb.net) decks and cards.\n- **[Any Chat Completions](https://github.com/pyroprompts/any-chat-completions-mcp)** - Interact with any OpenAI SDK Compatible Chat Completions API like OpenAI, Perplexity, Groq, xAI and many more.\n- **[ArangoDB](https://github.com/ravenwits/mcp-server-arangodb)** - MCP Server that provides database interaction capabilities through [ArangoDB](https://arangodb.com/).\n- **[Atlassian](https://github.com/sooperset/mcp-atlassian)** - Interact with Atlassian Cloud products (Confluence and Jira) including searching/reading Confluence spaces/pages, accessing Jira issues, and project metadata.\n- **[Base Free USDC Transfer](https://github.com/magnetai/mcp-free-usdc-transfer)** - Send USDC on [Base](https://base.org) for free using Claude AI! Built with [Coinbase CDP](https://docs.cdp.coinbase.com/mpc-wallet/docs/welcome).\n- **[BigQuery](https://github.com/LucasHild/mcp-server-bigquery)** (by LucasHild) - This server enables LLMs to inspect database schemas and execute queries on BigQuery.\n- **[BigQuery](https://github.com/ergut/mcp-bigquery-server)** (by ergut) - Server implementation for Google BigQuery integration that enables direct BigQuery database access and querying capabilities\n- **[Calendar](https://github.com/GongRzhe/Calendar-MCP-Server)** - Google Calendar integration server enabling AI assistants to manage calendar events through natural language interactions.\n- **[CFBD API](https://github.com/lenwood/cfbd-mcp-server)** - An MCP server for the [College Football Data API](https://collegefootballdata.com/).\n- **[ChatMCP](https://github.com/AI-QL/chat-mcp)** – An Open Source Cross-platform GUI Desktop application compatible with Linux, macOS, and Windows, enabling seamless interaction with MCP servers across dynamically selectable LLMs, by **[AIQL](https://github.com/AI-QL)**\n- **[ChatSum](https://github.com/mcpso/mcp-server-chatsum)** - Query and Summarize chat messages with LLM. by [mcpso](https://mcp.so)\n- **[Chroma](https://github.com/privetin/chroma)** - Vector database server for semantic document search and metadata filtering, built on Chroma\n- **[ClaudePost](https://github.com/ZilongXue/claude-post)** - ClaudePost enables seamless email management for Gmail, offering secure features like email search, reading, and sending.\n- **[Cloudinary](https://github.com/felores/cloudinary-mcp-server)** - Cloudinary Model Context Protocol Server to upload media to Cloudinary and get back the media link and details.\n- **[code-executor](https://github.com/bazinga012/mcp_code_executor)** - An MCP server that allows LLMs to execute Python code within a specified Conda environment.\n- **[code-sandbox-mcp](https://github.com/Automata-Labs-team/code-sandbox-mcp)** - An MCP server to create secure code sandbox environment for executing code within Docker containers.\n- **[cognee-mcp](https://github.com/topoteretes/cognee/tree/main/cognee-mcp)** - GraphRAG memory server with customizable ingestion, data processing and search\n- **[coin_api_mcp](https://github.com/longmans/coin_api_mcp)** - Provides access to [coinmarketcap](https://coinmarketcap.com/) cryptocurrency data.\n- **[Contentful-mcp](https://github.com/ivo-toby/contentful-mcp)** - Read, update, delete, publish content in your [Contentful](https://contentful.com) space(s) from this MCP Server.\n- **[Data Exploration](https://github.com/reading-plus-ai/mcp-server-data-exploration)** - MCP server for autonomous data exploration on .csv-based datasets, providing intelligent insights with minimal effort. NOTE: Will execute arbitrary Python code on your machine, please use with caution!\n- **[Dataset Viewer](https://github.com/privetin/dataset-viewer)** - Browse and analyze Hugging Face datasets with features like search, filtering, statistics, and data export\n- **[DeepSeek MCP Server](https://github.com/DMontgomery40/deepseek-mcp-server)** - Model Context Protocol server integrating DeepSeek's advanced language models, in addition to [other useful API endpoints](https://github.com/DMontgomery40/deepseek-mcp-server?tab=readme-ov-file#features)\n- **[Deepseek_R1](https://github.com/66julienmartin/MCP-server-Deepseek_R1)** - A Model Context Protocol (MCP) server implementation connecting Claude Desktop with DeepSeek's language models (R1/V3)\n- **[Descope](https://github.com/descope-sample-apps/descope-mcp-server)** - An MCP server to integrate with [Descope](https://descope.com) to search audit logs, manage users, and more.\n- **[DevRev](https://github.com/kpsunil97/devrev-mcp-server)** - An MCP server to integrate with DevRev APIs to search through your DevRev Knowledge Graph where objects can be imported from diff. sources listed [here](https://devrev.ai/docs/import#available-sources).\n- **[Dify](https://github.com/YanxingLiu/dify-mcp-server)** - A simple implementation of an MCP server for dify workflows.\n- **[Discord](https://github.com/v-3/discordmcp)** - A MCP server to connect to Discord guilds through a bot and read and write messages in channels\n- **[Docker](https://github.com/ckreiling/mcp-server-docker)** - Integrate with Docker to manage containers, images, volumes, and networks.\n- **[Drupal](https://github.com/Omedia/mcp-server-drupal)** - Server for interacting with [Drupal](https://www.drupal.org/project/mcp) using STDIO transport layer.\n- **[Elasticsearch](https://github.com/cr7258/elasticsearch-mcp-server)** - MCP server implementation that provides Elasticsearch interaction.\n- **[ElevenLabs](https://github.com/mamertofabian/elevenlabs-mcp-server)** - A server that integrates with ElevenLabs text-to-speech API capable of generating full voiceovers with multiple voices.\n- **[Eunomia](https://github.com/whataboutyou-ai/eunomia-MCP-server)** - Extension of the Eunomia framework that connects Eunomia instruments with MCP servers\n- **[Everything Search](https://github.com/mamertofabian/mcp-everything-search)** - Fast file searching capabilities across Windows (using [Everything SDK](https://www.voidtools.com/support/everything/sdk/)), macOS (using mdfind command), and Linux (using locate/plocate command).\n- **[Fetch](https://github.com/zcaceres/fetch-mcp)** - A server that flexibly fetches HTML, JSON, Markdown, or plaintext.\n- **[FireCrawl](https://github.com/vrknetha/mcp-server-firecrawl)** - Advanced web scraping with JavaScript rendering, PDF support, and smart rate limiting\n- **[FlightRadar24](https://github.com/sunsetcoder/flightradar24-mcp-server)** - A Claude Desktop MCP server that helps you track flights in real-time using Flightradar24 data.\n- **[Glean](https://github.com/longyi1207/glean-mcp-server)** - A server that uses Glean API to search and chat.\n- **[Gmail](https://github.com/GongRzhe/Gmail-MCP-Server)** - A Model Context Protocol (MCP) server for Gmail integration in Claude Desktop with auto authentication support.\n- **[Goal Story](https://github.com/hichana/goalstory-mcp)** - a Goal Tracker and Visualization Tool for personal and professional development.\n- **[Golang Filesystem Server](https://github.com/mark3labs/mcp-filesystem-server)** - Secure file operations with configurable access controls built with Go!\n- **[Google Calendar](https://github.com/v-3/google-calendar)** - Integration with Google Calendar to check schedules, find time, and add/delete events\n- **[Google Calendar](https://github.com/nspady/google-calendar-mcp)** - Google Calendar MCP Server for managing Google calendar events. Also supports searching for events by attributes like title and location.\n- **[Google Custom Search](https://github.com/adenot/mcp-google-search)** - Provides Google Search results via the Google Custom Search API\n- **[Google Tasks](https://github.com/zcaceres/gtasks-mcp)** - Google Tasks API Model Context Protocol Server.\n- **[Holaspirit](https://github.com/syucream/holaspirit-mcp-server)** - Interact with [Holaspirit](https://www.holaspirit.com/).\n- **[Home Assistant](https://github.com/tevonsb/homeassistant-mcp)** - Interact with [Home Assistant](https://www.home-assistant.io/) including viewing and controlling lights, switches, sensors, and all other Home Assistant entities.\n- **[HubSpot](https://github.com/buryhuang/mcp-hubspot)** - HubSpot CRM integration for managing contacts and companies. Create and retrieve CRM data directly through Claude chat.\n- **[HuggingFace Spaces](https://github.com/evalstate/mcp-hfspace)** - Server for using HuggingFace Spaces, supporting Open Source Image, Audio, Text Models and more. Claude Desktop mode for easy integration.\n- **[Inoyu](https://github.com/sergehuber/inoyu-mcp-unomi-server)** - Interact with an Apache Unomi CDP customer data platform to retrieve and update customer profiles\n- **[iTerm MCP](https://github.com/ferrislucas/iterm-mcp)** - Integration with iTerm2 terminal emulator for macOS, enabling LLMs to execute and monitor terminal commands.\n- **[JavaFX](https://github.com/mcpso/mcp-server-javafx)** - Make drawings using a JavaFX canvas\n- **[JDBC](https://github.com/quarkiverse/quarkus-mcp-servers/tree/main/jdbc)** - Connect to any JDBC-compatible database and query, insert, update, delete, and more. Supports MySQL, PostgreSQL, Oracle, SQL Server, sqllite and [more](https://github.com/quarkiverse/quarkus-mcp-servers/tree/main/jdbc#supported-jdbc-variants).\n- **[JSON](https://github.com/GongRzhe/JSON-MCP-Server)** - JSON handling and processing server with advanced query capabilities using JSONPath syntax and support for array, string, numeric, and date operations.\n- **[Keycloak MCP](https://github.com/ChristophEnglisch/keycloak-model-context-protocol)** - This MCP server enables natural language interaction with Keycloak for user and realm management including creating, deleting, and listing users and realms.\n- **[Kibela](https://github.com/kiwamizamurai/mcp-kibela-server)** (by kiwamizamurai) - Interact with Kibela API.\n- **[kintone](https://github.com/macrat/mcp-server-kintone)** - Manage records and apps in [kintone](https://kintone.com) through LLM tools.\n- **[Kubernetes](https://github.com/Flux159/mcp-server-kubernetes)** - Connect to Kubernetes cluster and manage pods, deployments, and services.\n- **[Lightdash](https://github.com/syucream/lightdash-mcp-server)** - Interact with [Lightdash](https://www.lightdash.com/), a BI tool.\n- **[Linear](https://github.com/jerhadf/linear-mcp-server)** - Allows LLM to interact with Linear's API for project management, including searching, creating, and updating issues.\n- **[LlamaCloud](https://github.com/run-llama/mcp-server-llamacloud)** (by marcusschiesser) - Integrate the data stored in a managed index on [LlamaCloud](https://cloud.llamaindex.ai/)\n- **[llm-context](https://github.com/cyberchitta/llm-context.py)** - Provides a repo-packing MCP tool with configurable profiles that specify file inclusion/exclusion patterns and optional prompts.\n- **[MCP Compass](https://github.com/liuyoshio/mcp-compass)** - Suggest the right MCP server for your needs\n- **[MCP Installer](https://github.com/anaisbetts/mcp-installer)** - This server is a server that installs other MCP servers for you.\n- **[mcp-k8s-go](https://github.com/strowk/mcp-k8s-go)** - Golang-based Kubernetes server for MCP to browse pods and their logs, events, namespaces and more. Built to be extensible.\n- **[mcp-proxy](https://github.com/sparfenyuk/mcp-proxy)** - Connect to MCP servers that run on SSE transport, or expose stdio servers as an SSE server.\n- **[MSSQL](https://github.com/aekanun2020/mcp-server/)** - MSSQL database integration with configurable access controls and schema inspection\n- **[MSSQL](https://github.com/JexinSam/mssql_mcp_server)** (by jexin) - MCP Server for MSSQL database in Python\n- **[MSSQL-Python](https://github.com/amornpan/py-mcp-mssql)** (by amornpan) - A read-only Python implementation for MSSQL database access with enhanced security features, configurable access controls, and schema inspection capabilities. Focuses on safe database interaction through Python ecosystem.\n- **[Markdownify](https://github.com/zcaceres/mcp-markdownify-server)** - MCP to convert almost anything to Markdown (PPTX, HTML, PDF, Youtube Transcripts and more)\n- **[Minima](https://github.com/dmayboroda/minima)** - MCP server for RAG on local files\n- **[MongoDB](https://github.com/kiliczsh/mcp-mongo-server)** - A Model Context Protocol Server for MongoDB.\n- **[Monday.com](https://github.com/sakce/mcp-server-monday)** - MCP Server to interact with Monday.com boards and items.\n- **[MySQL](https://github.com/benborla/mcp-server-mysql)** (by benborla) - MySQL database integration in NodeJS with configurable access controls and schema inspection\n- **[MySQL](https://github.com/designcomputer/mysql_mcp_server)** (by DesignComputer) - MySQL database integration in Python with configurable access controls and schema inspection\n- **[NS Travel Information](https://github.com/r-huijts/ns-mcp-server)** - Access Dutch Railways (NS) real-time train travel information and disruptions through the official NS API.\n- **[Neo4j](https://github.com/da-okazaki/mcp-neo4j-server)** - A community built server that interacts with Neo4j Graph Database.\n- **[Neovim](https://github.com/bigcodegen/mcp-neovim-server)** - An MCP Server for your Neovim session.\n- **[Notion](https://github.com/suekou/mcp-notion-server)** (by suekou) - Interact with Notion API.\n- **[Notion](https://github.com/v-3/notion-server)** (by v-3) - Notion MCP integration. Search, Read, Update, and Create pages through Claude chat.\n- **[oatpp-mcp](https://github.com/oatpp/oatpp-mcp)** - C++ MCP integration for Oat++. Use [Oat++](https://oatpp.io) to build MCP servers.\n- **[Obsidian Markdown Notes](https://github.com/calclavia/mcp-obsidian)** - Read and search through your Obsidian vault or any directory containing Markdown notes\n- **[obsidian-mcp](https://github.com/StevenStavrakis/obsidian-mcp)** - (by Steven Stavrakis) An MCP server for Obsidian.md with tools for searching, reading, writing, and organizing notes.\n- **[OpenAPI](https://github.com/snaggle-ai/openapi-mcp-server)** - Interact with [OpenAPI](https://www.openapis.org/) APIs.\n- **[OpenCTI](https://github.com/Spathodea-Network/opencti-mcp)** - Interact with OpenCTI platform to retrieve threat intelligence data including reports, indicators, malware and threat actors.\n- **[OpenRPC](https://github.com/shanejonas/openrpc-mpc-server)** - Interact with and discover JSON-RPC APIs via [OpenRPC](https://open-rpc.org).\n- **[Open Strategy Partners Marketing Tools](https://github.com/open-strategy-partners/osp_marketing_tools)** - Content editing codes, value map, and positioning tools for product marketing.\n- **[Pandoc](https://github.com/vivekVells/mcp-pandoc)** - MCP server for seamless document format conversion using Pandoc, supporting Markdown, HTML, PDF, DOCX (.docx), csv and more.\n- **[PIF](https://github.com/hungryrobot1/MCP-PIF)** - A Personal Intelligence Framework (PIF), providing tools for file operations, structured reasoning, and journal-based documentation to support continuity and evolving human-AI collaboration across sessions.\n- **[Pinecone](https://github.com/sirmews/mcp-pinecone)** - MCP server for searching and uploading records to Pinecone. Allows for simple RAG features, leveraging Pinecone's Inference API.\n- **[Placid.app](https://github.com/felores/placid-mcp-server)** - Generate image and video creatives using Placid.app templates\n- **[Playwright](https://github.com/executeautomation/mcp-playwright)** - This MCP Server will help you run browser automation and webscraping using Playwright\n- **[Postman](https://github.com/shannonlal/mcp-postman)** - MCP server for running Postman Collections locally via Newman. Allows for simple execution of Postman Server and returns the results of whether the collection passed all the tests.\n- **[Qwen_Max](https://github.com/66julienmartin/MCP-server-Qwen_Max)** - A Model Context Protocol (MCP) server implementation for the Qwen models.\n- **[RabbitMQ](https://github.com/kenliao94/mcp-server-rabbitmq)** - The MCP server that interacts with RabbitMQ to publish and consume messages.\n- **[RAG Web Browser](https://github.com/apify/mcp-server-rag-web-browser)** An MCP server for Apify's open-source RAG Web Browser [Actor](https://apify.com/apify/rag-web-browser) to perform web searches, scrape URLs, and return content in Markdown.\n- **[Reaper](https://github.com/dschuler36/reaper-mcp-server)** - Interact with your [Reaper](https://www.reaper.fm/) (Digital Audio Workstation) projects.\n- **[Redis](https://github.com/GongRzhe/REDIS-MCP-Server)** - Redis database operations and caching microservice server with support for key-value operations, expiration management, and pattern-based key listing.\n- **[Redis](https://github.com/prajwalnayak7/mcp-server-redis)** MCP server to interact with Redis Server, AWS Memory DB, etc for caching or other use-cases where in-memory and key-value based storage is appropriate\n- **[Rememberizer AI](https://github.com/skydeckai/mcp-server-rememberizer)** - An MCP server designed for interacting with the Rememberizer data source, facilitating enhanced knowledge retrieval.\n- **[Replicate](https://github.com/deepfates/mcp-replicate)** - Search, run and manage machine learning models on Replicate through a simple tool-based interface. Browse models, create predictions, track their status, and handle generated images.\n- **[Rijksmuseum](https://github.com/r-huijts/rijksmuseum-mcp)** - Interface with the Rijksmuseum API to search artworks, retrieve artwork details, access image tiles, and explore user collections.\n- **[Salesforce MCP](https://github.com/smn2gnt/MCP-Salesforce)** - Interact with Salesforce Data and Metadata\n- **[Scholarly](https://github.com/adityak74/mcp-scholarly)** - A MCP server to search for scholarly and academic articles.\n- **[SearXNG](https://github.com/ihor-sokoliuk/mcp-searxng)** - A Model Context Protocol Server for [SearXNG](https://docs.searxng.org)\n- **[Snowflake](https://github.com/isaacwasserman/mcp-snowflake-server)** - This MCP server enables LLMs to interact with Snowflake databases, allowing for secure and controlled data operations.\n- **[Spotify](https://github.com/varunneal/spotify-mcp)** - This MCP allows an LLM to play and use Spotify.\n- **[Stripe](https://github.com/atharvagupta2003/mcp-stripe)** - This MCP allows integration with Stripe for handling payments, customers, and refunds.\n- **[TMDB](https://github.com/Laksh-star/mcp-server-tmdb)** - This MCP server integrates with The Movie Database (TMDB) API to provide movie information, search capabilities, and recommendations.\n- **[Tavily search](https://github.com/RamXX/mcp-tavily)** - An MCP server for Tavily's search & news API, with explicit site inclusions/exclusions\n- **[Ticketmaster](https://github.com/delorenj/mcp-server-ticketmaster)** - Search for events, venues, and attractions through the Ticketmaster Discovery API\n- **[Todoist](https://github.com/abhiz123/todoist-mcp-server)** - Interact with Todoist to manage your tasks.\n- **[Travel Planner](https://github.com/GongRzhe/TRAVEL-PLANNER-MCP-Server)** - Travel planning and itinerary management server integrating with Google Maps API for location search, place details, and route calculations.\n- **[Vega-Lite](https://github.com/isaacwasserman/mcp-vegalite-server)** - Generate visualizations from fetched data using the VegaLite format and renderer.\n- **[Video Editor](https://github.com/burningion/video-editing-mcp)** - A Model Context Protocol Server to add, edit, and search videos with [Video Jungle](https://www.video-jungle.com/).\n- **[WildFly MCP](https://github.com/wildfly-extras/wildfly-mcp)** - WildFly MCP server that enables LLM to interact with running WildFly servers (retrieve metrics, logs, invoke operations, ...).\n- **[Windows CLI](https://github.com/SimonB97/win-cli-mcp-server)** - MCP server for secure command-line interactions on Windows systems, enabling controlled access to PowerShell, CMD, and Git Bash shells.\n- **[World Bank data API](https://github.com/anshumax/world_bank_mcp_server)** - A server that fetches data indicators available with the World Bank as part of their data API\n- **[X (Twitter)](https://github.com/EnesCinr/twitter-mcp)** (by EnesCinr) - Interact with twitter API. Post tweets and search for tweets by query.\n- **[X (Twitter)](https://github.com/vidhupv/x-mcp)** (by vidhupv) - Create, manage and publish X/Twitter posts directly through Claude chat.\n- **[XMind](https://github.com/apeyroux/mcp-xmind)** - Read and search through your XMind directory containing XMind files.\n- **[YouTube](https://github.com/ZubeidHendricks/youtube-mcp-server)** - Comprehensive YouTube API integration for video management, Shorts creation, and analytics.\n\n## 📚 Frameworks\n\nThese are high-level frameworks that make it easier to build MCP servers or clients.\n\n### For servers\n\n* **[EasyMCP](https://github.com/zcaceres/easy-mcp/)** (TypeScript)\n* **[FastMCP](https://github.com/punkpeye/fastmcp)** (TypeScript)\n* **[Foxy Contexts](https://github.com/strowk/foxy-contexts)** – A library to build MCP servers in Golang by **[strowk](https://github.com/strowk)**\n* **[Quarkus MCP Server SDK](https://github.com/quarkiverse/quarkus-mcp-server)** (Java)\n\n### For clients\n\n* **[codemirror-mcp](https://github.com/marimo-team/codemirror-mcp)** - CodeMirror extension that implements the Model Context Protocol (MCP) for resource mentions and prompt commands\n\n## 📚 Resources\n\nAdditional resources on MCP.\n\n- **[AiMCP](https://www.aimcp.info)** - A collection of MCP clients&servers to find the right mcp tools by **[Hekmon](https://github.com/hekmon8)**\n- **[Awesome Crypto MCP Servers by badkk](https://github.com/badkk/awesome-crypto-mcp-servers)** - A curated list of MCP servers by **[Luke Fan](https://github.com/badkk)**\n- **[Awesome MCP Servers by appcypher](https://github.com/appcypher/awesome-mcp-servers)** - A curated list of MCP servers by **[Stephen Akinyemi](https://github.com/appcypher)**\n- **[Awesome MCP Servers by punkpeye](https://github.com/punkpeye/awesome-mcp-servers)** (**[website](https://glama.ai/mcp/servers)**) - A curated list of MCP servers by **[Frank Fiegel](https://github.com/punkpeye)**\n- **[Awesome MCP Servers by wong2](https://github.com/wong2/awesome-mcp-servers)** (**[website](https://mcpservers.org)**) - A curated list of MCP servers by **[wong2](https://github.com/wong2)**\n- **[Discord Server](https://glama.ai/mcp/discord)** – A community discord server dedicated to MCP by **[Frank Fiegel](https://github.com/punkpeye)**\n- **[MCP Badges](https://github.com/mcpx-dev/mcp-badges)** – Quickly highlight your MCP project with clear, eye-catching badges, by **[Ironben](https://github.com/nanbingxyz)**\n- **[MCP Servers Hub](https://github.com/apappascs/mcp-servers-hub)** (**[website](https://mcp-servers-hub-website.pages.dev/)**) - A curated list of MCP servers by **[apappascs](https://github.com/apappascs)**\n- **[MCP X Community](https://x.com/i/communities/1861891349609603310)** – A X community for MCP by **[Xiaoyi](https://x.com/chxy)**\n- **[mcp-cli](https://github.com/wong2/mcp-cli)** - A CLI inspector for the Model Context Protocol by **[wong2](https://github.com/wong2)**\n- **[mcp-get](https://mcp-get.com)** - Command line tool for installing and managing MCP servers by **[Michael Latman](https://github.com/michaellatman)**\n- **[mcp-manager](https://github.com/zueai/mcp-manager)** - Simple Web UI to install and manage MCP servers for Claude Desktop by **[Zue](https://github.com/zueai)**\n- **[MCPHub](https://github.com/Jeamee/MCPHub-Desktop)** – An Open Source MacOS & Windows GUI Desktop app for discovering, installing and managing MCP servers by **[Jeamee](https://github.com/jeamee)**\n- **[mcp.run](https://mcp.run)** - A hosted registry and control plane to install & run secure + portable MCP Servers.\n- **[Open-Sourced MCP Servers Directory](https://github.com/chatmcp/mcp-directory)** - A curated list of MCP servers by **[mcpso](https://mcp.so)**\n- <img height=\"12\" width=\"12\" src=\"https://opentools.com/favicon.ico\" alt=\"OpenTools Logo\" /> **[OpenTools](https://opentools.com)** - An open registry for finding, installing, and building with MCP servers by **[opentoolsteam](https://github.com/opentoolsteam)**\n- **[PulseMCP](https://www.pulsemcp.com)** ([API](https://www.pulsemcp.com/api)) - Community hub & weekly newsletter for discovering MCP servers, clients, articles, and news by **[Tadas Antanavicius](https://github.com/tadasant)**, **[Mike Coughlin](https://github.com/macoughl)**, and **[Ravina Patel](https://github.com/ravinahp)**\n- **[r/mcp](https://www.reddit.com/r/mcp)** – A Reddit community dedicated to MCP by **[Frank Fiegel](https://github.com/punkpeye)**\n- **[Smithery](https://smithery.ai/)** - A registry of MCP servers to find the right tools for your LLM agents by **[Henry Mao](https://github.com/calclavia)**\n- **[Toolbase](https://gettoolbase.ai)** - Desktop application that manages tools and MCP servers with just a few clicks - no coding required by **[gching](https://github.com/gching)**\n\n## 🚀 Getting Started\n\n### Using MCP Servers in this Repository\nTypescript-based servers in this repository can be used directly with `npx`.\n\nFor example, this will start the [Memory](src/memory) server:\n```sh\nnpx -y @modelcontextprotocol/server-memory\n```\n\nPython-based servers in this repository can be used directly with [`uvx`](https://docs.astral.sh/uv/concepts/tools/) or [`pip`](https://pypi.org/project/pip/). `uvx` is recommended for ease of use and setup.\n\nFor example, this will start the [Git](src/git) server:\n```sh\n# With uvx\nuvx mcp-server-git\n\n# With pip\npip install mcp-server-git\npython -m mcp_server_git\n```\n\nFollow [these](https://docs.astral.sh/uv/getting-started/installation/) instructions to install `uv` / `uvx` and [these](https://pip.pypa.io/en/stable/installation/) to install `pip`.\n\n### Using an MCP Client\nHowever, running a server on its own isn't very useful, and should instead be configured into an MCP client. For example, here's the Claude Desktop configuration to use the above server:\n\n```json\n{\n  \"mcpServers\": {\n    \"memory\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@modelcontextprotocol/server-memory\"]\n    }\n  }\n}\n```\n\nAdditional examples of using the Claude Desktop as an MCP client might look like:\n\n```json\n{\n  \"mcpServers\": {\n    \"filesystem\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@modelcontextprotocol/server-filesystem\", \"/path/to/allowed/files\"]\n    },\n    \"git\": {\n      \"command\": \"uvx\",\n      \"args\": [\"mcp-server-git\", \"--repository\", \"path/to/git/repo\"]\n    },\n    \"github\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@modelcontextprotocol/server-github\"],\n      \"env\": {\n        \"GITHUB_PERSONAL_ACCESS_TOKEN\": \"<YOUR_TOKEN>\"\n      }\n    },\n    \"postgres\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@modelcontextprotocol/server-postgres\", \"postgresql://localhost/mydb\"]\n    }\n  }\n}\n```\n\n## 🛠️ Creating Your Own Server\n\nInterested in creating your own MCP server? Visit the official documentation at [modelcontextprotocol.io](https://modelcontextprotocol.io/introduction) for comprehensive guides, best practices, and technical details on implementing MCP servers.\n\n## 🤝 Contributing\n\nSee [CONTRIBUTING.md](CONTRIBUTING.md) for information about contributing to this repository.\n\n## 🔒 Security\n\nSee [SECURITY.md](SECURITY.md) for reporting security vulnerabilities.\n\n## 📜 License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n\n## 💬 Community\n\n- [GitHub Discussions](https://github.com/orgs/modelcontextprotocol/discussions)\n\n## ⭐ Support\n\nIf you find MCP servers useful, please consider starring the repository and contributing new servers or improvements!\n\n---\n\nManaged by Anthropic, but built together with the community. The Model Context Protocol is open source and we encourage everyone to contribute their own servers and improvements!",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "protocol",
        "chuanmingliu",
        "sdks",
        "chuanmingliu servers",
        "context protocol",
        "collaboration chuanmingliu"
      ],
      "category": "realtime-collaboration"
    },
    "cliffhall--puzzlebox": {
      "owner": "cliffhall",
      "name": "puzzlebox",
      "url": "https://github.com/cliffhall/puzzlebox",
      "imageUrl": "/freedevtools/mcp/pfp/cliffhall.webp",
      "description": "Manage dynamic finite state machines to coordinate multiple agents effectively, facilitating complex workflows and ensuring real-time updates on state changes. Streamline transitions between project phases to keep teams aligned with overall project goals.",
      "stars": 21,
      "forks": 2,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-10-02T10:06:34Z",
      "readme_content": "# puzzlebox\n\n\n## Coordinating agents with state machines \n\nAn [MCP server](https://github.com/modelcontextprotocol/specification/tree/main) that hosts  [finite state machines](https://en.wikipedia.org/wiki/Finite-state_machine) as dynamic resources that clients can subscribe to and be updated when their state changes.\n\n## Feature Roadmap and Status\n<details>\n<summary>A work in progress. Much is done, a few bits remain to be done.</summary>\n\n* [x] Unit tests for tool code\n* [x] Integration tests for MCP server (both SSE and StreamableHttp)\n* [x] SSE transport\n* [x] StreamableHttp transport\n* [x] Multiple simultaneous client connections (both transports)\n* [x] Create puzzles (state machines) as dynamic resources\n* [x] Subscribe to puzzles\n* [x] Receive update notifications when puzzles change\n* [x] Get puzzle snapshot (current state and available actions)\n* [x] Change puzzle state by performing action on puzzle\n* [ ] Resource creation by puzzle state (used by agents and guard prompts)\n* [ ] Transition guard prompt creation (uses resources by state)\n* [ ] Transition guard via sampling\n* [ ] Command line REPL\n* [ ] Demo via REPL\n</details>\n\n## What problem does puzzlebox address?\n\n<details>\n<summary>Collaboration and coordination are related but different problems. Collaboration is good for non-trivial yet relatively simple tasks. To tackle long horizon efforts, puzzlebox solves for coordination.</summary>\n\n### Teams need coordination\nMarshalling multiple agents toward a big goal is tougher than just breaking down a request into tasks, assigning them to available agents and [enabling collaboration](https://github.com/cliffhall/GooseTeam) between them. \n\nJust as a few agents can collaborate to complete a small project, several teams of process-aware agents need to operate within distinct project phases to tackle long horizon efforts.\n\nConsider enterprise-level software development processes:\n\n* A large software project typically moves through a multistep, occasionally backtracking path from inception to design to building to testing to documentation to marketing to production. \n\n* Different teams are focused on different aspects over time, informed by what's gone before and with an eye toward an ever-changing goal that is refined according to lessons learned. \n\n* Even within a phase, teams may cycle through their own phases, like Agile sprints. A certain amount of work is scoped for the sprint, the team works on their parts, and at the end of the sprint they decide what to tackle next. It accepts that each sprint could change the course of future development. These cycles can also be represented as puzzles.\n\nWith puzzlebox, members of agentic teams can be made process-aware, but the process itself is not subject to hallucination.\n\n### Scenario: Teams passing the torch\n\nThree agents are working. The current state of their shared puzzle is \"Specification\". \n* Agent 1 is specifying the domain language.\n* Agent 2 is defining project scope.\n* Agent 3 is producing the specification document.\n* The agents collaborate to reach the final specification document.\n* Once the spec is done, Agent 3 initiates a transition to \"Design\" state.\n  * First, the spec is checked by an exit guard (i.e., LLM sampling) for completeness. \n    * If problems are found, the state transition is canceled and the team continues.\n    * If acceptable, the state changes to \"Design\". \n      * The \"Specification\" agents are monitoring the puzzle and should clock out now. \n        * Their long (and expensive) contexts have been distilled into the specification.\n        * The \"Design\" team picks from here, with the spec as a resource and their contexts fresh and role-specific.\n\n</details>\n\n## What is a puzzle?\n\n<details>\n\n<summary>A puzzle is a finite state machine. It's just easier to say, write, and think about.</summary>\n\n### A stateful thing you can act upon\nImagine the Rubik's Cube puzzle. It has 43 quintillion states, and to transition between them, you act upon it by rotating the intersecting planes of the mechanism.\n\n### Properties of a puzzle\n- A finite number of discrete states, e.g., \"Series Concept and Tone\", \"World Building\", \"Arc Plotting\", \"Episode Planning\", \"Plotline Blending\", \"Episode Outline\", \"Script Writing\" etc.\n- Each state may have any number of actions (including 0) that initiate transition to another state.\n- There is an initial state.\n- There is a current state that may differ after actions have been performed on the puzzle.\n- Transitions can be canceled by state exit and enter guards, e.g., Consult LLM via client sampling request.\n\n### A Simple Example\n```json\n{\n  \"initialState\": \"LOBBY\",\n  \"states\": {\n    \"LOBBY\": {\n      \"name\": \"LOBBY\",\n      \"actions\": {\n        \"START_GAME\": { \"name\": \"START_GAME\", \"targetState\": \"PLAYING\" }\n      }\n    },\n    \"PLAYING\":  {\n      \"name\": \"PLAYING\",\n      \"actions\": {\n        \"END_GAME\": { \"name\": \"END_GAME\", \"targetState\": \"GAME_OVER\" }\n      }\n    },\n    \"GAME_OVER\": {\n      \"name\": \"GAME_OVER\",\n      \"actions\": {\n        \"RESTART\": { \"name\": \"RESTART\", \"targetState\": \"PLAYING\" }\n      }\n    }\n  }\n}\n```\n\n</details>\n\n## What is puzzlebox?\n\n<details>\n\n<summary>Most MCP servers have a one-to-one relationship with the client. Puzzlebox is different.</summary>\n\n### Many clients sharing dynamic resources\nPuzzlebox is an **MCP Server** implementation that:\n  - Supports multiple client connections that can create and monitor shared, dynamic resources. \n  - Manages puzzle instances\n  - Exposes tools for: \n    - Adding puzzles\n    - Getting a snapshot of the state and available actions for a given puzzle in the box\n    - Performing actions on a given puzzle in the box that trigger state transitions\n  - Exposes registered puzzles as resources\n    - Clients can use the `Puzzle Snapshot` resource template to fetch the resource by ID\n    - Resource URI is `puzzlebox:/puzzle/{puzzleId}`\n    - Clients can subscribe/unsubscribe to individual resource URIs\n\n### How It Works\n1. Clients connect to a puzzlebox SSE server.\n2. Clients register puzzles with the server.\n3. Clients can subscribe to a given puzzle to receive updates when its state changes.\n4. Clients perform actions on puzzles that may change their state and available actions.\n5. The puzzlebox server ensures that any attempted action is valid for the current state of the given puzzle.\n6. If an action is valid, a transition to the target state is initiated.\n7. During transition, optional exit and enter guards may send sampling requests to the client, the results of which could lead to cancellation of the transition (think acceptance testing by stakeholders)\n8. If guards pass, the state transition completes.\n9. When a client receives a resource updated notification, they can either read the resource or use the `get_puzzle_snapshot` tool to get the current state and available actions.\n10. Clients update their UI based on the new state.\n\n</details>\n\n## MCP Tools\n<details>\n<summary>These functions are exposed to the agents for managing puzzles.</summary>\n\n### ⚙️ **`add_puzzle`**\n#### Add a new instance of a puzzle (finite state machine).\n- **Inputs:** None\n- **Returns:** JSON object with boolean `success` and `puzzleId`\n\n### ⚙️ **`get_puzzle_snapshot`**\n#### Get a snapshot of a puzzle (its current state and available actions).\n- **Inputs:** `puzzleId`\n- **Returns:** JSON object with `currentState` and `availableActions` array\n- **Note:** MCP clients that don't support resource subscriptions can poll this tool to watch for state changes.\n\n### ⚙️ **`perform_action_on_puzzle`**\n#### Perform an action on a puzzle (attempt a state transition).\n- **Inputs:** `puzzleId` and `actionName`\n- **Returns:** JSON object with `currentState` and `availableActions` array\n\n### ⚙️ **`count_puzzles`**\n#### Get the count of registered puzzles\n- **Inputs:** None\n- **Returns:** JSON object with current `count` of registered puzzles\n\n</details>\n\n## Local Setup\n<details>\n<summary>\nRunning locally requires <a href=\"https://nodejs.org/en/download\" target=\"_blank\">Node and npm be installed</a>. Then follow these steps...\n</summary>\n\n### Install Dependencies\n\n- `cd /path/to/puzzlebox/`\n- `npm install`\n\n### Build\n\n- `npm run build`\n- Builds the MCP server runtime at `/dist/index.js`\n\n### Start\n\n- `npm run start`\n- Launches an SSE-based/MCP server on port `:3001` with endpoint `/sse`\n- **MUST BE LAUNCHED BEFORE RUNNING INSPECTOR**\n\n### Inspector\n\n- `npm run inspector`\n- Runs the [Model Context Protocol Inspector](https://modelcontextprotocol.io/docs/tools/inspector)\n- The Inspector UI will be available at: http://localhost:5173\n- In the Inspector UI:\n  - Make sure `Transport Type` is set to `SSE`\n  - Make sure `URL` is set to http://localhost:3001/sse\n  - Click its **\"Connect\"** button to connect to the puzzlebox server.\n    - You should see Green light 🟢and **\"Connected\"** message.\n  - Click its **List Tools** button\n\n### Format\n\n- `npm run format`\n- Runs `prettier` on the code, adjusting formatting\n\n### Typecheck\n\n- `npm run typecheck`\n- Runs `tsc` with args to check and report type issues\n\n### Lint\n\n- `npm run lint`\n- Runs `eslint` to non-destructively check for and report syntax problems\n\n### LintFix\n\n- `npm run lint:fix`\n- Runs `eslint` to check for and fix syntax problems\n\n### Test\n\n- `npm run test`\n- Run the unit tests\n\n</details>\n\n## Screenshots\n<details><summary>These screenshots show the various MCP tools and resources implemented by the sever.</summary>\n\nTesting of the server was done with the official reference client - [the MCP Inspector](https://github.com/modelcontextprotocol/inspector). \n\n### 0 - List Tools\n\n\n### 1 - Add Puzzle\n\n\n### 2 - Get Puzzle Snapshot (Initial State)\n\n\n### 3 - Perform Action On Puzzle\n\n\n### 4 - Get Puzzle Snapshot (New State)\n\n\n### 5 - Perform Action On Puzzle\n\n\n### 6 - Get Puzzle Snapshot (Another New State)\n\n\n### 7 - List Resources\n\n\n### 8 - Resource Template\n\n\n### 9 - Unsubscribed Resource\n\n\n### 10 - Subscribed Resource\n\n\n### 11 - Resource Updated Notification\n\n\n</details>",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "puzzlebox",
        "collaboration",
        "realtime",
        "realtime collaboration",
        "puzzlebox manage",
        "phases teams"
      ],
      "category": "realtime-collaboration"
    },
    "czh2774--cocosMCP": {
      "owner": "czh2774",
      "name": "cocosMCP",
      "url": "https://github.com/czh2774/cocosMCP",
      "imageUrl": "/freedevtools/mcp/pfp/czh2774.webp",
      "description": "Synchronizes logs between Cocos Creator and Cursor AI, enabling efficient problem analysis and resolution. Features include real-time log syncing, intelligent filtering, and scene management capabilities.",
      "stars": 23,
      "forks": 6,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-08-20T05:09:21Z",
      "readme_content": "# Cocos MCP Log Bridge\n\n一个强大的日志桥接工具，用于在 Cocos Creator 编辑器和 Cursor AI 之间同步日志信息，帮助开发者更有效地分析和解决问题。\n\n[![GitHub stars](https://img.shields.io/github/stars/czh2774/cocosMCP.svg)](https://github.com/czh2774/cocosMCP/stargazers)\n[![License](https://img.shields.io/github/license/czh2774/cocosMCP.svg)](https://github.com/czh2774/cocosMCP/blob/main/LICENSE)\n\n![Cocos Creator](https://img.shields.io/badge/Cocos%20Creator-3.8.0%2B-blue)\n![Cursor AI](https://img.shields.io/badge/Cursor%20AI-Compatible-green)\n\n## 🌟 功能特点\n\n- **实时日志同步**: 直接从 Cocos Creator 编辑器获取最新日志\n- **智能过滤**: 支持按类型过滤（普通日志、警告、错误）\n- **关键词搜索**: 精确定位特定问题\n- **一键清除**: 随时清空日志以减少干扰\n- **场景信息**: 获取当前场景的基本信息和节点列表\n- **场景操作**: 支持打开场景等基础操作\n- **TCP 通信桥接**: 稳定可靠的通信机制\n- **Cursor AI 集成**: 完全兼容 Cursor MCP 协议\n\n## 🚀 快速入门\n\n### 前置条件\n\n- Cocos Creator 3.8.0 或更高版本\n- Python 3.7 或更高版本\n- uv 包管理器 (推荐) 或 pip\n\n### 安装步骤\n\n1. **克隆仓库**\n   ```bash\n   git clone https://github.com/czh2774/cocosMCP.git\n   ```\n\n2. **复制到 Cocos Creator 项目**\n   \n   将克隆的 `cocosMCP` 目录复制到你的 Cocos Creator 项目的 `extensions` 目录下。\n\n3. **安装 Python 依赖**\n   ```bash\n   cd your-project/extensions/cocosMCP/Python\n   uv pip install -r requirements.txt\n   ```\n\n4. **在 Cocos Creator 中启用扩展**\n   \n   启动 Cocos Creator，进入 `扩展 -> 扩展管理器`，确保 `cocosMCP` 扩展已启用。\n\n5. **配置 Cursor AI**\n   \n   在 Cursor AI 设置中配置 MCP 服务器，指向 Python 服务器脚本。\n\n### 基本用法\n\n```python\n# 查询日志\nlogs = await mcp.query_logs({\n    \"show_logs\": True,\n    \"show_warnings\": True,\n    \"show_errors\": True\n})\n\n# 清除日志\nawait mcp.clear_logs()\n\n# 检查连接状态\nstatus = await mcp.connection_status()\n\n# 获取场景信息\nscene_info = await mcp.get_scene_info()\n\n# 列出场景中的所有节点\nnodes = await mcp.list_scene_nodes()\n\n# 打开指定UUID的场景\nawait mcp.open_scene(\"scene-uuid-here\")\n```\n\n## 📚 详细文档\n\n本项目包含三个详细的文档:\n\n- [用户使用指南](USAGE.md): 安装、配置和使用方法\n- [开发者指南](DEVELOPMENT.md): 代码结构、扩展功能和维护说明\n- [问题排查](TROUBLESHOOTING.md): 常见问题和解决方案\n\n## 🔧 技术架构\n\nCocos MCP 由三个主要部分组成:\n\n1. **Cocos Creator 扩展**: TypeScript 编写的编辑器扩展\n2. **TCP 通信桥**: 连接编辑器和 Python 服务器\n3. **Python MCP 服务器**: 处理 Cursor AI 的请求\n\n![架构图](https://via.placeholder.com/800x400?text=Cocos+MCP+Architecture)\n\n## 🤝 贡献指南\n\n欢迎贡献代码、报告问题或提出新功能建议！请查看 [开发者指南](DEVELOPMENT.md) 了解详情。\n\n## 📄 许可证\n\n本项目采用 MIT 许可证 - 详情请参阅 [LICENSE](LICENSE) 文件。\n\n## 🙏 致谢\n\n- Cocos Creator 团队提供的优秀游戏引擎\n- Cursor AI 团队开发的智能编程助手\n- 所有贡献者和用户的支持和反馈\n\n---\n\n如有问题或建议，请提交 [Issues](https://github.com/czh2774/cocosMCP/issues)。 ",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "cocosmcp",
        "cocos",
        "realtime",
        "cocosmcp synchronizes",
        "logs cocos",
        "cocos creator"
      ],
      "category": "realtime-collaboration"
    },
    "emiliobool--MCP-Relay": {
      "owner": "emiliobool",
      "name": "MCP-Relay",
      "url": "https://github.com/emiliobool/MCP-Relay",
      "imageUrl": "/freedevtools/mcp/pfp/emiliobool.webp",
      "description": "Send messages and prompts to a Discord channel and receive responses directly from an AI model. It integrates with Discord's API to facilitate real-time communication between AI and users.",
      "stars": 1,
      "forks": 1,
      "license": "No License",
      "language": "TypeScript",
      "updated_at": "2025-04-21T12:07:22Z",
      "readme_content": "# MCP Relay\n\nThis MCP server allows Claude to send messages and prompts to a Discord channel and receive responses.\n\n## Setup Instructions\n\n### 1. Create a Discord Application and Bot\n\n1. Go to the [Discord Developer Portal](https://discord.com/developers/applications)\n2. Click \"New Application\" and give it a name\n3. Go to the \"Bot\" section in the left sidebar\n4. Under the bot's token section, click \"Reset Token\" and copy the new token\n   - Keep this token secure! Don't share it publicly\n5. Under \"Privileged Gateway Intents\", enable:\n   - Message Content Intent\n   - Server Members Intent\n   - Presence Intent\n\n### 2. Invite the Bot to Your Server\n\n1. Go to the \"OAuth2\" section in the left sidebar\n2. Select \"URL Generator\"\n3. Under \"Scopes\", select:\n   - bot\n   - applications.commands\n4. Under \"Bot Permissions\", select:\n   - Send Messages\n   - Embed Links\n   - Read Message History\n5. Copy the generated URL and open it in your browser\n6. Select your server and authorize the bot\n\n### 3. Get Channel ID\n\n1. In Discord, enable Developer Mode:\n   - Go to User Settings > App Settings > Advanced\n   - Turn on \"Developer Mode\"\n2. Right-click the channel you want to use\n3. Click \"Copy Channel ID\"\n\n### 4. Configure MCP Settings\n\nThe server requires configuration in your MCP settings file. Add the following to your configuration file:\n\n```json\n{\n    \"mcpServers\": {\n        \"discord-relay\": {\n            \"command\": \"node\",\n            \"args\": [\n                \"/ABSOLUTE/PATH/TO/MCP Relay/build/index.js\"\n            ],\n            \"env\": {\n                \"DISCORD_TOKEN\": \"your_bot_token_here\",\n                \"DISCORD_CHANNEL_ID\": \"your_channel_id_here\"\n            }\n        }\n    }\n}\n```\n\nReplace:\n- `/ABSOLUTE/PATH/TO/MCP Relay` with the actual path to your MCP Relay project\n- `your_bot_token_here` with your Discord bot token\n- `your_channel_id_here` with your Discord channel ID\n\nNote: Make sure to use absolute paths in the configuration.\n\n## Usage\n\nThe server provides a tool called `send-message` that accepts the following parameters:\n\n```typescript\n{\n  type: 'prompt' | 'notification',  // Type of message\n  title: string,                    // Message title\n  content: string,                  // Message content\n  actions?: Array<{                 // Optional action buttons\n    label: string,                  // Button label\n    value: string                   // Value returned when clicked\n  }>,\n  timeout?: number                  // Optional timeout in milliseconds\n}\n```\n\n### Message Types\n\n1. **Notification**: Simple message that doesn't expect a response\n   ```json\n   {\n     \"type\": \"notification\",\n     \"title\": \"Hello\",\n     \"content\": \"This is a notification\"\n   }\n   ```\n\n2. **Prompt**: Message that waits for a response\n   ```json\n   {\n     \"type\": \"prompt\",\n     \"title\": \"Question\",\n     \"content\": \"Do you want to proceed?\",\n     \"actions\": [\n       { \"label\": \"Yes\", \"value\": \"yes\" },\n       { \"label\": \"No\", \"value\": \"no\" }\n     ],\n     \"timeout\": 60000  // Optional: 1 minute timeout\n   }\n   ```\n\nNotes:\n- Prompts can be answered either by clicking action buttons or sending a text message\n- Only one response is accepted per prompt\n- If a timeout is specified, the prompt will fail after the timeout period\n- Notifications don't wait for responses and return immediately\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "discord",
        "relay",
        "mcp",
        "mcp relay",
        "realtime collaboration",
        "discord channel"
      ],
      "category": "realtime-collaboration"
    },
    "eperez28--tldraw": {
      "owner": "eperez28",
      "name": "tldraw",
      "url": "https://github.com/eperez28/tldraw",
      "imageUrl": "/freedevtools/mcp/pfp/eperez28.webp",
      "description": "A digital whiteboard library designed for creating infinite canvas experiences in React applications, enabling collaborative drawing and design functionalities.",
      "stars": 1,
      "forks": 0,
      "license": "Other",
      "language": "",
      "updated_at": "2025-04-29T08:01:08Z",
      "readme_content": "# tldraw\n\nWelcome to the public monorepo for [tldraw](https://github.com/tldraw/tldraw). tldraw is a library for creating infinite canvas experiences in React. It's the software behind the digital whiteboard [tldraw.com](https://tldraw.com).\n\n- Read the docs and learn more at [tldraw.dev](https://tldraw.dev).\n- Learn about [our license](https://github.com/tldraw/tldraw#License).\n\n> [Click here](https://tldraw.dev/#pricing) to learn about our license and pricing.\n\n## Installation\n\n```bash\nnpm i tldraw\n```\n\n## Usage\n\n```tsx\nimport { Tldraw } from 'tldraw'\nimport 'tldraw/tldraw.css'\n\nexport default function App() {\n\treturn (\n\t\t<div style={{ position: 'fixed', inset: 0 }}>\n\t\t\t<Tldraw />\n\t\t</div>\n\t)\n}\n```\n\nLearn more at [tldraw.dev](https://tldraw.dev).\n\n## Local development\n\nThe local development server will run our examples app. The basic example will show any changes you've made to the codebase.\n\nTo run the local development server, first clone this repo.\n\nEnable [corepack](https://nodejs.org/api/corepack.html) to make sure you have the right version of `yarn`:\n\n```bash\nnpm i -g corepack\n```\n\nInstall dependencies:\n\n```bash\nyarn\n```\n\nStart the local development server:\n\n```bash\nyarn dev\n```\n\nOpen the example project at `localhost:5420`.\n\n## License\n\nThe tldraw SDK is provided under the [tldraw license](https://github.com/tldraw/tldraw/blob/main/LICENSE.md).\n\nYou can use the tldraw SDK in commercial or non-commercial projects so long as you preserve the \"Made with tldraw\" watermark on the canvas. To remove the watermark, you can purchase a [business license](https://tldraw.dev#pricing). Visit [tldraw.dev](https://tldraw.dev) to learn more.\n\n## Trademarks\n\nCopyright (c) 2024-present tldraw Inc. The tldraw name and logo are trademarks of tldraw. Please see our [trademark guidelines](https://github.com/tldraw/tldraw/blob/main/TRADEMARKS.md) for info on acceptable usage.\n\n## Distributions\n\nYou can find tldraw on npm [here](https://www.npmjs.com/package/@tldraw/tldraw?activeTab=versions).\n\n## Contribution\n\nPlease see our [contributing guide](https://github.com/tldraw/tldraw/blob/main/CONTRIBUTING.md). Found a bug? Please [submit an issue](https://github.com/tldraw/tldraw/issues/new).\n\n## Community\n\nHave questions, comments or feedback? [Join our discord](https://discord.tldraw.com/?utm_source=github&utm_medium=readme&utm_campaign=sociallink). For the latest news and release notes, visit [tldraw.dev](https://tldraw.dev).\n\n## Contributors\n\n<a href=\"https://github.com/tldraw/tldraw/graphs/contributors\">\n  <img alt=\"tldraw_max_400_columns_20\" src=\"https://contrib.rocks/image?repo=tldraw/tldraw&max=400&columns=20\" width=\"100%\"/>\n</a>\n\n## Star History\n\n<a href=\"https://star-history.com/#tldraw/tldraw\">\n\t<picture>\n\t  <source\n\t    media=\"(prefers-color-scheme: dark)\"\n\t    srcset=\"https://api.star-history.com/svg?repos=tldraw/tldraw&type=Date&theme=dark\"\n\t  />\n\t  <source\n\t    media=\"(prefers-color-scheme: light)\"\n\t    srcset=\"https://api.star-history.com/svg?repos=tldraw/tldraw&type=Date\"\n\t  />\n\t  <img src=\"https://api.star-history.com/svg?repos=tldraw/tldraw&type=Date\" alt=\"Star History Chart\" width=\"100%\" />\n\t</picture>\n</a>\n\n## Contact\n\nFind us on Twitter/X at [@tldraw](https://twitter.com/tldraw). You can contact us by email at [hello@tldraw.com](mailto:hello@tldraw.com).\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "canvas",
        "react",
        "collaborative",
        "canvas experiences",
        "collaborative drawing",
        "collaboration eperez28"
      ],
      "category": "realtime-collaboration"
    },
    "evalstate--mcp-miro": {
      "owner": "evalstate",
      "name": "mcp-miro",
      "url": "https://github.com/evalstate/mcp-miro",
      "imageUrl": "/freedevtools/mcp/pfp/evalstate.webp",
      "description": "Connects to the MIRO Whiteboard Application for board manipulation and sticky creation. Supports bulk operations and integrates with AI for enhanced functionality.",
      "stars": 88,
      "forks": 30,
      "license": "Apache License 2.0",
      "language": "TypeScript",
      "updated_at": "2025-10-01T15:19:53Z",
      "readme_content": "# mcp-miro MCP Server\n[![smithery badge](https://smithery.ai/badge/@llmindset/mcp-miro)](https://smithery.ai/server/@llmindset/mcp-miro)\n\nA Model Context Protocol server to connect to the MIRO Whiteboard Application.\n\n- Allows Board manipulation, sticky creation, bulk operations and more.\n- Pass your OAuth key as an Environment Variable, or using the \"--token\" argument.\n- Taking a photo of stickies and asking Claude to create MIRO equivalent works _really_ well.\n\n<a href=\"https://glama.ai/mcp/servers/gr5t7vthv3\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/gr5t7vthv3/badge\" alt=\"mcp-miro MCP server\" /></a>\n\n## Installation\n\n### Installing via Smithery\n\nTo install MIRO Whiteboard Connector for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@llmindset/mcp-miro):\n\n```bash\nnpx -y @smithery/cli install @llmindset/mcp-miro --client claude\n```\n\n### Using mcp-get\n\nYou can install this package using mcp-get:\n\n```bash\nnpx @michaellatman/mcp-get@latest install @llmindset/mcp-miro\n```\n\n_Note - if you are using an old version of Windows PowerShell, you may need to run_ `Set-ExecutionPolicy Bypass -Scope Process` _before this command._\n\n## Features\n\n\n\n### Resources\n- Get Board Contents \n\n### Tools\n- Create Sticky, Shape\n- Read Board, Frame, Contents\n- Bulk Create\n\n### Prompts\n- Instruct on Board Coordinates etc.\n\n## Development\n\nInstall dependencies:\n```bash\nnpm install\n```\n\nBuild the server:\n```bash\nnpm run build\n```\n\nFor development with auto-rebuild:\n```bash\nnpm run watch\n```\n\n## Installation\n\nTo use with Claude Desktop, add the server config:\n\nOn MacOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\nOn Windows: `%APPDATA%/Claude/claude_desktop_config.json`\n\n```json\n{\n  \"mcpServers\": {\n    \"mcp-miro\": {\n      \"command\": \"/path/to/node-or-npx\",\n      \"arguments\": [\n        \"/path/to/mcp-miro/build/index.js\",\n        \"--token\",\"MIRO-OAUTH-KEY\"\n      ]\n    }\n  }\n}\n```\n\n### Debugging\n\nSince MCP servers communicate over stdio, debugging can be challenging. We recommend using the [MCP Inspector](https://github.com/modelcontextprotocol/inspector), which is available as a package script:\n\n```bash\nnpm run inspector\n```\n\nThe Inspector will provide a URL to access debugging tools in your browser.\n\nIn Dev environment recommend adding https://github.com/miroapp/api-clients/blob/041de24ebf7955432b447d887ede066ad4c7e2c7/packages/generator/spec.json for reference.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "whiteboard",
        "miro",
        "board",
        "miro whiteboard",
        "whiteboard application",
        "board manipulation"
      ],
      "category": "realtime-collaboration"
    },
    "getzep--graphiti": {
      "owner": "getzep",
      "name": "graphiti",
      "url": "https://github.com/getzep/graphiti",
      "imageUrl": "/freedevtools/mcp/pfp/getzep.webp",
      "description": "Enables the construction and querying of real-time, temporally-aware knowledge graphs, managing entities, relationships, and episodes. Facilitates semantic and hybrid searches to enhance memory and reasoning in AI agents.",
      "stars": 18616,
      "forks": 1712,
      "license": "Apache License 2.0",
      "language": "Python",
      "updated_at": "2025-10-04T11:15:53Z",
      "readme_content": "<p align=\"center\">\n  <a href=\"https://www.getzep.com/\">\n    <img src=\"https://github.com/user-attachments/assets/119c5682-9654-4257-8922-56b7cb8ffd73\" width=\"150\" alt=\"Zep Logo\">\n  </a>\n</p>\n\n<h1 align=\"center\">\nGraphiti\n</h1>\n<h2 align=\"center\"> Build Real-Time Knowledge Graphs for AI Agents</h2>\n<div align=\"center\">\n\n[![Lint](https://github.com/getzep/Graphiti/actions/workflows/lint.yml/badge.svg?style=flat)](https://github.com/getzep/Graphiti/actions/workflows/lint.yml)\n[![Unit Tests](https://github.com/getzep/Graphiti/actions/workflows/unit_tests.yml/badge.svg)](https://github.com/getzep/Graphiti/actions/workflows/unit_tests.yml)\n[![MyPy Check](https://github.com/getzep/Graphiti/actions/workflows/typecheck.yml/badge.svg)](https://github.com/getzep/Graphiti/actions/workflows/typecheck.yml)\n\n![GitHub Repo stars](https://img.shields.io/github/stars/getzep/graphiti)\n[![Discord](https://img.shields.io/badge/Discord-%235865F2.svg?&logo=discord&logoColor=white)](https://discord.com/invite/W8Kw6bsgXQ)\n[![arXiv](https://img.shields.io/badge/arXiv-2501.13956-b31b1b.svg?style=flat)](https://arxiv.org/abs/2501.13956)\n[![Release](https://img.shields.io/github/v/release/getzep/graphiti?style=flat&label=Release&color=limegreen)](https://github.com/getzep/graphiti/releases)\n\n</div>\n<div align=\"center\">\n\n<a href=\"https://trendshift.io/repositories/12986\" target=\"_blank\"><img src=\"https://trendshift.io/api/badge/repositories/12986\" alt=\"getzep%2Fgraphiti | Trendshift\" style=\"width: 250px; height: 55px;\" width=\"250\" height=\"55\"/></a>\n\n</div>\n\n:star: _Help us reach more developers and grow the Graphiti community. Star this repo!_\n\n<br />\n\n> [!TIP]\n> Check out the new [MCP server for Graphiti](mcp_server/README.md)! Give Claude, Cursor, and other MCP clients powerful\n> Knowledge Graph-based memory.\n\nGraphiti is a framework for building and querying temporally-aware knowledge graphs, specifically tailored for AI agents\noperating in dynamic environments. Unlike traditional retrieval-augmented generation (RAG) methods, Graphiti\ncontinuously integrates user interactions, structured and unstructured enterprise data, and external information into a\ncoherent, queryable graph. The framework supports incremental data updates, efficient retrieval, and precise historical\nqueries without requiring complete graph recomputation, making it suitable for developing interactive, context-aware AI\napplications.\n\nUse Graphiti to:\n\n- Integrate and maintain dynamic user interactions and business data.\n- Facilitate state-based reasoning and task automation for agents.\n- Query complex, evolving data with semantic, keyword, and graph-based search methods.\n\n<br />\n\n<p align=\"center\">\n    \n</p>\n\n<br />\n\nA knowledge graph is a network of interconnected facts, such as _\"Kendra loves Adidas shoes.\"_ Each fact is a \"triplet\"\nrepresented by two entities, or\nnodes (\"Kendra\", \"Adidas shoes\"), and their relationship, or edge (\"loves\"). Knowledge Graphs have been explored\nextensively for information retrieval. What makes Graphiti unique is its ability to autonomously build a knowledge graph\nwhile handling changing relationships and maintaining historical context.\n\n## Graphiti and Zep's Context Engineering Platform.\n\nGraphiti powers the core of [Zep](https://www.getzep.com), a turn-key context engineering platform for AI Agents. Zep\noffers agent memory, Graph RAG for dynamic data, and context retrieval and assembly.\n\nUsing Graphiti, we've demonstrated Zep is\nthe [State of the Art in Agent Memory](https://blog.getzep.com/state-of-the-art-agent-memory/).\n\nRead our paper: [Zep: A Temporal Knowledge Graph Architecture for Agent Memory](https://arxiv.org/abs/2501.13956).\n\nWe're excited to open-source Graphiti, believing its potential reaches far beyond AI memory applications.\n\n<p align=\"center\">\n    <a href=\"https://arxiv.org/abs/2501.13956\"></a>\n</p>\n\n## Why Graphiti?\n\nTraditional RAG approaches often rely on batch processing and static data summarization, making them inefficient for\nfrequently changing data. Graphiti addresses these challenges by providing:\n\n- **Real-Time Incremental Updates:** Immediate integration of new data episodes without batch recomputation.\n- **Bi-Temporal Data Model:** Explicit tracking of event occurrence and ingestion times, allowing accurate point-in-time\n  queries.\n- **Efficient Hybrid Retrieval:** Combines semantic embeddings, keyword (BM25), and graph traversal to achieve\n  low-latency queries without reliance on LLM summarization.\n- **Custom Entity Definitions:** Flexible ontology creation and support for developer-defined entities through\n  straightforward Pydantic models.\n- **Scalability:** Efficiently manages large datasets with parallel processing, suitable for enterprise environments.\n\n<p align=\"center\">\n    \n</p>\n\n## Graphiti vs. GraphRAG\n\n| Aspect                     | GraphRAG                              | Graphiti                                         |\n|----------------------------|---------------------------------------|--------------------------------------------------|\n| **Primary Use**            | Static document summarization         | Dynamic data management                          |\n| **Data Handling**          | Batch-oriented processing             | Continuous, incremental updates                  |\n| **Knowledge Structure**    | Entity clusters & community summaries | Episodic data, semantic entities, communities    |\n| **Retrieval Method**       | Sequential LLM summarization          | Hybrid semantic, keyword, and graph-based search |\n| **Adaptability**           | Low                                   | High                                             |\n| **Temporal Handling**      | Basic timestamp tracking              | Explicit bi-temporal tracking                    |\n| **Contradiction Handling** | LLM-driven summarization judgments    | Temporal edge invalidation                       |\n| **Query Latency**          | Seconds to tens of seconds            | Typically sub-second latency                     |\n| **Custom Entity Types**    | No                                    | Yes, customizable                                |\n| **Scalability**            | Moderate                              | High, optimized for large datasets               |\n\nGraphiti is specifically designed to address the challenges of dynamic and frequently updated datasets, making it\nparticularly suitable for applications requiring real-time interaction and precise historical queries.\n\n## Installation\n\nRequirements:\n\n- Python 3.10 or higher\n- Neo4j 5.26 / FalkorDB 1.1.2 / Kuzu 0.11.2 / Amazon Neptune Database Cluster or Neptune Analytics Graph + Amazon\n  OpenSearch Serverless collection (serves as the full text search backend)\n- OpenAI API key (Graphiti defaults to OpenAI for LLM inference and embedding)\n\n> [!IMPORTANT]\n> Graphiti works best with LLM services that support Structured Output (such as OpenAI and Gemini).\n> Using other services may result in incorrect output schemas and ingestion failures. This is particularly\n> problematic when using smaller models.\n\nOptional:\n\n- Google Gemini, Anthropic, or Groq API key (for alternative LLM providers)\n\n> [!TIP]\n> The simplest way to install Neo4j is via [Neo4j Desktop](https://neo4j.com/download/). It provides a user-friendly\n> interface to manage Neo4j instances and databases.\n> Alternatively, you can use FalkorDB on-premises via Docker and instantly start with the quickstart example:\n\n```bash\ndocker run -p 6379:6379 -p 3000:3000 -it --rm falkordb/falkordb:latest\n\n```\n\n```bash\npip install graphiti-core\n```\n\nor\n\n```bash\nuv add graphiti-core\n```\n\n### Installing with FalkorDB Support\n\nIf you plan to use FalkorDB as your graph database backend, install with the FalkorDB extra:\n\n```bash\npip install graphiti-core[falkordb]\n\n# or with uv\nuv add graphiti-core[falkordb]\n```\n\n### Installing with Kuzu Support\n\nIf you plan to use Kuzu as your graph database backend, install with the Kuzu extra:\n\n```bash\npip install graphiti-core[kuzu]\n\n# or with uv\nuv add graphiti-core[kuzu]\n```\n\n### Installing with Amazon Neptune Support\n\nIf you plan to use Amazon Neptune as your graph database backend, install with the Amazon Neptune extra:\n\n```bash\npip install graphiti-core[neptune]\n\n# or with uv\nuv add graphiti-core[neptune]\n```\n\n### You can also install optional LLM providers as extras:\n\n```bash\n# Install with Anthropic support\npip install graphiti-core[anthropic]\n\n# Install with Groq support\npip install graphiti-core[groq]\n\n# Install with Google Gemini support\npip install graphiti-core[google-genai]\n\n# Install with multiple providers\npip install graphiti-core[anthropic,groq,google-genai]\n\n# Install with FalkorDB and LLM providers\npip install graphiti-core[falkordb,anthropic,google-genai]\n\n# Install with Amazon Neptune\npip install graphiti-core[neptune]\n```\n\n## Default to Low Concurrency; LLM Provider 429 Rate Limit Errors\n\nGraphiti's ingestion pipelines are designed for high concurrency. By default, concurrency is set low to avoid LLM\nProvider 429 Rate Limit Errors. If you find Graphiti slow, please increase concurrency as described below.\n\nConcurrency controlled by the `SEMAPHORE_LIMIT` environment variable. By default, `SEMAPHORE_LIMIT` is set to `10`\nconcurrent operations to help prevent `429` rate limit errors from your LLM provider. If you encounter such errors, try\nlowering this value.\n\nIf your LLM provider allows higher throughput, you can increase `SEMAPHORE_LIMIT` to boost episode ingestion\nperformance.\n\n## Quick Start\n\n> [!IMPORTANT]\n> Graphiti defaults to using OpenAI for LLM inference and embedding. Ensure that an `OPENAI_API_KEY` is set in your\n> environment.\n> Support for Anthropic and Groq LLM inferences is available, too. Other LLM providers may be supported via OpenAI\n> compatible APIs.\n\nFor a complete working example, see the [Quickstart Example](./examples/quickstart/README.md) in the examples directory.\nThe quickstart demonstrates:\n\n1. Connecting to a Neo4j, Amazon Neptune, FalkorDB, or Kuzu database\n2. Initializing Graphiti indices and constraints\n3. Adding episodes to the graph (both text and structured JSON)\n4. Searching for relationships (edges) using hybrid search\n5. Reranking search results using graph distance\n6. Searching for nodes using predefined search recipes\n\nThe example is fully documented with clear explanations of each functionality and includes a comprehensive README with\nsetup instructions and next steps.\n\n## MCP Server\n\nThe `mcp_server` directory contains a Model Context Protocol (MCP) server implementation for Graphiti. This server\nallows AI assistants to interact with Graphiti's knowledge graph capabilities through the MCP protocol.\n\nKey features of the MCP server include:\n\n- Episode management (add, retrieve, delete)\n- Entity management and relationship handling\n- Semantic and hybrid search capabilities\n- Group management for organizing related data\n- Graph maintenance operations\n\nThe MCP server can be deployed using Docker with Neo4j, making it easy to integrate Graphiti into your AI assistant\nworkflows.\n\nFor detailed setup instructions and usage examples, see the [MCP server README](./mcp_server/README.md).\n\n## REST Service\n\nThe `server` directory contains an API service for interacting with the Graphiti API. It is built using FastAPI.\n\nPlease see the [server README](./server/README.md) for more information.\n\n## Optional Environment Variables\n\nIn addition to the Neo4j and OpenAi-compatible credentials, Graphiti also has a few optional environment variables.\nIf you are using one of our supported models, such as Anthropic or Voyage models, the necessary environment variables\nmust be set.\n\n### Database Configuration\n\nDatabase names are configured directly in the driver constructors:\n\n- **Neo4j**: Database name defaults to `neo4j` (hardcoded in Neo4jDriver)\n- **FalkorDB**: Database name defaults to `default_db` (hardcoded in FalkorDriver)\n\nAs of v0.17.0, if you need to customize your database configuration, you can instantiate a database driver and pass it\nto the Graphiti constructor using the `graph_driver` parameter.\n\n#### Neo4j with Custom Database Name\n\n```python\nfrom graphiti_core import Graphiti\nfrom graphiti_core.driver.neo4j_driver import Neo4jDriver\n\n# Create a Neo4j driver with custom database name\ndriver = Neo4jDriver(\n    uri=\"bolt://localhost:7687\",\n    user=\"neo4j\",\n    password=\"password\",\n    database=\"my_custom_database\"  # Custom database name\n)\n\n# Pass the driver to Graphiti\ngraphiti = Graphiti(graph_driver=driver)\n```\n\n#### FalkorDB with Custom Database Name\n\n```python\nfrom graphiti_core import Graphiti\nfrom graphiti_core.driver.falkordb_driver import FalkorDriver\n\n# Create a FalkorDB driver with custom database name\ndriver = FalkorDriver(\n    host=\"localhost\",\n    port=6379,\n    username=\"falkor_user\",  # Optional\n    password=\"falkor_password\",  # Optional\n    database=\"my_custom_graph\"  # Custom database name\n)\n\n# Pass the driver to Graphiti\ngraphiti = Graphiti(graph_driver=driver)\n```\n\n#### Kuzu\n\n```python\nfrom graphiti_core import Graphiti\nfrom graphiti_core.driver.kuzu_driver import KuzuDriver\n\n# Create a Kuzu driver\ndriver = KuzuDriver(db=\"/tmp/graphiti.kuzu\")\n\n# Pass the driver to Graphiti\ngraphiti = Graphiti(graph_driver=driver)\n```\n\n#### Amazon Neptune\n\n```python\nfrom graphiti_core import Graphiti\nfrom graphiti_core.driver.neptune_driver import NeptuneDriver\n\n# Create a FalkorDB driver with custom database name\ndriver = NeptuneDriver(\n    host= < NEPTUNE\nENDPOINT >,\naoss_host = < Amazon\nOpenSearch\nServerless\nHost >,\nport = < PORT >  # Optional, defaults to 8182,\n         aoss_port = < PORT >  # Optional, defaults to 443\n)\n\ndriver = NeptuneDriver(host=neptune_uri, aoss_host=aoss_host, port=neptune_port)\n\n# Pass the driver to Graphiti\ngraphiti = Graphiti(graph_driver=driver)\n```\n\n## Using Graphiti with Azure OpenAI\n\nGraphiti supports Azure OpenAI for both LLM inference and embeddings. Azure deployments often require different\nendpoints for LLM and embedding services, and separate deployments for default and small models.\n\n> [!IMPORTANT]\n> **Azure OpenAI v1 API Opt-in Required for Structured Outputs**\n>\n> Graphiti uses structured outputs via the `client.beta.chat.completions.parse()` method, which requires Azure OpenAI\n> deployments to opt into the v1 API. Without this opt-in, you'll encounter 404 Resource not found errors during episode\n> ingestion.\n>\n> To enable v1 API support in your Azure OpenAI deployment, follow Microsoft's\n> guide: [Azure OpenAI API version lifecycle](https://learn.microsoft.com/en-us/azure/ai-foundry/openai/api-version-lifecycle?tabs=key#api-evolution).\n\n```python\nfrom openai import AsyncAzureOpenAI\nfrom graphiti_core import Graphiti\nfrom graphiti_core.llm_client import LLMConfig, OpenAIClient\nfrom graphiti_core.embedder.openai import OpenAIEmbedder, OpenAIEmbedderConfig\nfrom graphiti_core.cross_encoder.openai_reranker_client import OpenAIRerankerClient\n\n# Azure OpenAI configuration - use separate endpoints for different services\napi_key = \"<your-api-key>\"\napi_version = \"<your-api-version>\"\nllm_endpoint = \"<your-llm-endpoint>\"  # e.g., \"https://your-llm-resource.openai.azure.com/\"\nembedding_endpoint = \"<your-embedding-endpoint>\"  # e.g., \"https://your-embedding-resource.openai.azure.com/\"\n\n# Create separate Azure OpenAI clients for different services\nllm_client_azure = AsyncAzureOpenAI(\n    api_key=api_key,\n    api_version=api_version,\n    azure_endpoint=llm_endpoint\n)\n\nembedding_client_azure = AsyncAzureOpenAI(\n    api_key=api_key,\n    api_version=api_version,\n    azure_endpoint=embedding_endpoint\n)\n\n# Create LLM Config with your Azure deployment names\nazure_llm_config = LLMConfig(\n    small_model=\"gpt-4.1-nano\",\n    model=\"gpt-4.1-mini\",\n)\n\n# Initialize Graphiti with Azure OpenAI clients\ngraphiti = Graphiti(\n    \"bolt://localhost:7687\",\n    \"neo4j\",\n    \"password\",\n    llm_client=OpenAIClient(\n        config=azure_llm_config,\n        client=llm_client_azure\n    ),\n    embedder=OpenAIEmbedder(\n        config=OpenAIEmbedderConfig(\n            embedding_model=\"text-embedding-3-small-deployment\"  # Your Azure embedding deployment name\n        ),\n        client=embedding_client_azure\n    ),\n    cross_encoder=OpenAIRerankerClient(\n        config=LLMConfig(\n            model=azure_llm_config.small_model  # Use small model for reranking\n        ),\n        client=llm_client_azure\n    )\n)\n\n# Now you can use Graphiti with Azure OpenAI\n```\n\nMake sure to replace the placeholder values with your actual Azure OpenAI credentials and deployment names that match\nyour Azure OpenAI service configuration.\n\n## Using Graphiti with Google Gemini\n\nGraphiti supports Google's Gemini models for LLM inference, embeddings, and cross-encoding/reranking. To use Gemini,\nyou'll need to configure the LLM client, embedder, and the cross-encoder with your Google API key.\n\nInstall Graphiti:\n\n```bash\nuv add \"graphiti-core[google-genai]\"\n\n# or\n\npip install \"graphiti-core[google-genai]\"\n```\n\n```python\nfrom graphiti_core import Graphiti\nfrom graphiti_core.llm_client.gemini_client import GeminiClient, LLMConfig\nfrom graphiti_core.embedder.gemini import GeminiEmbedder, GeminiEmbedderConfig\nfrom graphiti_core.cross_encoder.gemini_reranker_client import GeminiRerankerClient\n\n# Google API key configuration\napi_key = \"<your-google-api-key>\"\n\n# Initialize Graphiti with Gemini clients\ngraphiti = Graphiti(\n    \"bolt://localhost:7687\",\n    \"neo4j\",\n    \"password\",\n    llm_client=GeminiClient(\n        config=LLMConfig(\n            api_key=api_key,\n            model=\"gemini-2.0-flash\"\n        )\n    ),\n    embedder=GeminiEmbedder(\n        config=GeminiEmbedderConfig(\n            api_key=api_key,\n            embedding_model=\"embedding-001\"\n        )\n    ),\n    cross_encoder=GeminiRerankerClient(\n        config=LLMConfig(\n            api_key=api_key,\n            model=\"gemini-2.5-flash-lite-preview-06-17\"\n        )\n    )\n)\n\n# Now you can use Graphiti with Google Gemini for all components\n```\n\nThe Gemini reranker uses the `gemini-2.5-flash-lite-preview-06-17` model by default, which is optimized for\ncost-effective and low-latency classification tasks. It uses the same boolean classification approach as the OpenAI\nreranker, leveraging Gemini's log probabilities feature to rank passage relevance.\n\n## Using Graphiti with Ollama (Local LLM)\n\nGraphiti supports Ollama for running local LLMs and embedding models via Ollama's OpenAI-compatible API. This is ideal\nfor privacy-focused applications or when you want to avoid API costs.\n\nInstall the models:\n\n```bash\nollama pull deepseek-r1:7b # LLM\nollama pull nomic-embed-text # embeddings\n```\n\n```python\nfrom graphiti_core import Graphiti\nfrom graphiti_core.llm_client.config import LLMConfig\nfrom graphiti_core.llm_client.openai_generic_client import OpenAIGenericClient\nfrom graphiti_core.embedder.openai import OpenAIEmbedder, OpenAIEmbedderConfig\nfrom graphiti_core.cross_encoder.openai_reranker_client import OpenAIRerankerClient\n\n# Configure Ollama LLM client\nllm_config = LLMConfig(\n    api_key=\"ollama\",  # Ollama doesn't require a real API key, but some placeholder is needed\n    model=\"deepseek-r1:7b\",\n    small_model=\"deepseek-r1:7b\",\n    base_url=\"http://localhost:11434/v1\",  # Ollama's OpenAI-compatible endpoint\n)\n\nllm_client = OpenAIGenericClient(config=llm_config)\n\n# Initialize Graphiti with Ollama clients\ngraphiti = Graphiti(\n    \"bolt://localhost:7687\",\n    \"neo4j\",\n    \"password\",\n    llm_client=llm_client,\n    embedder=OpenAIEmbedder(\n        config=OpenAIEmbedderConfig(\n            api_key=\"ollama\",  # Placeholder API key\n            embedding_model=\"nomic-embed-text\",\n            embedding_dim=768,\n            base_url=\"http://localhost:11434/v1\",\n        )\n    ),\n    cross_encoder=OpenAIRerankerClient(client=llm_client, config=llm_config),\n)\n\n# Now you can use Graphiti with local Ollama models\n```\n\nEnsure Ollama is running (`ollama serve`) and that you have pulled the models you want to use.\n\n## Documentation\n\n- [Guides and API documentation](https://help.getzep.com/graphiti).\n- [Quick Start](https://help.getzep.com/graphiti/graphiti/quick-start)\n- [Building an agent with LangChain's LangGraph and Graphiti](https://help.getzep.com/graphiti/integrations/lang-graph-agent)\n\n## Telemetry\n\nGraphiti collects anonymous usage statistics to help us understand how the framework is being used and improve it for\neveryone. We believe transparency is important, so here's exactly what we collect and why.\n\n### What We Collect\n\nWhen you initialize a Graphiti instance, we collect:\n\n- **Anonymous identifier**: A randomly generated UUID stored locally in `~/.cache/graphiti/telemetry_anon_id`\n- **System information**: Operating system, Python version, and system architecture\n- **Graphiti version**: The version you're using\n- **Configuration choices**:\n    - LLM provider type (OpenAI, Azure, Anthropic, etc.)\n    - Database backend (Neo4j, FalkorDB, Kuzu, Amazon Neptune Database or Neptune Analytics)\n    - Embedder provider (OpenAI, Azure, Voyage, etc.)\n\n### What We Don't Collect\n\nWe are committed to protecting your privacy. We **never** collect:\n\n- Personal information or identifiers\n- API keys or credentials\n- Your actual data, queries, or graph content\n- IP addresses or hostnames\n- File paths or system-specific information\n- Any content from your episodes, nodes, or edges\n\n### Why We Collect This Data\n\nThis information helps us:\n\n- Understand which configurations are most popular to prioritize support and testing\n- Identify which LLM and database providers to focus development efforts on\n- Track adoption patterns to guide our roadmap\n- Ensure compatibility across different Python versions and operating systems\n\nBy sharing this anonymous information, you help us make Graphiti better for everyone in the community.\n\n### View the Telemetry Code\n\nThe Telemetry code [may be found here](graphiti_core/telemetry/telemetry.py).\n\n### How to Disable Telemetry\n\nTelemetry is **opt-out** and can be disabled at any time. To disable telemetry collection:\n\n**Option 1: Environment Variable**\n\n```bash\nexport GRAPHITI_TELEMETRY_ENABLED=false\n```\n\n**Option 2: Set in your shell profile**\n\n```bash\n# For bash users (~/.bashrc or ~/.bash_profile)\necho 'export GRAPHITI_TELEMETRY_ENABLED=false' >> ~/.bashrc\n\n# For zsh users (~/.zshrc)\necho 'export GRAPHITI_TELEMETRY_ENABLED=false' >> ~/.zshrc\n```\n\n**Option 3: Set for a specific Python session**\n\n```python\nimport os\n\nos.environ['GRAPHITI_TELEMETRY_ENABLED'] = 'false'\n\n# Then initialize Graphiti as usual\nfrom graphiti_core import Graphiti\n\ngraphiti = Graphiti(...)\n```\n\nTelemetry is automatically disabled during test runs (when `pytest` is detected).\n\n### Technical Details\n\n- Telemetry uses PostHog for anonymous analytics collection\n- All telemetry operations are designed to fail silently - they will never interrupt your application or affect Graphiti\n  functionality\n- The anonymous ID is stored locally and is not tied to any personal information\n\n## Status and Roadmap\n\nGraphiti is under active development. We aim to maintain API stability while working on:\n\n- [x] Supporting custom graph schemas:\n    - Allow developers to provide their own defined node and edge classes when ingesting episodes\n    - Enable more flexible knowledge representation tailored to specific use cases\n- [x] Enhancing retrieval capabilities with more robust and configurable options\n- [x] Graphiti MCP Server\n- [ ] Expanding test coverage to ensure reliability and catch edge cases\n\n## Contributing\n\nWe encourage and appreciate all forms of contributions, whether it's code, documentation, addressing GitHub Issues, or\nanswering questions in the Graphiti Discord channel. For detailed guidelines on code contributions, please refer\nto [CONTRIBUTING](CONTRIBUTING.md).\n\n## Support\n\nJoin the [Zep Discord server](https://discord.com/invite/W8Kw6bsgXQ) and make your way to the **#Graphiti** channel!",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "ai",
        "graphiti",
        "knowledge",
        "ai agents",
        "reasoning ai",
        "knowledge graphs"
      ],
      "category": "realtime-collaboration"
    },
    "hanweg--mcp-discord": {
      "owner": "hanweg",
      "name": "mcp-discord",
      "url": "https://github.com/hanweg/mcp-discord",
      "imageUrl": "/freedevtools/mcp/pfp/hanweg.webp",
      "description": "Integrate and manage Discord servers, facilitate message sending, and handle user roles and interactions within channels.",
      "stars": 126,
      "forks": 37,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-10-03T22:32:05Z",
      "readme_content": "# Discord MCP Server\n\n[![smithery badge](https://smithery.ai/badge/@hanweg/mcp-discord)](https://smithery.ai/server/@hanweg/mcp-discord)\nA Model Context Protocol (MCP) server that provides Discord integration capabilities to MCP clients like Claude Desktop.\n\n<a href=\"https://glama.ai/mcp/servers/wvwjgcnppa\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/wvwjgcnppa/badge\" alt=\"mcp-discord MCP server\" /></a>\n\n## Available Tools\n\n### Server Information\n- `list_servers`: List available servers\n- `get_server_info`: Get detailed server information\n- `get_channels`: List channels in a server\n- `list_members`: List server members and their roles\n- `get_user_info`: Get detailed information about a user\n\n### Message Management\n- `send_message`: Send a message to a channel\n- `read_messages`: Read recent message history\n- `add_reaction`: Add a reaction to a message\n- `add_multiple_reactions`: Add multiple reactions to a message\n- `remove_reaction`: Remove a reaction from a message\n- `moderate_message`: Delete messages and timeout users\n\n### Channel Management\n- `create_text_channel`: Create a new text channel\n- `delete_channel`: Delete an existing channel\n\n### Role Management\n- `add_role`: Add a role to a user\n- `remove_role`: Remove a role from a user\n\n## Installation\n\n1. Set up your Discord bot:\n   - Create a new application at [Discord Developer Portal](https://discord.com/developers/applications)\n   - Create a bot and copy the token\n   - Enable required privileged intents:\n     - MESSAGE CONTENT INTENT\n     - PRESENCE INTENT\n     - SERVER MEMBERS INTENT\n   - Invite the bot to your server using OAuth2 URL Generator\n\n2. Clone and install the package:\n```bash\n# Clone the repository\ngit clone https://github.com/hanweg/mcp-discord.git\ncd mcp-discord\n\n# Create and activate virtual environment\nuv venv\n.venv\\Scripts\\activate # On macOS/Linux, use: source .venv/bin/activate\n\n### If using Python 3.13+ - install audioop library: `uv pip install audioop-lts`\n\n# Install the package\nuv pip install -e .\n```\n\n3. Configure Claude Desktop (`%APPDATA%\\Claude\\claude_desktop_config.json` on Windows, `~/Library/Application Support/Claude/claude_desktop_config.json` on macOS):\n```json\n    \"discord\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"--directory\",\n        \"C:\\\\PATH\\\\TO\\\\mcp-discord\",\n        \"run\",\n        \"mcp-discord\"\n      ],\n      \"env\": {\n        \"DISCORD_TOKEN\": \"your_bot_token\"\n      }\n    }\n```\n\n### Installing via Smithery\n\nTo install Discord Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@hanweg/mcp-discord):\n\n```bash\nnpx -y @smithery/cli install @hanweg/mcp-discord --client claude\n```\n\n## License\n\nMIT License - see LICENSE file for details.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "discord",
        "mcp",
        "hanweg",
        "mcp discord",
        "manage discord",
        "discord servers"
      ],
      "category": "realtime-collaboration"
    },
    "hirosuke0520--line-mcp-demo": {
      "owner": "hirosuke0520",
      "name": "line-mcp-demo",
      "url": "https://github.com/hirosuke0520/line-mcp-demo",
      "imageUrl": "/freedevtools/mcp/pfp/hirosuke0520.webp",
      "description": "Integrates with the LINE Messaging API to facilitate real-time communication and user engagement through text and rich messages. Allows AI agents to connect seamlessly to LINE Official Accounts for enhanced conversational interactions.",
      "stars": 0,
      "forks": 3,
      "license": "Apache License 2.0",
      "language": "TypeScript",
      "updated_at": "2025-04-15T03:21:59Z",
      "readme_content": "[日本語版 READMEはこちら](README.ja.md)\n\n# LINE Bot MCP Server\n\n[Model Context Protocol (MCP)](https://github.com/modelcontextprotocol) server implementation that integrates the LINE Messaging API to connect an AI Agent to the LINE Official Account.\n\n\n\n> [!NOTE]\n> This repository is provided as a preview version. While we offer it for experimental purposes, please be aware that it may not include complete functionality or comprehensive support.\n\n## Tools\n\n1. **push_text_message**\n   - Push a simple text message to user via LINE.\n   - **Inputs:**\n     - `user_id` (string): The user ID to receive a message. Defaults to DESTINATION_USER_ID.\n     - `message.text` (string): The plain text content to send to the user.\n2. **push_flex_message**\n   - Push a highly customizable flex message to user via LINE. Supports both bubble (single container) and carousel (multiple swipeable bubbles) layouts.\n   - **Inputs:**\n     - `user_id` (string): The user ID to receive a message. Defaults to DESTINATION_USER_ID.\n     - `message.altText` (string): Alternative text shown when flex message cannot be displayed.\n     - `message.content` (any): The content of the flex message. This is a JSON object that defines the layout and components of the message.\n     - `message.contents.type` (enum): Type of the container. 'bubble' for single container, 'carousel' for multiple swipeable bubbles.\n3. **get_profile**\n   - Get detailed profile information of a LINE user including display name, profile picture URL, status message and language.\n   - **Inputs:**\n     - `user_id` (string): The ID of the user whose profile you want to retrieve. Defaults to DESTINATION_USER_ID.\n\n\n## Installation\n\n### Step 1: Install line-bot-mcp-server\n\nrequirements:\n- Node.js v20 or later\n\nClone this repository:\n\n```\ngit clone git@github.com/line/line-bot-mcp-server.git\n```\n\nInstall the necessary dependencies and build line-bot-mcp-server when using Node.js. This step is not required when using Docker:\n\n```\ncd line-bot-mcp-server && npm install && npm run build\n```\n\n### Step 2: Get a channel access token\n\nThis MCP server utilizes a LINE Official Account. If you do not have one, please create it by following [this instructions](https://www.linebiz.com/jp-en/manual/OfficialAccountManager/new_account/). \n\nTo connect to the Messaging API, you need to have a channel access token. You can confirm this by following [this instructions](https://developers.line.biz/en/docs/basics/channel-access-token/#long-lived-channel-access-token).\n\nAdditionally, you will need the user ID of the recipient user for messages. You can confirm this by following [this instructions](https://developers.line.biz/en/docs/messaging-api/getting-user-ids/#get-own-user-id).\n\n### Step 3: Configure AI Agent\n\nPlease add the following configuration for an AI Agent like Claude Desktop or Cline. \nInsert the channel access token and user ID you obtained earlier into `CHANNEL_ACCESS_TOKEN` and `DESTINATION_USER_ID`, respectively. \nAdditionally, update the path to `line-bot-mcp-server` in  `mcpServers.args`.\n\n#### Option 1: Use Node\n\n```json\n{\n  \"mcpServers\": {\n    \"line-bot\": {\n      \"command\": \"node\",\n      \"args\": [\n        \"PATH/TO/line-bot-mcp-server/dist/index.js\"\n      ],\n      \"env\": {\n        \"CHANNEL_ACCESS_TOKEN\" : \"FILL_HERE\",\n        \"DESTINATION_USER_ID\" : \"FILL_HERE\"\n      }\n    }\n  }\n}\n```\n\n#### Option 2: Use Docker\n\nBuild the Docker image first:\n```\ndocker build -t line/line-bot-mcp-server .\n```\n\n```json\n{\n  \"mcpServers\": {\n    \"line-bot\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"-i\",\n        \"--rm\",\n        \"-e\",\n        \"CHANNEL_ACCESS_TOKEN\",\n        \"-e\",\n        \"DESTINATION_USER_ID\",\n        \"line/line-bot-mcp-server\"\n      ],\n      \"env\": {\n        \"CHANNEL_ACCESS_TOKEN\" : \"FILL_HERE\",\n        \"DESTINATION_USER_ID\" : \"FILL_HERE\"\n      }\n    }\n  }\n}\n```",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "realtime",
        "line",
        "messaging",
        "line messaging",
        "realtime collaboration",
        "line official"
      ],
      "category": "realtime-collaboration"
    },
    "hungryrobot1--MCP-PIF": {
      "owner": "hungryrobot1",
      "name": "MCP-PIF",
      "url": "https://github.com/hungryrobot1/MCP-PIF",
      "imageUrl": "/freedevtools/mcp/pfp/hungryrobot1.webp",
      "description": "Implement structured tools for meaningful human-AI interaction, facilitating progressive interaction patterns and development of understanding through the Model Context Protocol.",
      "stars": 53,
      "forks": 15,
      "license": "MIT License",
      "language": "Clojure",
      "updated_at": "2025-10-03T23:37:16Z",
      "readme_content": "# MCP-PIF-CLJS: A Self-Modifying MCP Server with Formal Reasoning\n\nA Model Context Protocol (MCP) server written in ClojureScript that explores homoiconicity, formal reasoning, and metaprogramming to enable runtime tool creation and safe self-modification capabilities. It allows models like Claude to create and execute new tools during runtime, evaluate lambda calculus expressions, perform type inference, and prove logical theorems—all without restarting the server.\n\nThis project combines Clojure's code-as-data philosophy with formal methods, providing a unique platform for exploring self-verifying code, type-driven development, and automated reasoning within an AI-assisted environment.\n\n## 🚀 Quick Start\n\n### Prerequisites\n\n- Node.js 16+\n- Java 11+ (for ClojureScript compiler)\n\n### Installation\n\n1. **Clone and build:**\n   ```bash\n   git clone <repository-url>\n   cd MCP-PIF\n   npm install\n   npx shadow-cljs compile mcp-server\n   ```\n\n2. **Configure Claude Desktop:**\n\n   Edit your Claude Desktop config file:\n   - macOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\n   - Windows: `%APPDATA%\\Claude\\claude_desktop_config.json`\n\n   ```json\n   {\n     \"mcpServers\": {\n       \"mcp-pif-cljs\": {\n         \"command\": \"node\",\n         \"args\": [\"/full/path/to/MCP-PIF/out/mcp-server.js\"]\n       }\n     }\n   }\n   ```\n\n3. **Restart Claude Desktop**\n\n### Building a .dxt Package\n\nFor easier use and distribution, you can create a .dxt package:\n\n```bash\n./package-dxt.sh\n```\n\nThis creates `mcp-pif-cljs.dxt` which can be installed via drag-and-drop in Claude Desktop.\n\n## 🛠️ Available Tools\n\n### Base Tools\n- `memory-store` - Store key-value pairs in memory\n- `memory-retrieve` - Retrieve stored values\n- `journal-recent` - View recent activity journal\n- `server-info` - Get comprehensive server information (all tools, state, statistics)\n\n### Meta-Programming Tools\n- `meta-evolve` - Create new tools at runtime (arithmetic, string, lambda, typed)\n- `execute-tool` - Execute any tool by name (including dynamic ones)\n\n### Formal Reasoning Tools\n- `lambda-eval` - Evaluate lambda calculus expressions with beta reduction\n- `type-check` - Perform Hindley-Milner type inference on expressions\n- `prove` - Automated theorem proving for propositional logic\n\n## 💡 Example Usage\n\n### Basic Memory Storage\n```\nYou: \"Store my favorite programming language as ClojureScript\"\nClaude: I'll store that for you using the memory-store tool.\n\nYou: \"What's my favorite programming language?\"\nClaude: Your favorite programming language is ClojureScript.\n```\n\n### Creating Custom Tools\n```\nYou: \"I need a tool that calculates the area of a circle\"\n\nClaude: I'll create that tool for you using meta-evolve...\n[Creates tool with code: (args) => Math.PI * args.radius * args.radius]\n\nYou: \"What's the area of a circle with radius 5?\"\nClaude: The area is 78.54 square units.\n```\n\n### Lambda Calculus & Formal Reasoning\n```\nYou: \"Evaluate the lambda expression for identity function applied to 42\"\nClaude: Using lambda-eval with expression: [(λ x x) 42]\nResult: 42 (reduced in 1 step)\n\nYou: \"What's the type of function composition?\"\nClaude: Using type-check on (λ f (λ g (λ x [g [f x]])))\nType: ((a → b) → ((b → c) → (a → c)))\n\nYou: \"Prove that if A implies B and we have A, then B follows\"\nClaude: Using the prove tool with modus ponens...\nProof found! B is derived from premises A and A→B.\n```\n\n### The Dynamic Tool Workflow\n\nDue to MCP client caching, newly created tools must be called via `execute-tool`:\n\n1. **Create a tool:**\n   ```\n   Use meta-evolve to create \"multiply\":\n   - code: \"(args) => args.x * args.y\"\n   - tool-type: \"arithmetic\"\n   ```\n\n2. **Verify creation:**\n   ```\n   Use server-info\n   (You'll see \"multiply [RUNTIME]\" in the tools list)\n   ```\n\n3. **Execute the tool:**\n   ```\n   Use execute-tool with:\n   - tool-name: \"multiply\"\n   - arguments: { x: 6, y: 7 }\n   Result: 42\n   ```\n\n## 🏗️ Architecture\n\n```\n┌─────────────────┐     ┌──────────────────┐     ┌─────────────────┐\n│   AI Client     │────▶│  MCP Protocol    │────▶│   Meta Engine   │\n│   (Claude)      │     │  (stdio/jsonrpc) │     │ (Self-Modifier) │\n└─────────────────┘     └──────────────────┘     └─────────────────┘\n                               │                          │\n                               ▼                          ▼\n                        ┌──────────────────┐     ┌─────────────────┐\n                        │   Tools/Memory   │     │    Journal DB   │\n                        │   (Extensible)   │     │   (DataScript)  │\n                        └──────────────────┘     └─────────────────┘\n```\n\n### Project Structure\n```\nsrc/mcp/\n├── core.cljs        # Main server & request routing\n├── protocol.cljs    # JSON-RPC/MCP protocol handling\n├── tools.cljs       # Tool definitions and handlers\n├── meta.cljs        # Self-modification engine\n├── evaluator.cljs   # Safe JavaScript evaluation\n├── journal.cljs     # Activity logging (DataScript)\n├── lambda.cljs      # Lambda calculus evaluator\n├── types.cljs       # Hindley-Milner type inference\n└── proof.cljs       # Automated theorem proving\n```\n\n## 🔒 Safety Mechanisms\n\n1. **Sandboxed Execution**: Limited to arithmetic and string operations\n2. **Code Validation**: Blocks dangerous operations (file system, network, process)\n3. **Namespace Protection**: Core namespaces cannot be modified\n4. **Activity Journal**: All actions are logged and auditable\n5. **Session-Only**: Changes don't persist between restarts\n\n## 🧪 Development\n\n```bash\n# Development build with hot reload\nnpx shadow-cljs watch mcp-server\n\n# Run tests\nnpm test\n\n# Create .dxt package\n./package-dxt.sh\n```\n\n### Testing the Server\n```bash\n# Basic protocol test\nnode test-clean-protocol.js\n\n# Dynamic tools test\nnode test-dynamic-tools.js\n\n# Formal reasoning test\nnode test-formal-reasoning.js\n```\n\n## 🎯 Philosophy\n\nThis project explores the intersection of:\n- **Homoiconicity**: Code as data, data as code\n- **Self-Reference**: A system that can reason about itself\n- **Controlled Evolution**: Safe boundaries for self-modification\n- **Formal Methods**: Type systems and proof checking for verified computation\n- **Human-AI Collaboration**: AI proposes, human uses\n\n## 📋 Roadmap\n\nCompleted:\n- Basic MCP server in ClojureScript\n- Runtime tool creation\n- Universal tool executor (workaround for client caching)\n- .dxt packaging support\n- Lambda calculus evaluator with Church encodings\n- Hindley-Milner type inference system\n- Automated theorem proving for propositional logic\n\nIn progress:\n- Tool composition (tools that use other tools)\n- Namespace evolution\n- Import capabilities from other MCP servers\n- Persistent tool storage\n- Dependent type system\n- SMT solver integration\n- Self-verifying tool creation\n\n## ⚠️ Important Notes\n\n1. **Tool Persistence**: Tools only exist while server is running\n2. **Client Caching**: Use `execute-tool` to call runtime-created tools\n3. **Real Computation**: Tools execute actual code, not LLM approximations\n4. **Experimental Server**: Work in progress, tools may not work as expected\n\n## 🤝 Contributing\n\nThis is an experimental project exploring metaprogramming in the context of AI tools. Contributions that enhance self-modification capabilities or improve safety are welcome!\n\n## 📄 License\n\nMIT\n\n---\n\n*\"The significant problems we face cannot be solved at the same level of thinking we were at when we created them.\" - Einstein*\n\n*This project asks: What if our tools could evolve their own thinking?*\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "ai",
        "hungryrobot1",
        "interaction",
        "collaboration hungryrobot1",
        "ai interaction",
        "hungryrobot1 mcp"
      ],
      "category": "realtime-collaboration"
    },
    "jiayao--mcp-chess": {
      "owner": "jiayao",
      "name": "mcp-chess",
      "url": "https://github.com/jiayao/mcp-chess",
      "imageUrl": "/freedevtools/mcp/pfp/jiayao.webp",
      "description": "Connect to a chess server to play games against large language models while utilizing interactive tools for visualization, move analysis, and PGN file exploration.",
      "stars": 13,
      "forks": 5,
      "license": "Apache License 2.0",
      "language": "Python",
      "updated_at": "2025-10-03T19:06:05Z",
      "readme_content": "# MCP Chess Server\n\nThis MCP let's you play chess against any LLM.\n\n## Installation\n\nTo use this chess server, add the following configuration to your MCP config:\n\n```json\n{\n  \"mcpServers\": {\n    \"chess\": {\n      \"command\": \"uvx\",\n      \"args\": [\n        \"mcp-chess\"\n      ]\n    }\n  }\n}\n```\n\n## Usage\n\nPlay a game:\n\n\n\n\nFind a position in a PGN for game analysis:\n\n\n\n## Available Tools\n\nThe server provides the following tools:\n\n*   `get_board_visualization()`: Provides the current state of the chessboard as an image. The board orientation automatically flips based on the user's assigned color.\n*   `get_turn()`: Indicates whose turn it is ('white' or 'black').\n*   `get_valid_moves()`: Lists all legal moves for the current player in UCI notation (e.g., 'e2e4', 'g1f3'). Returns an empty list if the game is over.\n*   `make_move(move_san: str)`: Makes a move on the board using Standard Algebraic Notation (SAN) (e.g., 'e4', 'Nf3', 'Bxe5'). Returns the move in SAN and UCI, the new board FEN, and game status.\n*   `new_game(user_plays_white: bool = True)`: Starts a new game, resetting the board. By default, the user plays white. Sets the user's color for board orientation. Returns a confirmation message.\n*   `find_position_in_pgn(pgn_string: str, condition: str)`: Finds the first board position in a PGN string matching a condition (e.g., \"bishop on a3\") and returns an image of that board state. The condition format is \"piece_type on square_name\". Valid piece types are \"pawn\", \"knight\", \"bishop\", \"rook\", \"queen\", \"king\".",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "chess",
        "interactive",
        "realtime",
        "chess server",
        "mcp chess",
        "chess connect"
      ],
      "category": "realtime-collaboration"
    },
    "jmagar--yarr": {
      "owner": "jmagar",
      "name": "yarr",
      "url": "https://github.com/jmagar/yarr",
      "imageUrl": "/freedevtools/mcp/pfp/null.webp",
      "description": "Automate and control your media services using natural language commands.",
      "stars": 0,
      "forks": 0,
      "license": "Unknown",
      "language": "Unknown",
      "updated_at": "",
      "readme_content": "",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "automate",
        "realtime",
        "jmagar",
        "collaboration jmagar",
        "realtime collaboration",
        "yarr automate"
      ],
      "category": "realtime-collaboration"
    },
    "johnnyinlee--lol-client-mcp": {
      "owner": "johnnyinlee",
      "name": "lol-client-mcp",
      "url": "https://github.com/johnnyinlee/lol-client-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/johnnyinlee.webp",
      "description": "Accesses real-time game data from the League of Legends client using the Live Client Data API. Provides tools for retrieving in-game statistics and other relevant information.",
      "stars": 7,
      "forks": 5,
      "license": "Apache License 2.0",
      "language": "Python",
      "updated_at": "2025-08-18T20:18:08Z",
      "readme_content": "# LoL Client MCP\n\nAn MCP (Model-Controller-Processor) server for accessing League of Legends client data. This server provides a collection of tools that communicate with the League of Legends Live Client Data API to retrieve in-game data.\n\n## Overview\n\nThis project accesses real-time game data using the League of Legends game client's Live Client Data API. It utilizes the FastMCP framework to expose various endpoints as tools.\n\nAPI information can be found at https://developer.riotgames.com/docs/lol.\n\n## Installation and Usage\n\n### Prerequisites\n\n- Python 3.8 or higher\n- [uv](https://github.com/astral-sh/uv) - Fast and reliable Python package manager\n  - Installation: `pip install uv`\n- League of Legends client installed\n\n### Project Setup\n\n1. Clone this repository:\n\n```bash\ngit clone https://github.com/yourusername/lol-client-mcp.git\ncd lol-client-mcp\n```\n\n2. Install required packages using uv:\n\n```bash\nuv pip install httpx fastmcp\n```\n\n### Running the MCP Server\n\nTo run directly:\n\n```bash\npython main.py\n```\n\n### Integration with Claude\n\nThere are two ways to use this with Claude:\n\n#### 1. Claude Desktop Configuration\n\nAdd the following to your `claude_desktop_config.json` file:\n\n```json\n{\n    \"mcpServers\": {\n        \"lol-client-mcp\": {\n            \"command\": \"uv\",\n            \"args\": [\n                \"--directory\",\n                \"C:\\\\ABSOLUTE\\\\PATH\\\\TO\\\\PARENT\\\\FOLDER\\\\lol-client-mcp\",\n                \"run\",\n                \"main.py\"\n            ]\n        }\n    }\n}\n```\n\n**Important**: Replace `C:\\\\ABSOLUTE\\\\PATH\\\\TO\\\\PARENT\\\\FOLDER\\\\lol-client-mcp` with the actual path to your project.\n\n#### 2. Using with Web Application\n\nTo connect the MCP server to the Claude web application:\n\n1. Run the MCP server:\n   ```bash\n   python main.py\n   ```\n\n2. Configure the server connection in the Claude web interface:\n   - Go to MCP settings at the bottom when starting a conversation\n   - Select 'lol-client-mcp' and connect\n\n## API Tools List\n\n### Game Data\n\n- `get_all_game_data()`: The Live League of Legends Client Data API has a number of endpoints that return a subset of the data returned by the /allgamedata endpoint. This endpoint is great for testing the Live Client Data API, but unless you actually need all the data from this endpoint, use one of the endpoints listed below that return a subset of the response.\n- `get_game_stats()`: Basic data about the game.\n- `get_event_data()`: Get a list of events that have occurred in the game.\n\n### Active Player Data\n\n- `get_active_player()`: Get all data about the active player.\n- `get_active_player_name()`: Returns the player name.\n- `get_active_player_abilities()`: Get Abilities for the active player.\n- `get_active_player_runes()`: Retrieve the full list of runes for the active player.\n\n### Player List and Individual Player Data\n\n- `get_player_list()`: Retrieve the list of heroes in the game and their stats.\n- `get_player_scores(riot_id)`: Retrieve the list of the current scores for the player.\n- `get_player_summoner_spells(riot_id)`: Retrieve the list of the summoner spells for the player.\n- `get_player_main_runes(riot_id)`: Retrieve the basic runes of any player.\n- `get_player_items(riot_id)`: Retrieve the list of items for the player.\n\n## Troubleshooting\n\n- **Connection Error**: Check if the League of Legends client is running.\n- **Timeout Error**: Verify that the game has actually started. This API does not work in the game lobby.\n\n## Precautions\n\n- This API only works when the League of Legends client is running and a game is in progress.\n- Use in compliance with Riot Games API policies.\n\n## License\n\nAll rights belong to Riot Games.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "realtime",
        "client",
        "api",
        "legends client",
        "live client",
        "game data"
      ],
      "category": "realtime-collaboration"
    },
    "kakehashi-inc--mcp-server-mattermost": {
      "owner": "kakehashi-inc",
      "name": "mcp-server-mattermost",
      "url": "https://github.com/kakehashi-inc/mcp-server-mattermost",
      "imageUrl": "/freedevtools/mcp/pfp/kakehashi-inc.webp",
      "description": "Connects to Mattermost API endpoints to retrieve and process information in real-time, allowing for monitoring of specific teams and channels with secure token-based authentication.",
      "stars": 3,
      "forks": 1,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-09-14T19:09:15Z",
      "readme_content": "# mcp-server-mattermost\n\nThis project implements a Model Context Protocol (MCP) server for Mattermost integration. It connects to Mattermost API endpoints to retrieve and process various information, making it available through standard MCP transports.\n\n## Features\n\n- Secure, token-based connection to Mattermost API endpoints\n- Supports multiple transport modes:\n  - `stdio`\n  - `http-stream`\n  - `sse`\n- Search for messages across multiple Mattermost channels\n- Customizable default channels and message fetch limits\n\n## Requirements\n\n- Node.js >= 22\n- npm >= 10\n\n## Setup\n\n1. Clone this repository:\n\n```bash\ngit clone https://github.com/kakehashi-inc/mcp-server-mattermost.git\ncd mcp-server-mattermost\n```\n\n2. Install dependencies:\n\n```bash\nnpm install\n```\n\n3. Set up your environment variables:\n\n### Required Environment Variables\n\n- `MATTERMOST_ENDPOINT`: Your Mattermost server URL\n- `MATTERMOST_TOKEN`: Your Mattermost authentication token\n- `MATTERMOST_TEAM`: The name of the team to monitor\n- `MATTERMOST_CHANNELS`: Comma-separated list of channel names to monitor\n\n### Environment Variable Setup Options\n\n#### Option 1: Direct Environment Variables\n```bash\nexport MATTERMOST_ENDPOINT=\"https://your-mattermost-server.com\"\nexport MATTERMOST_TOKEN=\"your-token-here\"\nexport MATTERMOST_TEAM=\"your-team-name\"\nexport MATTERMOST_CHANNELS=\"general,random,dev\"\n```\n\n#### Option 2: Using .env file (with dotenvx)\n```bash\n# Install dotenvx (optional)\nnpm install -g @dotenvx/dotenvx\n\n# Create .env file\ncp .env.example .env\n# Edit .env file with your values\n\n# Encrypt your .env file (recommended for production)\ndotenvx encrypt\n```\n\n4. Build the server:\n\n```bash\nnpm run build\n```\n\n## Usage\n\nThe server supports three transport modes: stdio (default), sse, and http-stream.\n\n### Standard I/O Transport Mode\n\n```bash\n# Using npm scripts (with dotenvx)\nnpm run start:stdio\n\n# Direct execution\nnode dist/main.js --transport stdio\n\n# Using npx\nnpx mcp-server-mattermost --transport stdio\n```\n\n### SSE Transport Mode\n\n```bash\n# Using npm scripts (with dotenvx)\nnpm run start:sse\n\n# Direct execution\nnode dist/main.js --transport sse\n```\n\n### HTTP Transport Mode\n\n```bash\n# Using npm scripts (with dotenvx)\nnpm run start:http\n\n# Direct execution\nnode dist/main.js --transport http-stream\n```\n\n## Claude Desktop Integration\n\nTo use this MCP server with Claude Desktop, add the following configuration to your Claude Desktop settings:\n\n### Sample Configuration\n\n```json\n{\n  \"mcpServers\": {\n    \"mattermost\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"mcp-server-mattermost@latest\",\n        \"--transport\", \"stdio\",\n        \"--endpoint\", \"https://your-mattermost-server/api/v4\",\n        \"--token\", \"your_personal_access_token\",\n        \"--team\", \"your_team_name\",\n        \"--channels\", \"town-square,general,your_channel_name\"\n      ]\n    }\n  }\n}\n```\n\n## Development\n\n- `npm run dev`: Start the server in development mode with hot reload\n- `npm run lint`: Run ESLint\n- `npm run format`: Format code using Prettier\n- `npm test`: Run tests\n- `npm run inspect`: Run MCP inspector\n\n## References\n\n- [Model Context Protocol TypeScript SDK](https://github.com/modelcontextprotocol/typescript-sdk)\n- [MCP inspector](https://github.com/modelcontextprotocol/inspector)\n\n## License\n\nMIT\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "kakehashi",
        "mcp",
        "monitoring",
        "collaboration kakehashi",
        "mattermost api",
        "kakehashi mcp"
      ],
      "category": "realtime-collaboration"
    },
    "kazuph--mcp-devin": {
      "owner": "kazuph",
      "name": "mcp-devin",
      "url": "https://github.com/kazuph/mcp-devin",
      "imageUrl": "/freedevtools/mcp/pfp/kazuph.webp",
      "description": "Integrate Devin AI sessions with Slack for automated task posting, maintaining context in conversations, and managing session information efficiently. Facilitates communication between Devin AI and Slack threads while providing session details and task updates.",
      "stars": 4,
      "forks": 3,
      "license": "No License",
      "language": "JavaScript",
      "updated_at": "2025-08-29T19:33:59Z",
      "readme_content": "# @kazuph/mcp-devin MCP Server with Slack Integration\n\nMCP server for Devin AI with Slack integration\n\nThis is a TypeScript-based MCP server that provides integration between Devin AI and Slack. The server enables:\n\n- Creating Devin sessions and automatically posting tasks to Slack\n- Sending messages to Devin sessions and the corresponding Slack threads\n- Managing sessions with enhanced Slack integration\n\n## Features\n\n### Slack Integration\n- Automatically posts Devin tasks to Slack with `@Devin` mentions\n- Maintains thread context between Devin sessions and Slack threads\n- Uses Slack Bot token for authentication\n\n### Tools\n- `create_devin_session` - Create a new Devin session and post to Slack\n  - Posts task to a designated Slack channel with `@Devin` mention\n  - Returns session details and Slack message information\n- `send_message_to_session` - Send a message to a Devin session with optional Slack thread\n  - Can simultaneously post to the Slack thread when provided\n- `get_devin_session` - Get session details with optional Slack message history\n- `list_devin_sessions` - List all Devin sessions\n- `get_organization_info` - Get information about your Devin organization\n\n## Development\n\nInstall dependencies:\n```bash\npnpm install\n```\n\nBuild the server:\n```bash\npnpm run build\n```\n\nFor development with auto-rebuild:\n```bash\npnpm run watch\n```\n\n## Configuration\n\n### MCP Server Configuration\n\nThe server is configured through the MCP server configuration file. Add the following to your configuration:\n\n```json\n\"devin-mono\": {\n  \"command\": \"node\",\n  \"args\": [\"/path/to/mcp-devin/build/index.js\"],\n  \"env\": {\n    \"DEVIN_API_KEY\": \"your-devin-api-key\",\n    \"DEVIN_ORG_NAME\": \"Your Organization\",\n    \"SLACK_BOT_TOKEN\": \"xoxb-your-slack-bot-token\",\n    \"SLACK_DEFAULT_CHANNEL\": \"general\"\n  }\n}\n```\n\n### Required Environment Variables\n\nThe following environment variables must be set in the `env` section:\n\n- `DEVIN_API_KEY`: Your Devin API key\n- `DEVIN_ORG_NAME`: (Optional) Your organization name, defaults to \"Default Organization\"\n- `DEVIN_BASE_URL`: (Optional) Base URL for the Devin API, defaults to \"https://api.devin.ai/v1\"\n- `SLACK_BOT_TOKEN`: Your Slack Bot User OAuth Token (starts with xoxb-)\n- `SLACK_DEFAULT_CHANNEL`: The default Slack channel where messages will be posted. You can use either:\n  - Channel ID (e.g. `C123ABC456`)\n  - Channel name (e.g. `general` or `#general`)\n\n## Installation\n\nTo use with Claude Desktop, add the server config:\n\nOn MacOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\nOn Windows: `%APPDATA%/Claude/claude_desktop_config.json`\n\n```json\n{\n  \"mcpServers\": {\n    \"@kazuph/mcp-devin\": {\n      \"command\": \"/path/to/@kazuph/mcp-devin/build/index.js\"\n    }\n  }\n}\n```\n\n### Debugging\n\nSince MCP servers communicate over stdio, debugging can be challenging. We recommend using the [MCP Inspector](https://github.com/modelcontextprotocol/inspector), which is available as a package script:\n\n```bash\npnpm run inspector\n```\n\nThe Inspector will provide a URL to access debugging tools in your browser.\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "slack",
        "realtime",
        "session",
        "realtime collaboration",
        "slack automated",
        "ai sessions"
      ],
      "category": "realtime-collaboration"
    },
    "kstrikis--ephor-mcp": {
      "owner": "kstrikis",
      "name": "ephor-mcp",
      "url": "https://github.com/kstrikis/ephor-mcp",
      "imageUrl": "/freedevtools/mcp/pfp/kstrikis.webp",
      "description": "Enables multiple AI agents to collaboratively share and access each other's responses to the same prompt, facilitating deeper dialogue and understanding across models.",
      "stars": 2,
      "forks": 2,
      "license": "MIT License",
      "language": "TypeScript",
      "updated_at": "2025-08-24T12:24:48Z",
      "readme_content": "# LLM Responses MCP Server\n\nA Model Context Protocol (MCP) server that allows multiple AI agents to share and read each other's responses to the same prompt.\n\n## Overview\n\nThis project implements an MCP server with two main tool calls:\n\n1. `submit-response`: Allows an LLM to submit its response to a prompt\n2. `get-responses`: Allows an LLM to retrieve all responses from other LLMs for a specific prompt\n\nThis enables a scenario where multiple AI agents can be asked the same question by a user, and then using these tools, the agents can read and reflect on what other LLMs said to the same question.\n\n## Installation\n\n```bash\n# Install dependencies\nbun install\n```\n\n## Development\n\n```bash\n# Build the TypeScript code\nbun run build\n\n# Start the server in development mode\nbun run dev\n```\n\n## Testing with MCP Inspector\n\nThe project includes support for the [MCP Inspector](https://github.com/modelcontextprotocol/inspector), which is a tool for testing and debugging MCP servers.\n\n```bash\n# Run the server with MCP Inspector\nbun run inspect\n```\n\nThe `inspect` script uses `npx` to run the MCP Inspector, which will launch a web interface in your browser for interacting with your MCP server.\n\nThis will allow you to:\n- Explore available tools and resources\n- Test tool calls with different parameters\n- View the server's responses\n- Debug your MCP server implementation\n\n## Usage\n\nThe server exposes two endpoints:\n\n- `/sse` - Server-Sent Events endpoint for MCP clients to connect\n- `/messages` - HTTP endpoint for MCP clients to send messages\n\n### MCP Tools\n\n#### submit-response\n\nSubmit an LLM's response to a prompt:\n\n```typescript\n// Example tool call\nconst result = await client.callTool({\n  name: 'submit-response',\n  arguments: {\n    llmId: 'claude-3-opus',\n    prompt: 'What is the meaning of life?',\n    response: 'The meaning of life is...'\n  }\n});\n```\n\n#### get-responses\n\nRetrieve all LLM responses, optionally filtered by prompt:\n\n```typescript\n// Example tool call\nconst result = await client.callTool({\n  name: 'get-responses',\n  arguments: {\n    prompt: 'What is the meaning of life?' // Optional\n  }\n});\n```\n\n## License\n\nMIT \n\n## Deployment to EC2\n\nThis project includes Docker configuration for easy deployment to EC2 or any other server environment.\n\n### Prerequisites\n\n- An EC2 instance running Amazon Linux 2 or Ubuntu\n- Security group configured to allow inbound traffic on port 62886\n- SSH access to the instance\n\n### Deployment Steps\n\n1. Clone the repository to your EC2 instance:\n   ```bash\n   git clone <your-repository-url>\n   cd <repository-directory>\n   ```\n\n2. Make the deployment script executable:\n   ```bash\n   chmod +x deploy.sh\n   ```\n\n3. Run the deployment script:\n   ```bash\n   ./deploy.sh\n   ```\n\nThe script will:\n- Install Docker and Docker Compose if they're not already installed\n- Build the Docker image\n- Start the container in detached mode\n- Display the public URL where your MCP server is accessible\n\n### Manual Deployment\n\nIf you prefer to deploy manually:\n\n1. Build the Docker image:\n   ```bash\n   docker-compose build\n   ```\n\n2. Start the container:\n   ```bash\n   docker-compose up -d\n   ```\n\n3. Verify the container is running:\n   ```bash\n   docker-compose ps\n   ```\n\n### Accessing the Server\n\nOnce deployed, your MCP server will be accessible at:\n- `http://<ec2-public-ip>:62886/sse` - SSE endpoint\n- `http://<ec2-public-ip>:62886/messages` - Messages endpoint\n\nMake sure port 62886 is open in your EC2 security group! ",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "ai",
        "collaboratively",
        "collaboration",
        "realtime collaboration",
        "agents collaboratively",
        "collaboration kstrikis"
      ],
      "category": "realtime-collaboration"
    },
    "kurror--mcp": {
      "owner": "kurror",
      "name": "mcp",
      "url": "https://github.com/kurror/mcp",
      "imageUrl": "/freedevtools/mcp/pfp/kurror.webp",
      "description": "Facilitates simultaneous communication with multiple unichat-based servers to combine responses from diverse language models, enhancing the richness of insights. Manages client connections to various unichat servers through a unified interface.",
      "stars": 0,
      "forks": 2,
      "license": "No License",
      "language": "JavaScript",
      "updated_at": "2025-03-20T18:10:14Z",
      "readme_content": "# Multichat MCP Server\n\n## Project Overview\n\nThis project is part of a larger effort to refactor a FiveM resource, aiming for a more robust and maintainable codebase. We are leveraging the Model Context Protocol (MCP) to extend the resource's capabilities by integrating with external services and APIs.\n\nThe `multichat-mcp` server specifically focuses on enabling communication with multiple unichat-based MCP servers simultaneously. This allows us to query different language models and combine their responses, potentially leading to more comprehensive and nuanced results.  It acts as a standard MCP server, exposing a `multichat` tool that the host (Roo/Cline) can use.  The `multichat-mcp` server then manages the client connections to the other unichat servers.\n\n## Current Issue\n\nWe were facing an issue with making cross-server MCP calls. The initial goal was to use the `multichat-mcp` server to directly call the `unichat` tool on other MCP servers, specifically `Lacayo 1` and `openrouter-chat`.  However, these calls were consistently returning a \"Method not found\" error (-32601).\n\nDirect calls to the `unichat` tools on `Lacayo 1` and `openrouter-chat` using the `use_mcp_tool` command *do* work correctly. This initially suggested the problem was within `multichat-mcp`'s cross-server communication. However, after extensive troubleshooting and research, we discovered that MCP does *not* support direct server-to-server communication.  The host (Roo/Cline) is responsible for coordinating all communication between servers.\n\n## Troubleshooting Steps and Approaches Tried\n\nWe have taken the following steps to diagnose and resolve the \"Method not found\" error, exploring various approaches:\n\n1.  **Verified MCP Server Configurations:** We carefully reviewed the `cline_mcp_settings.json` file to ensure that all servers (`multichat`, `Lacayo 1`, and `openrouter-chat`) are correctly configured, with the correct commands, arguments, and environment variables.\n\n2.  **Checked JSON-RPC Request Formats:** We consulted the MCP documentation and examples to ensure that the JSON-RPC requests sent by `multichat-mcp` were correctly formatted, including the `method`, `params`, and `id` fields.\n\n3.  **Tested Direct Calls:** We confirmed that direct calls to the `unichat` tools on `Lacayo 1` and `openrouter-chat` using `use_mcp_tool` work as expected. This isolated the issue to the cross-server communication attempts within `multichat-mcp`.\n\n4.  **Consulted Documentation via Perplexity:** We used the Perplexity MCP server extensively to search for relevant MCP documentation, examples, and troubleshooting tips.  We specifically searched for:\n\n    *   \"Model Context Protocol (MCP) cross-server communication best practices. How to make requests between MCP servers, server discovery, and client connection management. Focus on official documentation and examples.\"\n    *   \"Model Context Protocol (MCP) client connection handling and authentication. How to properly establish and verify connections between clients and servers in MCP. Focus on official documentation and TypeScript SDK examples.\"\n    *   \"MCP (Model Context Protocol) exact method names for tool execution. Looking for real-world examples of tool/call usage, JSON-RPC method names, and successful client-server interactions in the TypeScript SDK. Focus on GitHub issues and discussions about method naming.\"\n    *   \"Model Context Protocol (MCP) implementations on GitHub, focusing on server-to-server communication and cross-server request handling. Look for TypeScript/JavaScript examples from the last year. Include request routing and message passing patterns.\"\n\n    These searches helped us understand the core principles of MCP, the correct method names (`tools/list` and `tools/call`), and the client-host-server architecture.  We learned that direct server-to-server communication is *not* supported.\n\n5.  **Modified Server Code (Multiple Iterations):** We iteratively modified the `multichat-mcp` server code (`src/index.ts` and `src/server.ts`) to try different approaches, including:\n\n    *   **Incorrect Approaches (Discarded):**\n        *   Attempting direct server-to-server calls using raw JSON-RPC requests and stdio manipulation. This was based on a misunderstanding of the MCP architecture.\n        *   Creating multiple `Client` instances within a loop, each attempting to connect to a different server. This is incorrect as each client should connect to a single server.\n        *   Trying to use a custom `mcp_instructions` response type to instruct the host to make calls. MCP does not support custom message types for cross-server communication.\n        *   Using incorrect method names like `tool/call` (singular) instead of `tools/call` (plural).\n        *   Attempting to use `rpc.discover` and `mcp.tools.list` which are not standard MCP methods.\n        *   Trying to use a `tool/route` notification, which is not a standard MCP method.\n\n    *   **Correct Approach (Current Implementation):**\n        *   Creating a single `Client` instance *per target server* (Lacayo 1, openrouter-chat) and storing them in a `Map`.\n        *   Using the `StdioClientTransport` to spawn the `unichat-ts-mcp-server` as a subprocess. This is necessary because `multichat-mcp` needs to act as a *client* to the unichat servers.\n        *   Using the `client.listTools()` method to verify the connection and discover available tools.\n        *   Using the `client.request()` method with the correct `tools/call` method name and parameters, following the standard JSON-RPC 2.0 format.\n        *   Using the `client.connect()` and handling the transport correctly.\n        *   Properly initializing the client with capabilities.\n        *   Using Zod schema validation to ensure the request and response formats are correct.\n        *   Ensuring the necessary dependencies are installed (`package.json`) and the code is correctly built (`tsconfig.json` and `npm run build`).\n\n## Current Status\n\nThe `multichat-mcp` server is currently still returning \"Method not found\" errors. We are still debugging the issue, but we have made significant progress in understanding the correct MCP architecture and implementation patterns. We are now using the correct client-server communication model, but there may still be subtle issues with our request formatting or server configuration.\n\n**Files Involved:**\n\n*   `src/server.ts`: The main server implementation.\n*   `src/index.ts`: The server entry point.\n*   `package.json`: Dependencies and build scripts.\n*   `tsconfig.json`: TypeScript configuration.\n*   `../../../../../../Users/kurror/AppData/Roaming/Code/User/globalStorage/rooveterinaryinc.roo-cline/settings/cline_mcp_settings.json`:  MCP server configuration file.\n\n## Installation\n\n**Prerequisites:**\n\n*   Node.js and npm installed on your system.\n\n**Steps:**\n\n1.  Navigate to the MCP servers directory:\n    ```bash\n    cd C:\\\\Users\\\\kurror\\\\AppData\\\\Roaming\\\\Roo-Code\\\\MCP\n    ```\n\n2.  Clone or create the `multichat-mcp` directory.\n\n3.  Place the server files (`package.json`, `tsconfig.json`, `src/index.ts`, `src/server.ts`) inside the `multichat-mcp` directory.\n\n4.  Install the dependencies:\n    ```bash\n    npm install\n    ```\n\n5.  Build the TypeScript code:\n    ```bash\n    npm run build\n    ```\n\n## Configuration\n\nTo enable the `multichat-mcp` server, you need to add its configuration to the `cline_mcp_settings.json` file, located at `C:\\\\Users\\\\kurror\\\\AppData\\\\Roaming\\\\Code\\\\User\\\\globalStorage\\\\rooveterinaryinc.roo-cline\\\\settings\\\\cline_mcp_settings.json`.\n\nAdd the following entry to the `mcpServers` object:\n\n```json\n{\n  \"mcpServers\": {\n    \"multichat\": {\n      \"command\": \"node\",\n      \"args\": [\n        \"C:\\\\Users\\\\kurror\\\\AppData\\\\Roaming\\\\Roo-Code\\\\MCP\\\\multichat-mcp\\\\build\\\\index.js\"\n      ],\n      \"env\": {}\n    }\n  }\n}\n```\n\n## Usage (Testing)\n\nYou can test the `multichat-mcp` server using the `use_mcp_tool` command in your Cline environment.\n\n**`multichat` tool:**\n\nTo send messages to multiple unichat servers and save their responses, use the following format:\n\n```\n<use_mcp_tool>\n<server_name>multichat</server_name>\n<tool_name>multichat</tool_name>\n<arguments>\n{\n  \"messages\": [\n    {\n      \"role\": \"system\",\n      \"content\": \"You are a helpful assistant.\"\n    },\n    {\n      \"role\": \"user\",\n      \"content\": \"What is your opinion about async programming?\"\n    }\n  ],\n  \"servers\": [\"Lacayo 1\", \"openrouter-chat\"],\n  \"outputDir\": \"test_output\"\n}\n</arguments>\n</use_mcp_tool>\n```\n\n*   `messages`: An array of messages to send to each server. The format follows the standard unichat message format (array of objects with `role` and `content`).\n*   `servers`: An array of the names of the unichat servers to call (e.g., `\"Lacayo 1\"`, `\"openrouter-chat\"`).\n*   `outputDir`: The directory name (within the MCP server's working directory) where the responses will be saved.  **Important:** The responses are *not* currently being saved correctly due to the \"Method not found\" error.\n\n**`read_response` tool:**\nTo read a saved response file (once the server is functioning correctly), use:\n\n```\n<use_mcp_tool>\n<server_name>multichat</server_name>\n<tool_name>read_response</tool_name>\n<arguments>\n{\n  \"outputDir\": \"test_output\",\n  \"server\": \"Lacayo 1\"\n}\n</arguments>\n</use_mcp_tool>\n```\n\n*   `outputDir`: The directory where responses are saved.\n*   `server`: The name of the server whose response you want to read.\n\n**Important Notes for Testing:**\n\n*   Ensure that the `Lacayo 1` and `openrouter-chat` servers are running and correctly configured in `cline_mcp_settings.json`.\n*   After making code changes to `multichat-mcp`, you *must* run `npm run build` in the `multichat-mcp` directory to compile the TypeScript code.\n*   The MCP host (Roo/Cline) automatically restarts servers when the configuration file changes, but it may be necessary to manually restart if you encounter issues.\n\n## Using Perplexity and Unichat\n\n*   **Perplexity:** You can use the Perplexity MCP server to research and gather information related to MCP, FiveM development, and any other technical topics. This can be helpful for finding documentation, examples, and solutions to problems.\n\n*   **Unichat Servers:** The `Lacayo 1` and `openrouter-chat` servers provide access to language models through the `unichat` tool. You can use these servers for general coding assistance, debugging, and generating code snippets.  You can test them directly using `use_mcp_tool` to ensure they are functioning correctly.\n\n## Updated Usage and Troubleshooting (Corrected)\n\nThe original documentation and troubleshooting steps described an incorrect approach for cross-server communication. This section provides the corrected usage instructions and addresses the timeout issues we encountered.\n\n### Dependencies\n-   `@modelcontextprotocol/sdk`: Provides the core functionality for building MCP servers and clients.\n-   `zod`: Used for schema validation and type safety.\n-   `fs/promises`, `path`, `url`, `crypto`: Node.js built-in modules for file system operations, path manipulation, URL parsing, and cryptographic functions.\n\n### `multichat` tool (Corrected Usage)\n\nThe `multichat` tool is designed to send the *same* message to multiple unichat servers and collect their responses. It does *not* facilitate direct server-to-server communication. The correct way to use it is as follows:\n\n**Important:** The `multichat` server and the `unichat` servers it communicates with must be running in *separate* terminal windows.\n\n1.  **Start the unichat servers:** Open separate terminal windows for each unichat server you want to use (e.g., \"Lacayo 1\", \"openrouter-chat\"). In each terminal, navigate to the unichat server directory and run:\n\n    ```powershell\n    cd C:\\Users\\kurror\\AppData\\Roaming\\Roo-Code\\MCP\\unichat-ts-mcp-server\n    $env:UNICHAT_MODEL=\"gpt-4o\"  # Or your desired model\n    $env:UNICHAT_API_KEY=\"your_api_key\"  # Replace with your actual API key\n    node ./build/index.js\n    ```\n\n2.  **Start the multichat server:** In a *separate* terminal window, navigate to the `multichat-mcp` directory and run:\n\n    ```powershell\n    cd C:\\Users\\kurror\\AppData\\Roaming\\Roo-Code\\MCP\\multichat-mcp\n    node ./build/index.js\n    ```\n\n3.  **Send the request:** In a *third* terminal window, navigate to the `multichat-mcp` directory and create a `request.json` file with the request content.  Then, send the request using PowerShell:\n\n    ```powershell\n    cd C:\\Users\\kurror\\AppData\\Roaming\\Roo-Code\\MCP\\multichat-mcp\n    $request = @{\n        jsonrpc = \"2.0\"\n        id = 1\n        method = \"tools/call\"\n        params = @{\n            name = \"multichat\"\n            arguments = @{\n                messages = @(\n                    @{role = \"system\"; content = \"You are a helpful assistant.\"},\n                    @{role = \"user\"; content = \"Hello, world!\"}\n                )\n                servers = @(\"Lacayo 1\", \"openrouter-chat\")\n                outputDir = \"my-test-output\"\n            }\n        }\n    } | ConvertTo-Json -Depth 10\n\n    $request | Out-File -FilePath \"request.json\" -Encoding utf8\n    Get-Content \"request.json\" | node ./build/index.js\n    ```\n\n    This will create a directory `responses/my-test-output` within the `multichat-mcp` directory, containing the responses from each server (e.g., `Lacayo 1.json`, `openrouter-chat.json`) and a `_session.json` file to track the session. If a server fails to respond, an error file (e.g., `Lacayo 1_error.json`) will be created instead.\n\n### `read_response` tool (Corrected Usage)\n\nThe `read_response` tool reads a saved response file generated by a previous `multichat` call.\n\n**Example Request (within Roo):**\n\n```\n<use_mcp_tool>\n<server_name>multichat</server_name>\n<tool_name>read_response</tool_name>\n<arguments>\n{\n  \"outputDir\": \"my-test-output\",\n  \"server\": \"Lacayo 1\"\n}\n</arguments>\n</use_mcp_tool>\n```\n\n**Important:** The `outputDir` is relative to the `multichat-mcp/responses` directory. The tool expects to find a subdirectory within `responses` that matches the `outputDir` value. Inside that subdirectory, it looks for either a `<server>.json` file (for successful responses) or a `<server>_error.json` file (for errors).\n\n### Troubleshooting (Updated)\n\n*   **Timeouts:** The primary cause of timeouts was the incorrect assumption that `multichat` could directly call other servers.  The corrected usage, with all servers running in separate terminals, addresses this. Ensure all unichat servers are running *before* sending the `multichat` request.\n*   **Invalid session directory:** This error occurs when `read_response` cannot find the specified `outputDir` within the `multichat-mcp/responses` directory. Double-check the `outputDir` value and ensure that the `multichat` command was run successfully and created the output directory.\n*   **No response files created:** If no files are created in the `responses` directory, even after following the corrected usage, there might be an issue with file system permissions or a silent error within the `multichat` server's response handling. Check the server logs for any error messages. It's also crucial to verify that the unichat servers you are targeting are running and responding correctly. You can test them individually using `use_mcp_tool` with the `unichat` tool.\n\nThe errors were:\n\n1.  Incorrect usage of `setRequestHandler`: I was passing the method name and schema separately, instead of including the method name within the schema.\n2.  Incorrect parameter access: `request.params` was not correctly typed due to the schema issue.\n3.  Incorrect response schema in `client.request`: I was using `z.any()`, which is not compatible with `RequestOptions`.\n\nThe fixes are:\n\n1.  **Define `ForwardRequestSchema` correctly:** Include the `method` field with `z.literal(\"mcp.forward\")`.\n2.  **Use `setRequestHandler` correctly:** Pass only the schema and the handler function.\n3.  **Access parameters correctly:** Use `request.params` after parsing with the schema.\n4. Use `z.unknown()` in the client.request call.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "servers",
        "kurror",
        "collaboration",
        "unichat servers",
        "collaboration kurror",
        "realtime collaboration"
      ],
      "category": "realtime-collaboration"
    },
    "liveblocks--liveblocks-mcp-server": {
      "owner": "liveblocks",
      "name": "liveblocks-mcp-server",
      "url": "https://github.com/liveblocks/liveblocks-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/liveblocks.webp",
      "description": "Interact with Liveblocks to manage rooms, threads, comments, and notifications while providing read access to Liveblocks Storage and Yjs for enhanced collaborative features.",
      "stars": 11,
      "forks": 7,
      "license": "Apache License 2.0",
      "language": "TypeScript",
      "updated_at": "2025-08-24T01:49:07Z",
      "readme_content": "<p align=\"center\">\n  <a href=\"https://liveblocks.io#gh-light-mode-only\">\n    <img src=\"https://raw.githubusercontent.com/liveblocks/liveblocks/main/.github/assets/header-light.svg\" alt=\"Liveblocks\" />\n  </a>\n  <a href=\"https://liveblocks.io#gh-dark-mode-only\">\n    <img src=\"https://raw.githubusercontent.com/liveblocks/liveblocks/main/.github/assets/header-dark.svg\" alt=\"Liveblocks\" />\n  </a>\n</p>\n\n# `liveblocks-mcp-server`\n\n[![smithery badge](https://smithery.ai/badge/@liveblocks/liveblocks-mcp-server)](https://smithery.ai/server/@liveblocks/liveblocks-mcp-server)\n\nThis MCP server allows AI to use a number of functions from our [REST API](https://liveblocks.io/docs/api-reference/rest-api-endpoints). For example, it can create, modify, and delete different aspects of Liveblocks such as rooms, threads, comments, notifications, and more. It also has read access to Storage and Yjs. [Learn more in our docs](https://liveblocks.io/docs/tools/mcp-server).\n\n## Automatic setup\n\nTo install automatically, copy your Liveblocks secret key from a project in [your dashboard](https://liveblocks.io/dashboard) and run one of the following commands, replacing `[key]` with your secret key.\n\n### Cursor\n\n```bash\nnpx -y @smithery/cli install @liveblocks/liveblocks-mcp-server --client cursor --key [key]\n```\n\n### Claude Desktop\n\n```bash\nnpx -y @smithery/cli install @liveblocks/liveblocks-mcp-server --client claude --key [key]\n```\n\n### VS Code\n\n```bash\nnpx -y @smithery/cli install @liveblocks/liveblocks-mcp-server --client vscode --key [key]\n```\n\n### Other clients\n\nFind installation information for other clients on [Smithery](https://smithery.ai/server/@liveblocks/liveblocks-mcp-server).\n\n## Manual setup\n\n<details><summary>Read more</summary>\n\n<p></p>\n\n1. Clone this repo.\n\n```bash\ngit clone https://github.com/liveblocks/liveblocks-mcp-server.git\n```\n\n2. Build the project.\n\n```bash\nnpm install\nnpm run build\n```\n\n3. Get your Liveblocks secret key from the [dashboard](https://liveblocks.io/dashboard).\n\n```\nsk_dev_Ns35f5G...\n```\n\n### Cursor\n\n4. Go to File → Cursor Settings → MCP → Add new server.\n\n5. Add the following, with the full path to the repo and your secret key:\n\n```json\n{\n  \"mcpServers\": {\n    \"liveblocks-mcp-server\": {\n      \"command\": \"node\",\n      \"args\": [\"/full/path/to/the/repo/liveblocks-mcp-server/build/index.js\"],\n      \"env\": {\n        \"LIVEBLOCKS_SECRET_KEY\": \"sk_dev_Ns35f5G...\"\n      }\n    }\n  }\n}\n```\n\n6. Check it's enabled in the MCP menu.\n\n### Claude Desktop\n\n4. Go to File → Settings → Developer → Edit Config.\n\n5. Open the JSON file, `claude_desktop_config.json`.\n\n6. Add the following, with the full path to the repo and your secret key:\n\n```json\n{\n  \"mcpServers\": {\n    \"liveblocks-mcp-server\": {\n      \"command\": \"node\",\n      \"args\": [\"/full/path/to/the/repo/liveblocks-mcp-server/build/index.js\"],\n      \"env\": {\n        \"LIVEBLOCKS_SECRET_KEY\": \"sk_dev_Ns35f5G...\"\n      }\n    }\n  }\n}\n```\n\n</details>\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "liveblocks",
        "realtime",
        "collaborative",
        "collaboration liveblocks",
        "liveblocks mcp",
        "liveblocks manage"
      ],
      "category": "realtime-collaboration"
    },
    "martianbandit--trendFinder": {
      "owner": "martianbandit",
      "name": "trendFinder",
      "url": "https://github.com/martianbandit/trendFinder",
      "imageUrl": "/freedevtools/mcp/pfp/martianbandit.webp",
      "description": "Collects and analyzes social media posts from key influencers to identify trending topics and sends notifications via Slack or Discord when new trends emerge.",
      "stars": 1,
      "forks": 0,
      "license": "MIT License",
      "language": "",
      "updated_at": "2025-03-28T19:14:02Z",
      "readme_content": "# Trend Finder 🔦\n\n**Stay on top of trending topics on social media — all in one place.**\n\nTrend Finder collects and analyzes posts from key influencers, then sends a Slack or Discord notification when it detects new trends or product launches. This has been a complete game-changer for the Firecrawl marketing team by:\n\n- **Saving time** normally spent manually searching social channels\n- **Keeping you informed** of relevant, real-time conversations\n- **Enabling rapid response** to new opportunities or emerging industry shifts\n\n_Spend less time hunting for trends and more time creating impactful campaigns._\n\n## Watch the Demo & Tutorial video\n\n[![Thumbnail](https://i.ytimg.com/vi/puimQSun92g/hqdefault.jpg)](https://www.youtube.com/watch?v=puimQSun92g)\n\nLearn how to set up Trend Finder and start monitoring trends in this video!\n\n## How it Works\n\n1. **Data Collection** 📥\n   - Monitors selected influencers' posts on Twitter/X using the X API (Warning: the X API free plan is rate limited to only monitor 1 X account every 15 min)\n   - Monitors websites for new releases and news with Firecrawl's /extract\n   - Runs on a scheduled basis using cron jobs\n\n2. **AI Analysis** 🧠\n   - Processes collected content through Together AI\n   - Identifies emerging trends, releases, and news.\n   - Analyzes sentiment and relevance\n\n3. **Notification System** 📢\n   - When significant trends are detected, sends Slack or Discord notifications based on cron job setup\n   - Provides context about the trend and its sources\n   - Enables quick response to emerging opportunities\n\n## Features\n\n- 🤖 AI-powered trend analysis using Together AI\n- 📱 Social media monitoring (Twitter/X integration)\n- 🔍 Website monitoring with Firecrawl\n- 💬 Instant Slack or Discord notifications\n- ⏱️ Scheduled monitoring using cron jobs\n\n## Prerequisites\n\n- Node.js (v14 or higher)\n- npm or yarn\n- Docker\n- Docker Compose\n- Slack workspace with webhook permissions\n- API keys for required services\n\n## Environment Variables\n\nCopy `.env.example` to `.env` and configure the following variables:\n\n```\n# Optional: API key from Together AI for trend analysis (https://www.together.ai/)\nTOGETHER_API_KEY=your_together_api_key_here\n\n# Optional: API key from DeepSeek for trend analysis (https://deepseek.com/)\nDEEPSEEK_API_KEY=\n\n# Optional: API key from OpenAI for trend analysis (https://openai.com/)\nOPENAI_API_KEY=\n\n# Required if monitoring web pages (https://www.firecrawl.dev/)\nFIRECRAWL_API_KEY=your_firecrawl_api_key_here\n\n# Required if monitoring Twitter/X trends (https://developer.x.com/)\nX_API_BEARER_TOKEN=your_twitter_api_bearer_token_here\n\n# Notification driver. Supported drivers: \"slack\", \"discord\"\nNOTIFICATION_DRIVER=discord\n\n# Required (if NOTIFICATION_DRIVER is \"slack\"): Incoming Webhook URL from Slack for notifications\nSLACK_WEBHOOK_URL=https://hooks.slack.com/services/YOUR/WEBHOOK/URL\n\n# Required (if NOTIFICATION_DRIVER is \"discord\"): Incoming Webhook URL from Discord for notifications\nDISCORD_WEBHOOK_URL=https://discord.com/api/webhooks/WEBHOOK/URL\n```\n\n## Getting Started\n\n1. **Clone the repository:**\n   ```bash\n   git clone [repository-url]\n   cd trend-finder\n   ```\n\n2. **Install dependencies:**\n   ```bash\n   npm install\n   ```\n\n3. **Configure environment variables:**\n   ```bash\n   cp .env.example .env\n   # Edit .env with your configuration\n   ```\n\n4. **Run the application:**\n   ```bash\n   # Development mode with hot reloading\n   npm run start\n\n   # Build for production\n   npm run build\n   ```\n\n## Using Docker\n\n1. **Build the Docker image:**\n   ```bash\n   docker build -t trend-finder .\n   ```\n\n2. **Run the Docker container:**\n   ```bash\n   docker run -d -p 3000:3000 --env-file .env trend-finder\n   ```\n\n## Using Docker Compose\n\n1. **Start the application with Docker Compose:**\n   ```bash\n   docker-compose up --build -d\n   ```\n\n2. **Stop the application with Docker Compose:**\n   ```bash\n   docker-compose down\n   ```\n\n## Project Structure\n\n```\ntrend-finder/\n├── src/\n│   ├── controllers/    # Request handlers\n│   ├── services/       # Business logic\n│   └── index.ts        # Application entry point\n├── .env.example        # Environment variables template\n├── package.json        # Dependencies and scripts\n└── tsconfig.json       # TypeScript configuration\n```\n\n## Contributing\n\n1. Fork the repository\n2. Create your feature branch (`git checkout -b feature/amazing-feature`)\n3. Commit your changes (`git commit -m 'Add some amazing feature'`)\n4. Push to the branch (`git push origin feature/amazing-feature`)\n5. Open a Pull Request\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "trendfinder",
        "trending",
        "trends",
        "martianbandit trendfinder",
        "trending topics",
        "identify trending"
      ],
      "category": "realtime-collaboration"
    },
    "nazimboudeffa--mcp-server-thesportsdb": {
      "owner": "nazimboudeffa",
      "name": "mcp-server-thesportsdb",
      "url": "https://github.com/nazimboudeffa/mcp-server-thesportsdb",
      "imageUrl": "/freedevtools/mcp/pfp/nazimboudeffa.webp",
      "description": "Access sports data from The Sports DB to retrieve real-time and historical sports information, enhancing conversational agents and workflows with dynamic sports-related content.",
      "stars": 0,
      "forks": 0,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-05-21T21:15:32Z",
      "readme_content": "# MCP Server for The Sports DB\n\nAn MCP Server for The Sports DB\n\n# Prerequisites\n\n- Python 3.12+\n- uv package manager\n- MCP-compatible client (e.g., Claude for Desktop)\n\n# Setup\n\n1. Clone this repository:\n   ```bash\n   git clone https://github.com/nazimboudeffa/mcp-server-thesportsdb.git\n   cd mcp-server-thesportsdb\n   ```\n\n2. Install dependencies\n    ```bash\n    pip install uv\n    uv add \"mcp[cli]\"\n    ```\n\n4. Install the server\n    ```bash\n    mcp install server.py\n    ```\n\n# Usage\n\nThen check the Claude for Desktop config file\n\n```json\n{\n  \"mcpServers\": {\n    \"thesportsdb\": {\n      \"command\": \"C:\\\\Users\\\\YOU_USERNAME\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python313\\\\Scripts\\\\uv.EXE\",\n      \"args\": [\n        \"run\",\n        \"--with\",\n        \"mcp[cli]\",\n        \"mcp\",\n        \"run\",\n        \"C:\\\\Users\\\\YOUR_USERNAME\\\\Documents\\\\GitHub\\\\mcp-server-thesportsdb\\\\server.py\"\n      ]\n    }\n  }\n}\n```",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "thesportsdb",
        "sports",
        "realtime",
        "server thesportsdb",
        "thesportsdb access",
        "sports data"
      ],
      "category": "realtime-collaboration"
    },
    "quazaai--UnityMCPIntegration": {
      "owner": "quazaai",
      "name": "UnityMCPIntegration",
      "url": "https://github.com/quazaai/UnityMCPIntegration",
      "imageUrl": "/freedevtools/mcp/pfp/quazaai.webp",
      "description": "Facilitates real-time interaction between AI assistants and Unity projects by allowing access to scene information, execution of C# code, and log monitoring within the Unity Editor. Enhances development workflows with additional file access functionalities for large language models (LLMs).",
      "stars": 94,
      "forks": 20,
      "license": "MIT License",
      "language": "C#",
      "updated_at": "2025-10-02T17:19:35Z",
      "readme_content": "# 🚀 Advacned Unity MCP Integration \n\n[![MCP](https://badge.mcpx.dev)](https://modelcontextprotocol.io/introduction)\n[![smithery badge](https://smithery.ai/badge/@quazaai/unitymcpintegration)](https://smithery.ai/server/@quazaai/unitymcpintegration)\n[![Unity](https://img.shields.io/badge/Unity-2021.3%2B-green?logo=https://w7.pngwing.com/pngs/426/535/png-transparent-unity-new-logo-tech-companies-thumbnail.png)](https://unity.com)\n[![Node.js](https://img.shields.io/badge/Node.js-18%2B-green)](https://nodejs.org)\n[![TypeScript](https://img.shields.io/badge/TypeScript-5.0-blue)](https://www.typescriptlang.org)\n[![WebSockets](https://img.shields.io/badge/WebSockets-API-orange)](https://developer.mozilla.org/en-US/docs/Web/API/WebSockets_API)\n\n[![Stars](https://img.shields.io/github/stars/quazaai/UnityMCPIntegration)](https://github.com/quazaai/UnityMCPIntegration/stargazers)\n[![Forks](https://img.shields.io/github/forks/quazaai/UnityMCPIntegration)](https://github.com/quazaai/UnityMCPIntegration/network/members)\n[![License](https://img.shields.io/github/license/quazaai/UnityMCPIntegration)](https://github.com/quazaai/UnityMCPIntegration/blob/main/LICENSE)\n\n<div align=\"center\">\n  \n</div>\n\nThis package provides a seamless integration between [Model Context Protocol (MCP)](https://modelcontextprotocol.io/) and Unity Editor, allowing AI assistants to understand and interact with your Unity projects in real-time. With this integration, AI assistants can access information about your scene hierarchy, project settings, and execute code directly in the Unity Editor context.\n\n## 📚 Features\n- Browse and manipulate project files directly\n- Access real-time information about your Unity project\n- Understand your scene hierarchy and game objects\n- Execute C# code directly in the Unity Editor\n- Monitor logs and errors\n- Control the Editor's play mode\n- Wait For Code Execution\n\n\n\n\n\n## 🚀 Getting Started\n\n### Prerequisites\n\n- Unity 2021.3 or later\n- Node.js 18+ (for running the MCP server)\n\n### Installation\n\n#### 1. Install Unity Package\n\nYou have several options to install the Unity package:\n\n**Option A: Package Manager (Git URL)**\n1. Open the Unity Package Manager (`Window > Package Manager`)\n2. Click the `+` button and select `Add package from git URL...`\n3. Enter the repository URL: `https://github.com/quazaai/UnityMCPIntegration.git`\n4. Click `Add`\n\n**Option B: Import Custom Package**\n1. Clone this repository or [download it as a unityPackage](https://github.com/quazaai/UnityMCPIntegration/releases)\n2. In Unity, go to `Assets > Import Package > Custom Package`\n3. Select the `UnityMCPIntegration.unitypackage` file\n\n\n\n#### 2. Set up the MCP Server\n\nYou have two options to run the MCP server:\n\n**Option A: Run the server directly**\n\n1. Navigate to the `mcpServer (likely <path-to-project>\\Library\\PackageCache\\com.quaza.unitymcp@d2b8f1260bca\\mcpServer\\)` directory\n2. Install dependencies:\n   ```\n   npm install\n   ```\n3. Run the server:\n   ```\n   node build/index.js\n   ```\n\n**Option B: Add to MCP Host configuration**\n\nAdd the server to your MCP Host configuration for Claude Desktop, Custom Implementation etc\n\n```json\n{\n  \"mcpServers\": {\n    \"unity-mcp-server\": {\n      \"command\": \"node\",\n      \"args\": [\n        \"path-to-project>\\\\Library\\\\PackageCache\\\\com.quaza.unitymcp@d2b8f1260bca\\\\mcpServer\\\\mcpServer\\\\build\\\\index.js\"\n      ],\n      \"env\": {\n        \"MCP_WEBSOCKET_PORT\": \"5010\"\n      }\n    }\n  }\n}\n```\n### Demo Video\n[![YouTube](http://i.ytimg.com/vi/GxTlahBXs74/hqdefault.jpg)](https://www.youtube.com/watch?v=GxTlahBXs74)\n\n### Installing via Smithery\n\nTo install Unity MCP Integration for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@quazaai/unitymcpintegration):\n\n```bash\nnpx -y @smithery/cli install @quazaai/unitymcpintegration --client claude\n```\n\n### 🔧 Usage\n\n#### Debugging and Monitoring\n\nYou can open the MCP Debug window in Unity to monitor the connection and test features:\n\n1. Go to `Window > MCP Debug`\n2. Use the debug window to:\n   - Check connection status\n   - Test code execution\n   - View logs\n   - Monitor events\n\n#### Available Tools\n\nThe Unity MCP integration provides several tools to AI assistants:\n\n##### Unity Editor Tools\n- **get_editor_state**: Get comprehensive information about the Unity project and editor state\n- **get_current_scene_info**: Get detailed information about the current scene\n- **get_game_objects_info**: Get information about specific GameObjects in the scene\n- **execute_editor_command**: Execute C# code directly in the Unity Editor\n- **get_logs**: Retrieve and filter Unity console logs\n- **verify_connection**: Check if there's an active connection to Unity Editor\n\n##### Filesystem Tools\n- **read_file**: Read contents of a file in your Unity project\n- **read_multiple_files**: Read multiple files at once\n- **write_file**: Create or overwrite a file with new content\n- **edit_file**: Make targeted edits to existing files with diff preview\n- **list_directory**: Get a listing of files and folders in a directory\n- **directory_tree**: Get a hierarchical view of directories and files\n- **search_files**: Find files matching a search pattern\n- **get_file_info**: Get metadata about a specific file or directory\n- **find_assets_by_type**: Find all assets of a specific type (e.g. Material, Prefab)\n- **list_scripts**: Get a listing of all C# scripts in the project\n\nFile paths can be absolute or relative to the Unity project's Assets folder. For example, `\"Scenes/MyScene.unity\"` refers to `<project>/Assets/Scenes/MyScene.unity`.\n\n## 🛠️ Architecture\n\nThe integration consists of two main components:\n\n1. **Unity Plugin (C#)**: Resides in the Unity Editor and provides access to Editor APIs\n2. **MCP Server (TypeScript/Node.js)**: Implements the MCP protocol and communicates with the Unity plugin\n\nCommunication between them happens via WebSocket, transferring JSON messages for commands and data.\n\n## File System Access\n\nThe Unity MCP integration now includes powerful filesystem tools that allow AI assistants to:\n\n- Browse, read, and edit files in your Unity project\n- Create new files and directories\n- Search for specific files or asset types\n- Analyze your project structure\n- Make targeted code changes with diff previews\n\nAll file operations are restricted to the Unity project directory for security. The system intelligently handles both absolute and relative paths, always resolving them relative to your project's Assets folder for convenience.\n\nExample usages:\n- Get a directory listing: `list_directory(path: \"Scenes\")`\n- Read a script file: `read_file(path: \"Scripts/Player.cs\")`\n- Edit a configuration file: `edit_file(path: \"Resources/config.json\", edits: [{oldText: \"value: 10\", newText: \"value: 20\"}], dryRun: true)`\n- Find all materials: `find_assets_by_type(assetType: \"Material\")`\n\n## 👥 Contributing\n\nContributions are welcome! Here's how you can contribute:\n\n1. Fork the repository\n2. Create a feature branch (`git checkout -b feature/amazing-feature`)\n3. Make your changes\n4. Commit your changes (`git commit -m 'Add some amazing feature'`)\n5. Push to the branch (`git push origin feature/amazing-feature`)\n6. Open a Pull Request\n\n### Development Setup\n\n**Unity Side**:\n- Open the project in Unity\n- Modify the C# scripts in the `UnityMCPConnection/Editor` directory\n\n**Server Side**:\n- Navigate to the `mcpServer` directory\n- Install dependencies: `npm install`\n- Make changes to the TypeScript files in the `src` directory\n- Build the server: `npm run build`\n- Run the server: `node build/index.js`\n\n## 📄 License\n\nThis project is licensed under the MIT License - see the LICENSE file for details.\n\n## 📞 Support\n\nIf you encounter any issues or have questions, please file an issue on the GitHub repository.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "unity",
        "unitymcpintegration",
        "ai",
        "assistants unity",
        "ai assistants",
        "collaboration quazaai"
      ],
      "category": "realtime-collaboration"
    },
    "sparfenyuk--mcp-proxy": {
      "owner": "sparfenyuk",
      "name": "mcp-proxy",
      "url": "https://github.com/sparfenyuk/mcp-proxy",
      "imageUrl": "/freedevtools/mcp/pfp/sparfenyuk.webp",
      "description": "Enables connections to SSE-based MCP servers, facilitating data streaming and interaction between standard input/output and SSE/StreamableHTTP protocols.",
      "stars": 1820,
      "forks": 175,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-10-03T22:27:00Z",
      "readme_content": "# mcp-proxy\n\n![GitHub License](https://img.shields.io/github/license/sparfenyuk/mcp-proxy)\n![PyPI - Python Version](https://img.shields.io/pypi/pyversions/mcp-proxy)\n![PyPI - Downloads](https://img.shields.io/pypi/dm/mcp-proxy)\n[![codecov](https://codecov.io/gh/sparfenyuk/mcp-proxy/graph/badge.svg?token=31VV9L7AZQ)](https://codecov.io/gh/sparfenyuk/mcp-proxy)\n\n- [mcp-proxy](#mcp-proxy)\n  - [About](#about)\n  - [1. stdio to SSE/StreamableHTTP](#1-stdio-to-ssestreamablehttp)\n    - [1.1 Configuration](#11-configuration)\n    - [1.2 Example usage](#12-example-usage)\n  - [2. SSE to stdio](#2-sse-to-stdio)\n    - [2.1 Configuration](#21-configuration)\n    - [2.2 Example usage](#22-example-usage)\n  - [Named Servers](#named-servers)\n  - [Installation](#installation)\n    - [Installing via PyPI](#installing-via-pypi)\n    - [Installing via Github repository (latest)](#installing-via-github-repository-latest)\n    - [Installing as container](#installing-as-container)\n    - [Troubleshooting](#troubleshooting)\n  - [Extending the container image](#extending-the-container-image)\n  - [Docker Compose Setup](#docker-compose-setup)\n  - [Command line arguments](#command-line-arguments)\n    - [Example config file](#example-config-file)\n  - [Testing](#testing)\n\n## About\n\nThe `mcp-proxy` is a tool that lets you switch between server transports. There are two supported modes:\n\n1. stdio to SSE/StreamableHTTP\n2. SSE to stdio\n\n## 1. stdio to SSE/StreamableHTTP\n\nRun a proxy server from stdio that connects to a remote SSE server.\n\nThis mode allows clients like Claude Desktop to communicate to a remote server over SSE even though it is not supported\nnatively.\n\n```mermaid\ngraph LR\n    A[\"Claude Desktop\"] <--> |stdio| B[\"mcp-proxy\"]\n    B <--> |SSE| C[\"External MCP Server\"]\n\n    style A fill:#ffe6f9,stroke:#333,color:black,stroke-width:2px\n    style B fill:#e6e6ff,stroke:#333,color:black,stroke-width:2px\n    style C fill:#e6ffe6,stroke:#333,color:black,stroke-width:2px\n```\n\n### 1.1 Configuration\n\nThis mode requires providing the URL of the MCP Server's SSE endpoint as the program’s first argument. If the server uses Streamable HTTP transport, make sure to enforce it on the `mcp-proxy` side by passing `--transport=streamablehttp`.\n\nArguments\n\n| Name             | Required | Description                                                                                                       | Example                                       |\n| ---------------- | -------- | ----------------------------------------------------------------------------------------------------------------- | --------------------------------------------- |\n| `command_or_url` | Yes      | The MCP server SSE endpoint to connect to                                                                         | http://example.io/sse                         |\n| `--headers`      | No       | Headers to use for the MCP server SSE connection                                                                  | Authorization 'Bearer my-secret-access-token' |\n| `--transport`    | No       | Decides which transport protocol to use when connecting to an MCP server. Can be either 'sse' or 'streamablehttp' | streamablehttp                                |\n\nEnvironment Variables\n\n| Name               | Required | Description                                                                  | Example    |\n| ------------------ | -------- | ---------------------------------------------------------------------------- | ---------- |\n| `API_ACCESS_TOKEN` | No       | Can be used instead of `--headers Authorization 'Bearer <API_ACCESS_TOKEN>'` | YOUR_TOKEN |\n\n### 1.2 Example usage\n\n`mcp-proxy` is supposed to be started by the MCP Client, so the configuration must be done accordingly.\n\nFor Claude Desktop, the configuration entry can look like this:\n\n```json\n{\n  \"mcpServers\": {\n    \"mcp-proxy\": {\n      \"command\": \"mcp-proxy\",\n      \"args\": [\n        \"http://example.io/sse\"\n      ],\n      \"env\": {\n        \"API_ACCESS_TOKEN\": \"access-token\"\n      }\n    }\n  }\n}\n```\n\n## 2. SSE to stdio\n\nRun a proxy server exposing a SSE server that connects to a local stdio server.\n\nThis allows remote connections to the local stdio server. The `mcp-proxy` opens a port to listen for SSE requests,\nspawns a local stdio server that handles MCP requests.\n\n```mermaid\ngraph LR\n    A[\"LLM Client\"] <-->|SSE| B[\"mcp-proxy\"]\n    B <-->|stdio| C[\"Local MCP Server\"]\n\n    style A fill:#ffe6f9,stroke:#333,color:black,stroke-width:2px\n    style B fill:#e6e6ff,stroke:#333,color:black,stroke-width:2px\n    style C fill:#e6ffe6,stroke:#333,color:black,stroke-width:2px\n```\n\n### 2.1 Configuration\n\nThis mode requires the `--sse-port` argument to be set. The `--sse-host` argument can be set to specify the host IP\naddress that the SSE server will listen on. Additional environment variables can be passed to the local stdio server\nusing the `--env` argument. The command line arguments for the local stdio server must be passed after the `--`\nseparator.\n\nArguments\n\n| Name                                 | Required                   | Description                                                                                   | Example                                     |\n| ------------------------------------ | -------------------------- | --------------------------------------------------------------------------------------------- | ------------------------------------------- |\n| `command_or_url`                     | Yes                        | The command to spawn the MCP stdio server                                                     | uvx mcp-server-fetch                        |\n| `--port`                             | No, random available       | The MCP server port to listen on                                                              | 8080                                        |\n| `--host`                             | No, `127.0.0.1` by default | The host IP address that the MCP server will listen on                                        | 0.0.0.0                                     |\n| `--env`                              | No                         | Additional environment variables to pass to the MCP stdio server. Can be used multiple times. | FOO BAR                                     |\n| `--cwd`                              | No                         | The working directory to pass to the MCP stdio server process.                                | /tmp                                        |\n| `--pass-environment`                 | No                         | Pass through all environment variables when spawning the server                               | --no-pass-environment                       |\n| `--allow-origin`                     | No                         | Allowed origins for the SSE server. Can be used multiple times. Default is no CORS allowed.   | --allow-origin \"\\*\"                         |\n| `--stateless`                        | No                         | Enable stateless mode for streamable http transports. Default is False                        | --no-stateless                              |\n| `--named-server NAME COMMAND_STRING` | No                         | Defines a named stdio server.                                                                 | --named-server fetch 'uvx mcp-server-fetch' |\n| `--named-server-config FILE_PATH`    | No                         | Path to a JSON file defining named stdio servers.                                             | --named-server-config /path/to/servers.json |\n| `--sse-port` (deprecated)            | No, random available       | The SSE server port to listen on                                                              | 8080                                        |\n| `--sse-host` (deprecated)            | No, `127.0.0.1` by default | The host IP address that the SSE server will listen on                                        | 0.0.0.0                                     |\n\n### 2.2 Example usage\n\nTo start the `mcp-proxy` server that listens on port 8080 and connects to the local MCP server:\n\n```bash\n# Start the MCP server behind the proxy\nmcp-proxy uvx mcp-server-fetch\n\n# Start the MCP server behind the proxy with a custom port\n# (deprecated) mcp-proxy --sse-port=8080 uvx mcp-server-fetch\nmcp-proxy --port=8080 uvx mcp-server-fetch\n\n# Start the MCP server behind the proxy with a custom host and port\n# (deprecated) mcp-proxy --sse-host=0.0.0.0 --sse-port=8080 uvx mcp-server-fetch\nmcp-proxy --host=0.0.0.0 --port=8080 uvx mcp-server-fetch\n\n# Start the MCP server behind the proxy with a custom user agent\n# Note that the `--` separator is used to separate the `mcp-proxy` arguments from the `mcp-server-fetch` arguments\n# (deprecated) mcp-proxy --sse-port=8080 -- uvx mcp-server-fetch --user-agent=YourUserAgent\nmcp-proxy --port=8080 -- uvx mcp-server-fetch --user-agent=YourUserAgent\n\n# Start multiple named MCP servers behind the proxy\nmcp-proxy --port=8080 --named-server fetch 'uvx mcp-server-fetch' --named-server fetch2 'uvx mcp-server-fetch'\n\n# Start multiple named MCP servers using a configuration file\nmcp-proxy --port=8080 --named-server-config ./servers.json\n```\n\n## Named Servers\n\n- `NAME` is used in the URL path `/servers/NAME/`.\n- `COMMAND_STRING` is the command to start the server (e.g., 'uvx mcp-server-fetch').\n  - Can be used multiple times.\n  - This argument is ignored if `--named-server-config` is used.\n- `FILE_PATH` - If provided, this is the exclusive source for named servers, and `--named-server` CLI arguments are ignored.\n\nIf a default server is specified (the `command_or_url` argument without `--named-server` or `--named-server-config`), it will be accessible at the root paths (e.g., `http://127.0.0.1:8080/sse`).\n\nNamed servers (whether defined by `--named-server` or `--named-server-config`) will be accessible under `/servers/<server-name>/` (e.g., `http://127.0.0.1:8080/servers/fetch1/sse`).\nThe `/status` endpoint provides global status.\n\n**JSON Configuration File Format for `--named-server-config`:**\n\nThe JSON file should follow this structure:\n\n```json\n{\n  \"mcpServers\": {\n    \"fetch\": {\n      \"disabled\": false,\n      \"timeout\": 60,\n      \"command\": \"uvx\",\n      \"args\": [\n        \"mcp-server-fetch\"\n      ],\n      \"transportType\": \"stdio\"\n    },\n    \"github\": {\n      \"timeout\": 60,\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"@modelcontextprotocol/server-github\"\n      ],\n      \"env\": {\n        \"GITHUB_PERSONAL_ACCESS_TOKEN\": \"<YOUR_TOKEN>\"\n      },\n      \"transportType\": \"stdio\"\n    }\n  }\n}\n```\n\n- `mcpServers`: A dictionary where each key is the server name (used in the URL path, e.g., `/servers/fetch/`) and the value is an object defining the server.\n- `command`: (Required) The command to execute for the stdio server.\n- `args`: (Optional) A list of arguments for the command. Defaults to an empty list.\n- `enabled`: (Optional) If `false`, this server definition will be skipped. Defaults to `true`.\n- `timeout` and `transportType`: These fields are present in standard MCP client configurations but are currently **ignored** by `mcp-proxy` when loading named servers. The transport type is implicitly \"stdio\".\n\n## Installation\n\n### Installing via PyPI\n\nThe stable version of the package is available on the PyPI repository. You can install it using the following command:\n\n```bash\n# Option 1: With uv (recommended)\nuv tool install mcp-proxy\n\n# Option 2: With pipx (alternative)\npipx install mcp-proxy\n```\n\nOnce installed, you can run the server using the `mcp-proxy` command. See configuration options for each mode above.\n\n### Installing via Github repository (latest)\n\nThe latest version of the package can be installed from the git repository using the following command:\n\n```bash\nuv tool install git+https://github.com/sparfenyuk/mcp-proxy\n```\n\n> [!NOTE]\n> If you have already installed the server, you can update it using `uv tool upgrade --reinstall` command.\n\n> [!NOTE]\n> If you want to delete the server, use the `uv tool uninstall mcp-proxy` command.\n\n### Installing as container\n\nStarting from version 0.3.2, it's possible to pull and run the corresponding container image:\n\n```bash\ndocker run --rm -t ghcr.io/sparfenyuk/mcp-proxy:v0.3.2-alpine --help\n```\n\n### Troubleshooting\n\n- **Problem**: Claude Desktop can't start the server: ENOENT code in the logs\n\n  **Solution**: Try to use the full path to the binary. To do so, open a terminal and run the command`where mcp-proxy` (\n  macOS, Linux) or `where.exe mcp-proxy` (Windows). Then, use the output path as a value for 'command' attribute:\n  ```json\n    \"fetch\": {\n      \"command\": \"/full/path/to/bin/mcp-proxy\",\n      \"args\": [\n        \"http://localhost:8932/sse\"\n      ]\n    }\n  ```\n\n## Extending the container image\n\nYou can extend the `mcp-proxy` container image to include additional executables. For instance, `uv` is not included by\ndefault, but you can create a custom image with it:\n\n```Dockerfile\n# file: mcp-proxy.Dockerfile\n\nFROM ghcr.io/sparfenyuk/mcp-proxy:latest\n\n# Install the 'uv' package\nRUN python3 -m ensurepip && pip install --no-cache-dir uv\n\nENV PATH=\"/usr/local/bin:$PATH\" \\\n    UV_PYTHON_PREFERENCE=only-system\n\nENTRYPOINT [\"catatonit\", \"--\", \"mcp-proxy\"]\n```\n\n## Docker Compose Setup\n\nWith the custom Dockerfile, you can define a service in your Docker Compose file:\n\n```yaml\nservices:\n  mcp-proxy-custom:\n    build:\n      context: .\n      dockerfile: mcp-proxy.Dockerfile\n    network_mode: host\n    restart: unless-stopped\n    ports:\n      - 8096:8096\n    command: \"--pass-environment --port=8096 --sse-host 0.0.0.0 uvx mcp-server-fetch\"\n```\n\n> [!NOTE]\n> Don't forget to set `--pass-environment` argument, otherwise you'll end up with the error \"No interpreter found in\n> managed installations or search path\"\n\n## Command line arguments\n\n```bash\nusage: mcp-proxy [-h] [--version] [-H KEY VALUE] [--transport {sse,streamablehttp}]\n                 [-e KEY VALUE] [--cwd CWD]\n                 [--pass-environment | --no-pass-environment] [--log-level LEVEL] [--debug | --no-debug]\n                 [--named-server NAME COMMAND_STRING]\n                 [--named-server-config FILE_PATH] [--port PORT] [--host HOST]\n                 [--stateless | --no-stateless] [--sse-port SSE_PORT]\n                 [--sse-host SSE_HOST]\n                 [--allow-origin ALLOW_ORIGIN [ALLOW_ORIGIN ...]]\n                 [command_or_url] [args ...]\n\nStart the MCP proxy in one of two possible modes: as a client or a server.\n\npositional arguments:\n  command_or_url        Command or URL to connect to. When a URL, will run an SSE/StreamableHTTP client. Otherwise, if --named-server is not used, this will be the command for the default stdio client. If --named-server is used, this argument is ignored for stdio mode unless no default server is desired. See corresponding options for more details.\n\noptions:\n  -h, --help            show this help message and exit\n  --version             Show the version and exit\n\nSSE/StreamableHTTP client options:\n  -H, --headers KEY VALUE\n                        Headers to pass to the SSE server. Can be used multiple times.\n  --transport {sse,streamablehttp}\n                        The transport to use for the client. Default is SSE.\n\nstdio client options:\n  args                  Any extra arguments to the command to spawn the default server. Ignored if only named servers are defined.\n  -e, --env KEY VALUE   Environment variables used when spawning the default server. Can be used multiple times. For named servers, environment is inherited or passed via --pass-environment.\n  --cwd CWD             The working directory to use when spawning the default server process. Named servers inherit the proxy's CWD.\n  --pass-environment, --no-pass-environment\n                        Pass through all environment variables when spawning all server processes.\n  --log-level LEVEL     Set the log level. Default is INFO.\n  --debug, --no-debug   Enable debug mode with detailed logging output. Equivalent to --log-level DEBUG. If both --debug and --log-level are provided, --debug takes precedence.\n  --named-server NAME COMMAND_STRING\n                        Define a named stdio server. NAME is for the URL path /servers/NAME/. COMMAND_STRING is a single string with the command and its arguments (e.g., 'uvx mcp-server-fetch --timeout 10'). These servers inherit the proxy's CWD and environment from --pass-environment.\n  --named-server-config FILE_PATH\n                        Path to a JSON configuration file for named stdio servers. If provided, this will be the exclusive source for named server definitions, and any --named-server CLI arguments will be ignored.\n\nSSE server options:\n  --port PORT           Port to expose an SSE server on. Default is a random port\n  --host HOST           Host to expose an SSE server on. Default is 127.0.0.1\n  --stateless, --no-stateless\n                        Enable stateless mode for streamable http transports. Default is False\n  --sse-port SSE_PORT   (deprecated) Same as --port\n  --sse-host SSE_HOST   (deprecated) Same as --host\n  --allow-origin ALLOW_ORIGIN [ALLOW_ORIGIN ...]\n                        Allowed origins for the SSE server. Can be used multiple times. Default is no CORS allowed.\n\nExamples:\n  mcp-proxy http://localhost:8080/sse\n  mcp-proxy --transport streamablehttp http://localhost:8080/mcp\n  mcp-proxy --headers Authorization 'Bearer YOUR_TOKEN' http://localhost:8080/sse\n  mcp-proxy --port 8080 -- my-default-command --arg1 value1\n  mcp-proxy --port 8080 --named-server fetch1 'uvx mcp-server-fetch' --named-server tool2 'my-custom-tool --verbose'\n  mcp-proxy --port 8080 --named-server-config /path/to/servers.json\n  mcp-proxy --port 8080 --named-server-config /path/to/servers.json -- my-default-command --arg1\n  mcp-proxy --port 8080 -e KEY VALUE -e ANOTHER_KEY ANOTHER_VALUE -- my-default-command\n  mcp-proxy --port 8080 --allow-origin='*' -- my-default-command\n```\n\n### Example config file\n\n```json\n{\n  \"mcpServers\": {\n    \"fetch\": {\n      \"enabled\": true,\n      \"timeout\": 60,\n      \"command\": \"uvx\",\n      \"args\": [\n        \"mcp-server-fetch\"\n      ],\n      \"transportType\": \"stdio\"\n    },\n    \"github\": {\n      \"timeout\": 60,\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"@modelcontextprotocol/server-github\"\n      ],\n      \"env\": {\n        \"GITHUB_PERSONAL_ACCESS_TOKEN\": \"<YOUR_TOKEN>\"\n      },\n      \"transportType\": \"stdio\"\n    }\n  }\n}\n```\n\n## Testing\n\nCheck the `mcp-proxy` server by running it with the `mcp-server-fetch` server. You can use\nthe [inspector tool](https://modelcontextprotocol.io/docs/tools/inspector) to test the target server.\n\n```bash\n# Run the stdio server called mcp-server-fetch behind the proxy over SSE\nmcp-proxy --port=8080 uvx mcp-server-fetch &\n\n# Connect to the SSE proxy server spawned above using another instance of mcp-proxy given the URL of the SSE server\nmcp-proxy http://127.0.0.1:8080/sse\n\n# Send CTRL+C to stop the second server\n\n# Bring the first server to the foreground\nfg\n\n# Send CTRL+C to stop the first server\n```\n",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "streamablehttp",
        "proxy",
        "streaming",
        "mcp proxy",
        "mcp servers",
        "sse streamablehttp"
      ],
      "category": "realtime-collaboration"
    },
    "tian1ll1--mcp-server-demo": {
      "owner": "tian1ll1",
      "name": "mcp-server-demo",
      "url": "https://github.com/tian1ll1/mcp-server-demo",
      "imageUrl": "/freedevtools/mcp/pfp/tian1ll1.webp",
      "description": "Enables real-time communication between AI models and external tools while managing conversation history and tool registries. Implements a WebSocket interface for interactive client-server communication and includes context management capabilities.",
      "stars": 0,
      "forks": 2,
      "license": "No License",
      "language": "Python",
      "updated_at": "2025-03-24T01:04:34Z",
      "readme_content": "# MCP Server Demo\n\nThis project demonstrates the implementation of a Model Context Protocol (MCP) server. MCP is a protocol designed to facilitate communication between AI models and external tools/services while maintaining context awareness.\n\n## Features\n\n- Basic MCP server implementation\n- Example tool integrations\n- Context management demonstration\n- WebSocket-based real-time communication\n- Simple client example\n\n## Project Structure\n\n```\nmcp-server-demo/\n├── src/\n│   ├── server.py           # Main MCP server implementation\n│   ├── tools/              # Tool implementations\n│   │   ├── __init__.py\n│   │   └── basic_tools.py\n│   ├── context/            # Context management\n│   │   ├── __init__.py\n│   │   └── manager.py\n│   └── utils/             # Utility functions\n│       ├── __init__.py\n│       └── helpers.py\n├── examples/              # Example usage\n│   ├── client.py\n│   └── tools_demo.py\n├── tests/                # Test cases\n│   └── test_server.py\n├── requirements.txt      # Project dependencies\n└── README.md            # This file\n```\n\n## Installation\n\n1. Clone the repository:\n```bash\ngit clone https://github.com/tian1ll1/mcp-server-demo.git\ncd mcp-server-demo\n```\n\n2. Create a virtual environment (recommended):\n```bash\npython -m venv venv\nsource venv/bin/activate  # On Windows: venv\\Scripts\\activate\n```\n\n3. Install dependencies:\n```bash\npip install -r requirements.txt\n```\n\n## Usage\n\n1. Start the MCP server:\n```bash\npython src/server.py\n```\n\n2. Run the example client:\n```bash\npython examples/client.py\n```\n\n## How It Works\n\nThe MCP server implements the following key components:\n\n1. **Context Management**: Maintains conversation history and relevant context for each session.\n2. **Tool Registry**: Manages available tools and their specifications.\n3. **Message Processing**: Handles incoming messages and routes them to appropriate tools.\n4. **WebSocket Server**: Provides real-time communication with clients.\n\n## Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n\n## License\n\nThis project is licensed under the MIT License - see the LICENSE file for details.",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "websocket",
        "realtime",
        "server",
        "realtime collaboration",
        "websocket interface",
        "interactive client"
      ],
      "category": "realtime-collaboration"
    },
    "tonypan2--minesweeper-mcp-server": {
      "owner": "tonypan2",
      "name": "minesweeper-mcp-server",
      "url": "https://github.com/tonypan2/minesweeper-mcp-server",
      "imageUrl": "/freedevtools/mcp/pfp/tonypan2.webp",
      "description": "Engage in strategic gameplay of Minesweeper while the server manages game state and interactions. This server enables MCP client agents to play and challenge their problem-solving skills in an interactive environment.",
      "stars": 106,
      "forks": 6,
      "license": "No License",
      "language": "JavaScript",
      "updated_at": "2025-10-02T18:37:20Z",
      "readme_content": "# Minesweeper MCP Server\n\nThis is an [Model Context Protocol server](https://github.com/modelcontextprotocol/servers) that allows an MCP client agents to play a game of [Minesweeper](<https://en.wikipedia.org/wiki/Minesweeper_(video_game)>). It is intended to be run alongside the [Minesweeper game server](https://github.com/tonypan2/minesweeper-server).\n\n\nView the entire video demo at https://youtu.be/CXXMafVtlEQ (16x speedup).\n\n## Getting started\n\n- Follow the [instructions](https://github.com/tonypan2/minesweeper-server) of the game server to start it locally.\n\n* Build the MCP server:\n\n```bash\nnpm install\nnpm run build\n```\n\n- Configure your MCP client to add the tool. For example, here is how to add the tool to Claude Desktop on Windows's `claude_desktop_config.json` ([locating the file](https://gist.github.com/feveromo/7a340d7795fca1ccd535a5802b976e1f#3-configure-claude-desktop)), assuming you cloned the repo at `C:\\path\\to\\repo\\minesweeper-mcp-server`:\n\n```JSON\n{\n  \"mcpServers\": {\n    \"mcp-server\": {\n      \"command\": \"node\",\n      \"args\": [\"C:\\\\path\\\\to\\\\repo\\\\minesweeper-mcp-server\\\\build\\\\index.js\"],\n      \"env\": {\n        \"DEBUG\": \"*\"\n      }\n    }\n  }\n}\n\n```\n\n- Claude Desktop : Restart Claude Desktop to let it pick up the tools. Be sure to quit from the tray menu icon, not from the app (which simply hides the window). If you click the Tools icon, it should show the new tools:\n\n  \n\n  \n\n## Example prompt\n\n```\nStart a new game of Minesweeper. Try your best to keep playing until you have flagged all mines. Remember that the coordinates are 0-indexed.\n```\n\n## Example interaction\n\nThe actual conversation is very long. Here are some snippets:\n\n## Game start\n\n\n\n## Placing flag at the wrong place\n\n\n\n## Giving up after several attempts",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "minesweeper",
        "mcp",
        "tonypan2",
        "minesweeper mcp",
        "tonypan2 minesweeper",
        "gameplay minesweeper"
      ],
      "category": "realtime-collaboration"
    },
    "yeonupark--mcp-soccer-data": {
      "owner": "yeonupark",
      "name": "mcp-soccer-data",
      "url": "https://github.com/yeonupark/mcp-soccer-data",
      "imageUrl": "/freedevtools/mcp/pfp/yeonupark.webp",
      "description": "Provides real-time football match information through natural language queries, including live scores, match events, team lineups, betting odds, and league metadata for ongoing, upcoming, and recently finished matches worldwide.",
      "stars": 19,
      "forks": 6,
      "license": "MIT License",
      "language": "Python",
      "updated_at": "2025-08-20T03:01:59Z",
      "readme_content": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/yeonupark-mcp-soccer-data-badge.png)](https://mseep.ai/app/yeonupark-mcp-soccer-data)\n\n# ⚽️ Soccerdata MCP Server\n[![smithery badge](https://smithery.ai/badge/@yeonupark/mcp-soccer-data)](https://smithery.ai/server/@yeonupark/mcp-soccer-data)\n- **MCP-Soccerdata** is an open-source [Model Context Protocol (MCP)](https://modelcontextprotocol.io/introduction) server that connects to the **SoccerDataAPI to deliver up-to-date football match information via natural language interactions**. \n\n- Designed for use with MCP-enabled clients such as Claude Desktop, it allows users to retrieve football data by leveraging large language models (LLMs).\n\n---\n\n## ✨ Features\n\n### 🏟️ Live Football Match Insights\nMCP-Soccerdata focuses on delivering **real-time information about ongoing football matches around the world.**\n\n> \"What football matches are being played right now?\"      \n> \"What are the predicted lineups for PSG vs Aston Villa today?\"       \n> \"Please tell me the scores and number of goals from recent football matches.\"\n\n→ Provides relevant football data in a structured format, including the detailed categories described below.\n\n### - Match Listings & Basic Info\n- Global list of all currently active matches\n- Home and away team names\n- Kickoff time and match date\n- Stadium details\n- Current score\n\n\n### - Match Details\n- Match status: scheduled, in progress, or finished\n- Goal breakdown: first half, second half, extra time, penalty shootout\n- Final result: win, draw, or loss\n\n\n### - Key Match Events\n- Goal events (who scored, when, how)\n- Substitutions\n- Yellow and red cards\n- Penalties\n\n\n### - Team Lineups\n- Starting XI\n- Bench players\n- Injury status\n- Team formation\n\n\n### - Odds & Betting Information\n- Win / Draw / Lose odds\n- Over / Under odds\n- Handicap betting odds\n\n\n### - League Metadata\n- League name\n- Country\n- Competition format (e.g., regular season, knockout stage)\n\n\n> ⚠️ Focused exclusively on **live**, **upcoming**, and **recently finished** matches\n\n---\n## 🎥 Demo\n\n\n\n---\n\n## 🚀 Quick Start\n\n### Installing via Smithery\n\nTo install Amadeus MCP Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@yeonupark/mcp-soccer-data):\n\n```bash\nnpx -y @smithery/cli install @yeonupark/mcp-soccer-data --client claude\n```\n\n### Prerequisites\n- Python 3.12+\n- `uv` package manager\n- Soccerdata API account\n- MCP-compatible client (e.g., Claude for Desktop)\n\n\n### 1. Clone and Setup\n\n- Clone the repository\n```bash\ngit clone https://github.com/yeonupark/mcp-soccer-data.git\ncd mcp-soccer-data\n```\n- Install dependencies\n```\nuv sync\n```\n\n### 2. Get Your API Key and Set Environment\n\n- Create a .env file with your credentials:\n```\nAUTH_KEY=your_auth_key\n```\n> Sign up on https://soccerdataapi.com/ and get your own Auth keys.\n\n### 3. Configure MCP Client\n- Register this server in your MCP client (e.g., Claude for Desktop).\n\nEdit `~/Library/Application Support/Claude/claude_desktop_config.json:`\n```\n{\n  \"mcpServers\": {\n      \"mcp-soccer-data\": {\n          \"command\": \"/ABSOLUTE/PATH/TO/PARENT/FOLDER/uv\",\n          \"args\": [\n              \"--directory\",\n              \"/ABSOLUTE/PATH/TO/PARENT/FOLDER/src/\",\n              \"run\",\n              \"--env-file\",\n              \"/ABSOLUTE/PATH/TO/PARENT/FOLDER/.env\",\n              \"server.py\"\n          ]\n      }\n  }\n}\n```\n\n---\n## 🛠️ Tools\nThe follwing tool is exposed to MCP clients:  \n### `get_livescores()`\n-> Returns real-time information about ongoing football matches around the world.\n\n\n---\n## 📝 License\n- This project is licensed under the [MIT License](LICENSE). See the LICENSE file for details.\n- Built with [Model Context Protocol](https://modelcontextprotocol.io/introduction)",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "realtime",
        "matches",
        "soccer",
        "soccer data",
        "mcp soccer",
        "football match"
      ],
      "category": "realtime-collaboration"
    },
    "z9905080--mcp-slack": {
      "owner": "z9905080",
      "name": "mcp-slack",
      "url": "https://github.com/z9905080/mcp-slack",
      "imageUrl": "/freedevtools/mcp/pfp/z9905080.webp",
      "description": "Integrate AI assistants with Slack workspaces to manage channels, send messages, reply to threads, add reactions, and access user and message data. Streamline communication and collaboration within Slack environments using AI-driven tools.",
      "stars": 2,
      "forks": 2,
      "license": "Apache License 2.0",
      "language": "JavaScript",
      "updated_at": "2025-09-12T19:49:04Z",
      "readme_content": "# MCP Server for Slack\n\n[![npm version](https://img.shields.io/npm/v/shouting-mcp-slack.svg)](https://www.npmjs.com/package/shouting-mcp-slack)\n\nA Model Context Protocol (MCP) server implementation for integrating AI assistants with Slack workspaces.\n\n## Overview\n\nThis package provides an MCP server that enables AI assistants to interact with Slack workspaces. It allows AI models to:\n\n- List and browse channels\n- Send messages to channels\n- Reply to threads\n- Add reactions to messages\n- Retrieve channel history\n- Get thread replies\n- List users and retrieve user profiles\n\n## Installation\n\n```bash\n# Install from npm\nnpm install shouting-mcp-slack\n\n# Or install globally\nnpm install -g shouting-mcp-slack\n```\n\nYou can find the package on npm: [shouting-mcp-slack](https://www.npmjs.com/package/shouting-mcp-slack/access)\n\n## Prerequisites\n\nYou need to set up a Slack Bot and obtain the necessary credentials:\n\n1. Create a Slack App in the [Slack API Console](https://api.slack.com/apps)\n2. Add the following Bot Token Scopes:\n   - `channels:history`\n   - `channels:read`\n   - `chat:write`\n   - `reactions:write`\n   - `users:read`\n   - `users:read.email`\n3. Install the app to your workspace\n4. Copy the Bot User OAuth Token\n\n## Configuration\n\nThe server requires the following environment variables:\n\n- `SLACK_BOT_TOKEN`: Your Slack Bot User OAuth Token\n- `SLACK_TEAM_ID`: Your Slack Team ID\n\n## Usage\n\n### Running as a CLI Tool\n\n```bash\n# Set environment variables\nexport SLACK_BOT_TOKEN=xoxb-your-token\nexport SLACK_TEAM_ID=your-team-id\n\n# Run the server\nmcp-server-slack\n```\n\n### Using in Your Code\n\n```typescript\nimport { Server } from \"@modelcontextprotocol/sdk/server/index.js\";\nimport { SlackClient } from \"shouting-mcp-slack\";\n\n// Initialize the server and client\nconst server = new Server({...});\nconst slackClient = new SlackClient(process.env.SLACK_BOT_TOKEN);\n\n// Register your custom handlers\n// ...\n```\n\n## Available Tools\n\nThe server provides the following Slack integration tools:\n\n- `slack_list_channels`: List available channels\n- `slack_post_message`: Send a message to a channel\n- `slack_reply_to_thread`: Reply to a thread\n- `slack_add_reaction`: Add a reaction to a message\n- `slack_get_channel_history`: Get message history from a channel\n- `slack_get_thread_replies`: Get replies in a thread\n- `slack_get_users`: List users in the workspace\n- `slack_get_user_profile`: Get a user's profile\n\n## License\n\nISC\n\n## Author\n\nshouting.hsiao@gmail.com\n\n## Repository\n\nhttps://github.com/z9905080/mcp-slack",
      "npm_url": "",
      "npm_downloads": 0,
      "keywords": [
        "slack",
        "ai",
        "realtime",
        "mcp slack",
        "collaboration slack",
        "assistants slack"
      ],
      "category": "realtime-collaboration"
    }
  }
}